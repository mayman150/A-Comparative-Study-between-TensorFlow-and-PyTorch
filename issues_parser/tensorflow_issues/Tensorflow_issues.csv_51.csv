Issue Number,Issue Title,Issue Body
21024,KeyError: 'IteratorGetDevice' in tf.train.import_meta_graph,"------------------------

### System information
ubuntu 16.04LTS   
protobuf                      3.6.0                 
tensorflow                    1.8.0
and I'm using Python 3.5

### Describe the problem
I got a [KeyError: 'IteratorGetDevice'] when using [tf.train.import_meta_graph] to import a [tensorflow official resnet model](the model is trained using the source code online).

### Source code / logs

[Code:]

import tensorflow as tf
with tf.Session() as sess:
  new_saver = tf.train.import_meta_graph(
          '/home/smn/cifar10_model/model.ckpt-3907.meta')

[Trackback:]

runfile('/home/smn/project/code/test2.py', wdir='/home/smn/project/code')
Traceback (most recent call last):

  File ""<ipython-input-15-af704926d7fa>"", line 1, in <module>
    runfile('/home/smn/project/code/test2.py', wdir='/home/smn/project/code')

  File ""/home/smn/tensorflow/lib/python3.5/site-packages/spyder_kernels/customize/spydercustomize.py"", line 678, in runfile
    execfile(filename, namespace)

  File ""/home/smn/tensorflow/lib/python3.5/site-packages/spyder_kernels/customize/spydercustomize.py"", line 106, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""/home/smn/project/code/test2.py"", line 4, in <module>
    '/home/smn/cifar10_model/model.ckpt-3907.meta')

  File ""/home/smn/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1955, in import_meta_graph
    **kwargs)

  File ""/home/smn/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py"", line 743, in import_scoped_meta_graph
    producer_op_list=producer_op_list)

  File ""/home/smn/tensorflow/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)

  File ""/home/smn/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 460, in import_graph_def
    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)

  File ""/home/smn/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 227, in _RemoveDefaultAttrs
    op_def = op_dict[node.op]

KeyError: 'IteratorGetDevice'"
21022,Feature request : Moore-Penrose pseudoinverse,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Due to its importance in various fields in applied math, it would be beneficial to have a TensorFlow version of NumPy's `pinv` function that calculates the **Moore-Penrose** pseudoinverse of an array : https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.pinv.html

Of course, it is possible to create such function with current existing TensorFlow's functionality (e.g. using `tf.svd`), but it would be more appropriate to have it as an optimized core function. 

### Source code / logs
N/A"
21019,C++: Add gradient for image operators,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
v1.4.0-19-ga52c8d9 1.4.1
- **Python version**:
3.5.2 (default, Nov 23 2017, 16:37:01) 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
Feature request: Add gradient support for the image operators in the C++ API.
"
21018,Waste lots of time to redownload grpc when building with CMake,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 7 64 bit.
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:No.
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.9
- **Python version**:3.5
- **Bazel version (if compiling from source)**:No.
- **GCC/Compiler version (if compiling from source)**:VS2015
- **CUDA/cuDNN version**:CUDA 8.0 and cuDnn 7.0.
- **GPU model and memory**:No.
- **Exact command to reproduce**:
```
cd $TENDORFLOW_DIR/tensorflow/contrib/cmake
cmake .
make -j20
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I want to build tensorflow from source with CMake in Windows, but it fails with the error caused by the fail of gprc. I wonder if there is a way to avoid the redownload gprc when rebuilding form source.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

29>  Creating directories for 'grpc'
29>  Cloning into 'grpc'...
29>  Performing download step (git clone) for 'grpc'
29>  fatal: unable to access 'https://boringssl.googlesource.com/boringssl/': Failed to connect to boringssl.googlesource.com port 443: Timed out
29>  fatal: clone of 'https://boringssl.googlesource.com/boringssl' into submodule path 'D:/CNN/tensorflow/BUILD/grpc/src/grpc/third_party/boringssl-with-bazel' failed
29>  Failed to clone 'third_party/boringssl-with-bazel'. Retry scheduled
29>  Failed to clone 'third_party/boringssl-with-bazel' a second time, aborting
29>  CMake Error at D:/CNN/tensorflow/BUILD/grpc/tmp/grpc-gitclone.cmake:93 (message):
29>  Failed to update submodules in: 'D:/CNN/tensorflow/build/grpc/src/grpc'
"
21016,keras model to estimator in eager mode does not support the use of nested subclassed models,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes, code attached below.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 18.04

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A

- **TensorFlow installed from (source or binary)**:
From source

- **TensorFlow version (use command below)**:
b'v1.9.0-rc2-999-g78909bf81e' 1.9.0

- **Python version**:
3.6.6

- **Bazel version (if compiling from source)**:
0.16.0rc3

- **GCC/Compiler version (if compiling from source)**:
6

- **CUDA/cuDNN version**:
9.1 / 7.1

- **GPU model and memory**:
Titan X, 12GB

- **Exact command to reproduce**:
Run the attached code.

### Describe the problem
This is more of a feature request to add support for the use of nested subclassed models within keras model to estimator eager mode. There is a comment in the relevant section of the code (estimator/keras.py) which says:

    # This will not work for nested subclassed models used as layers.
    # This would be theoretically possible to support, but would add complexity.
    # Only do it if users complain.


### Source code / logs
```
import copy
import tensorflow as tf

tf.enable_eager_execution()
tf.logging.set_verbosity(tf.logging.INFO)

class BaseModel(tf.keras.Model):
    def __init__(self):
        super(BaseModel, self).__init__()
        self.dense1 = tf.keras.layers.Dense(units=10)
        self.dense2 = tf.keras.layers.Dense(units=1)
    def call(self, input):
        """"""Run the model.""""""
        result = self.dense1(input)
        result = self.dense2(result)
        return result
    def get_config(self):
        config = []
        for layer in self.layers:
            config.append({
                'class_name': layer.__class__.__name__,
                'config': layer.get_config()
            })
        return copy.deepcopy(config)

class NestedModel(tf.keras.Model):
    def __init__(self):
        super(NestedModel, self).__init__()
        self.base_model = BaseModel()
        self.dense = tf.keras.layers.Dense(units=1)
    def call(self, input):
        result = self.base_model(input)
        result = self.dense(result)
        return result
    def get_config(self):
        config = []
        for layer in self.layers:
            config.append({
                'class_name': layer.__class__.__name__,
                'config': layer.get_config()
            })
        return copy.deepcopy(config)

model = NestedModel()
model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.0001), loss='mean_squared_error', metrics=['accuracy'])
estimator = tf.keras.estimator.model_to_estimator(keras_model=model)
estimator.train(input_fn=lambda: tf.data.Dataset.from_tensor_slices((tf.random_uniform([100, 10]), tf.random_uniform([100, ]))).batch(2).repeat(10))
```
"
21015,How to get object coordinate with tensorflow?,"How to get object coordinate (tensorflow object_detection api)
(boxes, scores, classes, num) = sess.run(
        [detection_boxes, detection_scores, detection_classes, num_detections],
        feed_dict={image_tensor: frame_expanded})

    # Draw the results of the detection (aka 'visulaize the results')
    vis_util.visualize_boxes_and_labels_on_image_array(
        frame,
        np.squeeze(boxes),
        np.squeeze(classes).astype(np.int32),
        np.squeeze(scores),
        category_index,
        use_normalized_coordinates=True,
        line_thickness=4,
        min_score_thresh=0.80) # sonra elave edecem     
"
21013,"Installing Tensorflow, verifying ","Please go to Stack Overflow for help and support:

Microsoft Windows [Version 6.3.9600]
(c) 2013 Microsoft Corporation. Alle Rechte vorbehalten.

C:\Users\Fritz>pip install ""C:\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
on\dist\tensorflow_gpu-1.5.0-cp35-cp35m-win_amd64.whl""
Requirement 'C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\dist\
\tensorflow_gpu-1.5.0-cp35-cp35m-win_amd64.whl' looks like a filename, but the f
ile does not exist
tensorflow_gpu-1.5.0-cp35-cp35m-win_amd64.whl is not a supported wheel on this p
latform.
You are using pip version 9.0.1, however version 10.0.1 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' comm
and.

C:\Users\Fritz>pip install ""C:\tensorflow\tensorflow\contrib\cmake\build\tf_pyth
on\dist\tensorflow_gpu-1.5.0-cp35-cp35m-win_amd64.whl""

C:\Users\Fritz>pip install --upgrade --no-deps --force-reinstall tensorflow-gpu
Collecting tensorflow-gpu
  Cache entry deserialization failed, entry ignored
  Using cached https://files.pythonhosted.org/packages/51/bc/29202147b513f0ed5fb
dd40f05c6bc2a19722cfb4dd24d77a7c2080a06b4/tensorflow_gpu-1.9.0-cp36-cp36m-win_am
d64.whl
Installing collected packages: tensorflow-gpu
  Found existing installation: tensorflow-gpu 1.9.0
    Uninstalling tensorflow-gpu-1.9.0:
      Successfully uninstalled tensorflow-gpu-1.9.0
Successfully installed tensorflow-gpu-1.9.0
You are using pip version 9.0.1, however version 10.0.1 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' comm
and.

C:\Users\Fritz>python
Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AM
D64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\site-packages\
tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper

    return importlib.import_module(mname)
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\importlib\__in
it__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\site-packages\
tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\site-packages\
tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\site-packages\
tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper

    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\importlib\__in
it__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\site-packages\
tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\site-packages\
tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\site-packages\
tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\site-packages\
tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper

    return importlib.import_module(mname)
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\importlib\__in
it__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\site-packages\
tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\site-packages\
tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\site-packages\
tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper

    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Fritz\AppData\Local\Programs\Python\Python35\lib\importlib\__in
it__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_probl
ems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>
What is wrong ?"
21009,Fine-Grained Control Over TOCO Quantization,"Our (Syntiant Corpâ€™s) neural network inference chips use quantized weights and biases in order to minimize storage and energy consumption. The new Tensorflow experimental quantization feature [tf.contrib.quantize.experimental_create_training_graph](https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/experimental_create_training_graph) supports quantizing weights to between 2 and n bits, but the [tf.contrib.lite.toco_convert](https://www.tensorflow.org/api_docs/python/tf/contrib/lite/toco_convert) tool currently only supports 8 bit quantization. As a result, we have to internally fork the TFLite pipeline before generating the Flatbuffer.

Feature request: Update TOCO to support arbitrary (i.e., 2 to n bit) signed fixed point quantization of weights and biases for both symmetric and asymmetric quantization. Our desired solution would process the quantization [specified at the op or Keras layer level](https://github.com/tensorflow/tensorflow/issues/21008) and not involve quantization specification within the TOCO tool API.
"
21008,Controlling the Quantization Graph-Rewriter,"### System information
Not applicable to this feature request.

### Describe the problem

Our (Syntiant Corp's) neural network inference chips support a continuous range of parameter and activation quantization levels for reducing power consumption. Consequently, we aggressively tune our quantization levels for each application. Based on the research literature and product datasheets we are seeing, it is highly likely there are other chip makers with similar requirements. TF's current graph re-writer approach finds matching blocks in the graph and wraps them in fake quantization operations. This approach poorly serves our use cases for the following reasons:

1. Different layers can have different quantizations. The graph re-writing approach is global to the graph.
2. The graph re-writer attempts to heuristically match the properties of operations that should be re-written. This will generally work for traditional stored-program architectures, but when you are meddling with layers to match silicon you need to drop into TensorBoard to figure out whether the re-writer picked up the unit. If the unit is not picked up, then you are better off not using the re-writer.
3. We have little transparency into changes in the TF codebase on these features. With more explicit specification of layer quantization it is possible to know when the quantization assumptions change and we can track the latest releases of TF.

Our request: We would like to work with an API in which the quantization operations are more explicitly specified at the layer (Keras) or op level. We could then plug the API into our specification of neural network layers built to explicitly match the low-level operations implemented in silicon.

Thank you for open sourcing TF and your efforts in supporting the community. :)

For reference:

* [tf.contrib.quantize.experimental_create_training_graph](https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/experimental_create_training_graph)
* [tf.contrib.quantize.create_eval_graph](https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/create_eval_graph)
* [tf.contrib.quantize.create_training_graph](https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/create_training_graph)

### Source code / logs
Not applicable to this feature request.
"
21001,"Error during tf.evaluate when running on TPU: RuntimeError: All tensors outfed from TPU should preserve batch size dimension, but got scalar ","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Cloud Platform (Linux Debian)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:Binary
- **TensorFlow version (use command below)**:1.9
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: NA (TPU)
- **GPU model and memory**: NA (TPU)
- **Exact command to reproduce**:

### Describe the problem
I am using TPU on GCP to train my model. My model is fully TPU compatible and the training on TPU works as expected when I run `model.train(train_input, steps=num_steps)`. However, when I want to evaluate the model using `train_costs = lfads.evaluate(train_input, name='train_data', steps=100)` I get the following error:

`RuntimeError: All tensors outfed from TPU should preserve batch size dimension, but got scalar Tensor(""OutfeedDequeueTuple:0"", shape=(), dtype=float32, device=/job:worker/task:0/device:TPU:0)`

I constructed the `eval_metrics` in `TPUEstimatorSpec` as described in the MNIST example passing a tuple of (metric_fn, [tensors]) as
```
def m_fn(input):
       return {'test':tf.metrics.mean(input)}
tpu_eval_metrics = (m_fn, [rec_costs])
```


I used the same `input_fn` both for train and evaluate. I also used the same `TPUEstimatorSpec` line in my `model_fn` for training/evaluation. I cannot figure out what is causing this error and didn't have any luck searching online. 

**Note:** I tested the same code but for CPU/GPU and did not run into any errors during evaluation

### Source code / logs
`RuntimeError: All tensors outfed from TPU should preserve batch size dimension, but got scalar Tensor(""OutfeedDequeueTuple:0"", shape=(), dtype=float32, device=/job:worker/task:0/device:TPU:0)`
"
20999,incompatibility : Keras LearningRateScheduler callback and tf.train.optimizer,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem
To make it short, there is an incompatibility between Keras LearningRateScheduler callback and tf.train.optimizer.  Keras optimizers have some specific attributes which are required for Keras callbacks and that does not seem to be the case for tf.train.optimizer.

Unfortunately, Keras optimizers are incompatible with the eager execution mode. So, basically, the user is compelled to choose between using Keras callbacks and the eager execution mode.

Here is a code proving that. It runs fine with a typical Keras optimizer and fails with the tensorflow one.

### Source code / logs
```
import numpy as np
import tensorflow as tf
import tensorflow.keras as keras 
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.callbacks import LearningRateScheduler

def step_decay(epoch):
  initial_rate = 1e-3
  factor = int(epoch / 5)
  lr = initial_rate / (10 ** factor)
  return lr

lr_schedule = LearningRateScheduler(step_decay)

input1 = Input(shape=(10,), name=""input"")
out = Dense(5, activation=""relu"")(input1)
model = Model(inputs=input1, outputs=out)
model.compile(optimizer= tf.train.AdamOptimizer(1e-3), loss='mse')

np.random.seed(0)
X = np.random.random((20, 10)).astype(np.float32)
Y = np.random.random((20, 5)).astype(np.float32)

model.fit(x=X, y=Y, batch_size=1, epochs=10, callbacks=[lr_schedule])
```

### Logs

Epoch 1/10


ValueErrorTraceback (most recent call last)
<ipython-input-9-c41c71fed68e> in <module>()
     24 Y = np.random.random((20, 5)).astype(np.float32)
     25 
---> 26 model.fit(x=X, y=Y, batch_size=1, epochs=10, callbacks=[lr_schedule])

/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1346           initial_epoch=initial_epoch,
   1347           steps_per_epoch=steps_per_epoch,
-> 1348           validation_steps=validation_steps)
   1349 
   1350   def evaluate(self,

/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_arrays.pyc in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
    183       m.reset_states()
    184     # Update callbacks
--> 185     callbacks.on_epoch_begin(epoch)
    186     epoch_logs = {}
    187     if steps_per_epoch is not None:

/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in on_epoch_begin(self, epoch, logs)
     79     logs = logs or {}
     80     for callback in self.callbacks:
---> 81       callback.on_epoch_begin(epoch, logs)
     82     self._delta_t_batch = 0.
     83     self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in on_epoch_begin(self, epoch, logs)
    635   def on_epoch_begin(self, epoch, logs=None):
    636     if not hasattr(self.model.optimizer, 'lr'):
--> 637       raise ValueError('Optimizer must have a ""lr"" attribute.')
    638     lr = self.schedule(epoch)
    639     if not isinstance(lr, (float, np.float32, np.float64)):

ValueError: Optimizer must have a ""lr"" attribute.


"
20998,10 minutes to recover from ResourceExhaustedError,"While it is possible to create a model which is too large for GPU memory, catch the resulting `ResourceExhaustedError` exception, clear the session, and then change the configuration so it fits in memory (such as reducing batch size). However, the time from the initial allocation attempt to catching the exception can take as long as 10 minutes, when I would expect these steps should complete in milliseconds. 

This problem has remained unchanged since tf 1.0, I'm currently on tf 1.8, linux ubuntu 16.04 with an nvidia geforce titan x."
20997,ValueError: Empty Range for RandRange() in Train Function for Custom Classifier,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
I used stock example script provided by TensorFlow to write my own custom code.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 - Colab Notebook

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: 
N/A

- **TensorFlow installed from (source or binary)**:
Not sure

- **TensorFlow version (use command below)**:
1.9

- **Python version**:
3.6 (I think)

- **Bazel version (if compiling from source)**:
Unsure

- **GCC/Compiler version (if compiling from source)**:
Unsure

- **CUDA/cuDNN version**:
N/A

- **GPU model and memory**:
N/A

- **Exact command to reproduce**:

        # Train the CNN model
        mnist_classifier.train(input_fn=train_input_fn,steps=train_steps)

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I want to train multiple CNNs of the same structure (lenet) using randomly selected subsets of varying sizes from the MNIST training set. I was able to use the same line of code successfully in an earlier program. This work is contributing towards the development of a computational model for executive function deficits in ADHD as part of a PhD thesis in Pharmaceutical Sciences.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
[Quantitative WM Model.txt](https://github.com/tensorflow/tensorflow/files/2214655/Quantitative.WM.Model.txt)
"
20995,Cannot set random_seed for DNNClassifier,"### System information
My own code 
Windows 10
Python 3.6.5
Tensorflow 1.9 (problem also occurs with 1.8)
NVIDIA GeForce GTX 1050 (approx total memory 6011MB)
CUDA version 9.0 v9.0.176

Problem Description
Each time I run the code below, I get a different 'Loss for final step' when training my model. I have checked that the input data from train_test_split is constant. I have set the value of tf.random_seed, turned off shuffling and set the value of num_threads.  
Looking at INFO, I can see that when training my model, the config file has _tf_random_seed set to 'None'.  There does not appear to be a way of setting this  for tf.estimator.DNNClassifier. The only solution I have found is to 'hack' run_config.py and set random_seed there. Is this a serious flaw with tf.estimator.DNNClassifier or am I missing something?

from __future__ import print_function
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split

np.random.seed(1)
tf.set_random_seed(50)

df = pd.read_csv('diabetes.csv')
X = df.iloc[:,0:8]
y = df['Outcome']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
                                   stratify=None, random_state=1)

def create_feature_cols():
  return [
    tf.feature_column.numeric_column('Pregnancies'),
    tf.feature_column.numeric_column('Glucose'),
    tf.feature_column.numeric_column('BloodPressure'),
    tf.feature_column.numeric_column('SkinThickness'),
    tf.feature_column.numeric_column('Insulin'),
    tf.feature_column.numeric_column('BMI'),
    tf.feature_column.numeric_column('DiabetesPedigreeFunction'),
    tf.feature_column.numeric_column('Age')
  ]


input_func = tf.estimator.inputs.pandas_input_fn(x=X_train,y=y_train,
             batch_size=10,num_epochs=1000,shuffle=False,num_threads=1)
model  =  tf.estimator.DNNClassifier(hidden_units=[20,20],
          feature_columns=create_feature_cols(),n_classes=2)
model.config._tf_random_seed=1
tf.set_random_seed(1)
model.train(input_fn=input_func,steps=1000)

eval_input_func = tf.estimator.inputs.pandas_input_fn(
      x=X_test,
      y=y_test,
      batch_size=10,
      num_epochs=1,
      shuffle=False,
      num_threads=1)
results = model.evaluate(eval_input_func)"
20994,can not build tensoflow inference library,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
linux ubuntu 16.04 virtual machine 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
deeplearning@deep-learning-virtual-machine:~/Downloads/tensorflow-master$ /home/deeplearning/.bazel/bin/bazel build -c opt --jobs 1 --local_resources 5000,1.0,1.0 //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --verbose_failures
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_volume_patch.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:devices.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:devices.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:grappler_item.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:utils.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:utils.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/clusters:cluster.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/clusters:cluster.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/inputs:utils.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/inputs:utils.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:graph_optimizer.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:graph_rewriter.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:graph_rewriter.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:layout_optimizer.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:layout_optimizer.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:meta_optimizer.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:meta_optimizer.h' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:model_pruner.cc' directly. You should either move the file to this package or depend on an appropriate rule there.
WARNING: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:889:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:model_pruner.h' directly. You should either move the file to this package or depend on an appropriate rule there.
INFO: Found 1 target...
INFO: From Compiling external/protobuf/src/google/protobuf/io/coded_stream.cc:
external/protobuf/src/google/protobuf/io/coded_stream.cc: In member function 'google::protobuf::int64 google::protobuf::io::CodedInputStream::ReadVarint32Fallback(google::protobuf::uint32)':
external/protobuf/src/google/protobuf/io/coded_stream.cc:444:12: warning: 'temp' may be used uninitialized in this function [-Wmaybe-uninitialized]
     return temp;
            ^
INFO: From ProtoCompile tensorflow/core/example/example.pb.cc:
bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles/external/protobuf/src: warning: directory does not exist.
ERROR: /home/deeplearning/Downloads/tensorflow-master/tensorflow/core/BUILD:887:1: C++ compilation of rule '//tensorflow/core:android_tensorflow_lib_lite' failed (Exit 1): arm-linux-androideabi-gcc failed: error executing command 
  (cd /home/deeplearning/.cache/bazel/_bazel_deeplearning/c1eb141ed59057a0511bd96e3345b74f/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/deeplearning/bin:/home/deeplearning/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/deeplearning/bin \
    PWD=/proc/self/cwd \
  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/bin/tensorflow/core/_objs/android_tensorflow_lib_lite/tensorflow/core/common_runtime/function.d '-frandom-seed=bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/bin/tensorflow/core/_objs/android_tensorflow_lib_lite/tensorflow/core/common_runtime/function.o' -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles -iquote external/protobuf -iquote bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles/external/local_config_sycl -isystem external/protobuf/src -isystem bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/genfiles/external/eigen_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-mfpu=neon' '-std=c++11' -DTF_LEAN_BINARY -O2 -Os '--sysroot=external/androidndk/ndk/platforms/android-14/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c tensorflow/core/common_runtime/function.cc -o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-py3-opt/bin/tensorflow/core/_objs/android_tensorflow_lib_lite/tensorflow/core/common_runtime/function.o)deeplearning@deep-learning-virtual-machine:~/Downloads/tensorflow-master$ 

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20992,CheckpointSaverListener before_save,"code:
```
--->listener

 listeners = [
            EvalListener(estimator,
                         lambda: input.input_fn(mode=tf.estimator.ModeKeys.EVAL, params=params,
                                                data_path=params.eval_dir))
        ]

--->train 
        estimator.train(input_fn=train_input_fn, max_steps=FLAGS.max_steps, saving_listeners=listeners)

class EvalListener(CheckpointSaverListener):
    def __init__(self, parent, input_fn, name='eval_data'):

        self.parent = parent
        self.input_fn = input_fn
        self.name = name
        self.history = {}
---> before save
    def before_save(self, session, global_step_value):
        accuracy_spec = self.parent.evaluate(input_fn=self.input_fn, name=self.name)
        for k, v in sorted(six.iteritems(accuracy_spec)):
            if k == ACCU_HEAD:
                if v >= self.history[k][1]:
                    #   TODO
                    #   if this time is better than last time, the model will be save
                    print()
```
i'd wonder whether i can save the model when the accuracy score is better than last, if not the model will not be saved!"
20991,NotFoundError with tf.add_check_numerics_ops(),"I worked on Tensorflow 1.8, and I used tf.add_check_numerics_ops() to check my model.
But it fails if I add tf.add_check_numerics_ops().
```
NotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for model
	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
```

my code is here:
```
with tf.Session() as sess:
    logits, end_points = deeplab(inputs, reuse=False, is_training=False)
    sess.run(tf.global_variables_initializer())
    saver = tf.train.Saver()
    print 'Model established'
    saver.restore(sess, './models/xception65/model.ckpt')
    print 'Weight Loaded'
    checker = tf.add_check_numerics_ops()
    checker, logits_v = sess.run([checker, logits], feed_dict={inputs: img})
```

If I remove `checker` and `tf.add_check_numerics_ops()`, my code works fine."
20990,Convert_variables_to_constants return None after import,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes, minimal example below
- **OS Platform and Distribution**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: using pip3
- **TensorFlow version (use command below)**: git version: v1.8.0-0-g93bc2e2072 tf version 1.8.0
- **Python version**: 3.5.2
- **CUDA/cuDNN version**: not using (because of old GPU)
- **GPU model and memory**: geforce 265
- **Exact command to reproduce**:
1. Convert a graph using tf.graph_util.convert_variables_to_constants
2. Import resulting graph using tf.import_graph_def
3. Eval the variables you want
(4. you get ""None"" on eval)

### The problem
After converting variables to constant
Importing the graph_def results in a graph that always eval to None

```
# minimal testing
with tf.Graph().as_default() as graphy:
    with tf.Session() as sessy:
        # declaring placeholder
        place_holder = tf.placeholder(dtype=tf.float32,shape=[3])
        tmp_feed = {place_holder:[1,1,1]}
        
        # declaring vars and outputs
        rand = tf.random_uniform([3])
        the_var = tf.Variable(rand,name=""the_var"") # depend only on var
        out = tf.add(the_var,place_holder, name=""the_result_node"") # depend on var and placeholder
        out2 = tf.multiply(place_holder,2.0, name=""the_place_holder"") # depend only on placeholder
        
        # getting names of outputs & vars
        out_tensor = [the_var,out,out2]
        out_names = [t.name.split("":"")[0] for t in out_tensor]
        var_names = [place_holder.name.split("":"")[0]] # setting place_holder as blacklist as it must not be replaced by a constant ( doing this or not doesn't change result)
        
        # doing a first run to get a run comparison 
        sessy.run(tf.global_variables_initializer())
        for t in out_tensor:
            print(""first eval"",t.name,sessy.run(t, feed_dict=tmp_feed))
        
        # freezing the graph
        frozen_graphy = tf.graph_util.convert_variables_to_constants(
                                                sess=sessy,
                                                 input_graph_def=graphy.as_graph_def(),
                                                 output_node_names=out_names,
                                                 variable_names_blacklist=var_names)
        print(""freezing done, exported:"", out_names)
        
        # session has been closed seemingly by the convert
            
    # eval to check if the graph was actually modified. < return different values >
    with tf.Session(graph=graphy) as sessu:
        sessu.run(tf.global_variables_initializer())
        for t in out_tensor:
            print(""second eval of graph1"",t.name,sessu.run(t, feed_dict=tmp_feed))
        

#print(frozen_graphy) # just checking what it look like

prefixe = ""frozen_import""
with tf.Graph().as_default() as grapho:
    with tf.Session() as sesso:
        print(""importing frozen graph vars:"", out_names)
        out_list = tf.import_graph_def(frozen_graphy,return_elements=out_names,name=prefixe)
        print(""outlist:"",out_list)
        
        # creating the feed dict for the placeholder
        input_name = prefixe+""/""+var_names[0]
        frozen_feed_dict={input_name+"":0"":[1,1,1]}
        
        # second eval to compare < return None >
        sesso.run(tf.global_variables_initializer())
        for t in out_list:
            print(t.name,"" ="",sesso.run(t,feed_dict=frozen_feed_dict))
            
        # eval of placeholder: < return its value >
        p_h = grapho.get_tensor_by_name(input_name+"":0"")
        print(p_h.name, sesso.run(p_h, feed_dict=frozen_feed_dict))
```
"
20989,Whether tensorflow supports image training or retraining on c++ API,"Tensorflow has supported image training or retraining on python
https://www.tensorflow.org/hub/tutorials/image_retraining
but that tutorials doesn't have any tutorials about  training or retraining on c++ API.
And here are very few discussions about this.
Whether tensorflow supports image training or retraining on c++ API?
THX
"
20987,"install tf 1.4 or up in Virtualenv still suffers this problem, uninstall enum and install enum34 do not help.",
20986,"tf.data.Iterator has_next(), next(), feature request","Request:
has_next()  // opt that check if next data is present at graph execution time without resorting to python try/except
next()  // op that advance the iterator by one at graph execution time

Reason:
has_next():
Iterator seems to be designed for supervised learning. Data is fed from one end, OutOfRangeError occurs when all the data has been used and training stops (if repeat is not set).

In reinforcement learning, the end of episode (last example in file) can be a significant source of information during graph execution time. Catch the exception and run an additional ad-hoc training cycle for the last example is aweful.

next():
In RL, the data can be sequential and has specific time stamps. During graph execution time, the environment written in tensorflow should be able to advance the Iterator using tf.while_loop with iterator.next() until some condition is met 

for example, advance Iterator in a tf.while_loop until time stamp associated with that example is greater than some internal state (variable).

Perhaps the get_next() method should be parameter such that the node can be optionally required to re-executed after every visit. or just have a next() method and let us figure out the appropriate dependency control.

Have I written custom code: Yes
OS Platform and Distribution: Ubuntu 16.04 LTS
TensorFlow installed from: pip
TensorFlow version: 1.8
Bazel version: 0.15.2
CUDA/cuDNN version: Cuda 9.0, cuDNN v7
GPU model and memory: GTX 1070 8GB
Exact command to reproduce: N/A
Mobile device: N/A"
20984,"Can't convert .pb file to .lite, ""toco: error: argument --output_file is required""","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04 virtual machine that is run on legitimate Windows 10 OS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: github.com/googlecodelabs/tensorflow-for-poets-2
- **TensorFlow version (use command below)**: 1.9.0 (also tested 1.5.0, and 1.8.0)
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.15.2 
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A? the tf_env_collect.sh program didnt pull any information on this
- **GPU model and memory**:Rx480 with 4gb of ram
- **Exact command to reproduce**: 
/home/(user name)/.local/bin/toco \
 â€” input_file=tf_files/retrained_graph.pb \
 â€” output_file=tf_files/optimized_graph.lite \
 â€” input_format=TENSORFLOW_GRAPHDEF \
 â€” output_format=TFLITE \
 â€” input_shape=1,224,224,3 \
 â€” input_array=input \
 â€” output_array=final_result \
 â€” inference_type=FLOAT \
 â€” input_data_type=FLOAT

taken from THIS tutorial:
https://heartbeat.fritz.ai/working-through-a-tensorflow-lite-tutorial-on-windows-10-e27ee0e8b8cc

(I followed the tutorial pretty much word for word, virtual ubuntu machine and everything since TOCO never worked originally on my windows machine.)

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.git

When I executed the command above after following the directions of that tutorial, originally with tensorflow 1.5.0 installed, the command failes saying 
""Check failed: parsed_toco_flags.input_format.specified() Missing required flag: input_format
Aborted (core dumped)""
I'm a novice in tensor flow programming but doesn't the command specify input_format?


I looked on other github issues with a similar problem and the experts suggested upgrading tensorflow to 1.8.0 which only results in issues like missing module attributes or import errors. Its probably much more likely that I'm doing something wrong rather than there being a bug with tensorflow, but I'm just so confused as to what mistake I made; I need some expert help. 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

v. 1.5.0 
ubuntu@ubuntu:~$ /home/ubuntu/.local/bin/toco \â€” input_file=tf_files/retrained_graph.pb \â€” output_file=tf_files/optimized_graph.lite \â€” input_format=TENSORFLOW_GRAPHDEF \â€” output_format=TFLITE \â€” input_shape=1,224,224,3 \â€” input_array=input \â€” output_array=final_result \â€” inference_type=FLOAT \â€” input_data_type=FLOAT
2018-07-20 01:55:55.974573: F tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:165] Check failed: parsed_toco_flags.input_format.specified() Missing required flag: input_format
Aborted (core dumped)

v. 1.8.0
ubuntu@ubuntu:~$ /home/ubuntu/.local/bin/toco \â€” input_file=tf_files/retrained_graph.pb \â€” output_file=tf_files/optimized_graph.lite \â€” input_format=TENSORFLOW_GRAPHDEF \â€” output_format=TFLITE \â€” input_shape=1,224,224,3 \â€” input_array=input \â€” output_array=final_result \â€” inference_type=FLOAT \â€” input_data_type=FLOAT
Traceback (most recent call last):
  File ""/home/ubuntu/.local/bin/toco"", line 7, in <module>
    from tensorflow.contrib.lite.toco.python.toco_wrapper import main
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 63, in <module>
    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py"", line 104, in <module>
    from tensorflow.python.framework.importer import import_graph_def
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 32, in <module>
    from tensorflow.python.framework import function
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 36, in <module>
    from tensorflow.python.ops import resource_variable_ops
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 35, in <module>
    from tensorflow.python.ops import variables
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 40, in <module>
    class Variable(checkpointable.CheckpointableBase):
AttributeError: module 'tensorflow.python.training.checkpointable' has no attribute 'CheckpointableBase'

v. 1.9.0
ubuntu@ubuntu:~$ /home/ubuntu/.local/bin/toco \â€” input_file=tf_files/retrained_graph.pb \â€” output_file=tf_files/optimized_graph.lite \â€” input_format=TENSORFLOW_GRAPHDEF \â€” output_format=TFLITE \â€” input_shape=1,224,224,3 \â€” input_array=input \â€” output_array=final_result \â€” inference_type=FLOAT \â€” input_data_type=FLOAT
Traceback (most recent call last):
  File ""/home/ubuntu/.local/bin/toco"", line 7, in <module>
    from tensorflow.contrib.lite.python.tflite_convert import main
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 81, in <module>
    from tensorflow.python import keras
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py"", line 24, in <module>
    from tensorflow.python.keras import activations
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/activations/__init__.py"", line 22, in <module>
    from tensorflow.python.keras._impl.keras.activations import elu
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/__init__.py"", line 21, in <module>
    from tensorflow.python.keras._impl.keras import activations
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/activations.py"", line 23, in <module>
    from tensorflow.python.keras._impl.keras import backend as K
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py"", line 38, in <module>
    from tensorflow.python.layers import base as tf_base_layers
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 25, in <module>
    from tensorflow.python.keras.engine import base_layer
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/__init__.py"", line 21, in <module>
    from tensorflow.python.keras.engine.base_layer import InputSpec
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 33, in <module>
    from tensorflow.python.keras import backend
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend/__init__.py"", line 22, in <module>
    from tensorflow.python.keras._impl.keras.backend import abs
ImportError: cannot import name 'abs'"
20983,[Bug] tf.py_func : Bad tensor -> np.array conversion in py_func (return [] with dtype=int64 is returned as dtype=float64) ,"### Issue template:

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (minimal example attached inline)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX 10.12.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Probably
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: ('v1.5.0-0-g37aa430d84', '1.5.0')
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `python py_func_failure.py`

### Describe the problem

This bug occurs when a `tf.py_func` returns an empty python list `[]` which is intended to be a tensor of type `tf.int64` (as defined in the `Tout=` of the `py_func`). 

In the `ops.script_ops.py` the `_convert` static method is unable to tell the `numpy dtype` of the incoming tensor when it is an empty list.  This causes the `_convert` method to execute a `np.asarray([], dtype=None, order=""C"")` which gives us a `array([], dtype=float64)` instead of a `array([], dtype=int64)` which then does not agree with the output tensor description in the py_func. 

### Source code / logs

See gist here for the example that reproduces this issue: https://gist.githubusercontent.com/sabhiram/3f5eaf7e566ef9aefb3ae6e5b8d2edb0/raw/182ab6bc72b5b7b13fd55964c72030cbc53f7cb3/py_func_failure.py

Inlined here:
```
import tensorflow as tf

def test_func(x):
    """""" Builds a list of ints with length `x`.
    """"""
    return [i for i in range(x)],


def main():
    t0 = tf.constant(0, dtype=tf.int64)
    t0 = tf.py_func(test_func, [t0], tf.int64)

    t1 = tf.constant(1, dtype=tf.int64)
    t1 = tf.py_func(test_func, [t1], tf.int64)

    with tf.Session() as sess:
        sess.run(t1)            # OK
        sess.run(t0)            # 0-th value is double, expects int64

if __name__ == ""__main__"":
    print(tf.GIT_VERSION, tf.VERSION)
    main()
```"
20982,Keras Tensorboard callback and eager execution,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: Colab GPUs
- **Exact command to reproduce**:

### Describe the problem
To make it short, Keras's Tensorboard callback doesn't work in the eager execution mode.

### Source code / logs
RuntimeErrorTraceback (most recent call last)
<ipython-input-4-95ebde3dc4e8> in <module>()
     57 X = np.random.random((20, 10)).astype(np.float32)
     58 Y = np.random.random((20, 5)).astype(np.float32)
---> 59 model.fit(x={'input' : X}, y={'clustering' : Y}, batch_size=1, epochs=10, callbacks=[tensorboard])

/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1331           initial_epoch=initial_epoch,
   1332           steps_per_epoch=steps_per_epoch,
-> 1333           validation_steps=validation_steps)
   1334     else:
   1335       return training_arrays.fit_loop(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_eager.pyc in fit_loop(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
    977       callback_model = model
    978 
--> 979     callbacks.set_model(callback_model)
    980 
    981     callbacks.set_params({

/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in set_model(self, model)
     68   def set_model(self, model):
     69     for callback in self.callbacks:
---> 70       callback.set_model(model)
     71 
     72   def on_epoch_begin(self, epoch, logs=None):

/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in set_model(self, model)
    761         if hasattr(layer, 'output'):
    762           tf_summary.histogram('{}_out'.format(layer.name), layer.output)
--> 763     self.merged = tf_summary.merge_all()
    764 
    765     if self.write_graph:

/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/summary.pyc in merge_all(key, scope)
    309   if _context.executing_eagerly():
    310     raise RuntimeError(
--> 311         'Merging tf.summary.* ops is not compatible with eager execution. '
    312         'Use tf.contrib.summary instead.')
    313   summary_ops = _ops.get_collection(key, scope=scope)

RuntimeError: Merging tf.summary.* ops is not compatible with eager execution. Use tf.contrib.summary instead.
"
20980,High loss of accuracy when using TFLite on Android,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Only on Android (modified the TFLiteClassifier.java class to output the results in a more convenient manner and commented the call to applyFilter() so the model doesn't base its detection on other images than the one I am giving to it)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.3
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Samsung Galaxy S6
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: Python 3.6.0 :: Anaconda 4.3.1 (x86_64)
- **Bazel version (if compiling from source)**: Build label: 0.14.1-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.1.0 (clang-902.0.39.2)
- **CUDA/cuDNN version**: NA
- **GPU model and memory**:  NVIDIA GeForce GT 650M 1 GB
- **Exact command to reproduce**: 
```
cd ../tensorflow-for-poets-2/
python -m scripts.retrain --bottleneck_dir=bottlenecks --how_many_training_steps=$STEPS --model_dir=/Users/jean-baptistebuisson/new_training_dir/model/ --summaries_dir=/Users/jean-baptistebuisson/new_training_dir/training_summaries/mobilenet_1.0_224 --output_graph=/Users/jean-baptistebuisson/new_training_dir/$NAME.pb --output_labels=/Users/jean-baptistebuisson/new_training_dir/retrained_labels.txt --architecture=mobilenet_1.0_224 --image_dir=/Users/jean-baptistebuisson/tf_files/tw --tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/feature_vector/1
cd ../new_training_dir/
cd ../tensorflow/
bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=../new_training_dir/$NAME.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=$NAME.tflite --inference_type=FLOAT --inference_input_type=FLOAT --input_arrays=input --output_array=final_result --input_shapes=1,224,224,3
cd ../new_training_dir/
```

### Describe the problem

The above commands let me retrain a Mobilenet model with 2 categories (positive and negative) and outputs a .pb and a .tflite model that both work fine when tested with label_image with the same data (100 positive images and 100 negative):
For the .pb model:
`True positive: 97; true negative: 98; false positive: 2; false negative: 3`
For the .tflite model:
`True positive: 100; true negative: 100; false positive: 0; false negative: 0`

Then I import the model on Android and run on the same images I get the following result:
`True Positive: 100; True Negative: 0; False Positive: 100; False Negative: 0`


I don't know what is messing with the detection but on Android everything is detected as positive.
Here are the .pb and .tflite models I am using:
[retrainedMNetV1_7_8000.pb.zip](https://github.com/tensorflow/tensorflow/files/2211815/retrainedMNetV1_7_8000.pb.zip)
[retrainedMNetV1_7_8000.tflite.zip](https://github.com/tensorflow/tensorflow/files/2211816/retrainedMNetV1_7_8000.tflite.zip)

And here is my modified TFLiteClassifier.java class:
[TFLiteClassifier.java.zip](https://github.com/tensorflow/tensorflow/files/2211823/TFLiteClassifier.java.zip)

"
20978,Can I use this code to compute the receptive field of a model which I have created on my own. I am talking about calculating receptive fields of not popular conv nets.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20976,map_fn removes a dimension,"I am trying to remap values in an input Tensor using a defaultdict.

    class MyDataSet(object):
        def __init__(self):
            self.class_map = MyDataSet.remap_class()

        @staticmethod
        def remap_class():
            class_remap = defaultdict(lambda: 11)
            class_remap[128] = 0  
            class_remap[130] = 1  
            class_remap[132] = 2
            # ...

        def parser(self, serialized_example):
            features = tf.parse_single_example(
                serialized_example,
                features={
                    'image': tf.FixedLenFeature([], tf.string),
                    'label': tf.FixedLenFeature([], tf.string),
                })
            label = tf.decode_raw(features['label'], tf.uint8)
            label.set_shape([256 * 512])
            label = tf.cast(tf.reshape(label, [256, 512]), tf.int32)
    
            output_label = tf.map_fn(lambda x: self.class_map(x), label)

        #...
        dataset = tf.data.TFRecordDataset(filenames).repeat()
        dataset = dataset.map(self.parser, num_parallel_calls=batch_size)


The label shape is (256,512) but the output_label shape is (256,).  I can't change this shape with tf.reshape() or output_label.set_shape().

This seems to be a bug. "
20975,Toco not working on Windows,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: source, pip install --upgrade tf-nightly
- **TensorFlow version (use command below)**: GIT: 'v1.9.0-rc2-798-gc818bf016d', VERSION: '1.10.0-dev20180719'
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
### Python API
`import tensorflow as tf

converter = tf.contrib.lite.TocoConverter.from_keras_model_file(""model.h5"")
tflite_model = converter.convert()
open(""model.tflite"", ""wb"").write(tflite_model)`
and 
### Command-line
`toco`

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I'm trying to convert a keras model to the tflite format, but the sample code provided on the website doesn't seem to work. As advised in my [other issue](https://github.com/tensorflow/tensorflow/issues/20826), I installed tf-nightly. I used a venv to get a clean environment but for some reason, other issues have arisen. In the case of the Python API, 'lite' seems to be missing from 'tensorflow.contrib' whereas when I run 'toco' from the command line, it raises a ModuleNotFoundError as shown below. Any help would be greatly appreciated.

### Source code / logs
### Python API
`Traceback (most recent call last):
  File ""sandbox/run.py"", line 3, in <module>
    converter = tf.contrib.lite.TocoConverter.from_keras_model_file(""model.h5"")
  File ""C:\beta\lib\site-packages\tensorflow\python\util\lazy_loader.py"", line 54, in __getattr__
    return getattr(module, item)
AttributeError: module 'tensorflow.contrib' has no attribute 'lite'`

### Command-line (running toco)
`Traceback (most recent call last):
  File ""c:\users\user\appdata\local\programs\python\python36\Lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\user\appdata\local\programs\python\python36\Lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\beta\Scripts\toco.exe\__main__.py"", line 5, in <module>
ModuleNotFoundError: No module named 'tensorflow.contrib.lite.python.tflite_convert'`
"
20974,object detection api tflite export_tflite_ssd_graph  error ,"when i ran above SSD model

xport CONFIG_FILE=~/**ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync_2018_07_03**/pipeline.config
export CHECKPOINT_PATH=~/ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync_2018_07_03/model.ckpt

python object_detection/export_tflite_ssd_graph.py \

>â€Šâ€”â€Špipeline_config_path=$CONFIG_FILE \

>â€Šâ€”â€Štrained_checkpoint_prefix=$CHECKPOINT_PATH \

>â€Šâ€”â€Šoutput_directory=$OUTPUT_DIR \

>â€Šâ€”â€Šadd_postprocessing_op=true

/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.

from ._conv import register_converters as _register_converters

OMP: Warning #181: OMP_PROC_BIND: ignored because KMP_AFFINITY has been defined

2018â€“07â€“19 15:44:19.888752: I tensorflow/core/common_runtime/process_util.cc:63] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.

2018â€“07â€“19 15:44:22.853028: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key BoxPredictor_0/BoxEncodingPredictor/act_quant/max not found in checkpoint

Traceback (most recent call last):

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.pyâ€, line 1322, in _do_call

return fn(*args)

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.pyâ€, line 1307, in _run_fn

options, feed_dict, fetch_list, target_list, run_metadata)

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.pyâ€, line 1409, in _call_tf_sessionrun

run_metadata)

tensorflow.python.framework.errors_impl.NotFoundError: Key BoxPredictor_0/BoxEncodingPredictor/act_quant/max not found in checkpoint

[[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, â€¦, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=â€/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):

File â€œobject_detection/export_tflite_ssd_graph.pyâ€, line 137, in <module>

tf.app.run(main)

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.pyâ€, line 126, in run

_sys.exit(main(argv))

File â€œobject_detection/export_tflite_ssd_graph.pyâ€, line 133, in main

FLAGS.max_classes_per_detection)

File â€œ/home/ubuntu/models/research/object_detection/export_tflite_ssd_graph_lib.pyâ€, line 261, in export_tflite_graph

initializer_nodes=â€™â€™)

File â€œ/home/ubuntu/models/research/object_detection/exporter.pyâ€, line 72, in freeze_graph_with_def_protos

saver.restore(sess, input_checkpoint)

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.pyâ€, line 1802, in restore

{self.saver_def.filename_tensor_name: save_path})

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.pyâ€, line 900, in run

run_metadata_ptr)

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.pyâ€, line 1135, in _run

feed_dict_tensor, options, run_metadata)

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.pyâ€, line 1316, in _do_run

run_metadata)

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.pyâ€, line 1335, in _do_call

raise type(e)(node_def, op, message)

tensorflow.python.framework.errors_impl.NotFoundError: Key BoxPredictor_0/BoxEncodingPredictor/act_quant/max not found in checkpoint

[[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, â€¦, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=â€/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op â€˜save/RestoreV2â€™, defined at:

File â€œobject_detection/export_tflite_ssd_graph.pyâ€, line 137, in <module>

tf.app.run(main)

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.pyâ€, line 126, in run

_sys.exit(main(argv))

File â€œobject_detection/export_tflite_ssd_graph.pyâ€, line 133, in main

FLAGS.max_classes_per_detection)

File â€œ/home/ubuntu/models/research/object_detection/export_tflite_ssd_graph_lib.pyâ€, line 261, in export_tflite_graph

initializer_nodes=â€™â€™)

File â€œ/home/ubuntu/models/research/object_detection/exporter.pyâ€, line 67, in freeze_graph_with_def_protos

tf.import_graph_def(input_graph_def, name=â€™â€™)

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.pyâ€, line 432, in new_func

return func(*args, **kwargs)

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/importer.pyâ€, line 513, in import_graph_def

_ProcessNewOps(graph)

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/importer.pyâ€, line 303, in _ProcessNewOps

for new_op in graph._add_new_tf_operations(compute_devices=False): # pylint: disable=protected-access

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.pyâ€, line 3540, in _add_new_tf_operations

for c_op in c_api_util.new_tf_operations(self)

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.pyâ€, line 3540, in <listcomp>

for c_op in c_api_util.new_tf_operations(self)

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.pyâ€, line 3428, in _create_op_from_tf_operation

ret = Operation(c_op, self)

File â€œ/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.pyâ€, line 1718, in __init__

self._traceback = self._graph._extract_stack() # pylint: disable=protected-access

NotFoundError (see above for traceback): Key BoxPredictor_0/BoxEncodingPredictor/act_quant/max not found in checkpoint

[[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, â€¦, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=â€/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]"
20973,TFLITE object detection api  SSD model export issue toco,"Hi, while running

export CONFIG_FILE=~/ssdlite_mobilenet_v2_coco_2018_05_09/pipeline.config
export CHECKPOINT_PATH=~/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt

bazel run -c opt tensorflow/contrib/lite/toco:tocoâ€Šâ€”â€Šâ€” input_file=$OUTPUT_DIR/tflite_graph.pbâ€Šâ€”â€Šoutput_file=$OUTPUT_DIR/detect.tfliteâ€Šâ€”â€Šinput_shapes=1,300,300,3â€Šâ€”â€Šinput_arrays=normalized_input_image_tensorâ€Šâ€”â€Šoutput_arrays=â€™TFLite_Detection_PostProcessâ€™,â€™TFLite_Detection_PostProcess:1',â€™TFLite_Detection_PostProcess:2',â€™TFLite_Detection_PostProcess:3'â€Šâ€”â€Šinference_type=QUANTIZED_UINT8â€Šâ€”â€Šmean_values=128â€Šâ€”â€Šstd_values=128â€Šâ€”â€Šchange_concat_input_ranges=falseâ€Šâ€”â€Šallow_custom_ops

INFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).

INFO: Found 1 targetâ€¦

Target //tensorflow/contrib/lite/toco:toco up-to-date:

bazel-bin/tensorflow/contrib/lite/toco/toco

INFO: Elapsed time: 0.392s, Critical Path: 0.01s

INFO: 0 processes.

INFO: Build completed successfully, 1 total action

INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco â€˜â€Šâ€”â€Šinput_file=/home/ubuntu/tflite/tflite_graph.pbâ€™ â€˜â€Šâ€”â€Šoutput_file=/home/ubuntu/tflite/detect.tfliteâ€™ â€˜â€Šâ€”â€Šinput_shapes=1,300,300,3â€™ â€˜â€Šâ€”â€Šinput_arrays=normalized_input_image_tensorâ€™ â€˜â€Šâ€”â€Šoutput_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3â€™ â€˜INFO: Build completed successfully, 1 total action

2018â€“07â€“19 15:32:40.725811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TFLite_Detection_PostProcess

2018â€“07â€“19 15:32:40.742690: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1070 operators, 1570 arrays (0 quantized)

2018â€“07â€“19 15:32:40.791000: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1070 operators, 1570 arrays (0 quantized)

2018â€“07â€“19 15:32:40.849051: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 116 operators, 310 arrays (1 quantized)

2018â€“07â€“19 15:32:40.852027: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 116 operators, 310 arrays (1 quantized)

2018â€“07â€“19 15:32:40.853807: F tensorflow/contrib/lite/toco/tooling_util.cc:1621] Array FeatureExtractor/MobilenetV2/Conv/Relu6, which is an input to the DepthwiseConv operator producing the output array FeatureExtractor/MobilenetV2/expanded_conv/depthwise/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or passâ€Šâ€”â€Šdefault_ranges_min= andâ€Šâ€”â€Šdefault_ranges_max= if you do not care about the accuracy of results."
20972,Estimator does not work with tf.contrib.cudnn_rnn.CudnnGRU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  v1.9.0-0-g25c197e023 1.9.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0, 7.0.5
- **GPU model and memory**: GTX 1070 8G
- **Exact command to reproduce**: https://gist.github.com/matthew-z/57d531fa8ca5616b59afb75c1fd9d9f7
 
### Describe the problem

It raises `ValueError: All tensors of a saveable object must be on the same device: cudnn_gru/opaque_kernel_saveable` if `CudnnGRU` is used in `model_fn` of a customized estimator. However,`CudnnLSTM` `CudnnRNNTanh`, and `CudnnRNNRelu` work well

  
### Source code / logs

Source Code:
https://gist.github.com/matthew-z/57d531fa8ca5616b59afb75c1fd9d9f7

Logs:
https://gist.github.com/matthew-z/f43c2188aba7c62c971cf0127fe5b80d
"
20968,"tensorRT error '*** stack smashing detected ***: python terminated', ","I am using tensorflow version 1.7, cuda8.0, python 2.7 to optimize the tensorflow VGG-16 model using the code below:-


    vgg_checkpoint = 'trained_models/models/vgg16/vgg_16.ckpt'
    image_decoded = tf.placeholder(dtype=tf.float32, shape=[None, None, None, 3], name=""input"")
    logits, _ = vgg_16(image_decoded, is_training=False)
    saver = tf.train.Saver()
    saver.restore(sess, vgg_checkpoint)

    input_graph_def = sess.graph.as_graph_def()
    output_graph_def = tf.graph_util.convert_variables_to_constants(
        sess,
        input_graph_def,
        ['vgg_16/fc8/squeezed'],
    )
    frozen_graph = tf.graph_util.remove_training_nodes(output_graph_def)
    output_graph = ""../model/tensorRT_check.pb""
    with tf.gfile.GFile(output_graph, ""wb"") as f:
        f.write(frozen_graph.SerializeToString())
    trt_graph = trt.create_inference_graph(
        input_graph_def=frozen_graph,
        outputs=['input', 'vgg_16/fc8/squeezed'],
        max_batch_size=1,
        max_workspace_size_bytes=1 << 25,
        precision_mode='FP16',
        minimum_segment_size=50
    )
    

and I am getting following error.

`2018-07-19 19:00:34.093716: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-07-19 19:00:34.149018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-07-19 19:00:34.149335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 11.33GiB
2018-07-19 19:00:34.149350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-07-19 19:00:34.395658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-19 19:00:34.395685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
2018-07-19 19:00:34.395691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
2018-07-19 19:00:34.395901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7324 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Converted 32 variables to const ops.
2018-07-19 19:00:41.076819: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1
*** stack smashing detected ***: python terminated
Aborted (core dumped)
`

"
20966,switch for nccl,"in configure.py(r1.10):

> line 1132 to 1141

```
    if is_windows():
      nccl_lib_path = 'lib/x64/nccl.lib'
    elif is_linux():
      nccl_lib_path = '/usr/lib/x86_64-linux-gnu/libnccl.so.%s' % tf_nccl_version
      #nccl_lib_path = 'lib64/libnccl.so.%s' % tf_nccl_version
    elif is_macos():
      nccl_lib_path = 'lib/libnccl.%s.dylib' % tf_nccl_version

    nccl_lib_path = os.path.join(nccl_install_path, nccl_lib_path)
    nccl_hdr_path = os.path.join(nccl_install_path, '/usr/include/nccl.h')#'include/nccl.h')
```
in case of install nccl by `sudo apt install libnccl2=2.2.13-1+cuda9.0 libnccl-dev=2.2.13-1+cuda9.0
`"
20964,Using tf.contrib.training.batch_sequences_with_states with tf.data.Dataset,"As far as I know, tf.contrib.training.SequenceQueuingStateSaver relies on queues and queue-runners, so we'll still need to call tf.train.start_queue_runners() when used with the new tf.data.Dataset API.

However, when I use tf.data.Iterator.from_structure to construct the input that is reusable with many different datasets, error occurs unless I initialize the Iterator before calling tf.train.start_queue_runners(). So I need to switch to a certain dataset first, and it's a quite strange usage.

```python
iterator = Iterator.from_structure(tf.int64, tf.TensorShape([]))

dataset_range = Dataset.range(10)
range_initializer = iterator.make_initializer(dataset_range)

dataset_evens = dataset_range.filter(lambda x: x % 2 == 0)
evens_initializer = iterator.make_initializer(dataset_evens)

# Define a model based on the iterator; in this example, the model_fn
# is expected to take scalar tf.int64 Tensors as input (see
# the definition of 'iterator' above).
prediction, loss = model_fn(iterator.get_next())

# I need to pick a certain dataset, and run the initializer first.
sess.run(range_initializer)
tf.train.start_queue_runners(sess=sess, coord=coord)

# Train for `num_epochs`, where for each epoch, we first iterate over
# dataset_range, and then iterate over dataset_evens.
for _ in range(num_epochs):
  # Initialize the iterator to `dataset_range`
  sess.run(range_initializer)
  while True:
    try:
      pred, loss_val = sess.run([prediction, loss])
    except tf.errors.OutOfRangeError:
      break

  # Initialize the iterator to `dataset_evens`
  sess.run(evens_initializer)
  while True:
    try:
      pred, loss_val = sess.run([prediction, loss])
    except tf.errors.OutOfRangeError:
      break
```

By doing so, it seems that the queue will be closed when I switch to another dataset next time.

So will tensorflow add batch_sequences_with_states like support to tf.data.Dataset without relying on the old queue mechanisms and calling tf.train.start_queue_runners explicitly."
20963,find a bug for tensorflow\cc\framework\gradients.cc,"size_t dx_index = 0;
    for (const Edge* e : n->in_edges()) {
      if (e->IsControlEdge()) continue;
      if (dx_index == dx.size()) {
        return errors::Internal(
            ""Invalid gradient output index: "", dx_index, "" size: "", dx.size());
      }
      TF_RETURN_IF_ERROR(
          BackpropAlongEdge(dx[dx_index++], {e->src(), e->src_output()}));
    }

above code,  when   dx have different  in_edges() order. Edge is unsortd set, dx is vector"
20962,Miss-leading explanation of 'export_tflite_ssd_graph.py' ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: PC
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: newest from master branch
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.14
- **GCC/Compiler version (if compiling from source)**: gcc-7.1
- **CUDA/cuDNN version**: cuda-9.2,cudnn-7.1
- **GPU model and memory**: 16G
- **Exact command to reproduce**:

The function explanation of **export_tflite_ssd_graph.py** said 
`
Outputs:
If add_postprocessing_op is true: frozen graph adds a
  TFLite_Detection_PostProcess custom op node has four outputs:
  detection_boxes: a float32 tensor of shape [1, num_boxes, 4] with box
  locations
  detection_scores: a float32 tensor of shape [1, num_boxes]
  with class scores
  detection_classes: a float32 tensor of shape [1, num_boxes]
  with class indices
  num_boxes: a float32 tensor of size 1 containing the number of detected boxes
`

But according to my test the output sequence is  **boxes, indice, scores ,num** not **boxes,scores,indice,num**. (in `detection_postprocess.cc` seq is also **boxes, indice, scores ,num** )
And in **export_tflite_ssd_graph_lib.py** 
the `def get_const_center_size_encoded_anchors(anchors)` 's return shape is obviously 
**[num_anchors,4]** not **[4,num_anchors]**."
20961,can't use mnist data after download using tensoerflow.It's give below error,"**I'll use os as fedora
Install python via Conda-navigator
Editor use pycharm**

Os Platform:fedora
Tensorflow installed from anaconda
Tensorflow version:1.8.0
python version : 3.6.5
Bazel version:Not installed

WARNING:tensorflow:From /home/sunil/PycharmProjects/test/testFile.py:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.

Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
Extracting MNIST_data/train-images-idx3-ubyte.gz
WARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.

Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.

Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST_data/train-labels-idx1-ubyte.gz
WARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.

Instructions for updating:
Please use tf.data to implement this functionality.
WARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.

Instructions for updating:
Please use tf.one_hot on tensors.
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.

Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models."
20960,Placeholder will cause incompelet shape bug in tf.profiler.profile,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: Python 3.6.5 (default, Apr  1 2018, 05:46:30)
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Please run the code in Source code / logs section.


### Describe the problem
The [documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md#profile-model-float-operations) tells us that
> It is suggested to pass in -run_meta_path if shape is only known during runtime. tfprof can fill in the missing shape with the runtime shape information from RunMetadata. 

It cannot work when the graphs contain `placeholder`.  
The reason is that the node name of placeholder in RunMetadata is different from its name in graph_def.  
The bug will be reproduced in the next section.

### Source code / logs
The code reproduce the bug  
```python   
# The code reproduce the bug
import tensorflow as tf
import numpy as np

X = tf.placeholder(tf.float32, shape=(None, 3), name='X')
y = tf.constant([[1, 2, 3, 4],
                 [1, 2, 3, 4],
                 [1, 2, 3, 4]], dtype=tf.float32)
mul_op = tf.matmul(X, y)

if __name__ == ""__main__"":
    x = np.random.random([2, 3])

    tf.profiler.profile(
        tf.get_default_graph(),
        cmd='op',
        options=tf.profiler.ProfileOptionBuilder.float_operation())

    run_metadata = tf.RunMetadata()
    with tf.Session() as sess:
        print(sess.run(mul_op, feed_dict={X: x},
                       options=tf.RunOptions(
                           trace_level=tf.RunOptions.FULL_TRACE),
                       run_metadata=run_metadata))

    tf.profiler.profile(
        tf.get_default_graph(),
        cmd='op', run_meta=run_metadata,
        options=tf.profiler.ProfileOptionBuilder.float_operation())

```
The output is  
```
1 ops no flops stats due to incomplete shapes.
Parsing Inputs...
Incomplete shape.
Incomplete shape.
Incomplete shape.

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
op: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops

======================End of Report==========================
Incomplete shape.
Incomplete shape.
Incomplete shape.
2018-07-19 03:34:30.726274: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[[1.4957699 2.9915397 4.48731   5.9830794]
 [2.083395  4.16679   6.2501845 8.33358  ]]
Parsing Inputs...
Incomplete shape.
Incomplete shape.
Incomplete shape.

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================
Incomplete shape.
Incomplete shape.
Incomplete shape.

Doc:
op: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops

======================End of Report==========================
```
When I first call the tf.profiler.profile, I do not pass run_metadata to it. Therefore, here should be the `incomplete shape`. However, it does not make sense that it still reports the `Incomplete shape` even if I pass the run_matadata to call the tf.profiler.profile again. 

I think the problem is that the node name of placeholder in RunMetadata is different with its name in graph_def.  I traced the code until [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/profiler/tfprof_logger.py#L46), and printed the 'run_meta' and 'graph_def'.  
The output is  
```pdb  
(Pdb) p run_meta
step_stats {
  dev_stats {
    device: ""/job:localhost/replica:0/task:0/device:CPU:0""
    node_stats {
      node_name: ""_SOURCE""
      all_start_micros: 1531987298848369
      op_start_rel_micros: 2
      op_end_rel_micros: 3
      all_end_rel_micros: 9
      memory {
        allocator_name: ""cpu""
      }
      timeline_label: ""_SOURCE = NoOp()""
      scheduled_micros: 1531987298848358
      memory_stats {
      }
    }
    node_stats {
      node_name: ""_arg_X_0_0""
      all_start_micros: 1531987298848383
      op_end_rel_micros: 2
      all_end_rel_micros: 7
      memory {
        allocator_name: ""cpu""
      }
      output {
        tensor_description {
          dtype: DT_FLOAT
          shape {
            dim {
              size: 2
            }
            dim {
              size: 3
            }
          }
          allocation_description {
            requested_bytes: 24
            allocator_name: ""cpu""
          }
        }
      }
      timeline_label: ""_arg_X_0_0 = _Arg()""
      scheduled_micros: 1531987298848378
      memory_stats {
      }
    }
    node_stats {
      node_name: ""Const""
      all_start_micros: 1531987298848391
      op_end_rel_micros: 3
      all_end_rel_micros: 4
      memory {
        allocator_name: ""cpu""
      }
      output {
        tensor_description {
          dtype: DT_FLOAT
          shape {
            dim {
              size: 3
            }
            dim {
              size: 4
            }
          }
          allocation_description {
            requested_bytes: 48
            allocator_name: ""cpu""
            ptr: 140275567738880
          }
        }
      }
      timeline_label: ""Const = Const()""
      scheduled_micros: 1531987298848390
      memory_stats {
        persistent_memory_size: 48
      }
    }
    node_stats {
      node_name: ""MatMul""
      all_start_micros: 1531987298848396
      op_end_rel_micros: 69
      all_end_rel_micros: 72
      memory {
        allocator_name: ""cpu""
        total_bytes: 32
        peak_bytes: 32
        live_bytes: 32
        allocation_records {
          alloc_micros: 1531987298848437
          alloc_bytes: 32
        }
      }
      output {
        tensor_description {
          dtype: DT_FLOAT
          shape {
            dim {
              size: 2
            }
            dim {
              size: 4
            }
          }
          allocation_description {
            requested_bytes: 32
            allocated_bytes: 32
            allocator_name: ""cpu""
            allocation_id: 1
            has_single_reference: true
            ptr: 140275513085952
          }
        }
      }
      timeline_label: ""MatMul = MatMul(_arg_X_0_0, Const)""
      scheduled_micros: 1531987298848395
      memory_stats {
      }
    }
    node_stats {
      node_name: ""_retval_MatMul_0_0""
      all_start_micros: 1531987298848470
      op_start_rel_micros: 1
      op_end_rel_micros: 1
      all_end_rel_micros: 3
      memory {
        allocator_name: ""cpu""
      }
      timeline_label: ""_retval_MatMul_0_0 = _Retval(MatMul)""
      scheduled_micros: 1531987298848468
      memory_stats {
      }
    }
  }
}
```

```pdb    
(Pdb) p graph.as_graph_def()
node {
  name: ""X""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: -1
        }
        dim {
          size: 3
        }
      }
    }
  }
}
node {
  name: ""Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 3
          }
          dim {
            size: 4
          }
        }
        tensor_content: ""\000\000\200?\000\000\000@\000\000@@\000\000\200@\000\000\200?\000\000\000@\000\000@@\000\000\200@\000\000\200?\000\000\000@\000\000@@\000\000\200@""
      }
    }
  }
}
node {
  name: ""MatMul""
  op: ""MatMul""
  input: ""X""
  input: ""Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""transpose_a""
    value {
      b: false
    }
  }
  attr {
    key: ""transpose_b""
    value {
      b: false
    }
  }
}
versions {
  producer: 26
}

```

We can find the node name of placeholder is different in the two output, in the run_meatdata, the node name is `_arg_X_0_0`, however, it is `X` in the graph_def. 
As a result, code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/profiler/tfprof_logger.py#L46) will not work properly."
20959,Build tensorflow-model-server for gpu - Cannot find cuda library libcudnn.so.6,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux ip-172-30-1-83 4.4.0-1062-aws #71-Ubuntu SMP Fri Jun 15 10:07:39 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609

- **CUDA/cuDNN version**:

== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7

But I also have these files:
/usr/lib/x86_64-linux-gnu/libcudnn.so -> /etc/alternatives/libcudnn_so
/usr/lib/x86_64-linux-gnu/libcudnn.so.6 -> libcudnn.so.6.0.21
/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21
/usr/lib/x86_64-linux-gnu/libcudnn.so.7 -> libcudnn.so.7.0.5
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5

- **GPU model and memory**: Tesla K80

### Describe the problem

I try to build tensorflow-model-server with gpu support, as I saw that the apt-get version is only for CPU.

I did:
1. git clone --recurse-submodules https://github.com/tensorflow/serving
2. bazel clean --expunge && export TF_NEED_CUDA=1
3. bazel query 'kind(rule, @local_config_cuda//...)'

And got:

Cuda Configuration Error: Cannot find cuda library libcudnn.so.6

When I do: bazel build -c opt --config=cuda tensorflow_serving/model_servers:tensorflow_model_server
I get the same error.
"
20956,tensorflow.contrib.autograph generated the code to update variables with conditions but throws UnboundLocalError,"### Environment

python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.9.0-rc2-724-gc0dbd7e456 1.10.0-dev20180718

== cat /etc/issue ===============================================
Linux localhost 4.15.13-x86_64-linode106 #1 SMP Tue Mar 27 14:42:14 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.4 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux localhost 4.15.13-x86_64-linode106 #1 SMP Tue Mar 27 14:42:14 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy             1.14.5
protobuf          3.6.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.6.0
tf.GIT_VERSION = v1.6.0-0-gd2e24b6039
tf.COMPILER_VERSION = v1.6.0-0-gd2e24b6039
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

### Describe the problem

We use `tensorflow.contrib.autograph` and try to update the variable with conditional logic. The code is simple and should work with Python.

``` 
a = ""test""
if x == 0:
  a = ""test1""
else:
  a = ""test2""
```

But it throws `UnboundLocalError` and unable to access the existing variables.

### Source code / logs

Here is the example code to reproduce the issue.

```
#!/usr/bin/env python

import tensorflow as tf
from tensorflow.contrib import autograph as ag

def f(x):
  a = ""test""
  if x == 0:
    a = ""test1""
  else:
    a = ""test2""

converted_f = ag.to_graph(f)

print(ag.to_code(f))
print(converted_f(tf.constant(-1)))
```

And it is the generated code of the above code.

```
from __future__ import print_function
import tensorflow as tf

def tf__f(x):
  try:
    with tf.name_scope('f'):
      a = 'test'

      def if_true():
        with tf.name_scope('if_true'):
          a, = a,
          a = 'test1'
          return 1,

      def if_false():
        with tf.name_scope('if_false'):
          a_1, = a,
          a_1 = 'test2'
          return 1,
      with ag__.utils.control_dependency_on_returns(ag__.utils.run_cond(tf.
          equal(x, 0), if_true, if_false)):
        x_1, if_true_1, if_false_1 = ag__.utils.alias_tensors(x, if_true,
            if_false)
  except:
    ag__.rewrite_graph_construction_error(ag_source_map__)
```

It is the full error log when try to run generated graph with test data.

```
Traceback (most recent call last):
  File ""/tmp/tmpn2n1i2l2.py"", line 20, in tf__f
    equal(x, 0), if_true, if_false)):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/autograph/utils/multiple_dispatch.py"", line 50, in run_cond
    return control_flow_ops.cond(condition, true_fn, false_fn)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2048, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1895, in BuildCondBranch
    original_result = fn()
  File ""/tmp/tmpn2n1i2l2.py"", line 10, in if_true
    a, = a,
UnboundLocalError: local variable 'a' referenced before assignment

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./demo2.py"", line 16, in <module>
    print(converted_f(tf.constant(-1)))
  File ""/tmp/tmpn2n1i2l2.py"", line 24, in tf__f
    ag__.rewrite_graph_construction_error(ag_source_map__)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/autograph/core/errors.py"", line 158, in rewrite_graph_construction_error
    raise new_error
tensorflow.contrib.autograph.core.errors.GraphConstructionError: Traceback (most recent call last):
  File ""./demo2.py"", line 8, in f
    if x == 0:
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/autograph/utils/multiple_dispatch.py"", line 50, in run_cond
    return control_flow_ops.cond(condition, true_fn, false_fn)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 2048, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1895, in BuildCondBranch
    original_result = fn()
  File ""/tmp/tmpn2n1i2l2.py"", line 10, in if_true
    a, = a,

local variable 'a' referenced before assignment
```
"
20955, the quantized form of Shape operation is not yet implemented,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.9.0
- **Python version**:2.7.3
- **Bazel version (if compiling from source)**:0.12.0
- **GCC/Compiler version (if compiling from source)**:c++11
- **CUDA/cuDNN version**:7.5.18
- **GPU model and memory**:TITAN,12GB
- **Exact command to reproduce**:
 ./bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=~/deeplabv3_mobinetv2/frozen_inference_graph.pb   --output_file=~/deeplabv3_mobinetv2/foo.cc   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --inference_type=QUANTIZED_UINT8   --input_shape=1,513,513,3   --input_array=ImageTensor   --output_array=logits/semantic/BiasAdd   --default_ranges_min=0   --default_ranges_max=6   --mean_value=127.5   --std_value=127.5

### Describe the problem
I want to use dummy quantization to quantize deeplabv3_mobilenetv2 model ""mobilenetv2_coco_voc_trainaug"" from https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md.
But I got the shape operation is not yet implemented.
Do you have plan to implement it?

### Source code / logs
2018-07-19 13:49:26.114180: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:459] Unimplemented: this graph contains an operator of type Shape for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
Aborted (core dumped)
"
20954,Entry point not found error reported when calling tf.contrib.rnn.BasicRNNCell,"Python version = 3.5.0

- Have I written custom code --> No
- OS Platform and Distribution  --> Windows 10
- TensorFlow installed from --> pip install --ignore-installed --upgrade tensorflow-gpu 
- TensorFlow version --> 1.8.0-gpu
- Bazel version --> NA
- CUDA/cuDNN version --> 9.0
- GPU model and memory --> GeForce GTX 960M, 3GB
- Exact command to reproduce --> See below

X0 = tf.placeholder(tf.float32, [None, n_inputs])
X1 = tf.placeholder(tf.float32, [None, n_inputs])
basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)  # Errs out here

Error message:

""The procedure entry point
?AddCleanup@Arenalmpl@internal@protobuf@google@@QEAAXXEPEAXP6AX0@Z@Z could not be located in the dynamic link library C:\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\contrib\data\_dataset_ops.so""

![image](https://user-images.githubusercontent.com/8305769/42923931-d0e91e26-8adc-11e8-802c-29ff0a065301.png)
"
20952,How can tensorflow compiled with MKL rathan MKL-DNN,"


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.9.0
- **Python version**: python 3.4
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**:


### Describe the problem
Is it possible to compile tensorflow with Eigen using MKL as backend, but not using MKL-DNN's optimized kernels? And how? As mentioned here https://github.com/intel/mkl-dnn/issues/282 , I think in my case (conv2d on sandybridge), the MKL-DNN integration seems harm to performance.

Thanks very much.

### Source code / logs

"
20950,C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source (r1.9 and r1.10)
- **TensorFlow version (use command below)**: N/A
- **Python version**: 3.7.0
- **Bazel version (if compiling from source)**: 0.15.1
- **GCC/Compiler version (if compiling from source)**: gcc-7
- **CUDA/cuDNN version**: 9.2/7.1.4
- **GPU model and memory**: GTX 1060 3GB
- **Exact command to reproduce**: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`

### Describe the problem
Unable to build TensorFlow from source.

##### Output of `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`
```
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
Loading: 
Loading: 0 packages loaded
WARNING: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded).
INFO: Found 1 target...
[0 / 1] [-----] BazelWorkspaceStatusAction stable-status.txt
ERROR: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/protobuf_archive/BUILD:665:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::New(PyTypeObject*, PyObject*, PyObject*)':
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:139:46: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]
   static char* kwlist[] = {""descriptor_db"", 0};
                                              ^
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindMessageByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]
        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:169:7: note: in expansion of macro 'PyString_AsStringAndSize'
   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {
       ^~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindFileByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]
        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:190:7: note: in expansion of macro 'PyString_AsStringAndSize'
   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {
       ^~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindFieldByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]
        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:206:7: note: in expansion of macro 'PyString_AsStringAndSize'
   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {
       ^~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindExtensionByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]
        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:224:7: note: in expansion of macro 'PyString_AsStringAndSize'
   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {
       ^~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindEnumTypeByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]
        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:241:7: note: in expansion of macro 'PyString_AsStringAndSize'
   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {
       ^~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindOneofByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]
        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:258:7: note: in expansion of macro 'PyString_AsStringAndSize'
   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {
       ^~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindServiceByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]
        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:275:7: note: in expansion of macro 'PyString_AsStringAndSize'
   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {
       ^~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindMethodByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]
        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:292:7: note: in expansion of macro 'PyString_AsStringAndSize'
   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {
       ^~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindFileContainingSymbol(google::protobuf::python::PyDescriptorPool*, PyObject*)':
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]
        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:309:7: note: in expansion of macro 'PyString_AsStringAndSize'
   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {
       ^~~~~~~~~~~~~~~~~~~~~~~~
At global scope:
cc1plus: warning: unrecognized command line option '-Wno-writable-strings'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1.309s, Critical Path: 1.08s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
"
20949,How to add/print logs from TFLite JNI.cc (native code) files?,"Hello, I'm trying to put prints in the [native code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc) files of TFLite files in the repository to get good understanding of TFlite and Android ecosystem. I have tried including <stdio.h> and printing with printf/fprintf/cout. Noting seems to be coming in the adb logcat.

Can anyone help me how to get the prints from the native code?

Thanks"
20948,How can I do type cast:  Tensor<Float>  to  Tensor<String> ? ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20947,tensorflow 1.8.0 may not be compatible with numpy 1.14.5,
20942,tf.contrib.factorization.KMeansClustering training on CPU and not GPU,"I am currently running tensorflow-gpu 1.9 avx2 with cuda 9.2. When I train the model below, I notice that my CPU usage is 99% but my Tesla M60 usage is at 0%. 



    def input_fn():
        return tf.train.limit_epochs(tf.convert_to_tensor(points, dtype=tf.float32), num_epochs=1)

    kmeans = tf.contrib.factorization.KMeansClustering(num_clusters=num_clusters, use_mini_batch=False)
    num_iterations = 20

    previous_centers = None
    for _ in range(num_iterations):
        kmeans.train(input_fn)
        cluster_centers = kmeans.cluster_centers()
        print('score:', kmeans.score(input_fn))

    # map the input points to their clusters
    cluster_indices = list(kmeans.predict_cluster_index(input_fn))

`"
20938,"1.9 RC2 build from source fails cc1plus: warning: unrecognized command line option ""-Wno-writable-strings""","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Debian 9

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
n/a

- **TensorFlow installed from (source or binary)**:
Source

- **TensorFlow version (use command below)**:
1.9 rc2
- **Python version**:
3.6

- **Bazel version (if compiling from source)**:
0.15.2

- **GCC/Compiler version (if compiling from source)**:
gcc version 4.9.2 (Debian 4.9.2-10+deb8u1)

- **CUDA/cuDNN version**:
n/a

- **GPU model and memory**:
n/a

- **Exact command to reproduce**:
```
cd /usr/local/src 
git clone https://github.com/tensorflow/tensorflow
cd tensorflow
cat /dev/null | ./configure
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
pip install /tmp/tensorflow_pkg/tensorflow*.whl

```

### Describe the problem

Building Tensorflow from source fails with the error below. We're trying to update the docker images for over at Kaggle (https://github.com/kaggle/docker-python )

### Source code / logs
[tf.log](https://github.com/tensorflow/tensorflow/files/2206995/tf.log)

At global scope:
cc1plus: warning: unrecognized command line option ""-Wno-writable-strings""
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 295.663s, Critical Path: 79.71s
INFO: 2072 processes: 2072 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully

"
20937,__main__.UserInputError: Invalid TF_NCCL setting was provided 10 times in a row. Assuming to be a scripting mistake.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Modified Dockerfile provided for TensorFlow devel GPU
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source (attempt)
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7
- **GPU model and memory**: 
- **Exact command to reproduce**: docker build -t tensorflow_source .

### Describe the problem
I am trying to build a docker image with TensorFlow installed from source. The docker image I am using is the Dockerfile.devel-gpu (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu) file, with an updated version of Bazel. There is an issue with the NCCL link in the script, resulting in the following error. 

### Source code / logs

``` 
Step 19 : RUN ./configure
 ---> Running in 350827ccf62f
Extracting Bazel installation...
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.15.2 installed.
Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: Amazon AWS Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: No OpenCL SYCL support will be enabled for TensorFlow.

Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Do you wish to build TensorFlow with TensorRT support? [y/N]: No TensorRT support will be enabled for TensorFlow.

Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 

Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:

Invalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2
Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 

Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:

Invalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2
Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 

Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:

Invalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2
Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 

Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:

Invalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2
Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 

Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:

Invalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2
Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 

Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:

Invalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2
Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 

Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:

Invalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2
Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 

Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:

Invalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2
Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 

Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:

Invalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2
Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 

Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:

Invalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2
Traceback (most recent call last):
  File ""./configure.py"", line 1559, in <module>
    main()
  File ""./configure.py"", line 1503, in main
    set_tf_nccl_install_path(environ_cp)
  File ""./configure.py"", line 1156, in set_tf_nccl_install_path
    _DEFAULT_PROMPT_ASK_ATTEMPTS)
__main__.UserInputError: Invalid TF_NCCL setting was provided 10 times in a row. Assuming to be a scripting mistake.
```

Not sure what the problem is, despite the soft links provided. "
20930,Reinitializable iterator doesn't use cached dataset upon reinitializing,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, written custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 8.1
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6.5
- **Bazel version**: N/A
- **CUDA/cuDNN version**: CUDA 9.0, cuDNN v7.0
- **GPU model and memory**: NVIDIA GeForce GTX 960 (faced the same problem with 1080 Ti)
- **Exact command to reproduce**: see below

### Describe the problem
Found a likely bug when using a reinitialiable iterator with a cached dataset. Upon reinitializing the iterator, the iterator does not use the cached dataset. This can be verified using the sample code below.

### Source code
```
import tensorflow as tf
import numpy as np
import time

LENGTH = 3
CACHE = True
VERBOSE = True
SHAPE = (1,)

def data_generator():    
    for n in range(LENGTH):
        data = np.random.rand(*SHAPE)
        yield data

def get_dataset():
    train_data = tf.data.Dataset.from_generator(data_generator, output_types=tf.float32)
    if CACHE:
        train_data = train_data.cache()
    return train_data

def get_dataset_repeat():
    train_data = tf.data.Dataset.from_generator(data_generator, output_types=tf.float32)
    if CACHE:
        train_data = train_data.cache()
    train_data = train_data.repeat()
    return train_data

data = get_dataset()
data_repeat = get_dataset_repeat()

iterator = tf.data.Iterator.from_structure(data.output_types, data.output_shapes)
next_element = iterator.get_next()

data_init = iterator.make_initializer(data)
data_init_repeat = iterator.make_initializer(data_repeat)

sess = tf.Session()

print('Reading values from iterator by reinitializing')
epoch = 1

for _ in range(5):
    start = time.time()
    sess.run(data_init)
    print('Epoch %d:' % epoch, end=' ')
    for _ in range(LENGTH):
        output = sess.run(next_element)
        
        if VERBOSE:
            print(output, end=' ')
        
    end = time.time()
    print(""Time taken: %f"" % (end - start))

    epoch += 1

print('Reading values from iterator using repeat')
epoch = 1
sess.run(data_init_repeat)

for _ in range(5):
    start = time.time()
    print('Epoch %d:' % epoch, end=' ')
    for _ in range(LENGTH):
        output = sess.run(next_element)
        
        if VERBOSE:
            print(output, end=' ')

    end = time.time()
    print(""Time taken: %f"" % (end - start))
    
    epoch += 1
```

This code gives the following output:

> Reading values from iterator by reinitializing
> Epoch 1: [0.00544547] [0.19976228] [0.5371114] Time taken: 0.017011
> Epoch 2: [0.97181207] [0.35276905] [0.69020385] Time taken: 0.003002
> Epoch 3: [0.19892913] [0.2760849] [0.8980513] Time taken: 0.003003
> Epoch 4: [0.92679894] [0.5854017] [0.6552748] Time taken: 0.003002
> Epoch 5: [0.38050508] [0.7676437] [0.41214108] Time taken: 0.003002
> Reading values from iterator using repeat
> Epoch 1: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.003003
> Epoch 2: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.003002
> Epoch 3: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.000000
> Epoch 4: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.001001
> Epoch 5: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.001000

In the second case, the same values are printed at every epoch because the iterator uses the cached data. However, in the first case, different values are printed at every epoch - indicating that the iterator does not use the cached values. This can be further verified by looking at the time taken (which will be more prominent with larger data). So in the above code, if I set `LENGTH = 100`, `VERBOSE = False` and `SHAPE = (500, 1000, 3)`, I get the following output:

> Reading values from iterator by reinitializing
> Epoch 1: Time taken: 2.228584
> Epoch 2: Time taken: 2.182549
> Epoch 3: Time taken: 2.175545
> Epoch 4: Time taken: 2.185551
> Epoch 5: Time taken: 2.161536
> Reading values from iterator using repeat
> Epoch 1: Time taken: 2.147525
> Epoch 2: Time taken: 0.263195
> Epoch 3: Time taken: 0.259186
> Epoch 4: Time taken: 0.266189
> Epoch 5: Time taken: 0.268181

In the second case, the first epoch takes considerably longer than the subsequent epochs - understandably, since the further epochs just reuse the cached data. However, in the first case, all the epochs take a long time.

This seems like a bug to me because the reinitializable iterator has [been advocated](https://www.tensorflow.org/guide/datasets#creating_an_iterator) as being useful in precisely such kind of situations - when you have 2 different datasets (say training and validation) and you simply reinitialize the iterator to use any one of the two at a time. However, it seems that if I reinitialize the iterator with a cached dataset, I lose the entire advantage of caching. If I set `CACHE = False` with the large shape mentioned above, I get the following output as expected:

> Reading values from iterator by reinitializing
> Epoch 1: Time taken: 2.317660
> Epoch 2: Time taken: 2.251585
> Epoch 3: Time taken: 2.209581
> Epoch 4: Time taken: 2.151515
> Epoch 5: Time taken: 2.156546
> Reading values from iterator using repeat
> Epoch 1: Time taken: 2.182563
> Epoch 2: Time taken: 2.207575
> Epoch 3: Time taken: 2.205560
> Epoch 4: Time taken: 2.139524
> Epoch 5: Time taken: 2.198561

That is, each epoch takes the same time with either method since we don't cache the dataset."
20929,cannot fit training data to TensorForestEstimator,"Currently I am trying to to implement a random forest regression using Tensorflow's `TensorForestEstimator`. I have successfully done using scikit-learn's `RandomForestRegressor` and wants to replicate the same result using Tensorflow.

I uploaded the data using pandas and I split the training and test set using scikit-learn's `train_test_split`. It contains 4 features (all numerical).

```
>>> X_train.shape
(2711, 4)
>>> y_train.shape
(2711,)
```
I set the parameters of the for the tree

```
num_features = int(np.log2(len(clean_data.columns)))

params = ForestHParams(num_classes=1, num_features=num_features,
                       regression=True,num_trees=447, max_nodes=1000)

regressor = TensorForestEstimator(params)
```
Above i set the features as `int(np.log2(len(clean_data.columns)))` because i used `log2` for the `max_features` parameter in my original scikit-learn implementation.

However, when tried to fit the training data, i receive and error like such

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    509                 as_ref=input_arg.is_ref,
--> 510                 preferred_dtype=default_dtype)
    511           except TypeError as err:

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
   1108     if ret is None:
-> 1109       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1110 

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    945         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r"" %
--> 946         (dtype.name, t.dtype.name, str(t)))
    947   return t

ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float64: 'Tensor(""concat:0"", shape=(?, 4), dtype=float64)'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-18-79323408f7f7> in <module>()
      1 # from tensorflow import cast, float32
      2 # X_train_cast = cast(X_train, float32)
----> 3 regressor.fit(x=X_train, y=y_train)
      4 
      5 #regressor.score()

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    430                 'in a future version' if date is None else ('after %s' % date),
    431                 instructions)
--> 432       return func(*args, **kwargs)
    433     return tf_decorator.make_decorator(func, new_func, 'deprecated',
    434                                        _add_deprecated_arg_notice_to_docstring(

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)
    506     _verify_input_args(x, y, input_fn, None, batch_size)
    507     if x is not None:
--> 508       SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)
    509       return self
    510 

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in fit(self, x, y, batch_size, steps, max_steps, monitors)
   1525         steps=steps,
   1526         max_steps=max_steps,
-> 1527         monitors=all_monitors)
   1528     return self
   1529 

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    430                 'in a future version' if date is None else ('after %s' % date),
    431                 instructions)
--> 432       return func(*args, **kwargs)
    433     return tf_decorator.make_decorator(func, new_func, 'deprecated',
    434                                        _add_deprecated_arg_notice_to_docstring(

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)
    522       hooks.append(basic_session_run_hooks.StopAtStepHook(steps, max_steps))
    523 
--> 524     loss = self._train_model(input_fn=input_fn, hooks=hooks)
    525     logging.info('Loss for final step: %s.', loss)
    526     return self

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in _train_model(self, input_fn, hooks)
   1039       self._check_inputs(features, labels)
   1040       training_util._get_or_create_global_step_read()  # pylint: disable=protected-access
-> 1041       model_fn_ops = self._get_train_ops(features, labels)
   1042       ops.add_to_collection(ops.GraphKeys.LOSSES, model_fn_ops.loss)
   1043       all_hooks.extend(hooks)

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in _get_train_ops(self, features, labels)
   1262       `ModelFnOps` object.
   1263     """"""
-> 1264     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
   1265 
   1266   def _get_eval_ops(self, features, labels, metrics):

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in _call_model_fn(self, features, labels, mode, metrics, config)
   1225     if 'model_dir' in model_fn_args:
   1226       kwargs['model_dir'] = self.model_dir
-> 1227     model_fn_results = self._model_fn(features, labels, **kwargs)
   1228 
   1229     if isinstance(model_fn_results, model_fn_lib.ModelFnOps):

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/contrib/tensor_forest/client/random_forest.py in _model_fn(features, labels, mode)
    169 
    170     logits, tree_paths, regression_variance = graph_builder.inference_graph(
--> 171         features)
    172 
    173     summary.scalar('average_tree_size', graph_builder.average_size())

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/contrib/tensor_forest/python/tensor_forest.py in inference_graph(self, input_data, **inference_args)
    512             data_spec,
    513             sparse_features=processed_sparse_features,
--> 514             **inference_args)
    515         probabilities.append(probs)
    516         paths.append(path)

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/contrib/tensor_forest/python/tensor_forest.py in inference_graph(self, input_data, data_spec, sparse_features)
    686         sparse_shape,
    687         input_spec=data_spec.SerializeToString(),
--> 688         params=self.params.serialized_params_proto)
    689 
    690   def size(self):

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/contrib/tensor_forest/python/ops/gen_model_ops.py in tree_predictions_v4(tree_handle, input_data, sparse_input_indices, sparse_input_values, sparse_input_shape, input_spec, params, name)
    467         sparse_input_values=sparse_input_values,
    468         sparse_input_shape=sparse_input_shape, input_spec=input_spec,
--> 469         params=params, name=name)
    470     _result = _op.outputs[:]
    471     _inputs_flat = _op.inputs

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    531             if input_arg.type != types_pb2.DT_INVALID:
    532               raise TypeError(""%s expected type of %s."" %
--> 533                               (prefix, dtypes.as_dtype(input_arg.type).name))
    534             else:
    535               # Update the maps with the default, if needed.

TypeError: Input 'input_data' of 'TreePredictionsV4' Op has type float64 that does not match expected type of float32.
```
My assumption was that i have to set the number of features to the number of all the features (i.e. using all features instead of a subset of the features). But i still get the same error as above.

I tried to look at the source code directly but could not really understood where was the issue. A similar issue is being discuss on github here.

Another attempt was converting the input into float32.

`regressor.fit(x=X_train.astype(""float32""), y=y_train.astype(""float32""))`

However, i still got the same error as above. Then i tried using `tf.cast`

```
X_train_cast = cast(X_train, float32)
y_train_cast = cast(y_train, float32)
regressor.fit(x=X_train_cast, y=y_train_cast)
```
But i got a different error saying

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-19-89e4fa057afb> in <module>()
      2 X_train_cast = cast(X_train, float32)
      3 y_train_cast = cast(y_train, float32)
----> 4 regressor.fit(x=X_train_cast, y=y_train_cast)
      5 
      6 #regressor.score()

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    430                 'in a future version' if date is None else ('after %s' % date),
    431                 instructions)
--> 432       return func(*args, **kwargs)
    433     return tf_decorator.make_decorator(func, new_func, 'deprecated',
    434                                        _add_deprecated_arg_notice_to_docstring(

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)
    504     if (steps is not None) and (max_steps is not None):
    505       raise ValueError('Can not provide both steps and max_steps.')
--> 506     _verify_input_args(x, y, input_fn, None, batch_size)
    507     if x is not None:
    508       SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)

~/Desktop/88sparses/recommendation/recom/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in _verify_input_args(x, y, input_fn, feed_fn, batch_size)
    102 
    103     if tensor_util.is_tensor(x) or y is not None and tensor_util.is_tensor(y):
--> 104       raise ValueError('Inputs cannot be tensors. Please provide input_fn.')
    105 
    106     if feed_fn is not None:

ValueError: Inputs cannot be tensors. Please provide input_fn.
```
I was wondering if there was something missing in my implementation? Thanks in advance."
20926,C++: Add gradient for Fill operator,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
v1.4.0-19-ga52c8d9 1.4.1
- **Python version**:
3.5.2 (default, Nov 23 2017, 16:37:01) 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
Feature request: Add gradient support for the Fill operator in the C++ API.
"
20925,Use InlinedVector for ShapeIndex breaks tensorflow build from source using bazel 0.14.0 inside docker ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: latest
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:0.14.0
- **GCC/Compiler version (if compiling from source)**:gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609

- **CUDA/cuDNN version**:9.0/7.1
- **GPU model and memory**:Nvidia GeForce 1080 Ti
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
While building the tensorflow from source inside docker  using above configuration i encountered following errors.

### Source code / logs
ERROR: /root/tensorflow/tensorflow/compiler/xla/BUILD:216:1: C++ compilation of rule '//tensorflow/compiler/xla:shape_util' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-9.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \
    NCCL_INSTALL_PATH=/usr/local/cuda-9.0 \
    PATH=/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/opt/conda/bin/python3.6 \
    PYTHON_LIB_PATH=/opt/conda/lib/python3.6 \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=9.0 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION=2 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/xla/_objs/shape_util/tensorflow/compiler/xla/index_util.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/compiler/xla/_objs/shape_util/tensorflow/compiler/xla/index_util.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DTENSORFLOW_USE_JEMALLOC -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -iquote . -iquote bazel-out/k8-opt/genfiles -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/jemalloc -iquote bazel-out/k8-opt/genfiles/external/jemalloc -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/k8-opt/genfiles/external/jemalloc/include -isystem bazel-out/k8-opt/bin/external/jemalloc/include -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem bazel-out/k8-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include/crt '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '-D_GLIBCXX_USE_CXX11_ABI=0' -c tensorflow/compiler/xla/index_util.cc -o bazel-out/k8-opt/bin/tensorflow/compiler/xla/_objs/shape_util/tensorflow/compiler/xla/index_util.o)
In file included from tensorflow/compiler/xla/index_util.cc:21:0:
./tensorflow/compiler/xla/shape_util.h:77:26: error: 'gtl' does not name a type
   using container_type = gtl::InlinedVector<int64, 2>;
                          ^
./tensorflow/compiler/xla/shape_util.h:79:3: error: 'container_type' does not name a type
   container_type::const_iterator begin() const { return indices_.begin(); }
   ^
./tensorflow/compiler/xla/shape_util.h:80:3: error: 'container_type' does not name a type
   container_type::const_iterator end() const { return indices_.end(); }
   ^
./tensorflow/compiler/xla/shape_util.h:81:3: error: 'container_type' does not name a type
   container_type::iterator begin() { return indices_.begin(); }
   ^
./tensorflow/compiler/xla/shape_util.h:82:3: error: 'container_type' does not name a type
   container_type::iterator end() { return indices_.end(); }
   ^
./tensorflow/compiler/xla/shape_util.h:103:3: error: 'container_type' does not name a type
   container_type indices_;
   ^
./tensorflow/compiler/xla/shape_util.h: In constructor 'xla::ShapeIndex::ShapeIndex(std::initializer_list<long long int>)':
./tensorflow/compiler/xla/shape_util.h:65:51: error: class 'xla::ShapeIndex' does not have any field named 'indices_'
   ShapeIndex(std::initializer_list<int64> init) : indices_(init) {}
                                                   ^
./tensorflow/compiler/xla/shape_util.h: In constructor 'xla::ShapeIndex::ShapeIndex(InputIt, InputIt)':
./tensorflow/compiler/xla/shape_util.h:67:44: error: class 'xla::ShapeIndex' does not have any field named 'indices_'
   ShapeIndex(InputIt start, InputIt end) : indices_(start, end) {}
                                            ^
./tensorflow/compiler/xla/shape_util.h: In member function 'bool xla::ShapeIndex::empty() const':
./tensorflow/compiler/xla/shape_util.h:69:31: error: 'indices_' was not declared in this scope
   bool empty() const { return indices_.empty(); }
                               ^
./tensorflow/compiler/xla/shape_util.h: In member function 'size_t xla::ShapeIndex::size() const':
./tensorflow/compiler/xla/shape_util.h:70:32: error: 'indices_' was not declared in this scope
   size_t size() const { return indices_.size(); }
                                ^
./tensorflow/compiler/xla/shape_util.h: In member function 'void xla::ShapeIndex::push_back(tensorflow::int64)':
./tensorflow/compiler/xla/shape_util.h:71:33: error: 'indices_' was not declared in this scope
   void push_back(int64 value) { indices_.push_back(value); }
                                 ^
./tensorflow/compiler/xla/shape_util.h: In member function 'void xla::ShapeIndex::pop_back()':
./tensorflow/compiler/xla/shape_util.h:72:21: error: 'indices_' was not declared in this scope
   void pop_back() { indices_.pop_back(); }
                     ^
./tensorflow/compiler/xla/shape_util.h: In member function 'void xla::ShapeIndex::push_front(tensorflow::int64)':
./tensorflow/compiler/xla/shape_util.h:75:34: error: 'indices_' was not declared in this scope
   void push_front(int64 value) { indices_.insert(indices_.begin(), value); }
                                  ^
./tensorflow/compiler/xla/shape_util.h: In member function 'const int64* xla::ShapeIndex::data() const':
./tensorflow/compiler/xla/shape_util.h:84:38: error: 'indices_' was not declared in this scope
   const int64* data() const { return indices_.data(); }
                                      ^
./tensorflow/compiler/xla/shape_util.h: In member function 'tensorflow::int64 xla::ShapeIndex::back() const':
./tensorflow/compiler/xla/shape_util.h:86:31: error: 'indices_' was not declared in this scope
   int64 back() const { return indices_.back(); }
                               ^
./tensorflow/compiler/xla/shape_util.h: In member function 'tensorflow::int64& xla::ShapeIndex::back()':
./tensorflow/compiler/xla/shape_util.h:87:26: error: 'indices_' was not declared in this scope
   int64& back() { return indices_.back(); }
                          ^
./tensorflow/compiler/xla/shape_util.h: In member function 'const int64& xla::ShapeIndex::operator[](size_t) const':
./tensorflow/compiler/xla/shape_util.h:89:52: error: 'indices_' was not declared in this scope
   const int64& operator[](size_t i) const { return indices_[i]; }
                                                    ^
./tensorflow/compiler/xla/shape_util.h: In member function 'tensorflow::int64& xla::ShapeIndex::operator[](size_t)':
./tensorflow/compiler/xla/shape_util.h:90:40: error: 'indices_' was not declared in this scope
   int64& operator[](size_t i) { return indices_[i]; }
                                        ^
./tensorflow/compiler/xla/shape_util.h: In member function 'bool xla::ShapeIndex::operator==(const xla::ShapeIndex&) const':
./tensorflow/compiler/xla/shape_util.h:93:12: error: 'indices_' was not declared in this scope
     return indices_ == other.indices_;
            ^
./tensorflow/compiler/xla/shape_util.h:93:30: error: 'const class xla::ShapeIndex' has no member named 'indices_'
     return indices_ == other.indices_;
                              ^
./tensorflow/compiler/xla/shape_util.h: In member function 'bool xla::ShapeIndex::operator<(const xla::ShapeIndex&) const':
./tensorflow/compiler/xla/shape_util.h:97:12: error: 'indices_' was not declared in this scope
     return indices_ < other.indices_;
            ^
./tensorflow/compiler/xla/shape_util.h:97:29: error: 'const class xla::ShapeIndex' has no member named 'indices_'
     return indices_ < other.indices_;
                             ^
tensorflow/compiler/xla/index_util.cc: In static member function 'static bool xla::IndexUtil::IndexInBounds(const xla::Shape&, tensorflow::gtl::ArraySlice<long long int>)':
tensorflow/compiler/xla/index_util.cc:155:12: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (rank != index.size()) {
            ^
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/gtl/array_slice_internal.h:32,
                 from ./tensorflow/core/lib/gtl/array_slice.h:101,
                 from ./tensorflow/compiler/xla/index_util.h:25,
                 from tensorflow/compiler/xla/index_util.cc:16:
./tensorflow/core/platform/default/logging.h: In instantiation of 'std::string* tensorflow::internal::Check_LEImpl(const T1&, const T2&, const char*) [with T1 = long long int; T2 = long unsigned int; std::string = std::basic_string<char>]':
./tensorflow/compiler/xla/shape_util.h:117:5:   required from here
./tensorflow/core/platform/default/logging.h:232:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
 TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)
                                   ^
./tensorflow/core/platform/macros.h:88:49: note: in definition of macro 'TF_PREDICT_TRUE'
 #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))
                                                 ^
./tensorflow/core/platform/default/logging.h:232:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'
 TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)
 ^
./tensorflow/core/platform/default/logging.h: In instantiation of 'std::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = long unsigned int; T2 = long long int; std::string = std::basic_string<char>]':
tensorflow/compiler/xla/index_util.cc:170:3:   required from here
./tensorflow/core/platform/default/logging.h:230:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
                         ==)  // Compilation error with CHECK_EQ(NULL, x)?
                         ^
./tensorflow/core/platform/macros.h:88:49: note: in definition of macro 'TF_PREDICT_TRUE'
 #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))
                                                 ^
./tensorflow/core/platform/default/logging.h:229:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'
 TF_DEFINE_CHECK_OP_IMPL(Check_EQ,
"
20924,Not able to build tensorflow lite iOS library,"
### System information

Trying to follow steps from this https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/ios.md

OS: Mac 10.13.4
XCode: 9.4.1
Command Line Tools: 9.4.1
Branch: master
SHA: ff791a7fde3605493bef70de8a9c9779541daf66

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh (Throws error)

### Describe the problem

error: invalid argument '--std=c++11' not allowed with 'C'

I get this when i try to build. if i change CC in makefile to 'g++' then other errors pop up. i think this script is broken. 

### Source code / logs
[build_ios_universal_lib.log](https://github.com/tensorflow/tensorflow/files/2206029/build_ios_universal_lib.log)
"
20922,Decoding 16bit PNG bug,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I have written a test that (IMHO) should not fail, but it fails.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Mint 18
- **TensorFlow installed from (source or binary)**:
`pip install tensorflow==1.8` in a clean environment
- **TensorFlow version (use command below)**:
1.8
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
Please run 
```
# Copyright 2016 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""Tests for slim.data.tfexample_decoder.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np

from tensorflow.contrib.slim.python.slim.data import tfexample_decoder
from tensorflow.core.example import example_pb2
from tensorflow.core.example import feature_pb2
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.ops import image_ops
from tensorflow.python.ops import lookup_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import parsing_ops
from tensorflow.python.platform import test


class TFExampleDecoderTest(test.TestCase):

    def _EncodedFloatFeature(self, ndarray):
        return feature_pb2.Feature(float_list=feature_pb2.FloatList(
            value=ndarray.flatten().tolist()))

    def _EncodedInt64Feature(self, ndarray):
        return feature_pb2.Feature(int64_list=feature_pb2.Int64List(
            value=ndarray.flatten().tolist()))

    def _EncodedBytesFeature(self, tf_encoded):
        with self.test_session():
            encoded = tf_encoded.eval()

        def BytesList(value):
            return feature_pb2.BytesList(value=[value])

        return feature_pb2.Feature(bytes_list=BytesList(encoded))

    def _BytesFeature(self, ndarray):
        values = ndarray.flatten().tolist()
        for i in range(len(values)):
            values[i] = values[i].encode('utf-8')
        return feature_pb2.Feature(bytes_list=feature_pb2.BytesList(value=values))

    def _StringFeature(self, value):
        value = value.encode('utf-8')
        return feature_pb2.Feature(bytes_list=feature_pb2.BytesList(value=[value]))

    def _Encoder(self, image, image_format):
        assert image_format in ['jpeg', 'JPEG', 'png', 'PNG', 'raw', 'RAW']
        if image_format in ['jpeg', 'JPEG']:
            tf_image = constant_op.constant(image, dtype=dtypes.uint8)
            return image_ops.encode_jpeg(tf_image)
        if image_format in ['png', 'PNG']:
            tf_image = constant_op.constant(image, dtype=dtypes.uint8)
            return image_ops.encode_png(tf_image)
        if image_format in ['raw', 'RAW']:
            return constant_op.constant(image.tostring(), dtype=dtypes.string)

    def GenerateImage(self, image_format, image_shape):
        """"""Generates an image and an example containing the encoded image.

        Args:
          image_format: the encoding format of the image.
          image_shape: the shape of the image to generate.

        Returns:
          image: the generated image.
          example: a TF-example with a feature key 'image/encoded' set to the
            serialized image and a feature key 'image/format' set to the image
            encoding format ['jpeg', 'JPEG', 'png', 'PNG', 'raw'].
        """"""
        num_pixels = image_shape[0] * image_shape[1] * image_shape[2]
        image = np.linspace(
            0, num_pixels - 1, num=num_pixels).reshape(image_shape).astype(np.uint8)
        tf_encoded = self._Encoder(image, image_format)
        example = example_pb2.Example(features=feature_pb2.Features(feature={
            'image/encoded': self._EncodedBytesFeature(tf_encoded),
            'image/format': self._StringFeature(image_format)
        }))

        return image, example.SerializeToString()

    def DecodeExample(self, serialized_example, item_handler, image_format):
        """"""Decodes the given serialized example with the specified item handler.

        Args:
          serialized_example: a serialized TF example string.
          item_handler: the item handler used to decode the image.
          image_format: the image format being decoded.

        Returns:
          the decoded image found in the serialized Example.
        """"""
        serialized_example = array_ops.reshape(serialized_example, shape=[])
        decoder = tfexample_decoder.TFExampleDecoder(
            keys_to_features={
                'image/encoded':
                    parsing_ops.FixedLenFeature(
                        (), dtypes.string, default_value=''),
                'image/format':
                    parsing_ops.FixedLenFeature(
                        (), dtypes.string, default_value=image_format),
            },
            items_to_handlers={'image': item_handler})
        [tf_image] = decoder.decode(serialized_example, ['image'])
        return tf_image

    def RunDecodeExample(self, serialized_example, item_handler, image_format):
        tf_image = self.DecodeExample(serialized_example, item_handler,
                                      image_format)

        with self.test_session():
            decoded_image = tf_image.eval()

            # We need to recast them here to avoid some issues with uint8.
            return decoded_image.astype(np.float32)

    def testDecodeExampleWithPngEncodingAt16Bit(self):
        image_shape = (2, 3, 3)
        unused_image, serialized_example = self.GenerateImage(
            image_format='png', image_shape=image_shape)
        unused_decoded_image = self.RunDecodeExample(
            serialized_example,
            tfexample_decoder.Image(dtype=dtypes.uint16),
            image_format='png')
        self.assertAllClose(unused_image, unused_decoded_image)


if __name__ == '__main__':
    test.main()
```
It is a modified https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py

### Describe the problem
I would like to decode and encode a single channel 16bit (uint16) PNG and it is not possible. I have modified the tests also to create a np.uint16 image, then trying to decode it, but it does not work.

```
  def testDecodeExampleWithPngEncodingAt16Bit(self):
    image_shape = (2, 3, 3)
    unused_image, serialized_example = self.GenerateImage(
        image_format='png', image_shape=image_shape, dtype=np.uint16)
    unused_decoded_image = self.RunDecodeExample(
          serialized_example,
          tfexample_decoder.Image(dtype=dtypes.uint16),
          image_format='png')
    self.assertAllClose(unused_image, unused_decoded_image)
```
(modifying the `GenerateImage` method accordingly).

I understand that it should be possible since decode_png function allows uint16 as an output type: https://www.tensorflow.org/api_docs/python/tf/image/decode_png 

The test fails because the program raises
```
ValueError: Outputs of true_fn and false_fn must have the same type: uint16, uint8
```
I understand that it is due to the fact that `tf.cond` needs the same output types from branches.
"
20921," ValueError: Cannot feed value of shape (5, 100, 100, 3) for Tensor 'X:0', which has shape '(?, 100, 100)'","I'm very new to Tensorflow. And it hurts. A lot.
 
# coding: utf-8

# # Training a DCGAN to draw fake and real images

# In[1]:


directory = ""../Data/image_files/""
new_dir = ""../Data/image_files/cropped""
import urllib
import urllib.request
import tarfile
import os
import tarfile
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.image import imread
from scipy.misc import imresize, imsave
import tensorflow as tf
import imageio
import skimage
get_ipython().run_line_magic('matplotlib', 'inline')


# ## Modifying the images (reducing their size)

# In[2]:


height_width = 100

filepaths = []
for dir_, _, files in os.walk(directory):
    for fileName in files:
        relDir = os.path.relpath(dir_, directory)
        relFile = os.path.join(relDir, fileName)
        filepaths.append(directory + ""/"" + relFile)
fail_count = 0         
for i, fp in enumerate(filepaths):
    try: 
        img = imread(fp, 0) #/ 255.0
        img = skimage.transform.resize(img, (height_width, height_width))
        imageio.imwrite(new_dir + ""/"" + str(i) + "".png"", img)        
    except:
        fail_count += 1
        img = imread(""../Data/white_square.png"", 0) #/ 100 width square
        imageio.imwrite(new_dir + ""/"" + str(i) + "".png"", img) 
        with open(""../Data/fail_log.text"", ""a+"") as f:
            f.write(fp)
print(fail_count)


# Load the file names of files into a list

# In[3]:


filepaths_new = []
for dir_, _, files in os.walk(new_dir):
    for fileName in files:
        if not fileName.endswith("".png""):
            continue
        relDir = os.path.relpath(dir_, directory)
        relFile = os.path.join(relDir, fileName)
        filepaths_new.append(directory + ""/"" + relFile)


# ## Define next batch

# In[4]:


def next_batch(num=64, data=filepaths_new):
    idx = np.arange(0 , len(data))
    np.random.shuffle(idx)
    idx = idx[:num]
    print(idx)
    data_shuffle = [imread(data[i]) for i in idx]

    shuffled = np.asarray(data_shuffle)
    print(""shuffled:"", shuffled.shape)
    return np.asarray(data_shuffle)


# ## Code for creating montages (by Parag Mital)

# In[5]:


# Code by Parag Mital (https://github.com/pkmital/CADL/)
def montage(images):    
    if isinstance(images, list):
        images = np.array(images)
    img_h = images.shape[1]
    img_w = images.shape[2]
    n_plots = int(np.ceil(np.sqrt(images.shape[0])))
    if len(images.shape) == 4 and images.shape[3] == 3:
        m = np.ones(
            (images.shape[1] * n_plots + n_plots + 1,
             images.shape[2] * n_plots + n_plots + 1, 3)) * 0.5
    elif len(images.shape) == 4 and images.shape[3] == 1:
        m = np.ones(
            (images.shape[1] * n_plots + n_plots + 1,
             images.shape[2] * n_plots + n_plots + 1, 1)) * 0.5
    elif len(images.shape) == 3:
        m = np.ones(
            (images.shape[1] * n_plots + n_plots + 1,
             images.shape[2] * n_plots + n_plots + 1)) * 0.5
    else:
        raise ValueError('Could not parse image shape of {}'.format(
            images.shape))
    for i in range(n_plots):
        for j in range(n_plots):
            this_filter = i * n_plots + j
            if this_filter < images.shape[0]:
                this_img = images[this_filter]
                m[1 + i + i * img_h:1 + i + (i + 1) * img_h,
                  1 + j + j * img_w:1 + j + (j + 1) * img_w] = this_img
    return m


# ## Definition of the neural network

# In[6]:


tf.reset_default_graph()
batch_size = 5
n_noise = 28

#config = tf.ConfigProto()
#config.gpu_options.allocator_type ='BFC'
#config.gpu_options.per_process_gpu_memory_fraction = 0.90

X_in = tf.placeholder(dtype=tf.float32, shape=[None, height_width, height_width], name='X')
noise = tf.placeholder(dtype=tf.float32, shape=[None, n_noise])

keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')
is_training = tf.placeholder(dtype=tf.bool, name='is_training')

def lrelu(x):
    return tf.maximum(x, tf.multiply(x, 0.2))

def binary_cross_entropy(x, z):
    eps = 1e-12
    return (-(x * tf.log(z + eps) + (1. - x) * tf.log(1. - z + eps)))

def discriminator(img_in, reuse=None, keep_prob=keep_prob):
    activation = lrelu
    with tf.variable_scope(""discriminator"", reuse=reuse):
        x = tf.reshape(img_in, shape=[-1, height_width, height_width, 3])
        x = tf.layers.conv2d(x, kernel_size=5, filters=256, strides=2, padding='same', activation=activation)
        x = tf.layers.dropout(x, keep_prob)
        x = tf.layers.conv2d(x, kernel_size=5, filters=128, strides=1, padding='same', activation=activation)
        x = tf.layers.dropout(x, keep_prob)
        x = tf.layers.conv2d(x, kernel_size=5, filters=64, strides=1, padding='same', activation=activation)
        x = tf.layers.dropout(x, keep_prob)
        x = tf.contrib.layers.flatten(x)
        x = tf.layers.dense(x, units=128, activation=activation)
        x = tf.layers.dense(x, units=1, activation=tf.nn.sigmoid)
        return x
    
def generator(z, keep_prob=keep_prob, is_training=is_training):
    activation = lrelu
    momentum = 0.9
    with tf.variable_scope(""generator"", reuse=None):
        x = z
        
        d1 = 4#3
        d2 = 3
        
        x = tf.layers.dense(x, units=d1 * d1 * d2, activation=activation)
        x = tf.layers.dropout(x, keep_prob)      
        x = tf.contrib.layers.batch_norm(x, is_training=is_training, decay=momentum)  
        
        x = tf.reshape(x, shape=[-1, d1, d1, d2])
        x = tf.image.resize_images(x, size=[50, 50])
        
        
        
        x = tf.layers.conv2d_transpose(x, kernel_size=5, filters=256, strides=2, padding='same', activation=activation)
        x = tf.layers.dropout(x, keep_prob)
        x = tf.contrib.layers.batch_norm(x, is_training=is_training, decay=momentum)
        x = tf.layers.conv2d_transpose(x, kernel_size=5, filters=128, strides=2, padding='same', activation=activation)
        x = tf.layers.dropout(x, keep_prob)
        x = tf.contrib.layers.batch_norm(x, is_training=is_training, decay=momentum)
        x = tf.layers.conv2d_transpose(x, kernel_size=5, filters=64, strides=1, padding='same', activation=activation)
        x = tf.layers.dropout(x, keep_prob)
        x = tf.contrib.layers.batch_norm(x, is_training=is_training, decay=momentum)
        x = tf.layers.conv2d_transpose(x, kernel_size=5, filters=3, strides=1, padding='same', activation=tf.nn.sigmoid)
        return x    


# In[ ]:





# ## Losses and optimizers

# In[7]:




g = generator(noise, keep_prob, is_training)
print(g)
d_real = discriminator(X_in)
d_fake = discriminator(g, reuse=True)

vars_g = [var for var in tf.trainable_variables() if var.name.startswith(""generator"")]
vars_d = [var for var in tf.trainable_variables() if var.name.startswith(""discriminator"")]


d_reg = tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(1e-6), vars_d)
g_reg = tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(1e-6), vars_g)

loss_d_real = binary_cross_entropy(tf.ones_like(d_real), d_real)
loss_d_fake = binary_cross_entropy(tf.zeros_like(d_fake), d_fake)
loss_g = tf.reduce_mean(binary_cross_entropy(tf.ones_like(d_fake), d_fake))

loss_d = tf.reduce_mean(0.5 * (loss_d_real + loss_d_fake))

update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    optimizer_d = tf.train.RMSPropOptimizer(learning_rate=0.0001).minimize(loss_d + d_reg, var_list=vars_d)
    optimizer_g = tf.train.RMSPropOptimizer(learning_rate=0.0002).minimize(loss_g + g_reg, var_list=vars_g)
sess = tf.Session()
sess.run(tf.global_variables_initializer())


# ## Training the network

# In[8]:


for i in range(60000):
    train_d = True
    train_g = True
    keep_prob_train = 0.6 # 0.5
    
    
    n = np.random.uniform(0.0, 1.0, [batch_size, n_noise]).astype(np.float32)   
    batch = [b for b in next_batch(num=batch_size)]  
    
    d_real_ls, d_fake_ls, g_ls, d_ls = sess.run([loss_d_real, loss_d_fake, loss_g, loss_d], feed_dict={X_in: batch, noise: n, keep_prob: keep_prob_train, is_training:True})
    
    d_fake_ls_init = d_fake_ls
    
    d_real_ls = np.mean(d_real_ls)
    d_fake_ls = np.mean(d_fake_ls)
    g_ls = g_ls
    d_ls = d_ls
        
    if g_ls * 1.35 < d_ls:
        train_g = False
        pass
    if d_ls * 1.35 < g_ls:
        train_d = False
        pass
    
    if train_d:
        sess.run(optimizer_d, feed_dict={noise: n, X_in: batch, keep_prob: keep_prob_train, is_training:True})
        
        
    if train_g:
        sess.run(optimizer_g, feed_dict={noise: n, keep_prob: keep_prob_train, is_training:True})
        
        
    if not i % 10:
        print (i, d_ls, g_ls)
        if not train_g:
            print(""not training generator"")
        if not train_d:
            print(""not training discriminator"")
        gen_imgs = sess.run(g, feed_dict = {noise: n, keep_prob: 1.0, is_training:False})
        imgs = [img[:,:,:] for img in gen_imgs]
        m = montage(imgs)
        #m = imgs[0]
        plt.axis('off')
        plt.imshow(m, cmap='gray')
        plt.show()

Generates this error.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-8-3639a81d708d> in <module>()
      8     batch = [b for b in next_batch(num=batch_size)]
      9 
---> 10     d_real_ls, d_fake_ls, g_ls, d_ls = sess.run([loss_d_real, loss_d_fake, loss_g, loss_d], feed_dict={X_in: batch, noise: n, keep_prob: keep_prob_train, is_training:True})
     11 
     12     d_fake_ls_init = d_fake_ls

~\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    898     try:
    899       result = self._run(None, fetches, feed_dict, options_ptr,
--> 900                          run_metadata_ptr)
    901       if run_metadata:
    902         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1109                              'which has shape %r' %
   1110                              (np_val.shape, subfeed_t.name,
-> 1111                               str(subfeed_t.get_shape())))
   1112           if not self.graph.is_feedable(subfeed_t):
   1113             raise ValueError('Tensor %s may not be fed.' % subfeed_t)

ValueError: Cannot feed value of shape (5, 100, 100, 3) for Tensor 'X:0', which has shape '(?, 100, 100)'


No idea how to debug this. Need a bit of help."
20920,[ Interpreter.runForMultipleInputsOutputs ] Error When running  SSD-Mobilenet-v1 model in tensorflow-lite. ,"@andrewharp I tried your cutosm inference class TFLiteObjectDetectionAPIModel.java in [tensorflow/contrib/lite/examples/android](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android) , and i use it with your ssd mobilenet v1 tflite mobilenet_ssd_tflite_v1.zip but it seems i have a problem when i call tfLite.runForMultipleInputsOutputs(inputArray, outputMap); ( in function recognizeImage(final Bitmap bitmap) ) . it throws this exception

```
07-18 10:37:02.416 19957-19996/com.app.cerist.realtimeobjectdetectionapi E/AndroidRuntime: FATAL EXCEPTION: Camera
    Process: com.app.cerist.realtimeobjectdetectionapi, PID: 19957
    java.lang.IllegalArgumentException: Output error: Outputs do not match with model outputs.
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:170)
        at com.app.cerist.realtimeobjectdetectionapi.ImageClassifierTFLiteAPI.recognizeImage(ImageClassifierTFLiteAPI.java:207)
        at com.app.cerist.realtimeobjectdetectionapi.MainActivity.classifyFrame(MainActivity.java:421)
        at com.app.cerist.realtimeobjectdetectionapi.MainActivity.access$1000(MainActivity.java:48)
        at com.app.cerist.realtimeobjectdetectionapi.MainActivity$4.run(MainActivity.java:455)
        at android.os.Handler.handleCallback(Handler.java:739)
        at android.os.Handler.dispatchMessage(Handler.java:95)
        at android.os.Looper.loop(Looper.java:159)
        at android.os.HandlerThread.run(HandlerThread.java:61)
07-18 10:37:02.436 19957-19996/com.app.cerist.realtimeobjectdetectionapi V/Process: killProcess [19957] Callers=com.android.internal.os.RuntimeInit$UncaughtHandler.uncaughtException:99 java.lang.ThreadGroup.uncaughtException:693 java.lang.ThreadGroup.uncaughtException:690 <bottom of call stack> 
07-18 10:37:02.436 19957-19996/com.app.cerist.realtimeobjectdetectionapi I/Process: Sending signal. PID: 19957 SIG: 9
```

the error said that the length of outputs array is bigger than the length of inputs array
in this condition line in the Interpreter.java

``` 
public void runForMultipleInputsOutputs(Object[] inputs, @NonNull Map<Integer, Object> outputs) {
        if (this.wrapper == null) {
            throw new IllegalStateException(""Internal error: The Interpreter has already been closed."");
        } else {
            Tensor[] tensors = this.wrapper.run(inputs);
            if (outputs != null && tensors != null && outputs.size() <= tensors.length) {
                int size = tensors.length;
                //...
            }} else {
                throw new IllegalArgumentException(""Output error: Outputs do not match with model outputs."");
}
```
This is my inputs array : 

```
d.imgData = ByteBuffer.allocateDirect(1 * d.inputSize * d.inputSize * 3 * numBytesPerChannel);
d.imgData.order(ByteOrder.nativeOrder());
//...
 imgData.rewind();
        for (int i = 0; i < inputSize; ++i) {
            for (int j = 0; j < inputSize; ++j) {
                int pixelValue = intValues[i * inputSize + j];
                if (isModelQuantized) {
                    // Quantized model
                    imgData.put((byte) ((pixelValue >> 16) & 0xFF));
                    imgData.put((byte) ((pixelValue >> 8) & 0xFF));
                    imgData.put((byte) (pixelValue & 0xFF));
                } else { // Float model
                    imgData.putFloat((((pixelValue >> 16) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);
                    imgData.putFloat((((pixelValue >> 8) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);
                    imgData.putFloat(((pixelValue & 0xFF) - IMAGE_MEAN) / IMAGE_STD);
} 

```

```
Object[] inputArray = {imgData};
```

And this  the outputs arrays :

```
// Copy the input data into TensorFlow.
        Trace.beginSection(""feed"");
        outputLocations = new float[1][NUM_DETECTIONS][4];
        outputClasses = new float[1][NUM_DETECTIONS];
        outputScores = new float[1][NUM_DETECTIONS];
        numDetections = new float[1];
//...
        Map<Integer, Object> outputMap = new HashMap<>();
        outputMap.put(0, outputLocations);
        outputMap.put(1, outputScores);
        outputMap.put(2, numDetections);
        outputMap.put(3, outputClasses);
        Trace.endSection();
```

And the Inference :
```
// Run the inference call.
        Trace.beginSection(""run"");
        Log.d(""TAG_INPUT"",""""+String.valueOf(inputArray.length)); // inputArray has length of 1
        Log.d(""TAG_OUTPUT"",""""+String.valueOf(outputMap.size())); // outputMap has length of 4

        tfLite.runForMultipleInputsOutputs(inputArray, outputMap);
        Trace.endSection();
```

I didn't understand the meaning of this Error cuz i did exactly the same as your TFLiteObjectDetectionAPIModel.java class .
thank you for Help
"
20919,"bazel build failure, error occur no such package @androidndk","### System information
- **OS Platform and Distribution **: Ubuntu 16.04
- **TensorFlow install **: installed from source
- **TensorFlow version **: r1.9
- **Python version **: pyenv + virtualenv to support python 3.6.6
- **Bazel version **: 0.14.1 compiling from source
- **GCC/Compiler version  **: gcc 4.9
- **CUDA, cuDNN version**: CUDA 9.0, cuDNN 7.1
- **GPU  **: NVIDIA Titan X 12GB
- ** no custom code **: source compile from released r1.9 source

### Describe the problem
When compile tensor flow r1.9 from released source code, Bazel occur error message.
I try to compile r1.8 as same build env. there is no error occurred. 
 
### Source code / logs
- build env. setting 

```
cd /tmp/tensorflow-r${TF_VERSION} \
    && PYTHON_BIN_PATH=/usr/local/bin/python \
        PYTHON_LIB_PATH=/usr/local/lib/python3.6/site-packages \
        GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
        CC_OPT_FLAGS=""-march=x86-64 -mavx -mavx2 -mfma -mfpmath=both -msse4.1 -msse4.2"" \
        TF_NEED_JEMALLOC=1 \
        TF_NEED_GCP=1 \
        TF_NEED_HDFS=1 \
        TF_ENABLE_XLA=1 \
        TF_NEED_VERBS=0 \
        TF_NEED_OPENCL=0 \
        TF_NEED_OPENCL_SYCL=0 \
        TF_NEED_CUDA=1 \
        TF_CUDA_CLANG=0 \
        TF_DOWNLOAD_CLANG=0 \
        TF_NEED_TENSORRT=0 \
        TF_NEED_MPI=0 \
        TF_NEED_GDR=0 \
        TF_NEED_S3=1 \
        TF_NEED_KAFKA=0 \
        TF_SET_ANDROID_WORKSPACE=0 \
        TF_CUDA_VERSION=9.0 \
        TF_CUDNN_VERSION=7 \
        TF_NCCL_VERSION=2 \
        TF_CUDA_COMPUTE_CAPABILITIES=3.5,3.7,5.2,6.0,6.1,7.0 \
        CUDA_TOOLKIT_PATH=/usr/local/cuda \
        CUDNN_INSTALL_PATH=/usr/local/cuda \
        NCCL_INSTALL_PATH=/usr \
        bash configure \
   && bazel build \
        --config=opt --config=cuda \
        --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" \
        //tensorflow/tools/pip_package:build_pip_package \
     && ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg

```

```
Starting local Bazel server and connecting to it...
...............
Building: no action
Building: no action
Building: no action
ERROR: /tmp/tensorflow-r1.9/tensorflow/contrib/lite/kernels/internal/BUILD:529:1: no such package '@androidndk//': The repository could not be resolved and referenced by '//tensorflow/contrib/lite/kernels/internal:cpu_check'
ERROR: Evaluation of query ""deps(//tensorflow/tools/pip_package:build_pip_package)"" failed: errors were encountered while computing transitive closure
```"
20916,tf.contrib.graph_editor.graph_replace is broken for while loops,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 2.7.13

### Describe the problem
tf.contrib.graph_editor.graph_replace is broken for Graphs containing while-loops.

### Source code / logs
I've created a commit (1b9f2a5) which adds a simple test in tensorflow/contrib/graph_editor/tests/transform_test.py, which creates a graph with a while loop and replaces one of the variables therein:

```python
  def test_graph_replace_while_loop(self):
    ops.reset_default_graph()
    a = constant_op.constant(1, name=""a"")
    max_index = constant_op.constant(10)
    index_start = constant_op.constant(1)
    sum_start = constant_op.constant(0)
    _, result = control_flow_ops.while_loop(
        cond=lambda i, unused_s: i <= max_index,
        body=lambda i, s: (i + 1, s + a),
        loop_vars=[index_start, sum_start])
    a_new = constant_op.constant(2, name=""a_new"")
    result_new = ge.graph_replace(result, {a: a_new})
    with session.Session() as sess:
      sess.run(variables.global_variables_initializer())
      result_val, result_new_val = sess.run([result, result_new])
    self.assertEqual(result_val, 10, ERROR_TOLERANCE)
    self.assertEqual(result_new_val, 20, ERROR_TOLERANCE)
```

The test fails as follows:

```python
........I0718 03:06:24.128054    6838 control_flow_util.py:313] Cannot use 'while/LessEqual/Enter' as input to 'while/LessEqual_1' because 'while/LessEqual/Enter' is in a while loop.

while/LessEqual_1 while context: None
while/LessEqual/Enter while context: while/while_context
```

"
20915,Heavily increased memory consumption for optimizing batch_norm in tf versions > 1.3.0 ,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

    I attach a code example that is intended to profile memory consumption for different layers of a network under different tensorflow versions. This should in principle work out of the box.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

    VERSION=""16.04.3 LTS (Xenial Xerus)""
    VERSION_ID=""16.04""
    VERSION_CODENAME=xenial


- **TensorFlow installed from (source or binary)**:

    pip install tensorflow-gpu==1.3.0
    and
    pip install tensorflow-gpu==1.5.0
    and
    pip install tensorflow-gpu==1.8.0
    and
    pip install tensorflow-gpu==1.9.0

- **TensorFlow version (use command below)**:
The following tf versions were used to recreate the issue
    tf.GIT_VERSION = b'unknown' 
    tf.VERSION = 1.3.0
    and
    tf.GIT_VERSION = v1.5.0-0-g37aa430d84
    tf.VERSION = 1.5.0
    and
    tf.GIT_VERSION = v1.8.0-0-g93bc2e2072 
    tf.VERSION = 1.8.0
    and
    tf.GIT_VERSION = v1.9.0-0-g25c197e023
    tf.VERSION = 1.9.0
  
- **Python version**:
Python 3.5.5 :: Anaconda, Inc.
- **CUDA/cuDNN version**:
    For tf 1.3.0:
    nvcc: NVIDIA (R) Cuda compiler driver
    Copyright (c) 2005-2016 NVIDIA Corporation
    Built on Tue_Jan_10_13:22:03_CST_2017
    Cuda compilation tools, release 8.0, V8.0.61
    
    For tf 1.5.0, 1.8.0, 1.9.0
    nvcc: NVIDIA (R) Cuda compiler driver
    Copyright (c) 2005-2017 NVIDIA Corporation
    Built on Fri_Sep__1_21:08:03_CDT_2017
    Cuda compilation tools, release 9.0, V9.0.176
- **Bazel version (if compiling from source)**:
    N/A
- **GPU model and memory**:
    GeForce GTX 1080 Ti
    total memory shown as 10.91GiB
- **Exact command to reproduce**:
    python 'script shown below'

### Describe the problem
When trying to update from tensorflow 1.3.0 to a newer version, we noticed that memory consumption increased significantly for our networks.  
We tried to find out what was causing this issue and realized that there is a large discrepancy for memory consumption in our batch normalization layers when optimizing the network in training.
I attach the code necessary to reproduce the results. 
In the code the following happens:
   First, a rather useless network of 10 batch normalization layers is created. In the examples shown below, this network expects 1D input of width 500 with 32 channels and a batch size of 16. 
   Second, a GPU profiler is started, repeatedly calling nvidia-smi to check the current memory consumption. While the memory usage is measured, the network is evaluated 2500 times, under many different conditions. Lastly, the timings and the memory consumption over time are saved and plotted for comparison of the tf versions. 

### What is the problem? 

The results from running the code under 4 different conditions is shown in the plots below.
1. Fused batch norm is used if possible, *with gradient update*, version specific batch norm
2. Fused batch norm is used if possible, *without gradient update*, version specific batch norm
3. No fused batch norm, *without gradient update*, all use batch normalization as in 
https://raw.githubusercontent.com/tensorflow/tensorflow/r1.3/tensorflow/python/layers/normalization.py
4. No fused batch norm,*with gradient update*, all use batch normalization as in 
https://raw.githubusercontent.com/tensorflow/tensorflow/r1.3/tensorflow/python/layers/normalization.py

For 3 and 4, we still run the different tf versions, but we copied the mentioned file directly to our repo and call the batch_normalization from that file.

Furthermore, currently the layout optimizer in the session config is turned off, but we also tried this with the layout optimizer turned on and this has no effect on this issue.

#### 1.
[batch_norm_fused_optimizing_step.pdf](https://github.com/tensorflow/tensorflow/files/2205279/batch_norm_fused_optimizing_step.pdf)
#### 2.
[batch_norm_fused_without_optimizing_step.pdf](https://github.com/tensorflow/tensorflow/files/2205281/batch_norm_fused_without_optimizing_step.pdf)
#### 3.
[batch_norm_unfused_1.3_implementaion.pdf](https://github.com/tensorflow/tensorflow/files/2205284/batch_norm_unfused_1.3_implementaion.pdf)
#### 4.
[batch_norm_unfused_1.3_implementaion_with_update.pdf](https://github.com/tensorflow/tensorflow/files/2205285/batch_norm_unfused_1.3_implementaion_with_update.pdf)

From these plots, it seems that versions > 1.3.0 need a significant amount of extra memory when doing a gradient update step. For example, memory requirements for Batch norm in 1.9 increased by 50% compared to 1.3. Why is that?

### Request
Is there a way to circumvent the increased memory consumption? I assume some optimizing is happening in the background in newer versions, which leads to an increase in memory requirements. Can this be turned off?


### Source code / logs
Source code necessary to reproduce the problem is attached below.

In principle, however, I repeatedly run nvidia-smi for memory profiling and measure timing and memory for the following 
```
for i in range(self.steps):
    start = time.time()
    sess.run([net, train_op])
    durations[i] = time.time() - start
```
[code.zip](https://github.com/tensorflow/tensorflow/files/2205292/code.zip)
"
20914,ResourceVariable save will lead to OOM in distributed mode,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 0.9
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: 7.5
- **GPU model and memory**: None
- **Exact command to reproduce**: save ResourceVariable in a distributed mode


### Describe the problem
My model is more than 200GB so I run it in distributed mode on CPU, including 1000 workers and 100 ps. All the variables of my model are ResourceVariable partitioned by size,  and these variables are placed by the default tf.replica_device_setter. The model is triggered by a MonitoredTrainingSession. 

Problem happens when it begins to save model. The memory of ps0 rises to 200GB rapidly and then OOM.  I open the log placement and find that all variables are identity to ps0 when running save_op.
In ResourceVariableSaveable, line 182 reset the device, which leads to all save_ops are placed on ps0. I remove this line and re-run, it works correctly.

```
 168   class ResourceVariableSaveable(SaveableObject):
 169     """"""SaveableObject implementation that handles ResourceVariables.""""""
 170
 171     def __init__(self, var, slice_spec, name):
 172       self._var_device = var.device
 173       if isinstance(var, ops.Tensor):
 174         self.handle_op = var.op.inputs[0]
 175         tensor = var
 176       elif isinstance(var, resource_variable_ops.ResourceVariable):
 177
 178         def _read_variable_closure(v):
 179           def f():
 180             with ops.device(v.device):
 181               x = v.read_value()
 182             with ops.device(""/device:CPU:0""):
 183               return array_ops.identity(x)
 184           return f
 185
 186         self.handle_op = var.handle
 187         tensor = _read_variable_closure(var)
 188       else:
 189         raise ValueError(
 190             ""Saveable is neither a resource variable nor a read operation.""
 191             "" Got: %s"" % repr(var))
 192       spec = BaseSaverBuilder.SaveSpec(tensor, slice_spec, name)
 193       super(BaseSaverBuilder.ResourceVariableSaveable, self).__init__(
 194           var, [spec], name)
```

### Source code / logs
Open the log placement to see all save_ops are placed on ps0. So many such logs
```
 [2018-07-18 16:02:41.883522] [INFO] [31791] [tensorflow/core/common_runtime/placer.cc:698] Ignoring device specification /device:CPU:0 for node 'save_2/AssignVariableOp_176' because the input edge from 'Optimize/OptimizeLoss/CTR-PositionNetwork/position_hiddenlayer_1/weights/part_7/AdagradDecay_1'       is a reference connection and already has a device field set to /job:ps/task:16
```

[log.txt](https://github.com/tensorflow/tensorflow/files/2205249/log.txt)

"
20913,Keras guide webpage: tf.keras.version links to a dead webpage.,"## System Information
**Have I written custom code:** No
**OS Platform and Distribution:** Windows 10 Pro
**TensorFlow installed from:** source
**TensorFlow version:** N/A
**Bazel version:** N/A
**CUDA/cuDNN version:** N/A
**GPU model and memory:** Nvidia GeForce GTX 950m 4GB
**Exact command to reproduce:** N/A

## Describe the problem
- On [this](https://www.tensorflow.org/guide/keras) TensorFlow guide website, I found that it mentions using `tf.keras.version` to check the keras version. However, it should be `tf.keras.__version__` instead of `tf.keras.version`. 

- Also, it links to a [webpage](https://www.tensorflow.org/api_docs/python/tf/keras/__version__) which throws `404` error.

![tf keras issue](https://user-images.githubusercontent.com/29359259/42868356-bb35fbf0-8a8f-11e8-81e5-50b537942d62.PNG)
"
20912,"android how to get time consuming ,node type and other information","![20170623201903623](https://user-images.githubusercontent.com/11265867/42868674-7c9973c0-8aa5-11e8-9558-4ae018dd96bd.png)
 I find a tensorflow lite appâ€˜s screenshot ï¼Œhow to do it like that?
 "
20910,move_binary_operator_before_reshape.cc:76] Check failed: binary_op->inputs.size() == 2 (1 vs. 2),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux  4.17.5-1-ARCH
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: b'v1.9.0-rc2-623-geb04124bb7' 1.9.0-rc0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.14.1- (@non-git)
- **GCC/Compiler version (if compiling from source)**: gcc-7.1
- **CUDA/cuDNN version**: cuda-9.2 cudnn-7.1
- **GPU model and memory**: 16G
- **Exact command to reproduce**:

I use `models/research/object_detection/export_tflite_ssd_graph.py` to export the tflite_graph.pb and it's fine. Then use this command to convert it to tflite model:
`
bazel run -c opt tensorflow/contrib/lite/toco:toco -- \
      --input_file=$OUTPUT_DIR/tflite_graph.pb \
      --output_file=$OUTPUT_DIR/detect.tflite \
      --input_shapes=1,360,480,3 \
      --input_arrays=normalized_input_image_tensor \
      --output_arrays='raw_outputs/box_encodings','raw_outputs/class_predictions'  \
      --inference_type=QUANTIZED_UINT8 \
      --mean_values=128 \
      --std_values=128 \
      --change_concat_input_ranges=false \
      --allow_custom_ops
`
**It worked fine either yesterday but now when I ran  `git pull` on master branch** with newest commit id  81161f9d9987a8eb70793d95048c20be34292859 it crashed like this:
`
tensorflow/contrib/lite/toco/graph_transformations/move_binary_operator_before_reshape.cc:76] Check failed: binary_op->inputs.size() == 2 (1 vs. 2) Aborted
`
Can you take a look at it ?"
20909,Tflite_convert error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: tensorflow 1.9
- **Python version**: Python 2.7

### Describe the problem
I have written a script to convert my frozen file(.pb) into a quantized tflite (quant.lite). I keep getting the error shown below. Anyone can help me? 

tflite_convert: error: --default_ranges_min and --default_ranges_max must be used together

### Source code / logs
`INPUT=INPUT
OUTPUT=output_node0
INFILE=./mobilenet_v1_0.25_32_2class_batchsize1_eyes.h5.pb
TYPE=QUANTIZED_UINT8
OUTFILE=./mobilenet_v1_0.25_32_2class_batchsize1_eyes.quant.tflite
IMAGE_SIZE=32
MEAN=128 
STD=128
MIN=0
MAX=6'
tflite_convert \
  --graph_def_file=$INFILE \
  --output_file=$OUTFILE \
  --output_format=TFLITE \
  --inference_type=$TYPE \
  --input_arrays=$INPUT \
  --output_arrays=$OUTPUT \
  --mean_values=${MEAN} \
  --std_dev_values=${STD} \
  --default_ranges_min=${MIN} \
  --default_ranges_max=${MAX} \
  --input_shapes=1,${IMAGE_SIZE},${IMAGE_SIZE},3`
"
20908,tf.nn.conv2d() inconsistent dilation rate at runtime,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 LTS
- **TensorFlow installed from (source or binary)**: binary (pip install)
- **TensorFlow version (use command below)**: ('v1.7.0-3-g024aecf414', '1.7.0')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**: GTX1080, 8GB
- **Exact command to reproduce**: shown below

### Describe the problem
Dilated convolution via tf.nn.conv2d() with data_format='NHWC' gets corrupted to 'NCHW' during sess.run(). Since the data_format alone is corrupted and the dilation rate is unchanged, the code fails with an error message indicating that it does not support dilation along the depth dimension (dilation rate of [1, 2, 2, 1] is valid for 'NHWC' format but not for 'NCHW' format).

It seems that this is a CUDA problem, since if I disable the GPU using os.environ['CUDA_VISIBLE_DEVICES'] = '' line, the code does not error out. 
Weirdly enough, if I don't do anything to the output of tf.nn.conv2d(), the code does not error out either (corresponds to setting use_reduce_mean=False in the below example).
Also, if the dilation rate is set as [1, 1, 2, 2], the code does not error out, although this goes against the [documentation](https://www.tensorflow.org/versions/r1.7/api_docs/python/tf/nn/conv2d), which says that `The dimension order is determined by the value of data_format`

### Source code / logs

source code to reproduce the bug
```
import os

import numpy as np
import tensorflow as tf

# os.environ['CUDA_VISIBLE_DEVICES'] = ''


def bug():
    use_reduce_mean = True
    dilation_rate = 2

    # bug # 1: conv2d changes from NHWC to NCHW
    input_shape = [1, 32, 32, 1]
    in_place = tf.placeholder(dtype=tf.float32, shape=input_shape)
    filter_tensor = tf.Variable(tf.random_normal(
        [3, 3, 1, 1], dtype=tf.float32, stddev=0.1), trainable=True)

    out_tensor = tf.nn.conv2d(
        in_place, filter=filter_tensor, strides=(1, 1, 1, 1),
        padding='SAME', dilations=(1, dilation_rate, dilation_rate, 1),
        data_format='NHWC')

    if use_reduce_mean:
        out_tensor = tf.reduce_mean(out_tensor)
    
    with tf.Session() as sess:
        init_op = tf.global_variables_initializer()
        init_op.run()
    
        f_dict = {in_place: np.zeros(input_shape)}
        sess_out = sess.run(out_tensor, feed_dict=f_dict)


if __name__ == ""__main__"":
    bug()
```

error message:
```
 Executor failed to create kernel. Invalid argument: Current implementation does not yet support dilations in the batch and depth dimensions.
         [[Node: Conv2D = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 2, 2, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, Variable/read)]]
Traceback (most recent call last):
  File ""tmp.py"", line 48, in <module>
    bug()
  File ""tmp.py"", line 44, in bug
    sess_out = sess.run(out_tensor, feed_dict=f_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support dilations in the batch and depth dimensions.
         [[Node: Conv2D = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 2, 2, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, Variable/read)]]

Caused by op u'Conv2D', defined at:
  File ""tmp.py"", line 48, in <module>
    bug()
  File ""tmp.py"", line 27, in bug
    data_format='NHWC')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 953, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3290, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Current implementation does not yet support dilations in the batch and depth dimensions.
         [[Node: Conv2D = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 2, 2, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, Variable/read)]]
```"
20906,failed to load keras model with tensorflow r1.9,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: from binary(pip install)
- **TensorFlow version (use command below)**:1.9.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**:  CUDA-8.0
- **GPU model and memory**: GTX1080Ti, 12G
- **Exact command to reproduce**: just as the code below

### Describe the problem
As the official document [here](https://www.tensorflow.org/guide/keras), tf.keras.models could be saved with `model.save('my_model.h5')`, namely, ""The entire model can be saved to a file that contains the weight values, the model's configuration, and even the optimizer's configuration"". However, when I load the model with `model = tf.keras.models.load_model('lenet_mnist.h5')`, it throws an error: **""You are trying to load a weight file containing 5 layers into a model with 0 layers.""**

### Source code / logs
#### source code(lenet_mnist.py)
```
import tensorflow as tf
import numpy as np
def train_test_data():
    (x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    x_train = np.expand_dims(x_train, -1)
    x_test = np.expand_dims(x_test, -1)
    return x_train, y_train, x_test, y_test

def train_fn():
    x_train, y_train, x_test, y_test = train_test_data()
    model = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(6, [5, 5], strides=1, padding=""same"", use_bias=True),
        tf.keras.layers.AvgPool2D([2, 2], 2, padding=""valid""),   
        tf.keras.layers.LeakyReLU(0),
        tf.keras.layers.Conv2D(16, [5, 5], strides=1, padding=""valid"", use_bias=True),
        tf.keras.layers.AvgPool2D([2, 2], 2, padding=""valid""),
        tf.keras.layers.LeakyReLU(0),
        tf.keras.layers.Conv2D(120, [5, 5], strides=1, padding=""valid"", use_bias=True),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(84, activation=tf.nn.relu),
        tf.keras.layers.Dense(10, activation=tf.nn.softmax),
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model.fit(x_train, y_train, epochs=5, batch_size=300)
    loss, test_acc = model.evaluate(x_test, y_test, verbose=1)
    print(""test_acc: "" + str(test_acc))
    model.save('lenet_mnist.h5')

def test_fn():
    x_train, y_train, x_test, y_test = train_test_data()
    model = tf.keras.models.load_model('lenet_mnist.h5')
    loss, test_acc = model.evaluate(x_test, y_test, verbose=1)
    print(""test_acc: "" + str(test_acc))

def main(unused):
    train_fn()
    test_fn()

if __name__ == '__main__':
    tf.app.run()
```

#### log
```
Traceback (most recent call last):
  File ""lenet_mnist.py"", line 60, in <module>
    tf.app.run()
  File ""/home/wangwenxiao/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""lenet_mnist.py"", line 57, in main
    test_fn()
  File ""lenet_mnist.py"", line 49, in test_fn
    model = tf.keras.models.load_model('lenet_mnist.h5')
  File ""/home/wangwenxiao/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py"", line 232, in load_model
    load_weights_from_hdf5_group(f['model_weights'], model.layers)
  File ""/home/wangwenxiao/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py"", line 732, in load_weights_from_hdf5_group
    ' layers.')
ValueError: You are trying to load a weight file containing 5 layers into a model with 0 layers.
```
"
20905,Supporting a TF lite shared library target,"### Describe the problem
Would the TF project be open to supporting a TF lite shared library target (as is done already with `libtensorflow.so` and `libtensorflow_cc.so`)?

I believe many people would benefit from this, based on recent related issues:
https://github.com/tensorflow/tensorflow/issues/18060
https://github.com/tensorflow/tensorflow/issues/17826
https://github.com/tensorflow/tensorflow/issues/16219

I am happy to do the upfront work, but I would require assistance from TF devs for e.g. correct build configuration and on-going support.

### Source code / logs

We could add a `tf_cc_shared_object` target to `tensorflow/contrib/lite/BUILD`:
```
tf_cc_shared_object(
    name = ""libtensorflow_lite.so"",
    framework_so = [],
    linkopts = tflite_linkopts() + [""-s""],  # Strip library.
    visibility = [""//visibility:public""],
    deps = [
        "":framework"",
        ""//tensorflow/contrib/lite/kernels:builtin_ops"",
    ]
)
```

We could also add a header-only target to define the headers for use with `libtensorflow_lite.so` (although this won't be useful until [issue 5192](https://github.com/bazelbuild/bazel/issues/5192) has been resolved).

As mentioned above, I'd need some input to decide the correct build config (e.g. `linkopts`, use of `framework_so`)."
20904,build_all_ios.sh error,"```
In file included from /Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/Tensor:79:
In file included from /Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/ThreadPool:58:
/Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/SimpleThreadPool.h:153:5: error: 
      thread-local storage is not supported for the current target
    EIGEN_THREAD_LOCAL PerThread per_thread;
    ^
/Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/ThreadLocal.h:15:35: note: 
      expanded from macro 'EIGEN_THREAD_LOCAL'
#define EIGEN_THREAD_LOCAL static __thread
                                  ^
<command line>:3:18: note: expanded from here
#define __thread thread_local
                 ^
In file included from tensorflow/core/common_runtime/graph_runner.cc:21:
In file included from ./tensorflow/core/common_runtime/graph_runner.h:23:
In file included from ./tensorflow/core/common_runtime/device.h:35:
In file included from ./tensorflow/core/framework/allocator.h:23:
In file included from ./tensorflow/core/framework/numeric_types.h:20:
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from /Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/Tensor:79:
In file included from /Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/ThreadPool:59:
/Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:326:5: error: 
      thread-local storage is not supported for the current target
    EIGEN_THREAD_LOCAL PerThread per_thread_;
    ^
/Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/ThreadLocal.h:15:35: note: 
      expanded from macro 'EIGEN_THREAD_LOCAL'
#define EIGEN_THREAD_LOCAL static __thread
                                  ^
<command line>:3:18: note: expanded from here
#define __thread thread_local
                 ^
In file included from tensorflow/core/common_runtime/local_device.cc:18:
In file included from ./tensorflow/core/common_runtime/local_device.h:19:
In file included from ./tensorflow/core/common_runtime/device.h:35:
In file included from ./tensorflow/core/framework/allocator.h:23:
In file included from ./tensorflow/core/framework/numeric_types.h:20:
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from /Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/Tensor:79:
In file included from /Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/ThreadPool:58:
/Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/SimpleThreadPool.h:153:5: error: 
      thread-local storage is not supported for the current target
    EIGEN_THREAD_LOCAL PerThread per_thread;
    ^
/Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/ThreadLocal.h:15:35: note: 
      expanded from macro 'EIGEN_THREAD_LOCAL'
#define EIGEN_THREAD_LOCAL static __thread
                                  ^
<command line>:3:18: note: expanded from here
#define __thread thread_local
                 ^
In file included from tensorflow/core/common_runtime/local_device.cc:18:
In file included from ./tensorflow/core/common_runtime/local_device.h:19:
In file included from ./tensorflow/core/common_runtime/device.h:35:
In file included from ./tensorflow/core/framework/allocator.h:23:
In file included from ./tensorflow/core/framework/numeric_types.h:20:
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from /Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/Tensor:79:
In file included from /Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/ThreadPool:59:
/Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:326:5: error: 
      thread-local storage is not supported for the current target
    EIGEN_THREAD_LOCAL PerThread per_thread_;
    ^
/Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/ThreadLocal.h:15:35: note: 
      expanded from macro 'EIGEN_THREAD_LOCAL'
#define EIGEN_THREAD_LOCAL static __thread
                                  ^
<command line>:3:18: note: expanded from here
#define __thread thread_local
                 ^
gcc --std=c++11 -DIS_SLIM_BUILD -fno-exceptions -DNDEBUG -O3 -mios-simulator-version-min=9.0 -arch i386 -mno-sse -fembed-bitcode -D__thread=thread_local -DUSE_GEMM_FOR_CONV -Wno-c++11-narrowing -DTF_LEAN_BINARY -D__ANDROID_TYPES_SLIM__ -fno-exceptions -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator11.3.sdk -MT /Users/viatom/tensorflow/tensorflow/contrib/makefile/gen/obj/ios_I386/tensorflow/core/common_runtime/memory_types.o -MMD -MP -MF /Users/viatom/tensorflow/tensorflow/contrib/makefile/gen/dep/ios_I386//tensorflow/core/common_runtime/memory_types.Td -I. -I/Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/ -I/Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/eigen -I/Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp -I/Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public -I/Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/fft2d -I/Users/viatom/tensorflow/tensorflow/contrib/makefile/downloads/double_conversion -I/Users/viatom/tensorflow/tensorflow/contrib/makefile/gen/proto/ -I/Users/viatom/tensorflow/tensorflow/contrib/makefile/gen/proto_text/ -I/Users/viatom/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/include -I/usr/local/include -c tensorflow/core/common_runtime/memory_types.cc -o /Users/viatom/tensorflow/tensorflow/contrib/makefile/gen/obj/ios_I386/tensorflow/core/common_runtime/memory_types.o
2 errors generated.
make: *** [/Users/viatom/tensorflow/tensorflow/contrib/makefile/gen/obj/ios_I386/tensorflow/core/common_runtime/local_device.o] Error 1
make: *** Waiting for unfinished jobs....
2 errors generated.
make: *** [/Users/viatom/tensorflow/tensorflow/contrib/makefile/gen/obj/ios_I386/tensorflow/core/common_runtime/graph_runner.o] Error 1
+ '[' 2 -ne 0 ']'
+ echo 'i386 compilation failed.'
i386 compilation failed.
+ exit 1

```
I tried running `tensorflow/contrib/makefile/build_all_ios.sh` and then this issue occurred. I searched for a lot of similar issue, but not fixed the issue. 
So, i need help."
20903,Preinstalled tensorflow on ubuntu image ,"Hi 
Is There any Ubuntu image that comes with Tensorflow preinstalled,it will be more easier to start learning Tensorflow more quickly for beginners 
I actually I have troubles with installing Tensorflow-gpu

Thanks "
20901,"""Getting Started with TensorFlow"" tutorial is missing input_shape argument","In the tutorial ""_Getting Started with TensorFlow_"" there is a code segment for image classification, using the MNIST dataset. The segment includes a Keras Sequential model, which is missing the `input_shape` argument in the first layer. 

In particular, I propose changing: 
- `tf.keras.layers.Flatten()` to `tf.keras.layers.Flatten(input_shape = (28, 28))`.

""_Getting Started with TensorFlow_"" link: https://www.tensorflow.org/tutorials/"
20900,[Tensorflow lite] iOS build for any archs fails : error: no matching member function for call to 'Verify',"-----------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.3
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 2.7 / 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: 4.2.1
== compiler =====================================================
Apple LLVM version 9.1.0 (clang-902.0.39.2)
Target: x86_64-apple-darwin17.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: tensorflow/contrib/lite/build_ios_universal_lib.sh

### Describe the problem
iOS build for any architecture will fail.
I checked 'verifier' exist and defined in tensorflow/contrib/lite/tools/ and Verify is in there.
If i am right I guess it is related to the feature 'template deduction' which is c++11 or gcc version 
I am not sure if this failure comes from the bug of the code or my environmental setups but I think I throughly read the build documentation and followed the guidelines.

Here is my practice.

1. I installed xcode from the app store, (success)
 - run xcode-select --install
2. install homebrew (success)
3. brew install automake libtool (success)
4. git cloned tensorflow, (no issue)
5. download_install_dependencies (success)


### Source code / logs
...
clang++ -fPIC -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -pthread  -std=c++11 -fPIC -O3 -DNDEBUG -miphoneos-version-min=9.0 -DGEMMLOWP_ALLOW
_SLOW_SCALAR_FALLBACK -DTFLITE_USE_APPLE_ACCELERATE_FOR_CONV -fembed-bitcode -Wno-c++11-narrowing -mno-thumb -fno-exceptions -isysroot 
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS11.4.sdk -arch arm64 -O3 -I. -I/Users/con
tinuumai/tensorflow/tensorflow/contrib/lite/../../../ -I/Users/june/tensorflow/tensorflow/contrib/lite/downloads/ -I/Users/conti
nuumai/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/Users/june/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I
/Users/june/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/Users/june/tensorflow/tensorflow/contrib/lite/down
loads/farmhash/src -I/Users/june/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/Users/june/tensorflo
w/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/kernels/lsh_projection.cc -o /Users/june/tens
orflow/tensorflow/contrib/lite/gen/obj/ios_arm64/tensorflow/contrib/lite/kernels/lsh_projection.o                                      clang: warning: argument unused during compilation: '-mno-thumb' [-Wunused-command-line-argument]                                      5 warnings generated.                                                                                                                  In file ```
**included from tensorflow/contrib/lite/interpreter.cc:33:                                                                       
**./tensorflow/contrib/lite/schema/schema_generated.h:1730:21: error: no matching member function for call to 'Verify'                   
           verifier.Verify(min()) &&                                                                                                   
           ~~~~~~~~~^~~~~~** **                                                                                                             
/Users/june/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1775:29: note: candidate 
      template ignored: couldn't infer template argument 'T'                                                                           
  template<typename T> bool Verify(uoffset_t elem) const {                                                                             
                            ^                                                                                                          /Users/june/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1784:29: note: candidate 
      function template not viable: requires 2 arguments, but 1 was provided                                                           
  template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)
                            ^
/Users/june/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1763:8: note: candidate
      function not viable: requires 2 arguments, but 1 was provided
  bool Verify(uoffset_t elem, size_t elem_len) const {
       ^
/Users/june/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1780:8: note: candidate
      function not viable: requires 3 arguments, but 1 was provided
  bool Verify(const uint8_t *base, voffset_t elem_off, size_t elem_len) const {
       ^
In file included from tensorflow/contrib/lite/interpreter.cc:33:
./tensorflow/contrib/lite/schema/schema_generated.h:1732:21: error: no matching member function for call to 'Verify'
           verifier.Verify(max()) &&
           ~~~~~~~~~^~~~~~
/Users/june/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1775:29: note: candidate
      template ignored: couldn't infer template argument 'T'
  template<typename T> bool Verify(uoffset_t elem) const {
                            ^
/Users/june/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1784:29: note: candidate
      function template not viable: requires 2 arguments, but 1 was provided
  template<typename T> bool Verify(const uint8_t *base, voffset_t elem_off)
                            ^
/Users/june/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:1763:8: note: candidate
      function not viable: requires 2 arguments, but 1 was provided

```
[log.txt](https://github.com/tensorflow/tensorflow/files/2203945/log.txt)
"
20899,"load_op_library fails with undefined symbol in 1.7, but not 1.1","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux SLES 12.1
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 3.5.4 (as Anaconda environment)
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: 4.8.5 (for custom Op)
- **CUDA/cuDNN version**: 7.1.2
- **GPU model and memory**: Nvidia Tesla M40 11443MiB
- **Exact command to reproduce**: `tf.load_op_library('zero_out.so')`

### Describe the problem
When I follow the instructions on the [adding a new op page](https://www.tensorflow.org/extend/adding_an_op#top_of_page), loading the example `zero_out.so` fails with `tensorflow.python.framework.errors_impl.NotFoundError: zero_out.so: undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringEv` in tensorflow 1.7. However, the same procedure succeeds with tensorflow 1.1, python 3.5.2.   
I have seen similar issues (e.g. #9137), however adding `-D_GLIBCXX_USE_CXX11_ABI=0` doesn't help (and probably shouldn't, as GCC < 5). The closest issue is seemingly #17619 since the problem seems to be with the linker being unable to find the correct library at run time.

### Source code / logs
Zero-out source code is available on the [adding a new op page](https://www.tensorflow.org/extend/adding_an_op#top_of_page).

**Full Traceback**

```sh
>>> import tensorflow as tf
>>> tf.load_op_library('zero_out.so')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/anaconda3/envs/TF17/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py"", line 58, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)
  File ""/usr/local/anaconda3/envs/TF17/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: zero_out.so: undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringEv
```
"
20891,Feature Request: tf.Session().as_default() should have a flag to keep the session alive,"### Describe the problem
I find a scenario in which keeping a session alive (even after closing the context manager) is useful. Let's say that I have a method `train(steps)` that will train a model for the next `steps` iterations. Given the current API r1.9, I could think of no other way than to save and restore the session between method calls. That would incur a huge overhead indeed! 

I could think of two possible ways out:
- providing an API to set a default session at will without automatically closing the session. It could come in a form of a context manager.
- adding a flag to the `.as_default()` method in `tf.Session()` to keep it alive after the context manager is closed.

### Source code / logs
The following code shows the hypothetical scenario.

```
model = # some model
model.train(100) # train for 100 iterations
# default session should be None here (this rules out the usage of InteractiveSession)
model.train(100) # train for another 100 iterations
```

The following code shows the two possible solutions:

```
# a new context mananger to set default session
sess = tf.Session()
with tf.session_as_default(sess):
    ...
sess.close()

# adding a flag to the as_default method
sess = tf.Session()
with sess.as_default(keep_session=True):
    ...
with sess.as_default():
    ...
# session closed
```"
20887,"tf_nightly fails with from tensorflow.python.keras._impl.keras.backend import abs ""cannot import name 'abs'""","```
pip install tf_nightly
python
import tensorflow

Python 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 17:14:51) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 81, in <module>
    from tensorflow.python import keras
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py"", line 24, in <module>
    from tensorflow.python.keras import activations
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/activations/__init__.py"", line 22, in <module>
    from tensorflow.python.keras._impl.keras.activations import elu
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/__init__.py"", line 21, in <module>
    from tensorflow.python.keras._impl.keras import activations
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/activations.py"", line 23, in <module>
    from tensorflow.python.keras._impl.keras import backend as K
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py"", line 38, in <module>
    from tensorflow.python.layers import base as tf_base_layers
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 25, in <module>
    from tensorflow.python.keras.engine import base_layer
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/__init__.py"", line 23, in <module>
    from tensorflow.python.keras.engine.base_layer import InputSpec
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 33, in <module>
    from tensorflow.python.keras import backend
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/backend/__init__.py"", line 22, in <module>
    from tensorflow.python.keras._impl.keras.backend import abs
ImportError: cannot import name 'abs'
>>> ```"
20886,Freezing + fake quantization of graph,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source (v1.8) / binary (v1.9)
- **TensorFlow version (use command below)**:  v1.8.0-0-g93bc2e2072 1.8.0 AND v1.9.0-0-g25c197e023 1.9.0
- **Python version**: 3.6 
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**: (result of gcc -v) gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9) 
- **CUDA/cuDNN version**: Not used
- **GPU model and memory**: Not used
- **Exact command to reproduce**: Code given below

### Describe the problem
Problem described [here](https://stackoverflow.com/questions/51327204/freezing-quantizing-fake-quantized-graph), but didn't have any luck; essentially, I cannot freeze a graph which has been fake quantized using [tf.contrib.quantize.create_training_graph()](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/quantize/python/quantize_graph.py). I have a sample below (there may be a better way to add a freezing operation at the end of the [graph quantization test](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/quantize/python/quantize_graph.py) script).

The code shown spits out the following error (or variants dependent on architecture):

> ValueError: Input 0 of node import/weights_quant/AssignMinLast was passed float from import/weights_quant/min:0 incompatible with expected float_ref

I may be doing something wrong - this arises regardless of which order I apply the creation of training/eval graphs and which of the 3 methods I use for freezing the graph. I also tried the fixes [here](https://github.com/davidsandberg/facenet/issues/161) to no avail.

### Source code / logs
```
import tensorflow as tf


def conv_simple(_input):
    _input_r = tf.reshape(_input, shape=[-1, 28, 28, 1])
    _conv1 = tf.nn.conv2d(_input_r, tf.Variable(tf.random_normal([3, 3, 1, 64], stddev=0.1)), strides=[1, 1, 1, 1],
                          padding='SAME')
    _conv2 = tf.nn.bias_add(_conv1, tf.Variable(tf.random_normal([64], stddev=0.1)))
    _conv3 = tf.nn.relu(_conv2)
    _pool = tf.nn.max_pool(_conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
    _dense = tf.reshape(_pool, [-1, 14 * 14 * 64])
    _out = tf.add(tf.matmul(_dense, tf.Variable(tf.random_normal([14 * 14 * 64, 10], stddev=0.1))),
                  tf.Variable(tf.random_normal([10], stddev=0.1)), name='Output')
    out = {
        'input_r': _input_r, 'conv1': _conv1, 'conv2': _conv2, 'conv3': _conv3
        , 'pool': _pool, 'dense': _dense, 'out': _out
    }
    return out


# tf Graph input
x = tf.placeholder(tf.float32, [None, 784], name='X')
_pred = conv_simple(x)['out']

tf.contrib.quantize.create_training_graph()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    # FREEZE GRAPH
    output_graph_def = sess.graph.as_graph_def()
    input_names = [""X""]
    output_names = [""Output""]
    LOG_DIR = '/some/path/to/log/dir/'

    output_graph_def = tf.graph_util.convert_variables_to_constants(
        sess,  # The session is used to retrieve the weights
        output_graph_def,  # The graph_def is used to retrieve the nodes
        output_names  # The output node names are used to select the useful nodes,
    )

    g = tf.import_graph_def(output_graph_def)
```

"
20885,AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike',"I am using keras where tensorflow as backend.Getting the following error:


---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-8-509bb069f0c0> in <module>()
     10 KEYS = [str(key) for key in range(len(database))]
     11 
---> 12 encoder    = load_model(ENCODER_PATH, custom_objects={'VisualBinaryRegulizer': VisualBinaryRegulizer})
     13 
     14 input_size  = encoder.layers[0].input.shape.as_list()[1:3]

~/.local/lib/python3.5/site-packages/keras/engine/saving.py in load_model(filepath, custom_objects, compile)
    259             raise ValueError('No model found in config file.')
    260         model_config = json.loads(model_config.decode('utf-8'))
--> 261         model = model_from_config(model_config, custom_objects=custom_objects)
    262 
    263         # set weights

~/.local/lib/python3.5/site-packages/keras/engine/saving.py in model_from_config(config, custom_objects)
    333                         '`Sequential.from_config(config)`?')
    334     from ..layers import deserialize
--> 335     return deserialize(config, custom_objects=custom_objects)
    336 
    337 

~/.local/lib/python3.5/site-packages/keras/layers/__init__.py in deserialize(config, custom_objects)
     53                                     module_objects=globs,
     54                                     custom_objects=custom_objects,
---> 55                                     printable_module_name='layer')

~/.local/lib/python3.5/site-packages/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    143                     config['config'],
    144                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +
--> 145                                         list(custom_objects.items())))
    146             with CustomObjectScope(custom_objects):
    147                 return cls.from_config(config['config'])

~/.local/lib/python3.5/site-packages/keras/engine/network.py in from_config(cls, config, custom_objects)
   1034         # First, we create all layers and enqueue nodes to be processed
   1035         for layer_data in config['layers']:
-> 1036             process_layer(layer_data)
   1037         # Then we process nodes in order of layer depth.
   1038         # Nodes that cannot yet be processed (if the inbound node

~/.local/lib/python3.5/site-packages/keras/engine/network.py in process_layer(layer_data)
   1020 
   1021             layer = deserialize_layer(layer_data,
-> 1022                                       custom_objects=custom_objects)
   1023             created_layers[layer_name] = layer
   1024 

~/.local/lib/python3.5/site-packages/keras/layers/__init__.py in deserialize(config, custom_objects)
     53                                     module_objects=globs,
     54                                     custom_objects=custom_objects,
---> 55                                     printable_module_name='layer')

~/.local/lib/python3.5/site-packages/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    143                     config['config'],
    144                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +
--> 145                                         list(custom_objects.items())))
    146             with CustomObjectScope(custom_objects):
    147                 return cls.from_config(config['config'])

~/.local/lib/python3.5/site-packages/keras/engine/network.py in from_config(cls, config, custom_objects)
   1044                 if layer in unprocessed_nodes:
   1045                     for node_data in unprocessed_nodes.pop(layer):
-> 1046                         process_node(layer, node_data)
   1047 
   1048         name = config.get('name')

~/.local/lib/python3.5/site-packages/keras/engine/network.py in process_node(layer, node_data)
   1001             if input_tensors:
   1002                 if len(input_tensors) == 1:
-> 1003                     layer(input_tensors[0], **kwargs)
   1004                 else:
   1005                     layer(input_tensors, **kwargs)

~/.local/lib/python3.5/site-packages/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)
    412                 # Raise exceptions in case the input is not compatible
    413                 # with the input_spec specified in the layer constructor.
--> 414                 self.assert_input_compatibility(inputs)
    415 
    416                 # Collect input shapes to build layer.

~/.local/lib/python3.5/site-packages/keras/engine/base_layer.py in assert_input_compatibility(self, inputs)
    277         for x in inputs:
    278             try:
--> 279                 K.is_keras_tensor(x)
    280             except ValueError:
    281                 raise ValueError('Layer ' + self.name + ' was called with '

~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py in is_keras_tensor(x)
    467     ```
    468     """"""
--> 469     if not is_tensor(x):
    470         raise ValueError('Unexpectedly found an instance of type `' +
    471                          str(type(x)) + '`. '

~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py in is_tensor(x)
    475 
    476 def is_tensor(x):
--> 477     return isinstance(x, tf_ops._TensorLike) or tf_ops.is_dense_tensor_like(x)
    478 
    479 

AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'



"
20883,How to distribute custom OPs with pypi?,"We've got an instruction how to build custom OPs https://www.tensorflow.org/extend/adding_an_op

But there is no information about how to pack custom ops and distribute them with pypi.
Shared library built with system compiler could not be uploaded to pypi due to PEP 513 limitation.
And there is no instructions for building custom OPs outside ""user_ops"" dir in tensorflow source.

So, some boilerplate for building pip packages with custom OPs required.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS X
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6

"
20882,Tf reports DLL load failed but passes existance check if wrong subversion of cudnn used.,"Windows 7, python 3.5

Using pip3 install --upgrade tensorflow-gpu yields ""ImportError: DLL load failed: The specified module could not be found."" when incorrect subversion of cudnn64_7.dll is used. 

NVIDIA provides several versions of cudnn for windows which are all called cudnn64_7.dll.  Thus, the existence checks tf does for the file cudnn64_7.dll pass but when loading the DLL python yields the error mentioned above.

Suggest adding checks for correct subversion of DLL.  Minimally, let the user know the state can occur as part of the error message."
20881,How to use LSTM layer in tfLite ?,"How to use the following code ?

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/lstm.cc"
20880,AttributeError: 'DataFrame' object has no attribute 'dtype',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
AttributeError: 'DataFrame' object has no attribute 'dtype'


### Source code / logs

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-14-3c90f2017a02> in <module>()
----> 1 **reg.fit(input_fn=training_input_fn(batch_size=BATCH_SIZE),steps=STEPS_PER_EPOCH)**

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    430                 'in a future version' if date is None else ('after %s' % date),
    431                 instructions)
--> 432       return func(*args, **kwargs)
    433     return tf_decorator.make_decorator(func, new_func, 'deprecated',
    434                                        _add_deprecated_arg_notice_to_docstring(

~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)
    522       hooks.append(basic_session_run_hooks.StopAtStepHook(steps, max_steps))
    523 
--> 524     loss = self._train_model(input_fn=input_fn, hooks=hooks)
    525     logging.info('Loss for final step: %s.', loss)
    526     return self

~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in _train_model(self, input_fn, hooks)
   1036       random_seed.set_random_seed(self._config.tf_random_seed)
   1037       global_step = training_util.create_global_step(g)
-> 1038       features, labels = input_fn()
   1039       self._check_inputs(features, labels)
   1040       training_util._get_or_create_global_step_read()  # pylint: disable=protected-access

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/numpy_io.py in input_fn()
    194         num_threads=num_threads,
    195         enqueue_size=batch_size,
--> 196         num_epochs=num_epochs)
    197 
    198     batch = (

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py in _enqueue_data(data, capacity, shuffle, min_after_dequeue, num_threads, seed, name, enqueue_size, num_epochs, pad_value)
    390     elif isinstance(data, collections.OrderedDict):
    391       types = [dtypes.int64
--> 392               ] + [dtypes.as_dtype(col.dtype) for col in data.values()]
    393       queue_shapes = [()] + [col.shape[1:] for col in data.values()]
    394       get_feed_fn = _OrderedDictNumpyFeedFn

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py in <listcomp>(.0)
    390     elif isinstance(data, collections.OrderedDict):
    391       types = [dtypes.int64
--> 392               ] + [dtypes.as_dtype(col.dtype) for col in data.values()]
    393       queue_shapes = [()] + [col.shape[1:] for col in data.values()]
    394       get_feed_fn = _OrderedDictNumpyFeedFn

~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __getattr__(self, name)
   4370             if self._info_axis._can_hold_identifiers_and_holds_name(name):
   4371                 return self[name]
-> 4372             return object.__getattribute__(self, name)
   4373 
   4374     def __setattr__(self, name, value):
"
20879,NCCL not detected on Arch Linux,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: source (r1.9)
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.7.0
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: 8.1.1
- **CUDA/cuDNN version**: 9.2/7.1.4
- **NCCL version**: 2.2.13
- **GPU model and memory**: Nvidia GTX 1060 3GB
- **Exact command to reproduce**: ./configure

### Problem
`./configure` does not detect my `libnccl.so.2` located at `/opt/cuda/lib64`.

#### Output of `./configure`
```
Please specify the location of python. [Default is /home/mukundan/.pyenv/versions/machine_learning/bin/python]: 


Found possible Python library paths:
  /home/mukundan/.pyenv/versions/machine_learning/lib/python3.7/site-packages
Please input the desired Python library path to use.
Default is [/home/mukundan/.pyenv/versions/machine_learning/lib/python3.7/site-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: 
Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: 
Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: 
Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: 
Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: 
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.2


Please specify the location where CUDA 9.2 toolkit is installed. Refer to README.md for more details. [Default is /opt/cuda]: 


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.1.4


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /opt/cuda]:


Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Please specify the NCCL version you want to use. [Leave empty to default to NCCL 1.3]: 2.2.13


Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /opt/cuda]:


Invalid path to NCCL 2 toolkit, /opt/cuda/lib/libnccl.so.2 or /opt/cuda/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2
```"
20878,Issue with converting Keras .h5 files to .tflite files,"There seems to be an issue with the above mentioned conversion.
According to this documentation (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md#keras) it should be possible to convert a Keras model into a .tflite file. However, when I try to convert the file, the following output is given: 

> tflite_convert: error: one of the arguments --graph_def_file --saved_model_dir is required.

I made sure that the .h5 contains both the model and the weights. Other conversions like .pb files have worked with this file as well. Also in the help section of the commad there is no reference to --keras_model_file.  

**Update**
I was using the wrong TF version so I got the error message above. However, using the nightly build results in other error messages: 

> ValueError: Unknown activation function:relu6

This error normally occurs when using load_model but there is the possibility to specify custom_objects (like relu6) to avoid it.

How can this be done with tflite_convert?"
20874,Error when using batch_norm with MirroredStrategy,"### System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): using nvidia containter: https://docs.nvidia.com/deeplearning/dgx/tensorflow-release-notes/rel_18.06.html#rel_18.06
TensorFlow version (use command below):1.8.0
Python version: 3.5
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: 9.0.176
GPU model and memory: nvidia tesla v100
Exact command to reproduce: N/A

### Describe the problem
I want to apply transfer learning using an existing pretrained network (inception v4). For this I use the tf-slim models. When running this on a single GPU, This works as expected, however when using `tf.contrib.distribute.MirroredStrategy`  I get an exception. Apparently the MirroredStrategy has some issues with batch_norm

### Source code / logs
I tried to extract the relevant part from my code:
```
import research.slim.nets.nets_factory as nets_factory

...

def construct_architecture(self, input_tensor, mode):
        # network topology
        network_fn = nets_factory.get_network_fn('inception_v4', num_classes=self.configuration.get(""nr_classes""),
                                                      is_training=True)
        logits,_ = network_fn(input_tensor)

        if mode == tf.estimator.ModeKeys.PREDICT:
            predicted_classes = tf.argmax(logits, 1)
            predictions = {
                'class_ids': predicted_classes[:, tf.newaxis],
                'probabilities': tf.nn.softmax(logits),
                'logits': logits,
            }
            self.output_tensor = predictions
        else:
            self.output_tensor = logits
```

This results in the following error trace:
```
Traceback (most recent call last):
  File ""train.py"", line 27, in <module>
    experimenter.run_training_experiment(config)
  File ""/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py"", line 39, in run_training_experiment
    tf.estimator.train_and_evaluate(self.trainer, self.training_specs, self.eval_specs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 451, in train_and_evaluate
    return executor.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 590, in run
    return self.run_local()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 691, in run_local
    saving_listeners=saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 376, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1143, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1255, in _train_model_distributed
    self.config)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py"", line 777, in call_for_each_tower
    return self._call_for_each_tower(fn, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 308, in _call_for_each_tower
    coord.join(threads)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/onnx/six.py"", line 693, in reraise
    raise value
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 519, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1133, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py"", line 111, in get_model_fn
    self.configure_network(input_tensor=features, output_tensor=labels, mode=mode)
  File ""/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py"", line 75, in configure_network
    self.network.construct_network(input_tensor=input_tensor, output_tensor=output_tensor, mode=mode)
  File ""/media/local/myfiles/mynetwork.py"", line 64, in construct_network
    self.construct_architecture(input_tensor=input_tensor,mode=mode)
  File ""/media/local/myfiles/mynetwork.py"", line 48, in construct_architecture
    logits,_ = network_fn(input_tensor)
  File ""/opt/tf-slim/models/research/slim/nets/nets_factory.py"", line 141, in network_fn
    return func(images, num_classes, is_training=is_training, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/slim-0.1-py3.5.egg/nets/inception_v4.py"", line 286, in inception_v4
  File ""/usr/local/lib/python3.5/dist-packages/slim-0.1-py3.5.egg/nets/inception_v4.py"", line 178, in inception_v4_base
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1154, in convolution2d
    conv_dims=2)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1066, in convolution
    outputs = normalizer_fn(outputs, **normalizer_params)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 650, in batch_norm
    outputs = layer.apply(inputs, training=is_training)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 805, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py"", line 362, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 736, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/normalization.py"", line 158, in call
    return super(BatchNormalization, self).call(inputs, training=training)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py"", line 514, in call
    outputs = self._fused_batch_norm(inputs, training=training)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py"", line 420, in _fused_batch_norm
    momentum)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py"", line 369, in _assign_moving_average
    with ops.colocate_with(variable):
  File ""/usr/lib/python3.5/contextlib.py"", line 59, in __enter__
    return next(self.gen)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3939, in _colocate_with_for_gradient
    with self.colocate_with(op, ignore_existing):
  File ""/usr/lib/python3.5/contextlib.py"", line 59, in __enter__
    return next(self.gen)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3992, in colocate_with
    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1255, in internal_convert_to_tensor_or_indexed_slices
    value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1094, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py"", line 414, in _tensor_conversion_mirrored
    assert not as_ref
AssertionError
```


Thanks for the help

Jonas"
20873,Will tensorflow support 'PMI' like process management protocol for large scale cluster mode? ,"### System information

not related.

### Describe the problem

I'm  trying to use distributed tensorflow in a very large scale mode, say, more that 5000 nodes.

As the distributed tf using guide described in: https://www.tensorflow.org/deploy/distributed, I have to launch the python script by giving it a very long parameter list,  which is used to specify the endpoint of each node in the cluster including itself. just like:

`python train.py --worker_hosts=""ip1:port1,ip2:port2,..."" --ps_hosts=""ip1:port1,ip2:port2,...""` 

This list will be very long in large scale mode, and may cause stack overflow. Because the stack used to store parameters is limited in size.

Have the tf developers considered to add support for PMI (refer to: http://www.mcs.anl.gov/papers/P1760.pdf)? With PMI, we can use a ""tracker"" like role to manage all the endpoints. Each node has only to know the tracker endpoint when launching, the it can connect to the tracker to acquire endpoints of the other nodes."
20872,Cannot allocate memory for the interpreter,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: example source code and custom model
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:github master branch
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:0.11.0
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:



### Describe the problem

I'm trying to build & run tensorflow lite example using custom model.
Build is ok, but error occurs running this program.
why this error occur?

`root@90f212114f89:/tensorflow# bazel-bin/tensorflow/contrib/lite/examples/minimal/minimal /root/DNNSE/model/tflite_model.lite`
`tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.`
`tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.`
`tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.`
`tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.`
`Node 8 failed to prepare.`

`Error at tensorflow/contrib/lite/examples/minimal/minimal.cc:59`

Used model is custom model training on tensorflow and convert tensorflow lite using toco.
I think some error in the process of converting model to lite.
But i don't know how to debugging it.

I also try to run in android example.
There are same error.

E/AndroidRuntime: FATAL EXCEPTION: main
                  Process: android.example.com.tflitecamerademo, PID: 13202
                  java.lang.RuntimeException: Unable to start activity ComponentInfo{android.example.com.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity}: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.Node 8 failed to prepare.

                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2875)
                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2950)
                      at android.app.ActivityThread.-wrap11(Unknown Source:0)
                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1616)
                      at android.os.Handler.dispatchMessage(Handler.java:105)
                      at android.os.Looper.loop(Looper.java:164)
                      at android.app.ActivityThread.main(ActivityThread.java:6759)
                      at java.lang.reflect.Method.invoke(Native Method)
                      at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)
                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:770)
                   Caused by: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.tensorflow/contrib/lite/kernels/reduce.cc:107 current >= 0 && current < input_num_dims was not true.Node 8 failed to prepare.

                      at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)
                      at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:75)
                      at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:54)
                      at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:114)
                      at com.example.android.tflitecamerademo.ImageClassifier.<init>(ImageClassifier.java:92)
                      at com.example.android.tflitecamerademo.ImageClassifierQuantizedMobileNet.<init>(ImageClassifierQuantizedMobileNet.java:38)
                      at com.example.android.tflitecamerademo.Camera2BasicFragment.onActivityCreated(Camera2BasicFragment.java:332)
                      at android.app.Fragment.performActivityCreated(Fragment.java:2620)
                      at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1296)
                      at android.app.FragmentManagerImpl.addAddedFragments(FragmentManager.java:2415)
                      at android.app.FragmentManagerImpl.executeOpsTogether(FragmentManager.java:2194)
                      at android.app.FragmentManagerImpl.removeRedundantOperationsAndExecute(FragmentManager.java:2148)
                      at android.app.FragmentManagerImpl.execPendingActions(FragmentManager.java:2049)
                      at android.app.FragmentManagerImpl.dispatchMoveToState(FragmentManager.java:3044)
                      at android.app.FragmentManagerImpl.dispatchActivityCreated(FragmentManager.java:2991)
                      at android.app.FragmentController.dispatchActivityCreated(FragmentController.java:178)
                      at android.app.Activity.performCreateCommon(Activity.java:6974)
                      at android.app.Activity.performCreate(Activity.java:6982)
                      at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1214)
                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2828)
                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2950)Â 
                      at android.app.ActivityThread.-wrap11(Unknown Source:0)Â 
                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1616)Â 
                      at android.os.Handler.dispatchMessage(Handler.java:105)Â 
                      at android.os.Looper.loop(Looper.java:164)Â 
                      at android.app.ActivityThread.main(ActivityThread.java:6759)Â 
                      at java.lang.reflect.Method.invoke(Native Method)Â 
                      at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)Â 
                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:770)Â 
Application terminated.
"
20871,TFLite -- Converting the trained pb file into tflite failed predictions on android,"**Question descriptionï¼š**
####  training a pb file, and then convert to tflite file, i use python testing pb file is correct ,but after into tflite file on android testing is wrong, I think this problem should be related to BN(tf.nn.batch_normalization),because when i remove BN get results of android and python are all the same, but with BN is different, the BN is tflite support,and the input data are the same, I don't know why?
------------------------------------

**Test Demo:**
```
def save_to_pb(sess):
    constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['out'])
    with tf.gfile.FastGFile(MODEL_DIR + 'expert-graph.pb', mode='wb') as f:
        f.write(constant_graph.SerializeToString())

def read_data(session):
    image_name = '182133.jpg'
    TEST_IMAGINE_PATH = '/home/leve/lcr/ClelebA/Celebra_crop_20w+_w128_dataset/images/test_100/'
    image = cv.imread(TEST_IMAGINE_PATH + image_name)
    image = tf.reshape(image, [128, 128, 3])
    image = image.eval(session=session)
    image = image[np.newaxis, :]
    return image

def batch_norm_lite(x, train=True, bn_decay=0.5,epsilon = 0.001,name='bn'):
    is_training = tf.convert_to_tensor(train,dtype='bool',name='is_training')
    x_shape = x.get_shape()
    params_shape = x_shape[-1:]
    axis = list(range(len(x_shape) - 1))
    beta = tf.get_variable(name+'_beta', params_shape, initializer=tf.zeros_initializer())
    gamma = tf.get_variable(name+'_gamma', params_shape, initializer=tf.ones_initializer())
    moving_mean = tf.get_variable(name+'_moving_mean', params_shape, initializer=tf.zeros_initializer(), trainable=False)
    moving_variance = tf.get_variable(name+'_moving_variance', params_shape, initializer=tf.ones_initializer(), trainable=False)
    mean, variance = tf.nn.moments(x, axis)
    update_moving_mean = moving_averages.assign_moving_average(moving_mean, mean, bn_decay)
    update_moving_variance = moving_averages.assign_moving_average(moving_variance, variance, bn_decay)
    tf.add_to_collection(name+'_update_moving_mean', update_moving_mean)
    tf.add_to_collection(name+'_update_moving_variance', update_moving_variance)
    mean, variance = control_flow_ops.cond(
        is_training, lambda: (mean, variance),
        lambda: (moving_mean, moving_variance))

    return tf.nn.batch_normalization(x, mean, variance, beta, gamma, epsilon,name=name)

def train_model():
    img = tf.placeholder(name=""img"", dtype=tf.float32, shape=(1, 128, 128, 3))
    b = tf.Variable(tf.truncated_normal((1, 128, 128, 3), seed=1),name='w1')
    y_real = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])
    val = tf.add(img, b)
    val = batch_norm_lite(val)
    out = tf.identity(val, name=""out"")
    MSE = tf.reduce_mean(tf.square(y_real - out), name='mse')
    train_step = tf.train.GradientDescentOptimizer(0.9).minimize(MSE)
    saver = tf.train.Saver(max_to_keep=10)
    os.environ[""CUDA_VISIBLE_DEVICES""] = ''
    config = tf.ConfigProto()
    config.gpu_options.per_process_gpu_memory_fraction = 0.1
    config.gpu_options.allow_growth = True
    with tf.Session() as sess:
        sess.run(tf.initialize_all_variables())
        for epoch in range(100):
            sess.run(train_step,feed_dict={img:read_data(sess)})
            if epoch % 20 == 0:
                save_path = saver.save(sess, MODEL_DIR + MODEL_NAME, global_step=epoch + 1)
            print('step = ' + str(epoch))
        save_to_pb(sess)
train_model()
```
-------------------------

#### run upper  code and get pb file, when you test pb file with python you maybe get a result like this:
39.09046 45.927864 84.797905 45.952957 62.701195 68.00796 41.789146 80.99372 81.67459 81.2203....

#### then run this command to create tflite file:
```
bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/path/expert-graph.pb --output_file=/path/expert-graph.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,128,128,3 --input_array=img --output_array=out --inference_type=FLOAT --input_data_type=FLOAT --allow_custom_ops
```

#### you will get expert-graph.tflite,then use tflite file to android,will get result like this:
49.392387ï¼Œ 42.290344ï¼Œ135.30707ï¼Œ 7.7764254 ï¼Œ 125.98051ï¼Œ79.317825 ï¼Œ157.29184 ï¼Œ107.58522 ï¼Œ 283.46997 ï¼Œ36.103508....

#### Android and python have the same input data,but the result is different,if i remove BN and train, that's ok,but BN is very important,i can remove it ,i don't know how to solve it,this question has been around me for a long time,please help solve it, if don't use TF Lite, what should i do?Thanks
------------------------

### System information
Linux Ubuntu 16.04:
TensorFlow installed from source:
TensorFlow version 1.8.0:
Python version:3.6:
Bazel version 0.11.1:
GCC/Compiler version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) :
CUDA/cuDNN 9.0/7.0.5:
GPU 1060-6G:"
20870,[Question/Feature request] How to stack variable length tensors in a TensorArray?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs Sierra
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:no
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
How to stack variable length tensors in a TensorArray cleanly and efficiently?

I am implementing a form of RNN that produce variable length (time step dimension) tensors using tf.while_loop and tf.TensorArray to store the tensors. I need when `tf.while_loop` is done, all tensors are stacked into 1 tensor and the length dimension for each individual tensor is padded with 0 (or any constant) to the maximum length of the array.

Note that the time dimension is unknown at compile time

### Source code / logs
It would be something like this.

```python
import numpy as np
import tensorflow as tf

BATCH = 2
DIM = 4

TIME_X = 2 # unknown at compile time
TIME_Y = 4 # unknown at compile time

tensor_array = tf.TensorArray(size=2, infer_shape=False, dtype=x.dtype)
x = tf.random_uniform(shape=[BATCH, TIME_X, DIM], name='x')
# at this point, x is created but y is not created yet
output_ta = tensor_array.write(0, x)

# at this point, y is created
y = tf.random_uniform(shape=[BATCH, TIME_Y, DIM], name='y')
output_ta = output_ta.write(1, y)

# the maximum time dimension shape is unknown when the tensor is written to the TensorArray
# meaning that we can't pad each individual tensor beforehand.

output = output_ta.stack()

with tf.Session() as sess:
    print(sess.run(output)) # this will raise exception as shape is not compatible

# expected output shape: [2, BATCH, max(TIME_X, TIME_Y), DIM] = [2, 2, 4, 4]
```

Thank you"
20869,tf.contrib.lookup.HasTable,"
== cat /etc/issue ===============================================
Linux george-OMEN-by-HP-Laptop 4.10.0-38-generic #42~16.04.1-Ubuntu SMP Tue Oct 10 16:32:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""18.3 (Sylvia)""
VERSION_ID=""18.3""
VERSION_CODENAME=sylvia

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux george-OMEN-by-HP-Laptop 4.10.0-38-generic #42~16.04.1-Ubuntu SMP Tue Oct 10 16:32:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                              1.14.0        
numpydoc                           0.7.0         
protobuf                           3.5.2.post1   
tensorflow-gpu                     1.8.0         

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/home/george/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-9.1/lib64:/usr/local/cuda/lib64:/usr/local/cuda-9.1/lib64:/usr/local/cuda/extras/CUPTI/lib64/:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue Jul 17 09:00:20 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1050    Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   56C    P0    N/A /  N/A |    709MiB /  4038MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1439      G   /usr/lib/xorg/Xorg                           286MiB |
|    0      2503      G   cinnamon                                     160MiB |
|    0      2630      G   ...-token=2E210DC1F44A55A6DDE70E07757D319C    63MiB |
|    0      2865      G   /usr/lib/firefox/firefox                       3MiB |
|    0      2995      G   ...-token=E5432E224C69AA6F39ED672CDE9108B3    79MiB |
|    0      3500      G   /usr/lib/firefox/firefox                       1MiB |
|    0      3574      G   /usr/lib/firefox/firefox                       1MiB |
|    0      3632      G   /usr/lib/firefox/firefox                       1MiB |
|    0      7199      C   /home/george/anaconda3/bin/python             97MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart.so.9.1.85
/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.1/lib64/libcudart.so.9.1.85
/usr/local/cuda-9.1/lib64/libcudart_static.a
/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.1/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/lib64/libcudart_static.a
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7

Tensorflow 1.9

Please provide an oportunity to use tf.contrib.lookup.HashTables  
with key_dtype=tf.int32 and value_dtype=tf.string
Synthetic code:
table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(
    tf.constant([1, 2, 3], dtype=tf.int32), tf.constant(['a', 'b', 'c'], dtype=tf.string)
), '')
table.init.run() raises with error 
InvalidArgumentError: No OpKernel was registered to support Op 'HashTableV2' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_INT32]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_BOOL]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_STRING]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_FLOAT]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_INT64]
  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_STRING]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_INT64]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_INT32]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_FLOAT]
  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_DOUBLE]"
20868,Will cycle-gan have a Estimator implementation like tf.contrib.gan.estimator.GANEstimator?,"or can GanEstimator be used on cycle-gan for training/predicting?

Thanks."
20867,quantization deeplabv3(mobielentv2) error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.9.0
- **Python version**: 2.7.3
- **Bazel version (if compiling from source)**:0.12.0
- **GCC/Compiler version (if compiling from source)**:c++11
- **CUDA/cuDNN version**:7.5.18
- **GPU model and memory**:TITAN,12GB
- **Exact command to reproduce**:N/A


### Describe the problem
I want to train a quantization deeplabv3+(mobienetv2) model, use ""mobilenetv2_coco_voc_trainaug"" from https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md.


### Source code / logs
 I add `tf.contrib.quantize.create_training_graph(quant_delay=0)` in line 315 https://github.com/tensorflow/models/blob/master/research/deeplab/train.py
but I got the error like below:
```
INFO:tensorflow:Training on train set
Traceback (most recent call last):
  File ""deeplab/train.py"", line 359, in <module>
    tf.app.run()
  File ""/home/liufang/deeplab_venv/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""deeplab/train.py"", line 281, in main
    tf.contrib.quantize.create_training_graph(quant_delay=0)
  File ""/home/liufang/deeplab_venv/local/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/quantize_graph.py"", line 112, in create_training_graph
    freeze_bn_delay=freeze_bn_delay)
  File ""/home/liufang/deeplab_venv/local/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/quantize_graph.py"", line 66, in _create_graph
    is_training=is_training)
  File ""/home/liufang/deeplab_venv/local/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py"", line 54, in FoldBatchNorms
    graph, is_training, freeze_batch_norm_delay=freeze_batch_norm_delay)
  File ""/home/liufang/deeplab_venv/local/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py"", line 100, in _FoldFusedBatchNorms
    fused_batch_norm=True))
  File ""/home/liufang/deeplab_venv/local/lib/python2.7/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py"", line 323, in _ComputeBatchNormCorrections
    match.moving_variance_tensor + match.batch_epsilon)
TypeError: unsupported operand type(s) for +: 'NoneType' and 'float'
```


"
20866,"Why tf.losses.softmax_cross_entropy doesn't have ""dim"" (axis) argument?","Hi!
The function ""tf.losses.softmax_cross_entropy"" calls ""nn.softmax_cross_entropy_with_logits_v2"" which has a dim argument with a default value of -1.
Can we add the same ""dim"" or ""axis"" parameter to the calling function "" tf.losses.softmax_cross_entropy"", so that we can choose which axis to apply the softmax  ?
Thanks!"
20865,tensorflow 1.9 still asks for cuda 9.0 though i have 9.2,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10
- **TensorFlow installed from (source or binary)**: pip3
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.2
- **GPU model and memory**:
- **Exact command to reproduce**:

When importing tensorflow it fails and asks for cuda 9.0 dll
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""D:\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""D:\Python36\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit
"
20858,AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20857,Issue with tf.gradients() taking too long,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 2.7.12
- **CUDA/cuDNN version**: 7.5.17
- **GPU model and memory**: GeForce GTX 1080 Ti
- **Bazel version**: N/A
- **Exact command to reproduce**: tf.gradients()

### Describe the problem

So I am using tensorflow/models/object_detection with different networks (Currently using a SSD MobileNet network). I am trying to prune the network and I am using tf.gradients(total_loss, activation_tensor) on the activation tensors to see which filters need to be pruned. However, the issue I am currently facing is that each call to tf.gradients is costing me at least 3 seconds regardless if I am doing this on the GPU or CPU. Is this a known issue and is there a fix or a replacement for tf.gradients? 
"
20856, GPUBFCAllocator breaks Allocator API,"Tensorflow Allocator API has a guarantee that AllocateRaw() will honor the requested alignment. 
( https://github.com/tensorflow/tensorflow/blob/33af29b33f14cc74725ff081d50e6e59247ef546/tensorflow/core/framework/allocator.h#L82 )

However, GPUBFCAllocator disregards this parameter and thus breaks the guarantee given by the base class.

https://github.com/tensorflow/tensorflow/blob/33af29b33f14cc74725ff081d50e6e59247ef546/tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.h#L60"
20852,AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20848,TF slim support for model_pruning not working,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:na
- **GCC/Compiler version (if compiling from source)**:na
- **CUDA/cuDNN version**:9/7.1
- **GPU model and memory**:1080ti
- **Exact command to reproduce**:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/model_pruning/python/learning.py
```
from tensorflow.contrib.model_pruning.python import learning
from tensorflow.contrib.model_pruning.python import pruning
  # Load data and create the model:
  images, labels = LoadData(...)
  predictions = MyModel(images)
  # Define the loss:
  slim.losses.log_loss(predictions, labels)
  total_loss = slim.losses.get_total_loss()
  # Define the optimizer:
  optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate, FLAGS.momentum)
  # Create the train_op
  train_op = slim.learning.create_train_op(total_loss, optimizer)
  # Set up sparsity
  sparsity = pruning.setup_gradual_sparsity(self.global_step)
  # Create mask update op
  mask_update_op = pruning.add_mask_update_ip(sparsity)
  # Run training.
  learning.train(train_op,
                 my_log_dir,
                 mask_update_op)

```


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
AttributeError: module 'tensorflow.contrib.model_pruning.python.pruning' has no attribute 'setup_gradual_sparsity'

If this is not possible yet, can you please describe a way to use apply_mask from pruning instead ?
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20846,"Tensorflow inference on Android, can't find 'Iterator' op. No OpKernel was registered to support Op","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.8.0 (For training and for creating the .so and .jar files)
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: None, running using CPU
- **Android Studio**: 3.1.3
- **Android NDK**: 14b
- **Andorid SDK**: v28
- **Exact command to reproduce**:


### Describe the problem

I am trying to run a trained Tensorflow model on an Android device.  The model I am trying to run on the android mobile device uses an Iterator operation in the inference graph.

I am compiling from source using:
`bazel build -c opt --copt=-D__ANDROID_TYPES_FULL__ //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a`

17.2 MB (.so file)
I can load the graph and use the feed method from TensorFlowInferenceInterface, but whenever I try to make a prediction by using a run method from the  TensorFlowInferenceInterface.  I get the ""java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Iterator' with these attrs."" error.

I asked on Stackoverflow here https://stackoverflow.com/questions/51331474/tensorflow-inferences-on-android-cant-find-iterator-op-no-opkernel-was-regi but I believe this might be a better place to ask because this appears to be a bug.

I am compiling from source using the D__ANDROID_TYPES_FULL__ so I would expect it to be able to find the iterator operation, which is why I believe it's a bug.  

I am able to run this using a Python/Tensorflow script on my laptop, but am unable to make it run on an Android device.  I give the python script I use to run the model below.

 I tried attaching the frozen model that I run in the code, but I couldn't attach on Github because the file size was too big.  The model was made from using this code: https://github.com/tensorflow/nmt and saving the model when making an inference.
### Source code / logs

This the error I get in Android Studio
```
I/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded
    E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)
    I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference
    I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)
    I/TensorFlowInferenceInterface: Model load took 764ms, TensorFlow version: 1.8.0
    I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/laptop_frozen_graph_init_tables.pb'
    E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[batch_size, src_data], outputs:[index_to_string_Lookup]
    E/AndroidRuntime: FATAL EXCEPTION: Thread-5769
                      Process: com.example.student.projecttest, PID: 6805
                      java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Iterator' with these attrs.  Registered devices: [CPU], Registered kernels:
                        <no registered kernels>
    
                      	 [[Node: Iterator = Iterator[container=""infer"", output_shapes=[[?,?], [?]], output_types=[DT_INT32, DT_INT32], shared_name=""""]()]]
                          at org.tensorflow.Session.run(Native Method)
                          at org.tensorflow.Session.access$100(Session.java:48)
                          at org.tensorflow.Session$Runner.runHelper(Session.java:298)
                          at org.tensorflow.Session$Runner.run(Session.java:248)
                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)
                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)
                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:187)
                          at com.example.student.projecttest.MainActivity.translateToFrench(MainActivity.java:79)
                          at com.example.student.projecttest.MainActivity$1$1.run(MainActivity.java:42)
```


This is the code I use to run the model on my laptop
```
import tensorflow as tf

def load_graph(frozen_graph_filename):
    # We load the protobuf file from the disk and parse it to retrieve the 
    # unserialized graph_def
    with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    # Then, we import the graph_def into a new Graph and returns it 
    with tf.Graph().as_default() as graph:
        # The name var will prefix every op/nodes in your graph
        # Since we load everything in a new graph, this is not needed
        tf.import_graph_def(graph_def, name=""prefix"")
    return graph

tgt_eos = '</s>'
frozen_model_path =  'laptop_frozen_graph_init_tables.pb'
graph = load_graph(frozen_model_path)

with tf.Session(graph = graph) as sess:

  #Get input tensors
  src_file_placeholder = sess.graph.get_tensor_by_name(""prefix/src_data:0"")
  batch_size_placeholder = sess.graph.get_tensor_by_name(""prefix/batch_size:0"")
  
  
  #Initialize tables
  init_tables_operation = sess.graph.get_operation_by_name(""prefix/init_all_tables"")
  sess.run(init_tables_operation)
  
  #Get init iterator operation
  make_iterator_operation = sess.graph.get_operation_by_name(""prefix/MakeIterator"")
  
  
  src_data = ['Bonjour', 'tout', 'le', 'monde', 'c', '\'', 'est', 'vrai', tgt_eos]
  
  #Make_iterator has to be fed with src_data
  sess.run(make_iterator_operation, feed_dict={
            src_file_placeholder: src_data,
            batch_size_placeholder: 32
        })

  #Get output tensor
  output0 = graph.get_tensor_by_name(""prefix/index_to_string_Lookup:0"")
  
  #Iterate through input until we run out of input
  try:
    while True:
      txt_output = sess.run(output0)
      print(txt_output)
  except tf.errors.OutOfRangeError:
      print('Done inferencing')
```

This is the code I use in Android Studio which produces a very error to the one mentioned above.
```
        String modelName = ""laptop_frozen_graph_init_tables.pb"";
        String fullModelPath = ""file:///android_asset/"" + modelName;

        AssetManager assetManager = getAssets();

        TensorFlowInferenceInterface inferenceInterface = new TensorFlowInferenceInterface(assetManager, fullModelPath);
        Graph graph = inferenceInterface.graph();

        inferenceInterface.run(new String[]{""init_all_tables""});
```

"
20844,MonitoredTrainingSession is not handling the dummy_QueueRunner cleanly?,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Fedora Red Hat Enterprise Linux Server release 7.5 (Maipo)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA9.0/ CUDNN7.0
- **GPU model and memory**:  P100 16GB
- **Exact command to reproduce**:  bash run.sh

### Describe the problem
I am using the MonitoredTrainingSession to test a simple script (base on [this](https://github.com/tmulc18/Distributed-TensorFlow-Guide/blob/master/Synchronous-SGD/ssgd.py) of synchronized distributed training/multi_gpu training. When the training is finished, the session always throws out Exception (CancelledError) in thread QueueRunnerThread.  I suspect this is related to QueueRunner is not handled cleanly when sess.close() is called. 

### Source code / logs
`REPLICAS_TO_AGGREGATE = 1
def main():
    # Configure
    config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)
    config.gpu_options.allow_growth = True

    # Server Setup
    cluster = tf.train.ClusterSpec({
        'ps':['localhost:2222'],
        'worker':['localhost:2223']
        }) #allows this node know about all other nodes
    if FLAGS.job_name == 'ps': #checks if parameter server
        with tf.device('/job:ps/task:0/cpu:0'):
            server = tf.train.Server(cluster,
                            job_name=""ps"",
                            task_index=FLAGS.task_index,
                            config=config)
            server.join()
    else: #it must be a worker server
        is_chief = (FLAGS.task_index == 0) #checks if this is the chief node
        with tf.device('/gpu:%d' % (FLAGS.task_index)):
            server = tf.train.Server(cluster,
                    job_name=""worker"",
                    task_index=FLAGS.task_index,
                    config=config)
    
        # Graph
        # ps_device = ""/job:ps/task:0/cpu:0""
        worker_device = ""/job:worker/task:%d/gpu:%d"" % (FLAGS.task_index, FLAGS.task_index)
        with tf.device(tf.train.replica_device_setter(ps_tasks=1,
                       #ps_device=ps_device,
                       worker_device=worker_device)):

            a = tf.Variable(tf.constant(0.,shape=[2]),dtype=tf.float32)
            b = tf.Variable(tf.constant(0.,shape=[2]),dtype=tf.float32)
            c= a+b

            global_step = tf.Variable(0,dtype=tf.int32,trainable=False,name='global_step')
            target = tf.constant(100.,shape=[2],dtype=tf.float32)
            loss = tf.reduce_mean(tf.square(c-target))

            # create an optimizer then wrap it with SynceReplicasOptimizer
            optimizer = tf.train.GradientDescentOptimizer(.0001)
            optimizer1 = tf.train.SyncReplicasOptimizer(optimizer,
                     replicas_to_aggregate=REPLICAS_TO_AGGREGATE, total_num_replicas=REPLICAS_TO_AGGREGATE)
      
            opt = optimizer1.minimize(loss,global_step=global_step) # averages gradients
            #opt = optimizer1.minimize(REPLICAS_TO_AGGREGATE*loss,
            #                           global_step=global_step) # hackily sums gradients

        # Session
        sync_replicas_hook = optimizer1.make_session_run_hook(is_chief)
        stop_hook = tf.train.StopAtStepHook(last_step=10)
        hooks = [sync_replicas_hook,stop_hook]

        # Monitored Training Session
        with tf.train.MonitoredTrainingSession(master = server.target, 
              is_chief=is_chief,
              config=config,
              hooks=hooks,
              stop_grace_period_secs=10) as sess:

            print('Starting training on worker %d'%FLAGS.task_index)
            while not sess.should_stop():
                _,r,gs=sess.run([opt,c,global_step])
                print(r,'step: ',gs,'worker: ',FLAGS.task_index)
                # if is_chief: time.sleep(1)
                time.sleep(1)
            print('Done',FLAGS.task_index)
  
            time.sleep(10) #grace period to wait before closing session
            #sess.close() # if uncomment this, it will raise error 'Session is already closed' after the CanceledError 
        print('Session from worker %d closed cleanly'%FLAGS.task_index)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    # Flags for defining the tf.train.ClusterSpec
    parser.add_argument(
        ""--job_name"",
        type=str,
        default="""",
        help=""One of 'ps', 'worker'""
      )
    # Flags for defining the tf.train.Server
    parser.add_argument(
        ""--task_index"",
        type=int,
        default=0,
        help=""Index of task within the job""
      )
    FLAGS, unparsed = parser.parse_known_args()
    print(FLAGS.task_index)
    main()`



**************** I use the following script to run the code 
`#!/bin/bash -e
export CUDA_VISIBLE_DEVICES='' 
python text_multi_gpu.py --job_name=""ps"" --task_index=0 &
export CUDA_VISIBLE_DEVICES='0' 
python text_multi_gpu.py --job_name=""worker"" --task_index=0`



****************** Below is part of the log
`[0.15988803 0.15988803] step:  8 worker:  0
[0.17985605 0.17985605] step:  9 worker:  0
Done 0
Exception in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:
Traceback (most recent call last):
  File ""/usr/lib64/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib64/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 268, in _run
    coord.request_stop(e)
  File ""/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 213, in request_stop
    six.reraise(*sys.exc_info())
  File ""/home/python3_env/lib64/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 252, in _run
    enqueue_callable()
  File ""/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/client/session.py"", line 1249, in _single_operation_run
    self._call_tf_sessionrun(None, {}, [], target_list, None)
  File ""/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/client/session.py"", line 1420, in _call_tf_sessionrun
    status, run_metadata)
  File ""/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to ""Session::Close()"".

Session from worker 0 closed cleanly`"
20843,Performance decrease in TensorFlow 1.9 for large graphs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0/1.8.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: GTX 980 Ti, 6GB
- **Exact command to reproduce**:

```
import time

import tensorflow as tf

x = tf.zeros((1, 10))

for i in range(5000):
    x = tf.layers.dense(x, units=10)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    print(""a"")
    start = time.time()
    sess.run(x)
    print(time.time() - start)

    print(""b"")
    start = time.time()
    sess.run(x)
    print(time.time() - start)
```

### Describe the problem
On TensorFlow 1.9, this shows:
```
a
17.57940149307251
b
0.09075760841369629
```
On TensorFlow 1.8, this shows:
```
a
11.689541101455688
b
0.08776521682739258
```
So roughly a 50% increase in run time in TensorFlow 1.9.  Note that this only occurs on the first call to `sess.run` (""a"" rather than ""b""), so I suspect this is some change in the graph optimization step.  So, a couple questions:

1. What changed?  I can't find anything obviously related to this in the release notes.
2. Is this intended/expected (e.g., some new graph optimization steps were added that increase the initial time, but provide benefits in the long run)?  Or is it just a bug?
3. If it is intended, is there some way to optionally disable this new feature if we find that initial build time is dominating the run time?

"
20842,BUG: tf.keras.layers and tf.REUSE.AUTO fail to share parameters in 1.9.0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
v1.9.0-0-g25c197e023 1.9.0
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
run MWE under 1.9.0, which works fine for 1.8.0

### Describe the problem
I think a bug was introduced, where `tf.keras.layers.Conv3D` in context of `tf.REUSE.AUTO` no longer works as intended (.e.g no parameter-sharing is performed).
This might be caused by the bugfix mentioned in the [release notes](https://github.com/tensorflow/tensorflow/releases/tag/v1.9.0) (Using tf.keras.layers with custom variable scopes).

This bug seemingly does not affect the `tf.keras.layers.Lambda` layer, as outlined by the MWE, where `tf.contrib.layers.instance_norm` and `tf.layers.conv3d` share paramters as intended.

### Source code / logs
MWE which runs fine under 1.8.0 but fails to yield complete parameter sharing under 1.9.0.

```
import tensorflow as tf

Conv3D = tf.keras.layers.Conv3D
Lambda = tf.keras.layers.Lambda
Activation = tf.keras.layers.Activation
tf_instance_norm = tf.contrib.layers.instance_norm    

def Conv_block_keras(input_layer, n_filters, kernel=(3, 3, 3), 
               padding='same', strides=(1, 1, 1)):

    with tf.variable_scope('conv_block'):
        #layer = Conv3D(n_filters, kernel, padding=padding, strides=strides)(input_layer) # does not work
        #layer = Lambda(lambda x: Conv3D(n_filters, kernel, padding=padding, strides=strides)(x))(input_layer) # does not work
        layer = Lambda(lambda x: tf.layers.conv3d(inputs=x, filters=n_filters, padding=padding, strides=strides, kernel_size=kernel))(input_layer)
        layer = Lambda(lambda x: tf_instance_norm(inputs=x, data_format='NCHW'))(layer)
        return Activation('relu')(layer)

with tf.variable_scope('reuse', reuse=tf.AUTO_REUSE):
    conv_1 = Conv_block_keras(tf.zeros(shape=[1,1]+[128]*3), 3)
    vars_conv_1 = [x.name for x in tf.global_variables(scope='reuse')]
    conv_2 = Conv_block_keras(tf.zeros(shape=[1,1]+[128]*3), 3)
    vars_conv_2 = [x.name for x in tf.global_variables(scope='reuse')]
    print(vars_conv_1 == vars_conv_2)
```
"
20841,"toco tensorflow lite conversion fails with ParseFromStringEitherTextOrBinary(input_file_contents, tf_graph.get())","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.7.1-2-g9156fcc7a8 1.7.1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: ---
- **GCC/Compiler version (if compiling from source)**: ---
- **CUDA/cuDNN version**: 9.0 / 7.1
- **GPU model and memory**: NVIDIA GeForce GTX 1080 Ti
- **Exact command to reproduce**: make

I am trying to build a tensorflow model and export it as an tensorflow lite model using **toco**. It crashes with the issue below. I actually tried to run it on a far larger model, and build this just to reproduce it. 

I put the script into a zip file [simple_export_test.zip], if you want to run it quickly, using make.
The exported model on my system looks like this: [exported.zip]

[exported.zip]: https://github.com/tensorflow/tensorflow/files/2197981/exported.zip


[simple_export_test.zip]: https://github.com/tensorflow/tensorflow/files/2197923/simple_export_test.zip

_simple_model_export.py_

```python
import tensorflow as tf
import numpy as np
import os
script_path = os.path.dirname(os.path.abspath(__file__))

x = tf.placeholder(tf.float32, shape=[2])
y = tf.Variable([2., 3.])
z = tf.pow(x, y)

export_dir = script_path + '/export'

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    sess.run(y, feed_dict={x: np.array([5, 6])})
    tf.saved_model.simple_save(session=sess, export_dir=export_dir, inputs={'x': x}, outputs={'y': y})
```
_Makefile_
```make
.PHONY: all standard_export tflite_export
  
all: standard_export tflite_export

standard_export:
    python simple_model_export.py

tflite_export:
    toco \
        --input_file=export/saved_model.pb \
        --output_file=export/optimized_graph.lite \
        --input_format=TENSORFLOW_GRAPHDEF \
        --output_format=TFLITE \
        --input_shape=2 \
        --input_array=x \
        --output_array=y \
        --inference_type=FLOAT \
        --input_data_type=FLOAT
```

Executing it leads to:

```log
[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:1: Invalid control characters encountered in text.
[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:4: Interpreting non ascii codepoint 188.
[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/text_format.cc:288] Error parsing text-format tensorflow.GraphDef: 1:4: Expected identifier, got:
2018-07-16 15:58:04.116409: F tensorflow/contrib/lite/toco/import_tensorflow.cc:2189] Check failed: ParseFromStringEitherTextOrBinary(input_file_contents, tf_graph.get())
Makefile:9: recipe for target 'tflite_export' failed
```

"
20839,[feature request] differentiate through eager_py_func,"According to the [`tf.contrib.eager.py_func`](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/py_func) api doc,

> tf.contrib.eager.py_func is not differentiable, though a gradient may be implemented in the future; if you would like to differentiate through it, please file an issue on Github.

so I open an issue here."
20838,[Feature request] Extending set of canned estimators,"The `tensorflow.contrib.learn` module was deprecated and some estimators were moved to `tensorflow.estimator`. There are the algorithms (SVM, k-means, Random Forest) which were not covered by the new estimators API.

I'm wondering is there a chance that remaining estimators from deprecated `contrib.learn` module will be moved to `tensorflow.estimator`? Any plans to extend the current set of estimators with new algorithms?"
20837,Does the order of Java api runner.feed(#1).feed(#2).feed(#3)... matters?,"Hi, guys, I have a quesion about the Tensorflow Java api.
When I feed the data to the model, the feed order matters ?
For example, runner.feed(#1).feed(#2).feed(#3)... is equal to runner.feed(#2).feed(#1).feed(#3) ? 
Besides, recently, I run some response generation work from github(https://github.com/tuxchow/ecm), which use the old seq2seq code (implement with buckets), and i use the python to train the model, java to predict, as a result, I feed the test sentence word by word into the model, e.x. runner.feed(word1).feed(word2).feed(word3)... , However, I found that except the first word's logits is correct,  from the second word it's wrong, why is that?

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows10
- **TensorFlow installed from (source or binary)**:  binary
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:None"
20836,tensorflow binary was not compile to use AVX2,"I have a problem. What should I do?

![adsiz](https://user-images.githubusercontent.com/41289364/42754121-ede8db66-88fb-11e8-8ece-d31cee69f63b.png

"
20835,Get corrupted records file's name,"### System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora Red Hat Enterprise Linux Server release 7.5 (Maipo)
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.3.0
Python version: 2.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA9.0/ CUDNN7.0
GPU model and memory: P40 16GB
Exact command to reproduce: bash run.sh

### Describe the problem
Now, I get many record files from raw data .  But when i train a model with records data,  i get : DataLossError(see above for traeback): corrupted record at 3289898.     Is a record file data is inlegalï¼Ÿ How to get the file name? with iterator?  thanks!


"
20834,SyncReplicasOptimizer + MonitoredTrainingSession go through network many times with one session.run,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 7.0
- **GPU model and memory**: 16G
- **Exact command to reproduce**: None

### Describe the problem
training model by `tf.train.MonitoredTrainingSession` combined with `tf.train.SyncReplicasOptimizer`,
I find the model network will be run twice with only once `session.run` calling, and many NaN values will be raised. More, this will update the batch normalization parameters.
I do the initialization by `tf.train.SyncReplicasOptimizer.make_session_run_hook`. By the way, code will be right without `tf.train.SyncReplicasOptimizer`.


### Source code / logs
```
# 1. model define
input = tf.placeholder_with_default()
phase = tf.placeholder_with_default(True, shape=(), name='phase')
... network define ...
self.loss = xxx

# 2. optimizer define
self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)
self.optimizer = tf.train.SyncReplicasOptimizer(self.optimizer, replicas_to_aggregate=worker_num, total_num_replicas=worker_numï¼‰
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
     self.train_op = self.optimizer.minimize(self.loss, global_step=self.global_step)

# 3. hook define
sync_replicas_hook = self.optimizer.make_session_run_hook((flags.task_index == 0), num_tokens=0)

# 4. train
with tf.train.MonitoredTrainingSession(master=server.target, hooks=[sync_replicas_hook], is_chief=is_chief):
    outpus = sess.run([self.train_op, global_step], feed_dict={'phase:0':True, ...})
```

Problem A:
I add `tf.Print` line when define in network, I find this line executing twice with once sess.run calling.

Problem B: 
when `phase = True`, the BatchNorm Parameter update right, but when `phase = False`, `moving_mean` and `moving_variance` turn to NaN, so does the loss.

Everything is OK when training with async optimizer."
20833,Coordinator stopped with threads still running,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: NV-P40
- **Exact command to reproduce**: N/A

### Describe the problem
Running distributed tensorflow using estimator in sync mode, there is always a exception after the last training step, as followed:
```
INFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany
Exception in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:
Traceback (most recent call last):
  File ""/usr/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py"", line 268, in _run
    coord.request_stop(e)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py"", line 213, in request_stop
    six.reraise(*sys.exc_info())
  File ""/usr/local/lib/python3.5/dist-packages/six.py"", line 693, in reraise
    raise value
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py"", line 252, in _run
    enqueue_callable()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1244, in _single_operation_run
    self._call_tf_sessionrun(None, {}, [], target_list, None)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to `Session::Close()`.
``` 

It seems there is a closing problem when using tf.train.SyncReplicasOptimizer.
Any one know how to fix this?  

### Source code / logs
Main Code Fragment(model_fn) as followed:
```
with tf.device('/job:worker/task:%d' % task_index):
    with slim.arg_scope([slim.variables.variable, slim.variables.global_step],
                 device=slim.variables.VariableDeviceChooser(num_parameter_servers)):
             global_step = slim.variables.global_step()
             # Calculate the learning rate schedule.
             num_batches_per_epoch = (num_examples_per_epoch / FLAGS.batch_size)
             decay_steps = int(num_batches_per_epoch * FLAGS.num_epochs_per_decay)

             # Decay the learning rate exponentially based on the number of steps.
             lr = tf.train.exponential_decay(FLAGS.initial_learning_rate,
                                             global_step,
                                             decay_steps,
                                             FLAGS.learning_rate_decay_factor,
                                             staircase=True)
             opt = tf.train.RMSPropOptimizer(learning_rate=lr,
                                                 decay=RMSPROP_DECAY,
                                                 momentum=RMSPROP_MOMENTUM,
                                                 epsilon=RMSPROP_EPSILON)
            # forward
             logits = inception.inference(features, num_classes, for_training=True)

             # loss
             inception.loss(logits, labels)
             losses = tf.get_collection(slim.losses.LOSSES_COLLECTION)
             losses += tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)

             total_loss = tf.add_n(losses, name='total_loss')

              # Create synchronous replica optimizer.
             opt = tf.train.SyncReplicasOptimizer(
                     opt,
                     replicas_to_aggregate=num_replicas_to_aggregate,
                     total_num_replicas=num_workers)

             # Compute gradients with respect to the loss.
             grads = opt.compute_gradients(total_loss)
             apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)
             sync_replicas_hook = opt.make_session_run_hook(is_chief, num_tokens=num_workers)
             
              return tf.estimator.EstimatorSpec(
                 mode=mode,
                 loss=total_loss,
                 train_op=train_op,
                 training_chief_hooks=[],
                 training_hooks=[sync_replicas_hook])
```
--------------------
And some  logs:
```
INFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=2; total_num_replicas=2
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:tokens_needed = 0
INFO:tensorflow:Graph was finalized.
2018-07-16 09:13:43.014523: I tensorflow/core/distributed_runtime/master_session.cc:1142] Start master session a13a7f78ee358c84 with config: allow_soft_placement: true
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:loss = 13.082163, step = 0
INFO:tensorflow:Saving checkpoints for 1 into /tmp/aves/train_dir/model.ckpt.
INFO:tensorflow:loss = 13.082163
INFO:tensorflow:loss = 13.115122 (15.557 sec)
INFO:tensorflow:Saving checkpoints for 2 into /tmp/aves/train_dir/model.ckpt.
INFO:tensorflow:loss = 13.103788 (7.314 sec)
INFO:tensorflow:global_step/sec: 0.043722
INFO:tensorflow:Saving checkpoints for 3 into /tmp/aves/train_dir/model.ckpt.
INFO:tensorflow:loss = 13.137239 (6.563 sec)
INFO:tensorflow:global_step/sec: 0.152378
INFO:tensorflow:Saving checkpoints for 4 into /tmp/aves/train_dir/model.ckpt.
INFO:tensorflow:loss = 13.200961 (6.763 sec)
INFO:tensorflow:global_step/sec: 0.147865
INFO:tensorflow:Saving checkpoints for 5 into /tmp/aves/train_dir/model.ckpt.
INFO:tensorflow:loss = 13.265466 (6.318 sec)
INFO:tensorflow:global_step/sec: 0.158291
INFO:tensorflow:Saving checkpoints for 6 into /tmp/aves/train_dir/model.ckpt.
INFO:tensorflow:loss = 13.165977 (6.270 sec)
INFO:tensorflow:global_step/sec: 0.159476
INFO:tensorflow:Saving checkpoints for 7 into /tmp/aves/train_dir/model.ckpt.
INFO:tensorflow:loss = 13.478054 (7.017 sec)
INFO:tensorflow:global_step/sec: 0.14252
INFO:tensorflow:Saving checkpoints for 8 into /tmp/aves/train_dir/model.ckpt.
WARNING:tensorflow:Ignoring: /tmp/aves/train_dir/model.ckpt-3.meta; No such file or directory
INFO:tensorflow:loss = 13.231509 (6.544 sec)
INFO:tensorflow:global_step/sec: 0.152785
INFO:tensorflow:Saving checkpoints for 9 into /tmp/aves/train_dir/model.ckpt.
INFO:tensorflow:loss = 13.367797 (6.686 sec)
INFO:tensorflow:global_step/sec: 0.149593
INFO:tensorflow:Saving checkpoints for 10 into /tmp/aves/train_dir/model.ckpt.
INFO:tensorflow:loss = 13.461123 (6.659 sec)
INFO:tensorflow:global_step/sec: 0.150159
INFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany
Exception in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:
Traceback (most recent call last):
  File ""/usr/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py"", line 268, in _run
    coord.request_stop(e)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py"", line 213, in request_stop
    six.reraise(*sys.exc_info())
  File ""/usr/local/lib/python3.5/dist-packages/six.py"", line 693, in reraise
    raise value
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py"", line 252, in _run
    enqueue_callable()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1244, in _single_operation_run
    self._call_tf_sessionrun(None, {}, [], target_list, None)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to `Session::Close()`.

INFO:tensorflow:Loss for final step: 13.461123.
```"
20832,'tflite_convert' is not recognized as an internal or external command ,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: none
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.9
- **Python version**:  3.5.1-32 bit
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: nothing since i have AMD radeon
- **GPU model and memory**:
- **Exact command to reproduce**:
```
tflite_convert \
  --output_file=/saved_model/maonani.tflite \
  --saved_model_dir=/saved_model/saved_model
```

### Describe the problem
 when i enter the command on cmd i get a response saying:

**'tflite_convert'** is not recognized as an internal or external command,
operable program or batch file.

i dont know why im getting this since i already have cloned the git repository of tensorflow

### Source code / logs

C:\Users\LENOVO-PC\tensorflow> tflite_convert \ --output_file=/saved_model/maonani.tflite \ --saved_model_dir=/saved_model/saved_model
'tflite_convert' is not recognized as an internal or external command,
operable program or batch file.
"
20831,[Feature request] Allow `tf.estimator.train_and_evaluate` to support chief and evaluator working with different `model_dir`,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Cent OS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
Currently `tf.estimator.train_and_evaluate` makes it easy to use an Estimator to perform both training and evaluation, possibly in a distributed environment. 

 In my case, `evaluator` have to 

  (1) read from chief worker's  `model_dir`,
  (2) do some extra processing,
  (3) generate **new model_dir** , 
  (4) finally concrete evaluation is based on the **new model_dir**. 

However, `train_and_evaluate` only supports one estimator **with single `model_dir`**,  i.e. `chief worker` and `evaluator` shares the same `checkpoint path`, which makes above requirements imcompatible. It would be ideal if we could perhaps pass more than one `model_dir` to train_and_evaluate to handle the above scenes.

Or maybe there are some workarounds to deal with this problem?"
20830,Performance Tensorflow Lite with NDK 17,"Hi , After moving to NDK 17 , our internal timers showing ~30-40% degradation in Tensorflow Lite  performace .

Any suggestion why clang will create such difference?

Thanks."
20829,feeding back curr state to next state causing dynamic_rnn to crash if batch size is varying,"Im feeding data to the graph with tf.train.batch (dynamic_pad=True,allow_smaller_final_batch=True).
also im feeding in the final state returned by tf.train.dynamic_rnn as the next state during the training process. (initial state configured to zeros)

it was working fine, however when end of epoch is reached, due to allow_smaller_final_batch=True above, it is trying to fetch smaller batch sizes. this seems to crash the program as the last state is of standard batch size(say 100) and current batch is lesser than standard batch size (eg: 90)

if i dont feed the last state, things are working fine as the state always initialized to zeros based on the current batch size. I could not find proper explanation for dynamic batch sizes (pls not not im not referring to variable sequence lenghts here)  assoiated with state feedback to ascertain if this is a bug or a coding issue

kindly help"
20828,Could not find com.androidx.test.espresso:espresso-core:2.2.2,"When I tried to build the project, I got this:
Error:A problem occurred configuring project ':app'.
> Could not resolve all dependencies for configuration ':app:_debugAndroidTestApkCopy'.
   > Could not find com.androidx.test.espresso:espresso-core:2.2.2.
     Required by:
         project :app

"
20827,Keras guide: input_shape needed when fitting example model using datasets (instead of numpy arrays),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux fedora 28
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

Below, I'm extracting code from the Keras guide (sequentially)

https://www.tensorflow.org/guide/keras

Here a model is being fit twice, to show you can use numpy or datasets. This works fine:

```
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential()
# Adds a densely-connected layer with 64 units to the model:
model.add(keras.layers.Dense(64, activation='relu'))
# Add another:
model.add(keras.layers.Dense(64, activation='relu'))
# Add a softmax layer with 10 output units:
model.add(keras.layers.Dense(10, activation='softmax'))

model.compile(optimizer=tf.train.AdamOptimizer(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

import numpy as np

data = np.random.random((1000, 32))
labels = np.random.random((1000, 10))

val_data = np.random.random((100, 32))
val_labels = np.random.random((100, 10))

model.fit(data, labels, epochs=10, batch_size=32)
          
dataset = tf.data.Dataset.from_tensor_slices((data, labels))
dataset
dataset = dataset.batch(32)
dataset = dataset.repeat()
dataset
# Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.
model.fit(dataset, epochs=10, steps_per_epoch=30)
```

However,  if you comment the first fit, the second will fail:

```
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential()
# Adds a densely-connected layer with 64 units to the model:
model.add(keras.layers.Dense(64, activation='relu'))
# Add another:
model.add(keras.layers.Dense(64, activation='relu'))
# Add a softmax layer with 10 output units:
model.add(keras.layers.Dense(10, activation='softmax'))

model.compile(optimizer=tf.train.AdamOptimizer(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

import numpy as np

data = np.random.random((1000, 32))
labels = np.random.random((1000, 10))

#model.fit(data, labels, epochs=10, batch_size=32)
          
dataset = tf.data.Dataset.from_tensor_slices((data, labels))
dataset
dataset = dataset.batch(32)
dataset = dataset.repeat()
dataset
# Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.
model.fit(dataset, epochs=10, steps_per_epoch=30)
```

with

```
  TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'.
```

When I add an `input_shape` to the model, the fit using datasets works fine standalone, too.

So if `input_shape` is required with datasets, the model in the guide should probably have it, so people can copy out and directly use the code?"
20826,TOCO (TensorFlow 1.9.0) -- 'TocoConverter' has no attribute 'from_keras_model_file',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04 on Windows 10 VirtualBox
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: [Git] 'v1.9.0-0-g25c197e023'; [Version] '1.9.0'
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: 
```
import tensorflow as tf

converter = tf.contrib.lite.TocoConverter.from_keras_model_file(""keras_model.h5"")
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I'm trying to convert a keras model to the TensorFlow Lite format via TOCO. For some reason, though, the keras conversion functions seem to be missing from the TocoConverter. I've also tried using the command line feature and the same thing happens when I use the command-line version of TOCO. Is there something wrong? I just followed the [Python API tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md) and the [command-line examples.](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md)
### Source code / logs
Traceback (most recent call last):
  File ""modelconv.py"", line 3, in <module>
    converter = tf.contrib.lite.TocoConverter.from_keras_model_file(""model.h5"")
AttributeError: type object 'TocoConverter' has no attribute 'from_keras_model_file'

"
20825,"TFLite --- TypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(""MobilenetV1/Conv2d_0/BatchNorm/beta:0"", shape=(32,), dtype=float32)","we can get mobilenet tflite&pb(Mobilenet_1.0_224) from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md

when I use the command:
bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/path/to/mobilenet_v1_1.0_224_frozen.pb --input_checkpoint=/path/to/mobilenet_v1_1.0_224.ckpt --output_graph=/path/to/freezed_mobilenet.pb --output_node_names=MobilenetV1/Predictions/Reshape_1 --input_binary=true

error occured:
/home/leve/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-07-16 11:04:48.613520: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-07-16 11:04:48.728926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-07-16 11:04:48.729264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7845
pciBusID: 0000:01:00.0
totalMemory: 5.93GiB freeMemory: 5.31GiB
2018-07-16 11:04:48.729276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-07-16 11:04:48.873004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-16 11:04:48.873030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-07-16 11:04:48.873035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-07-16 11:04:48.873192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5086 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 382, in <module>
    run_main()
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 379, in run_main
    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 378, in <lambda>
    my_main = lambda unused_args: main(unused_args, flags)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 272, in main
    flags.saved_model_tags, checkpoint_version)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 254, in freeze_graph
    checkpoint_version=checkpoint_version)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 128, in freeze_graph_with_def_protos
    var_list=var_list, write_version=checkpoint_version)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1338, in __init__
    self.build()
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1384, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 813, in _build_internal
    saveables = self._ValidateAndSliceInputs(names_to_saveables)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 714, in _ValidateAndSliceInputs
    variable)
**TypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(""MobilenetV1/Conv2d_0/BatchNorm/beta:0"", shape=(32,), dtype=float32)**

### System information
- **Linux Ubuntu 16.04**:
- **TensorFlow installed from source**:
- **TensorFlow version 1.8.0**:
- **Python version:3.6**: 
- **Bazel version 0.11.1**:
- **GCC/Compiler version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) **:
- **CUDA/cuDNN 9.0/7.0.5**:
- **GPU  1060-6G**:
- **Exact command to reproduce**:"
20824,"TF Lite ---- Not a variable: Tensor(""w1:0"", shape=(1, 128, 128, 3), dtype=float32)","**Environment:**
tensorflow 1.8,ubuntu16.0.4,cuda9.0,cudnn:7.0.5

**Test Demo:**

def save_to_pb(sess):
    constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['out'])
    with tf.gfile.FastGFile(MODEL_DIR + 'expert-graph.pb', mode='wb') as f:
        f.write(constant_graph.SerializeToString())

train_model():
    img = tf.placeholder(name=""img"", dtype=tf.float32, shape=(1, 128, 128, 3))
    b = tf.Variable(tf.truncated_normal((1, 128, 128, 3), seed=1),name='w1')
    y_real = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])

    val = tf.add(img, b)
    val = batch_norm_lite(val)
    out = tf.identity(val, name=""out"")

    MSE = tf.reduce_mean(tf.square(y_real - out), name='mse')
    train_step = tf.train.GradientDescentOptimizer(0.9).minimize(MSE)

    saver = tf.train.Saver(var_list={'w1':b},max_to_keep=10)

    os.environ[""CUDA_VISIBLE_DEVICES""] = ''
    config = tf.ConfigProto()
    config.gpu_options.per_process_gpu_memory_fraction = 0.1
    config.gpu_options.allow_growth = True

    with tf.Session() as sess:
        sess.run(tf.initialize_all_variables())
        for epoch in range(100):
            sess.run(train_step,feed_dict={img:read_data(sess)})
            if epoch % 20 == 0:
                save_path = saver.save(sess, MODEL_DIR + MODEL_NAME, global_step=epoch + 1)
            print('step = ' + str(epoch))
        save_to_pb(sess)

**Terminal:**
bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/path/to/expert-graph.pb --input_checkpoint=/path/to/model.ckpt --output_node_names=output --output_graph=/path/to/frozen.pb --input_binary=true

**Errorï¼š**
/home/leve/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-07-13 18:37:04.467417: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-07-13 18:37:04.579724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-07-13 18:37:04.580044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7845
pciBusID: 0000:01:00.0
totalMemory: 5.93GiB freeMemory: 5.29GiB
2018-07-13 18:37:04.580057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-07-13 18:37:04.747082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-13 18:37:04.747104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-07-13 18:37:04.747109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-07-13 18:37:04.747225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5066 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 382, in <module>
    run_main()
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 379, in run_main
    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 378, in <lambda>
    my_main = lambda unused_args: main(unused_args, flags)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 272, in main
    flags.saved_model_tags, checkpoint_version)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 254, in freeze_graph
    checkpoint_version=checkpoint_version)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 128, in freeze_graph_with_def_protos
    var_list=var_list, write_version=checkpoint_version)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1338, in __init__
    self.build()
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 1384, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 813, in _build_internal
    saveables = self._ValidateAndSliceInputs(names_to_saveables)
  File ""/home/leve/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/training/saver.py"", line 714, in _ValidateAndSliceInputs
    variable)
**TypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(""w1:0"", shape=(1, 128, 128, 3), dtype=float32)**
"
20823,Grappler warning and segfault,"### System information
- **Have I written custom code**: I used this TensorFlow [patch](https://gist.github.com/Willian-Zhang/a3bd10da2d8b343875f3862b2a62eb3b).
- **OS Platform and Distribution**: macOS 10.13.6 and 10.13.5 before
- **TensorFlow installed from**: Source
- **TensorFlow version**: ('v1.8.0-0-g93bc2e2072', '1.8.0')
- **Python**: 2.7
- **Bazel version**: 0.13.1
- **GCC/Compiler**: from Xcode 9.2
- **CUDA/cuDNN**: 9.1/7.0.5
- **GPU model and memory**: NVIDIA 1080 8.00GiB
- **Exact command to reproduce**: Cifar 10 training from examples


Hardware: 
MacBook Pro 13,3 eGPU 1080
NVIDIA Web Driver 387.10.10.10.40.105 
CUDA Driver 396.148 
CUDA 9.1 Toolkit 
cuDNN 7.0.5 
Python 2.7 
NCCL 2.1.15 
Xcode 9.2

import tensorflow as tf
tf.test.is_gpu_available():
`tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:859] OS X does not support NUMA - returning NUMA node zero
tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:46:00.0
totalMemory: 8.00GiB freeMemory: 2.32GiB
tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 2025 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:46:00.0, compute capability: 6.1)
True`

When I try to run something I get this error messages with segfault in the end:

`Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:859] OS X does not support NUMA - returning NUMA node zero
tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:46:00.0
totalMemory: 8.00GiB freeMemory: 3.39GiB
tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:46:00.0, compute capability: 6.1)
E tensorflow/core/grappler/clusters/utils.cc:127] Not found: TF GPU device with id 0 was not registered
Segmentation fault: 11`

In other program I tried to reduce per_process_gpu_memory_fraction and batch size so it crashed after first batch with the same error code. I have seen people encounter same grappler warning without segfault. 

Any idea how to resolve this? Any help appreciated!! Thank you!"
20819,Tensorflow 1.9: android build of 'libtensorflow_cc.so' fails using android-ndk-r14b & android-ndk-r12b,"I'm trying to build tensorflow 1.9 for android but build always fails due to different reasons... 

For android-ndk-r14b I've tried the following:

**bazel --output_user_root=/mnt/d/ai/.madman/bazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --cxxopt=""-std=c++11"" --cxxopt=""-DTENSORFLOW_DISABLE_META"" --jobs 1**

_But got this:_ 

```
INFO: Analysed target //tensorflow:libtensorflow_cc.so (0 packages loaded).
INFO: Found 1 target...
ERROR: /mnt/d/ai/.madman/tensorflow_r1.9/tensorflow/core/distributed_runtime/rpc/BUILD:160:1: C++ compilation of rule '//tensorflow/core/distributed_runtime/rpc:grpc_worker_service' failed (Exit 1)
In file included from tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc:20:
In file included from external/grpc/include/grpc++/alarm.h:26:
In file included from external/grpc/include/grpcpp/alarm.h:25:
In file included from external/grpc/include/grpcpp/impl/codegen/completion_queue.h:37:
In file included from external/grpc/include/grpcpp/impl/codegen/core_codegen_interface.h:25:
external/grpc/include/grpcpp/impl/codegen/config.h:37:12: error: no member named 'to_string' in namespace 'std'
using std::to_string;
      ~~~~~^
```

_... also tried to switch to llvm's STL like this:_ 

**bazel --output_user_root=/mnt/d/ai/.madman/bazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=@androidndk//:toolchain-libcpp --android_crosstool_top=@androidndk//:toolchain-libcpp --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --cxxopt=""-std=c++11"" --cxxopt=""-DTENSORFLOW_DISABLE_META"" --jobs 1**

_... and got another issue:_ 

```
WARNING: /mnt/d/ai/.madman/bazel/cd3928a854e918c00cc4e280f6815890/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /mnt/d/ai/.madman/bazel/cd3928a854e918c00cc4e280f6815890/external/grpc/bazel/grpc_build_system.bzl:172:12
INFO: Analysed target //tensorflow:libtensorflow_cc.so (0 packages loaded).
INFO: Found 1 target...
ERROR: /mnt/d/ai/.madman/tensorflow_r1.9/tensorflow/core/kernels/BUILD:340:1: C++ compilation of rule '//tensorflow/core/kernels:stage_op' failed (Exit 1)
In file included from tensorflow/core/kernels/stage_op.cc:22:
In file included from ./tensorflow/core/framework/op_kernel.h:23:
In file included from ./tensorflow/core/framework/allocator.h:23:
In file included from ./tensorflow/core/framework/numeric_types.h:19:
In file included from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/complex:246:
In file included from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cmath:305:
In file included from external/androidndk/ndk/sources/android/support/include/math.h:31:
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:661:91: error: use of undeclared identifier 'acosl'
inline _LIBCPP_INLINE_VISIBILITY long double acos(long double __lcpp_x) _NOEXCEPT {return acosl(__lcpp_x);}
                                                                                          ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:673:91: error: use of undeclared identifier 'asinl'
inline _LIBCPP_INLINE_VISIBILITY long double asin(long double __lcpp_x) _NOEXCEPT {return asinl(__lcpp_x);}
                                                                                          ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:685:91: error: use of undeclared identifier 'atanl'
inline _LIBCPP_INLINE_VISIBILITY long double atan(long double __lcpp_x) _NOEXCEPT {return atanl(__lcpp_x);}
                                                                                          ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:697:114: error: use of undeclared identifier 'atan2l'
inline _LIBCPP_INLINE_VISIBILITY long double atan2(long double __lcpp_y, long double __lcpp_x) _NOEXCEPT {return atan2l(__lcpp_y, __lcpp_x);}
                                                                                                                 ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:732:90: error: use of undeclared identifier 'cosl'
inline _LIBCPP_INLINE_VISIBILITY long double cos(long double __lcpp_x) _NOEXCEPT {return cosl(__lcpp_x);}
                                                                                         ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:744:91: error: use of undeclared identifier 'coshl'
inline _LIBCPP_INLINE_VISIBILITY long double cosh(long double __lcpp_x) _NOEXCEPT {return coshl(__lcpp_x);}
                                                                                          ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:756:90: error: use of undeclared identifier 'expl'
inline _LIBCPP_INLINE_VISIBILITY long double exp(long double __lcpp_x) _NOEXCEPT {return expl(__lcpp_x);}
                                                                                         ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:792:113: error: use of undeclared identifier 'fmodl'
inline _LIBCPP_INLINE_VISIBILITY long double fmod(long double __lcpp_x, long double __lcpp_y) _NOEXCEPT {return fmodl(__lcpp_x, __lcpp_y);}
                                                                                                                ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:839:90: error: use of undeclared identifier 'logl'
inline _LIBCPP_INLINE_VISIBILITY long double log(long double __lcpp_x) _NOEXCEPT {return logl(__lcpp_x);}
                                                                                         ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:851:92: error: use of undeclared identifier 'log10l'
inline _LIBCPP_INLINE_VISIBILITY long double log10(long double __lcpp_x) _NOEXCEPT {return log10l(__lcpp_x);}
                                                                                           ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:863:114: error: use of undeclared identifier 'modfl'
inline _LIBCPP_INLINE_VISIBILITY long double modf(long double __lcpp_x, long double* __lcpp_y) _NOEXCEPT {return modfl(__lcpp_x, __lcpp_y);}
                                                                                                                 ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:870:112: error: use of undeclared identifier 'powl'
inline _LIBCPP_INLINE_VISIBILITY long double pow(long double __lcpp_x, long double __lcpp_y) _NOEXCEPT {return powl(__lcpp_x, __lcpp_y);}
                                                                                                               ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:893:90: error: use of undeclared identifier 'sinl'
inline _LIBCPP_INLINE_VISIBILITY long double sin(long double __lcpp_x) _NOEXCEPT {return sinl(__lcpp_x);}
                                                                                         ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:905:91: error: use of undeclared identifier 'sinhl'
inline _LIBCPP_INLINE_VISIBILITY long double sinh(long double __lcpp_x) _NOEXCEPT {return sinhl(__lcpp_x);}
                                                                                          ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:917:91: error: use of undeclared identifier 'sqrtl'
inline _LIBCPP_INLINE_VISIBILITY long double sqrt(long double __lcpp_x) _NOEXCEPT {return sqrtl(__lcpp_x);}
                                                                                          ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:931:90: error: use of undeclared identifier 'tanl'
inline _LIBCPP_INLINE_VISIBILITY long double tan(long double __lcpp_x) _NOEXCEPT {return tanl(__lcpp_x);}
                                                                                         ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:943:91: error: use of undeclared identifier 'tanhl'
inline _LIBCPP_INLINE_VISIBILITY long double tanh(long double __lcpp_x) _NOEXCEPT {return tanhl(__lcpp_x);}
                                                                                          ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:955:92: error: use of undeclared identifier 'acoshl'
inline _LIBCPP_INLINE_VISIBILITY long double acosh(long double __lcpp_x) _NOEXCEPT {return acoshl(__lcpp_x);}
                                                                                           ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:967:92: error: use of undeclared identifier 'asinhl'
inline _LIBCPP_INLINE_VISIBILITY long double asinh(long double __lcpp_x) _NOEXCEPT {return asinhl(__lcpp_x);}
                                                                                           ^
fatal error: too many errors emitted, stopping now [-ferror-limit=]
20 errors generated.
Target //tensorflow:libtensorflow_cc.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 7.012s, Critical Path: 1.84s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```


So I've tried the same for android-ndk-r12b:

_first in this way:_ 

**bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a  --jobs 1**

_... but got another issue:_ 

```
ERROR: /mnt/d/ai/.madman/tensorflow_r1.9-gcc/tensorflow/core/BUILD:2891:1: C++ compilation of rule '//tensorflow/core:gpu_runtime_impl' failed (Exit 1)
tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc: In member function 'virtual void* tensorflow::GPUNanResetAllocator::AllocateRaw(size_t, size_t)':
tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:176:27: error: 'nanf' is not a member of 'std'
                           std::nanf(""""));
                           ^
tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:176:27: note: suggested alternative:
In file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:44:0,
                 from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/complex:44,
                 from ./tensorflow/core/framework/numeric_types.h:19,
                 from ./tensorflow/core/framework/allocator.h:23,
                 from ./tensorflow/core/common_runtime/visitable_allocator.h:20,
                 from ./tensorflow/core/common_runtime/gpu/gpu_debug_allocator.h:24,
                 from tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:16:
external/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:359:7: note:   'nanf'
 float nanf(const char *) __NDK_FPABI_MATH__ __pure2;
       ^
tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc: In member function 'virtual void tensorflow::GPUNanResetAllocator::DeallocateRaw(void*)':
tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:191:29: error: 'nanf' is not a member of 'std'
                             std::nanf(""""));
                             ^
tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:191:29: note: suggested alternative:
In file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:44:0,
                 from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/complex:44,
                 from ./tensorflow/core/framework/numeric_types.h:19,
                 from ./tensorflow/core/framework/allocator.h:23,
                 from ./tensorflow/core/common_runtime/visitable_allocator.h:20,
                 from ./tensorflow/core/common_runtime/gpu/gpu_debug_allocator.h:24,
                 from tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:16:
external/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:359:7: note:   'nanf'
 float nanf(const char *) __NDK_FPABI_MATH__ __pure2;
       ^
Target //tensorflow:libtensorflow_cc.so failed to build
```


_... then again tried to switch to llvm:_ 

**bazel build tensorflow/core:android_tensorflow_lib --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config monolithic**

_and again no luck:_ 

```
ERROR: /mnt/d/ai/.madman/tensorflow_r1.9-gcc/tensorflow/core/BUILD:2014:1: C++ compilation of rule '//tensorflow/core:lib_internal_impl' failed (Exit 1)
In file included from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libcxx/include/memory:614:0,
                 from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libcxx/include/functional:477,
                 from ./tensorflow/core/lib/core/threadpool.h:19,
                 from tensorflow/core/lib/core/threadpool.cc:16:
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libcxx/include/atomic: In member function 'void Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*)':
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libcxx/include/atomic:720:75: error: invalid failure memory model for '__atomic_compare_exchange'
                                    __gcc_atomic::__to_gcc_order(__failure));
                                                                           ^
Target //tensorflow:libtensorflow_cc.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 617.731s, Critical Path: 11.03s
INFO: 264 processes: 264 local.
FAILED: Build did NOT complete successfully

```
What am I doing wrong? 

P.S. '--jobs 1' seems to be necessary for my environment, otherwise I'm getting different random build issues."
20817,Java SavedModelBundle.load do not support long path in windows,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 64bit.
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
### Describe the problem
Java SavedModelBundle.load do not support long path in windows.
It will throw something like:
```
org.tensorflow.TensorFlowException: NewRandomAccessFile failed to Create/Open: C:\Projects\veryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryveryverylongpath/variables/variables.data-00000-of-00001 : The system cannot find the path specified.
; No such process
	 [[Node: save_1/RestoreV2 = RestoreV2[_output_shapes=[<unknown>, <unknown>, <unknown>, <unknown>, <unknown>, ..., <unknown>, <unknown>, <unknown>, <unknown>, <unknown>], dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]
	at org.tensorflow.SavedModelBundle.load(Native Method)
```
I shorten the model path, everything works fine.
I enabled long path based on this [doc](https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file). It still has this issue.

I know it's a limit of Windows OS. I just wandering is there any way to let the C++ part bypass this limit?
"
20816,Feature request: tfe.Variblae should support Python's float casting,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8.0 and 19.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
In short, `tfe.Variable` should support:

```
a = tfe.Variable(1.0)
float(a)
```

It is useful in my case, but to state my case it would be very convoluted. If contributors deem appropriate then plaese add a support to this."
20815,java.lang.RuntimeException: Unrecoverable error while evaluating node '@local_config_cc//:cc-compiler-darwin_x86_64',"## System Information

**OS VERSION:** OS X 0.13.6
**Bazel Verion:** release 0.15.0-homebrew
**CPU:** Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz
**GPU:**   Radeon Pro 555 Ram: 2048 MB
**tensorflow version:** 1.9.0
**CUDA/cuDNN version:** n/a
**Python version:** Python 2.7.15

#### Commands

```go get -d github.com/tensorflow/tensorflow/tensorflow/go ```

```
cd ${GOPATH}/src/github.com/tensorflow/tensorflow
./configure
bazel build --config opt //tensorflow:libtensorflow.so
```

Starting local Bazel server and connecting to it...
...........
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/util/tensor_bundle/BUILD:64:1: In rule 'tensor_bundle_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/util/tensor_bundle/BUILD:64:1: In rule 'tensor_bundle_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:40:1: In rule 'static_schedule_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:40:1: In rule 'static_schedule_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:40:1: In rule 'static_schedule_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:40:1: In rule 'static_schedule_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:75:1: In rule 'auto_parallel_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:75:1: In rule 'auto_parallel_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:75:1: In rule 'auto_parallel_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:75:1: In rule 'auto_parallel_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:112:1: In rule 'constant_folding_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:112:1: In rule 'constant_folding_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:156:1: In rule 'function_optimizer_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:156:1: In rule 'function_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:156:1: In rule 'function_optimizer_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:156:1: In rule 'function_optimizer_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:360:1: In rule 'model_pruner_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:360:1: In rule 'model_pruner_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:360:1: In rule 'model_pruner_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:360:1: In rule 'model_pruner_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:433:1: In rule 'memory_optimizer_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:433:1: In rule 'memory_optimizer_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:477:1: In rule 'layout_optimizer_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:477:1: In rule 'layout_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:477:1: In rule 'layout_optimizer_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:477:1: In rule 'layout_optimizer_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:536:1: In rule 'meta_optimizer_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:536:1: In rule 'meta_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:536:1: In rule 'meta_optimizer_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:536:1: In rule 'meta_optimizer_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:620:1: In rule 'loop_optimizer_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:620:1: In rule 'loop_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:620:1: In rule 'loop_optimizer_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:620:1: In rule 'loop_optimizer_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:658:1: In rule 'shape_optimizer_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:658:1: In rule 'shape_optimizer_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:693:1: In rule 'remapper_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:693:1: In rule 'remapper_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:693:1: In rule 'remapper_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:693:1: In rule 'remapper_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:723:1: In rule 'symbolic_shapes_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/grappler/optimizers/BUILD:723:1: In rule 'symbolic_shapes_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3231:1: In rule 'lib_random_random_distributions_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3231:1: In rule 'lib_random_random_distributions_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3356:1: In rule 'lib_jpeg_jpeg_mem_unittest', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3356:1: In rule 'lib_jpeg_jpeg_mem_unittest', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3369:1: In rule 'lib_strings_ordered_code_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3369:1: In rule 'lib_strings_ordered_code_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3381:1: In rule 'lib_random_weighted_picker_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3381:1: In rule 'lib_random_weighted_picker_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3405:1: In rule 'quantize_training_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3405:1: In rule 'quantize_training_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3596:1: In rule 'common_runtime_ring_reducer_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3596:1: In rule 'common_runtime_ring_reducer_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3806:1: In rule 'util_cuda_kernel_helper_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:3806:1: In rule 'util_cuda_kernel_helper_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:4235:1: In rule 'gpu_allocator_retry_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:4235:1: In rule 'gpu_allocator_retry_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:4258:1: In rule 'gpu_debug_allocator_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/BUILD:4258:1: In rule 'gpu_debug_allocator_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1086:1: In rule 'conv_ops_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1086:1: In rule 'conv_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1715:1: In rule 'scoped_allocator_ops_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1715:1: In rule 'scoped_allocator_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1715:1: In rule 'scoped_allocator_ops_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:1715:1: In rule 'scoped_allocator_ops_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'adjust_contrast_op_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'adjust_contrast_op_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'colorspace_op_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'colorspace_op_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'crop_and_resize_op_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'crop_and_resize_op_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'non_max_suppression_op_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'non_max_suppression_op_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_area_op_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_area_op_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_bicubic_op_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_bicubic_op_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_bilinear_op_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_bilinear_op_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_nearest_neighbor_op_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2278:1: In rule 'resize_nearest_neighbor_op_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2309:1: In rule 'adjust_contrast_op_benchmark_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2309:1: In rule 'adjust_contrast_op_benchmark_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2309:1: In rule 'adjust_contrast_op_benchmark_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2309:1: In rule 'adjust_contrast_op_benchmark_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2325:1: In rule 'resize_benchmark_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2325:1: In rule 'resize_benchmark_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2325:1: In rule 'resize_benchmark_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2325:1: In rule 'resize_benchmark_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3148:1: In rule 'immutable_constant_op_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3148:1: In rule 'immutable_constant_op_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3170:1: In rule 'shape_op_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3170:1: In rule 'shape_op_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3534:1: In rule 'lrn_op_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3534:1: In rule 'lrn_op_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3534:1: In rule 'lrn_op_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3534:1: In rule 'lrn_op_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3554:1: In rule 'xent_op_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3554:1: In rule 'xent_op_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3554:1: In rule 'xent_op_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3554:1: In rule 'xent_op_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3574:1: In rule 'nn_ops_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3574:1: In rule 'nn_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3574:1: In rule 'nn_ops_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3574:1: In rule 'nn_ops_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3720:1: In rule 'spacetobatch_benchmark_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3720:1: In rule 'spacetobatch_benchmark_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3720:1: In rule 'spacetobatch_benchmark_test_gpu', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3720:1: In rule 'spacetobatch_benchmark_test_gpu', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3812:1: In rule 'parse_tensor_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3812:1: In rule 'parse_tensor_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3913:1: In rule 'sendrecv_ops_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3913:1: In rule 'sendrecv_ops_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:4580:1: In rule 'spectrogram_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:4580:1: In rule 'spectrogram_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:5419:1: In rule 'quantization_utils_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:5419:1: In rule 'quantization_utils_test', timeout 'illegal' is not a valid timeout.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:5480:1: In rule 'quantized_activation_ops_test', size 'medium' is not a valid size.
ERROR: /Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:5480:1: In rule 'quantized_activation_ops_test', timeout 'illegal' is not a valid timeout.
Unhandled exception thrown during build; message: Unrecoverable error while evaluating node '@local_config_cc//:cc-compiler-darwin_x86_64 BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false' (requested by nodes '//tensorflow:libtensorflow.so BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '@bazel_tools//tools/cpp:malloc BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '@bazel_tools//tools/cpp:stl BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '//tensorflow:libtensorflow_framework.so BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false')
INFO: Elapsed time: 5,309s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (18 packages loaded)
java.lang.RuntimeException: Unrecoverable error while evaluating node '@local_config_cc//:cc-compiler-darwin_x86_64 BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false' (requested by nodes '//tensorflow:libtensorflow.so BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '@bazel_tools//tools/cpp:malloc BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '@bazel_tools//tools/cpp:stl BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '//tensorflow:libtensorflow_framework.so BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false')
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:460)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:355)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: No supported apple platform registered for target cpu ?os_x86_64
	at com.google.devtools.build.lib.rules.apple.ApplePlatform.forTargetCpu(ApplePlatform.java:165)
	at com.google.devtools.build.lib.rules.apple.ApplePlatform.forTarget(ApplePlatform.java:151)
	at com.google.devtools.build.lib.rules.apple.AppleConfiguration.getSingleArchPlatform(AppleConfiguration.java:260)
	at com.google.devtools.build.lib.rules.apple.cpp.AppleCcToolchain.addBuildVariables(AppleCcToolchain.java:70)
	at com.google.devtools.build.lib.rules.cpp.CcToolchain.getBuildVariables(CcToolchain.java:901)
	at com.google.devtools.build.lib.rules.cpp.CcToolchain.create(CcToolchain.java:596)
	at com.google.devtools.build.lib.rules.cpp.CcToolchain.create(CcToolchain.java:79)
	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:380)
	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:250)
	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:534)
	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:747)
	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:311)
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:382)
	... 4 more
java.lang.RuntimeException: Unrecoverable error while evaluating node '@local_config_cc//:cc-compiler-darwin_x86_64 BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false' (requested by nodes '//tensorflow:libtensorflow.so BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '@bazel_tools//tools/cpp:malloc BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '@bazel_tools//tools/cpp:stl BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false', '//tensorflow:libtensorflow_framework.so BuildConfigurationValue.Key[156382fd78241fc2dfb97724ea59dc0b] false')
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:460)
	at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:355)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: No supported apple platform registered for target cpu ?os_x86_64
	at com.google.devtools.build.lib.rules.apple.ApplePlatform.forTargetCpu(ApplePlatform.java:165)
	at com.google.devtools.build.lib.rules.apple.ApplePlatform.forTarget(ApplePlatform.java:151)
	at com.google.devtools.build.lib.rules.apple.AppleConfiguration.getSingleArchPlatform(AppleConfiguration.java:260)
	at com.google.devtools.build.lib.rules.apple.cpp.AppleCcToolchain.addBuildVariables(AppleCcToolchain.java:70)
	at com.google.devtools.build.lib.rules.cpp.CcToolchain.getBuildVariables(CcToolchain.java:901)
	at com.google.devtools.build.lib.rules.cpp.CcToolchain.create(CcToolchain.java:596)
	at com.google.devtools.build.lib.rules.cpp.CcToolchain.create(CcToolchain.java:79)
	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createRule(ConfiguredTargetFactory.java:380)
	at com.google.devtools.build.lib.analysis.ConfiguredTargetFactory.createConfiguredTarget(ConfiguredTargetFactory.java:250)
	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.createConfiguredTarget(SkyframeBuildView.java:534)
	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.createConfiguredTarget(ConfiguredTargetFunction.java:747)
	at com.google.devtools.build.lib.skyframe.ConfiguredTargetFunction.compute(ConfiguredTargetFunction.java:311)
	at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:382)
FAILED: Build did NOT complete successfully (18 packages loaded)"
20814, No module named 'datasets',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary by pip
- **TensorFlow version (use command below)**:1.2.0
- **Python version**:3.6 
- **Bazel version (if compiling from source)**:1.15.0
- **GCC/Compiler version (if compiling from source)**:5.5.0
- **CUDA/cuDNN version**:8.0
- **GPU model and memory**:5.5.10
- **Exact command to reproduce**:N/A

### Describe the problem
I want to run the command 'python mobilenet.py', but this error occurs to me. I followed some solutions, such as [link1](https://github.com/tensorflow/models/issues/3233) and [link2](https://stackoverflow.com/questions/39811840/how-can-jupyter-access-a-new-tensorflow-module-installed-in-the-right-path). However they don't work for me. I wonder if there is a way to solve it and how. Thank you!
"
20813,"cannot find package ""github.com/tensorflow/tensorflow/tensorflow/go""","Hi,
## System Information

#### os version: OS X 10.13.6

#### python version: Python 2.7.15

go get -d github.com/tensorflow/tensorflow/tree/master/tensorflow/go
```
package github.com/tensorflow/tensorflow/tree/master/tensorflow/go: cannot find package ""github.com/tensorflow/tensorflow/tree/master/tensorflow/go"" in any of:
	/usr/local/opt/go/libexec/src/github.com/tensorflow/tensorflow/tree/master/tensorflow/go (from $GOROOT)
	/Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tree/master/tensorflow/go (from $GOPATH)
```"
20812,Upgrade canned estimators for TensorFlow 2.0 (support for tf.optimizers).,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**:  3.5.4
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: 

`
# Code example of train method with tf.contrib.learn ( Which works )
    def get_stepw_decay_optimizer():
      def step_decay(global_step):
        return tf.train.piecewise_constant(global_step, boundaries=[10000,20000,50000,100000], values=[0.001,0.0005,0.0001,0.00005,0.00002])
                                         # use customized decay function in learning_rate
      return tf.train.AdamOptimizer(learning_rate=step_decay(tf.train.get_global_step()))#,epsilon=0.001)

    classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_cols,
                                          n_classes=2,
                                          hidden_units=[32,64,64,64,64,64,64,64,64,64,64,64,64,
                                                        64,64,64,64,64,32],
                                          dropout = 0.1,
                                          optimizer=get_stepw_decay_optimizer)

    classifier.fit(input_fn=get_input_fn(training_set), steps=125000)
`
`
# Same code example of train_and_evaluate with tf.estimator ( Which fails )
    def get_stepw_decay_optimizer():
      def step_decay(global_step):
        return tf.train.piecewise_constant(global_step, boundaries=[10000,20000,50000,100000], values=[0.001,0.0005,0.0001,0.00005,0.00002])
                                         # use customized decay function in learning_rate
      return tf.train.AdamOptimizer(learning_rate=step_decay(tf.train.get_global_step()))#,epsilon=0.001)

    estimator = tf.estimator.DNNClassifier(feature_columns=create_feature_cols(),
                                          n_classes=2,
                                          hidden_units=[32,64,64,64,64,64,
                                                        64,64,64,64,64,64,
                                                        64,64,64,64,64,64,
                                                        32],
                                          dropout = 0.1,
                                          optimizer=get_stepw_decay_optimizer)

    train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), 
                                      max_steps = num_train_steps)

    exp = tf.estimator.LatestExporter(""decision"", serving_fn)

    eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), 
                                    steps = None, 
                                    exporters = exp,
                                    start_delay_secs = 1, # start evaluating after N seconds, 
                                    throttle_secs = 40)  # evaluate every N seconds

    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
`

### Describe the problem
**While trying to implement learning rate decay in canned estimator, I encountered the below issue.**

When using the canned DNNClassifier from **tf.contrib.learn** it is possible to pass a function as input to parameter ""optimizer"" when using ""fit"" method but errors out when using ""train_and_evaluate"" of estimator from class **tf.estimator.DNNClassifier**, **The given object is not an Optimizer instance. Given: <function get_stepw_decay_optimizer at 0x000002076BACBF28>**. I believe, both the methods should work the same way and not raise any errors.

### Source code / logs
Error Logs
`INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: C:\Users\hrafiq\AppData\Local\Temp\tmpa1rpeahr
INFO:tensorflow:Using config: {'_session_config': None, '_log_step_count_steps': 100, '_is_chief': True, '_service': None, '_tf_random_seed': None, '_save_checkpoints_steps': None, '_save_summary_steps': 100, '_num_worker_replicas': 1, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_model_dir': 'C:\\Users\\hrafiq\\AppData\\Local\\Temp\\tmpa1rpeahr', '_num_ps_replicas': 0, '_task_type': 'worker', '_master': '', '_task_id': 0, '_save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002076BFCBC88>}
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 40 secs (eval_spec.throttle_secs) or training is finished.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-28-53806042763d> in <module>()
----> 1 train_and_evaluate(None, num_train_steps=200000)

<ipython-input-27-182baf3372f9> in train_and_evaluate(output_dir, num_train_steps)
     20                                     start_delay_secs = 1, # start evaluating after N seconds,
     21                                     throttle_secs = 40)  # evaluate every N seconds
---> 22     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\estimator\training.py in train_and_evaluate(estimator, train_spec, eval_spec)
    428       config.task_type != run_config_lib.TaskType.EVALUATOR):
    429     logging.info('Running training and evaluation locally (non-distributed).')
--> 430     executor.run_local()
    431     return
    432 

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\estimator\training.py in run_local(self)
    607           input_fn=self._train_spec.input_fn,
    608           max_steps=self._train_spec.max_steps,
--> 609           hooks=train_hooks)
    610 
    611       # Final export signal: For any eval result with global_step >= train

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\estimator\estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    300 
    301     saving_listeners = _check_listeners_type(saving_listeners)
--> 302     loss = self._train_model(input_fn, hooks, saving_listeners)
    303     logging.info('Loss for final step: %s.', loss)
    304     return self

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\estimator\estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
    709       with ops.control_dependencies([global_step_read_tensor]):
    710         estimator_spec = self._call_model_fn(
--> 711             features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
    712       # Check if the user created a loss summary, and add one if they didn't.
    713       # We assume here that the summary is called 'loss'. If it is not, we will

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\estimator\estimator.py in _call_model_fn(self, features, labels, mode, config)
    692     if 'config' in model_fn_args:
    693       kwargs['config'] = config
--> 694     model_fn_results = self._model_fn(features=features, **kwargs)
    695 
    696     if not isinstance(model_fn_results, model_fn_lib.EstimatorSpec):

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\estimator\canned\dnn.py in _model_fn(features, labels, mode, config)
    332           dropout=dropout,
    333           input_layer_partitioner=input_layer_partitioner,
--> 334           config=config)
    335     super(DNNClassifier, self).__init__(
    336         model_fn=_model_fn, model_dir=model_dir, config=config)

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\estimator\canned\dnn.py in _dnn_model_fn(features, labels, mode, head, hidden_units, feature_columns, optimizer, activation_fn, dropout, input_layer_partitioner, config)
    167                      'Given type: {}'.format(type(features)))
    168   optimizer = optimizers.get_optimizer_instance(
--> 169       optimizer, learning_rate=_LEARNING_RATE)
    170   num_ps_replicas = config.num_ps_replicas if config else 0
    171 

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\estimator\canned\optimizers.py in get_optimizer_instance(opt, learning_rate)
     75   if not isinstance(opt, optimizer_lib.Optimizer):
     76     raise ValueError(
---> 77         'The given object is not an Optimizer instance. Given: {}'.format(opt))
     78   return opt

ValueError: The given object is not an Optimizer instance. Given: <function get_stepw_decay_optimizer at 0x000002076BACBF28>
`
"
20811,MKL_DNN License File missing ,"Hello, 

It seems like since commit #20576 (specific PR : [here](https://github.com/tensorflow/tensorflow/commit/03ab64c3a68f3b990bf690ede06e3066ad4e35a0#diff-2f07a0589130c83666437024dd03d252)), building TF with MKL option forces the configure step to look for a LICENSE file inside the `third_party/mkl_dnn` folder, which actually doesn't exist. 

Currently, I'm using this hack in my Docker for the build to proceed without needing the explicit DNN LICENSE file (by copying the MKL license file into the MKL_DNN folder since they are identical licenses), but it'd be great if we could have this fixed sometime? I opened a small PR with this small fix (copied the LICENSE file from the original MKL-DNN repo in #20810 (identical to MKL license, but just for pedantic reasons)) so maybe this will help other users out as well.

## Environment Info:
Have I written custom code: No
OS Platform and Distribution: Tested on Debian:Stretch
TensorFlow installed from: Source
TensorFlow version:  1.9.0 (Master branch)
Bazel version: 0.15.0
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: 
```
d /opt && \
    git clone https://github.com/tensorflow/tensorflow.git && \
    cd /opt/tensorflow && \
    cp /opt/tensorflow/third_party/mkl/LICENSE /opt/tensorflow/third_party/mkl_dnn/LICENSE && \
    /bin/bash ./configure \
    && \
    bazel build --config=opt --config=mkl \
    tensorflow/tools/pip_package:build_pip_package
```

Thanks!

Nafis"
20806,tf.estimator.export_savedmodel is not exporting properly ,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

== cat /etc/issue ===============================================
Linux 309288d6d453 4.14.33+ #1 SMP Wed Jun 20 01:15:52 PDT 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""17.10 (Artful Aardvark)""
VERSION_ID=""17.10""
VERSION_CODENAME=artful

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 7.2.0-8ubuntu3.2) 7.2.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 309288d6d453 4.14.33+ #1 SMP Wed Jun 20 01:15:52 PDT 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                    1.14.5   
protobuf                 3.6.0    
tensorflow               1.9.0rc2 
tensorflow-hub           0.1.0    

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.9.0-rc2
tf.GIT_VERSION = unknown
tf.COMPILER_VERSION = unknown
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh.1: line 105: nvidia-smi: command not found

== cuda libs  ===================================================


You can obtain the TensorFlow version with

unknown 1.9.0-rc2

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

```
feature_columns = [tf.feature_column.numeric_column(""x"", shape=[28, 28])]

classifier = tf.estimator.DNNClassifier(
 feature_columns=feature_columns,
 hidden_units=[256, 32],
 optimizer=tf.train.AdamOptimizer(1e-4),
 n_classes=10,
 dropout=0.1,
 model_dir=""mnist_model"",
)

classifier.train(input_fn=train_input_fn, steps=10)

image = tf.placeholder(tf.float32, [None, 28*28])
model_path = classifier.export_savedmodel(""exported_fashion_minist"",
                          tf.estimator.export.build_raw_serving_input_receiver_fn({""x"":image}))
```
would not contain `serving_default` signature, and if i do `image = tf.placeholder(tf.string, [None, 28*28])`, there would be no way to convert the string to numberical_column, which means the feature column is not working

```
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Signatures INCLUDED in export for Classify: None
INFO:tensorflow:Signatures INCLUDED in export for Regress: None
INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']
INFO:tensorflow:Signatures INCLUDED in export for Train: None
INFO:tensorflow:Signatures INCLUDED in export for Eval: None
INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:
INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'x': <tf.Tensor 'Placeholder_3:0' shape=(?, 784) dtype=float32>}
INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'x': <tf.Tensor 'Placeholder_3:0' shape=(?, 784) dtype=float32>}
WARNING:tensorflow:Export includes no default signature!
INFO:tensorflow:Restoring parameters from mnist_model/model.ckpt-10
INFO:tensorflow:Assets added to graph.
INFO:tensorflow:No assets to write.
INFO:tensorflow:SavedModel written to: exported_fashion_minist/temp-b'1531602129'/saved_model.pb
b'exported_fashion_minist/1531602129'
```
"
20805,Eager execution and custom layer,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colaboratory
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**: 1.9.0-rc2
- **Python version**:  version 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: Colab GPU
- **Exact command to reproduce**: https://gist.github.com/nairouz/035a830d1e58a3759a6a1e193f5defed

### Describe the problem
I have a simple Keras model with one custom layer which works fine on the graph based execution. When I switched to eager execution via tf.enable_eager_execution(), I got stuck on a weird error.

### Source code 
    import numpy as np
    import tensorflow as tf
    import tensorflow.keras.backend as K
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Layer, Input
    from tensorflow.keras.losses import kullback_leibler_divergence

    tf.enable_eager_execution()

    class ClusteringLayer(Layer):
        def __init__(self, output_dim, input_dim=None, alpha=1.0, **kwargs):
            self.output_dim = output_dim
            self.input_dim = input_dim
            self.alpha = alpha
            super(ClusteringLayer, self).__init__(**kwargs)

        def build(self, input_shape):
            self.W = self.add_weight(name='kernel', shape=(self.output_dim, input_shape[1].value), initializer='Identity', trainable=True)
            super(ClusteringLayer, self).build(input_shape)

        def call(self, x, mask=None):
            q = 1.0/(1.0 + K.sqrt(K.sum(K.square(K.expand_dims(x, 1) - self.W), axis=2))**2 /self.alpha)
            q = q**((self.alpha+1.0)/2.0)
            q = K.transpose(K.transpose(q)/K.sum(q, axis=1))
            return q

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_dim)

    def clustering_loss(y_true, y_pred): 
        a = K.square(y_pred) / K.sum(y_pred, axis=0) 
        p = K.transpose(K.transpose(a) / K.sum(a, axis=1))
        loss = kullback_leibler_divergence(p, y_pred)
        return loss

    input1 = Input(shape=(10,), name=""input"")
    out = ClusteringLayer(output_dim = 5, name='clustering')(input1)
    model = Model(inputs=input1, outputs=out) 
    model.compile(optimizer=tf.train.AdamOptimizer(1e-3), loss={'clustering' : clustering_loss})
    np.random.seed(0)
    X = np.random.random((20, 10)).astype(np.float32)
    Y = np.random.random((20, 5)).astype(np.float32)
    model.fit(x={'input' : X}, y={'clustering' : Y}, batch_size=1, epochs=10)

###Logs
clustering
Epoch 1/10

---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-2-1f474aaabb09> in <module>()
     37 Y = np.random.random((20, 5))
     38 
---> 39 model.fit(x={'input' : X}, y={'clustering' : Y}, batch_size=1, epochs=10)
     40 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1331           initial_epoch=initial_epoch,
   1332           steps_per_epoch=steps_per_epoch,
-> 1333           validation_steps=validation_steps)
   1334     else:
   1335       return training_arrays.fit_loop(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in fit_loop(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
   1039             shuffle=shuffle,
   1040             num_train_samples=num_train_samples,
-> 1041             do_validation=do_validation)
   1042       callbacks.on_epoch_end(epoch, epoch_logs)
   1043       if callback_model.stop_training:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in batch_fit_loop(model, inputs, targets, epoch_logs, index_array, out_labels, callback_model, batch_size, sample_weights, val_inputs, val_targets, val_sample_weights, callbacks, shuffle, num_train_samples, do_validation)
    395         targets_batch,
    396         sample_weights=sample_weights_batch,
--> 397         training=True)
    398 
    399     if not isinstance(outs, list):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, sample_weights, training)
    787       outs, loss, loss_metrics = _model_loss(model, inputs, targets,
    788                                              sample_weights=sample_weights,
--> 789                                              training=training)
    790       if loss is None:
    791         raise ValueError('The model cannot be run '

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _model_loss(model, inputs, targets, sample_weights, training)
    126       outs = model.call(inputs[0], training=training)
    127     else:
--> 128       outs = model.call(inputs[0])
    129   else:
    130     if model._expects_training_arg:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in call(self, inputs, training, mask)
    718     outputs, _ = self._run_internal_graph(inputs,
    719                                           training=training,
--> 720                                           mask=masks)
    721     return outputs
    722 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)
    939     output_shapes = []
    940     for x in self.outputs:
--> 941       assert str(id(x)) in tensor_map, 'Could not compute output ' + str(x)
    942       tensor, mask = tensor_map[str(id(x))]
    943       output_shapes.append(backend.int_shape(x))

AssertionError: Could not compute output DeferredTensor('None', shape=(5,), dtype=float32)


"
20804,Error converting saved_model.pb to .tflite,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.5.1-32 bit
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: nothing since i have AMD radeon
- **GPU model and memory**:
- **Exact command to reproduce**:

bazel run -c opt tensorflow/contrib/lite/toco:toco -- --savedmodel_directory=C:\Users\LENOVO-PC\DIZIZIT\saved_model -- output_file=C:\Users\LENOVO-PC\DIZIZIT\saved_model\maonani.tflite


### Describe the problem
Im trying to convert my saved_model.pb from object detection api file to tensorflow lite but it gives me error such as 


```
ERROR: C:/users/lenovo-pc/tensorflow/tensorflow/contrib/lite/kernels/BUILD:63:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:gemm_support' failed (Exit 2)
.\tensorflow/contrib/lite/context.h(183): error C2144: syntax error: 'float' should be preceded by ';'
.\tensorflow/contrib/lite/context.h(183): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int
Target //tensorflow/contrib/lite/toco:toco failed to build

```


I tried reinstalling all the pre req of tensorflow and use  the code again and it gave different error saying

```
ERROR: C:/users/lenovo-pc/tensorflow/tensorflow/contrib/lite/kernels/BUILD:41:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:eigen_support' failed (Exit 2)
cl : Command line error D8021 : invalid numeric argument '/Wno-error=reorder'
Target //tensorflow/contrib/lite/toco:toco failed to build

```
for the 3rd attempt it gives me error saying:

```
ERROR: C:/users/lenovo-pc/tensorflow/tensorflow/contrib/lite/profiling/BUILD:37:1: C++ compilation of rule '//tensorflow/contrib/lite/profiling:time' failed (Exit 2)
tensorflow/contrib/lite/profiling/time.cc(17): fatal error C1083: Cannot open include file: 'sys/time.h': No such file or directory
Target //tensorflow/contrib/lite/toco:toco failed to build
Use --verbose_failures to see the command lines of failed build steps.
```

### Source code / logs
C:\Users\LENOVO-PC\tensorflow>bazel run -c opt tensorflow/contrib/lite/toco:toco -- --savedmodel_directory=C:\Users\LENOVO-PC\DIZIZIT\saved_model -- output_file=C:\Users\LENOVO-PC\DIZIZIT\saved_model\maonani.tflite
INFO: Build options have changed, discarding analysis cache.
INFO: Analysed target //tensorflow/contrib/lite/toco:toco (1 packages loaded).
INFO: Found 1 target...
ERROR: C:/users/lenovo-pc/tensorflow/tensorflow/contrib/lite/kernels/BUILD:63:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:gemm_support' failed (Exit 2)
.\tensorflow/contrib/lite/context.h(183): error C2144: syntax error: 'float' should be preceded by ';'
.\tensorflow/contrib/lite/context.h(183): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int
Target //tensorflow/contrib/lite/toco:toco failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 6.715s, Critical Path: 1.81s
INFO: 1 process: 1 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully


2nd attempt was too long to post here
"
20803,"Initialize variable from TF Hub/MobilNet checkpoint fails for ""expanded_conv_15/expand/weights"" with Container localhost does not exist. ","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Collab
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9_rc2
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:   https://colab.research.google.com/drive/1X2TqjiK3ctRWCGEmJ3Id2VbbL64MWuzT

### Describe the problem
Initialize variable from TF Hub/MobilNet checkpoint fails for ""expanded_conv_15/expand/weights"" with Container localhost does not exist. 

### Source code / logs
Can be executed at https://colab.research.google.com/drive/1X2TqjiK3ctRWCGEmJ3Id2VbbL64MWuzT
Logs: [tflog.txt](https://github.com/tensorflow/tensorflow/files/2195060/tflog.txt)

```
!wget https://www.tensorflow.org/images/daisies.jpg
import tensorflow as tf, tensorflow_hub as hub

def read_tensor_from_image_file(file_name, height, width, mean=0, std=128):  
  file_reader = tf.read_file(file_name, name=""file_reader"")
  image_reader = tf.image.decode_jpeg(file_reader, channels=3, name=""jpeg_reader"")
  float_caster = tf.cast(image_reader, tf.float32)
  dims_expander = tf.expand_dims(float_caster, 0)
  resized = tf.image.resize_bilinear(dims_expander, [height, width])
  normalized = tf.divide(tf.subtract(resized, [mean]), [std])
  return normalized

module = hub.Module(""https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/2"")
height, width = hub.get_expected_image_size(module)
image = read_tensor_from_image_file(""daisies.jpg"", height, width)
features = module(image)

sess = tf.Session()
result = sess.run(features)
print(result)

```


FailedPreconditionError: Error while reading resource variable module_4/MobilenetV2/expanded_conv_15/expand/weights from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/module_4/MobilenetV2/expanded_conv_15/expand/weights)
	 [[Node: module_4_apply_default/MobilenetV2/expanded_conv_15/expand/Conv2D/ReadVariableOp = ReadVariableOp[dtype=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](module_4/MobilenetV2/expanded_conv_15/expand/weights)]]
"
20802,"tensorflow.exp : error lnk2001 : unresolved symbol ""public: class Eigen::TensorMap<>....""","windows build tensorflow-1.8  almost success,the finnal tensorflow.dll failed
anybody has ideas"
20801,Is eager execution scoped in each tf.Graph context?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 18.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem

It is documented that enabling eager execution is for a lifetime (https://www.tensorflow.org/guide/eager). I normally understand that `tf.executing_eagerly()` will be a global attribute and not affected by any context whether graphs or sessions. However, my following code tells otherwise. You shall see that the output from `tf.executing_eagerly()` differs between graphs in the same lifetime. Is this really an expected behavior? (I think this isn't the case in Tensorflow 1.8.0)

### Source code / logs
```
import tensorflow as tf

tf.enable_eager_execution()
print(tf.executing_eagerly()) # True

with tf.Graph().as_default():
    print(tf.executing_eagerly()) # False
```"
20798,Can not convet .pb to .tflite format using tflite_convert,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: GTX 1060/6G
- **Exact command to reproduce**: 
tflite_convert   --output_file=/tmp/net.tflite --saved_model_dir=models/ssd_mobilenet_v1_plate_0.004_set2_150\*150/saved_model

### Describe the problem
Hi, I want to covert a .pb model to .tflite one. The model is train with tensorflow object detection API. The input tensor shape is (None, None, None, 3) but it seems that tflite_convert doesn't support this kind of input.

### Source code / logs
ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor:0' has invalid shape '[None, None, None, 3]'."
20797,import tensorflow failed with python3.7 on Mac,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
1. Mac 10.13.6 Sierra
2. Python3.7 (brew install python3)
3. install tensorflow1.9.0

- **TensorFlow installed from (source or binary)**:
binary

- **TensorFlow version (use command below)**:
 pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.9.0-py3-none-any.whl

- **Python version**: 
Python 3.7.0 (default, Jun 29 2018, 20:13:13) 
[Clang 9.1.0 (clang-902.0.39.2)] on darwin

- **Exact command to reproduce**:
import tensorflow;

You can obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
DudeMacBook-Pro:~ simbaba$ python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 114
    def TFE_ContextOptionsSetAsync(arg1, async):
                                             ^
SyntaxError: invalid syntax
DudeMacBook-Pro:~ simbaba$ 

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Python 3.7.0 (default, Jun 29 2018, 20:13:13) 
[Clang 9.1.0 (clang-902.0.39.2)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensor as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensor'
>>> import tensorf as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorf'
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 114
    def TFE_ContextOptionsSetAsync(arg1, async):
                                             ^
SyntaxError: invalid syntax

"
20796,TypeError: 'EagerTensor' object cannot be interpreted as an integer,"Running automatic_differentiation.ipynb on Colab, and there's a error.

```
def f(x, y):
  output = 1
  for i in range(y):
    output = tf.multiply(output, x)
  return output

def g(x, y):
  # Return the gradient of `f` with respect to it's first parameter
  return tfe.gradients_function(f)(x, y)[0]

assert f(3.0, 2).numpy() == 9.0   # f(x, 2) is essentially x * x
assert g(3.0, 2).numpy() == 6.0   # And its gradient will be 2 * x
assert f(4.0, 3).numpy() == 64.0  # f(x, 3) is essentially x * x * x
assert g(4.0, 3).numpy() == 48.0  # And its gradient will be 3 * x * x
```

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-21-2c4ab5c7a8b8> in <module>()
     10 
     11 assert f(3.0, 2).numpy() == 9.0   # f(x, 2) is essentially x * x
---> 12 assert g(3.0, 2).numpy() == 6.0   # And its gradient will be 2 * x
     13 assert f(4.0, 3).numpy() == 64.0  # f(x, 3) is essentially x * x * x
     14 assert g(4.0, 3).numpy() == 48.0  # And its gradient will be 3 * x * x

<ipython-input-21-2c4ab5c7a8b8> in g(x, y)
      7 def g(x, y):
      8   # Return the gradient of `f` with respect to it's first parameter
----> 9   return tfe.gradients_function(f)(x, y)[0]
     10 
     11 assert f(3.0, 2).numpy() == 9.0   # f(x, 2) is essentially x * x

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)
    367     """"""Computes the gradient of the decorated function.""""""
    368 
--> 369     _, grad = val_and_grad_function(f, params=params)(*args, **kwds)
    370     return grad
    371 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)
    466       raise ValueError(""Functions to be differentiated cannot ""
    467                        ""receive keyword arguments."")
--> 468     val, vjp = make_vjp(f, params)(*args, **kwds)
    469     return val, vjp(dy=dy)
    470 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)
    522         sources.append(args[i])
    523         tape.watch(args[i])
--> 524       result = f(*args)
    525       if result is None:
    526         raise ValueError(""Cannot differentiate a function that returns None; ""

<ipython-input-21-2c4ab5c7a8b8> in f(x, y)
      1 def f(x, y):
      2   output = 1
----> 3   for i in range(y):
      4     output = tf.multiply(output, x)
      5   return output

TypeError: 'EagerTensor' object cannot be interpreted as an integer"
20790,pip3 install tensorflow fails on Python 3.7,"Have I written custom code
`No`

OS Platform and Distribution
`macOS 10.13.6 (17G65)`

TensorFlow installed from
`install failed`

TensorFlow version
`latest`

Bazel version
CUDA/cuDNN version
`N/A`

GPU model and memory
`N/A`

Exact command to reproduce
```
~: pip3 --version
pip 10.0.1 from /usr/local/lib/python3.7/site-packages/pip (python 3.7)
~: pip3 install tensorflow
Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow
```"
20789,Tensorflow 1.8.0 and 1.9.0 behavioural difference for tf.layers.Layer API ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0 and 1.9.0
- **Python version**: 3.5.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: cpu
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

      import tensorflow as tf


      class LayersBaseTest(tf.test.TestCase):

        def test_create_variable(self):
  
          class MyLayer(tf.layers.Layer):

             def build(self, _):
               # Do not mark the layer as built.
                pass

              def call(self, inputs):
                self.my_var = self.add_variable('my_var', [2, 2])
                if self.built:
                  self.add_variable('this_will_break_on_second_call', [2, 2])
                return inputs + tf.square(self.my_var)

          layer = MyLayer(name='my_layer')
          inputs = tf.random_uniform((2,), seed=1)
          outputs = layer.apply(inputs)
          self.assertEqual(layer.built, True)
          self.assertEqual(outputs.op.name, 'my_layer/add')
          self.assertListEqual([v.name for v in layer.variables], ['my_layer/my_var:0'])
          with self.assertRaisesRegex(ValueError, 'my_layer/this_will_break_on_second_call'):
            layer.apply(inputs)
          # The list of variables hasn't changed.
          self.assertListEqual([v.name for v in layer.variables], ['my_layer/my_var:0'])


      if __name__ == '__main__':
        tf.test.main()

### Describe the problem
the above tests passed with tensorflow version `1.8.0`

Throws the following errors with tensorflow version `1.9.0`. The `tf.layers.Layer` shouldn't create another variable with the same name even after throwing the `ValueError` on the second call to `layer.apply`. This behaviours s unexpected. 
###  logs
>FAIL: test_create_variable (__main__.LayersBaseTest)
 >----------------------------------------------------------------------
 >Traceback (most recent call last):
  >File ""test_layer_base_tf1.9.py"", line 29, in test_create_variable
   >self.assertListEqual([v.name for v in layer.variables], ['my_layer/my_var:0'])
  >AssertionError: Lists differ: ['my_layer/my_var:0', 'my_layer/my_var:0'] != ['my_layer/my_var:0']
   >First list contains 1 additional elements.
   >First extra element 1:  
    >'my_layer/my_var:0'
    > - ['my_layer/my_var:0', 'my_layer/my_var:0']
    > + ['my_layer/my_var:0']"
20788,Issue with `natural_exp_decay` documentation,"The example in the `tf.train.natural_exp_decay` documentation uses an nonexistent function called `exponential_time_decay`. Also, it is missing the `decay_steps` parameter in both the example and the showed equation

Equation:
https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/learning_rate_decay.py#L316
Example
https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/learning_rate_decay.py#L326"
20787,tensorflow-1.9.0-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.,"From my Cmd:  I have downloaded it from here: https://pypi.org/project/tensorflow/#files

F:\Python\Scripts>pip3 install tensorflow-1.9.0-cp36-cp36m-win_amd64.whl
tensorflow-1.9.0-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.

F:\Python\Scripts>python --version
Python 3.6.5

I tried upgrade Pip, Wheel but they're all up-to-date."
20784,BUG: Estimator training with tf.contrib.distribute fails under 1.9.0 but works under 1.8.0,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.9.0 and and v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**:
3.6.5 
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
9.0
- **GPU model and memory**:
Titan 1080 Ti 12GB
- **Exact command to reproduce**:
See description

### Describe the problem
Training Estimator single or multi-gpu with OneDeviceStrategy and MirroredStrategy respectively works under 1.8.0 but produces a graph assertion error under 1.9.0. I know of no breaking changes regarding this mentioned in the [release notes](https://github.com/tensorflow/tensorflow/releases/tag/v1.9.0).

### Source code / logs

I broke it down to the following MWE. Under 1.8.0, both versions work fine, whereas in 1.9.0, the commented out lines work fine and the marked line produces the error as outlined in the Traceback.


```
import tensorflow as tf

def model_fn(features, labels=None, mode=tf.estimator.ModeKeys.TRAIN, params=None):

    train_images_batch = features
    res = tf.layers.conv2d(inputs=train_images_batch, filters=3, kernel_size=3, strides=1, padding='same', data_format='channels_first')
    loss = tf.reduce_mean((train_images_batch - res) ** 2)
    optimizer = tf.train.AdamOptimizer().minimize(loss = loss)
    return tf.estimator.EstimatorSpec(
        mode=tf.estimator.ModeKeys.TRAIN,
        loss=loss, train_op=optimizer)

class Dataset():
    def __init__(self):
        self.dataset = tf.data.Dataset.from_tensors(tf.ones([3,124,124]))

    def map_func(self, entry):

        return (tf.zeros([3,124,124]))
      
    def input_fn(self):

        #dataset = tf.data.Dataset.from_tensors(np.ones(shape=(3,124,124)))
        #dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(1, count=1))
        dataset = self.dataset.apply(tf.contrib.data.shuffle_and_repeat(1, count=1)) # this line actually produces the error
        dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func= self.map_func, num_parallel_batches=1, batch_size=1))
        dataset = dataset.prefetch(buffer_size = 1)

        return dataset

dset = Dataset()

single_gpu = tf.contrib.distribute.OneDeviceStrategy(device='/gpu:0')

runconfig = tf.estimator.RunConfig(train_distribute=single_gpu)

estimator = tf.estimator.Estimator(model_fn=model_fn,
                                   config=runconfig)

estimator.train(input_fn = lambda: dset.input_fn())
```

Traceback:
```
Traceback (most recent call last):
  File ""/home/staff/rippel/TF_MSD/MWE.py"", line 40, in <module>
    estimator.train(input_fn = lambda: dset.input_fn())
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1117, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1145, in _train_model_distributed
    input_fn, model_fn_lib.ModeKeys.TRAIN))
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 987, in _get_features_and_labels_from_input_fn
    return estimator_util.parse_input_fn_result(result)
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/estimator/util.py"", line 104, in parse_input_fn_result
    iterator = result.make_initializable_iterator()
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py"", line 606, in make_initializable_iterator
    dataset_iterator = self._dataset.make_initializable_iterator()
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/prefetching_ops_v2.py"", line 181, in make_initializable_iterator
    shared_name=shared_name)
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/prefetching_ops_v2.py"", line 105, in __init__
    self._input_dataset)
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 311, in make_initializer
    dataset._as_variant_tensor(), self._iterator_resource, name=name)  # pylint: disable=protected-access
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2082, in _as_variant_tensor
    self._input_dataset._as_variant_tensor(),  # pylint: disable=protected-access
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/batching.py"", line 485, in _as_variant_tensor
    input_resource = self._input_dataset._as_variant_tensor()
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/shuffle_ops.py"", line 62, in _as_variant_tensor
    sparse.as_dense_shapes(self.output_shapes, self.output_classes)))
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3554, in shuffle_and_repeat_dataset
    output_types=output_types, output_shapes=output_shapes, name=name)
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 350, in _apply_op_helper
    g = ops._get_graph_from_inputs(_Flatten(keywords.values()))
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 5663, in _get_graph_from_inputs
    _assert_same_graph(original_graph_element, graph_element)
  File ""/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 5599, in _assert_same_graph
    original_item))
ValueError: Tensor(""buffer_size:0"", shape=(), dtype=int64, device=/device:CPU:0) must be from the same graph as Tensor(""TensorDataset:0"", shape=(), dtype=variant).
```"
20783,Tensorflow issue with libcublas.so.9.0,"Hi,

I receive the following error while I want to import tensorflow. The python version is 3.5 and Cuda version is 9.2. 

/usr/bin/python3.5 /home/usr/Desktop/Project1/test.py
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/solale/Desktop/LSTM/lstm/PhasedLSTM-Keras-master/test.py"", line 1, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Process finished with exit code 1"
20781,Feature request: preserve cycle order of open iterators in tf.data.Dataset.interleave,"I'm trying to train RNNs with truncated BPTT with `tf.data` (a great API by the way!) but got tripped up by [these lines](https://github.com/tensorflow/tensorflow/blob/744cf3d3e06fb63ffa40086766137daedc01a5ba/tensorflow/core/kernels/data/interleave_dataset_op.cc#L190-L195) as I've assumed an exhausted iterator would result in a new element being opened directly at the same position in the cycle (in order to pass around RNN states reliably).

Instead what seems to be happening is that my sequences are accidentally shifted in in the subsequent `.batch()` call whenever a sequence is done. Could the default be changed so that a new element is consumed directly as long as there are any left, such that consecutive dataset elements can be batched in a more straightforward way for RNN training.

Or could we have a `tf.contrib.data.batched_interleave` or similar?"
20780,TensorRT inference error: TensorFlow device was not registered,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
last commit from pull request #20350 
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.15.0
- **GCC/Compiler version (if compiling from source)**:
gcc 5.4.0
- **CUDA/cuDNN version**:
Cuda 9.0/cuDNN 7
- **GPU model and memory**:
2 x Tesla P40 (22919MiB)
- **Exact command to reproduce**:


### Describe the problem
Using tensorflow.contrib.tensorrt.create_inference_graph results in an error:
`Non-OK-status: GpuIdManager::TfToCudaGpuId(tf_gpu_id, &cuda_gpu_id) status: Not found: TensorFlow device GPU:0 was not registered`
I use custom frozen graph to convert (2 x conv->3 x gru rnn->dense->softmax).
Other Tensorflow scripts (which are not using tensorrt) works correctly with both GPU.

### Source code / logs
**Source code:**

```
def load_graph(frozen_graph_filename):
    with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
    return graph_def

graph_def = load_graph(model_path)

import tensorflow.contrib.tensorrt as trt2
trt_graph = trt2.create_inference_graph(
    input_graph_def=graph_def,
    outputs=['softmax_var/output'],
    max_batch_size=5,
    max_workspace_size_bytes=5 << 25,
    precision_mode='FP16',
    minimum_segment_size=1
)
with tf.Graph().as_default() as infer_graph:
    tf.import_graph_def(trt_graph, name="""")
```

**Traceback:**
```
INFO:tensorflow:Running against TensorRT version 4.0.1
[INFO 2018-07-13 10:40:36,867 tf_logging.py:115] Running against TensorRT version 4.0.1
2018-07-13 10:40:38.625151: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 2
2018-07-13 10:40:41.120095: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:747] MULTIPLE tensorrt candidate conversion: 428
2018-07-13 10:40:41.120688: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2858] Segment @scope 'rnn/RNN_2/BGRU_2/bw/bw/while/custom_gru_cell/', converted to graph
<...many segment conversion messages...>
2018-07-13 10:41:02.943808: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:711]  Can't find a GPU device to work with. Please instantiate a session to initialize devices
2018-07-13 10:41:02.943813: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:833] Can't identify the cuda device. Running on device 0
2018-07-13 10:41:02.946091: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:846] Engine creation for segment 108, composed of 1 nodes failed: Invalid argument: Output node 'rnn/RNN/BGRU_0/bw/bw/TensorArrayUnstack/strided_slice/stack_2' is weights not tensor. Skipping...
2018-07-13 10:41:02.946118: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:711]  Can't find a GPU device to work with. Please instantiate a session to initialize devices
2018-07-13 10:41:02.946123: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:833] Can't identify the cuda device. Running on device 0
2018-07-13 10:41:02.948399: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:846] Engine creation for segment 109, composed of 1 nodes failed: Invalid argument: Output node 'rnn/RNN/BGRU_0/bw/custom_gru_cell/BatchNorm/moving_mean/read' is weights not tensor. Skipping...
2018-07-13 10:41:02.948426: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:711]  Can't find a GPU device to work with. Please instantiate a session to initialize devices
2018-07-13 10:41:02.948430: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:833] Can't identify the cuda device. Running on device 0
2018-07-13 10:41:02.950709: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:846] Engine creation for segment 110, composed of 1 nodes failed: Invalid argument: Output node 'rnn/RNN_1/BGRU_1/fw/fw/ExpandDims/dim' is weights not tensor. Skipping...
2018-07-13 10:41:02.950719: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:715] Can't determine the device, constructing an allocator at device 0
2018-07-13 10:41:02.950731: F ./tensorflow/core/common_runtime/gpu/gpu_id_utils.h:50] Non-OK-status: GpuIdManager::TfToCudaGpuId(tf_gpu_id, &cuda_gpu_id) status: Not found: TensorFlow device GPU:0 was not registered
Aborted (core dumped)

```"
20779,AttributeError: module 'pandas' has no attribute 'rolling_count',"Hi,

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Hugh Sierra
- **TensorFlow installed from (source or binary)**: I did pip install tensor flow
- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0
- **Python version**:  3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
Please find below the error am having.

Building the network
Traceback (most recent call last):
  File ""./src/NeirbiLSTM-occupancy.py"", line 49, in <module>
    trainoccupancy.run()
  File ""/Users/jasonachonu/git/SeqHub_Summer2018/src/nblib/trainoccupancy.py"", line 173, in run
    config.cost_type)
  File ""/Users/jasonachonu/git/SeqHub_Summer2018/src/nblib/network.py"", line 34, in train_net
    from tensorflow.contrib import rnn
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 39, in <module>
    from tensorflow.contrib import distributions
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distributions/__init__.py"", line 40, in <module>
    from tensorflow.contrib.distributions.python.ops.estimator import *
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/estimator.py"", line 21, in <module>
    from tensorflow.contrib.learn.python.learn.estimators.head import _compute_weighted_loss
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/__init__.py"", line 95, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/__init__.py"", line 28, in <module>
    from tensorflow.contrib.learn.python.learn import *
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/__init__.py"", line 30, in <module>
    from tensorflow.contrib.learn.python.learn import estimators
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/__init__.py"", line 302, in <module>
    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py"", line 35, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py"", line 36, in <module>
    from tensorflow.contrib.learn.python.learn.estimators import estimator
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 52, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_io/__init__.py"", line 26, in <module>
    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_io/dask_io.py"", line 33, in <module>
    import dask.dataframe as dd
  File ""/anaconda3/lib/python3.6/site-packages/dask/dataframe/__init__.py"", line 12, in <module>
    from .rolling import (rolling_count, rolling_sum, rolling_mean, rolling_median,
  File ""/anaconda3/lib/python3.6/site-packages/dask/dataframe/rolling.py"", line 200, in <module>
    rolling_count = wrap_rolling(pd.rolling_count, 'count')
AttributeError: module 'pandas' has no attribute 'rolling_count'

"
20778,ImportError: cannot import name 'abs',"Windows 10 ,  CUDA 9 + CUDNN 7,  Python3.6(Anaconda5.2)
install the TensorFlow 1.9:
`pip install --upgrade tensorflow-gpu`
when I import the TensorFlow in ipyhon:
` from tensorflow.python.keras._impl.keras.backend import abs
ImportError: cannot import name 'abs'`
![image](https://user-images.githubusercontent.com/37954865/42689854-ab49a7c8-86d4-11e8-9a14-43e05d4e3f90.png)
"
20776,bug about boosted_trees,"In tensorflow 1.9 , win10, python36,  I run the example ""tensorflow-master\tensorflow\contrib\boosted_trees\examples\boston.py"", report :

  File ""<ipython-input-1-0c2875deaba9>"", line 6, in <module>
    from tensorflow.contrib.boosted_trees.estimator_batch import custom_export_strategy

  File ""D:\ProgramFiles\Anaconda3\lib\site-packages\tensorflow\contrib\boosted_trees\estimator_batch\custom_export_strategy.py"", line 25, in <module>
    from tensorflow.contrib.boosted_trees.python.training.functions import gbdt_batch

  File ""D:\ProgramFiles\Anaconda3\lib\site-packages\tensorflow\contrib\boosted_trees\python\training\functions\gbdt_batch.py"", line 26, in <module>
    from tensorflow.contrib.boosted_trees.lib.learner.batch import categorical_split_handler

ModuleNotFoundError: No module named 'tensorflow.contrib.boosted_trees.lib'
++++++++++++++++++++++++++++++++++++++++++++++
And when use tensorflow 1.8 ,python 36 ,centos5 or win10, run the same example ""tensorflow-master\tensorflow\contrib\boosted_trees\examples\boston.py"", report :
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 225, in run
    return _execute_schedule(experiment, schedule)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 52, in _execute_schedule
    return task()
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 666, in train_and_evaluate
    self.train(delay_secs=0)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 389, in train
    saving_listeners=self._saving_listeners)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 879, in _call_train
    input_fn=input_fn, steps=steps, max_steps=max_steps, monitors=hooks)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 524, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1080, in _train_model
    scaffold=scaffold)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 421, in __init__
    self._save_path = os.path.join(checkpoint_dir, checkpoint_basename)
  File ""/usr/local/python36/lib/python3.6/posixpath.py"", line 92, in join
    genericpath._check_arg_types('join', a, *p)
  File ""/usr/local/python36/lib/python3.6/genericpath.py"", line 151, in _check_arg_types
    raise TypeError(""Can't mix strings and bytes in path components"") from None
TypeError: Can't mix strings and bytes in path components
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
and  I run other example again ,""tensorflow-master\tensorflow\contrib\boosted_trees\examples\mnist.py"" ,report:

  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 225, in run
    return _execute_schedule(experiment, schedule)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 52, in _execute_schedule
    return task()
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 666, in train_and_evaluate
    self.train(delay_secs=0)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 389, in train
    saving_listeners=self._saving_listeners)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 879, in _call_train
    input_fn=input_fn, steps=steps, max_steps=max_steps, monitors=hooks)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 524, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1041, in _train_model
    model_fn_ops = self._get_train_ops(features, labels)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1264, in _get_train_ops
    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1227, in _call_model_fn
    model_fn_results = self._model_fn(features, labels, **kwargs)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/estimator_batch/model.py"", line 116, in model_builder
    logits=logits)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py"", line 1085, in create_model_fn_ops
    enable_centered_bias=self._enable_centered_bias)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py"", line 669, in _create_model_fn_ops
    batch_size, loss_fn, weight_tensor)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py"", line 1944, in _train_op
    train_op = train_op_fn(loss)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/estimator_batch/model.py"", line 105, in _train_op_fn
    update_op = gbdt_model.train(loss, predictions_dict, labels)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/training/functions/gbdt_batch.py"", line 696, in train
    control_flow_ops.no_op))
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2063, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1913, in BuildCondBranch
    original_result = fn()
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/training/functions/gbdt_batch.py"", line 932, in _update_bias_stats
    ensemble_stamp, partition_ids, feature_ids, grads_sum, hess_sum)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/ops/stats_accumulator_ops.py"", line 117, in add
    partition_ids, feature_ids, gradients, hessians))
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/ops/stats_accumulator_ops.py"", line 154, in _make_summary
    partition_ids, feature_ids, gradients, hessians)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/ops/gen_stats_accumulator_ops.py"", line 1232, in stats_accumulator_tensor_make_summary
    name=name)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 533, in _apply_op_helper
    (prefix, dtypes.as_dtype(input_arg.type).name))
TypeError: Input 'partition_ids' of 'StatsAccumulatorTensorMakeSummary' Op has type int64 that does not match expected type of int32.
"
20775,tf1.2 windows dll error,"ä¸¥é‡æ€§	ä»£ç 	è¯´æ˜Ž	é¡¹ç›®	æ–‡ä»¶	è¡Œ	ç¦æ­¢æ˜¾ç¤ºçŠ¶æ€
é”™è¯¯	LNK2001	æ— æ³•è§£æžçš„å¤–éƒ¨ç¬¦å· ""public: void __cdecl google::protobuf::internal::LogFinisher::operator=(class google::protobuf::internal::LogMessage &)"" (??4LogFinisher@internal@protobuf@google@@QEAAXAEAVLogMessage@123@@Z)	win_trainer	H:\2018\001_TFopencv\002_demo\TF1.2\DemoTF1.2\win_trainer\example_trainer.obj	1	



"
20773,tf.Print related wired bug,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 (latest)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: r1.9 (v1.9.0-0-g25c197e023)
- **Python version**: Python 3.6.5 |Anaconda, Inc.| [MSC v.1900 64 bit (AMD64)] on win32
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 64_7
- **GPU model and memory**: GTX 1060-6GB(Laptop)
- **Exact command to reproduce**: N/A

### Describe the problem
While developing a prime number generator using `tensorflow-gpu`, I came across a wired bug. Given the source code, if you use `tf.Print` result is correct and run-time is less than without using `tf.Print`. 

### Source code / logs
``` python
# check first 'n' numbers for primity using tensorflow

import warnings
with warnings.catch_warnings():
    warnings.filterwarnings(""ignore"",category=FutureWarning)
    import h5py
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
import tensorflow as tf
import numpy as np
import time

def prime_tf_graf(max_count,dump):
	ttyp = tf.int32
	data = tf.range(2, max_count, dtype=ttyp)
	if dump:
		data = tf.Print(data, [data], 'Data Loaded!')
	X, Y = tf.meshgrid(data, data); 
	temp = Y%X
	mask = tf.cast(tf.equal(temp, Y), dtype=ttyp)
	temp = temp - Y * mask
	temp = tf.cast(tf.not_equal(temp, 0), dtype=ttyp)
	sumr = tf.reduce_sum(temp, axis=1)
	nums = data - 2
	rato = tf.cast(sumr/nums, ttyp)
	indx =  tf.cast(tf.not_equal(rato, 0), ttyp)
	shap =  tf.reshape(tf.where(indx), [-1]) + 2
	return shap

def prime_tf(max_count,dump):
	with tf.device('/gpu:0'):
		graf = prime_tf_graf(max_count, dump)
	sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))
	sess.run(tf.global_variables_initializer())
	prim = sess.run(graf)
	return prim

def bench(task):
	now = time.time()
	res = task()
	return res, time.time() - now

p, t = bench(lambda: prime_tf(10000, True))
print('With Dump: found %d primes in %f time'%(len(p), t))
p, t = bench(lambda: prime_tf(10000, False))
print('Without Dump: found %d primes in %f time'%(len(p), t))
```
and here is output on my system:

```
PS E:\Research\TFLearn> python .\bug.py
Data Loaded![2 3 4...]
With Dump: found 1229 primes in 12.599950 time
Without Dump: found 1085 primes in 25.162465 time
```
"
20771,Where is the nightly-android ?,"--------------------

### Describe the problem
The link https://ci.tensorflow.org/view/Nightly/job/nightly-android/ (prebuilt android libraries) is broken but is still used in various files in documentation :
- https://www.tensorflow.org/mobile/android_build#android_inference_library
- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/README.md

I use it for my app, but obviously I can't download the new versions. 

Is there a new link from which we could retrieve built binaries for Android ? 
Why has the nightly-android stopped running and will it restart eventually ?
Can someone fix the documentation files ?

Thanks. 

### UPDATE
Have I written custom code N/A
OS Platform and Distribution N/A
TensorFlow installed from N/A
TensorFlow version N/A
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce N/A
"
20770,CUDA 9.2 + cuDNN 7.1.4 Support for Windows,"I installed CUDA 9.2 and cuDNN 7xx on windows 10. when I import tensorflow in python, it says that tensorflow works only with CUDA 9.0, so when wil the support for CUDA 9.2 come."
20769,Configure Tensorflow on GPU using Java API,"Hello,

How to set following config ""gpu_options.allow_growth = True"" for Tensorflow, using Java API?
I tried using this code but it doesn't work:

```
        ConfigProto config = ConfigProto.newBuilder()
                  .setGpuOptions(GPUOptions.newBuilder()
                                         .setAllowGrowth(true)
                                         .setPerProcessGpuMemoryFraction(0.04)
                                         .build()
                                        ).build();
           model.session().runner()
                    .setOptions(config.toByteArray())
                    .feed(""image_tensor"", input).fetch(""detection_scores"")
                    .fetch(""detection_classes"").fetch(""detection_boxes"").fetch(""num_detections"").run();
```

I know the method setOption is an experimental method. 
Is there another away to achieve it?

Thanks"
20767,How to access the tensorflow model directly from PC?,"The main issue I am facing is that I am not able to access to object_detection_models even though it has been saved offline. I have described my problem in the comment below:
The main error I am having is this:
**The filename, directory name, or volume label syntax is incorrect.**"
20765,Failed:  bazel build tensorflow/tools/graph_transforms:transform_graph,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:    Mac OS 10.13.6
- **TensorFlow installed from (source or binary)**:   Installed from source by git clone
- **TensorFlow version (use command below)**:     version 1.8
- **Python version**: version 3.6     
- **Bazel version (if compiling from source)**:  0.15.0-homebrew
- **CUDA/cuDNN version**:  NO
- **GPU model and memory**: NO
- **Exact command to reproduce**: 


### Describe the problem
I want to quantize my object detection model trained by tensorflow object detection api. But when I build transform_graph, there comes an error.

### Source code / logs
` bazel build tensorflow/tools/graph_transforms:transform_graph`
Error log is as follows:

> Starting local Bazel server and connecting to it...
..........
INFO: Analysed target //tensorflow/tools/graph_transforms:transform_graph (67 packages loaded).
INFO: Found 1 target...
ERROR: /private/var/tmp/_bazel_lee/f925e5225c4f573b45c7ce2445e70455/external/protobuf_archive/BUILD:260:1: Linking of rule '@protobuf_archive//:js_embed' failed (Exit 1)
ld: unknown option: -no-as-needed
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/tools/graph_transforms:transform_graph failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 5.752s, Critical Path: 0.20s
INFO: 0 processes.
FAILED: Build did NOT complete successfully

I see [https://github.com/tensorflow/tensorflow/issues/19525](url), and I tried tensorflow branch r1.8 and r1.7, both return the same error as above.
"
20764,"Can't build custom ops in tensorflow-gpu 1.9.0 if include ""cuda_kernel_helper.h"" with C++14","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:v1.9.0-0-g25c197e023 1.9.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:5.5.0
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:GTX1060
- **Exact command to reproduce**:N?A

### Describe the problem
If I build a custom ops with C++ 14 and ""cuda_kernel_helper.h"", following errors are produced:
```bash
>> nvcc -std=c++14 -c -o fill_functor.cu.o fill_functor.cu.cc    -arch=sm_61 ${TF_CFLAGS[@]} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC -I/usr/local --expt-relaxed-constexpr

/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of â€˜bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = stream_executor::dnn::VersionInfo]â€™:
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from â€˜stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = stream_executor::dnn::VersionInfo]â€™
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type â€˜const Status {aka const tensorflow::Status}â€™ to type â€˜stream_executor::port::Status& {aka tensorflow::Status&}â€™
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of â€˜bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = std::unique_ptr<stream_executor::dnn::RnnDescriptor>]â€™:
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from â€˜stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = std::unique_ptr<stream_executor::dnn::RnnDescriptor>]â€™
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type â€˜const Status {aka const tensorflow::Status}â€™ to type â€˜stream_executor::port::Status& {aka tensorflow::Status&}â€™
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of â€˜bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = std::unique_ptr<stream_executor::dnn::RnnSequenceTensorDescriptor>]â€™:
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from â€˜stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = std::unique_ptr<stream_executor::dnn::RnnSequenceTensorDescriptor>]â€™
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type â€˜const Status {aka const tensorflow::Status}â€™ to type â€˜stream_executor::port::Status& {aka tensorflow::Status&}â€™
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of â€˜bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = std::unique_ptr<stream_executor::dnn::RnnStateTensorDescriptor>]â€™:
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from â€˜stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = std::unique_ptr<stream_executor::dnn::RnnStateTensorDescriptor>]â€™
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here
/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type â€˜const Status {aka const tensorflow::Status}â€™ to type â€˜stream_executor::port::Status& {aka tensorflow::Status&}â€™
```
If I build it with `std=c++11` or tensorflow 1.8.0, there is no problem. But I need auto lambda to use static loop in my host code which only available in c++14.
### Source code / logs
The following code is a minimal example to reproduce that error.
```C++
// fill_functor.h
#define EIGEN_USE_THREADS

#include ""third_party/eigen3/unsupported/Eigen/CXX11/Tensor""
#include ""tensorflow/core/framework/tensor_types.h""
#include ""tensorflow/core/framework/types.h""

namespace tensorflow {
namespace functor {

template <typename Device, typename T>
struct FillFunctor {
  // Computes on device ""d"": out = out.constant(in(0)),
  void operator()(const Device& d, typename TTypes<T>::Flat out,
                  typename TTypes<T>::ConstScalar in);
  void operator()(const Device& d, typename TTypes<T>::Flat out,
                  T in);
};

template <typename Device, typename T>
struct SetZeroFunctor {
  // Computes on device ""d"": out = out.setZero(),
  void operator()(const Device& d, typename TTypes<T>::Flat out);
};


}  // namespace functor
}  // namespace tensorflow

// fill_functor.cu.cc
#if GOOGLE_CUDA

#define EIGEN_USE_GPU

#include ""tensorflow/core/framework/register_types.h""
#include ""tensorflow/core/framework/tensor_types.h""
#include ""fill_functor.h""
#include ""tensorflow/core/platform/types.h""
#include ""tensorflow/core/util/cuda_kernel_helper.h""
namespace Eigen {
namespace internal {

template <typename T>
struct scalar_const_op {
  typedef typename packet_traits<T>::type Packet;

  const T* val;

  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE
  scalar_const_op(const scalar_const_op& x)
      : val(x.val) {}

  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE scalar_const_op(const T* v) : val(v) {}

  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const T operator()() const {
    return *val;
  }

  template <typename PacketType = Packet>
  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const PacketType packetOp() const {
    return internal::pset1<PacketType>(*val);
  }
};

template <typename T>
struct functor_traits<scalar_const_op<T> > {
  enum {
    Cost = 1,
    PacketAccess = packet_traits<T>::Vectorizable,
    IsRepeatable = true
  };
};

}  // end namespace internal
}  // end namespace Eigen

namespace tensorflow {

namespace functor {

typedef Eigen::GpuDevice GPUDevice;

// Partial specialization FillFunctor<Device=GPUDevice, T>
template <typename T>
struct FillFunctor<GPUDevice, T> {
  void operator()(const GPUDevice& d, typename TTypes<T>::Flat out,
                  typename TTypes<T>::ConstScalar in) {
    Eigen::internal::scalar_const_op<T> f(in.data());
    To32Bit(out).device(d) = To32Bit(out).nullaryExpr(f);
  }
  void operator()(const GPUDevice& d, typename TTypes<T>::Flat out,
                  T in) {
    To32Bit(out).device(d) = To32Bit(out).constant(in);
  }
};

#define DEFINE_FILL_GPU(T) template struct FillFunctor<GPUDevice, T>;
TF_CALL_NUMBER_TYPES(DEFINE_FILL_GPU);
TF_CALL_bool(DEFINE_FILL_GPU);
#undef DEFINE_FILL_GPU

// Partial specialization of FillFunctor<Device=GPUDevice, T>.
template <typename T>
struct SetZeroFunctor<GPUDevice, T> {
  void operator()(const GPUDevice& d, typename TTypes<T>::Flat out) {
    To32Bit(out).device(d) = To32Bit(out).constant(T(0));
  }
};

#define DEFINE_SETZERO_GPU(T) template struct SetZeroFunctor<GPUDevice, T>;
TF_CALL_NUMBER_TYPES(DEFINE_SETZERO_GPU);
TF_CALL_bool(DEFINE_SETZERO_GPU);
#undef DEFINE_SETZERO_GPU


}  // end namespace functor
}  // end namespace tensorflow

#endif  // GOOGLE_CUDA
```
"
20761,Error occurs when I create subprocess after creating tf.train.Server with RDMA.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Redhat 7.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.8.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: CUDA-9.0/cuDNN-7.0.5
- **GPU model and memory**: Tesla-P100
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I compile the TensorFlow-v1.8.0 with RDMA support and modify the mnist_replicas.py based on: 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py

```
...
  if not FLAGS.existing_servers:
    # Not using existing servers. Create an in-process server.
    server = tf.train.Server(
        cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index, protocol='grpc+verbs')
    if FLAGS.job_name == ""ps"":
      server.join()
...
```

It works just fine.
Then I try to create a subprocess after `tf.train.Server` is created which leads to an error:

```
...
  if not FLAGS.existing_servers:
    # Not using existing servers. Create an in-process server.
    server = tf.train.Server(
        cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index, protocol='grpc+verbs')

    import multiprocessing; multiprocessing.Process(target=time.sleep, args=(100,)).start()

    if FLAGS.job_name == ""ps"":
      server.join()
...
```

 Here is the log:

worker:
```
...
job name = worker
task index = 0
2018-07-13 09:22:57.115547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:b5:00.0
totalMemory: 15.90GiB freeMemory: 15.27GiB
2018-07-13 09:22:57.115607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Ignoring visible gpu device (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:b5:00.0, compute capability: 6.0) with Cuda compute capability 6.0. The minimum required Cuda capability is 7.0.
2018-07-13 09:22:57.115628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-13 09:22:57.115636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0
2018-07-13 09:22:57.115644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N
2018-07-13 09:22:57.116866: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 172.30.96.109:49998}
2018-07-13 09:22:57.116883: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:49999}
2018-07-13 09:22:57.118450: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 172.30.96.109:49998}
2018-07-13 09:22:57.118468: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:49999}
2018-07-13 09:22:57.125228: I tensorflow/contrib/verbs/rdma.cc:315] RoCE v2 is not configured for GID_INDEX 0
2018-07-13 09:22:57.127868: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:49999
2018-07-13 09:22:57.134897: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:0
2018-07-13 09:22:57.770456: I tensorflow/contrib/verbs/rdma_mgr.cc:311] Instrumenting CPU allocator cpu_rdma_bfc
2018-07-13 09:22:57.770480: I tensorflow/contrib/verbs/rdma_mgr.cc:311] Instrumenting CPU allocator cpu_pool
WARNING:tensorflow:From mnist_replica_ib.py:217: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
Worker 0: Initializing session...
2018-07-13 09:22:58.064007: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 6cb605bf203f3160 with config: device_filters: ""/job:ps"" device_filters: ""/job:worker/task:0"" allow_soft_placement: true
2018-07-13 09:22:58.090607: F tensorflow/contrib/verbs/rdma.cc:691] Check failed: iter != request_table_.end()
Aborted
```

ps
```
...
job name = ps
task index = 0
2018-07-13 09:22:54.747787: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE
2018-07-13 09:22:54.747853: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: bms-60c5
2018-07-13 09:22:54.747865: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: bms-60c5
2018-07-13 09:22:54.747923: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 390.67.0
2018-07-13 09:22:54.747988: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 390.67.0
2018-07-13 09:22:54.748000: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 390.67.0
2018-07-13 09:22:54.750241: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:49998}
2018-07-13 09:22:54.750261: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 172.30.96.109:49999}
2018-07-13 09:22:54.751826: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:49998}
2018-07-13 09:22:54.751843: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 172.30.96.109:49999}
2018-07-13 09:22:54.759391: I tensorflow/contrib/verbs/rdma.cc:315] RoCE v2 is not configured for GID_INDEX 0
2018-07-13 09:22:54.762222: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:49998
2018-07-13 09:22:57.770347: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:0
2018-07-13 09:22:57.770929: I tensorflow/contrib/verbs/rdma_mgr.cc:311] Instrumenting CPU allocator cpu_rdma_bfc
2018-07-13 09:22:57.770944: I tensorflow/contrib/verbs/rdma_mgr.cc:311] Instrumenting CPU allocator cpu_pool
```

Then I  try creating subprocess before `tf.train.Server` is created, it works without any error. (worker will stuck after finish training because subprocess is still sleeping.)
Code:
```
...
  if not FLAGS.existing_servers:
    # Not using existing servers. Create an in-process server.
    import multiprocessing; multiprocessing.Process(target=time.sleep, args=(100,)).start()
    server = tf.train.Server(
        cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index, protocol='grpc+verbs')
    if FLAGS.job_name == ""ps"":
      server.join()
...
```

Any idea?"
20751,bug in tf.Print summarized formatting,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

If you print a tensor of shape [n, 4] with tf.Print, by default (summarize=3 is the default value), you get:

[[9 21 55]...]

which wrongly looks like your tensor is of shape [n, 3].

The correct output should be:

[[9 21 55...]...]

Here is what you get with tf.Print(summarize=10):

[[9 21 55 30][190 -42 236 4][89 -5]...]

Now the vectors of size 4 are visible although the last one is still wrong (looks like a vector of size 2
)"
20750,RunOptions.FULL_TRACE produce wrong timestamps,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:v1.9.0-0-g25c197e023 1.9.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:n/a
- **GCC/Compiler version (if compiling from source)**:n/a
- **CUDA/cuDNN version**:9.0.176/7.1.1
- **GPU model and memory**:P100 16G (can reproduce on other GPU model as well)
- **Exact command to reproduce**: see below

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import sys
import os
import tqdm
from datetime import datetime
import tensorflow as tf
from tensorflow.python.client import timeline

def build():
    image = tf.random_normal(shape=[64,32,32,3], dtype=tf.float32)
    label = tf.random_uniform(shape=[64], maxval=10, dtype=tf.int32)
    l = tf.transpose(image, [0, 3, 1, 2])
    for k in range(1, 100):
        l = tf.layers.conv2d(l, 16, 3, data_format='channels_first',
                name='conv{}'.format(k), padding='SAME')
    l = tf.reduce_mean(l, [2, 3])

    logits = tf.layers.dense(l, 10, name='linear')
    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)
    cost = tf.reduce_mean(cost, name='cross_entropy_loss')
    return cost

if __name__ == '__main__':
    with tf.device('/gpu:0'):
        cost1 = build()
    with tf.device('/gpu:1'), tf.variable_scope(tf.get_variable_scope(), reuse=True):
        cost2 = build()
    cost = cost1 + cost2
    opt = tf.train.GradientDescentOptimizer(0.1)
    train_op = opt.minimize(cost)

    def write_tracing(idx, metadata):
        tl = timeline.Timeline(step_stats=metadata.step_stats)
        fname = os.path.join(
            '.', 'chrome-trace-{}.json'.format(idx))
        with open(fname, 'w') as f:
            f.write(tl.generate_chrome_trace_format(
                show_dataflow=True, show_memory=True))

    config = tf.ConfigProto()
    config.allow_soft_placement = True
    with tf.Session(config=config) as sess:
        sess.run(tf.global_variables_initializer())
        opt = tf.RunOptions()
        opt.trace_level = tf.RunOptions.FULL_TRACE

        for k in tqdm.trange(100):
            meta = tf.RunMetadata()
            sess.run(train_op, options=opt, run_metadata=meta)

            write_tracing(k, meta)

            for devst in meta.step_stats.dev_stats:
                for ns in devst.node_stats:
                    micro = timestamp = ns.all_start_micros // 1000000
                    timestamp = datetime.fromtimestamp(timestamp)
                    diff = timestamp - datetime.now()
                    if diff.days > 100:
                        print(k, micro, timestamp)
                        #import IPython as IP; IP.embed()
                        #sys.exit()
```

The code above trains a CNN on two GPUs with `FULL_TRACE` enabled. The returned profiling information contains correct timestamps, but sometimes contains timestamps that are many years in the future. It prints the following output:
```
...
13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  
13 18446744073 2554-07-21 16:34:33               
13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  
13 18446744073 2554-07-21 16:34:33               
13 18446744073 2554-07-21 16:34:33               
13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  
13 18446744073 2554-07-21 16:34:33               
13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  
13 18446744073 2554-07-21 16:34:33               
13 18446744073 2554-07-21 16:34:33               
13 18446744073 2554-07-21 16:34:33
...
```

The issue was originally reported at https://github.com/tensorpack/tensorpack/issues/819.
The issue is more likely to happen after running about 50 steps.
The issue seems to disappear when training on one GPU, or training a small model."
20748,Miss a '.support',"https://github.com/tensorflow/tensorflow/blob/365d2fc4d62540b2c6524500a7a58e7edab0dfa9/tensorflow/contrib/lite/java/demo/app/build.gradle#L47

It should be com.android.support.test instead of com.androidx.test. Otherwise, gradle will report an error"
20747,TPU code running locally but not on TPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, I did put the code below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 9
- **TensorFlow installed from (source or binary)**: already installed while creating the VM
- **TensorFlow version (use command below)**: 1.9.0-rc2
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: Not compiled
- **GCC/Compiler version (if compiling from source)**: Not compiled
- **CUDA/cuDNN version**: not installed
- **GPU model and memory**: no GPU
- **Exact command to reproduce**: ```python3 test.py --tpu=$TPU_NAME --model_dir=output -use-tpu=True --batch_size=24```

### Describe the problem
When I run the following code:

```python
from tensorflow.python.keras.applications.vgg16 import VGG16
from tensorflow.python.keras import models
from tensorflow.python.keras import layers
from tensorflow.python.keras.preprocessing import image
import numpy as np
import tensorflow as tf
from tensorflow.python import keras
from absl import flags
import absl.logging as _logging
from tensorflow.contrib.tpu.python.tpu import tpu_config
from tensorflow.contrib.tpu.python.tpu import tpu_estimator
from tensorflow.contrib.tpu.python.tpu import tpu_optimizer
import numpy as np
import random
import math
import os


#  Cloud TPU Cluster Resolver flags
tf.flags.DEFINE_string(
    ""tpu"", default=None,
    help=""The Cloud TPU to use for training. This should be either the name ""
    ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 ""
    ""url."")
tf.flags.DEFINE_string(
    ""tpu_zone"", default=None,
    help=""[Optional] GCE zone where the Cloud TPU is located in. If not ""
    ""specified, we will attempt to automatically detect the GCE project from ""
    ""metadata."")
tf.flags.DEFINE_string(
    ""gcp_project"", default=None,
    help=""[Optional] Project name for the Cloud TPU-enabled project. If not ""
    ""specified, we will attempt to automatically detect the GCE project from ""
    ""metadata."")

# Model specific parameters
tf.flags.DEFINE_string(
    ""master"", default=None,
    help=""GRPC URL of the master (e.g. grpc://ip.address.of.tpu:8470). You ""
    ""must specify either this flag or --tpu."")
tf.flags.DEFINE_string(""data_dir"", """",
                       ""Path to directory containing the dataset"")
tf.flags.DEFINE_string(""model_dir"", 'output', ""Estimator model_dir"")
tf.flags.DEFINE_integer(""batch_size"", 3,
                        ""Mini-batch size for the training. Note that this ""
                        ""is the global batch size and not the per-shard batch."")
tf.flags.DEFINE_integer(""train_steps"", 1, ""Total number of training steps."")
tf.flags.DEFINE_float(""learning_rate"", 0.001, ""Learning rate."")

tf.flags.DEFINE_bool(""use_tpu"", True, ""Use TPUs rather than plain CPUs"")
tf.flags.DEFINE_integer(""iterations"", 1,
                        ""Number of iterations per TPU training loop."")
tf.flags.DEFINE_integer(""num_shards"", 8, ""Number of shards (TPU chips)."")

FLAGS = tf.flags.FLAGS

feature_names = [
    'query',
    'positive',
    'negative']

def load_triplets():
    triplets = []
    triplet_file = os.path.join(FLAGS.data_dir, 'triplets.txt')
    with tf.gfile.GFile(triplet_file) as f:
        count = 0
        for line in f:
            triplets.append(line.strip().split(','))
            count += 1
            if count % 1000 == 0:
                tf.logging.info(""Loading {} triplets"".format(count))

    tf.logging.info(""Loading {} triplets"".format(count))
    return triplets

def my_input_fn(triplet, label):
    query_img = tf.image.decode_jpeg(triplet[0], channels=3)
    query_img.set_shape([None, None, None])
    query_img = tf.image.resize_images(query_img, [225, 225])
    query_img.set_shape([225, 225, 3])
    positive_img = tf.image.decode_jpeg(tf.read_file(triplet[1]), channels=3)
    positive_img.set_shape([None, None, None])
    positive_img = tf.image.resize_images(positive_img, [225, 225])
    positive_img.set_shape([225, 225, 3])
    negative_img = tf.image.decode_jpeg(tf.read_file(triplet[2]), channels=3)
    negative_img.set_shape([None, None, None])
    negative_img = tf.image.resize_images(negative_img, [225, 225])
    negative_img.set_shape([225, 225, 3])

    return dict(zip(feature_names, [query_img, positive_img, negative_img])), label

def random_flip_left_right(image, label):
    augmented_query = tf.to_float(image['query'])
    augmented_query = tf.image.random_flip_left_right(image['query'])
    augmented_query = tf.cast(augmented_query, np.uint8)
    augmented_positive = tf.to_float(image['positive'])
    augmented_positive = tf.image.random_flip_left_right(image['positive'])
    augmented_positive = tf.cast(augmented_positive, np.uint8)
    augmented_negative = tf.to_float(image['negative'])
    augmented_negative = tf.image.random_flip_left_right(image['negative'])
    augmented_negative = tf.cast(augmented_negative, np.uint8)

    return dict(zip(feature_names, [augmented_query, augmented_positive, augmented_negative])), label

def random_flip_up_down(image, label):
    augmented_query = tf.to_float(image['query'])
    augmented_query = tf.image.random_flip_up_down(image['query'])
    augmented_query = tf.cast(augmented_query, np.uint8)
    augmented_positive = tf.to_float(image['positive'])
    augmented_positive = tf.image.random_flip_up_down(image['positive'])
    augmented_positive = tf.cast(augmented_positive, np.uint8)
    augmented_negative = tf.to_float(image['negative'])
    augmented_negative = tf.image.random_flip_up_down(image['negative'])
    augmented_negative = tf.cast(augmented_negative, np.uint8)

    return dict(zip(feature_names, [augmented_query, augmented_positive, augmented_negative])), label


def random_brightness(image, label):
    augmented_query = tf.to_float(image['query'])
    augmented_query = tf.image.random_brightness(image['query'], max_delta=random.uniform(0.0, 1.0))
    augmented_query = tf.cast(augmented_query, np.uint8)
    augmented_positive = tf.to_float(image['positive'])
    augmented_positive = tf.image.random_brightness(image['positive'], max_delta=random.uniform(0.0, 1.0))
    augmented_positive = tf.cast(augmented_positive, np.uint8)
    augmented_negative = tf.to_float(image['negative'])
    augmented_negative = tf.image.random_brightness(image['negative'], max_delta=random.uniform(0.0, 1.0))
    augmented_negative = tf.cast(augmented_negative, np.uint8)

    return dict(zip(feature_names, [augmented_query, augmented_positive, augmented_negative])), label

def random_contrast(image, label):
    augmented_query = tf.to_float(image['query'])
    augmented_query = tf.image.random_contrast(image['query'], lower=0.3, upper=1.0)
    augmented_query = tf.cast(augmented_query, np.uint8)
    augmented_positive = tf.to_float(image['positive'])
    augmented_positive = tf.image.random_contrast(image['positive'], lower=0.3, upper=1.0)
    augmented_positive = tf.cast(augmented_positive, np.uint8)
    augmented_negative = tf.to_float(image['negative'])
    augmented_negative = tf.image.random_contrast(image['negative'], lower=0.3, upper=1.0)
    augmented_negative = tf.cast(augmented_negative, np.uint8)

    return dict(zip(feature_names, [augmented_query, augmented_positive, augmented_negative])), label

def random_hue(image, label):
    augmented_query = tf.to_float(image['query'])
    augmented_query = tf.image.random_hue(image['query'], max_delta=random.uniform(0.0, 0.5))
    augmented_query = tf.cast(augmented_query, np.uint8)
    augmented_positive = tf.to_float(image['positive'])
    augmented_positive = tf.image.random_hue(image['positive'], max_delta=random.uniform(0.0, 0.5))
    augmented_positive = tf.cast(augmented_positive, np.uint8)
    augmented_negative = tf.to_float(image['negative'])
    augmented_negative = tf.image.random_hue(image['negative'], max_delta=random.uniform(0.0, 0.5))
    augmented_negative = tf.cast(augmented_negative, np.uint8)

    return dict(zip(feature_names, [augmented_query, augmented_positive, augmented_negative])), label

def random_saturation(image, label):
    augmented_query = tf.to_float(image['query'])
    augmented_query = tf.image.random_saturation(image['query'], lower=0.0, upper=2.0)
    augmented_query = tf.cast(augmented_query, np.uint8)
    augmented_positive = tf.to_float(image['positive'])
    augmented_positive = tf.image.random_saturation(image['positive'], lower=0.0, upper=2.0)
    augmented_positive = tf.cast(augmented_positive, np.uint8)
    augmented_negative = tf.to_float(image['negative'])
    augmented_negative = tf.image.random_saturation(image['negative'], lower=0.0, upper=2.0)
    augmented_negative = tf.cast(augmented_negative, np.uint8)

    return dict(zip(feature_names, [augmented_query, augmented_positive, augmented_negative])), label

def random_rotate(image, label):
    augmented_query = tf.to_float(image['query'])
    augmented_query = tf.contrib.image.rotate(image['query'], angles=random.uniform(0, 360) * math.pi / 180)
    augmented_query = tf.cast(augmented_query, np.uint8)
    augmented_positive = tf.to_float(image['positive'])
    augmented_positive = tf.contrib.image.rotate(image['positive'], angles=random.uniform(0, 360) * math.pi / 180)
    augmented_positive = tf.cast(augmented_positive, np.uint8)
    augmented_negative = tf.to_float(image['negative'])
    augmented_negative = tf.contrib.image.rotate(image['negative'], angles=random.uniform(0, 360) * math.pi / 180)
    augmented_negative = tf.cast(augmented_negative, np.uint8)

    return dict(zip(feature_names, [augmented_query, augmented_positive, augmented_negative])), label

def train_input_fn(params):
    batch_size = params[""batch_size""]
    triplets = load_triplets()
    triplets_const = tf.constant(triplets)
    labels_const =tf.zeros([len(triplets)], tf.int32)
    dataset = tf.data.Dataset.from_tensor_slices((triplets_const, labels_const))
    dataset = dataset.shuffle(buffer_size=len(triplets))
    triplets.clear()
    dataset = dataset.map(my_input_fn)

    augmented_flip_left_right = dataset.map(random_flip_left_right)
    dataset = dataset.concatenate(augmented_flip_left_right)
    augmented_flip_up_down = dataset.map(random_flip_up_down)
    dataset = dataset.concatenate(augmented_flip_up_down)
    augmented_brightness = dataset.map(random_brightness)
    dataset = dataset.concatenate(augmented_brightness)
    augmented_contrast = dataset.map(random_contrast)
    dataset = dataset.concatenate(augmented_contrast)
    augmented_hue = dataset.map(random_hue)
    dataset = dataset.concatenate(augmented_hue)
    augmented_saturation = dataset.map(random_saturation)
    dataset = dataset.concatenate(augmented_saturation)
    augmented_rotate = dataset.map(random_rotate)
    dataset = dataset.concatenate(augmented_rotate)

    ds = dataset.cache().repeat().apply(tf.contrib.data.batch_and_drop_remainder(batch_size))
    triplets, labels = ds.make_one_shot_iterator().get_next()

    return triplets, labels


def convnet_model_():
    vgg_model = VGG16(weights=None, include_top=False)
    x = vgg_model.output
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(4096, activation='relu')(x)
    x = layers.Dropout(0.6)(x)
    x = layers.Dense(4096, activation='relu')(x)
    x = layers.Dropout(0.6)(x)
    x = layers.Lambda(lambda x_: keras.backend.l2_normalize(x_, axis=1))(x)
    convnet_model = models.Model(inputs=vgg_model.input, outputs=x)

    return convnet_model

_EPSILON = keras.backend.epsilon()
def _loss_tensor(y_true, y_pred):
    y_pred = keras.backend.clip(y_pred, _EPSILON, 1.0-_EPSILON)
    loss = tf.convert_to_tensor(0, dtype=tf.float32)
    g = tf.constant(1.0, shape=[1], dtype=tf.float32)
    for i in range(0, FLAGS.batch_size, 3):
        try:
            q_embedding = y_pred[i+0]
            p_embedding = y_pred[i+1]
            n_embedding = y_pred[i+2]
            D_q_p = keras.backend.sqrt(keras.backend.sum((q_embedding - p_embedding)**2))
            D_q_n = keras.backend.sqrt(keras.backend.sum((q_embedding - n_embedding)**2))
            loss = (loss + g + D_q_p - D_q_n)
        except:
            continue
    loss = loss / FLAGS.batch_size
    zero = tf.constant(0.0, shape=[1], dtype=tf.float32)

    return tf.maximum(loss, zero)


def model_fn(features, labels, mode, params):
    del params

    convnet_model = convnet_model_()
    first_input = layers.Input(shape=(225,225,3), name='low_visual1_input')
    first_subsample = layers.Conv2D(3, kernel_size=1, strides=4, padding='same', name='low_visual1_subsampling')(first_input)
    first_conv = layers.Conv2D(96, kernel_size=8,strides=4, padding='same', name='low_visual1_conv')(first_subsample)
    first_max = layers.MaxPool2D(pool_size=3,strides=4,padding='same', name='low_visual1_max')(first_conv)
    first_max = layers.Flatten(name='low_visual1_flatten')(first_max)
    second_input = layers.Input(shape=(225,225,3), name='low_visual2_input')
    second_subsample = layers.Conv2D(3, kernel_size=1, strides=8, padding='same', name='low_visual2_subsampling')(second_input)
    second_conv = layers.Conv2D(96, kernel_size=8,strides=4, padding='same', name='low_visual2_conv')(second_subsample)
    second_max = layers.MaxPool2D(pool_size=7,strides =2,padding='same', name='low_visual2_max')(second_conv)
    second_max = layers.Flatten(name='low_visual2_flatten')(second_max)
    merge_low_visual = layers.concatenate([first_max, second_max])
    l2_norm_low_visual = layers.Lambda(lambda x: K.l2_normalize(x, axis=1), name='low_visual_l2_normalization')(merge_low_visual)
    merge_two = layers.concatenate([l2_norm_low_visual, convnet_model.output])
    emb = layers.Dense(4096, name='linear_embedding')(merge_two)
    l2_norm_final = layers.Lambda(lambda  x: K.l2_normalize(x,axis=1), name='final_l2_normalization')(emb)
    final_model = models.Model(inputs=[convnet_model.input, first_input, second_input], outputs=l2_norm_final)
    final_model.summary()
    if mode == tf.estimator.ModeKeys.PREDICT:
        logits = final_model([features['query'], features['positive'], features['negative']], training=False)
        predictions = {
            'embedding': logits[0]
        }
        return tf.estimator.EstimatorSpec(
            mode=tf.estimator.ModeKeys.PREDICT,
            predictions=predictions,
            export_outputs={
                'classify': tf.estimator.export.PredictOutput(predictions)
            })
    logits_1 = final_model([features['query'], features['query'], features['query']], training=(mode == tf.estimator.ModeKeys.TRAIN))
    logits_2 = final_model([features['positive'], features['positive'], features['positive']], training=(mode == tf.estimator.ModeKeys.TRAIN))
    logits_3 = final_model([features['negative'], features['negative'], features['negative']], training=(mode == tf.estimator.ModeKeys.TRAIN))
    logits = tf.concat([logits_1, logits_2, logits_3], 0)
    loss = _loss_tensor(None, logits)
    optimizer = tf.train.MomentumOptimizer(learning_rate=FLAGS.learning_rate, momentum=0.9, use_nesterov=True)

    if FLAGS.use_tpu:
        optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)
    return tf.contrib.tpu.TPUEstimatorSpec(mode=tf.estimator.ModeKeys.TRAIN, loss=loss, train_op=optimizer.minimize(loss, tf.train.get_global_step()))


def main(argv):
    del argv
    tf.logging.set_verbosity(tf.logging.INFO)

    if FLAGS.master is None and FLAGS.tpu is None:
        raise RuntimeError('You must specify either --master or --tpu.')
    if FLAGS.master is not None:
        if FLAGS.tpu is not None:
            tf.logging.warn('Both --master and --tpu are set. Ignoring '
                      '--tpu and using --master.')
        tpu_grpc_url = FLAGS.master
    else:
        tpu_cluster_resolver = (tf.contrib.cluster_resolver.TPUClusterResolver(FLAGS.tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project))
        tpu_grpc_url = tpu_cluster_resolver.get_master()

    run_config = tpu_config.RunConfig(master=tpu_grpc_url, model_dir=FLAGS.model_dir, save_checkpoints_secs=3600, session_config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True), tpu_config=tpu_config.TPUConfig(iterations_per_loop=FLAGS.iterations, num_shards=FLAGS.num_shards),)
    estimator = tpu_estimator.TPUEstimator(model_fn=model_fn, use_tpu=FLAGS.use_tpu, config=run_config, train_batch_size=FLAGS.batch_size)
    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.train_steps)
    img_input = tf.placeholder(tf.float32, [None, 225, 225, 3])
    input_fn =tf.estimator.export.build_raw_serving_input_receiver_fn({
        'query': img_input,
        'positive': img_input,
        'negative': img_input,
    })
    estimator.export_savedmodel('output', input_fn)


if __name__ == ""__main__"":
    tf.app.run()
```
I get the following error:

```
Traceback (most recent call last):
  File ""test.py"", line 370, in <module>
    tf.app.run()
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""test.py"", line 359, in main
    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.train_steps)
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 376, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 1143, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 1168, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 2162, in _call_model_fn
    features, labels, mode, config)
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 1131, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 2414, in _model_fn
    _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 2724, in _train_on_tpu_system
    device_assignment=ctx.device_assignment)
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/tpu.py"", line 829, in shard
    name=name)
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/tpu.py"", line 475, in replicate
    device_assignment, name)[1]
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/tpu.py"", line 635, in split_compile_and_replicate
    outputs = computation(*computation_inputs)
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 2717, in multi_tpu_train_steps_on_single_shard
    single_tpu_train_step, [_INITIAL_LOSS])
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py"", line 207, in repeat
    cond, body_wrapper, inputs=inputs, infeed_queue=infeed_queue, name=name)
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py"", line 169, in while_loop
    name="""")
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3232, in while_loop
    return_same_structure)
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2952, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2924, in _BuildLoop
    next_vars.append(_AddNextAndBackEdge(m, v))
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 666, in _AddNextAndBackEdge
    _EnforceShapeInvariant(m, v)
  File ""/home/julien_plu/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 610, in _EnforceShapeInvariant
    (input_t.name, input_t.shape, n_shape))
ValueError: Input tensor 'Const_1:0' enters the loop with shape (), but has shape (1,) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.
```
While if I run the exact same code with ```--use_tpu=False --master=''``` it works like a charm.

Is it somehow related to a bug or is it me who is doing something wrong with dedicated TPU code?

Thanks in advance."
20745,ar,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20743,Tensorflow C++ GPU support,"

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Window 10 64Bit
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.7.0-rc1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:GTX960
- **Exact command to reproduce**:N/A

### Describe the problem
I have successfully compiled the tensorflow1.7C++ version on windows10.In the prediction, I found that GPU is not used in the program. Is the GPU not supported by the tensorflowC++ interface?I am very anxious and hope to get help. Thank you very much.

"
20741,TFLite: Cannot use boolean scalar as input to TFLite model,"Edit: Adding issue template
Have I written custom code: No
OS Platform and Distribution: Ubuntu 14.04 
TensorFlow installed from: pip
TensorFlow version: r1.9
Bazel version
CUDA/cuDNN version
GPU model and memory
Exact command to reproduce: tf.contrib.lite.toco_convert (More details below)


I am trying to convert the [Facenet Tensorflow](https://github.com/davidsandberg/facenet) model into `TFLITE` format which I could use for an Android app. This Facenet model takes a tensor containing image data and a boolean scalar (is_training) as input:

  `  feed_dict = { images_placeholder: images, phase_train_placeholder:False }`

I follow the [Tensorflow lite guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/mobile/tflite/devguide.md) to convert models into TFLITE format. Here is an example: 

   ```
 import tensorflow as tf
    
    img = tf.placeholder(name=""img"", dtype=tf.float32, shape=(1, 64, 64, 3))
    val = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])
    out = tf.identity(val, name=""out"")
    
    with tf.Session() as sess:
      tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out])
      open(""converteds_model.tflite"", ""wb"").write(tflite_model)

```
Here is my code:

   ```
 with tf.Graph().as_default():
    
            with tf.Session() as sess:
    
                # Load the model
                facenet.load_model(args.model)
    
                # Get input and output tensors
                images_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")
                embeddings = tf.get_default_graph().get_tensor_by_name(""embeddings:0"")
                phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(""phase_train:0"")
                #reshape as tflite does not accepts None dimension
                images_placeholder = tf.reshape(images_placeholder, [1,160,160,3])
                tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [images_placeholder, phase_train_placeholder], [embeddings])
                open(""converteds_model.tflite"", ""wb"").write(tflite_model)
```

The program gives an error saying (I guess this happens as `tflite` does not support `tf.bool` just yet):

```
    File ""/.../facenet/tensorflow1.8/lib/python3.4/site-packages/tensorflow/contrib/lite/python/convert.py"", line 206, in toco_convert
        input_tensor.dtype))
    ValueError: Tensors phase_train:0 not known type tf.bool

```
If I cast the `phase_train_placeholder` to the supported type `tf.int32`, just to see how it goes. Then it gives another error (I guess this happens as the convert function does not accept scalar):

 

```
File ""/.../facenet/tensorflow1.8/lib/python3.4/site-packages/tensorflow/contrib/lite/python/convert.py"", line 217, in toco_convert
        input_array.shape.dims.extend(map(int, input_tensor.get_shape()))
      File ""/.../facenet/tensorflow1.8/lib/python3.4/site-packages/tensorflow/python/framework/tensor_shape.py"", line 591, in __iter__
        raise ValueError(""Cannot iterate over a shape with unknown rank."")
    ValueError: Cannot iterate over a shape with unknown rank.
```

Could you please suggest an workaround for this? I understand that TFLite is still in development but I could try to contribute this part if it is possible. Many thanks.
 
I also posted to [SO](https://stackoverflow.com/questions/51307107/tensorflow-lite-boolean-scalar-as-input-tensor); however I think this is also an issue therefore I put it here as well.  "
20740,Should the loading model be in a separate thread?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7.0
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**:  4.8.5
- **CUDA/cuDNN version**: 5.1.10
- **GPU model and memory**: p40 etc
- **Exact command to reproduce**: 

### Describe the problem

Now, loading new model is using default thread pool, this may affect predict latency in serving.

Sorry to disturb, I have addressed this problem in https://github.com/tensorflow/serving/issues/910, but I think the root cause is in tensorflow's code.

Should we provide a config for serving to use a separate thread pool when loading new model versions?

I can provide a solution.
"
20739,"Tensorflow lite, invalid quantization ranges","### System information
- **Have I written custom code**: Y
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:  source
- **TensorFlow version (use command below)**: v1.9.0-rc1-48-ge3f2b5903c 1.9.0-rc2
- **Python version**: 3.5.2
- **Bazel version**: release 0.15.0
- **GCC/Compiler version**: 5.5
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A  

### Describe the problem  
Hello! I am not really sure is it a bug or just my misunderstanding. According to the quantized training [tutorial](https://www.tensorflow.org/performance/quantization) command `create_eval_graph` is supposed to generate toco-ready graph with fake quantization nodes inside. For weights it's done via the following command:  
```python 
    _InsertQuantOp(
        context,
        'weights_quant',
        layer_match.weight_tensor.op, [layer_match.layer_op],
        is_training,
        moving_avg=False,
        ema_decay=ema_decay,
        quant_delay=quant_delay,
        narrow_range=True, # [!!!!!!]
        vars_collection=vars_collection,
        bits=weight_bits,
        consumer_scope=scope)
```  
Here narrow_range parameter forces mapping of float values to the [1, 255] range. However, as far as I was able to understand, all tensors quantization(weights included) is performed using function [ChooseQuantizationParams](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/internal/quantization_util.h) which assumes, that available range is [0, 255](for uint8). Hence, float weights are mapped to values that might be different then ones that was used during training. In my situation it leads to the significant difference(deep model) between output of the Tensorflow fake-quantized graph and Tensorflow lite model.  Difference is about 1e-1 for a single layer.

Described is true for the --inference_type=QUANTIZED_UINT8 toco parameter, not sure about --inference_type=FLOAT and --quantize_weights=true(probably the same).

### Source code / logs  
I will prepare code snippets to reproduce in case I am right and described behavior can be considered as unexpected.   "
20738,AttributeError: 'DataFrame' object has no attribute 'dtype',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **I'm using the FashionMNIST code tutorial in  [ https://goo.gl/GtM6ov](https://goo.gl/GtM6ov)**:
- **Mac OS 10.13.3**:
- **Source**:
- **TensorFlow version (1.8.0)**:
- *Python 3.6.5 :: Anaconda, Inc.**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:

train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""pixels"": features.values/255},
        y=labels,
        batch_size=100,
        num_epochs=3,
        shuffle=True)
feature_columns = [tf.feature_column.numeric_column(""pixels"", shape=784)]

classifier = tf.estimator.LinearClassifier(
    feature_columns = feature_columns,
    n_classes = 10,
    model_dir = ""./models/linear1""
)

classifier.train(input_fn=train_input_fn)


--------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-84-5c962f10a4d9> in <module>()
----> 1 classifier.train(input_fn=train_input_fn)

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    361 
    362     saving_listeners = _check_listeners_type(saving_listeners)
--> 363     loss = self._train_model(input_fn, hooks, saving_listeners)
    364     logging.info('Loss for final step: %s.', loss)
    365     return self

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
    841       return self._train_model_distributed(input_fn, hooks, saving_listeners)
    842     else:
--> 843       return self._train_model_default(input_fn, hooks, saving_listeners)
    844 
    845   def _train_model_default(self, input_fn, hooks, saving_listeners):

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)
    851       features, labels, input_hooks = (
    852           self._get_features_and_labels_from_input_fn(
--> 853               input_fn, model_fn_lib.ModeKeys.TRAIN))
    854       worker_hooks.extend(input_hooks)
    855       estimator_spec = self._call_model_fn(

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _get_features_and_labels_from_input_fn(self, input_fn, mode)
    689   def _get_features_and_labels_from_input_fn(self, input_fn, mode):
    690     """"""Extracts the `features` and labels from return values of `input_fn`.""""""
--> 691     result = self._call_input_fn(input_fn, mode)
    692     # TODO(anjalisridhar): What about the default DistributionStrategy? Perhaps
    693     # using any input is alright in that case. There is also a

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _call_input_fn(self, input_fn, mode)
    796       kwargs['config'] = self.config
    797     with ops.device('/cpu:0'):
--> 798       return input_fn(**kwargs)
    799 
    800   def _call_model_fn(self, features, labels, mode, config):

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/numpy_io.py in input_fn()
    194         num_threads=num_threads,
    195         enqueue_size=batch_size,
--> 196         num_epochs=num_epochs)
    197 
    198     batch = (

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py in _enqueue_data(data, capacity, shuffle, min_after_dequeue, num_threads, seed, name, enqueue_size, num_epochs, pad_value)
    390     elif isinstance(data, collections.OrderedDict):
    391       types = [dtypes.int64
--> 392               ] + [dtypes.as_dtype(col.dtype) for col in data.values()]
    393       queue_shapes = [()] + [col.shape[1:] for col in data.values()]
    394       get_feed_fn = _OrderedDictNumpyFeedFn

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py in <listcomp>(.0)
    390     elif isinstance(data, collections.OrderedDict):
    391       types = [dtypes.int64
--> 392               ] + [dtypes.as_dtype(col.dtype) for col in data.values()]
    393       queue_shapes = [()] + [col.shape[1:] for col in data.values()]
    394       get_feed_fn = _OrderedDictNumpyFeedFn

~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in __getattr__(self, name)
   4370             if self._info_axis._can_hold_identifiers_and_holds_name(name):
   4371                 return self[name]
-> 4372             return object.__getattribute__(self, name)
   4373 
   4374     def __setattr__(self, name, value):

AttributeError: 'DataFrame' object has no attribute 'dtype'

This is the error I'm getting."
20737,Extend the support of the exponential distribution to include 0,"The support of the [Exponential Distribution](https://en.wikipedia.org/wiki/Exponential_distribution) includes 0. However in tensorflow (v1.8 compiled from source on ubuntu 16.04 using the cpu) it does not. Example:
```
import tensorflow as tf
exp = tf.distributions.Exponential(rate=1./2.)
llh = exp.log_prob(value=0.) 

with tf.Session() as sess:
    print(sess.run(llh))
```
Result:
`nan`

In that case Tensorflow differs to scipy's behaviour:
```
import scipy as sp
sp.stats.expon(scale=2.).logpdf(0.)
```
Result:
`-0.6931471805599453`

Would be great to adopt scipy's behaviour. Thank you

EDIT:
Have I written custom code: No
OS Platform and Distribution: Ubuntu 16.04
TensorFlow installed from: source
TensorFlow version: 1.8.0
Bazel version: Build label: 0.11.1
CUDA/cuDNN version: -/- (using CPU)
GPU model and memory: - (using CPU)
Exact command to reproduce: see above
Python:  3.6.3"
20732,Getting KeyError: u'LSTMBlockCell while loading graph,"I have trained an LSTM model using tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell when I tried to load that model I got KeyError: u'LSTMBlockCell' and it was working fine when I was using Basic LSTM cell.
Tensorflow-gpu - 1.8.0

Error Log

File ""<stdin>"", line 1, in <module>
  File ""/home/abhay/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1955, in import_meta_graph
    **kwargs)
  File ""/home/abhay/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py"", line 743, in import_scoped_meta_graph
    producer_op_list=producer_op_list)
  File ""/home/abhay/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/home/abhay/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 460, in import_graph_def
    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)
  File ""/home/abhay/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 227, in _RemoveDefaultAttrs
    op_def = op_dict[node.op]
KeyError: u'LSTMBlockCell'"
20731,Multiple dex files define Lorg/tensorflow/lite/NativeInterpreterWrapper;,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No, but have added images and used TOCO to make it an android app.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MacOS 10.12.6

- **TensorFlow installed from (source or binary)**:
anaconda3

- **TensorFlow version (use command below)**:
v1.7.1-0-gc8137f3a8e 1.7.1, as TOCO wouldn't work on 1.8

- **Python version**: 
Python 3.6.5 :: Anaconda, Inc.

- **Bazel version (if compiling from source)**:
N/A

- **GCC/Compiler version (if compiling from source)**:
N/A

- **CUDA/cuDNN version**:
N/A

- **GPU model and memory**:
Intel HD 6000 
8GB DDR3

- **Exact command to reproduce**:
N/A

### Describe the problem
When trying to build the ""tflite"" app from tensorflow-for-poets2, I encounter an error; `Multiple dex files define Lorg/tensorflow/lite/NativeInterpreterWrapper;`. I have tried updating the SDK and finding a duplicate of the file defining NativeInterpreterWrapper (with no luck).

### Source code / logs

Error log:
```
org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':app:transformDexArchiveWithExternalLibsDexMergerForDebug'.
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:100)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:70)
	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:63)
	at org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:54)
	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58)
	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88)
	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:52)
	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52)
	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54)
	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)
	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34)
	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.run(DefaultTaskGraphExecuter.java:248)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:336)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:328)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:197)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:107)
	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:241)
	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:230)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.processTask(DefaultTaskPlanExecutor.java:124)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.access$200(DefaultTaskPlanExecutor.java:80)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:105)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:99)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.execute(DefaultTaskExecutionPlan.java:625)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.executeWithTask(DefaultTaskExecutionPlan.java:580)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.run(DefaultTaskPlanExecutor.java:99)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: com.android.builder.dexing.DexArchiveMergerException: Unable to merge dex
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1431)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinTask.externalAwaitDone(ForkJoinTask.java:326)
	at java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:391)
	at java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:719)
	at java.util.ArrayList.forEach(ArrayList.java:1249)
	at com.android.builder.dexing.DxDexArchiveMerger.mergeMultidex(DxDexArchiveMerger.java:266)
	at com.android.builder.dexing.DxDexArchiveMerger.mergeDexArchives(DxDexArchiveMerger.java:133)
	at com.android.build.gradle.internal.transforms.DexMergerTransformCallable.call(DexMergerTransformCallable.java:97)
	at com.android.build.gradle.internal.transforms.ExternalLibsMergerTransform.transform(ExternalLibsMergerTransform.kt:121)
	at com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:222)
	at com.android.build.gradle.internal.pipeline.TransformTask$2.call(TransformTask.java:218)
	at com.android.builder.profile.ThreadRecorder.record(ThreadRecorder.java:102)
	at com.android.build.gradle.internal.pipeline.TransformTask.transform(TransformTask.java:213)
	at sun.reflect.GeneratedMethodAccessor524.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)
	at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$IncrementalTaskAction.doExecute(DefaultTaskClassInfoStore.java:173)
	at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.execute(DefaultTaskClassInfoStore.java:134)
	at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.execute(DefaultTaskClassInfoStore.java:121)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActionsTaskExecuter.java:122)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:336)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:328)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:197)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:107)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:111)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:92)
	... 30 more
Caused by: com.android.builder.dexing.DexArchiveMergerException: Unable to merge dex
	at com.android.builder.dexing.DexArchiveMergerCallable.call(DexArchiveMergerCallable.java:72)
	at com.android.builder.dexing.DexArchiveMergerCallable.call(DexArchiveMergerCallable.java:36)
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424)
	... 57 more
Caused by: com.android.dex.DexException: Multiple dex files define Lorg/tensorflow/lite/NativeInterpreterWrapper;
	at com.android.dx.merge.DexMerger.readSortableTypes(DexMerger.java:661)
	at com.android.dx.merge.DexMerger.getSortedTypes(DexMerger.java:616)
	at com.android.dx.merge.DexMerger.mergeClassDefs(DexMerger.java:598)
	at com.android.dx.merge.DexMerger.mergeDexes(DexMerger.java:171)
	at com.android.dx.merge.DexMerger.merge(DexMerger.java:198)
	at com.android.builder.dexing.DexArchiveMergerCallable.call(DexArchiveMergerCallable.java:61)
	... 59 more
```
"
20730,tensorflow uses recomputer technology,"System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary):source
TensorFlow version (use command below):1.8.0
Python version: 3.5
Bazel version (if compiling from source):0.10.0
GCC/Compiler version (if compiling from source): c++11
CUDA/cuDNN version: 9/7
GPU model and memory: gtx 1080ti, 11G
Exact command to reproduce:N/A
Describe the problem:

In order to understand how much gpu memory resources tensorflow can economize, I did some experiments.
My network is vgg-16 and dataset is mnist, batch size is 128. i estimate my gpu memory usage is 2.8GB.
I close  all the RewriterConfig like:
  rewrite_options.memory_optimization = rewriter_config_pb2.RewriterConfig.NO_MEM_OPT
  rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF
  rewrite_options.function_optimization = rewriter_config_pb2.RewriterConfig.OFF
  rewrite_options.constant_folding = rewriter_config_pb2.RewriterConfig.OFF
  rewrite_options.arithmetic_optimization = rewriter_config_pb2.RewriterConfig.OFF
  rewrite_options.loop_optimization = rewriter_config_pb2.RewriterConfig.OFF
  rewrite_options.dependency_optimization = rewriter_config_pb2.RewriterConfig.OFF
And i gradually reduce the amount of gpu memory usage :

GPU   memory usage | 2.883 | 2.5 | 2 | 1.5 | 1.307
-- | -- | -- | -- | -- | --
average(iter   5000) | 337.161 | 336.968 | 339.018 | 338.409 | 412.119
average(iter   50000) | 3350.804 | 3352.093 | 3358.742 | 3373.366 | 4116.673

I observed that the method used by tensorflow is recomputer. Why use recomputer technology instead of swapping technology? Is recompute faster than swapping? or ?
In the memory_optimizer.cc:1226, it has RecomputationRewritingPass function. After I close the function to test, i find that tensorflow has other block do same event.
Why tensorflow has two block to do same event ?

Thanks.
"
20729,Person Detection issue in Android App Using tenserflow Lite,"Hi i am Using Android Detection Example App , here  i want to monitor a person in or out activity.
but my problem is it take more time to detect when a person walk ,some time cnt detect person.
 another problem is that after detecting cant track properly ..
 is this is the issue of camera video quality..? 
how can i change the quality and  brightness of  camera in this app..?

"
20728,"An error has occurred,when build the tensorflow demo using Android Studio","
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)*1.8*:
- **Python version*2.7*: 
- **Bazel version (if compiling from source)*0.15.0*:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory*non*:
- **Exact command to reproduce**:
refer link: https://www.tensorflow.org/mobile/android_build
**### Error info, as follow:**
Proxy Authentication Required ( Forefront TMG requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )

org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':downloadFile'.
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:100)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:70)
	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:63)
	at org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:54)
	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58)
	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88)
	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:52)
	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52)
	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54)
	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)
	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34)
	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.run(DefaultTaskGraphExecuter.java:248)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:336)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:328)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:197)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:107)
	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:241)
	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:230)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.processTask(DefaultTaskPlanExecutor.java:124)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.access$200(DefaultTaskPlanExecutor.java:80)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:105)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:99)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.execute(DefaultTaskExecutionPlan.java:625)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.executeWithTask(DefaultTaskExecutionPlan.java:580)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.run(DefaultTaskPlanExecutor.java:99)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.gradle.api.UncheckedIOException: org.apache.http.client.ClientProtocolException: Proxy Authentication Required ( Forefront TMG requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )
	at org.gradle.internal.UncheckedException.throwAsUncheckedException(UncheckedException.java:57)
	at org.gradle.internal.UncheckedException.throwAsUncheckedException(UncheckedException.java:40)
	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:76)
	at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.doExecute(DefaultTaskClassInfoStore.java:141)
	at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.execute(DefaultTaskClassInfoStore.java:134)
	at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.execute(DefaultTaskClassInfoStore.java:121)
	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:731)
	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:705)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActionsTaskExecuter.java:122)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:336)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:328)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:197)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:107)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:111)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:92)
	... 30 more
Caused by: org.apache.http.client.ClientProtocolException: Proxy Authentication Required ( Forefront TMG requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )
	at de.undercouch.gradle.tasks.download.DownloadAction.openConnection(DownloadAction.java:400)
	at de.undercouch.gradle.tasks.download.DownloadAction.executeHttpProtocol(DownloadAction.java:202)
	at de.undercouch.gradle.tasks.download.DownloadAction.execute(DownloadAction.java:176)
	at de.undercouch.gradle.tasks.download.DownloadAction.execute(DownloadAction.java:127)
	at de.undercouch.gradle.tasks.download.Download.download(Download.java:64)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)
	... 42 more

"
20727,"tf.contrib.seq2seq.AttentionWrapperState TypeError: __new__() missing 4 required positional arguments: 'time', 'alignments', 'alignment_history', and 'attention_state'","in the tensorflow r1.8:
```
 tf.contrib.seq2seq.AttentionWrapperState(cell_state, attention, time, alignments, alignment_history,attention_state)
```
in the tensorflow r1.2 
```
 tf.contrib.seq2seq.AttentionWrapperState(cell_state, attention, time, alignments, alignment_history,attention_state)
```

my code is r1.2, but now i want to run it in the r1.8.
```
 initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],
                                                             _zero_state_tensors(rnn_size, 
                                                                                 batch_size, 
                                                                                 tf.float32)) 
 with tf.variable_scope(""decode""):
        training_logits = training_decoding_layer(dec_embed_input, 
                                                  summary_length, 
                                                  dec_cell, 
                                                  initial_state,
                                                  output_layer,
                                                  vocab_size, 
                                                  max_summary_length)
    with tf.variable_scope(""decode"", reuse=True):
        inference_logits = inference_decoding_layer(embeddings,  
                                                    vocab_to_int['<GO>'], 
                                                    vocab_to_int['<EOS>'],
                                                    dec_cell, 
                                                    initial_state, 
                                                    output_layer,
                                                    max_summary_length,
                                                    batch_size)
```
who can help me ?
because the number of the argument of the AttentionWrapperState in the r1.2 is the same to the r1.8 , why it occurs error in the r1.8 , but well in the r1.2?
"
20726,"How to get float type output data from ""dummy-quantization"" model?","
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.8.0
- **Python version**: 2.7.3
- **Bazel version (if compiling from source)**:0.12.0
- **GCC/Compiler version (if compiling from source)**: c++11
- **CUDA/cuDNN version**:7.5.18
- **GPU model and memory**:TITAN,12GB
- **Exact command to reproduce**:N/A


### Describe the problem
I use ""dummy-quantization"" to generate the quantization type model of deeplabv3(mobilenetv2), the command like below:
`
 --input_format=TENSORFLOW_GRAPHDEF \
  --output_format=TFLITE \
  --inference_type=QUANTIZED_UINT8 \
  --input_shape=1,513,513,3 \
  --input_array=sub_7 \
  --output_array=logits/semantic/BiasAdd \
  --default_ranges_min=0 \
  --default_ranges_max=6 \
  --mean_value=127.5 \
  --std_value=127.5`
I want to run this model on the phone, but the output_array is not the last node in model, and I need to get the float type output data of ""logits/semantic/BiasAdd "". Now I use `out_ptr_uint8_t = interpreter->typed_output_tensor<uint8_t>(0);`  to get the uint8_t type output data, can you help me with how to get float type data?


"
20724,TF 1.3 with AMD ROCm build error on ARM64 Ubuntu 16.04,"I meet error at ARM64 ubuntu 16.04 with AMD ROCm and TF 1.3 https://github.com/ROCmSoftwarePlatform/tensorflow

root@:~/tensorflow# bazel build --config=opt --config=rocm //tensorflow/tools/pip_package:build_pip_package --verbose_failures
ERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/io_bazel_rules_closure/closure/filegroup_external.bzl:23:16: name 'set' is not defined
ERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/io_bazel_rules_closure/closure/webfiles/web_library.bzl:43:14: name 'set' is not defined
ERROR: error loading package '': Extension 'closure/filegroup_external.bzl' has errors
ERROR: error loading package '': Extension 'closure/filegroup_external.bzl' has errors
INFO: Elapsed time: 4.595s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)"
20723,Mismatched Gradient Update with `tf.gather` Operation,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7
- **Python version**: 2.7.14 (Anaconda)
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 7.0
- **GPU model and memory**: NVIDIA GTX 1080Ti
- **Exact command to reproduce**: See below

### Describe the problem
I met the problem when I use tf.gather to slice into a 1D `codebook`  with the specified index, and generate an output Tensor. The loss function is determined based on the generated output Tensor (In the following code, I simply adopt the reduced sum of output Tensor). Then an optimization phase will be applied to fine tune the `codebook` to minimize the loss. The issue happens there is a mismatched updated codebook results when I use built-in `GradientDescentOptimizer` and manually defined one step update results.

### Source code / logs
```python
import numpy as np
import tensorflow as tf

# Generate the synthetic codebook and index for testing
N = 16
shape = (4000, 4000)
np.random.seed(121)
codebook_np = np.random.randn(N)
index_np = np.random.randint(low=0, high=N, size=shape)
lr = 0.001

# Simple computation graph
codebook = tf.get_variable('codebook', shape=[N], dtype=tf.float32, 
                                              initializer=tf.constant_initializer(codebook_np))
index = tf.constant(index_np, name='index', dtype=tf.int64)
output = tf.gather(codebook, index, name='output')
loss = tf.reduce_sum(output, name='loss')
[grad_codebook] = tf.gradients(loss, codebook)
grad_codebook_dense = tf.convert_to_tensor(grad_codebook, name='grad_codebook_dense')
codebook_update = codebook - lr * grad_codebook_dense
opt = tf.train.GradientDescentOptimizer(lr)
train_op = opt.minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

print('codebook [update]')
print(sess.run(codebook_update))
_ = sess.run(train_op)
print('codebook [train_opt]:')
print(sess.run(codebook))
```
I expect to see these 2 updated codebook be same. However, the logs are shown belowed:
```
codebook [update]
[-1000.6481   -999.8909  -1001.59094 -1002.16235  -999.7602   -998.70636
  -998.9787   -999.86896  -999.7241   -998.3734   -997.3994  -1000.765
  -999.6122  -1000.2615   -997.9392  -1002.35077]
codebook [train_opt]:
[-991.77435 -991.03503 -992.6951  -993.2531  -990.9072  -989.8781
 -990.1441  -991.01355 -990.8721  -989.553   -988.60187 -991.8886
 -990.7627  -991.3968  -989.12897 -993.43713]
```
Roughly 1% difference between the updated codebook. The updated code obtained from `train_opt` is always larger than the manually calculated value. 
"
20722,TFSA-2018-001 commit link is invalid,"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2018-001.md

Links to:
https://github.com/tensorflow/tensorflow/commit/49f73c55d56edffebde4bca4a407ad69c1cae4333c55

Which is 404. Can you please update this with the correct commit? Thanks!"
20718,Optimized model is slow on Android ,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android 7.0 
- **TensorFlow installed from (source or binary)**: Binary 
- **TensorFlow version (use command below)**: Tensorflow Android 1.6 with Java TFInferenceInterface
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**: 
- **Exact command to reproduce**: Please see below. 

### Describe the problem
I am using graph transformation tool to optimize SSD model. Here is the command I used to strip the nodes, quantize the weights and for other optimization methods. 
```markdown
bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=/workspace/frozen_inference_graph.pb --out_graph=/workspace/frozen_inference_graph_opti.pb \
      --inputs='image_tensor:0' --outputs='detection_boxes:0,detection_scores:0,detection_classes:0,num_detections:0' \
          --transforms='strip_unused_nodes(type=float, shape=""1,299,299,3"") remove_nodes(op=Identity, op=CheckNumerics)\
              fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights'
```
The model size changed from 53 MB to 13 MB, new optmization model is working perfectly fine on Ubundu 16.04 and Windows 10 with the following numbers (without the GPU) : 
# On Ubuntu 16.04 & Windows 10 : 
Before the Optimization :  
Inference time = 616 mSec/image 
After the optmization : 
Inference time =200 mSec /image
As you can observe that, there is clear three fold speed boost with the optimized model without compromising the accuracy. However, on android inference time remains same after the optmization. 
# On Android 
Before the optmization : 
Inference time = 900 mSec/image
After the optimization : 
Inference time = 890 mSec/image 

The mobile device I am using is Galaxy Tab S2 8.0 with Octa-core processor. 
I have digged most of the issues here but those are either issues with slower model on one OS or some sort of error with optimized model. This is the first time I am seeing this strange behavior when moving to mobile plateform.  

 Let me know if you need more information. 
Thanks in advance. 
"
20717,go api speed up array of frames,"I have raw decoded video frame format *image.YCbCr

but for work I need to encode it to jpeg or png

op.DecodeJpeg
op.DecodePng

this operation takes about 20 ms

can I somehow convey a raw image so as not to waste this time?

or maybe I can convert to rgb and send pix() to  tensorflow op

It's really not convenient to code to decode and slow down the work with an array of frames; (


"
20714,Seq2seq tutorial dissapeared,"Hi, 

[https://www.tensorflow.org/tutorials/seq2seq](https://www.tensorflow.org/tutorials/seq2seq) tutorial returns 404 not found. This is where an extensive tutorial by Luong et al was located until several days ago, I assume it was deleted or moved by mistake. 
"
20703,Character access is not supported in tf.contrib.autograph,"For example this is not working and it returns an error
```
@autograph.convert()
def is_hashtag(x):
  if x[0] == ""#"":
    return True
  return False

with tf.Graph().as_default():  
  with tf.Session() as sess:
    print(sess.run(is_hashtag(tf.constant(""#asdf""))))
```"
20702,[Improvement] Make tf.contrib.model_pruning.masked_conv2d API compatible with tf.layers.conv2d,"TensorFlow version: v1.8.0-0-g93bc2e2072 1.8.0

### Describe the problem
I know contrib isn't officially supported, but I don't know who I should request this to:
masked_conv2d is intended to be a plug-in replacement of conv2d, but argument names are different. Some simples changes such as: num_outputs->filters, scope->name etc should be enough to make it compatible"
20701,Eager execution does not work for R interface under Python 3,"Hi there, I am the maintainer of the R interface to TensorFlow. We are currently in the process of porting various Eager examples to R. We haven't had trouble with Python 2 versions of TensorFlow, but with Python 3 versions we get some strange errors. 

I realize that this is within the R interface so technically falls outside of the scope of TF for Python. However, in order for us to address this we need some insight as to what might be different for Eager under Python 3. I'll provide a detailed repro and explanation of it's under the hood behavior below.

cc @martinwicke @random-forests 

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: TensorFlow v1.10.0-dev20180710 
- **Python version**:  3.6.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See below

### Describe the problem

Using the R interface to TensorFlow:

```r
library(tensorflow)
tf$enable_eager_execution()
x <- tf$constant(1)
tf$add(x, x)
```

Results in this error:

```
SystemError: <built-in function TFE_Py_FastPathExecute> returned a result with an error set 
```

This error occurs within the definition of `add()` within `gen_math_ops.py`:

```python
  _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(
        _ctx._context_handle, _ctx._eager_context.device_name, ""Add"", name,
        _ctx._post_execution_callbacks, x, y)
```

This code works as expected under TF w/ Python 2.

Again, I realize that this is the R interface so you might not have an intuition about what could be wrong. You can think of the R interface conceptually as just using the C Python API to invoke functions. So in the above code we are essentially using:

- `PyImport_Import` to import the tensorflow module
- `PyObject_CallFunctionObjArgs` to call Python functions (e.g. `tf.enable_eager_execution`, `tf.constant`, etc.)

My theory is that under Python 3 there is something being done at the Python language level that we aren't emulating or capture when calling through the Python C interface.  Hopefully this provides you with some clues as to what that might be and we will be able to make whatever changes are required to make this work within R.

"
20699,Increasing memory use of Estimator with Dataset,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: Binary (pip install)
- **TensorFlow version (use command below)**: Tested on 1.8.0 and 1.10.0-dev20180620
- **Python version**: 3.6
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: GTX1050
- **Exact command to reproduce**: See below for minimal test case

### Describe the problem
I'm experiencing a memory leak that leaves some allocated memory behind after each Estimator training/evaluation run. I noticed this when my ML Engine jobs started failing after 1.5 hours of training with an OOM error on big CSV files, using the standard Dataset input pipeline with some preprocessing, batching, prefetching etc. The memory graph shows slow but steady increase of memory use until total RAM is filled. I observe the same behavior on my local machine.

I've distilled it down to a minimal test case with use of Estimator and Dataset input and tested on my local machine, where I also get slowly increasing memory use over time. It seems most pronounced when using a big shuffle buffer, but after all this time staring at it I'm not so sure of anything any more.

Test script (first `pip install -U memory_usage`):
```
import tempfile, time, argparse, os

# Install with pip install -U memory_usage
from memory_profiler import memory_usage
import tensorflow as tf
from tensorflow.python.client import device_lib

parser = argparse.ArgumentParser()
parser.add_argument('--apply-shuffle-repeat', action='store_true')
parser.add_argument('--shuffle', action='store_true')
parser.add_argument('--gpu', action='store_true')
args = parser.parse_args()

# Turn on/off gpu
if not args.gpu:
    print(""Turning off GPU"")
    os.environ[""CUDA_VISIBLE_DEVICES""]=""""

print(f""Tensorflow version: {tf.GIT_VERSION} {tf.VERSION}"")

print(f""Devices: {device_lib.list_local_devices()}"")

tmpdir = tempfile.mkdtemp()

_input = [1] * 5000000
_target = [1] * 5000000

def input_fn(batch_size=4096):
    data = tf.data.Dataset.from_tensor_slices(
        ({'input': _input}, _target))
    if args.shuffle:
        data = data.shuffle(1000000)
    elif args.apply_shuffle_repeat:
        data = data.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=1000000, count=1))
    data = data.batch(batch_size)
    return data

estimator = tf.estimator.DNNRegressor(
    [64],
    [tf.feature_column.numeric_column(key='input')],
    model_dir=tmpdir)

train_spec = tf.estimator.TrainSpec(input_fn=input_fn)

eval_spec = tf.estimator.EvalSpec(input_fn=input_fn)

# Train and evaluate forever
tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
```

Output of `mprof run tf_estimator_memoryleak_minimal.py --apply-shuffle-repeat
` (so it runs on CPU, for GPU use `--gpu` option) and plotting with `mprof plot` shows memory usage slowly but surely creeping upward:
![tf-memoryuse](https://user-images.githubusercontent.com/5527529/42563920-b7986b62-84ff-11e8-9fa4-304f319927a0.png)

I would expect memory usage after every train/evaluate loop to return to the initial level.

Am I using the Dataset in the wrong way? Should I convert to an Iterator first? Is slightly increasing memory use normal and should I restart my train script every once in a while?
"
20698,tf.keras multi input models don't work when using tf.data.Dataset,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.5 and Debian GNU/Linux 9 (stretch)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.9.0-rc2-359-g95cfd8b3d9 1.10.0-dev20180711 also reproduces on v1.9.0
- **Python version**: 3.6.5 and 3.5.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: see below

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
`tf.keras` multi input models don't work when used together with `tf.data.Dataset` due to input broken validation checks. This problem reproduces both on tf@1.9.0 and the latest nightly.

@fchollet Do you have any ideas what's going on here, or am I missing something obvious?

### Source code / logs

#### Multi input model
Consider the following toy model:
```python
import numpy as np
import tensorflow as tf
from tensorflow import keras

data_a = np.array([300, 455, 350, 560, 700, 800, 200, 250], dtype=np.float32)
labels = np.array([455, 350, 560, 700, 800, 200, 250, 300], dtype=np.float32)
data_b = np.array([200, 255, 350, 470, 600, 300, 344, 322], dtype=np.float32)
data_a = np.reshape(data_a, (8, 1, 1))
data_b = np.reshape(data_b, (8, 1, 1))

x = keras.layers.Input(shape=(1, 1), name='input_x')
y = keras.layers.Input(shape=(1, 1), name='input_y')
admi = keras.layers.LSTM(40, return_sequences=False)(x)
pla = keras.layers.LSTM(40, return_sequences=False)(y)
out = keras.layers.concatenate([admi, pla], axis=-1)
output = keras.layers.Dense(1, activation='sigmoid')(out)
model = keras.models.Model(inputs=[x, y], outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
```

#### Using `numpy` data
When fitting using `numpy` data this works as expected when passing a list or dictionary of inputs:
```python
model.fit([data_a, data_b], labels, batch_size=2, epochs=10)
model.fit({'input_x': data_a, 'input_y': data_b}, labels, batch_size=2, epochs=10)
```
#### Using `tf.data.Dataset.from_tensor_slices` dictionary
When trying the same with a `tf.data.Dataset` the following fails due to incorrect input validation:
```python
dataset = tf.data.Dataset.from_tensor_slices(({'input_x': data_a, 'input_y': data_b}, labels)).batch(2).repeat()
model.fit(dataset, epochs=10, steps_per_epoch=4)
````

```python-traceback
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-6-d35bacd274cc> in <module>()
      1 dataset = tf.data.Dataset.from_tensor_slices(({'input_x': data_a, 'input_y': data_b}, labels)).batch(2).repeat()
----> 2 model.fit(dataset, epochs=10, steps_per_epoch=4)

/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1276         steps_name='steps_per_epoch',
   1277         steps=steps_per_epoch,
-> 1278         validation_split=validation_split)
   1279 
   1280     # Prepare validation data.

/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)
    915           feed_output_shapes,
    916           check_batch_axis=False,  # Don't enforce the batch size.
--> 917           exception_prefix='target')
    918 
    919       # Generate sample-wise weight values given the `sample_weight` and

/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    180                            ': expected ' + names[i] + ' to have ' +
    181                            str(len(shape)) + ' dimensions, but got array '
--> 182                            'with shape ' + str(data_shape))
    183         if not check_batch_axis:
    184           data_shape = data_shape[1:]

ValueError: Error when checking target: expected dense to have 2 dimensions, but got array with shape (None,)
```

#### Using `tf.data.Dataset.from_generator` dictionary
However using the same network together with `tf.data.Dataset.from_generator` works. Probably because less validation is done:
```python
def generator():
    while True:
        for i in np.random.permutation(8):
            yield {'input_x': data_a[i], 'input_y': data_b[i]}, labels[i]

dataset = tf.data.Dataset.from_generator(generator, ({'input_x': tf.float32, 'input_y': tf.float32}, tf.float32)).batch(2)
model.fit(dataset, epochs=10, steps_per_epoch=4)
```

#### Using `tf.data.Dataset` tuple
Passing the multi-input as a tuple to the model both datasets generated with `from_tensor_slices` and `from_generator` fail:
```python
dataset = tf.data.Dataset.from_tensor_slices(((data_a, data_b), labels)).batch(2).repeat()
model.fit(dataset, epochs=10, steps_per_epoch=4)
```
```python
def generator():
    while True:
        for i in np.random.permutation(8):
            yield (data_a[i], data_b[i]), labels[i]

dataset = tf.data.Dataset.from_generator(generator, ((tf.float32, tf.float32), tf.float32)).batch(2)
model.fit(dataset, epochs=10, steps_per_epoch=4)
```
```python-traceback
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-7-512a95f0c2a7> in <module>()
      1 dataset = tf.data.Dataset.from_tensor_slices(((data_a, data_b), labels)).batch(2).repeat()
----> 2 model.fit(dataset, epochs=10, steps_per_epoch=4)

/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1276         steps_name='steps_per_epoch',
   1277         steps=steps_per_epoch,
-> 1278         validation_split=validation_split)
   1279 
   1280     # Prepare validation data.

/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)
    876         feed_input_shapes,
    877         check_batch_axis=False,  # Don't enforce the batch size.
--> 878         exception_prefix='input')
    879 
    880     if y is not None:

/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    141     data = data.values if data.__class__.__name__ == 'DataFrame' else data
    142     data = [data]
--> 143   data = [standardize_single_array(x) for x in data]
    144 
    145   if len(data) != len(names):

/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in <listcomp>(.0)
    141     data = data.values if data.__class__.__name__ == 'DataFrame' else data
    142     data = [data]
--> 143   data = [standardize_single_array(x) for x in data]
    144 
    145   if len(data) != len(names):

/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_single_array(x)
     79   elif tensor_util.is_tensor(x):
     80     return x
---> 81   elif x.ndim == 1:
     82     x = np.expand_dims(x, 1)
     83   return x

AttributeError: 'tuple' object has no attribute 'ndim'
```"
20697,terminate called after throwing an instance of 'std::bad_alloc',"```
2018-07-11 09:31:19.563525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-07-11 09:31:19.564174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties:
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.721
pciBusID: 0000:01:00.0
totalMemory: 7.93GiB freeMemory: 7.55GiB
2018-07-11 09:31:19.564193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-07-11 09:31:20.828223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-11 09:31:20.829019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0
2018-07-11 09:31:20.829029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N
2018-07-11 09:31:20.839815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7296 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
```"
20696,ValueError when loading multiple tf.contrib.keras models in the same scope at different times,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.5.2
- **Bazel version**: N/A
- **CUDA/cuDNN version**: CUDA 7.5, cuDNN 9.0
- **GPU model and memory**: GTX1080 (8GB)
- **Exact command to reproduce**: See [**this snippet**](https://gist.github.com/Sergio0694/aa36c7ed94091ce5503ad908b142aaf0)

### Describe the problem
TensorFlow throws a `ValueError: You are trying to load a weight file containing 16 layers into a model with 0 layers` when trying to create multiple instances of a pretrained Keras model from within the same scope. This only happens if the instances are created one at a time, reopening the same scope multiple times, and works if all the instances are created consecutively, after opening the scope a single time.

Of course, in a real world scenario these instances are created from different places and not all at the same time while building the model, so one would actually need to reopen that scope multiple times.
Hope this helps and that the snippet is clear enough!

### Source code / logs
See [**this snippet**](https://gist.github.com/Sergio0694/aa36c7ed94091ce5503ad908b142aaf0), same one posted under system information.

Here's the stack trace for the snippet above:
```
Traceback (most recent call last):
  File ""/home/sergio/Documents/code/keras_repro.py"", line 21, in <module>
    vgg2 = load_vgg19(t)
  File ""/home/sergio/Documents/code/keras_repro.py"", line 12, in load_vgg19
    m = tf.contrib.keras.applications.VGG19(weights='imagenet', include_top=False, input_tensor=x)
  File ""/home/sergio/.local/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/applications/vgg19.py"", line 234, in VGG19
    model.load_weights(weights_path)
  File ""/home/sergio/.local/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py"", line 1190, in load_weights
    saving.load_weights_from_hdf5_group(f, self.layers)
  File ""/home/sergio/.local/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/saving.py"", line 697, in load_weights_from_hdf5_group
    ' layers.')
ValueError: You are trying to load a weight file containing 16 layers into a model with 0 layers."
20695,TF1.9 - Wheels for Python 3.6 (Linux) are missing on PyPi,"The following wheel files are missing on PyPi:

tensorflow: 
- tensorflow-1.9.0-cp36-cp36m-manylinux1_x86_64.whl

tensorflow_gpu:
- tensorflow_gpu-1.9.0-cp36-cp36m-manylinux1_x86_64.whl"
20694,tf.contrib.quantize bug: errors within weights/activations quantization,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, provided
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: - 
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: python quantization_test.py (file provided)

### Describe the problem
**tf.contrib.quantize** (experimental and non-experimental) automatic quantization produce close to but not quantized results as when you manually introduce fakequant operations.
Weights and/or activations are close to the quantized values, but they are not correct.


### Source code / logs

```
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

D_SIZE = 2048
BATCH_SIZE = 128
LR = 1e-2
EPOCHS = 2000

SHOW_INTERVAL = 20
OUT_CH = 8


def create_dummy_model(is_training=True):
    # prepare dataset
    np_features = np.arange(-int(D_SIZE) / 2, int(D_SIZE / 2))
    features_arr = tf.constant(np_features, dtype=tf.float32)
    np_labels = np.zeros((D_SIZE, OUT_CH), dtype=float)
    print(np_labels.shape)
    for i in range(OUT_CH):
        np_labels[:, i] = 2 * (i + 1) * np_features
    print('np_features:')
    print(np_features[0:8])
    print('np_labels:')
    print(np_labels[0:8])

    labels_arr = tf.constant(np_labels, dtype=tf.float32)
    trn_data = tf.data.Dataset.zip(
        (tf.data.Dataset.from_tensor_slices(features_arr),
         tf.data.Dataset.from_tensor_slices(labels_arr))
    )
    trn_data = trn_data.shuffle(D_SIZE)
    trn_data = trn_data.batch(BATCH_SIZE)
    trn_data = trn_data.repeat(1)
    trn_data = trn_data.prefetch(1)

    iterator = tf.data.Iterator.from_structure(trn_data.output_types,
                                               trn_data.output_shapes)
    features, labels = iterator.get_next()
    data_init_op = iterator.make_initializer(trn_data)

    # global step
    global_step = tf.train.get_or_create_global_step()

    # simple model dense, WITH ACTIVATION TO TEST QUANTIFICATION
    input = tf.reshape(features, (BATCH_SIZE, 1))
    flow = tf.identity(input, name='input_tensor')
    # batch_size x INPUT_CH
    flow = tf.reshape(flow, (-1, 1))
    flow = tf.layers.dense(flow, units=OUT_CH, name='dense')
    flow = tf.reshape(flow, (-1, OUT_CH), name='out')

    # Build forward pass of model.
    # Make the loss op
    with tf.variable_scope('loss'):
        tf.logging.debug('[loss] size of out: %s',
                         flow.get_shape().as_list())
        power = tf.pow(labels - flow, 2)
        power = tf.reduce_sum(power, 1)
        # power = tf.reduce_sum(power)
        loss = tf.reduce_mean(power, name='m_loss')
    tf.summary.scalar('loss', loss)

    # Optimizer
    if is_training:
        opt_op = tf.train.AdamOptimizer(LR).minimize(loss, global_step)
        # opt_op = tf.train.GradientDescentOptimizer(1e-4).minimize(loss, global_step)
    else:
        opt_op = False
    # Merge all the summaries
    merged = tf.summary.merge_all()

    return {
        'iterator': iterator,
        'features': features,
        'labels': labels,
        'data_init_op': data_init_op,
        'global_step': global_step,
        'flow': flow,
        'loss': loss,
        'opt_op': opt_op,
        'merged': merged
    }


def quantize_tensor(np_array, n_bits):
    """"""Quantizes an np_array""""""
    min_wt = np.min(np_array)
    max_wt = np.max(np_array)
    # clap values if required
    if min_wt < -2**(n_bits-1):
        min_wt = -2**(n_bits-1)
    if max_wt > (2**(n_bits-1) - 1) :
        max_wt = (2**(n_bits-1) - 1)
    # find number of integer bits to represent this range
    int_bits = int(np.ceil(np.log2(max(abs(min_wt), abs(max_wt)))))
    print('int_bits: ', int_bits)
    # remaining bits are fractional bits (1-bit for sign)
    frac_bits = n_bits -1 - int_bits
    # floating point weights are scaled and rounded to [-128,127], which are used in
    # the fixed-point operations on the actual hardware (i.e., microcontroller)
    quantized = np.round(np_array * (2**frac_bits))
    # To quantify the impact of quantized weights, scale them back to
    # original range to run inference using quantized weights
    return quantized / (2**frac_bits)


def train_quantization(quantize=True, w_b=3, a_b=3):

    g_tr = tf.Graph()
    with g_tr.as_default():

        model = create_dummy_model(True)

        # Call the training rewrite which rewrites the graph in-place with
        # FakeQuantization nodes and folds batchnorm for training. It is
        # often needed to fine tune a floating point model for quantization
        # with this training tool. When training from scratch, quant_delay
        # can be used to activate quantization after training to converge
        # with the float graph, effectively fine-tuning the model.
        if quantize:
            quant_delay = int((D_SIZE / BATCH_SIZE) * EPOCHS * 3 / 4)
            print('quant_delay: ', quant_delay)
            print('w_b: ', w_b)
            print('a_b: ', a_b)
            tf.contrib.quantize.experimental_create_training_graph(weight_bits=w_b,
                                                                   activation_bits=a_b,
                                                                   quant_delay=quant_delay)

        with tf.Session(graph=g_tr) as sess:
            # global variables Initializing
            sess.run(tf.global_variables_initializer())
            # local variables Initializing
            sess.run(tf.local_variables_initializer())

            if quantize:
                train_writer = tf.summary.FileWriter(
                    './tmp/train_quantized_w' + str(w_b) + '_a_' + str(a_b),
                    sess.graph)
            else:
                train_writer = tf.summary.FileWriter('./tmp/train_normal',
                                                     sess.graph)

            epoch_counter = 0
            nn_in = None
            nn_out = None
            nn_loss = None
            for epoch_counter in range(EPOCHS):

                # re-initialize the dataset iterator
                sess.run(model['data_init_op'])

                try:
                    while True:
                        _, nn_in, nn_out, nn_labels, nn_loss, gs, summary = sess.run(
                            [model['opt_op'],
                             model['features'],
                             model['flow'],
                             model['labels'],
                             model['loss'],
                             model['global_step'],
                             model['merged']])

                        # tensorboard and  statistics
                        train_writer.add_summary(summary, gs)
                except tf.errors.OutOfRangeError:
                    # do nothing
                    if epoch_counter % SHOW_INTERVAL == 0:
                        print(epoch_counter,  '/', EPOCHS,
                              ': loss ', nn_loss,
                              '. Global step: ', gs)

            weights = sess.run(tf.get_default_graph().get_tensor_by_name(
                'dense/kernel:0'))
            saver = tf.train.Saver()
            if quantize:
                trained_model_path = './tmp/quantized_w' + \
                    str(w_b) + '_a_' + str(a_b) + '.ckpt'
            else:
                trained_model_path = './tmp/normal.ckpt'

            saver.save(sess, trained_model_path)

        # debug
        print('nn_out shape: ', nn_out.shape)
        print('nn_labels shape: ', nn_labels.shape)
        print('nn_out different values in OUT_CH (', OUT_CH, ') elements:',
              len(np.unique(nn_out)))
        print('weights shape: ', weights.shape)
        print('weights: ', weights)
        print('weights different values in OUT_CH (', OUT_CH, ') elements:',
              len(np.unique(weights)))

        plt.plot(nn_in[:], nn_out[:, 0], label='nn_out',
                 marker='o', markersize=5)
        plt.plot(nn_in[:], nn_labels[:, 0], label='labels',
                 marker='*', markersize=5)
        plt.legend(loc='lower right')
        plt.show()
        plt.plot(nn_in[:], nn_out[:, 5], label='nn_out',
                 marker='o', markersize=5)
        plt.plot(nn_in[:], nn_labels[:, 5], label='labels',
                 marker='*', markersize=5)
        plt.legend(loc='lower right')
        plt.show()

        plt.plot(weights[0, :], label='weights',
                 marker='o', markersize=5)
        plt.legend(loc='lower right')
        plt.show()

        print('[debug] exit training')
        return trained_model_path


def inference_quantization(trained_model_path, quantize=True, w_b=3, a_b=3):

    #################################
    # Reset default graph
    tf.reset_default_graph()
    #################################

    g_inf = tf.Graph()
    with g_inf.as_default():

        model = create_dummy_model(False)

        with tf.Session(graph=g_inf) as sess:
            # Call the eval rewrite which rewrites the graph in-place with
            # FakeQuantization nodes and fold batchnorm for eval.
            if quantize:
                print('w_b: ', w_b)
                print('a_b: ', a_b)
                tf.contrib.quantize.experimental_create_eval_graph(
                    weight_bits=w_b,
                    activation_bits=a_b)

            saver = tf.train.Saver()
            saver.restore(sess, trained_model_path)
            # frozen_graphdef = tf.graph_util.convert_variables_to_constants(
            #       sess, sess.graph_def, [""out""])
            print('[debug] restored')

            # re-initialize the iterator, but this time with training data

            sess.run(model['data_init_op'])

            nn_in, nn_out, nn_labels, nn_loss = sess.run([
                model['features'],
                model['flow'],
                model['labels'],
                model['loss']])
            weights = sess.run(tf.get_default_graph(
            ).get_tensor_by_name('dense/kernel:0'))

        # debug
        print('nn_out shape: ', nn_out.shape)
        print('nn_labels shape: ', nn_labels.shape)
        print('nn_out different values in OUT_CH (', OUT_CH, ') elements:',
              len(np.unique(nn_out)))
        print('weights shape: ', weights.shape)
        # print('weights: ', weights)
        print('weights different values in OUT_CH (', OUT_CH, ') elements:',
              len(np.unique(weights)))
        print('weights: ', weights)
        q_weights = quantize_tensor(weights, w_b)
        print('q_weights: ', q_weights)
        plt.plot(nn_in[:], nn_out[:, 0], label='nn_out',
                 marker='o', markersize=5)
        plt.plot(nn_in[:], nn_labels[:, 0], label='labels',
                 marker='*', markersize=5)
        plt.legend(loc='lower right')
        plt.show()
        plt.plot(nn_in[:], nn_out[:, 5], label='nn_out',
                 marker='o', markersize=5)
        plt.plot(nn_in[:], nn_labels[:, 5], label='labels',
                 marker='*', markersize=5)
        plt.legend(loc='lower right')
        plt.show()

        plt.plot(weights[0, :], label='weights',
                 marker='o', markersize=5)
        plt.legend(loc='lower right')
        plt.show()


def test_quantization():

    # train_quantization(False)
    trained_model_path = train_quantization(quantize=True, w_b=2, a_b=2)
    inference_quantization(
        trained_model_path=trained_model_path, quantize=True, w_b=2, a_b=2)

test_quantization()
```

"
20693,tf.train.MonitoredTrainingSession get error when data is large,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.13.5
- **TensorFlow installed from (source or binary)**: pip install --upgrade tensorflow
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**:  2.7.10
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
with tf.train.MonitoredTrainingSession(master=server.target, is_chief=(FLAGS.task_index == 0),
                                               hooks=[saver_hooks],
                                               config=tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True,
                                                                     log_device_placement=True)) as sess:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
It is a distributed model, there are 1 ps and 10 worker.
When train with low-dimensional model, everything is fine.
But if trained with high-dimensional model, the error occur and traceback shows it happen in tf.train.MonitoredTrainingSession.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

18/07/10 18:44:02 INFO XLearningContainer: weights/truncated_normal/mean: (Const): /job:worker/replica:0/task:2/device:CPU:0
18/07/10 18:44:02 INFO XLearningContainer: weights/truncated_normal/shape: (Const): /job:worker/replica:0/task:2/device:CPU:0
18/07/10 18:44:02 INFO XLearningContainer: weights/random_uniform/max: (Const): /job:worker/replica:0/task:2/device:CPU:0
18/07/10 18:44:02 INFO XLearningContainer: weights/random_uniform/min: (Const): /job:worker/replica:0/task:2/device:CPU:0
18/07/10 18:44:02 INFO XLearningContainer: weights/random_uniform/shape: (Const): /job:worker/replica:0/task:2/device:CPU:0
18/07/10 18:44:02 INFO XLearningContainer: input/Placeholder_1: (Placeholder): /job:worker/replica:0/task:2/device:CPU:0
18/07/10 18:44:02 INFO XLearningContainer: input/Placeholder: (Placeholder): /job:worker/replica:0/task:2/device:CPU:0
18/07/10 18:44:02 INFO XLearningContainer: train/Adagrad/value: (Const): /job:ps/replica:0/task:0/device:CPU:0
18/07/10 18:44:02 INFO XLearningContainer: global_step/Initializer/zeros: (Const): /job:ps/replica:0/task:0/device:CPU:0
18/07/10 18:44:02 INFO XLearningContainer: Traceback (most recent call last):
18/07/10 18:44:02 INFO XLearningContainer:   File ""location_embedding_tf.py"", line 203, in <module>
18/07/10 18:44:02 INFO XLearningContainer:     tf.app.run(main=main)
18/07/10 18:44:02 INFO XLearningContainer:   File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
18/07/10 18:44:02 INFO XLearningContainer:     _sys.exit(main(argv))
18/07/10 18:44:02 INFO XLearningContainer:   File ""location_embedding_tf.py"", line 90, in main
18/07/10 18:44:02 INFO XLearningContainer:     log_device_placement=True)) as sess:
18/07/10 18:44:02 INFO XLearningContainer:   File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 370, in MonitoredTrainingSession
18/07/10 18:44:02 INFO XLearningContainer:     stop_grace_period_secs=stop_grace_period_secs)
18/07/10 18:44:02 INFO XLearningContainer:   File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 816, in __init__
18/07/10 18:44:02 INFO XLearningContainer:     stop_grace_period_secs=stop_grace_period_secs)
18/07/10 18:44:02 INFO XLearningContainer:   File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 539, in __init__
18/07/10 18:44:02 INFO XLearningContainer:     self._sess = _RecoverableSession(self._coordinated_creator)
18/07/10 18:44:02 INFO XLearningContainer:   File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1002, in __init__
18/07/10 18:44:02 INFO XLearningContainer:     _WrappedSession.__init__(self, self._create_session())
18/07/10 18:44:02 INFO XLearningContainer:   File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1007, in _create_session
18/07/10 18:44:02 INFO XLearningContainer:     return self._sess_creator.create_session()
18/07/10 18:44:02 INFO XLearningContainer:   File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 696, in create_session
18/07/10 18:44:02 INFO XLearningContainer:     self.tf_sess = self._session_creator.create_session()
18/07/10 18:44:02 INFO XLearningContainer:   File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 509, in create_session
18/07/10 18:44:02 INFO XLearningContainer:     max_wait_secs=self._max_wait_secs
18/07/10 18:44:02 INFO XLearningContainer:   File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 421, in wait_for_session
18/07/10 18:44:02 INFO XLearningContainer:     ""Session was not ready after waiting %d secs."" % (max_wait_secs,))
18/07/10 18:44:02 INFO XLearningContainer: tensorflow.python.framework.errors_impl.DeadlineExceededError: Session was not ready after waiting 7200 secs.
18/07/10 18:44:03 ERROR XLearningContainer: XLearningContainer run failed!"
20692,ImportError: cannot import name 'build_info' when using command import tensorflow as tf,"I can successfully follow the tutorial of installing tensorflow (CPU version) with Anaconda yesterday. However, after trying to apply the GPU version and download CUDA9.0, CuDNN v7.1.4 for CUDA9.0, suddenly everything gets wrong,,,

![default](https://user-images.githubusercontent.com/41112671/42566657-d220b842-8539-11e8-8ad1-c475621cee5c.PNG)


PLEASE CAN ANYONE HELP ME...T_T  I am new to it as I work for it new because of my final year project.
"
20691,Failed to convert squeezenet quantized model to .tflite," System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:    NO
- OS Platform and Distribution :
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: 
SOURCE
- **TensorFlow version (use command below)**:
1.8.0
- **Python version**: 
2.7.12
- **Bazel version (if compiling from source)**:
0.11.0
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

        ####Command to convert  to .tflite
         bazel-bin/tensorflow/contrib/lite/toco/toco \
         --input_file=/pathOfQuantizedFile/xyz.pb \
         --output_file=/pathOfQuantizedFile/xyz.tflite \
         --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \
         --inference_type=QUANTIZED_UINT8 \
         --input_shape=1,256,256 \
         --input_array=input_node \
         --output_array=output_node \
         --std_value=127.5 --mean_value=127.5

### Describe the problem
I quantized squeezenet Model using command below:
        
        bazel build tensorflow/python/tools:strip_unused
        bazel-bin/tensorflow/python/tools/strip_unused \
        --input_graph=/path/xyz.pb \
        --output_graph=/path/tem/xyz.pb\
        --input_node_names=input_node \
        --output_node_names=output_node \
        --input_binary=true

        bazel build tensorflow/tools/quantization:quantize_graph
        bazel-bin/tensorflow/tools/quantization/quantize_graph \
        --input=/path/tem/xyz.pb\
        --output=/pathOfQuantizedFile/xyz.pb\
        --output_node_names=output_node \
        --mode=weights
 but when I tried to convert this quantized file to .tflite. It gave error:

        2018-07-11 15:51:46.317809: F tensorflow/contrib/lite/toco/tooling_util.cc:1445] Array                    
        Conv2D_bias, which is an input to the Conv operator producing the output array Conv2D, is    
        lacking min/max data, which is necessary for quantization. Either target a non-quantized output 
        format, or   change the input graph to contain min/max information, or pass                                                    
       --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of  
       results.
       Aborted (core dumped)



### Source code / logs

2018-07-11 15:51:46.288981: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289073: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289110: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289137: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289189: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289236: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289261: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289282: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289324: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289372: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289394: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289457: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289485: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289543: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289572: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289651: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289691: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289737: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289765: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289794: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289823: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289852: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289905: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289933: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289962: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.289993: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290024: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290082: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290116: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290177: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290226: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290284: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290314: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290343: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290383: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290410: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290440: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290468: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290497: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290545: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290576: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290635: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290685: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290742: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290771: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290800: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290841: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290898: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290927: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.290956: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291023: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291062: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291204: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291256: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291284: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291316: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291350: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291446: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291475: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291504: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291533: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291587: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291764: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291816: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291844: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291874: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291903: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291933: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.291974: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292030: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292059: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292089: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292152: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292681: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292739: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292798: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292828: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292857: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292898: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292926: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292956: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.292985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.293014: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.293113: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.293154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.293182: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.293211: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.293240: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.293270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1265] Converting unsupported operation: Dequantize
2018-07-11 15:51:46.296627: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 381 operators, 700 arrays (0 quantized)
2018-07-11 15:51:46.301535: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 381 operators, 700 arrays (0 quantized)
2018-07-11 15:51:46.307561: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 283 operators, 618 arrays (1 quantized)
2018-07-11 15:51:46.312002: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 283 operators, 618 arrays (1 quantized)
2018-07-11 15:51:46.314295: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 283 operators, 618 arrays (1 quantized)
2018-07-11 15:51:46.317809: F tensorflow/contrib/lite/toco/tooling_util.cc:1445] Array Conv2D_bias, which is an input to the Conv operator producing the output array Conv2D, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.
Aborted (core dumped)

       
"
20690,Invalid syntax error while importing tensorflow in python3.7.0,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MacOS High Sierra 10.13.5

- **TensorFlow installed from (source or binary)**:
pip3

- **TensorFlow version (use command below)**:
1.9.0

- **Python version**: 
3.7.0

- **Exact command to reproduce**:
import tensorflow as tf

### Describe the problem
when import tensorflow in python3 prompt, it says ""SyntaxError: invalid syntax"".

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 114
    def TFE_ContextOptionsSetAsync(arg1, async):_
                                             ^
SyntaxError: invalid syntax

### Source code / logs
line 114, 115, 150 of pywrap_tensorflow_internal.py has ""async"" as parameter which seems to be a keyword.
After changed to ""async1"", importing tensorflow works.

def TFE_ContextOptionsSetAsync(arg1, async1):
    return _pywrap_tensorflow_internal.TFE_ContextOptionsSetAsync(arg1, async1)
TFE_ContextOptionsSetAsync = _pywrap_tensorflow_internal.TFE_ContextOptionsSetAsync

"
20689,Trying to open a non-existing file in tf.data.Dataset on Windows crashes Python instance,"## System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 Enterprise
- **TensorFlow installed from (source or binary)**: binary (compiled by conda-forge)
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072, 1.8.0
- **Python version**: Python 3.6.5
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: none (CPU only)
- **GPU model and memory**: none (CPU only)
- **Exact command to reproduce**:
```python
import tensorflow as tf

sess = tf.Session()

filenames = [r'C:\nonexistent.tfrecords']
dataset = tf.data.TFRecordDataset(filenames)
iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

print(sess.run(next_element))
```

### Describe the problem
Trying to open a non-existing tfrecords dataset using `tf.data.Dataset` on Windows crashes Python instance. On Linux the same code properly raises a `NotFoundError`.

### Source code / logs
Error on Windows:
![image](https://user-images.githubusercontent.com/9080823/42564829-1d66b532-8502-11e8-9fd9-a9a235ad6523.png)
Error details (in Polish):
```
Podpis problemu:
  Nazwa zdarzenia problemu:	APPCRASH
  Nazwa aplikacji:	python.exe
  Wersja aplikacji:	3.6.5150.1013
  Sygnatura czasowa aplikacji:	5ac79d8c
  Nazwa moduÅ‚u z bÅ‚Ä™dem:	_pywrap_tensorflow_internal.pyd
  Wersja moduÅ‚u z bÅ‚Ä™dem:	0.0.0.0
  Sygnatura czasowa moduÅ‚u z bÅ‚Ä™dem:	5ae380ee
  Kod wyjÄ…tku:	c0000005
  PrzesuniÄ™cie wyjÄ…tku:	0000000000236acf
  Wersja systemu operacyjnego:	6.1.7601.2.1.0.256.4
  Identyfikator ustawieÅ„ regionalnych:	1045
  Dodatkowe informacje 1:	2583
  Dodatkowe informacje 2:	258324ee3ae15385fdd62136546133f9
  Dodatkowe informacje 3:	1bc4
  Dodatkowe informacje 4:	1bc4034f8418cb6d1dc9351873ca4aa5
```
Result:
```
Process finished with exit code -1073741819 (0xC0000005)
```

Proper error on Linux (part of, full stacktrace removed):
```
NotFoundError (see above for traceback): C:\nonexistent.tfrecords; No such file or directory
         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_STRING], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]
```

"
20688,[Bazel] Error while build the grpc_tensorflow_server in windows 7.,"### Describe the problem

I wont to build the grpc_tensorflow_server in windows 7 64-bit, then an error occurs.

Then I installed Bazel, MSYS2 shell, Visual Studio 2015 and JAVA 8 Update 171,
and I have setup to BAZEL_VS, BAZEL_VC, BAZEL_SH and JAVA_HOME following site.
https://docs.bazel.build/versions/master/install-windows.html


### System information
- **Have I written custom code** : None
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)** : Windows 7 64-bit
- **TensorFlow installed from (source or binary)** : pip install --upgrade tensorflow==1.8.0
- **TensorFlow version (use command below)** : b'v1.8.0-0-g93bc2e2072' 1.8.0
- **Python version** : Python 3.5.5 :: Anaconda, Inc.
- **Bazel version (if compiling from source)** : bazel-0.15.0-windows-x86_64.exe
- **GCC/Compiler version (if compiling from source)** : None
- **CUDA/cuDNN version** : None
- **GPU model and memory** : None
- **Exact command to reproduce**:bazel-0.15.0-windows-x86_64.exe build -c opt //tensorflow/core/distributed_runtime/rpc:grpc_tensorflow_server --verbose_failures

### Source code

git clone -b v1.8.0 --recurse-submodules https://github.com/tensorflow/tensorflow.git
cd tensorflow
set BAZEL_VS=C:\Program Files (x86)\Microsoft Visual Studio 14.0
set BAZEL_VC=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC
set BAZEL_SH=C:\msys64\usr\bin\bash.exe
set JAVA_HOME=C:\Program Files\Java\jre1.8.0_171
set PATH=C:\msys64\usr\bin;%PATH%
bash.exe
bazel-0.15.0-windows-x86_64.exe build -c opt //tensorflow/core/distributed_runtime/rpc:grpc_tensorflow_server --verbose_failures

### Logs

ERROR: ./tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:311:1: Linking of rule '//tensorflow/core/distributed_runtime/rpc:grpc_tensorflow_server' failed (Exit 1181): link.exe failed: error executing command
  cd %USERPROFILE%/_bazel_%USERNAME%/do34jop7/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\ProgramFiles (x86)\Windows Kits\10\lib\10.0.10240.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.10240.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\Windows\system32
    SET PWD=/proc/self/cwd
    SET TEMP=%USERPROFILE%\AppData\Local\Temp
    SET TMP=%USERPROFILE%\AppData\Local\Temp
    SET USE_LINKER=1
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /OUT:bazel-out/x64_windows-opt/bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server.exe tensorflow_framework /SUBSYSTEM:CONSOLE -DEFAULTLIB:advapi32.lib -pthread /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server.exe-2.params
LINK : warning LNK4044: unrecognized option '/pthread' ignored
LINK : warning LNK4044: unrecognized option '/ldl' ignored
LINK : warning LNK4044: unrecognized option  '/lpthread' ignored
LINK : warning LNK4044: unrecognized option '/lm' ignored
LINK : warning LNK4044: unrecognized option '/lm' ignored
LINK : fatal error LNK1181: cannot open input file 'tensorflow_framework.obj'
Target //tensorflow/core/distributed_runtime/rpc:grpc_tensorflow_server failed to build
INFO: Elapsed time: 3507.723s, Critical Path: 1648.75s
INFO: 2235 processes: 2235 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully"
20687,Speeding up TF custom ops on GPU,"I have created some TF custom Ops [which you can think of as matmul or conv operations], as per TF's [tutorial](https://www.tensorflow.org/extend/adding_an_op), and would like to speed them up. I have used AWS' P3 instances (NVIDIA V100 GPU) to run my Ops. Depending on the size of the model, my GPU ops are between 1.5x and 5x slower than TF functions.

I have seen that TF's implementation of various operations (such as Con2D, MatMul, Pooling etc) use DeviceMemory objects to encapsulate tensor data to be passed to the target device (GPU), then call different wrappers that end up launching Kernels using ""[Stream::ThenLaunch](https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/stream_executor/stream_executor_pimpl.h#L729)"".

A different type of implementation [(OpenAI's block sparse conv2d)](https://github.com/openai/blocksparse), choses to launch the Kernel using cuLaunchKernel after they allocate CUdeviceptr to the different Kernel arguments.

In my implementation, when I register my TF Ops, the Input/Output tensors are presumably on the device, as I do not set them to be located on the Host(is this correct?). Is TF copying tensors back and forth, between host and GPU after each OP?

Would it make any difference, in terms of speed, if the Input/Output tensors are set on the Host then copied to the device (cudaMalloc/cudaMemCpy) used by GPU Kernels then copy the results to the Host?

Would my Kernels run any faster if I were to use TF's approach, to use the DeviceMemory class for my arguments and the Stream::ThenLaunch method to launch the Kernel, or OpenAI's CUdeviceptr and cuLaunchKernel approach?

Thanks"
20684,[tflite][quantization][deeplabv3] Constant array MobilenetV2/expanded_conv_7/depthwise/depthwise_weights lacks MinMax information,"**System information**
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:Source
- **TensorFlow version (use command below)**:1.9.0
- **Python version**:2.7.12
- **Bazel version (if compiling from source)**:0.12.0
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:cuda-9.0/7.0
- **GPU model and memory**:GeForce GTX 1080/8105MiB
- **Phone**:xiaomi5 (Snapdragon 820)
- **Exact command to reproduce**:
bazel run --config=opt //tensorflow/contrib/lite/toco:toco --
--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb
--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite
--inference_type=QUANTIZED_UINT8
--input_shape=1,513,513,3
--input_array=sub_7
--output_array=ResizeBilinear_3

**Describe the problem**
I have tried to quantize MobileNetV2 for deeplabV3+ with TFlite. But I fail to convert the model.
From the following issue, I saw that the operations were not supported for the option of quantization.

https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md
Checkpoint name: mobilenetv2_coco_voc_trainaug

As we can see graphs from the tensorboard, there is one big problem.

In ""import/MobilenetV2/expanded_conv_7/depthwise/depthwise"",

the operation of depthwise consists of the subgraph with 3 nodes: (depthwise) and BatchToSpaceND, SpaceToBatchND.

But, in ""import/MobilenetV2/expanded_conv_6/depthwise/depthwise"",

the operation of depthwise is DepthwiseConv2dNative itself.

From the difference, we can not quantize deeplabv3 based on mobilenetv2.

The one thing is that MobilenetV2/expanded_conv_7~16 does not have min/max value to be needed for quantization with tflite.

Although I implement the needed min/max value in hardcode_min_max.cc,

This model does not run well in mobile environments.

The ultimate problem is caused by the fact that depthwise_conv_7~16 consist of 3 nodes with BatchToSpaceND and SpaceToBatchND.

I request you to notify the method to resolve above issues.

**Source code / logs**

bazel run --config=opt //tensorflow/contrib/lite/toco:toco --
--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb
--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite
--inference_type=QUANTIZED_UINT8
--input_shape=1,513,513,3
--input_array=sub_7
--output_array=ResizeBilinear_3

2018-07-11 04:40:01.330069: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 166 operators, 340 arrays (1 quantized)
2018-07-11 04:40:01.330711: W tensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:339] Tweaking the MinMax of array ResizeBilinear_1, which is an input to {Concatenation operator with output concat}, because we want all inputs and outputs of a Concatenation operator to have the same MinMax so that it can be implemented as a pure byte-copy, no arithmetic.
2018-07-11 04:40:01.332983: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 111 operators, 285 arrays (1 quantized)
2018-07-11 04:40:01.335731: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 111 operators, 285 arrays (1 quantized)
2018-07-11 04:40:01.337575: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_7/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.337670: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_7/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.337695: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_7/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.338553: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_8/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.338711: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_8/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.338786: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_8/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.339777: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_9/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.339918: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_9/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.339985: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_9/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.340933: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_10/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.341034: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_10/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.341059: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_10/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.342497: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_11/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.342593: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_11/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.342620: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_11/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.344311: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_12/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.344422: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_12/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.344452: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_12/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.345978: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_13/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.346094: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_13/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.346122: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_13/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.349163: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_14/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.349318: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_14/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.349351: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_14/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.353356: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_15/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.353511: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_15/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.353545: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_15/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.357264: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_16/depthwise/depthwise_weights lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.357400: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_16/depthwise/BatchNorm/FusedBatchNorm_mul_0_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2018-07-11 04:40:01.357432: W tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:92] Constant array MobilenetV2/expanded_conv_16/depthwise/BatchNorm/FusedBatchNorm_add_param lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
"
20683,matrix matmul ,"Is there any way of computing the matrix inner of a matrix  and a vector, I use the tf.matmul operation and it raised an error : 

 Shape must be rank 2 but is rank 3 for 'attention_query_1/MatMul' (op: 'MatMul') with input shapes: [?,1000,100], [100].

It seems that the operation does not support broadcasting. 
"
20682,Run model on ios,"Hi friends,
when I run a model on ios, I got an error:
Running model failed:Invalid argument: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[""SAME"", ""VALID""]; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW""]>; NodeDef: conv2d/Conv2D = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_x_0, conv2d/kernel/read). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
I using tensorflow 1.8.0 to trainning (but when I using tensorflow 1.5.0 I still got same error when run model on ios)
Please kindly give me an advice for this. Thanks"
20681,how can I use mkl with tbb in tensorflow ,"I found that tbb is fast than openMp
https://software.intel.com/en-us/articles/using-intel-mkl-and-intel-tbb-in-the-same-application

But in tensorflow, I cant use mkl with tbb, Is there a better solutionï¼Ÿ"
20680,The default values of tf.app.flags are printed event though passed parameters at the first time,"
### System information

== cat /etc/issue ===============================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.1.0 (clang-902.0.39.1)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy                              1.14.2
protobuf                           3.5.2.post1
tensorflow                         1.8.0
tensorflow-hub                     0.1.0
tensorflow-model-analysis          0.6.0
tensorflow-serving-api             1.8.0
tensorflow-tensorboard             1.5.0
tensorflow-transform               0.6.0
tensorflowjs                       0.1.0
tensorflowonspark                  1.0.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 106: nvidia-smi: command not found

== cuda libs  ===================================================

### Describe the problem

We usually use `tf.app.flags` to declare the command-line parameters for TensorFlow scripts. But If we try to print the values of these parameters, it prints the default values at the first time even though we have passed the values.

And if we actually ""get"" the values equal or more than one times, we can print the parameter values as expected.

### Source code / logs

Here is the example code to re-produce the issue.

```
import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_integer(""image_width"", 224, ""Width of the image"")
flags.DEFINE_integer(""image_height"", 224, ""Height of the image"")
flags.DEFINE_integer(""channels"", 3, ""Channel of the image"")

FLAGS = flags.FLAGS
parameter_value_map = {}
for key in FLAGS.__flags.keys():
  parameter_value_map[key] = FLAGS.__flags[key].value
print(""Parameters: {}"".format(parameter_value_map))
# Parameters: {'channels': 3, 'image_height': 224, 'image_width': 224}

FLAGS.channels
for key in FLAGS.__flags.keys():
    parameter_value_map[key] = FLAGS.__flags[key].value
print(""Parameters: {}"".format(parameter_value_map))
# Parameters: {'channels': 1, 'image_height': 1, 'image_width': 1}
```

We can run the script with parameter `--image_width=1 --image_height=1 --channels=1`. And the results to print are different.
"
20679,convert the t2t model to tflite error,"Hi,

Recently I am trying to convert the tensor2tensor machine translation model to android. There showed an error when trying to use tflite-convert tool.
Here is the error code:
```
Traceback (most recent call last):
  File ""/usr/local/bin/tflite_convert"", line 11, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 320, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 316, in run_main
    _convert_model(tflite_flags)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 121, in _convert_model
    output_data = converter.convert()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/lite.py"", line 309, in convert
    allow_custom_ops=self.allow_custom_ops)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/convert.py"", line 206, in toco_convert
    input_tensor.dtype))
ValueError: Tensors serialized_example:0 not known type tf.string
```
I am using tensorflow 1.9.0rc2, firstly I have convert the original model to saved model with t2t-exporter tool, then the error came out when trying to convert the saved model to tflite model. 
Can someone help me fix this problem? Thanks.

-------------------------------------------------
update
OS: Ubuntu 16.04
TensorFlow Install from pip
TensorFlow version 1.9.0rc2
Bazel version:0.15.0
CUDA 9.0 cuDNN 7.0
GPU: Nvidia GTX Titan V 12GB
I firstly used the following command to export the model
t2t-exporter \ --model=transformer \ --hparams_set=transformer_base_single_gpu \ --problem=translate_enzh_wmt32k \ --data_dir=../\ --output_dir=./
then I got the exported model with a new folder ""export""
after that I used the command
tflite_convert --output_file=example.tflite --saved_model_dir=export/Servo/1531989270/
Now I got the error I have described before."
20677,C++ compilation of rule '@boringssl//:crypto' failed (Exit 1),"[jingfeng@Jingfeng_redhat tensorflow]$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

ERROR: /home/jingfeng/.cache/bazel/_bazel_jingfeng/29120beece5f660cdcebf72073ed1bba/external/boringssl/BUILD:115:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1)
external/boringssl/src/crypto/cipher_extra/e_chacha20poly1305.c: In function 'poly1305_update_length':
external/boringssl/src/crypto/cipher_extra/e_chacha20poly1305.c:145:3: error: 'for' loop initial declarations are only allowed in C99 mode
for (unsigned i = 0; i < sizeof(length_bytes); i++) {
^
external/boringssl/src/crypto/cipher_extra/e_chacha20poly1305.c:145:3: note: use option -std=c99 or -std=gnu99 to compile your code
external/boringssl/src/crypto/cipher_extra/e_chacha20poly1305.c: In function 'aead_chacha20_poly1305_seal_scatter':
external/boringssl/src/crypto/cipher_extra/e_chacha20poly1305.c:229:5: error: 'for' loop initial declarations are only allowed in C99 mode
for (size_t done = 0; done < extra_in_len; block_counter++) {
^
external/boringssl/src/crypto/cipher_extra/e_chacha20poly1305.c:233:7: error: 'for' loop initial declarations are only allowed in C99 mode
for (size_t i = offset; i < sizeof(block) && done < extra_in_len;
^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 8.244s, Critical Path: 6.30s
INFO: 5 processes: 5 local.
FAILED: Build did NOT complete successfully

[jingfeng@Jingfeng_redhat ~]$ uname -a
Linux Jingfeng_redhat 3.10.0-862.3.2.el7.ppc64le #1 SMP Tue May 15 22:31:16 UTC 2018 ppc64le ppc64le ppc64le GNU/Linux

[jingfeng@Jingfeng_redhat ~]$ gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/libexec/gcc/ppc64le-redhat-linux/4.8.5/lto-wrapper
Target: ppc64le-redhat-linux
Configured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-ppc64le-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-ppc64le-redhat-linux/cloog-install --enable-gnu-indirect-function --enable-secureplt --with-long-double-128 --enable-targets=powerpcle-linux --disable-multilib --with-cpu-64=power8 --with-tune-64=power8 --build=ppc64le-redhat-linux
Thread model: posix
gcc version 4.8.5 20150623 (Red Hat 4.8.5-28) (GCC)

jingfeng@Jingfeng_redhat ~]$ bazel version
Extracting Bazel installation...
Build label: 0.11.0- (@non-git)
Build target: bazel-out/ppc-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 13 14:25:29 +50491 (1531191651929)
Build timestamp: 1531191651929
Build timestamp as int: 1531191651929

python3.4 
cuda9.0 
cudnn latest

[root@Jingfeng_redhat ~]# nvidia-smi
Tue Jul 10 17:50:17 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           On   | 00000000:03:00.0 Off |                  Off |
| N/A   30C    P8    26W / 149W |      0MiB / 12206MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           On   | 00000000:04:00.0 Off |                  Off |
| N/A   25C    P8    30W / 149W |      0MiB / 12206MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla K80           On   | 00000002:03:00.0 Off |                  Off |
| N/A   41C    P0    N/A /  N/A |      0MiB / 12206MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla K80           On   | 00000002:04:00.0 Off |                  Off |
| N/A   26C    P8    30W / 149W |      0MiB / 12206MiB |      0%      Default |


May I know the reason for this problem? & how to solve it? Many thanks for this

"
20676,"FLAGS,what's mean in the code?","I could't understand the mean ''FLAGS"" in the code,just like in your code:


`with tf.name_scope('train'):
  train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(
      cross_entropy)

with tf.name_scope('accuracy'):
  with tf.name_scope('correct_prediction'):
    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
  with tf.name_scope('accuracy'):
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
tf.summary.scalar('accuracy', accuracy)

# Merge all the summaries and write them out to /tmp/mnist_logs (by default)
merged = tf.summary.merge_all()
train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train',
                                      sess.graph)
test_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/test')`


FLAGS is None by default, so what's the  mean of FLAGS.learning_rate,FLAGS.summaries_dir?
or could you give me a example about the dir?"
20674,Tensorflow 1.8.0 cpu fails on import on Windows 10 used through SQL Server 2017,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.  import tensorflow as tf
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64 on SQL Server 2017
- **TensorFlow installed from (source or binary)**: Binary - pip install N:\Packages\Python\tensorflow-1.8.0-cp35-cp35m-win_amd64.whl -f ./ --no-index
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:  import tensorflow as tf

running through sql the full code is:

EXECUTE sp_execute_external_script
@language=N'Python',
@script = N'
import tensorflow as tf
'



### Describe the problem
All dependencies for tensorflow 1.8.0 are installed.
Installation through pip ends with ""Successfully installed tensorflow-1.8.0""

Attempting to import tensorflow fails with:
Msg 39019, Level 16, State 2, Line 7
An external script error occurred: 
Failed to load the native TensorFlow runtime.

Full error report below.


I have checked that MSVCP140.DLL exists in System32 and in SysWOW64.
I don't know how to check for an AVX issue or to bypass it.



### Source code / logs
Traceback (most recent call last):
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed

Msg 39019, Level 16, State 2, Line 7
An external script error occurred: 
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)

Msg 39019, Level 16, State 2, Line 7
An external script error occurred: 
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 5, in <module>
  File ""D:\MSSQL14.MSSQLSERVER\MSSQL\ExtensibilityData\MSSQLSERVER01\EA86FFCC-FD02-4E32-B3CB-E9AF5CE7CC5F\sqlindb.py"", line 31, in transform
    import tensorflow as tf
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):

Msg 39019, Level 16, State 2, Line 7
An external script error occurred: 
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Msg 39019, Level 16, State 2, Line 7
An external script error occurred: 
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""D:\MSSQL14.MSSQLSERVER\PYTHON_SERVICES\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'



Msg 39019, Level 16, State 2, Line 7
An external script error occurred: 
Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
20673,Frozen model is twice as large after applying tf.contrib.model_pruning,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**:1.9.0rc1
- **Python version**:  2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9/7.1
- **GPU model and memory**: 1080ti
- **Exact command to reproduce**:
1. Follow the workflow from [here,](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/model_pruning
) 
2. freeze_graph script from tensorflow
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
The pruned model(frozen protobuf) is exactly twice in terms of storage (13.1mb vs 26.2 mb) for a MNIST model 
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20670,"KeyError: ""Couldn't find message protos.FasterRcnnBoxCoder""","### System information
-  **Using ssd_mobilenet_pets_config and edit serval codes to solve the import errors**
- **Windows 10**:
- **installation via pip**:
- **Tensorflow 1.8**:
- **Python  3.6.0**: 
- **CUDA 9.0, cudnn 9.0*:
- **Geforce GTX 1050Ti 4GB VRAM**:
- **Exact command to reproduce:** python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config running via cmd.exe from folder object_detection:



### Describe the problem
First of all I face import issues with for examples ""from object_detection.core import box_coder"", which was solved by ""from core import box_coder"".

    `Traceback (most recent call last):
     File ""train.py"", line 52, in <module>
     from builders import model_builder
    File ""C:\Users\azach\Desktop\python\tensorflow\models0- 
    master\research\object_detection\builders\model_builder.py"", line 18, in <module>
    from builders import box_coder_builder
    File ""C:\Users\azach\Desktop\python\tensorflow\models- 
    master\research\object_detection\builders\box_coder_builder.py"", line 21, in <module>
    from protos import box_coder_pb2
    File ""C:\Users\azach\Desktop\python\tensorflow\models- 
    master\research\object_detection\protos\box_coder_pb2.py"", line 16, in <module>
    from protos import faster_rcnn_box_coder_pb2 as protos_dot_faster__rcnn__box__coder__pb2
    File ""C:\Users\azach\Desktop\python\tensorflow\models- 
    master\research\object_detection\protos\faster_rcnn_box_coder_pb2.py"", line 76, in <module>
    serialized_end=191,
    File ""C:\Users\azach\Anaconda3\lib\site-packages\google\protobuf\descriptor.py"", line 288, in __ 
    new__
    return _message.default_pool.FindMessageTypeByName(full_name)
    KeyError: ""Couldn't find message protos.FasterRcnnBoxCoder""`

### Source code / logs

    `# Generated by the protocol buffer compiler.  DO NOT EDIT!
     # source: object_detection/protos/faster_rcnn_box_coder.proto
      import sys
    _b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))
     from google.protobuf import descriptor as _descriptor
     from google.protobuf import message as _message
     from google.protobuf import reflection as _reflection
     from google.protobuf import symbol_database as _symbol_database
     from google.protobuf import descriptor_pb2
     # @@protoc_insertion_point(imports)

    _sym_db = _symbol_database.Default()
    DESCRIPTOR = _descriptor.FileDescriptor(
    name='protos/faster_rcnn_box_coder.proto',
    package='protos',
    syntax='proto2', 
    #next part is shortend for increased readability  
serialized_pb=_b('\n3object_detection/protos/faster_rcnn_box_coder.proto\x12\x17object_detection.protos\""o\n\x12\x46\x61sterRcnnBoxCoder....')
)




    _FASTERRCNNBOXCODER = _descriptor.Descriptor(
     name='FasterRcnnBoxCoder',
     full_name='protos.FasterRcnnBoxCoder',
     filename=None,
     file=DESCRIPTOR,
     containing_type=None,
     fields=[
    _descriptor.FieldDescriptor(
      name='y_scale', full_name='object_detection.protos.FasterRcnnBoxCoder.y_scale', index=0,
      number=1, type=2, cpp_type=6, label=1,
      has_default_value=True, default_value=float(10),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    _descriptor.FieldDescriptor(
      name='x_scale', full_name='object_detection.protos.FasterRcnnBoxCoder.x_scale', index=1,
      number=2, type=2, cpp_type=6, label=1,
      has_default_value=True, default_value=float(10),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    _descriptor.FieldDescriptor(
      name='height_scale', full_name='object_detection.protos.FasterRcnnBoxCoder.height_scale', index=2,
      number=3, type=2, cpp_type=6, label=1,
      has_default_value=True, default_value=float(5),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
    _descriptor.FieldDescriptor(
      name='width_scale', full_name='object_detection.protos.FasterRcnnBoxCoder.width_scale', index=3,
      number=4, type=2, cpp_type=6, label=1,
      has_default_value=True, default_value=float(5),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
      ],
     extensions=[
     ],
     nested_types=[],
     enum_types=[
     ],
     options=None,
     is_extendable=False,
     syntax='proto2',
     extension_ranges=[],
     oneofs=[
     ],
     serialized_start=80,
     serialized_end=191,
    )

    DESCRIPTOR.message_types_by_name['FasterRcnnBoxCoder'] = _FASTERRCNNBOXCODER
    _sym_db.RegisterFileDescriptor(DESCRIPTOR)

     FasterRcnnBoxCoder = _reflection.GeneratedProtocolMessageType('FasterRcnnBoxCoder', 
     (_message.Message,), dict(
     DESCRIPTOR = _FASTERRCNNBOXCODER,
      __module__ = 'protos.faster_rcnn_box_coder_pb2'
      # @@protoc_insertion_point(class_scope:object_detection.protos.FasterRcnnBoxCoder)
      ))
     _sym_db.RegisterMessage(FasterRcnnBoxCoder)
 

     # @@protoc_insertion_point(module_scope)`


This is the script which causes the error.
"
20669,Error in Compiling the CPU Module for Python,"System information
- Have I written custom code: No
- OS Platform and Distribution: Windows 10 64bit
- TensorFlow installed from: Latest master source
- TensorFlow version: commit dfcec822728c6569914db37eb55a78a019866e6f
- Python version: 3.6.5 
- CMake version: 3.12.0-rc2
- MS C+_+ Compiler version: 19.00.24234.1
- CPU model and memory: i5-4460 with 16GB of RAM
- Exact command to reproduce:

1- Opening Developer Command Line as admin
2- Choosing the 64bit compiler
""C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\Tools\vsdevcmd\ext\vcvars.bat"" amd64
3- cd D:\opencv\tensorflow\tensorflow\contrib\cmake\build\
4- 
cmake .. -A x64 -T host=x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=D:/opencv/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=""C:/Users/FiFo/AppData/Local/Programs/Python/Python36/python.exe"" -DPYTHON_LIBRARIES=""C:/Users/FiFo/AppData/Local/Programs/Python/Python36/libs/python36.lib"" -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 -Dtensorflow_BUILD_CC_TESTS=OFF -Dtensorflow_BUILD_PYTHON_TESTS=OFF -Dtensorflow_BUILD_MORE_PYTHON_TESTS=OFF -Dtensorflow_BUILD_CC_EXAMPLE=ON -Dtensorflow_BUILD_PYTHON_BINDINGS=ON -Dtensorflow_BUILD_CC_TESTS=OFF -Dtensorflow_OPTIMIZE_FOR_NATIVE_ARCH=ON -Dtensorflow_ENABLE_MKL_SUPPORT=ON -Dtensorflow_ENABLE_MKLDNN_SUPPORT=ON -Dtensorflow_VERBOSE=ON -Dtensorflow_BUILD_SHARED_LIB=ON
5- MSBuild /p:Configuration=Release ALL_BUILD.vcxproj
SUCCESS
6- MSBuild  /p:Configuration=Release INSTALL.vcxproj
SUCCESS
7- MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj
Fails with:

  Generating __init__.py files for Python API.
  Traceback (most recent call last):
    File ""D:\opencv\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
      return importlib.import_module(mname)
    File ""C:\Users\FiFo\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
      return _bootstrap._gcd_import(name[level:], package, level)
    File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
    File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
    File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
    File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
    File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
    File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
    File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  ImportError: DLL load failed: NÃ’o foi possÃvel encontrar o mÂ¾dulo especificado.

  During handling of the above exception, another exception occurred:

  Traceback (most recent call last):
    File ""D:\opencv\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
      from tensorflow.python.pywrap_tensorflow_internal import *
    File ""D:\opencv\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
      _pywrap_tensorflow_internal = swig_import_helper()
    File ""D:\opencv\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
      return importlib.import_module('_pywrap_tensorflow_internal')
    File ""C:\Users\FiFo\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
      return _bootstrap._gcd_import(name[level:], package, level)
  ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

Failed to load the native TensorFlow runtime.

It used to work before with version 1.8
I don't want to install the PIP version
Compilation works without any problem on Ubuntu 16.04"
20668,tensorflow error-fatal: not a git repository (or any parent up to mount point /media/ds) Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set). Not a git repository,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I was trying to run facenet preprocessing as described here :
**https://github.com/davidsandberg/facenet/wiki/Validate-on-lfw**
- **OS Platform and Distribution **:Linux Ubuntu 18.04
- **TensorFlow installed from**: source
- **TensorFlow version **:1.9.0.rc0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: gcc6
- **CUDA/cuDNN version**: cuda9.2/ cudnn7.1.3
- **GPU model and memory**:Nvidia geforce 940m
- **Exact command to reproduce**:  
python src/align/align_dataset_mtcnn.py \
~/datasets/lfw/raw \
~/datasets/lfw/lfw_mtcnnpy_160 \
--image_size 160 \
--margin 32 \


### Describe the problem
While running it above file along with arguments command in spyder, it throws the  following error in detect_face module

### Source code / logs
fatal: not a git repository (or any parent up to mount point /media/ds)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
Not a git repository
To compare two paths outside a working tree:
usage: git diff [â€‘â€‘noâ€‘index] 
2018óˆš«óˆš® 16:37:25.006640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (ð”‚«), but there must be at least one NUMA node, so returning NUMA node zero
2018óˆš«óˆš® 16:37:25.007978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1404] Found device 0 with properties: 
name: GeForce 940M major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:01:00.0
totalMemory: 1.96GiB freeMemory: 1.84GiB
2018óˆš«óˆš® 16:37:25.008008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0
2018óˆš«óˆš® 16:37:25.334405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:
2018óˆš«óˆš® 16:37:25.334449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970] 0 
2018óˆš«óˆš® 16:37:25.334458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0: N 
2018óˆš«óˆš® 16:37:25.334616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2004 MB memory) â€‘> physical GPU (device: 0, name: GeForce 940M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2018óˆš«óˆš® 16:37:25.334817: E tensorflow/core/common_runtime/gpu/gpu_device.cc:228] Illegal GPUOptions.experimental.num_dev_to_dev_copy_streams=0 set to 1 instead.
2018óˆš«óˆš® 16:37:25.337581: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.96G (2101870592 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2018óˆš«óˆš® 16:37:28.896856: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
"
20666,BUG on saving bidirectional CuDNNLSTM/GRU,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0rc2 and also test on nightly
- **Python version**:  3.5
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**:  not important
- **GPU model and memory**: not important
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

Wights are not stored when using `tf.keras.layers.CuDNNLSTM/GRU` with `tf.keras.layers.Bidirectional`

### Source code / logs
```
import tensorflow as tf


class Model(tf.keras.Model):
    def __init__(self):
        super(Model, self).__init__()
        # this line works fine, but when commented and uncomment the second line, weights of rnn
        # will not be saved
        self.lstm = tf.keras.layers.CuDNNGRU(2)
        # self.lstm = tf.keras.layers.Bidirectional(tf.keras.layers.CuDNNGRU(2))

    def call(self, inputs):
        return self.lstm(inputs)


i = tf.random_normal((3, 4, 5))
m = Model()
o = m(i)
m.save_weights(""test/save"")

reader = tf.train.NewCheckpointReader(""test/save"")
# weights are not store when using CuDNNLSTM/GRU with bidirectional layer
print(reader.get_variable_to_dtype_map().keys())

```
"
20664,ImportError: cannot import name bessel_i0,"Hi, I cannot figure out what happened. Anyone help?
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/yinxiaoyi/Software/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *  # pylint: disable=redefined-builtin
  File ""/home/yinxiaoyi/Software/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>
    import numpy as np
  File ""/home/yinxiaoyi/Software/anaconda2/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>
    from . import add_newdocs
  File ""/home/yinxiaoyi/Software/anaconda2/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>
    from numpy.lib import add_newdoc
  File ""/home/yinxiaoyi/Software/anaconda2/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 3, in <module>
    import math
  File ""math/__init__.py"", line 11, in <module>
    from tensorflow.python import bessel_i0
ImportError: cannot import name bessel_i0
"
20663,use pre-trained embedding with tf.feature_column.embedding_column by initializer,"I  used pre-trained embedding with tf.feature_column.embedding_column by parameter initializer  ,my code is blow

# code here
```
weight, vocab_size, emb_size = _create_pretrained_emb_from_txt(FLAGS.vocab,FLAGS.pre_emb)
 W = tf.get_variable(trainable=False, name='W', shape=[vocab_size, emb_size])
embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, emb_size])
embedding_init = W.assign(embedding_placeholder)

sess = tf.Session()
sess.run(embedding_init, feed_dict={embedding_placeholder: weight})

itemx_vocab = tf.feature_column.categorical_column_with_vocabulary_file(
    key='itemx',
    vocabulary_file=FLAGS.vocabx)

itemx_emb = tf.feature_column.embedding_column(itemx_vocab,
                                               dimension=emb_size,
                                               initializer=W,
                                               trainable=False)
```

# it says:

 ValueError: initializer must be callable if specified. Embedding of column_name: itemx

### then I tried set` lambda w: W`. 
```
itemx_emb = tf.feature_column.embedding_column(itemx_vocab,
                                               dimension=emb_size,
                                               initializer=lambda w: W,
                                               trainable=False)
```

but it doesn't work and  got a typeError: TypeError: () got an unexpected keyword argument 'dtype'

I'm confused is this a bug?

thks fou your guys suggestion! 

"
20662,"Compilation with Cuda 9.1 Cudnn 7.0.5, In python3, ""help(tensorflow)"" caused core dump","Have I written custom code: No
OS Platform and Distribution: Ubuntu 16.04 Server
TensorFlow installed from: source cloned from current branch
TensorFlow version: 1.9.0-rc2 the active version from the master branch
Bazel version: 1.5.0, a most recent release
CUDA/cuDNN version: 9.1/7.0.5
GPU model and memory: NVIDIA GTX 1080Ti 11GB
Test Environment: Python 3.5
Exact command to reproduce: python3 -c ""import tensorflow; help(tensorflow)""


If this could help....
Code Version: v1.9.0-rc2-205-g216887d 1.9.0-rc0



Hello guys, 
I recently cloned the bench and compiled with all (Y/n) set to y and (y/N) set to n. while the Cuda is set to yes.
The cuda environment is 9.1 and cudnn is 7.0.5.
The compilation succeeded, and I tried some subtle code to run tensorflow on gpu, it also returns.
Then I tried to read some help, but when I typed ""help(tensorflow)"" python halted and crushed.

> Segmentation fault (core dumped)

I'm wondering why this happened? Could you lend some help?


The template data is filled up, If you require a snapshot of installed python packages, I would make a list.
Currently I have no idea what's happening, There is no error report, only a core dump....

Fine, after all I'll try an earlier version of tensorflow.

Today I rebuilt with tensorflow 1.8.0 release source code, it works. It might imply there was something wrong with the 1.9.0 source
"
20661,tensorflow c++ api session->run() consumes too much time.,"Here are my codes:
```
void tensorflow_class_wrapper::load_model(string model_path)
{
	// set gpu options
	SessionOptions opts;
	GraphDef graph_def;
	ConfigProto* config = &opts.config;
	opts.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(0.3);
	opts.config.mutable_gpu_options()->set_allow_growth(true);
	assert(_session == NULL);
	TF_CHECK_OK(NewSession(opts, &_session));
	printf(""Create session done.\n"");

	//read the pb file into the grapdef member
	TF_CHECK_OK(ReadBinaryProto(Env::Default(), model_path, &graph_def));
	printf(""Load model done.\n"");

	//Add the graph to the session
	TF_CHECK_OK(_session->Create(graph_def));
	printf(""Create graph done.\n"");
	return;
}
```

```
void tensorflow_class_wrapper::predict()
{
	// convert data
	convert_data();
	printf(""Convert data done.\n"");

	clock_t t1, t2;
	t1 = clock();
	//The session will initialize the outputs
	vector<Tensor> outputs;
	TF_CHECK_OK( _session->Run(_input_dict,  _output_tensor_names , {}, &outputs));
	t2 = clock();
	cout << ""predict consume time: "" << float(t2 - t1) / CLOCKS_PER_SEC << endl;

	for (auto _o : outputs)
		cout << _o.DebugString() << endl;
	vector<vector<float>> predict_res(outputs.size());
	for (int nr = 0; nr < outputs.size(); nr++)
		predict_res[nr].assign(outputs[nr].flat<float>().data(), outputs[nr].flat<float>().data() + outputs[nr].dims());

}
```
My input img size is 512x512x1. PB model is just single convolution and the output shape is 512x512x1. It's confused that session->Run() consumes over 2 seconds even more. By the way, i compile tensorflow1.8 release codes to get tensorflow lib/dll. Is there anything wrong when i compiled the source codes? Or I did not set some options in the right way?"
20660,Attempting to use uninitialized value Variable,"Hi all,

I had Import a graph with go:

```go
			bmodel := res.GetBytes(0)

			graph := tf.NewGraph()
			if err := graph.Import(bmodel, """"); err != nil {
				log.Error(err)
				continue
			}

			sess, err := tf.NewSession(graph, nil)
			if err != nil {
				log.Error(err)
				continue
			}

			p.model = &tf.SavedModel{
				Graph:   graph,
				Session: sess,
			}
```

use the model:

```go
	output := tf.Output{
		Op:    p.model.Graph.Operation(""input""),
		Index: 0,
	}
	target, err := tf.NewTensor([][]float32{})

	if err != nil {
		log.Error(err)
		return ""accept"", err
	}

	tensor, err := tf.NewTensor([][]float32{preData})
	if err != nil {
		log.Error(err)
		return ""accept"", err
	}

	feeds := map[tf.Output]*tf.Tensor{
		output: tensor,
		p.model.Graph.Operation(""target"").Output(0): target,
	}

	fetches := []tf.Output{
		{
			Op:    p.model.Graph.Operation(""infer""),
			Index: 0,
		},
	}

	result, err := p.model.Session.Run(
		feeds,
		fetches,
		nil,
	)
```

but get error: 

```sh
2018-07-10 14:36:41 ERROR (shield-ai-engine/predict.go:188):20187 Attempting to use uninitialized value Variable_3
	 [[Node: Variable_3/read = Identity[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Variable_3)]]
```
It haven't init in NewSession? how to initialize variable?

env:

OS: mac 10.11.6
python version: python2.7
tensorflow install base on https://www.tensorflow.org/install/install_go
tensorflow-go version: v1.9.0-rc2
go version: go1.9.2 darwin/amd64"
20659,tensorflow,"
"
20658,How to call object tracking  by C++ Interface ? ,"Object tracking support in the [demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) is provided by libtensorflow_demo.so. I successfully had built it with the cmake options. But I'm not sure how to call the dynamic library correctly, using the C++ interface. The way the JAVA is called in the demo, but I don't really understand the specific process of its work. I hope I can call it in the way of C++ to implement my application, and do not know how to do it?

@andrewharp ,

Please help, Thanks
"
20656,please change allocation.cc line 102 copied_buffer_ = std::move(buffer);,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
CentOS 6.9 
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.9.0-rc2
- **Python version**: 
3.6.2
- **Bazel version (if compiling from source)**:
0.14.1
- **GCC/Compiler version (if compiling from source)**:
gcc 6.2.0
- **CUDA/cuDNN version**:
cuda 9.0 and cudnn 7.1.1
- **GPU model and memory**:
NVIDIA K80 (12GB/GPU) and P100(16GB/GPU)
- **Exact command to reproduce**:
>   exec env - \
>     CUDA_TOOLKIT_PATH=/apps/cuda/9.0 \
>     CUDNN_INSTALL_PATH=/apps/cudnn/7.1.1-cuda9.0 \
>     GCC_HOST_COMPILER_PATH=/apps/gcc/6.2.0/wrapper/gcc \
>     LD_LIBRARY_PATH=/apps/gcc/6.2.0/lib/gcc/x86_64-pc-linux-gnu/6.2.0:/apps/gcc/6.2.0/lib64:/apps/nccl/2.2.13-cuda9.0/lib:/apps/openmpi/3.1.0/lib:/apps/openmpi/3.1.0/lib/profilers:/apps/intel-ct/17.0.1.132/mkl/lib/intel64:/apps/python3/3.6.2/lib:/apps/binutils/2.25/lib:/apps/java/jdk1.8.0_60/lib:/apps/cudnn/7.1.1-cuda9.0/lib64:/apps/cuda/9.0/extras/CUPTI/lib64:/apps/cuda/9.0/lib64 \
>     NCCL_INSTALL_PATH=/apps/nccl/2.2.13-cuda9.0 \
>     PATH=/apps/gcc/6.2.0/wrapper:/apps/gcc/6.2.0/bin:/apps/openmpi/wrapper/fortran:/apps/openmpi/3.1.0/bin:/apps/python3/3.6.2/bin:/apps/binutils/2.25/bin:/apps/bazel/0.14.1/bin:/apps/java/jdk1.8.0_60/bin:/apps/cuda/9.0/bin:/home/900/yxs900/.local/bin:/opt/bin:/bin:/usr/bin \
>     PWD=/proc/self/cwd \
>     PYTHON_BIN_PATH=/apps/python3/3.6.2/bin/python3 \
>     PYTHON_LIB_PATH=/apps/python3/3.6.2/lib/python3.6/site-packages \
>     TF_CUDA_CLANG=0 \
>     TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0 \
>     TF_CUDA_VERSION=9.0 \
>     TF_CUDNN_VERSION=7 \
>     TF_NCCL_VERSION=2 \
>     TF_NEED_CUDA=1 \
>     TF_NEED_OPENCL_SYCL=0 \
>   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/contrib/lite/_objs/framework/tensorflow/contrib/lite/allocation.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/contrib/lite/_objs/framework/tensorflow/contrib/lite/allocation.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/k8-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote external/gemmlowp -iquote bazel-out/k8-opt/genfiles/external/gemmlowp -iquote external/flatbuffers -iquote bazel-out/k8-opt/genfiles/external/flatbuffers -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/genfiles/third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem tensorflow/contrib/lite/schema -isystem bazel-out/k8-opt/genfiles/tensorflow/contrib/lite/schema -isystem bazel-out/k8-opt/bin/tensorflow/contrib/lite/schema -isystem external/flatbuffers/include -isystem bazel-out/k8-opt/genfiles/external/flatbuffers/include -isystem bazel-out/k8-opt/bin/external/flatbuffers/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '-D_GLIBCXX_USE_CXX11_ABI=0' '-march=native' -DFARMHASH_NO_CXX_STRING -c tensorflow/contrib/lite/allocation.cc -o bazel-out/k8-opt/bin/tensorflow/contrib/lite/_objs/framework/tensorflow/contrib/lite/allocation.pic.o


### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The build stops because the variables buffer and copied_buffer have different types,  char*& and const char*&, respectively, and std::swap expects identical types of their inputs and code found no candidate for std::swap, see details in the error message below. Simply change line 102 in tensorflow/contrib/lite/allocation.cc {{ copied_buffer_ = std::move(buffer); }} to {{copied_buffer_.reset(const_cast<char const *>(buffer.release()));}} fix the bug.

### Source code / logs
tensorflow/contrib/lite/allocation.cc:102:36:   required from here
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: error: no matching function for call to 'swap(const char*&, char*&)'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_pair.h:59:0,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/utility:70,
                 from tensorflow/contrib/lite/allocation.cc:27:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/move.h:179:5: note: candidate: template<class _Tp> typename std::enable_if<std::__and_<std::is_move_constructible<_Tp>, std::is_move_assignable<_Tp> >::value>::type std::swap(_Tp&, _Tp&)
     swap(_Tp& __a, _Tp& __b)
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/move.h:179:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   deduced conflicting types for parameter '_Tp' ('const char*' and 'char*')
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_pair.h:59:0,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/utility:70,
                 from tensorflow/contrib/lite/allocation.cc:27:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/move.h:202:5: note: candidate: template<class _Tp, long unsigned int _Nm> typename std::enable_if<std::__is_swappable<_Tp>::value>::type std::swap(_Tp (&)[_Nm], _Tp (&)[_Nm])
     swap(_Tp (&__a)[_Nm], _Tp (&__b)[_Nm])
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/move.h:202:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types '_Tp [_Nm]' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/utility:70:0,
                 from tensorflow/contrib/lite/allocation.cc:27:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_pair.h:403:5: note: candidate: template<class _T1, class _T2> void std::swap(std::pair<_T1, _T2>&, std::pair<_T1, _T2>&)
     swap(pair<_T1, _T2>& __x, pair<_T1, _T2>& __y)
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_pair.h:403:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::pair<_T1, _T2>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/vector:64:0,
                 from ./tensorflow/contrib/lite/allocation.h:22,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_vector.h:1557:5: note: candidate: template<class _Tp, class _Alloc> void std::swap(std::vector<_Tp, _Alloc>&, std::vector<_Tp, _Alloc>&)
     swap(vector<_Tp, _Alloc>& __x, vector<_Tp, _Alloc>& __y)
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_vector.h:1557:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::vector<_Tp, _Alloc>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/vector:65:0,
                 from ./tensorflow/contrib/lite/allocation.h:22,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:112:3: note: candidate: void std::swap(std::_Bit_reference, std::_Bit_reference)
   swap(_Bit_reference __x, _Bit_reference __y) noexcept
   ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:112:3: note:   no known conversion for argument 1 from 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}' to 'std::_Bit_reference'
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:120:3: note: candidate: void std::swap(std::_Bit_reference, bool&)
   swap(_Bit_reference __x, bool& __y) noexcept
   ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:120:3: note:   no known conversion for argument 1 from 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}' to 'std::_Bit_reference'
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:128:3: note: candidate: void std::swap(bool&, std::_Bit_reference)
   swap(bool& __x, _Bit_reference __y) noexcept
   ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:128:3: note:   no known conversion for argument 2 from 'char*' to 'std::_Bit_reference'
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/list:63:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:18,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_list.h:1918:5: note: candidate: template<class _Tp, class _Alloc> void std::swap(std::list<_Tp, _Alloc>&, std::list<_Tp, _Alloc>&)
     swap(list<_Tp, _Alloc>& __x, list<_Tp, _Alloc>& __y)
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_list.h:1918:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::list<_Tp, _Alloc>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/string:52:0,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/stdexcept:39,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/array:39,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/tuple:39,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/functional:55,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/memory:79,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/basic_string.h:5287:5: note: candidate: template<class _CharT, class _Traits, class _Alloc> void std::swap(std::basic_string<_CharT, _Traits, _Alloc>&, std::basic_string<_CharT, _Traits, _Alloc>&)
     swap(basic_string<_CharT, _Traits, _Alloc>& __lhs,
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/basic_string.h:5287:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::basic_string<_CharT, _Traits, _Alloc>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/tuple:39:0,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/functional:55,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/memory:79,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/array:275:5: note: candidate: template<class _Tp, long unsigned int _Nm> void std::swap(std::array<_Tp, _Nm>&, std::array<_Tp, _Nm>&)
     swap(array<_Tp, _Nm>& __one, array<_Tp, _Nm>& __two)
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/array:275:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::array<_Tp, _Nm>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/functional:55:0,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/memory:79,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/tuple:1546:5: note: candidate: template<class ... _Elements> void std::swap(std::tuple<_Elements ...>&, std::tuple<_Elements ...>&)
     swap(tuple<_Elements...>& __x, tuple<_Elements...>& __y)
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/tuple:1546:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::tuple<_Elements ...>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:79:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/functional:2238:5: note: candidate: template<class _Res, class ... _Args> void std::swap(std::function<_Res(_ArgTypes ...)>&, std::function<_Res(_ArgTypes ...)>&)
     swap(function<_Res(_Args...)>& __x, function<_Res(_Args...)>& __y)
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/functional:2238:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::function<_Res(_ArgTypes ...)>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~


"
20653,Error with Quantizing MNIST with tf.lite,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: bazel
- **TensorFlow version (use command below)**:  commit bcf7e315b4031b3c355af12ca2a4961bcd25c248
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: Build label: 0.14.1
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: 1080ti
- **Exact command to reproduce**:

`bazel build tensorflow/contrib/lite/toco:toco && \
  ./bazel-bin/tensorflow/contrib/lite/toco/toco \
  --input_file=comple_path/frozen_model.pb \
  --output_file=tflite_model.tflite \
  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \
  --inference_type=QUANTIZED_UINT8 \
  --input_shape=""1,28, 28,1"" \
  --input_array=""Inputs"" \
  --output_array=""fc2/Relu""  `


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

`2018-07-09 14:00:04.623621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign
2018-07-09 14:00:04.623726: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign
2018-07-09 14:00:04.623752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd
2018-07-09 14:00:04.623829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.623846: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd
2018-07-09 14:00:04.623879: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.623914: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.623930: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd
2018-07-09 14:00:04.623961: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.624008: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign
2018-07-09 14:00:04.624030: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign
2018-07-09 14:00:04.624098: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.624114: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd
2018-07-09 14:00:04.624146: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.624178: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.624193: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd
2018-07-09 14:00:04.624224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.624270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign
2018-07-09 14:00:04.624292: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign
2018-07-09 14:00:04.624355: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.624371: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd
2018-07-09 14:00:04.624403: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.624436: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.624451: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd
2018-07-09 14:00:04.624482: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.624522: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign
2018-07-09 14:00:04.624547: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: Assign
2018-07-09 14:00:04.624606: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.624621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd
2018-07-09 14:00:04.624652: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.624685: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.624700: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignAdd
2018-07-09 14:00:04.624732: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: AssignSub
2018-07-09 14:00:04.627783: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 229 operators, 362 arrays (0 quantized)
2018-07-09 14:00:04.630029: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 173 operators, 282 arrays (0 quantized)
2018-07-09 14:00:04.632293: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 173 operators, 282 arrays (0 quantized)
2018-07-09 14:00:04.672712: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 107 operators, 201 arrays (1 quantized)
2018-07-09 14:00:04.674373: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 107 operators, 201 arrays (1 quantized)
2018-07-09 14:00:04.676019: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 107 operators, 201 arrays (1 quantized)
2018-07-09 14:00:04.676780: F tensorflow/contrib/lite/toco/tooling_util.cc:1612] Array conv1/act_quant/AssignMinEma/conv1/act_quant/min/Pow, which is an input to the Sub operator producing the output array conv1/act_quant/AssignMinEma/conv1/act_quant/min/sub_2, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.
Aborted (core dumped)
`

I tried it with default_ranges_min, max and it throws the following error
`
2018-07-09 13:56:44.667379: E tensorflow/core/util/command_line_flags.cc:124] Couldn't interpret value  for flag default_ranges_min.
2018-07-09 13:56:44.667451: E tensorflow/core/util/command_line_flags.cc:124] Couldn't interpret value  for flag default_ranges_max.
`

![screenshot from 2018-07-09 14-03-56](https://user-images.githubusercontent.com/4759327/42475903-ae68adc6-83bb-11e8-9317-fe565127f784.png)



### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20651,MirroredStrategy doesn't support feature embedding across multiple GPUs,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0.176
- **GPU model and memory**: 4 Tesla P40 22919MiB
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I want to train a NN model with 4 GPUs, and this is how I set up my model config in tf.estimator.DNNClassifier:
`distribution = tf.contrib.distribute.MirroredStrategy()
    run_config = tf.estimator.RunConfig(
        train_distribute=distribution,
        log_step_count_steps=1000
    )`

It works when I don't have embedding features, and it works if I comment out the ""train_distribute=distribution"" part. When I have embedding features with the distribution config, this is the error I get:
`InternalError: No unary variant device copy function found for direction: 1 and Variant type_name: tensorflow::Tensor
	 [[Node: FunctionBufferingResourceGetNext_1 = FunctionBufferingResourceGetNext[output_types=[DT_INT64, DT_FLOAT, DT_FLOAT, DT_VARIANT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, D`

I'm wondering whether the current MirroredStrategy doesn't support feature embedding?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20647,tf-trt converting. support for FC.,"Hello.
tensorflow.contrib.tensorrt.convert will support the fully connected layer?
Thx.
"
20644,feature request: Robbins-Monro type learning rate decay,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: 1.8
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

I was wondering if there is any appetite for a [Robbins-Monro](https://en.wikipedia.org/wiki/Stochastic_approximation#Robbins%E2%80%93Monro_algorithm) type learning rate decay in tensorflow? The decay would be roughly (a more general solution is implemented at the bottom):

```python
decayed_learning_rate = learning_rate * (global_step + 1) ^ (-decay_rate) 
```

As far as I can tell it is not already implemented in tensorflow, which surprised me since I think this is the learning rate decay rate required for theoretical convergence using the Adam optimizer in Section 4 of [the paper](https://arxiv.org/pdf/1412.6980.pdf), which has:

```python
alpha_t = alpha / sqrt(t)
```

which is the same as the first equation with `decay_rate = 0.5`, and I assume they start at `t = 1` while tensorflow starts with `global_step = 0`.

I have an implementation I have been using (mostly copied from the [already implemented ones](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/learning_rate_decay.py)):

```python
def robbins_monro_decay(learning_rate,
                        global_step,
                        decay_steps,
                        decay_rate,
                        staircase=False,
                        name=None):
    """"""A Robbins-Monro type decay""""""

    if global_step is None:
        raise ValueError(""global_step is required for robbins_monro_decay."")

    with tf.name_scope(
            name,
            ""RobbinsMonroDecay"",
            [learning_rate, global_step, decay_steps, decay_rate]) as name:

        learning_rate = tf.convert_to_tensor(
            learning_rate, name=""learning_rate"")
        dtype = learning_rate.dtype
        decay_steps = tf.cast(decay_steps, dtype)
        decay_rate = tf.cast(decay_rate, dtype)
        global_step = tf.cast(global_step, dtype)
        p = global_step / decay_steps

        if staircase:
            p = tf.floor(p)

        return tf.multiply(
            learning_rate, tf.pow(p + 1, -decay_rate), name=name)
```

I can make a full pull request if that would be useful? I would be more than happy to add some additional documentation/code or change any of the code/naming/whatever.

Thanks!"
20643,Custom loss function for computation graph,I want to evaluate my model in android . But it has a custom loss function similar to the one used in popular siamese neural network. How can I use TensorflowInferenceInterface in android to achieve it.
20642,init_fn bug in estimator._train_model_distributed,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 2.7
- **CUDA/cuDNN version**: 7.0
- **GPU model and memory**: GTX 980M

### Describe the problem
There is a bug in https://github.com/tensorflow/tensorflow/blob/4e883df1eaf52b7c655365eb5f6ecd9e5fe37457/tensorflow/python/estimator/estimator.py#L1327.

init_fn should take two arguments. However, the init_fn obtained in https://github.com/tensorflow/tensorflow/blob/4e883df1eaf52b7c655365eb5f6ecd9e5fe37457/tensorflow/python/estimator/estimator.py#L1269 is a lambda function which only takes one argument.


Currently, I am using the following line as a workaround.
```
init_fn=init_fn and (lambda x, y: init_fn(y)))
```"
20640,Weird issue when using TextLineReader nested in a function,"### System information
- WIndows 7 enterprise:
- TensorFlow installed from conda:
- TensorFlow version (1.8.0):
- Python version (3.6.5), installed with Anaconda: 
- Not using GPU
- Reproduction: unzip the following zip file, two files, test.py and test.txt, under tensorflow environment, 
run test.py.
[tf bug report.zip](https://github.com/tensorflow/tensorflow/files/2179025/tf.bug.report.zip)

### Describe the problem
I was testing the functionality of TextLineReader, a very simple try. Here is my code, just a few lines:
###### MY CODE#######
import tensorflow as tf

def input_func():
    file_queue = tf.train.string_input_producer([""D:/tmp_10_data_test.txt""], num_epochs=5)
    reader = tf.TextLineReader(skip_header_lines=1)
    return reader.read_up_to(file_queue, num_records=5)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(tf.local_variables_initializer())
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord, sess=sess)
    print(sess.run(input_func()))
    coord.request_stop()
    coord.join(threads)


###################
The weird part is, when I'm running this piece of code, the program keeps running without any error, any output, and it just runs forever. This issue happens when I run the code with PyCharm and Jupyter Notebook. I resolved it by moving the file_queue declaration outside the function as:
#####################
file_queue = tf.train.string_input_producer([""D:/tmp_10_data_test.txt""], num_epochs=5)
def input_func():
    reader = tf.TextLineReader(skip_header_lines=1)
    return reader.read_up_to(file_queue, num_records=5)
######################
And the program runs correctly and outputs the result as expected. But This is just weird, what happened when I declare the file queue inside the very simple function? Shouldn't it be the same regardless of where it is declared? I guess you can replace the file with your own multi-line txt file and the problem should still exist, although I didn't try it...
"
20639,Assignment inside while loop affected by concurrency,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
      Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
        Windows 8 (CPU)
- **TensorFlow installed from (source or binary)**:
        Binary
- **TensorFlow version (use command below)**:
       1.2.1
- **Python version**: 
      Python 3.6.5 |Anaconda
- **Bazel version**: 
    N/A
- **CUDA/cuDNN version**:
      N/A
- **GPU model and memory**:
     N/A
- **Exact command to reproduce**:

No gpu

### Describe the problem

I wrote this code to answer a SO question but then realized the results are inconsistent probably due to the concurrency involved. The correct answer is returned repeatedly a few times but then very randomly. All other answers are wrong.
I have also read about a switch that turns off concurrent execution but generally the concurrency API and the memory model take care of almost all cases without the involvement of the programmer. In the case of Java it is [JMM](http://download.oracle.com/otndocs/jcp/memory_model-1.0-pfd-spec-oth-JSpec/)

It isn't clear whether this is concurrency or parallelism ?


### Source code / logs

```
import tensorflow as tf

v = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
n = len(v)
a1 = tf.Variable(v, name = 'a')

def cond(i, _):
    return i < n

s = tf.InteractiveSession()
s.run(tf.global_variables_initializer())

def body( i, _):
    x = a1[i-1]
    y = a1[i-2]
    z = tf.add(x,y)
    op = a1[i].assign( tf.add(x,y) )
    increment = tf.add(i, 1)
    return increment, op

print(s.run(tf.while_loop(cond, body, [2, a1])))
```
"
20638,[Feature Request] Deformable Convolution,"Hi,  TF.

I wonder if you have any plan to add Deformable Convolution Feature.
https://arxiv.org/abs/1703.06211
https://github.com/msracver/Deformable-ConvNets

Thank you always, for nice work.

------------------------"
20635,Unexpected result when load new model versions,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7.0
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**:  4.8.5
- **CUDA/cuDNN version**: 5.1.10
- **GPU model and memory**: p40 etc
- **Exact command to reproduce**: 

### Describe the problem
Using tensorflow serving with hdfs and serving automatic loading new versions based on 1 second interval. Due to the load of hdfs, variables.index file may appeared later than saved_model.pb.

So, the unexpected things appeard, you can see the log in the next few lines.

### Source code / logs

when predict serving returns :

Attempting to use uninitialized value conv1d_1_1/kernel
       [[Node: conv1d_1_1/kernel/read = Identity[T=DT_FLOAT, _class=[""loc:@conv1d_1_1/kernel""], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](conv1d_1_1/kernel)]]
       [[Node: dense_1_1/BiasAdd/_31 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_178_dense_1_1/BiasAdd"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

when load new version, serving print :

2018-06-25 16:56:40.483957: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:165] The specified SavedModel has no variables; no checkpoints were restored.

### Some thoughts

It's hard to say who is wrong, but should tensorflow consider and prevent this inconsistency?

"
20631,Broken TFLite Model Benchmark Tool Example,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: adb shell taskset f0 /data/local/tmp/benchmark_model --graph=/data/local/tmp/mobilenet.tflite --input_layer=""input"" --input_layer_shape=""1,224,224,3"" --num_threads=-1

### Describe the problem
I'm attempting to replicate an Android TFLite model benchmark from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/benchmark.

The benchmark works on my own model but the example is broken. The tflife graph is the mobilenet graph from https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip

The benchmark otherwise works with my own tflite models.

### Source code / logs
I'm running the following command:

```
adb shell taskset f0 /data/local/tmp/benchmark_model --graph=/data/local/tmp/mobilenet.tflite --input_layer=""input"" --input_layer_shape=""1,224,224,3"" --num_threads=-1
```

```
WARNING: linker: ""/data/local/tmp/benchmark_model"" unused DT entry: type 0xf arg 0x6ca
native : benchmark_model.cc:469 Graph: [/data/local/tmp/mobilenet.tflite]
native : benchmark_model.cc:470 Init ops:
native : benchmark_model.cc:471 Input layers: [input]
native : benchmark_model.cc:472 Input shapes: [1,224,224,3]
native : benchmark_model.cc:473 Input types: [float]
native : benchmark_model.cc:474 Output layers: [output:0]
native : benchmark_model.cc:475 Target layers: []
native : benchmark_model.cc:476 Num runs: [1000]
native : benchmark_model.cc:477 Inter-inference delay (seconds): [-1.0]
native : benchmark_model.cc:478 Inter-benchmark delay (seconds): [-1.0]
native : benchmark_model.cc:480 Num threads: [-1]
native : benchmark_model.cc:481 Benchmark name: []
native : benchmark_model.cc:482 Output prefix: []
native : benchmark_model.cc:483 Show sizes: [0]
native : benchmark_model.cc:484 Warmup runs: [1]
native : benchmark_model.cc:251 Loading TensorFlow.
native : benchmark_model.cc:258 Got config, 0 devices
can't determine number of CPU cores: assuming 4
can't determine number of CPU cores: assuming 4
native : benchmark_model.cc:496 Initialized session in 0.00239s
native : benchmark_model.cc:327 Running benchmark for max 1 iterations, max -1 seconds without detailed stat logging, with -1s sleep between inferences
native : benchmark_model.cc:306 Error during inference: Invalid argument: Session was not created with a graph before Run()!
native : benchmark_model.cc:348 Failed on run 0
native : benchmark_model.cc:566 Timing failed with Invalid argument: Session was not created with a graph before Run()!
```
"
20630,Eager execution guide: using GradientTape with keras.model and tf.keras.layer,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Fedora 28
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  1.9
- **Python version**:  3.6
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:N/A

### Describe the problem

The eager execution doc 

https://www.tensorflow.org/programmers_guide/eager 

does not provide a simple complete example that shows how to use gradient tape optimization with keras layers (instead of tfe.Variables). 
It would be great if that could be added, as I seem to be getting gradients that are None when I try to replace tfe.Variables in the following, running, example (copied from the doc but even more simplified for ease of experimentation):

```
import tensorflow as tf
tf.enable_eager_execution()
tfe = tf.contrib.eager
n = 10
x = tf.random_normal([n, 2])
noise = tf.random_normal([n, 2])
y = x * 3 + 2 + noise

class Model(tf.keras.Model):
  def __init__(self):
    super(Model, self).__init__()
    self.W = tfe.Variable(5., name='weight')
    self.B = tfe.Variable(10., name='bias')
  def predict(self, inputs):
    return inputs * self.W + self.B

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)

model = Model()
with tf.GradientTape() as tape:
  error = model.predict(x) - y
  loss_value = tf.reduce_mean(tf.square(error))
gradients = tape.gradient(loss_value, model.variables)
print(gradients)
optimizer.apply_gradients(zip(gradients, model.variables),
                            global_step=tf.train.get_or_create_global_step())

```

by a Keras layer instead of weight and a bias (based on, but again simplified, the MNISTModel from the doc):

```
import tensorflow as tf
tf.enable_eager_execution()

n = 10
x = tf.random_normal([n, 2])
noise = tf.random_normal([n, 2])

y = x * 3 + 2 + noise

class Model(tf.keras.Model):
    def __init__(self):
        super(Model, self).__init__()
        self.dense1 = tf.keras.layers.Dense(units = 1, activation='relu')
    def call(self, inputs):
        return self.dense1(x)

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)

model = Model()
with tf.GradientTape() as tape:
  error = model.predict(x) - y
  loss_value = tf.reduce_mean(tf.square(error))
gradients = tape.gradient(loss_value, model.variables)
# now gradients are None
print(gradients)
optimizer.apply_gradients(zip(gradients, model.variables),
                            global_step=tf.train.get_or_create_global_step())
                            
```

In this version, the gradients are None. Same if I use another construct from the doc (again, simplified):

```
model = tf.keras.Sequential([
  tf.keras.layers.Dense(1, input_shape=(2,))  # must declare input shape
])
```

It would be great if the doc could be extended to show a complete tf.keras.layer example with GradientTape. Thank you!




"
20629,Improve the documentation for tf.tables_initializer,"### System information

No relevant for this issue.

### Describe the problem

The current documentation for [`tf.tables_initializer`](https://www.tensorflow.org/api_docs/python/tf/tables_initializer) states

> Returns an Op that initializes all tables of the default graph.

As a beginner, it is not clear to me why we would need tables in TF. So, I think the documentation should add a few examples of use-cases where tables are created (initialized) and used. In other words, the documentation should briefly answer the questions 

1. Why do we need tables in TF? 
2. What does it even mean to initialize a table? 
   1. In which cases would tables need (or not) to be initialized? 
3. What kind of tables are these? Hash-tables?
4. In general, what are examples of use-cases where tables are used?

In the documentation, there's also ""See the guide: Variables > Sparse Variable Updates"", but [Variables > Sparse Variable Updates](https://www.tensorflow.org/api_guides/python/state_ops#Sparse_Variable_Updates), if I understood correctly, doesn't answer the questions above.

Given that I don't think I am very qualified to answer the questions above, I decided to open this issue, instead of submitting a PR.

### Source code / logs
 
No relevant for this issue."
20628,prefetch_to_device doesn't overlap copy(HtoD) with computation and also may fail,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04.4 LTS
- **TensorFlow installed from (source or binary)**: pip3
- **TensorFlow version (use command below)**:1.8.0
- **Python version**: 3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**:GTX 1070
- **Exact command to reproduce**:
```
import tensorflow as tf
import numpy as np

x = tf.placeholder(shape=[None, 128], dtype=np.float32)
y = tf.placeholder(shape=[None], dtype=np.float32)

ds = tf.data.Dataset.from_tensor_slices((x, y))
ds = ds.repeat()
ds = ds.batch(1024)
############################################
prefetch = 1024 # if it's bigger than 512 it results in an error
############################################
ds = ds.apply(tf.contrib.data.prefetch_to_device(""/gpu:0"", prefetch))
it = ds.make_initializable_iterator()

w = tf.get_variable(
                    name='w',
                    initializer=tf.contrib.layers.xavier_initializer(),
                    shape=[128, 1],
                    dtype=tf.float32)
b = tf.get_variable(
                    name='b',
                    initializer=tf.contrib.layers.xavier_initializer(),
                    shape=[1],
                    dtype=tf.float32)

optimizer = tf.train.AdamOptimizer(1e-3)

next_x, next_y = it.get_next()
prediction = tf.matmul(next_x, w) + b
train = optimizer.minimize((prediction - next_y)**2)

with tf.Session() as session:
    x_val = np.random.normal(size=(1024*1024, 128)).astype(np.float32)
    y_val = np.random.normal(size=(1024 * 1024)).astype(np.float32)
    session.run(tf.global_variables_initializer())
    session.run(it.initializer, feed_dict={x: x_val, y: y_val})
    for _ in range(1024):
        session.run(train)
```


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.8.0-0-g93bc2e2072 1.8.0

### Describe the problem

If I specify `prefetch` size to 128 I don't see any parallel data copy(HtoD) overlapping gpu computation(check screenshot and nvidia profiler report).
<img width=""1072"" alt=""screen shot 2018-07-08 at 17 36 25"" src=""https://user-images.githubusercontent.com/2821871/42421527-412a3124-82d7-11e8-94e7-1f9114d88cec.png"">


And If I increase it it fails
```
$ python3 test.py 
2018-07-08 17:32:18.472300: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-07-08 17:32:18.550265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-07-08 17:32:18.550841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
totalMemory: 7.93GiB freeMemory: 7.83GiB
2018-07-08 17:32:18.550852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-07-08 17:32:18.711860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-08 17:32:18.711888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-07-08 17:32:18.711907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-07-08 17:32:18.712052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7568 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-07-08 17:32:25.310286: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:650] failed to record completion event; therefore, failed to create inter-stream dependency
2018-07-08 17:32:25.310311: I tensorflow/stream_executor/stream.cc:4737] stream 0x1448efe0 did not memcpy host-to-device; source: 0x7f8feae97000
2018-07-08 17:32:25.310316: E tensorflow/stream_executor/stream.cc:309] Error recording event in stream: error recording CUDA event on stream 0x1448f080: CUDA_ERROR_DEINITIALIZED; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.
2018-07-08 17:32:25.310323: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED
2018-07-08 17:32:25.310328: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:208] Unexpected Event status: 1
Aborted (core dumped)
```

It'd be nice to have some canonical examples how to use it.


[prefetch_on_device.nvvp.zip](https://github.com/tensorflow/tensorflow/files/2173870/prefetch_on_device.nvvp.zip)
"
20627,Predictions from provided tflite files of same Mobilenet model do not match,"Hi Guys. I downloaded the Mobilenet_v1_1.0_224 and Mobilenet_v1_1.0_224_quant from the links provided in the documentation. I then use the tflite models provided in both of these for prediction using the tflite interpeter [Link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#using-the-interpreter-from-a-model-file-) I am able to make the predictions. However, the tflite predictions from the 'quant' version are very random. The float tflite model makes the correct prediction while the quantised tfite model makes very weird predictions. I even comapred the answers to normal frozen models provided and the quantized tflite model does not match the predictions at all while the float tflite makes all the predictions same as the frozen .pb model. 

I then tried the same thing with different mobilenet versions but the same observations.
Kindly help.
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.10 dev
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:



### Describe the problem
I then use the tflite models provided in both of these for prediction using the tflite interpeter [Link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#using-the-interpreter-from-a-model-file-) I am able to make the predictions. However, the tflite predictions from the 'quant' version are very random. The float tflite model makes the correct prediction while the quantised tfite model makes very weird predictions. I even comapred the answers to normal frozen models provided and the quantized tflite model does not match the predictions at all while the float tflite makes all the predictions same as the frozen .pb model. 

I then tried the same thing with different mobilenet versions but the same observations.

I am also making sure that I use the exact same images for both so that is not an issue just in case.
Kindly help.

### Source code / logs
img = np.array(PIL.Image.open(image_path).resize((224, 224))).astype(np.uint8) / 128 - 1

	img = img.reshape(1,224,224,3)
	#print (img.shape)
	
	start = time.time()

	interpreter.set_tensor(input_details[0]['index'], img)

	interpreter.invoke()
	end = time.time()

	output_data = interpreter.get_tensor(output_details[0]['index'])
	#output_data_1 = interpreter.get_tensor(output_details[1]['index'])

	#print(output_data.argmax(),output_data.max())
	label_map = imagenet.create_readable_names_for_imagenet_labels()  
	print(""Top 1 Prediction: "", output_data.argmax(),label_map[output_data.argmax()], output_data.max(), k+1)
"
20626,Dataset.concatenate() agrees to concat dictionaries with different keys,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.13.5
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem

It seems like Dataset.concatenate will concatenate datasets of dictionaries with different keys (values from the second key will be concatenated to the first one). 
(a small demo is attached)

I've looked at `python/data/util/nest.py` and in `_recursive_assert_same_structure` it seems like `_yield_value` only returns values for dictionaries. 
Is that intended? I would expect it to either fail or put None in the missing fields. 

### Source code / logs

The following code: 
```
ds1 = tf.data.Dataset.from_tensor_slices({'f1': list(range(20)), 'f2': [1]*20})
ds2 = tf.data.Dataset.from_tensor_slices({'f2': list(range(100,120)), 'f3': [2]*20})
dataset = ds1.concatenate(ds2).batch(5)
curr_batch = dataset.make_one_shot_iterator().get_next()

with tf.Session() as sess:
  for _ in range(8):
    data = sess.run(curr_batch)
    print(data)
```
Results in:
```
{'f1': array([0, 1, 2, 3, 4], dtype=int32), 'f2': array([1, 1, 1, 1, 1], dtype=int32)}
{'f1': array([5, 6, 7, 8, 9], dtype=int32), 'f2': array([1, 1, 1, 1, 1], dtype=int32)}
{'f1': array([10, 11, 12, 13, 14], dtype=int32), 'f2': array([1, 1, 1, 1, 1], dtype=int32)}
{'f1': array([15, 16, 17, 18, 19], dtype=int32), 'f2': array([1, 1, 1, 1, 1], dtype=int32)}
{'f1': array([100, 101, 102, 103, 104], dtype=int32), 'f2': array([2, 2, 2, 2, 2], dtype=int32)}
{'f1': array([105, 106, 107, 108, 109], dtype=int32), 'f2': array([2, 2, 2, 2, 2], dtype=int32)}
{'f1': array([110, 111, 112, 113, 114], dtype=int32), 'f2': array([2, 2, 2, 2, 2], dtype=int32)}
{'f1': array([115, 116, 117, 118, 119], dtype=int32), 'f2': array([2, 2, 2, 2, 2], dtype=int32)}
```
"
20625,How do I represent a TShape in the TF_Output object ?,"In tesnorflow/core array_op.cc file,the Reshape defined as follows [Source Code](https://github.com/tensorflow/tensorflow/blob/35287be3bb7daa0448af064f5d005a25201d6853/tensorflow/core/ops/array_ops.cc#L1274)
```c
REGISTER_OP(""Reshape"")
    .Input(""tensor: T"")
    .Input(""shape: Tshape"")
    .Output(""output: T"")
    .Attr(""T: type"")
    .Attr(""Tshape: {int32, int64} = DT_INT32"")
    .SetShapeFn([](InferenceContext* c) {
      return SetOutputShapeForReshape(c);
    });
```
in tensorflow/c_api.cc file,the TF_AddInput and TF_Output define as follows
```c
void TF_AddInput(TF_OperationDescription* desc, TF_Output input) {
  desc->node_builder.Input(&input.oper->node, input.index);
}
```
```c
typedef struct TF_Output {
  TF_Operation* oper;
  int index;  // The index of the output within oper.
} TF_Output;
```
When I bind ""Reshape"" method in c#,the first parameter ""input tensor:T"" can be replaced with the previous return value .The second paramater type is ""long[]"". How do i convert object type  'long[]' to 'TF_Output'?

"
20624,Makefile: build_all_ios.sh - No rule to make target 'distclean'.  Stop.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.5
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**:  -
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**:  just execute ./build_all_ios.sh

Hello, 

when building TensorFlow for iOS using the Makefile (./build_all_ios.sh), I am getting the following error:

................
rm -f gogo/cpp_no_group/datasets/google_message1/proto2/.deps/cpp_no_group_benchmark-benchmark_message1_proto2.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message1/proto3/.deps/cpp_no_group_benchmark-benchmark_message1_proto3.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message2/.deps/cpp_no_group_benchmark-benchmark_message2.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_1.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_2.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_3.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_4.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_5.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_6.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_7.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_8.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message4/.deps/cpp_no_group_benchmark-benchmark_message4.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message4/.deps/cpp_no_group_benchmark-benchmark_message4_1.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message4/.deps/cpp_no_group_benchmark-benchmark_message4_2.pb.Po
rm -f gogo/cpp_no_group/datasets/google_message4/.deps/cpp_no_group_benchmark-benchmark_message4_3.pb.Po
rm -f python/.deps/libbenchmark_messages_la-python_benchmark_messages.Plo
rm -f util/.deps/gogo_data_scrubber-gogo_data_scrubber.Po
rm -f util/.deps/protoc_gen_gogoproto-protoc-gen-gogoproto.Po
rm -f Makefile
Making distclean in third_party/googletest
make[1]: *** No rule to make target `distclean'.  Stop.
make: *** [distclean-recursive] Error 1"
20622,InMemoryEvaluatorHook failed to control printing freq with every_n_iter ,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  v1.9.0-rc2-202-g6d5b8b7cae 1.10.0-dev20180707
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:  https://gist.github.com/matthew-z/aec927462416969b9b4e9c39a15eabca
 
### Describe the problem

InMemoryEvaluatorHook prints the evaluation result after every training iteration even every_n_iter is set to 10.
 
### Source code / logs

Source code: [https://gist.github.com/matthew-z/aec927462416969b9b4e9c39a15eabca](https://gist.github.com/matthew-z/aec927462416969b9b4e9c39a15eabca)


Log: https://gist.github.com/matthew-z/921990af6957a1d9292759c933a1c16a"
20621,error LNK 2001 private:void _cdecl tensorflow::GraphDef::InternalSwap(class tensorflow::GraphDef *),"I have built tensorflow.lib and tensorflow.dll successfully in visual studio 2015,but I'm trying to use those two files I got this error LNK 2001 private:void _cdecl tensorflow::GraphDef::InternalSwap(class tensorflow::GraphDef *)
Can anyone help me to fix this error?

OS:Win 10 visual studio 2015 
CMake 3.12.0
swigwin-3.0.12
GPU:Nivdia 1080 ti
Cuda 9.0 
Cudnn 7
I just following this website
https://hk.saowen.com/a/e1c9146c6f02026ab6d25f79c5a21a847be18c734ff0dfe58bf74b72331b604b
"
20620,Why dense layer cannot be speed up in tf.contrib.trt ?,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**:  2.7.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 / 7.0.5
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem

When using tf.contrib.trt.create_inference_graph, I meet the following error, it seems that dense layer is not support because the input tensor is not rank 4 ? Why has this request on dense layer ?

> subgraph conversion error for subgraph_index:9 due to: ""Unimplemented: Require 4 dimensional input. Got 2 dense/MatMul"" SKIPPING

### Source code / logs

Need not to code, clear above ...
"
20619,ReduceLROnPlateau with native optimizer: 'TFOptimizer' object has no attribute 'lr',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
macOS 10.12.6

- **TensorFlow installed from (source or binary)**:
Binary (pip)

- **TensorFlow version (use command below)**:
1.9.0rc2

- **Python version**: 
Python 3.6.4 :: Anaconda custom (x86_64)

- **Bazel version (if compiling from source)**:
N/A

- **GCC/Compiler version (if compiling from source)**:
N/A

- **CUDA/cuDNN version**:
N/A

- **GPU model and memory**:
N/A

- **Exact command to reproduce**:
```
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.layers import Dense
from tensorflow.python.training.adam import AdamOptimizer
import numpy as np

model = Sequential()
model.add(Dense(8, input_shape=(2, )))
model.add(Dense(1, activation='softmax'))
model.compile(optimizer=AdamOptimizer(), loss='mse')

lr_schedule = tf.keras.callbacks.ReduceLROnPlateau()

x = np.random.uniform(0, 1, (100, 2))
y = np.random.uniform(0, 1, (100, 1))
model.fit(x=x, y=y, callbacks=[lr_schedule], validation_split=0.2)
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Using a native optimizer (AdamOptimizer) I can't get ReduceLROnPlateau to work, but it does work using an optimizer from tf.keras.optimizers. Only TF native optimizers are supported in Eager mode, so right now I just don't use ReduceLROnPlateau while in eager mode, but I thought this should be reported. Thank you.

### Source code / logs
```
  File ""/Users/ken/Documents/Projects/keras-try/src/lr.py"", line 16, in <module>
    model.fit(x=x, y=y, callbacks=[lr_schedule], validation_split=0.2)
  File ""/Users/ken/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1348, in fit
    validation_steps=validation_steps)
  File ""/Users/ken/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 277, in fit_loop
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/Users/ken/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 95, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/Users/ken/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 921, in on_epoch_end
    logs['lr'] = K.get_value(self.model.optimizer.lr)
AttributeError: 'TFOptimizer' object has no attribute 'lr'
```
"
20618,RAM,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20615,undefined reference to 'fegetround' in android,"Build command failed.
Error while executing process /Users/luojie/Library/Android/sdk/cmake/3.6.4111459/bin/cmake with arguments {--build /Users/luojie/WorkSpaceOfSaiDeSheng/WorkSpaceOfOpenCV/android/OpencvDemo/opencvlib/.externalNativeBuild/cmake/debug/armeabi-v7a --target native-lib}
[1/1] Linking CXX shared library ../../../../build/intermediates/cmake/debug/obj/armeabi-v7a/libnative-lib.so
FAILED: : && /Users/luojie/Library/Android/sdk/ndk-bundle/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang++  --target=armv7-none-linux-androideabi --gcc-toolchain=/Users/luojie/Library/Android/sdk/ndk-bundle/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 --sysroot=/Users/luojie/Library/Android/sdk/ndk-bundle/sysroot -fPIC -isystem /Users/luojie/Library/Android/sdk/ndk-bundle/sysroot/usr/include/arm-linux-androideabi -D__ANDROID_API__=16 -g -DANDROID -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -march=armv7-a -mfloat-abi=softfp -mfpu=vfpv3-d16 -mthumb -Wa,--noexecstack -Wformat -Werror=format-security  -frtti -fexceptions -std=c++11 -std=gnu++11 -fexceptions -frtti -O0 -fno-limit-debug-info  -Wl,--exclude-libs,libgcc.a -Wl,--exclude-libs,libatomic.a -nostdlib++ --sysroot /Users/luojie/Library/Android/sdk/ndk-bundle/platforms/android-16/arch-arm -Wl,--build-id -Wl,--warn-shared-textrel -Wl,--fatal-warnings -Wl,--fix-cortex-a8 -Wl,--no-undefined -Wl,-z,noexecstack -Qunused-arguments -Wl,-z,relro -Wl,-z,now -shared -Wl,-soname,libnative-lib.so -o ../../../../build/intermediates/cmake/debug/obj/armeabi-v7a/libnative-lib.so CMakeFiles/native-lib.dir/src/main/cpp/AIEdgeDectectionPublic/zf_log.c.o CMakeFiles/native-lib.dir/src/main/cpp/AIEdgeDectectionPublic/fm_ocr_scanner.cpp.o CMakeFiles/native-lib.dir/src/main/cpp/AIEdgeDectectionPublic/EdgeDetection.cpp.o CMakeFiles/native-lib.dir/src/main/cpp/AIEdgeDectectionPublic/AIEdgeDetection.cpp.o CMakeFiles/native-lib.dir/src/main/cpp/AIEdgeDectectionPublic/imgproc/scannerLite.cpp.o CMakeFiles/native-lib.dir/src/main/cpp/AIEdgeDectectionPublic/imgproc/ImgEffect.cpp.o CMakeFiles/native-lib.dir/src/main/cpp/native-lib.cpp.o  ../../../../src/main/jniLibs/armeabi-v7a/libtensorflow-core.a ../../../../src/main/jniLibs/armeabi-v7a/libprotobuf.a ../../../../src/main/jniLibs/armeabi-v7a/libnsync.a ../../../../src/main/jniLibs/armeabi-v7a/libopencv_imgcodecs.a ../../../../src/main/jniLibs/armeabi-v7a/liblibjasper.a ../../../../src/main/jniLibs/armeabi-v7a/liblibjpeg.a ../../../../src/main/jniLibs/armeabi-v7a/libIlmImf.a ../../../../src/main/jniLibs/armeabi-v7a/liblibtiff.a ../../../../src/main/jniLibs/armeabi-v7a/liblibpng.a ../../../../src/main/jniLibs/armeabi-v7a/libopencv_java3.so -llog -lz -latomic -lm ""/Users/luojie/Library/Android/sdk/ndk-bundle/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libgnustl_static.a"" && :
../../../../src/main/jniLibs/armeabi-v7a/libtensorflow-core.a(setround.o):setround.cc:function tensorflow::port::ScopedSetRound::ScopedSetRound(int): error: undefined reference to 'fegetround'
../../../../src/main/jniLibs/armeabi-v7a/libtensorflow-core.a(setround.o):setround.cc:function tensorflow::port::ScopedSetRound::ScopedSetRound(int): error: undefined reference to 'fesetround'
../../../../src/main/jniLibs/armeabi-v7a/libtensorflow-core.a(setround.o):setround.cc:function tensorflow::port::ScopedSetRound::~ScopedSetRound(): error: undefined reference to 'fesetround'
../../../../src/main/jniLibs/armeabi-v7a/libprotobuf.a(strutil.o):strutil.cc:function tf_protobuf3_private::protobuf::safe_strtof(char const*, float*): error: undefined reference to 'strtof'
clang++: error: linker command failed with exit code 1 (use -v to see invocation)
ninja: build stopped: subcommand failed.

"
20614,Cuda Configuration Error: Cannot find libdevice.10.bc under /usr/local/cuda-8.0,"I want to install TensorFlow through compiling from source. After I finished the configure, I run the command `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`. But some errors occurs. Here are the info:
```
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1166
		_create_local_cuda_repository(repository_ctx)
	File ""/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1001, in _create_local_cuda_repository
		_find_nvvm_libdevice_dir(repository_ctx, cuda_config)
	File ""/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl"", line 724, in _find_nvvm_libdevice_dir
		auto_configure_fail((""Cannot find libdevice.10.bc un...))
	File ""/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl"", line 210, in auto_configure_fail
		fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Cannot find libdevice.10.bc under /usr/local/cuda-8.0
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
	File ""/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1166
		_create_local_cuda_repository(repository_ctx)
	File ""/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1001, in _create_local_cuda_repository
		_find_nvvm_libdevice_dir(repository_ctx, cuda_config)
	File ""/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl"", line 724, in _find_nvvm_libdevice_dir
		auto_configure_fail((""Cannot find libdevice.10.bc un...))
	File ""/home/cxt/tensorflow/third_party/gpus/cuda_configure.bzl"", line 210, in auto_configure_fail
		fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Cannot find libdevice.10.bc under /usr/local/cuda-8.0
```
I tried the solution on this [link](https://github.com/tensorflow/tensorflow/issues/17801). But it didn't work. 
The following are the info about my PC.
### System information
- **Have I written custom code**: No
- **OS Platform and Distribution**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
- **Python version**:3.6 
- **Bazel version**:0.15.0
- **GCC/Compiler version**:5.5.0
- **CUDA/cuDNN version**:8.0/5.1.10
- **GPU model and memory**:N/A
- **Exact command to reproduce**:N/A

### Describe the problem
I want to install TensorFlow on my system. But I tried plenty of ways, none of them works. It took me a lot of time. Can anybody help me out or provide me a useful way?
Thanks a lot!
"
20613,[bug] May be a bug of gradients calculation.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.9
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

I write a test case to test operation ops::Square and it's gradient function SquareGrad, here is the code:

    TEST(MathTest, SingleComplex64Function) {
     //
     //data prepare
     Scope root = Scope::NewRootScope();
     DataType x_type = DataTypeToEnum<complex64>::v();
     TensorShape shape({1});

     Tensor x_data(x_type, shape);
     auto x_data_flat = x_data.flat<complex64>();
     x_data_flat(0) = {1,2};
     
     //
     //graph construct: y = x*x, x and y are complex64
     auto x = ops::Placeholder(root, x_type, Placeholder::Shape(shape));
     auto y = ops::Square(root, x);

     ClientSession session(root);
     std::vector<Tensor> outputs;

     //
     //calculate y
     Status s = session.Run({{x, x_data}} , {y}, &outputs);
     ASSERT_TRUE(s.ok());
     ASSERT_TRUE(outputs.size() == 1);
     ASSERT_TRUE(outputs[0].dtype() == DT_COMPLEX64);
     ASSERT_TRUE(outputs[0].flat<complex64>()(0) == complex64(-3,4));

     std::vector<Output> grads;
     s = AddSymbolicGradients(root, {y}, {x}, &grads);
     ASSERT_TRUE(s.ok());
     
     //
     //calculate gradients of y w.r.t x
     std::vector<Tensor> grad_outputs;
     s = session.Run({{x, x_data}}, grads, &grad_outputs);
     ASSERT_TRUE(s.ok());
     ASSERT_TRUE(grad_outputs.size() == 1);
     ASSERT_TRUE(grad_outputs[0].dtype() == DT_COMPLEX64);
     ASSERT_TRUE(grad_outputs[0].flat<complex64>()(0) == complex64(2,4));
    }

when i run it, i get this error:

    [ RUN      ] MathTest.SingleComplex64Function
    tensorflow/cc/gradients/math_grad_test.cc:84: Failure
    Value of: grad_outputs[0].flat<complex64>()(0) == complex64(2,4)
      Actual: false
      Expected: true
    [  FAILED  ] MathTest.SingleComplex64Function (39 ms)

I find the calculated value of grad_outputs[0] is (2, -4) , is this correct?


According to my knowledge, i have defined a function R -> R : y = x^2 (x and y are complex64),  the first derived function of it should be : dy/dx = 2x, and according to the definition of gradient: grad y(x) = [dy/dx], so if z=1+2i, y should be (1+2i)(1+2i)=1+4i+4i*i =1+4i-4=-3+4i,  and dy/dx = 2(1+2i)=2+4i, so theoretically grad y should be [(2,4)].
"
20610,c++ inference time is different with python,I met a very puzzled questionï¼ŒImodel inference time is 16ms by pythonï¼Œand then I use c++ to get inference time is 50ms.
20608,Allow restoring subgraph from checkpoint for weight sharing between different models,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Sierra
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.7.0-3-g024aecf414 1.7.0
- **Python version**: 3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: Read the summary below

### Describe the problem
There are many scenarios where we want to do parameter sharing between two different models (let's call `A` and `B`). If `A` is trained and checkpointed, we can either load all of the variables of `A` or selectively load variables from A's checkpoint by passing a list `L` containing all shared params. But the second way is not helpful if we don't know `L` beforehand. Can we have a way by which we can share variables just based on the common subset between two graphs.
PS - I am assuming that parameters of `A` and `B` follow same naming convention in scopes.

### Source code / logs
Here is what I was trying to do using metagraph but this also doesn't seem to work. Please note that I deliberately do `tf.reset_default_graph()` as objective is to share parameter with a checkpointed model. Sorry lots of redundant lines in the code.
```
import tensorflow as tf
mnist = tf.contrib.learn.datasets.load_dataset(""mnist"")

def model_1(features, labels):
    with tf.variable_scope(""input"", reuse=tf.AUTO_REUSE):
        out = tf.reshape(features, [-1, 28, 28, 1])
    with tf.variable_scope(""conv_1_5x5"", reuse=tf.AUTO_REUSE):
        out = tf.layers.conv2d(
           inputs=out,
           filters=32,
           kernel_size=[5, 5],
           padding=""same"",
           activation=tf.nn.relu)
    with tf.variable_scope(""conv_2_3x3"", reuse=tf.AUTO_REUSE):
        out = tf.layers.conv2d(
           inputs=out,
           filters=32,
           kernel_size=[3, 3],
           padding=""same"",
           activation=tf.nn.relu)
    with tf.variable_scope(""pool_3_2x2"", reuse=tf.AUTO_REUSE):
        out = tf.layers.max_pooling2d(inputs=out, pool_size=[2, 2], strides=2)
    with tf.variable_scope(""conv_4_3x3"", reuse=tf.AUTO_REUSE):
        out = tf.layers.conv2d(
           inputs=out,
           filters=64,
           kernel_size=[3, 3],
           padding=""same"",
           activation=tf.nn.relu)
    with tf.variable_scope(""dense"", reuse=tf.AUTO_REUSE):
        flattened = tf.reshape(out, [-1, 14 * 14 * 64])
        logits = tf.layers.dense(flattened, units=10, activation=tf.nn.relu)
    softmax = tf.nn.softmax(logits, name=""softmax"")

    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_or_create_global_step())
    return loss, train_op

def model_2(features, labels, old_graph):
    with old_graph.as_default():
        with tf.variable_scope(""input"", reuse=tf.AUTO_REUSE):
            out = tf.reshape(features, [-1, 28, 28, 1])
        with tf.variable_scope(""conv_1_5x5"", reuse=tf.AUTO_REUSE):
            out = tf.layers.conv2d(
               inputs=out,
               filters=32,
               kernel_size=[5, 5],
               padding=""same"",
               activation=tf.nn.relu)
        with tf.variable_scope(""conv_2_3x3"", reuse=tf.AUTO_REUSE):
            out = tf.layers.conv2d(
               inputs=out,
               filters=32,
               kernel_size=[3, 3],
               padding=""same"",
               activation=tf.nn.relu)
        with tf.variable_scope(""pool_3_2x2"", reuse=tf.AUTO_REUSE):
            out = tf.layers.max_pooling2d(inputs=out, pool_size=[2, 2], strides=2)
        # This is the place where model 1 and 2 are actually different
        with tf.variable_scope(""conv_4_5x5"", reuse=tf.AUTO_REUSE):
            out = tf.layers.conv2d(
               inputs=out,
               filters=64,
               kernel_size=[5, 5],
               padding=""same"",
               activation=tf.nn.relu)
        with tf.variable_scope(""dense"", reuse=tf.AUTO_REUSE):
            flattened = tf.reshape(out, [-1, 14 * 14 * 64])
            logits = tf.layers.dense(flattened, units=10, activation=tf.nn.relu)
        softmax = tf.nn.softmax(logits, name=""softmax"")

    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_or_create_global_step())
    return loss, train_op

features = tf.placeholder(shape=[None, 784], dtype=tf.float32, name=""features"")
labels = tf.placeholder(shape=[None], dtype=tf.int32, name=""labels"")

steps = 0
loss_op, train_op = model_1(features, labels)

with tf.train.MonitoredTrainingSession(checkpoint_dir=""./model_1"") as sess:
    while steps <= 20:
        features_, labels_ = mnist.train.next_batch(100)
        loss, _ = sess.run([loss_op, train_op], feed_dict={
            ""features:0"": features_, ""labels:0"": labels_})
        if steps % 5 == 0:
            print(""loss at step {} = {}"".format(steps, loss))
        steps += 1

tf.reset_default_graph()
tf.train.import_meta_graph(""./model_1/model.ckpt-""+str(steps)+"".meta"")
old_graph = tf.get_default_graph()

steps = 0
loss_op, train_op = model_2(features, labels, old_graph)

with tf.train.MonitoredTrainingSession(checkpoint_dir=""./model_2"") as sess:
    while steps <= 20:
        features_, labels_ = mnist.train.next_batch(100)
        loss, _ = sess.run([loss_op, train_op], feed_dict={
            ""features:0"": features_, ""labels:0"": labels_})
        if steps % 5 == 0:
            print(""loss at step {} = {}"".format(steps, loss))
        steps += 1

"
20607,Performance drop for CPU graph calculation after upgrading from 1.3 to newer versions,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7
- **TensorFlow installed from (source or binary)**: binary (pip)
- **TensorFlow version (use command below)**: 1.4 and newer for CPU
- **Python version**: 2.7.5 3.5.4
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: python test1.py

### Describe the problem

Initially I found training using CPU in TensorFlow 1.8 is slow and it appeared only one thread was used. I can confirm multi-threading for individual operations works well (like multiplying two large matrices). But it seems only one operation can be done at the same time. Specifying inter_op_parallelism_threads doesn't help. For version 1.8 and 1.9rc2 and the following test code I wrote, the CPU usage was almost consistently 100% (1 core) according to ""top"". For version 1.3 and 1.0, it was obvious that the following code used multiple cores and ran significantly faster. I reproduced most results with both Python versions. Here are my test results:
- Version time printed
- 1.0, 1.3: 20s
- 1.4, 1.5: 50s
- 1.6: 60s
- 1.7, 1.8: 80s
- 1.9 rc2: 130s

The above results were based on a 64GB, 20-core (2x E5 without hyperthreading) machine. I also tried on an AWS t2.2xlarge instance (32GB, 8-core) with Ubuntu 16.04, Python 3.5.2, TensorFlow 1.3 and 1.9 and got similar results.

### Source code / logs
```
import tensorflow as tf
import time
A=[None]*100000
B=[None]*100000
for i in range(0,2):
	A[i]=tf.Variable(tf.random_normal([100,100]))
	B[i]=tf.Variable(tf.random_normal([100,100]))
for i in range(2,100000):
	A[i]=tf.matmul(A[i-1],A[i-2])
	B[i]=tf.matmul(B[i-1],B[i-2])
c=tf.matmul(A[-1],B[-1])
print('graph created')
#the config doesn't help
config=tf.ConfigProto()
config.intra_op_parallelism_threads = 4
config.inter_op_parallelism_threads = 10
t=time.time()
s=tf.Session(config=config)
s.run(tf.global_variables_initializer())
s.run(c)
print(tf.__version__)
print(time.time()-t)
```

Appended:
I just tried to collect the timeline in 1.3, 1.8 and 1.9rc. They all look similar and reported 5-10 seconds with good parallelism. Maybe the issue is caused by overhead from other parts like scheduling?
Timeline:
<img width=""943"" alt=""timeline"" src=""https://user-images.githubusercontent.com/10559772/42405897-56f3c342-81d0-11e8-9a8b-bf4a08a6a6c7.png"">
Timeline enlarged:
<img width=""886"" alt=""timeline_enlarged"" src=""https://user-images.githubusercontent.com/10559772/42405908-6ec434c0-81d0-11e8-871b-a25a559dc745.PNG"">
"
20606,[bug] events.out.tfevents files do not get closed.,"Running a python script that trains and tests MANY models (tf.Estimator) failes with
```
tf.estimator Error: ResourceExhausted: too many open files (TF keeps events.out.tfevents files open)
```

Stackoverflow question & proposed solution (by another person): https://stackoverflow.com/questions/50956551/tf-estimator-error-resourceexhausted-too-many-open-files-tf-keeps-events-out

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 1604
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 
== cat /etc/issue ===============================================
Linux gpubox1 4.4.0-128-generic #154-Ubuntu SMP Fri May 25 14:15:18 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.4 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux gpubox1 4.4.0-128-generic #154-Ubuntu SMP Fri May 25 14:15:18 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                              1.14.3   
numpydoc                           0.8.0    
protobuf                           3.6.0    
tensorflow                         1.9.0rc0 

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.9.0-rc0
tf.GIT_VERSION = b'v1.8.0-3463-g39ea5a7'
tf.COMPILER_VERSION = b'v1.8.0-3463-g39ea5a7'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Jul  6 21:20:55 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |
| 42%   61C    P2    63W / 250W |  10664MiB / 11175MiB |     24%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |
| 29%   44C    P2    56W / 250W |  10631MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1092      G   /usr/lib/xorg/Xorg                            89MiB |
|    0     19389      C   python                                     10563MiB |
|    1     19389      C   python                                     10619MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
- **Bazel ver:** NA
- **CUDA/cuDNN**: 9.0 / 7.1
- **GPU**: NVIDIA 1080 Ti

- **Exact command to reproduce**: N/A; Running  `tf.Estimator.train ` for some 1000 different estimators reproduces the problem.

### Describe the problem
Each tensorflow run does not close events.out.tfevent. Instead, they remain open, and eventually stops the process because there are too many open file handles.

### Source code / logs
I'll do that on request

"
20605,"Quantize: lacking min/max data, when i transform the quantize graph to tflite","tensorflow version: 1.8
model: my model structure is very simliar to mobilenet-v1
desc: I trained my model use the tf.contrib.quantize.create_training_graph function, and call the tf.contrib.quantize.create_eval_graph() when i generate the eval graph.
And then I use 
bazel-bin/tensorflow/contrib/lite/toco/toco \
--input_file=/home/admin_pc/model_test/output_quant.pb \
--output_file=/home/admin_pc/model_test/mobilenet_qg.tflite \
--input_fromat=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--inference_type=QUANTIZED_UINT8 \
--input_array=image \
--output_array=Openpose/MConv_Stage3_L2_5_pointwise/BatchNorm/FusedBatchNorm \
--input_shape=1,224,224,3 \
--change_concat_input_ranges=false \
--std_value=128 --mean_value=128
to generate the tflite

but some errores occurs:

2018-07-07 12:34:56.116401: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 58 operators, 169 arrays (1 quantized)
2018-07-07 12:34:56.117540: F tensorflow/contrib/lite/toco/tooling_util.cc:1581] Array MobilenetV1/Conv2d_1_depthwise/depthwise, which is an input to the Conv operator producing the output array MobilenetV1/Conv2d_1_pointwise/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.
Aborted (core dumped)
"
20602,Different accuracy for quantized model when inferring from one image vs batch,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: gtx 1080ti
- **Exact command to reproduce**:

```
!python quantize_graph.py \
--input=logs_vgg/frozen_model.pb \
--output_node_names=""vgg_16/fc8/squeezed"" --print_nodes --output=logs_vgg/quantized_graph.pb \
--mode=eightbit --logtostderr
```
```
tf.reset_default_graph()
tf.set_random_seed(0)
g2 = load_graph('logs_vgg/quantized_model.pb')
ctr = 0
with g2.as_default():
    with tf.Session(graph=g2) as sess:
        restored_x = g2.get_tensor_by_name('import/Inputs:0')
        restored_logits = g2.get_tensor_by_name('import/vgg_16/fc8/squeezed:0')
        times = []
        for i in range(num_images):
            t0 = time.time()
            a = sess.run(restored_logits, feed_dict={restored_x:np.expand_dims(images[i, :, :, :], axis=0)})
            t1 = time.time()
            total_time = t1 - t0
            times.append(total_time)
            pred = np.argsort(a[0])
            pred = pred[-5:]
            if y[i] in set(pred):
                ctr += 1   
print(""TF Prediction acc:"" + str(ctr/float(num_images)))
print(""Inference time(ms): "" + str((sum(times[1:])/float(num_images - 1)) * 1000))
```
TF Prediction acc:0.79
Inference time(ms): 56.0006372856

```
tf.reset_default_graph()
tf.set_random_seed(0)
g = load_graph('logs_vgg/quantized_model.pb')
with g.as_default():
    with tf.Session(graph=g) as sess:
        restored_x = g.get_tensor_by_name('import/Inputs:0')
        restored_logits = g.get_tensor_by_name('import/vgg_16/fc8/squeezed:0')
        t0 = time.time()
        a = sess.run(restored_logits, feed_dict={restored_x:images})
        t1 = time.time()
        total_time = t1 - t0
ctr = 0
for i in range(num_images):
    pred = np.argsort(a[i])
    pred = pred[-5:]
    if y[i] in set(pred):
        ctr += 1   
print(""TF Prediction acc:"" + str(ctr/float(num_images)))
print(""Inference time(ms): "" + str(total_time/float(num_images) * 1000))
```


 TF Prediction acc:0.76
Inference time(ms): 406.530380249



You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Different accuracy for quantized model when inferring from one image vs batch

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20600,Tensorflow 1.8 graph destructor hanging indefinitely,"I'm sorry to dump this on y'all on a weekend, but we're having strange problems with our CI when trying to upgrade to Tensorflow 1.8.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian Stretch
- **TensorFlow installed from (source or binary)**: Binary (CPU wheel)
- **TensorFlow version (use command below)**: `v1.8.0-0-g93bc2e2072`
- **Python version**: `3.6.6`
- **Bazel version**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory** n/a

**Exact command to reproduce**: ... it's complicated

### Describe the problem
When we run our test suite, it will intermittently hang (we've let it run overnight without any progress). We've traced this problem to TensorFlow 1.8 -- when we set `TF_C_API_GRAPH_CONSTRUCTION=0`, our test suite passes without problems and when we use Tensorflow 1.6 and 1.7, we don't seem to have any problems.

Unfortunately, I have not been able to create a minified, reproducible test case (it's also frustrating because I can't replicate this locally, despite using the same Docker image that our CI build is using). I have, however, been able to use [Pyflame](https://github.com/uber/pyflame) and gdb to grab stack traces (see https://gist.github.com/alanhdu/9ce45f9061a8e48ba2b0728489cb4ee7).

I think the relevant lines are:
```
/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/c_api_util.py:__del__:50
/usr/local/lib/python3.6/inspect.py:getmodule:732
/usr/local/lib/python3.6/inspect.py:findsource:780
/usr/local/lib/python3.6/inspect.py:getframeinfo:1445
/usr/local/lib/python3.6/inspect.py:getouterframes:1471
/usr/local/lib/python3.6/inspect.py:stack:1494
/usr/local/lib/python3.6/site-packages/coverage/debug.py:short_stack:143
/usr/local/lib/python3.6/site-packages/coverage/collector.py:__init__:107
/usr/local/lib/python3.6/site-packages/coverage/control.py:_init:266
/usr/local/lib/python3.6/site-packages/coverage/control.py:load:675
/usr/local/lib/python3.6/site-packages/pytest_cov/embed.py:init:67
/usr/local/lib/python3.6/site-packages/pytest_cov/embed.py:multiprocessing_start:23
/usr/local/lib/python3.6/multiprocessing/util.py:_run_after_forkers:132
/usr/local/lib/python3.6/multiprocessing/process.py:_bootstrap:251
/usr/local/lib/python3.6/multiprocessing/popen_fork.py:_launch:73
/usr/local/lib/python3.6/multiprocessing/popen_fork.py:__init__:19
/usr/local/lib/python3.6/multiprocessing/context.py:_Popen:277
/usr/local/lib/python3.6/multiprocessing/process.py:start:105
/usr/local/lib/python3.6/multiprocessing/pool.py:_repopulate_pool:239
/usr/local/lib/python3.6/multiprocessing/pool.py:__init__:174
/usr/local/lib/python3.6/multiprocessing/context.py:Pool:119
```

which seems to be hanging at

```
#0  __pthread_cond_destroy (cond=0x56312aa153d0) at pthread_cond_destroy.c:76
#1  0x00007f6eef4deebb in tensorflow::thread::ThreadPool::Impl::~Impl() () from /usr/local/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#2  0x00007f6eef4df89a in tensorflow::thread::ThreadPool::~ThreadPool() () from /usr/local/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007f6eef87d665 in tensorflow::SingleThreadedCpuDevice::~SingleThreadedCpuDevice() ()
   from /usr/local/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#4  0x00007f6eef87d85a in tensorflow::GraphRunner::~GraphRunner() () from /usr/local/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
#5  0x00007f6ef13e3aef in TF_DeleteGraph () from /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007f6ef1038784 in _wrap_TF_DeleteGraph () from /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007f6f4a6c78e9 in _PyCFunction_FastCallDict (func_obj=0x7f6ef4de69d8, args=0x7f6e84006730, nargs=<optimized out>, kwargs=kwargs@entry=0x0)
at Objects/methodobject.c:234
```

Do y'all happen to know what could have caused this error? Even tips on how to debug this would be super helpful.
"
20598,Cmake build with GPU fails on Windows 7,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 64-Bit
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: commit id: b2fe2a874bade4782aaca5c44bf29e7ff6c39200
- **Python version**: 3.55
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: 3.6.3
- **CUDA/cuDNN version**: CUDA 9.0 cuDNN 7.1
- **GPU model and memory**: NVIDIA NVS 510 2GB
- **Exact command to reproduce**:
First:
cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.10/swig.exe -DPYTHON_EXECUTABLE=C:/Users/{username}/Anaconda3/envs/py35/python.exe -DPYTHON_LIBRARIES=C:/Users/{username}/Anaconda3/envs/py35/libs/python35.lib -DPYTHON_INCLUDE_DIRS=C:/Users/{username}/Anaconda3/envs/py35/include -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0"" -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX

Then
MSBuild /p:Configuration=Release /filelogger tf_python_build_pip_package.vcxproj




### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

This is what is at the end of the log:
""C:\Users\{username}\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj"" (default target) (1) ->
""C:\Users\{username}\tensorflow\tensorflow\contrib\cmake\build\estimator_python_api.vcxproj"" (default target) (2) ->
(CustomBuild target) -> 
  C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 1. [C:\Users\{username}\tensorflow\tensorflow\contrib\cmake\build\estimator_python_api.vcxproj]


""C:\Users\{username}\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj"" (default target) (1) ->
""C:\Users\{username}\tensorflow\tensorflow\contrib\cmake\build\tf_python_api.vcxproj"" (default target) (264) ->
  C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 1. [C:\Users\{username}\tensorflow\tensorflow\contrib\cmake\build\tf_python_api.vcxproj]

    27022 Warning(s)
    2 Error(s)

I searched in the log for the keyword ""Error:"" and came across this passage:

  Traceback (most recent call last):
    File ""C:/Users/{username}/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/tools/api/generator/create_python_api.py"", line 27, in <module>
      from tensorflow.python.util import tf_decorator
    File ""C:\Users\{username}\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\python\__init__.py"", line 52, in <module>
      from tensorflow.core.framework.graph_pb2 import *
    File ""C:\Users\{username}\tensorflow\tensorflow\contrib\cmake\build\tf_python\tensorflow\core\framework\graph_pb2.py"", line 6, in <module>
      from google.protobuf import descriptor as _descriptor
  ImportError: No module named 'google'

Time Elapsed 18:43:53.58
### Source code / logs
[msbuild - Copy.zip](https://github.com/tensorflow/tensorflow/files/2171451/msbuild.-.Copy.zip)



Thank you in advance :)"
20597,Proposal: Add GPU Details to TensorFlow data store,"Weâ€™re looking to extend the TensorFlow and TensorBoard user interface to display interesting GPU details for each node in a neural network. The GPU Details that we plan to display focus on the usage of tensor cores. Tensor cores enable deep learning applications to run significantly faster.

The UI TensorBoard proposal is here: https://github.com/tensorflow/tensorboard/issues/1271

In order to display the interesting GPU Details information in TensorBoard, we need to add the GPU Details to a data store.  One idea is for NodeDef to encapsulate a new gpu_details protobuf.  See proposed protobuf definitions below.

 Update existing NodeDef
```
message NodeDef {
  string name = 1;
  string op = 2;
  repeated string input = 3;
  string device = 4;
  map<string, AttrValue> attr = 5;

  GpuDetails gpu_details = 6;		// new, optional
};
```

Create new GpuDetails protobuf
```
message GpuDetails {
  repeated GpuKernelAttrs gpuKernelAttrs = 1;
};
```

Create new GpuKernelAttrs protobuf
```
message GpuKernelAttrs {
  string kernel_name = 1;
  int32 calls = 2;
  float total_duration = 3;
  float average_duration = 4;
  float minimum_duration = 5;
  float maximum_duration = 6; 
}
```

We're aware that this is a change to a fundamental data structure.  Another potential solution we considered is adding a new event type in TF that captures the GPU Details information.

We're open to other suggestions you have - any guidance here would be appreciated."
20594,grpc_master_service method names backward-incompatible,"After grpc updated(#20513 ), the master service method names are:
```
//generated automatically

static const char* MasterService_method_names[] = {
  ""/tensorflow.grpc.MasterService/CreateSession"",
  ""/tensorflow.grpc.MasterService/ExtendSession"",
  ""/tensorflow.grpc.MasterService/PartialRunSetup"",
  ""/tensorflow.grpc.MasterService/RunStep"",
  ""/tensorflow.grpc.MasterService/CloseSession"",
  ""/tensorflow.grpc.MasterService/ListDevices"",
  ""/tensorflow.grpc.MasterService/Reset"",
  ""/tensorflow.grpc.MasterService/MakeCallable"",
  ""/tensorflow.grpc.MasterService/RunCallable"",
  ""/tensorflow.grpc.MasterService/ReleaseCallable"",
};
```
but the old version is 
[v1.8 grpc_master_service_impl.cc](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/core/distributed_runtime/rpc/grpc_master_service_impl.cc)
```
static const char* grpcMasterService_method_names[] = {
    ""/tensorflow.MasterService/CreateSession"",
    ""/tensorflow.MasterService/ExtendSession"",
    ""/tensorflow.MasterService/PartialRunSetup"",
    ""/tensorflow.MasterService/RunStep"",
    ""/tensorflow.MasterService/CloseSession"",
    ""/tensorflow.MasterService/ListDevices"",
    ""/tensorflow.MasterService/Reset"",
    ""/tensorflow.MasterService/MakeCallable"",
    ""/tensorflow.MasterService/RunCallable"",
    ""/tensorflow.MasterService/ReleaseCallable"",
};
```

So older version (e.g. 1.8) clients cannot talk to new version server and vice versa,  as I have tested. Should this be fixed?

"
20592,TensorFlow failing on Bazel CI with Ubuntu 14.04,"https://buildkite.com/bazel/tensorflow/builds/863

Full log: https://buildkite.com/bazel/bazel-with-downstream-projects-bazel/builds/330#f4d9112b-3525-45b1-a2c7-09bc95905c6d

Error message:
```
external/com_github_googlecloudplatform_google_cloud_cpp/google/cloud/bigtable/internal/rowreaderiterator.h:57:44: error: ambiguous overload for 'operator*' (operand type is 'std::remove_reference<const google::cloud::v0::internal::optional<google::cloud::bigtable::v0::Row>&>::type {aka const google::cloud::v0::internal::optional<google::cloud::bigtable::v0::Row>}')
```

Because it's only failing on 14.04, I suspect it's caused by compiling with an old gcc version.

culprit: https://github.com/tensorflow/tensorflow/commit/2e764644d6

Do we care about building TF on Ubuntu 14.04? If so we might need to fix code from https://github.com/GoogleCloudPlatform/google-cloud-cpp
"
20591,Possibly outdated docs on iOS selective registration,"The documentation at [`tensorflow/examples/ios/README.md #reducing-the-binary-size`](https://github.com/tensorflow/tensorflow/blob/c941c087a9dfd5b27eff00ead928c9ee208e9a35/tensorflow/examples/ios/README.md#reducing-the-binary-size) features the following (two-year-old) snippet:

> After that, you can manually look at modifying the list of kernels included in `tensorflow/contrib/makefile/tf_op_files.txt` to reduce the number of implementations to the ones you're actually using in your own model. We're hoping to automate this step in the future, but for now manually removing them is the best approach.

Today, selective registration uses `tensorflow/core/framework/ops_to_register.h` and commit
c4ef927b5eaf144dbf1e0419c0d1d3fd968177bd introduced the `OPTIMIZE_FOR_GRAPH` option in `tensorflow/contrib/makefile/build_all_ios.sh` which automates its creation.

I think the documentation (and possibly tooling) for iOS selective registration need to be updated."
20590,Profiler: Check Failed tf_start,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Archlinux
- **TensorFlow installed from (source or binary)**: pip tf-nightly
- **TensorFlow version (use command below)**: v1.8.0-3463-g39ea5a7044 1.10.0-dev20180620
- **Python version**: 3.6.5

### Describe the problem
I am trying do some profiling as described here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md

Session setup code
```python
run_meta = tf.RunMetadata()
builder = tf.profiler.ProfileOptionBuilder
    opts = builder(builder.time_and_memory()).order_by('micros').build()
    opts2 = tf.profiler.ProfileOptionBuilder.trainable_variables_parameter()
    with tf.contrib.tfprof.ProfileContext('.tmp/trace',
                                      trace_steps=range(0, 2),
                                      dump_steps=[2]) as pctx:
      pctx.add_auto_profiling('op', opts, [0, 1])
      pctx.add_auto_profiling('scope', opts2, [0, 1])
      self._session = tf.Session(
          target='', graph=self._graph,
          config=session_util.config_proto(
              session_settings['config_proto'],
              session_settings['gpu_options']))
    self.pctx = pctx
```

Then at a different point in my code I do
```python
loss = self._session.run(fetches, ..., run_meta)
```

This works for the first step, but at the second step it throws `F tensorflow/core/profiler/internal/print_model_analysis.cc:125] Check failed: tf_stat`"
20589,Can't restore saved model in windows 10,"I have trained a model in ubuntu (16.04) machine and when I tried that model in different ubuntu (16.04), model is able to predict. But the same thing is not happening in windows(10).

Configurations in both ubuntu and windows:
1. python - 3.6.5
2. Tensorflow - 1.8.0

We have used Estimator api, I am seeing error logs in estimator.py only.
Error Log:
Caused by op 'save/RestoreV2', defined at:
  File ""test_model.py"", line 70, in <module>
    for i in predictions:
  File ""C:\Python36\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 507, in predict
    ,hooks=all_hooks) as mon_sess:
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 816, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 539, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1002, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1007, in _create_session
    return self._sess_creator.create_session()
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 696, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 458, in create_session
    self._scaffold.finalize()
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 212, in finalize
    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 910, in _get_saver_or_default
    saver = Saver(sharded=True, allow_empty=True)
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 1338, in __init__
    self.build()
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 1384, in _build
    build_save=build_save, build_restore=build_restore)
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 829, in _build_internal
    restore_sequentially, reshape)
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 525, in _AddShardedRestoreOps
    name=""restore_shard""))
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 472, in _AddRestoreOps
    restore_sequentially)
  File ""C:\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 886, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""C:\Python36\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 1546, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""C:\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""C:\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): Read fewer bytes than requested
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Please help me."
20588,simple_estimator_example.py does not working,"Hello. 
Tried to figure out with your
and try https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/simple_estimator_example.py
but it doesn't working with that errors:

/usr/bin/python3.5 /___SOME_PATH____/simple_estimator_example.py
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp0qhilmok
2018-07-06 17:41:20.153970: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-07-06 17:41:20.269549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-07-06 17:41:20.269961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.83GiB
2018-07-06 17:41:20.269973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2018-07-06 17:41:20.475627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-06 17:41:20.475650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2018-07-06 17:41:20.475655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2018-07-06 17:41:20.475841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:0 with 7559 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-07-06 17:41:20.564665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2018-07-06 17:41:20.564699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-06 17:41:20.564705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2018-07-06 17:41:20.564709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2018-07-06 17:41:20.564810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7559 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""/home/bocharick/HDD/UbuntuFiles/PycharmProjects/LanguageDetector/bin/estimator_tst.py"", line 89, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/home/bocharick/HDD/UbuntuFiles/PycharmProjects/LanguageDetector/bin/estimator_tst.py"", line 73, in main
    estimator.train(input_fn=input_fn, steps=10)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1117, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1160, in _train_model_distributed
    self.config)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py"", line 794, in call_for_each_tower
    return self._call_for_each_tower(fn, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 269, in _call_for_each_tower
    coord.join(threads)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/lib/python3/dist-packages/six.py"", line 686, in reraise
    raise value
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 263, in _call_for_each_tower
    self, *merge_args, **merge_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py"", line 652, in _distributed_apply
    reduced_grads = distribution.batch_reduce(""sum"", grads_and_vars)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py"", line 840, in batch_reduce
    return self._batch_reduce(method_string, value_destination_pairs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 310, in _batch_reduce
    value_destination_pairs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py"", line 171, in batch_reduce
    raise ValueError(""`value_destination_pairs` must be a list or a tuple of ""
ValueError: `value_destination_pairs` must be a list or a tuple of tuples of PerDevice objects and destinations

Process finished with exit code 1

I tried two versions of tensorflow:
1) 1.10.nigthly
2) 1.9.rc2
Python 3.5

Same problems both TF versions.
Thank you. Help me, plz"
20587,log_prob() for tf.distributions.Dirichlet returns values > 0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution**: Debian Stretch, 64 bits
- **TensorFlow installed from**: virtualenv / pip
- **TensorFlow version**: 1.9.0rc2
- **Python version**: 3.5
- **Bazel version**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: Geforce GTX 1080 Ti, 11172 MiB
- **Exact command to reproduce**: See the description and source code below.

### Describe the problem
The method log_prob() for tf.distributions.Dirichlet sometimes returns values greater than 0, which is not mathematically possible.

### Source code / logs
Piece of code that fails:

```
dist = tf.distributions.Dirichlet(
    tf.exp(log_alphas),
    validate_args=True,
    allow_nan_stats=False,
)

objective = tf.reduce_sum(dist.log_prob(target))
```

Manual solution that works properly, using the mathematical definition for the Dirichlet:

```
def dirichlet_log_prob(log_alphas, x):
    return tf.reduce_sum(
        tf.lgamma(tf.reduce_logsumexp(log_alphas, axis=1)) +
        tf.reduce_sum(tf.multiply(tf.exp(log_alphas) - 1, tf.log(x)) - tf.lgamma(log_alphas), axis=1)
    )

objective = dirichlet_log_prob(log_alphas, target)
```

Data used in both examples:

```
log_alphas = [
    [-1.6231717, -2.112546 , -2.1379914, -2.0885904, -1.9052815, -1.759051 ],
    [-1.4579483, -1.8557736, -1.9981235, -1.8574138, -1.5318792, -1.3888079],
    [-1.4608178, -1.8787189, -1.9842598, -1.8636204, -1.6005809, -1.4765729],
    [-1.4781257, -1.8943347, -2.0051253, -1.880945 , -1.613416 , -1.4802316],
    [-1.447619 , -1.9153082, -1.9177424, -1.8622615, -1.7686105, -1.712473 ],
    [-1.490995 , -2.0010724, -2.0137913, -1.9128757, -1.8797761, -1.8842031],
    [-1.5973192, -2.0856535, -2.1033392, -2.0476089, -1.9143845, -1.8061923],
    [-1.5467792, -2.0158603, -2.0372252, -1.9859502, -1.8327508, -1.7130013]
]
target = [
    [2.7760255e-01, 3.2513769e-06, 3.6051267e-01, 3.2513769e-06, 8.0276497e-02, 2.8160176e-01],
    [6.5851367e-01, 3.4333350e-06, 1.7245641e-01, 3.4333350e-06, 2.1149343e-02, 1.4787373e-01],
    [3.1866562e-02, 2.0533266e-02, 3.3333222e-06, 2.0566598e-02, 6.1813128e-01, 3.0889896e-01],
    [3.4036295e-06, 1.3675784e-01, 3.4036295e-06, 8.6322856e-01, 3.4036295e-06, 3.4036295e-06],
    [1.6921267e-01, 2.1246435e-02, 7.6704454e-01, 2.1246435e-02, 2.1246435e-02, 3.4490965e-06],
    [3.4373238e-06, 4.7641307e-02, 3.4373238e-06, 9.5234495e-01, 3.4373238e-06, 3.4373238e-06],
    [1.3007499e-01, 4.5678858e-02, 6.3374266e-02, 7.3981225e-01, 3.4293435e-06, 2.1056170e-02],
    [5.3522962e-01, 3.5806104e-06, 1.7695376e-01, 3.5806104e-06, 4.4113118e-02, 2.4369633e-01]
]
```"
20586,REGISTER_KERNEL_BUILDER fails type constraint check for valid integral types,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18.2 Sonya (based on Ubuntu 16.04)
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: latest master (last commit https://github.com/tensorflow/tensorflow/commit/b2fe2a874bade4782aaca5c44bf29e7ff6c39200)
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**: 9.2/7.1
- **GPU model and memory**: GX1080Ti 11GB
- **Exact command to reproduce**: N/A

### Describe the problem
When writing a custom op and registering a kernel builder using `REGISTER_KERNEL_BUILDER`, specifying a type constraint of int64_t will fail on some machines. This also fails for unsigned types too (as you would expect).

The problem occurs due to lines 381-403 in `tensorflow/core/framework/types.h`
```
MATCH_TYPE_AND_ENUM(float, DT_FLOAT);
MATCH_TYPE_AND_ENUM(double, DT_DOUBLE);
MATCH_TYPE_AND_ENUM(int32, DT_INT32);
MATCH_TYPE_AND_ENUM(uint32, DT_UINT32);
MATCH_TYPE_AND_ENUM(uint16, DT_UINT16);
MATCH_TYPE_AND_ENUM(uint8, DT_UINT8);
MATCH_TYPE_AND_ENUM(int16, DT_INT16);
MATCH_TYPE_AND_ENUM(int8, DT_INT8);
MATCH_TYPE_AND_ENUM(string, DT_STRING);
MATCH_TYPE_AND_ENUM(complex64, DT_COMPLEX64);
MATCH_TYPE_AND_ENUM(complex128, DT_COMPLEX128);
MATCH_TYPE_AND_ENUM(int64, DT_INT64);
MATCH_TYPE_AND_ENUM(uint64, DT_UINT64);
MATCH_TYPE_AND_ENUM(bool, DT_BOOL);
MATCH_TYPE_AND_ENUM(qint8, DT_QINT8);
MATCH_TYPE_AND_ENUM(quint8, DT_QUINT8);
MATCH_TYPE_AND_ENUM(qint16, DT_QINT16);
MATCH_TYPE_AND_ENUM(quint16, DT_QUINT16);
MATCH_TYPE_AND_ENUM(qint32, DT_QINT32);
MATCH_TYPE_AND_ENUM(bfloat16, DT_BFLOAT16);
MATCH_TYPE_AND_ENUM(Eigen::half, DT_HALF);
MATCH_TYPE_AND_ENUM(ResourceHandle, DT_RESOURCE);
MATCH_TYPE_AND_ENUM(Variant, DT_VARIANT);
```
In this case, `int64` is being `typedef`'d to `long long` (`tensorflow/core/platform/default/integral_types.h`), however on some systems `int64_t` is `typedef`'d as `__int64` (or potentially some other variation that results in a 64-bit signed type). Because of this `int64_t != to int64` and the type constraint check fails.

On my system in particular, this code
```
#include <cstdint>
#include <iostream>

int main(void) {
    std::cout << ""int64_t...............: ""
              << ""("" << typeid(int64_t).name() << "") "" << sizeof(int64_t) << std::endl;
    std::cout << ""long..................: ""
              << ""("" << typeid(long).name() << "") "" << sizeof(long) << std::endl;
    std::cout << ""long int..............: ""
              << ""("" << typeid(long int).name() << "") "" << sizeof(long int) << std::endl;
    std::cout << ""long long int.........: ""
              << ""("" << typeid(long long int).name() << "") "" << sizeof(long long int) << std::endl;
    std::cout << ""long long.............: ""
              << ""("" << typeid(long long).name() << "") "" << sizeof(long long) << std::endl;
    std::cout << ""uint64_t..............: ""
              << ""("" << typeid(uint64_t).name() << "") "" << sizeof(int64_t) << std::endl;
    std::cout << ""unsigned long.........: ""
              << ""("" << typeid(unsigned long).name() << "") "" << sizeof(unsigned long) << std::endl;
    std::cout << ""unsigned long int.....: ""
              << ""("" << typeid(unsigned long int).name() << "") "" << sizeof(unsigned long int) << std::endl;
    std::cout << ""unsigned long long int: ""
              << ""("" << typeid(unsigned long long int).name() << "") "" << sizeof(unsigned long long int) << std::endl;
    std::cout << ""unsigned long long....: ""
              << ""("" << typeid(unsigned long long).name() << "") "" << sizeof(unsigned long long) << std::endl;
    std::cout << ""long long == int64_t? "" << (std::is_same<long long, int64_t>::value ? ""yes"" : ""no"") << std::endl;
    std::cout << ""(sizeof(long long) == 8) && std::is_signed<long long>::value? ""
              << ((sizeof(long long) == 8) && std::is_signed<long long>::value ? ""yes"" : ""no"") << std::endl;
    std::cout << ""(sizeof(int64_t) == 8) && std::is_signed<int64_t>::value? ""
              << ((sizeof(int64_t) == 8) && std::is_signed<int64_t>::value ? ""yes"" : ""no"") << std::endl;
    std::cout << ""unsigned long long == uint64_t? ""
              << (std::is_same<unsigned long long, uint64_t>::value ? ""yes"" : ""no"") << std::endl;
    std::cout << ""(sizeof(unsigned long long) == 8) && !std::is_signed<unsigned long long>::value? ""
              << ((sizeof(unsigned long long) == 8) && !std::is_signed<unsigned long long>::value ? ""yes"" : ""no"")
              << std::endl;
    std::cout << ""(sizeof(uint64_t) == 8) && !std::is_signed<uint64_t>::value? ""
              << ((sizeof(uint64_t) == 8) && !std::is_signed<uint64_t>::value ? ""yes"" : ""no"") << std::endl;
    return 0;
}

```
produces this output
```
int64_t...............: (l) 8
long..................: (l) 8
long int..............: (l) 8
long long int.........: (x) 8
long long.............: (x) 8
uint64_t..............: (m) 8
unsigned long.........: (m) 8
unsigned long int.....: (m) 8
unsigned long long int: (y) 8
unsigned long long....: (y) 8
long long == int64_t? no
(sizeof(long long) == 8) && std::is_signed<long long>::value? yes
(sizeof(int64_t) == 8) && std::is_signed<int64_t>::value? yes
unsigned long long == uint64_t? no
(sizeof(unsigned long long) == 8) && !std::is_signed<unsigned long long>::value? yes
(sizeof(uint64_t) == 8) && !std::is_signed<uint64_t>::value? yes

```

On a system where the current type constraint check works, the same code outputs this
```
int64_t...............: (x) 8
long..................: (l) 8
long int..............: (l) 8
long long int.........: (x) 8
long long.............: (x) 8
uint64_t..............: (y) 8
unsigned long.........: (m) 8
unsigned long int.....: (m) 8
unsigned long long int: (y) 8
unsigned long long....: (y) 8
long long == int64_t? yes
(sizeof(long long) == 8) && std::is_signed<long long>::value? yes
(sizeof(int64_t) == 8) && std::is_signed<int64_t>::value? yes
unsigned long long == uint64_t? yes
(sizeof(unsigned long long) == 8) && !std::is_signed<unsigned long long>::value? yes
(sizeof(uint64_t) == 8) && !std::is_signed<uint64_t>::value? yes
```

Since using an `int64_t` should be supported (as it is equivalent to `long long`), I propose that the code be modified to either use the integral types defined in `cstdint` (`int8_t`, `int16_t`, `int32_t`, `int64_t`), or the check should be changed to look for a 64-bit type that is signed using a combination of `sizeof` and `std::is_signed`

### Source code / logs
The following example produces the error
```
#include <tensorflow/core/framework/op.h>
#include <tensorflow/core/framework/op_kernel.h>

REGISTER_OP(""ExampleOp"").Attr(""T: {int32, int64, uint32, uint64}"");

template <typename T>
class ExampleOp : public tensorflow::OpKernel {
public:
  explicit ExampleOp(tensorflow::OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(tensorflow::OpKernelContext* context) override {}
};

REGISTER_KERNEL_BUILDER(Name(""ExampleOp"").Device(tensorflow::DEVICE_CPU).TypeConstraint<int32_t>(""T""), ExampleOp<int32_t>);
REGISTER_KERNEL_BUILDER(Name(""ExampleOp"").Device(tensorflow::DEVICE_CPU).TypeConstraint<int64_t>(""T""), ExampleOp<int64_t>);
REGISTER_KERNEL_BUILDER(Name(""ExampleOp"").Device(tensorflow::DEVICE_CPU).TypeConstraint<uint32_t>(""T""), ExampleOp<uint32_t>);
REGISTER_KERNEL_BUILDER(Name(""ExampleOp"").Device(tensorflow::DEVICE_CPU).TypeConstraint<uint64_t>(""T""), ExampleOp<uint64_t>);
```

Here is the resulting compilation log
```
/usr/bin/c++  -Dmwe_EXPORTS -isystem /usr/local/lib/python3.5/dist-packages/tensorflow/include -I../src -march=native -mtune=native -fPIC -O3 -DNDEBUG -fPIC   -march=native -mtune=native -MD -MT CMakeFiles/mwe.dir/mwe.cpp.o -MF CMakeFiles/mwe.dir/mwe.cpp.o.d -o CMakeFiles/mwe.dir/mwe.cpp.o -c ../mwe.cpp
In file included from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:23:0,
                 from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/device_base.h:23,
                 from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:26,
                 from ../mwe.cpp:2:
/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/types.h: In instantiation of â€˜struct tensorflow::DataTypeToEnum<long int>â€™:
/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h:82:62:   required from â€˜tensorflow::KernelDefBuilder& tensorflow::KernelDefBuilder::TypeConstraint(const char*) [with T = long int]â€™
../mwe.cpp:16:1:   required from here
/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:356:3: error: static assertion failed: Specified Data Type not supported
   static_assert(IsValidDataType<T>::value, ""Specified Data Type not supported"");
   ^~~~~~~~~~~~~
In file included from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:28:0,
                 from ../mwe.cpp:2:
/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h: In instantiation of â€˜tensorflow::KernelDefBuilder& tensorflow::KernelDefBuilder::TypeConstraint(const char*) [with T = long int]â€™:
../mwe.cpp:16:1:   required from here
/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h:82:62: error: â€˜vâ€™ is not a member of â€˜tensorflow::DataTypeToEnum<long int>â€™
   return this->TypeConstraint(attr_name, DataTypeToEnum<T>::v());
                                          ~~~~~~~~~~~~~~~~~~~~^~
In file included from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:23:0,
                 from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/device_base.h:23,
                 from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:26,
                 from ../mwe.cpp:2:
/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/types.h: In instantiation of â€˜struct tensorflow::DataTypeToEnum<long unsigned int>â€™:
/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h:82:62:   required from â€˜tensorflow::KernelDefBuilder& tensorflow::KernelDefBuilder::TypeConstraint(const char*) [with T = long unsigned int]â€™
../mwe.cpp:20:1:   required from here
/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:356:3: error: static assertion failed: Specified Data Type not supported
   static_assert(IsValidDataType<T>::value, ""Specified Data Type not supported"");
   ^~~~~~~~~~~~~
In file included from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:28:0,
                 from ../mwe.cpp:2:
/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h: In instantiation of â€˜tensorflow::KernelDefBuilder& tensorflow::KernelDefBuilder::TypeConstraint(const char*) [with T = long unsigned int]â€™:
../mwe.cpp:20:1:   required from here
/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h:82:62: error: â€˜vâ€™ is not a member of â€˜tensorflow::DataTypeToEnum<long unsigned int>â€™
   return this->TypeConstraint(attr_name, DataTypeToEnum<T>::v());
                                          ~~~~~~~~~~~~~~~~~~~~^~
```

EDIT: Added a minimal working example"
20585,non-chief workers hang in training distributed seq2seq model when time-major is true,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs High Sierra version 10.13.3 (also found in Linux RHEL 6.5)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.7.0 (also found in 1.8.0)
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:N/A CPU version
- **GPU model and memory**:N/A
- **Exact command to reproduce**:
python nonchief_hang.py --out_dir=/tmp/rnn/work0 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=0 --job_name=worker --time_major
python nonchief_hang.py --out_dir=/tmp/rnn/work1 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=1 --job_name=worker --time_major
python nonchief_hang.py --out_dir=/tmp/rnn/ps0 --ps_hosts=localhost:2222  --worker_hosts=localhost:1111,localhost:3333 --task_index=0 --job_name=ps --time_major

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
In a distributed seq2seq model (see a short example below), when time-major is set to true, the non-chief workers hang, further debugging showed that the non-chief workers were waiting for embedding matrices to be initialized. The chief worker trains without issues.

If we remove (""--time_major"") from the command line, then non-chief workers run fine.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
This is a simple seq2seq model following the implementation of [tensorflow/nmt](https://github.com/tensorflow/nmt), with optimizer removed.

```python
import argparse
import sys

import tensorflow as tf
import numpy as np
import time
FLAGS = None


def main(_):
  ps_hosts = FLAGS.ps_hosts.split("","")
  worker_hosts = FLAGS.worker_hosts.split("","")

  # Create a cluster from the parameter server and worker hosts.
  cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})

  # Create and start a server for the local task.
  server = tf.train.Server(cluster,
                           job_name=FLAGS.job_name,
                           task_index=FLAGS.task_index)
  if FLAGS.job_name == ""ps"":
    server.join()
  elif FLAGS.job_name == ""worker"":

    # Assigns ops to the local worker by default.
    with tf.device(tf.train.replica_device_setter(
        worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
        cluster=cluster)):

      # Build model...
      batch_size = 24
      src_len = 25
      source = np.ones((batch_size, src_len), dtype=int)

      embedding_encoder = tf.get_variable(
          ""embedding_encoder"", (99, 128), tf.float32)

      embedding_decoder = tf.get_variable(
          ""embedding_decoder"", (99, 128), tf.float32)

      with tf.variable_scope(""encoder"") as decoder_scope:
        encoder_emb_inp = tf.nn.embedding_lookup(
           embedding_encoder, source)

        encoder_cell = tf.contrib.rnn.BasicLSTMCell(
          num_units=256,
          forget_bias=1.0)

        if FLAGS.time_major:
            sequence_length=np.ones(src_len, dtype=int)*batch_size
        else:
            sequence_length=np.ones(batch_size, dtype=int)*src_len
 
        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(
            encoder_cell,
            encoder_emb_inp,
            dtype=tf.float32,
            sequence_length=sequence_length,
            time_major=FLAGS.time_major,
            swap_memory=True)

      with tf.variable_scope(""decoder"") as decoder_scope:
        target_input = np.ones((batch_size, src_len), dtype=int)
        decoder_emb_inp = tf.nn.embedding_lookup(
            embedding_decoder, target_input) 

        # Helper
        if FLAGS.time_major:
           dummy = tf.fill([src_len], batch_size)
        else:
           dummy = tf.fill([batch_size], src_len)

        helper = tf.contrib.seq2seq.TrainingHelper(
            decoder_emb_inp, dummy,
            time_major=FLAGS.time_major)

        decoder_cell = tf.contrib.rnn.BasicLSTMCell(
            num_units=256,
            forget_bias=1.0)
        decoder_initial_state = encoder_state

        # Decoder
        my_decoder = tf.contrib.seq2seq.BasicDecoder(
            decoder_cell,
            helper,
            decoder_initial_state)

        # Dynamic decoding
        decoder_outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(
            my_decoder,
            output_time_major=FLAGS.time_major,
            swap_memory=True,
            scope=decoder_scope)

      global_step = tf.train.get_or_create_global_step()
      
      train_op = decoder_outputs

    # The StopAtStepHook handles stopping after running given steps.
    hooks=[tf.train.StopAtStepHook(last_step=100)]

    # The MonitoredTrainingSession takes care of session initialization,
    # restoring from a checkpoint, saving to a checkpoint, and closing when done
    # or an error occurs.
    with tf.train.MonitoredTrainingSession(master=server.target,
                                           is_chief=(FLAGS.task_index == 0),
                                           checkpoint_dir=FLAGS.out_dir,
                                           hooks=hooks) as mon_sess:
      # This is an infinite while loop since global_step does not increment at all.
      # In a real training, global_step is passed to an optimizer, then increments
      # after each step.
      while not mon_sess.should_stop():
        x = mon_sess.run(train_op)
        global_step_val = global_step.eval(session=mon_sess)
        print('global_step = {0}, time_major is {1}'.format(global_step_val, FLAGS.time_major))

if __name__ == ""__main__"":
  parser = argparse.ArgumentParser()
  parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")
  # Flags for defining the tf.train.ClusterSpec
  parser.add_argument(
      ""--ps_hosts"",
      type=str,
      default="""",
      help=""Comma-separated list of hostname:port pairs""
  )
  parser.add_argument(
      ""--worker_hosts"",
      type=str,
      default="""",
      help=""Comma-separated list of hostname:port pairs""
  )
  parser.add_argument(
      ""--job_name"",
      type=str,
      default="""",
      help=""One of 'ps', 'worker'""
  )
  # Flags for defining the tf.train.Server
  parser.add_argument(
      ""--task_index"",
      type=int,
      default=0,
      help=""Index of task within the job""
  )
  parser.add_argument(
      ""--out_dir"",
      type=str,
      default="""",
      help=""output dir""
  )

  parser.add_argument(
      ""--time_major"",
      action='store_true',
      help=""time major""
  )
  FLAGS, unparsed = parser.parse_known_args()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
```
"
20584,Can't Import Tensor Flow in Annaconda Python 3.6,"I just installed CUDA 9.2 and CUDANN and Tensor Flow on Windows 10.  I'm using Anaconda 3.6 Python.

Here is a trace that I get when I try to import tensor flow.  It's telling me it can't load a dll but its is not telling me which one.  Can you help.  

Here is a directory listing of the CUDA dlls in my path and the trace.  

********************  Directory Listing ************************

PS C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\bin> dir


    Directory: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\bin


Mode                LastWriteTime         Length Name
----                -------------         ------ ----
d-----         7/5/2018   8:32 PM                crt
-a----        4/12/2018   1:20 AM         202752 bin2c.exe
-a----        4/12/2018   1:21 AM       54600704 cublas64_92.dll
-a----        4/12/2018   1:20 AM         370176 cuda-memcheck.exe
-a----        4/12/2018   1:20 AM        4092416 cudafe++.exe
-a----        4/12/2018   1:21 AM         299008 cudart32_92.dll
-a----        4/12/2018   1:21 AM         368128 cudart64_90.dll
-a----        4/12/2018   1:21 AM         368128 cudart64_92.dll
-a----         7/5/2018  10:17 PM      336443392 cudnn64_7.dll
-a----        4/12/2018   1:21 AM       87017472 cufft64_92.dll
-a----        4/12/2018   1:21 AM         197632 cufftw64_92.dll
-a----        4/12/2018   1:21 AM        3692032 cuinj32_92.dll
-a----        4/12/2018   1:21 AM        4625408 cuinj64_92.dll
-a----        4/12/2018   1:20 AM        1695744 cuobjdump.exe
-a----        4/12/2018   1:21 AM       47990784 curand64_92.dll
-a----        4/12/2018   1:21 AM      114172416 cusolver64_92.dll
-a----        4/12/2018   1:21 AM       65776128 cusparse64_92.dll
-a----        4/12/2018   1:20 AM         284672 fatbinary.exe
-a----        4/12/2018   1:20 AM        1306112 gpu-library-advisor.exe
-a----        4/12/2018   1:21 AM         203264 nppc64_92.dll
-a----        4/12/2018   1:21 AM       10174464 nppial64_92.dll
-a----        4/12/2018   1:21 AM        3985920 nppicc64_92.dll
-a----        4/12/2018   1:21 AM        1010688 nppicom64_92.dll
-a----        4/12/2018   1:21 AM        6928896 nppidei64_92.dll
-a----        4/12/2018   1:21 AM       51902976 nppif64_92.dll
-a----        4/12/2018   1:21 AM       25260032 nppig64_92.dll
-a----        4/12/2018   1:21 AM        6574592 nppim64_92.dll
-a----        4/12/2018   1:21 AM       15030272 nppist64_92.dll
-a----        4/12/2018   1:21 AM         177152 nppisu64_92.dll
-a----        4/12/2018   1:21 AM        2621440 nppitc64_92.dll
-a----        4/12/2018   1:21 AM        8543232 npps64_92.dll
-a----        4/12/2018   1:20 AM         241152 nvblas64_92.dll
-a----        4/12/2018   1:20 AM         379904 nvcc.exe
-a----        4/12/2018   1:20 AM            310 nvcc.profile
-a----        4/12/2018   1:20 AM       18156032 nvdisasm.exe
-a----        4/12/2018   1:21 AM       66096128 nvgraph64_92.dll
-a----        4/12/2018   1:20 AM        7804928 nvlink.exe
-a----        4/12/2018   1:20 AM        4115968 nvprof.exe
-a----        4/12/2018   1:20 AM         220160 nvprune.exe
-a----        4/12/2018   1:20 AM        3213312 nvrtc-builtins64_92.dll
-a----        4/12/2018   1:20 AM       15532544 nvrtc64_92.dll
-a----        4/12/2018   1:20 AM             53 nvvp.bat
-a----        4/12/2018   1:20 AM        7686144 ptxas.exe

********************  Python Output  ************************

PS C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\bin> python
Python 3.6.0 |Anaconda 4.3.0 (64-bit)| (default, Dec 23 2016, 11:57:41) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>

"
20583,Unable to run fetch_imagenet_models.shï¼Œcan not download the models of VGG16ï¼ŒThe url is False.,"If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
Â· Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes

Â· OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10-x64

Â· TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/tensorflow.git

Â· TensorFlow version (use command below): r1.8, command :
git checkout -b v1.8 -f origin/r1.8

Â· Python version: Anaconda3 - python3.6

Â· Bazel version (if compiling from source): I used CMAKE 3.11.1

Â· GCC/Compiler version (if compiling from source): both Visual Studio 2015 and Visual Studio 2015' MSBuild

Â· CUDA/cuDNN version: CUDA9.0, cudnn-9.0-win10-7.1

Â· GPU model and memory: GTX-860m with 2Gb Memory

### Describe the problem
When configuring the environment of the Faster-RCNN project  from githubï¼š
Unable to run file â€œfetch_imagenet_models.shâ€ï¼ŒUnable to download the models of VGG16 cause the url is not OK,  so I can not get download ""voc_0712_80k-110k.tgz"".

`
cd $FRCN_ROOT
./data/scripts/fetch_imagenet_models.sh
`
### Source code / logs
`
cd $FRCN_ROOT
./data/scripts/fetch_imagenet_models.sh
`"
20582,TF-hub command line cannot download module,"When I try retrain my dataset as said in https://www.tensorflow.org/tutorials/image_retraining,
no matter which model I choose to fill in the --tfhub_module option from
https://www.tensorflow.org/hub/modules/image#mobilenet

I always stucked in ""INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/imagenet/mobilenet_v1_100_128/feature_vector/1'.""

I have tried to change tfhub.dev/ into tensorflow.google.cn/hub/modules/ ,but fail too.

I am using tensorflow version1.7
Do I missed something?How can I fix it?
"
20579,[Feature Request] Support S3 KMS client-side decryption when loading data,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: V100

### Describe the problem
It would be great to support S3 KMS client side decryption within TensorFlow, ideally transparently.

We use S3 with KMS to store sensitive data and decrypting it before loading it into a `tf.Dataset` can get really gross/be a major bottle neck, in part because AWS's boto3 [client library does not support client side KMS encryption](https://docs.aws.amazon.com/general/latest/gr/aws_sdk_cryptography.html) so decrypting data needs to be done manually ([example here](https://github.com/tedder/s3-client-side-encryption/blob/master/get.py)).

However the AWS C++ client does support KMS client-side decryption.

I'm not super familiar with TensorFlow's codebase but seems like there could be a couple of ways this could be added:

1) Another contrib `tf.Dataset` implementation ([example here of an existing Dataset contrib](https://github.com/tensorflow/tensorflow/pull/19712/files))
In which case an extra method would likely need to be added to [`s3_file_system.h`](https://github.com/tensorflow/tensorflow/blob/935d5d8550ad06bc77e41e9a3d987658d3731be9/tensorflow/core/platform/s3/s3_file_system.h) that uses the C++ client for decryption, or handle the envelope encryption as part of how the Python Dataset class loads data

or

2) From what I can tell `tf.python.lib.io.file_io` uses [`NewRandomAccessFile`](https://github.com/tensorflow/tensorflow/blob/935d5d8550ad06bc77e41e9a3d987658d3731be9/tensorflow/core/platform/s3/s3_file_system.h#L30) from `s3_file_system.h` when loading datasets from S3 (as shown in the [s3 deploy docs](https://www.tensorflow.org/versions/r1.9/deploy/s3), correct me if I am wrong). 
Another envar could be added (`S3_KMS_ARN`) and if set the C++ S3 client would decrypt client side. To me this would seem like the better solution since no new datasets would need to be worried about, encrypted files in S3 would behave like regular files.

An example of the C++ S3 encryption client can be found [here](https://aws.amazon.com/blogs/developer/amazon-s3-encryption-client-is-now-available-for-c/).

### Exact command to reproduce
N/A"
20577,Unsupported tensor size Error with TF-TRT integration,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: 1.8
- **Python version**:  3.4
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:9/7.1
- **GPU model and memory**: gtx 1080ti
- **Exact command to reproduce**:

`import tensorflow as tf
from tensorflow.contrib import tensorrt as trt #MDFY
from tensorflow.python.framework import tensor_util

def load_graph(frozen_graph_filename):
    # We load the protobuf file from the disk and parse it to retrieve the 
    # unserialized graph_def
    with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    # Then, we can use again a convenient built-in function to import a graph_def into the 
    # current default Graph
    with tf.Graph().as_default() as graph:
        tf.import_graph_def(
            graph_def
        )
    return graph
g = load_graph(path_to_pb_file)
with tf.Session(graph=g) as sess:
	trt_graph = trt.create_inference_graph(
input_graph_def=tf.get_default_graph().as_graph_def(),	
outputs=nodenames,
max_batch_size=1,
max_workspace_size_bytes=1 << 25,
precision_mode=""FP32"",  # TRT Engine precision ""FP32"",""FP16"" or ""INT8""
minimum_segment_size=2  # minimum number of nodes in an engine
)
[print(n.name) for n in g.as_graph_def().node]

`

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I want to tracebacack which layer the error is originating from, but the log information is not sufficient to do so, is there a workaround ?
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
Traceback (most recent call last):
  File ""xyz.py"", line zz, in <module>
    minimum_segment_size=2  # minimum number of nodes in an engine
  File ""/home/dhingratul/opt/Python-3.4.4/py34/lib/python3.4/site-packages/tensorflow/contrib/tensorrt/python/trt_convert.py"", line 115, in create_inference_graph
    int(msg[0]))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Unsupported tensor size: 2
"
20575,Boosted Trees Multi-Label Regression ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I plan to write custom code. 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Docker - GPU 
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: V9.0.176
- **GPU model and memory**:  4 x GTX 1080 Ti 
- **Exact command to reproduce**: 

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When can I expect boosted trees to support multiple **label_dimension** ? Currently, **_HOLD_FOR_MULTI_CLASS_SUPPORT** is the placeholder for the implementation in _contrib_, while **label_dimension=_HOLD_FOR_MULTI_DIM_SUPPORT** is the placeholder in v1.8. Is there a pre-release implementation that is available to test before it is a part of TF? 

[https://arxiv.org/pdf/1710.11547.pdf](https://arxiv.org/pdf/1710.11547.pdf) is where I am getting the information about multi-class boosting. 

Page for [BoostedTreesRegressor](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesRegressor)

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20573,Features parameters for retrain.py used model,"

------------------------

### System information
- **Using retrain.py to train my own data**:
- **Windows**:
- **install tensorflow using cmd**:
- ** tensorflow version 2.0**:
- ** python version 3.6.6rc1**: 
- **No bazel**:


### Describe the problem
Need help regarding setting parameters for the model in ClassifierActivity.java .I have trained by custom dataset using retrain.py gave on the tensorflow.But when I run the app on the android by deploying my own model. The app runs successfully but not labeling the detecting images. I think there is some issue with parameters 
private static final int INPUT_SIZE = 299;
  private static final int IMAGE_MEAN = 128;
  private static final float IMAGE_STD = 128;
  private static final String INPUT_NAME = ""Mul"";
  private static final String OUTPUT_NAME = ""final_result"";
Or there is another issue you need to tell me.Please help as I have to submit my work as soon as possible. 


"
20571,Cannot query shape of opaque params in CudnnGRU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Yes
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.9.0-rc0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: NVIDIA GTX 1080 Ti, 11176MiB

### Describe the problem
When trying to compute the number of parameters in `tf.contrib.cudnn_rnn.CudnnGRU`, TensorFlow will crash.

The reason to do this is to share the weights between `CudnnGRU` and `CudnnCompatibleGRUCell`. There is no documentation with examples on how to do this (see #20273 ), but there is a high-level description from the tf 1.2 docs:
```
Cudnn implementation of the GRU model. Cudnn RNN has an opaque parameter buffer that can be used for inference and training. But it is possible that the layout of the parameter buffers changes between generations. So it is highly recommended to use RNNParamsSaveable to save and restore weights and biases in a canonical format.

This is a typical use case:
  * The user creates a CudnnRNN model.
  * The user query that parameter buffer size.
  * The user creates a variable of that size that serves as the parameter buffers.
  * The user either initialize the parameter buffer, or load the canonical weights into the parameter buffer.
  * The user calls the model with the parameter buffer for inference, or training.
  * If training, the user creates a Saver object.
  * If training, the user creates a RNNParamsSaveable object from the parameter buffer for it to be later saved in the canonical format. When creating a RNNParamsSaveable object, a name could be provided, which is useful in distinguishing the names of multiple RNNParamsSaveable objects (e.g. for an encoder-decoder model).
  * Once a while, the user saves the parameter buffer into model checkpoints with Saver.save().
  * When restoring, the user creates a RNNParamsSaveable object and uses Saver.restore() to restore the paramter buffer from the canonical format to a user-defined format, as well as to restore other savable objects in the checkpoint file.
```
So the second step in this process fails.

Also to consider: [the test file has deprecated code and will fail](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/cudnn_rnn/python/kernel_tests/cudnn_rnn_ops_test.py), using `cuddn_gru_instance.params_size()` rather than `.count_params()`, which is no longer defined. These test cases are the only resource available for users attempting to find out how undocumented features should be used, but they're outdated.

### Source code / logs
```
import tensorflow as tf
rnn = tf.contrib.cudnn_rnn.CudnnGRU(1, 7, direction='unidirectional')
rnn.build([2,3,5])
rnn.count_params()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/danielwatson/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1298, in count_params
    weight_shapes = [w.get_shape().as_list() for w in self.weights]
  File ""/home/danielwatson/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1298, in <listcomp>
    weight_shapes = [w.get_shape().as_list() for w in self.weights]
  File ""/home/danielwatson/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 903, in as_list
    raise ValueError(""as_list() is not defined on an unknown TensorShape."")
ValueError: as_list() is not defined on an unknown TensorShape.
```
"
20570,Slow tf.hessians slicing,"Hi, I'd like to calculate the diagonal matrix of some hessian:

I've got some variables `a, b` of shape `(5, <num_features>)` and I'd need for each feature the hessian's diagonal:

```python3
>>> [self.a, self.b]
[<tf.Tensor 'Linear_Batch_Model/a/concat:0' shape=(5, 10000) dtype=float32>,
 <tf.Tensor 'Linear_Batch_Model/b/concat:0' shape=(5, 10000) dtype=float32>]

>>> tf.hessians(self.full_loss, [self.a, self.b])
[<tf.Tensor 'Reshape_1:0' shape=(5, 10000, 5, 10000) dtype=float32>,
 <tf.Tensor 'Reshape_3:0' shape=(5, 10000, 5, 10000) dtype=float32>]

>>> [tf.diag_part(t) for t in tf.hessians(self.full_loss, [self.a, self.b])]
[<tf.Tensor 'DiagPart_3:0' shape=(5, 10000) dtype=float32>,
 <tf.Tensor 'DiagPart_4:0' shape=(5, 10000) dtype=float32>]
```

**However, this calculation seems to run infinitely:**
The full hessian matrices are huge, but since I only take the diagonal elements, they should never be calculated explicitly.

- I can calculate the diagonal by hand (`gradient(gradient(loss, [a,b]), [a,b])`), but I always expected tensorflow to optimize such operations implicitely...
- Also, this trick does not work if I'd need other parts of the Hessian matrix, e.g. if I need for each independent gene the `parm x parm`-shaped hessian:
TF does not allow to differentiate only slices of a and b (e.g. `tf.hessians(loss, [a[:, 0], b[:,0]])`)

.

### The problem
Tensorflow seems to explicitly calculate the full hessian matrix

### What I'd expect
Fast calculation since I only request the diagonal

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: none
- **GCC/Compiler version (if compiling from source)**: none
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**: none"
20569,"Tensorflow 1.8.0 ALWAYS looking for libcublas.so.9.0 with Cuda 9.1, Cudnn 7.1.12 (and libcublas.so.9.1)"," **System information**
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: binary (via virtualenv)
- **TensorFlow version (use command below)**: tensorflow-gpu 1.8.0
- **Python version**: Python 3.6.5
- **CUDA/cuDNN version**: CUDA 9.1/CuDNN 7.1.12
- **GPU model and memory**: NVIDIA TITAN X (Pascal)/64 GB DDR4
- **GPU driver**: 390.48
- **Exact command to reproduce**:  import tensorflow as tf

### Describe the problem
Tensorflow is looking for libcublas.so.9.0. When I run ""import tensorflow as tf"" inside my virtual environment, I get the following error message:

ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory
Failed to load the native TensorFlow runtime.

Outputs of: Idconfig -v

/sbin/ldconfig.real: Can't stat /usr/local/lib/i386-linux-gnu: No such file or directory
/sbin/ldconfig.real: Can't stat /usr/local/lib/i686-linux-gnu: No such file or directory
/sbin/ldconfig.real: Can't stat /lib/i686-linux-gnu: No such file or directory
/sbin/ldconfig.real: Can't stat /usr/lib/i686-linux-gnu: No such file or directory
/sbin/ldconfig.real: Can't stat /usr/local/lib/x86_64-linux-gnu: No such file or directory
/sbin/ldconfig.real: Can't stat /usr/local/cuda-9.0/lib64: No such file or directory
/sbin/ldconfig.real: Path `/lib/x86_64-linux-gnu' given more than once
/sbin/ldconfig.real: Path `/usr/lib/x86_64-linux-gnu' given more than once
/usr/lib/x86_64-linux-gnu/libfakeroot:
	libfakeroot-0.so -> libfakeroot-tcp.so
/lib/i386-linux-gnu:
	libgcc_s.so.1 -> libgcc_s.so.1
	libnsl.so.1 -> libnsl-2.27.so
	libresolv.so.2 -> libresolv-2.27.so
	libnss_files.so.2 -> libnss_files-2.27.so
	libSegFault.so -> libSegFault.so
	libdl.so.2 -> libdl-2.27.so
	libutil.so.1 -> libutil-2.27.so
	libnss_nis.so.2 -> libnss_nis-2.27.so
	librt.so.1 -> librt-2.27.so
	libcidn.so.1 -> libcidn-2.27.so
	libnss_nisplus.so.2 -> libnss_nisplus-2.27.so
	libc.so.6 -> libc-2.27.so
	libBrokenLocale.so.1 -> libBrokenLocale-2.27.so
	libnss_dns.so.2 -> libnss_dns-2.27.so
/sbin/ldconfig.real: /lib/i386-linux-gnu/ld-2.27.so is the dynamic linker, ignoring


/usr/local/lib:
	libcublas.so.9.1 -> libcublas.so.9.1

/usr/lib/x86_64-linux-gnu:
/sbin/ldconfig.real: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2 is for unknown machine 21.

/sbin/ldconfig.real: /usr/lib/x86_64-linux-gnu/libcudnn.so.7 is for unknown machine 21.

/sbin/ldconfig.real: /usr/lib/x86_64-linux-gnu/libcudnn.so is for unknown machine 21.




### Source code / logs
Traceback (most recent call last):
  File ""/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/meriemlio/tensorflow/venv/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/meriemlio/tensorflow/venv/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/meriemlio/tensorflow/venv/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/meriemlio/tensorflow/venv/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
"
20568,CMake: Build break occurs when making tf_python_build_pip_package using cmake.,"### Problem
Build break occurs when making __tf_python_build_pip_package__ using __cmake__ instead of Bazel.

### System information
* Have I written custom code: N/A
* OS Platform and Distribution: Ubuntu 16.04.4 LTS
* TensorFlow installed from: Not installed
* TensorFlow version: N/A
* Python version: 2.7.12
* Bazel version: Not used since CMake build
* CMake version: 3.5.1
* GCC/Compiler version: 5.4.0
* CUDA/cuDNN version: 9.0 (But not important in this case)
* GPU model and memory: Nvidia 1080ti / 11G
* Exact command to reproduce: refer the below 'How to reproduce'

### Source code information
* Branch: r1.9
* Commit ID: e3f2b5903c56ec09fa501dd8e95dd0e4336843fc (latest when I built)
* Just clone without modification

### How to reproduce 
```bash
$ git clone https://github.com/tensorflow/tensorflow.git
$ cd tensorflow
$ git checkout -b cmake_build_test e3f2b5903c56ec09fa501dd8e95dd0e4336843fc
$ mkdir build
$ cd build
$ cmake -DCMAKE_BUILD_TYPE=Release ../tensorflow/contrib/cmake
$ make VERBOSE=1 -j4  tf_python_build_pip_package &> cmake_build_log.txt
```

### Error message
* ImportError: No module named tensorflow.python.util
* Download the full build log message: [link](https://www.dropbox.com/s/5hfgqugzbwwu3i2/cmake_build_log.txt?dl=0)
```bash
make[3]: Entering directory '/home/again4you/tensorflow/build'                  
[100%] Generating __init__.py files for Python API.                             
cd /home/again4you/tensorflow/build/tf_python && /usr/bin/cmake -E remove -f /home/again4you/tensorflow/build/tf_python/tensorflow/__init__.py
cd /home/again4you/tensorflow/build/tf_python && /usr/bin/cmake -E env PYTHONPATH=/home/again4you/tensorflow/build/tf_python /usr/bin/python /home/again4you/tensorflow/build/tf_python/tensorflow/tools/api/generator/create_python_api.py --root_init_template=/home/again4you/tensorflow/build/tf_python/tensorflow/api_template.__init__.py --apidir=/home/again4you/tensorflow/build/tf_python/tensorflow /home/again4you/tensorflow/api_init_files_list.txt
Traceback (most recent call last):                                              
  File ""/home/again4you/tensorflow/build/tf_python/tensorflow/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.util import tf_decorator                             
ImportError: No module named tensorflow.python.util                             
CMakeFiles/tf_python_api.dir/build.make:57: recipe for target 'tf_python/tensorflow/__init__.py' failed
make[3]: *** [tf_python/tensorflow/__init__.py] Error 1                         
make[3]: Leaving directory '/home/again4you/tensorflow/build'
```

### Detailed reason that I found
* __tf_python/tensorflow/\_\_init.py\_\___ does not exist as below.
```bash
$ ls tf_python/tensorflow/__init__.py
ls: cannot access 'tf_python/tensorflow/__init__.py': No such file or directory
```

* ~~When building tensorflow using __python 3__, it does not matter since python3 supports [Implicit Namespace Packages](https://www.python.org/dev/peps/pep-0420/). So `create_python_api.py` runs without errors. However, __ImportError__ occurs when using __python 2__ if tf_python/tensorflow/\_\_init\_\_.py does not exist.~~
* The similar error occurs when using __python 3__. Refer my below comment.

* After checking the build log with 'VERBOSE' option, I found out the below message.
Because of 'tf_python/tensorflow/**app**' module, 'tf_python/tensorflow/\_\_init\_\_.py' is deleted. But I can't find any 'app' module in tf_python.

```bash
...
[100%] Built target tf_extension_ops                                            
make -f CMakeFiles/tf_python_api.dir/build.make CMakeFiles/tf_python_api.dir/depend
make[3]: Entering directory '/home/again4you/tensorflow/build'                  
cd /home/again4you/tensorflow/build && /usr/bin/cmake -E cmake_depends ""Unix Makefiles"" /home/again4you/tensorflow/tensorflow/contrib/cmake /home/again4you/tensorflow/tensorflow/contrib/cmake /home/again4you/tensorflow/build /home/again4you/tensorflow/build /home/again4you/tensorflow/build/CMakeFiles/tf_python_api.dir/DependInfo.cmake --color=
Deleting primary custom command output ""/home/again4you/tensorflow/build/tf_python/tensorflow/__init__.py"" because another output ""/home/again4you/tensorflow/build/tf_python/tensorflow/app/__init__.py"" does not exist.
Dependee ""/home/again4you/tensorflow/build/CMakeFiles/tf_python_api.dir/DependInfo.cmake"" is newer than depender ""/home/again4you/tensorflow/build/CMakeFiles/tf_python_api.dir/depend.internal"".
...
```
"
20567,Feature request: verify py_func tensor's shape when evaluating,"`tf.py_func(my_py_func, args, dype)` returns a tensor of known type but unknown shape.
When my_py_func runs, py_func will check that the type is what's expected and fail otherwise.

But, if you set an incorrect shape with .set_shape there's no warning. Having a loud warning would have saved me a few hours yesterday.

Minimal example;

```python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
import numpy as np

def my_py_func(x):
  return x

def main(argv):
  del argv

  print(tf.GIT_VERSION, tf.VERSION)

  t = tf.constant(0, dtype=tf.int64)
  print('tf.constant(0):', t)
  t = tf.py_func(my_py_func, [t], tf.int64)
  print('tf.py_func:', t)
  t.set_shape([100,100])
  print('t.set_shape:', t)

  with tf.Session() as sess:
    t = sess.run(t)
    print('sess.run(t)', t)

if __name__ == '__main__':
  tf.app.run()
```

Output;
```
$ python using_py_func.py
v1.8.0-0-g93bc2e2072 1.8.0
tf.constant(0): Tensor(""Const:0"", shape=(), dtype=int64)
tf.py_func: Tensor(""PyFunc:0"", dtype=int64)
t.set_shape: Tensor(""PyFunc:0"", shape=(100, 100), dtype=int64)
sess.run(t) 0
```
"
20566,Cannot use warm startup of an estimator with MirroredStrategy,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:  using nvidia containter: https://docs.nvidia.com/deeplearning/dgx/tensorflow-release-notes/rel_18.06.html#rel_18.06 
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**: 9.0.176
- **GPU model and memory**: nvidia tesla v100
- **Exact command to reproduce**:N/A

### Describe the problem
I'm training an network using tf.estimator where I load a set of variables from an existing checkpoint. This is done using the warm start-up option from the estimator class. While the code works when running on a single GPU, It generates an error when I use the MirroredStrategy.

### Source code / logs
The code:
```
ws = tf.estimator.WarmStartSettings(ckpt_to_initialize_from=""init_checkpoint.ckpt""),
                                                vars_to_warm_start=""^((?!Logits).)*$""
                                                )
self.trainer = tf.estimator.Estimator(
            model_fn=self.get_model_fn,  # First-class function
            params=None,  # HParams are passed using the config file
            config=run_config,
            warm_start_from=ws
        )
```

This generate the following trace:
```
Traceback (most recent call last):
  File ""train.py"", line 37, in <module>
    experimenter.run_training_experiment(config)
  File ""/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py"", line 39, in run_training_experiment
    tf.estimator.train_and_evaluate(self.trainer, self.training_specs, self.eval_specs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 439, in train_and_evaluate
    executor.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 518, in run
    self.run_local()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 650, in run_local
    hooks=train_hooks)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 363, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 841, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 977, in _train_model_distributed
    saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 986, in _train_with_estimator_spec
    warm_starting_util.warm_start(*self._warm_start_settings)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 342, in warm_start
    _warm_start_var(variable, ckpt_to_initialize_from, prev_var_name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/warm_starting_util.py"", line 139, in _warm_start_var
    ""PartitionedVariable, but is {}"".format(type(var)))
TypeError: var MUST be one of the following: a Variable, list of Variable or PartitionedVariable, but is <class 'tensorflow.contrib.distribute.python.values.MirroredVariable'>
Makefile:19: recipe for target 'train_gpu' failed
make: *** [train_gpu] Error 1
```"
20565,while_loop on GPU doesn't iterate,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

no

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

""16.04.4 LTS (Xenial Xerus)

- **TensorFlow installed from (source or binary)**:

binary

- **TensorFlow version (use command below)**:

tensorflow-gpu binary

tf.VERSION = 1.9.0-rc2
tf.GIT_VERSION = v1.9.0-rc1-48-ge3f2b5903c
tf.COMPILER_VERSION = v1.9.0-rc1-48-ge3f2b5903c

- **Python version**: 

Python 2.7.12


- **Bazel version (if compiling from source)**:

N/A

- **GCC/Compiler version (if compiling from source)**:

N/A

- **CUDA/cuDNN version**:

CUDA 8.0
cuDNN 6.0

- **GPU model and memory**:

GeForce GTX 960M 4GB RAM
NVIDIA Driver Version: 384.130

- **Exact command to reproduce**:

```python
from __future__ import print_function

import tensorflow as tf

with tf.Graph().as_default() as graph:
    with tf.device('/gpu:0'):
        N = tf.constant(10, dtype=tf.int64)

        def cond(i, s):
            return tf.less(i, N)

        def body(i, s):
            n = (i+1)*2
            return i+1, n

        loop_vars = [tf.constant(0, dtype=tf.int64)]*2
        loop = tf.while_loop(cond, body, loop_vars)

    init = tf.global_variables_initializer()

    graph.finalize()

config = tf.ConfigProto(log_device_placement=True)

with tf.Session(graph=graph, config=config) as S:
    S.run(init)

    print(S.run(loop))

```

### Describe the problem

When the above script is run, it produces `(0,0)` instead of the expected `(10,20)`.

If one changes the device to `/cpu:0` the expected result is produced.

I've tested that the GPU works with the following code:

```python
import tensorflow as tf

with tf.device(""/gpu:0""):
    A = tf.ones([100,100])
    B = A*2

init = tf.global_variables_initializer()

with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as S:
    S.run(init)

    result = S.run(B)
    print(result.shape, result)
```

### Source code / logs

N/A"
20564,How tf.contrib.seq2seq.AttentionWrapper . working ?,"I am trying to understand tf.contrib.seq2seq.AttentionWrapper  working , I have created simple program to understand the working of AttentionWrapper : 


```
num_units=32

batch_size = 12
max_time = 10
num_dim = 11



input_data = np.random.randn(batch_size,max_time,num_dim).astype(np.float32)


# atch_size = np.random.randn(batch_size,num_dim).astype(np.float32)

sequence_length =[len(i) for i in input_data]

lstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_units)

output,last_state = tf.nn.dynamic_rnn(lstm_cell,input_data,sequence_length=sequence_length,dtype=tf.float32)

print(output,last_state)



#now let's use attention

#rnn_no_units , encoder output , sequence_length , dtype , name

attention_Bahdanau = tf.contrib.seq2seq.BahdanauAttention(num_units=num_units,memory=output,memory_sequence_length=sequence_length,dtype=tf.float32,name='BahdanauAttention')

batch_size = attention_Bahdanau.batch_size
memory_layer = attention_Bahdanau.memory_layer
keys=attention_Bahdanau.keys
values=attention_Bahdanau.values
alignment_size= attention_Bahdanau.alignments_size
state_size = attention_Bahdanau.state_size



query_ = tf.get_variable(name='query_dta_1',
                         shape=[batch_size,num_units],
                         dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))

state_ = tf.get_variable(name='state__dta_l',
                         shape=[batch_size,alignment_size],
                         dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))




initial_alignments = attention_Bahdanau.initial_alignments(batch_size,dtype=tf.float32)

initial_state   =    attention_Bahdanau.initial_state(batch_size,dtype=tf.float32)

fg=attention_Bahdanau.__call__(query_,state_)

#(12, 10)
#(12, 10)

#lstm_cell , attention_mech , rnn_num_units

attention_wrapper = tf.contrib.seq2seq.AttentionWrapper(lstm_cell,attention_Bahdanau,num_units)

zero_s= attention_wrapper.zero_state(batch_size=batch_size,dtype=tf.float32)
print(zero_s)

print(attention_wrapper.__call__(query_,zero_s))
```

Then i am getting this error:

```
AttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state:0' shape=(12, 32) dtype=float32>, h=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state_1:0' shape=(12, 32) dtype=float32>), attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(12, 32) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=(12, 10) dtype=float32>, alignment_history=(), attention_state=<tf.Tensor 'AttentionWrapperZeroState/zeros_3:0' shape=(12, 10) dtype=float32>)
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
   1588   try:
-> 1589     c_op = c_api.TF_FinishOperation(op_desc)
   1590   except errors.InvalidArgumentError as e:

InvalidArgumentError: Dimensions must be equal, but are 96 and 43 for 'rnn/while/lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [12,96], [43,128].

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-80-0926d476036c> in <module>()
      5 print(zero_s)
      6 
----> 7 print(attention_wrapper.__call__(query_,zero_s))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)
    230         setattr(self, scope_attrname, scope)
    231       with scope:
--> 232         return super(RNNCell, self).__call__(inputs, state)
    233 
    234   def _rnn_get_variable(self, getter, *args, **kwargs):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    327 
    328       # Actually call layer
--> 329       outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
    330 
    331     if not context.executing_eagerly():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    701 
    702       if not in_deferred_mode:
--> 703         outputs = self.call(inputs, *args, **kwargs)
    704         if outputs is None:
    705           raise ValueError('A layer\'s `call` method should return a Tensor '

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in call(self, inputs, state)
   1410     cell_inputs = self._cell_input_fn(inputs, state.attention)
   1411     cell_state = state.cell_state
-> 1412     cell_output, next_cell_state = self._cell(cell_inputs, cell_state)
   1413 
   1414     cell_batch_size = (

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope, *args, **kwargs)
    337     # method.  See the class docstring for more details.
    338     return base_layer.Layer.__call__(self, inputs, state, scope=scope,
--> 339                                      *args, **kwargs)
    340 
    341 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    327 
    328       # Actually call layer
--> 329       outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
    330 
    331     if not context.executing_eagerly():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    701 
    702       if not in_deferred_mode:
--> 703         outputs = self.call(inputs, *args, **kwargs)
    704         if outputs is None:
    705           raise ValueError('A layer\'s `call` method should return a Tensor '

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)
    855     # i = input_gate, j = new_input, f = forget_gate, o = output_gate
    856     lstm_matrix = math_ops.matmul(
--> 857         array_ops.concat([inputs, m_prev], 1), self._kernel)
    858     lstm_matrix = nn_ops.bias_add(lstm_matrix, self._bias)
    859 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)
   2012     else:
   2013       return gen_math_ops.mat_mul(
-> 2014           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
   2015 
   2016 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in mat_mul(a, b, transpose_a, transpose_b, name)
   4277     _, _, _op = _op_def_lib._apply_op_helper(
   4278         ""MatMul"", a=a, b=b, transpose_a=transpose_a, transpose_b=transpose_b,
-> 4279         name=name)
   4280     _result = _op.outputs[:]
   4281     _inputs_flat = _op.inputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,
    786                          input_types=input_types, attrs=attr_protos,
--> 787                          op_def=op_def)
    788       return output_structure, op_def.is_stateful, op
    789 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)
   3412           input_types=input_types,
   3413           original_op=self._default_original_op,
-> 3414           op_def=op_def)
   3415 
   3416       # Note: shapes are lazily computed with the C API enabled.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)
   1754           op_def, inputs, node_def.attr)
   1755       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,
-> 1756                                 control_input_ops)
   1757     else:
   1758       self._c_op = None

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
   1590   except errors.InvalidArgumentError as e:
   1591     # Convert to ValueError for backwards compatibility.
-> 1592     raise ValueError(str(e))
   1593 
   1594   return c_op

ValueError: Dimensions must be equal, but are 96 and 43 for 'rnn/while/lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [12,96], [43,128].
```

I am not getting from where this [43,128] coming from and how to compute_attention successfully using this wrapper ?

For live experiment i have created google colab notebook , You can run and check code there : 

https://colab.research.google.com/drive/1oR039KJgtpNsJFGh8VtQt-NRcAtXi4NR"
20563,"Exception in thread ""main"" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS","Have I written custom code (as opposed to using a stock example script provided in TensorFlow)
OS Platform and Distribution (window 7 64b):
TensorFlow installed from (binary) 1.6 later:
TensorFlow version NA:
Python version NA: 
Bazel version NA:
GCC/Compiler version NA:
CUDA/cuDNN version NA:
GPU model and memory NA:
Exact command to reproduce NA:

### Describe the problem
when i  get start  with [https://www.tensorflow.org/install/install_java](https://www.tensorflow.org/install/install_java]) I found a problem.
if the tensorflow_jni.dll version Higher than 1.5.1 then  will receive the errorï¼š

`Exception in thread ""main"" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.
	at org.tensorflow.NativeLibrary.load(NativeLibrary.java:78)
	at org.tensorflow.TensorFlow.init(TensorFlow.java:66)
	at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)
	at org.tensorflow.Graph.<clinit>(Graph.java:258)
	at HelloTF.main(HelloTF.java:25)`

1. 1.3.0  
2. 1.4.1
3. 1.5.1

these versions lower than **1.6.0** is good.

1.  1.6.0
2.  1.7.0
3.  1.8.0
4.  1.9.0.rc2

these versions receive this error.



### Source code / logs
[https://www.tensorflow.org/install/install_java](https://www.tensorflow.org/install/install_java)   HelloTF.java
"
20562,gcc: error trying to exec 'cc1plus': execvp: No such file or directory : while installing tensorflow from source on ubuntu 14.04,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 14.04
- **TensorFlow installed from (source or binary)**:source 
- **TensorFlow version (use command below)**:latest 
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**:gcc version 5.5.0 (Homebrew gcc 5.5.0_4)
- **CUDA/cuDNN version**:8.0/5
- **GPU model and memory**:K5200 quadro
- **Exact command to reproduce**: bazel build --config=opt --config=cuda te //tensorflow/tools
/pip_package:build_pip_package 
-**Have i written custom code**: N/A
I am unable to execute the command above. I get the following error, I have searched around but unable to find the exact solution matching my problem. 
`WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: /home/navidme/.cache/bazel/_bazel_navidme/ae0be448885ac013f0d1c5e70ad8620c/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/navidme/.cache/bazel/_bazel_navidme/ae0be448885ac013f0d1c5e70ad8620c/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/navidme/.cache/bazel/_bazel_navidme/ae0be448885ac013f0d1c5e70ad8620c/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/navidme/.cache/bazel/_bazel_navidme/ae0be448885ac013f0d1c5e70ad8620c/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/navidme/.cache/bazel/_bazel_navidme/ae0be448885ac013f0d1c5e70ad8620c/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/navidme/.cache/bazel/_bazel_navidme/ae0be448885ac013f0d1c5e70ad8620c/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/navidme/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/navidme/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /home/navidme/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:356:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/navidme/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/navidme/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/navidme/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/navidme/tensorflow/tensorflow/contrib/kfac/python/ops/BUILD:80:1: in py_library rule //tensorflow/contrib/kfac/python/ops:loss_functions: target '//tensorflow/contrib/kfac/python/ops:loss_functions' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/navidme/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/navidme/tensorflow/tensorflow/contrib/BUILD:14:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded).
INFO: Found 1 target...
ERROR: /home/navidme/.cache/bazel/_bazel_navidme/ae0be448885ac013f0d1c5e70ad8620c/external/double_conversion/BUILD.bazel:7:1: C++ compilation of rule '@double_conversion//:double-conversion' failed (Exit 1)
gcc: error trying to exec 'cc1plus': execvp: No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/navidme/tensorflow/tensorflow/python/BUILD:1486:1 C++ compilation of rule '@double_conversion//:double-conversion' failed (Exit 1)
INFO: Elapsed time: 3.624s, Critical Path: 3.17s
INFO: 20 processes: 20 local.
FAILED: Build did NOT complete successfully

`
"
20561,failed to embed python code that import tensorflow in c++ ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: Tesla K40c,  11440 MBytes
- **Exact command to reproduce**: see below

### Describe the problem
I want the embedded python in my C++ code to import tensorflow, but the line ""import tensorflow"" gives an error at run time. That's no happening while importing other python modules. There is no problem with using tensorflow from my python shell/scripts, I can also compile and use custom ops written in C++, so that proofs that the installation worked without problems. I don't know if it's relevant, but my C++ library already uses tensorflow as third part shared library.

Here my very easy C++ code that calls python:

            PyObject* pInt;

            Py_Initialize();

            PyRun_SimpleString(""import tensorflow \n""
                               ""print('Hello TF!!!')"");

            Py_Finalize();


Here is the error I get at runtime:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK6google8protobuf7Message11GetTypeNameEv

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK6google8protobuf7Message11GetTypeNameEv


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

"
20559,gfile.Glob is Case Sensitive for file extensions in Unix but not in Windows,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 and macOS High Sierra 10.13.4
- **TensorFlow installed from (source or binary)**: `pip3 install --upgrade tensorflow`
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6.5
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: In source code

### Describe the problem
gfile.Glob() distinguishes between "".case"" and "".CASE"" in file extension in Unix-based systems, but not in Windows.
Ideally, they should work the same way.

### Source code
Stand in a root folder, add a image.jpg in the folder.
Then start python and run:
```
from tensorflow.python.platform import gfile
print(gfile.Glob(""./*.jpg""))
print(gfile.Glob(""./*.JPG""))
```
In Windows, you will get:
```
["".\\image.jpg""""]
["".\\image.jpg""""]
```
In Unix, you will get:
```
[""./image.jpg""]
[]
```"
20557,"gfile.Glob - Recursive in Windows, not in Unix","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 and macOS High Sierra 10.13.4
- **TensorFlow installed from (source or binary)**: `pip3 install --upgrade tensorflow`
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6.5
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: In source code

### Describe the problem
gfile.Glob() works recursively in Windows, but not in Unix-based systems.
Ideally, they should work the same way.

### Source code
Stand in a root folder, create subfolder and add a image.jpg in the subfolder.
Then start python and run:
```
from tensorflow.python.platform import gfile
print(gfile.Glob(""./*.jpg""))
```
In Windows, you will get `["".\\subfolder\\image.jpg""]`.
In Unix, you will get `[]`."
20555,Tensorflow-mkl does not work on 3D input data,"------------------------

### System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): tensorflow-mkl installed from anaconda
TensorFlow version: 1.8
Python version: Python 3.6
CUDA/cuDNN version: Both 9.2
GPU model and memory: 1080 TI 11 GB
Exact command to reproduce: Any code that takes 3D data (my code is too long to paste in here)

I am training a model to perform volumetric segmentation (3D data). I am training on CPU (two Xeon E5 v4 2699) due to the size of the input data that will not fit in vram. When I train the model, I get anÂ error
""tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'data_format' of ""NDHWC"" is not in the list of allowed values: ""NHWC"", ""NCHW""

However, on Intel's github it says it works on volumetric segmentation (https://github.com/intel/mkl-dnn). How may I resolve this issue so I can train my 3D-Unet withÂ mkl?
"
20553,tensorflow cuda build configure hanging(after NCCL),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 x64
- **TensorFlow installed from (source or binary)**:  source master and source r1.9
- **TensorFlow version (use command below)**: 1.9.0rc0 and 1.9.0rc2
- **Python version**:  3.6.6
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.2/7.1
- **GPU model and memory**: GTX1080Ti 11GB  X 2 
- **Exact command to reproduce**:

checkout to r1.9 or master and ./configure

after configure cuda and cudnn

and configure NCCL ( default 1.3 or 2.2, whatever I choose, same )

the process hangs forever"
20552,tf.keras.estimator.model_to_estimator giving error in TensorFlow >= 1.8.0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8.0 and 1.9.0-rc2
- **Python version**: Python3
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
`estimator = tf.keras.estimator.model_to_estimator(model, model_dir=MODEL_DIR)`

### Describe the problem
When calling tf.keras.estimator.model_to_estimator, I got following error. However, it works fine with TensorFlow == 1.7.0.

```
INFO:tensorflow:Using the Keras model provided.
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_model_dir': './model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f61f0ede908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-2-176d5ff06eb1> in <module>()
     13 
     14 model = get_keras_model()
---> 15 estimator = tf.keras.estimator.model_to_estimator(model, model_dir='./model')

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/estimator.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config)
    502                            estimator,
    503                            custom_objects,
--> 504                            keras_weights)
    505   elif keras_model.built:
    506     logging.warning('You are creating an Estimator from a Keras model '

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/estimator.py in _save_first_checkpoint(keras_model, estimator, custom_objects, keras_weights)
    413           model.set_weights(keras_weights)
    414         # Make update ops and initialize all variables.
--> 415         if not model.train_function:
    416           # pylint: disable=protected-access
    417           model._make_train_function()

AttributeError: 'Sequential' object has no attribute 'train_function'
```

### Source code / logs
Here is the code to reproduce the problem.

```
import tensorflow as tf

def get_keras_model():
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.InputLayer(input_shape=[28*28]))
  model.add(tf.keras.layers.Dense(300, activation='relu'))
  model.add(tf.keras.layers.Dense(100, activation='relu'))
  model.add(tf.keras.layers.Dense(10, activation='softmax'))
  model.compile(loss='categorical_crossentropy',
                optimizer=tf.keras.optimizers.SGD(lr=0.005),
                metrics=['accuracy'])
  return model

model = get_keras_model()
estimator = tf.keras.estimator.model_to_estimator(model, model_dir='./model')
```"
20551,contrib.layers in 1.9.0-rc2 doesn't expose 1-d convolutions anymore,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
  yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version**: 1.9.0-rc2
- **Python version**: Python 3.5.2 (and Python 2.7.13)
- **CUDA/cuDNN version**: 9.0, V9.0.176
- **GPU model and memory**: NVidia Tesla V100
- **Exact command to reproduce**:
```
#!/usr/bin/env python
import tensorflow as tf
import tensorflow.contrib.layers as layers
x = tf.zeros((1,10,1))
y = layers.conv2d(x, kernel_size=1, num_outputs=1)      # Crashes with dimension error,
# layers.convolution is not exposed.  layers.conv1d doesn't exist and layers.convolution1d isn't exposed.
# But the below sleazy trick works:
from tensorflow.contrib.layers.python.layers.layers import convolution
y = convolution(x, kernel_size=1, num_outputs=1) 
```
*From Environment Capture Script:
== cat /etc/issue ===============================================
Linux bn-gpuwkstn01 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
DGX_SWBUILD_VERSION=""3.1.3""
VERSION=""16.04.4 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 6.2.1-7ubuntu1~16.04.york0) 6.2.1 20161215
Copyright (C) 2016 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux bn-gpuwkstn01 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy           1.14.5
protobuf        3.6.0
tensorflow      1.9.0rc2

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.9.0-rc2
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Jul  4 17:05:57 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.111                Driver Version: 384.111                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 |
| N/A   32C    P0    60W / 300W |     28MiB / 16149MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |
| N/A   30C    P0    37W / 300W |     10MiB / 16149MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |
| N/A   30C    P0    35W / 300W |     10MiB / 16149MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |
| N/A   30C    P0    36W / 300W |     10MiB / 16149MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1755      G   /usr/lib/xorg/Xorg                            17MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
"
20550,Training speed much slower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
The code used is adapted from [Keras FastText](https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py)  || [Exact Code](https://github.com/frunkad/sent-aog/blob/master/imdb_fasttext.py)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 18.04 
- **TensorFlow installed from (source or binary)**: Anaconda
- **Bacel Version**: Not applicable
- **TensorFlow version**: v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: Python 3.6.4 :: Anaconda custom (64-bit)
- **CUDA/cuDNN version**: 9.0, V9.0.176 
- **GPU model and memory**: GeForce GTX 980M totalMemory: 7.93GiB
- **Exact command to reproduce**: device specific

From Environment Capture Script:
```
== cat /etc/issue ===============================================
Linux dracarys 4.15.0-23-generic #25-Ubuntu SMP Wed May 23 18:02:16 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""18.04 LTS (Bionic Beaver)""
VERSION_ID=""18.04""
VERSION_CODENAME=bionic

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 7.3.0-16ubuntu3) 7.3.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux dracarys 4.15.0-23-generic #25-Ubuntu SMP Wed May 23 18:02:16 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                              1.14.4     
numpydoc                           0.7.0      
protobuf                           3.5.2.post1
tensorflow-gpu                     1.8.0      

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/home/frunkad/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Thu Jul  5 02:05:17 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 980M    Off  | 00000000:01:00.0  On |                  N/A |
| N/A   48C    P0    31W /  N/A |    588MiB /  8118MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1530      G   /usr/lib/xorg/Xorg                            16MiB |
|    0      1579      G   /usr/bin/gnome-shell                          48MiB |
|    0      1762      G   /usr/lib/xorg/Xorg                           212MiB |
|    0      1907      G   /usr/bin/gnome-shell                          88MiB |
|    0      2394      G   ...-token=8EBF6E3FCA82B57AEE9186BDD7AFEAF3   108MiB |
|    0      2902      G   ...-token=FA087B07103596ED40954ABE2071373B   105MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/lib64/libcudart_static.a
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7

```
### Describe the problem
From the keras code the GTX 980M should have taken 2s for each epoch, but it takes around 58s. Also the same code runs within 10s on colab. The possible reason as per me is that I am using it with Ubuntu 18. Should I give it a try on Windows on same machine?

### Source code / logs

```
/home/frunkad/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Loading data...
25000 train sequences
25000 test sequences
Average train sequence length: 238
Average test sequence length: 230
Adding 2-gram features
Average train sequence length: 476
Average test sequence length: 428
Pad sequences (samples x time)
x_train shape: (25000, 400)
x_test shape: (25000, 400)
Build model...
Train on 25000 samples, validate on 25000 samples
Epoch 1/5
2018-07-05 01:26:20.525149: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-07-05 01:26:20.601376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-07-05 01:26:20.601794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties:
name: GeForce GTX 980M major: 5 minor: 2 memoryClockRate(GHz): 1.1265
pciBusID: 0000:01:00.0
totalMemory: 7.93GiB freeMemory: 7.32GiB
2018-07-05 01:26:20.601813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-07-05 01:26:20.751183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-05 01:26:20.751214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0
2018-07-05 01:26:20.751220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N
2018-07-05 01:26:20.751487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7064 MB memory)-> physical GPU (device: 0, name: GeForce GTX 980M, pci bus id: 0000:01:00.0, compute capability: 5.2)
25000/25000 [==============================] - 58s 2ms/step - loss: 0.6651 - acc: 0.6945 - val_loss: 0.6278 - val_acc: 0.7829
Epoch 2/5
25000/25000 [==============================] - 57s 2ms/step - loss: 0.5572 - acc: 0.8524 - val_loss: 0.5116 - val_acc: 0.8378
Epoch 3/5
25000/25000 [==============================] - 57s 2ms/step - loss: 0.4034 - acc: 0.9102 - val_loss: 0.4015 - val_acc: 0.8692
Epoch 4/5
25000/25000 [==============================] - 58s 2ms/step - loss: 0.2781 - acc: 0.9420 - val_loss: 0.3359 - val_acc: 0.8827
Epoch 5/5
25000/25000 [==============================] - 58s 2ms/step - loss: 0.1966 - acc: 0.9619 - val_loss: 0.2986 - val_acc: 0.8923
``` "
20547,can't import tensorflow,"when i try to import tensorflow, i got this traceback:
################################
Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 903, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#1>"", line 1, in <module>
    import tensorflow
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 903, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems
################################
for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


I'm using Python3.5 and tensorflow 1.8.0
Any help would be very appreciated!
"
20545,Bug in tflite benchmark_model,"```
bazel build -c opt   --config=android_arm   --cxxopt='--std=c++11'   tensorflow/contrib/lite/tools/benchmark:benchmark_model
```

The compilation output is:
```
INFO: Analysed target //tensorflow/contrib/lite/tools/benchmark:benchmark_model (0 packages loaded).
INFO: Found 1 target...
ERROR: /usr/local/google/home/nandiw/Projects/tensorflow/tflite_tools/tensorflow/tensorflow/contrib/lite/profiling/BUILD:44:1: C++ compilation of rule '//tensorflow/contrib/lite/profiling:profile_summarizer' failed (Exit 1): clang failed: error executing command 
  (cd /usr/local/google/home/nandiw/.cache/bazel/_bazel_nandiw/caad1bccb22acfdb24a81d2c3569fb6e/execroot/org_tensorflow && \
  exec env - \
    ANDROID_BUILD_TOOLS_VERSION=28.0.0 \
    ANDROID_NDK_API_LEVEL=16 \
    ANDROID_NDK_HOME=/usr/local/google/home/nandiw/Developments/android-ndk-r16b \
    ANDROID_SDK_API_LEVEL=28 \
    ANDROID_SDK_HOME=/usr/local/google/home/nandiw/Developments/android-sdk \
    PATH=/usr/local/google/home/nandiw/Projects/tensorflow/tflite_tools/venv/bin:/usr/local/google/home/nandiw/Developments/google-cloud-sdk/bin:/usr/lib/google-golang/bin:/usr/local/buildtools/java/jdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/google/home/nandiw/Developments/android-sdk/platform-tools:/usr/local/google/home/nandiw/Developments/nodejs/npm/lib/node_modules/@angular/cli/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/local/google/home/nandiw/Projects/tensorflow/tflite_tools/venv/bin/python \
    PYTHON_LIB_PATH=/usr/local/google/home/nandiw/Projects/tensorflow/tflite_tools/venv/lib/python3.6/site-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang '-D__ANDROID_API__=16' -isystemexternal/androidndk/ndk/sysroot/usr/include/arm-linux-androideabi -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -fno-integrated-as -target armv7-none-linux-androideabi '-march=armv7-a' '-mfloat-abi=softfp' '-mfpu=vfpv3-d16' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/armeabi-v7a-opt/bin/tensorflow/contrib/lite/profiling/_objs/profile_summarizer/tensorflow/contrib/lite/profiling/profile_summarizer.d '-frandom-seed=bazel-out/armeabi-v7a-opt/bin/tensorflow/contrib/lite/profiling/_objs/profile_summarizer/tensorflow/contrib/lite/profiling/profile_summarizer.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/armeabi-v7a-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/armeabi-v7a-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/armeabi-v7a-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/armeabi-v7a-opt/genfiles/external/local_config_sycl -iquote external/gemmlowp -iquote bazel-out/armeabi-v7a-opt/genfiles/external/gemmlowp -iquote external/flatbuffers -iquote bazel-out/armeabi-v7a-opt/genfiles/external/flatbuffers -isystem external/eigen_archive -isystem bazel-out/armeabi-v7a-opt/genfiles/external/eigen_archive -isystem bazel-out/armeabi-v7a-opt/bin/external/eigen_archive -isystem tensorflow/contrib/lite/schema -isystem bazel-out/armeabi-v7a-opt/genfiles/tensorflow/contrib/lite/schema -isystem bazel-out/armeabi-v7a-opt/bin/tensorflow/contrib/lite/schema -isystem external/flatbuffers/include -isystem bazel-out/armeabi-v7a-opt/genfiles/external/flatbuffers/include -isystem bazel-out/armeabi-v7a-opt/bin/external/flatbuffers/include '--std=c++11' -Wall -DFARMHASH_NO_CXX_STRING '-mfpu=neon' '-mfloat-abi=softfp' '-std=c++11' -O3 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '--sysroot=external/androidndk/ndk/platforms/android-16/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/contrib/lite/profiling/profile_summarizer.cc -o bazel-out/armeabi-v7a-opt/bin/tensorflow/contrib/lite/profiling/_objs/profile_summarizer/tensorflow/contrib/lite/profiling/profile_summarizer.o)
tensorflow/contrib/lite/profiling/profile_summarizer.cc:86:27: error: use of undeclared identifier 'string'
    details.name += "":"" + string(profiling_string);
                          ^
1 error generated.
Target //tensorflow/contrib/lite/tools/benchmark:benchmark_model failed to build
```

The problem is in:
https://github.com/tensorflow/tensorflow/blob/b2fe2a874bade4782aaca5c44bf29e7ff6c39200/tensorflow/contrib/lite/profiling/profile_summarizer.cc#L86

`details.name += "":"" + string(profiling_string);` should be `details.name += "":"" + std::string(profiling_string);` 
"
20543,how to run multiple models parallel in different graph?,"hi, all:
  I have a machine with 2 GPUs. How could I train 2 models parallel with 2 sub-processes in seperable graphs please?
  I tried this:

```python
loaded_data = load_data(dir_name)
GPU_NUM = 2
for i in range(GPU_NUM):
    with tf.device('/gpu:{}'.format(i)):
        with tf.Graph().as_default():
            sub_process = multiprocessing.Process(target=func, args=(loaded_data))
```
 
  I called nvidia-smi, but only one GPU is running.
  Any suggestions would be appreciated. 
  Thanks a lot:)"
20532,freeze_graph doesn't work,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu 
- **TensorFlow version (use command below)**: 1.8
- **Python version**:  3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9/7.1
- **GPU model and memory**: dual gtx 1080ti , 11gb
- **Exact command to reproduce**: 

`python freeze_graph.py \
    --input_checkpoint=full-path-to-logs/mnist-9999 \
    --output_graph=model.pb \
    --output_node_names=fc2/Relu`

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Traceback (most recent call last):
  File ""freeze_graph.py"", line 382, in <module>
    run_main()
  File ""freeze_graph.py"", line 379, in run_main
    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)
  File ""/home/dhingratul/.virtualenvs/venv/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""freeze_graph.py"", line 378, in <lambda>
    my_main = lambda unused_args: main(unused_args, flags)
  File ""freeze_graph.py"", line 272, in main
    flags.saved_model_tags, checkpoint_version)
  File ""freeze_graph.py"", line 254, in freeze_graph
    checkpoint_version=checkpoint_version)
  File ""freeze_graph.py"", line 128, in freeze_graph_with_def_protos
    var_list=var_list, write_version=checkpoint_version)
  File ""/home/dhingratul/.virtualenvs/venv/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1338, in __init__
    self.build()
  File ""/home/dhingratul/.virtualenvs/venv/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/home/dhingratul/.virtualenvs/venv/lib/python3.5/site-packages/tensorflow/python/training/saver.py"", line 1372, in _build
    raise ValueError(""No variables to save"")
ValueError: No variables to save

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20531,Problem with callbacks and LSTM : is failing,"I try to use callbacks and it is failing with following example, if I use GRU with callbacks is ok but with LSTM is failing:


This is system info :

== cat /etc/issue ===============================================
Linux gpuMachine 4.13.0-45-generic #50~16.04.1-Ubuntu SMP Wed May 30 11:18:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.4 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux strategy.ca.alcatel-lucent.com 4.13.0-45-generic #50~16.04.1-Ubuntu SMP Wed May 30 11:18:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                             1.14.5   
protobuf                          3.6.0    
tensorflow                        1.8.0    
tensorflow-gpu                    1.8.0    

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-9.0/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue Jul  3 16:05:54 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 107...  Off  | 00000000:02:00.0  On |                  N/A |
|  0%   43C    P2    41W / 180W |   1141MiB /  8118MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     28189      C   ...user1/.conda/envs/tensorflow/bin/python  1007MiB |
|    0     28294      G   /usr/lib/xorg/Xorg                           122MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/lib64/libcudart_static.a
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7



with this example I have problem:
```
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
from keras.callbacks import TensorBoard
import numpy as np
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.1
session = tf.Session(config=config)
path_checkpoint = 'test.keras'
SEQUENCES = 5
TIME_STEPS = 10

model = Sequential()
model.add(Embedding(100, 4))
model.add(LSTM(32))
model.add(Dense(1))
model.compile(optimizer='rmsprop', loss='mse')

tensor_board = TensorBoard(log_dir='log', batch_size=2, write_graph=False,
write_grads=True, histogram_freq=4)
callback_early_stopping = EarlyStopping(monitor='val_loss',
patience=2, verbose=2)
callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,
monitor='val_loss',
verbose=2,
save_weights_only=True,
save_best_only=True)
callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',
factor=0.1,
min_lr=1e-4,
patience=0,
verbose=2)
x_train = np.random.randint(100, size=(SEQUENCES, TIME_STEPS), dtype='int8')

y_train = np.random.rand(SEQUENCES)

model.fit(x_train, y_train, batch_size=2, epochs=4, shuffle=True, callbacks=[callback_reduce_lr,tensor_board,callback_early_stopping,callback_checkpoint])
```
and I got this :+1: 
Error +1
Epoch 1/4

FailedPreconditionError Traceback (most recent call last)
in ()
36 y_train = np.random.rand(SEQUENCES)
37
---> 38 model.fit(x_train, y_train, batch_size=2, epochs=4, shuffle=True, callbacks=[callback_reduce_lr,tensor_board,callback_early_stopping,callback_checkpoint])

~/.conda/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
1040 initial_epoch=initial_epoch,
1041 steps_per_epoch=steps_per_epoch,
-> 1042 validation_steps=validation_steps)
1043
1044 def evaluate(self, x=None, y=None,

~/.conda/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
197 ins_batch[i] = ins_batch[i].toarray()
198
--> 199 outs = f(ins_batch)
200 if not isinstance(outs, list):
201 outs = [outs]

~/.conda/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in call(self, inputs)
2659 return self._legacy_call(inputs)
2660
-> 2661 return self._call(inputs)
2662 else:
2663 if py_any(is_tensor(x) for x in inputs):

~/.conda/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _call(self, inputs)
2629 symbol_vals,
2630 session)
-> 2631 fetched = self._callable_fn(*array_vals)
2632 return fetched[:len(self.outputs)]
2633

~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py in call(self, *args)
1452 else:
1453 return tf_session.TF_DeprecatedSessionRunCallable(
-> 1454 self._session._session, self._handle, args, status, None)
1455
1456 def del(self):

~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in exit(self, type_arg, value_arg, traceback_arg)
517 None, None,
518 compat.as_text(c_api.TF_Message(self.status.status)),
--> 519 c_api.TF_GetCode(self.status.status))
520 # Delete the underlying status object from memory otherwise it stays alive
521 # as there is a reference to status from this from the traceback due to

FailedPreconditionError: Attempting to use uninitialized value RMSprop_16/lr
[[Node: RMSprop_16/lr/read = IdentityT=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]]
[[Node: loss_16/mul/_621 = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1571_loss_16/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]
"
20529,[tf.keras] Stateful Metrics assorted errors.,"I will break this issue down into several code snippets each displaying a different error. @fchollet. In total 3 issues. All of these issues are only relevant to ``tf.keras`` implementation. The ``keras`` implementation works as intended.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary.
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

### Problem 1
Issues with multi-input/multi-output and batch averaging. This happens for both train and validation metrics.


### Source code/logs

```python
import tensorflow as tf

from tensorflow.python.keras.datasets import mnist
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, UpSampling2D


class BatchCounter(tf.keras.layers.Layer):

        def __init__(self, name='batch_counter', **kwargs):
            super(BatchCounter, self).__init__(name=name, **kwargs)
            self.stateful = True
            self.batches = tf.keras.backend.variable(value=0, dtype='int32')

        def reset_states(self):
            tf.keras.backend.set_value(self.batches, 0)

        def __call__(self, y_true, y_pred):
            updates = [tf.keras.backend.update_add(self.batches, tf.keras.backend.variable(value=1, dtype='int32'))]
            self.add_update(updates)
            return self.batches


batch_size = 100
num_classes = 10
epochs = 1

# input image dimensions
img_rows, img_cols = 28, 28

# Data
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1).astype('float32') / 255
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1).astype('float32') / 255
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

# Convolutional Encoder
input_img = Input(shape=(img_rows, img_cols, 1))
conv_1 = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)
pool_1 = MaxPooling2D((2, 2), padding='same')(conv_1)
conv_2 = Conv2D(8, (3, 3), activation='relu', padding='same')(pool_1)
pool_2 = MaxPooling2D((2, 2), padding='same')(conv_2)
conv_3 = Conv2D(8, (3, 3), activation='relu', padding='same')(pool_2)
encoded= MaxPooling2D((2, 2), padding='same')(conv_3)

# Classification
flatten = Flatten()(encoded)
fc = Dense(128, activation='relu')(flatten)
softmax = Dense(num_classes, activation='softmax', name='classification')(fc)

# Decoder
conv_4 = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)
up_1 = UpSampling2D((2, 2))(conv_4)
conv_5 = Conv2D(8, (3, 3), activation='relu', padding='same')(up_1)
up_2 = UpSampling2D((2, 2))(conv_5)
conv_6 = Conv2D(16, (3, 3), activation='relu')(up_2)
up_3 = UpSampling2D((2, 2))(conv_6)
decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same', name='autoencoder')(up_3)

model = Model(inputs=input_img, outputs=[softmax, decoded])

model.compile(loss={'classification': 'categorical_crossentropy',
                    'autoencoder': 'binary_crossentropy'},
              loss_weights={'classification': 1.0,
                            'autoencoder': 0.5},
              optimizer='adam',
              metrics={'classification': 'accuracy', 'autoencoder': BatchCounter()})

history = model.fit(x_train,
          {'classification': y_train, 'autoencoder': x_train},
          batch_size=batch_size,
          epochs=epochs,
          validation_data= (x_test, {'classification': y_test, 'autoencoder': x_test}),
          verbose=1)
```

```
Epoch 1/1
60000/60000 [==============================] - 41s 677us/step - loss: 0.5086 - classification_loss: 0.4051 - autoencoder_loss: 0.2069 - classification_acc: 0.8755 - autoencoder_batch_counter: 299.7983 - val_loss: 0.2001 - val_classification_loss: 0.1242 - val_autoencoder_loss: 0.1518 - val_classification_acc: 0.9596 - val_autoencoder_batch_counter: 50.1000
```

``autoencoder_batch_counter`` & ``val_autoencoder_batch_counter`` should always be (600, 100) respectively.  These metrics are batch averaged. This does not happen in the Keras implementation."
20528,[Feature Request] random_poisson GPU Kernel,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.8
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: GTX 1050 4GB
- **Exact command to reproduce**: random_poisson

### Describe the problem
[`random_poisson` currently only has a CPU kernel and not a GPU kernel.](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/random_poisson_op.cc#L322) [See also this Stack Overflow post for a specific manifestation of the issue.](https://stackoverflow.com/questions/50858572/tensorflows-random-poisson-only-runs-on-cpu/51159962)
"
20527,"tf.layers.conv3d with ""channels_first"" does not accept dynamic shapes","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
v1.8.0-0-g93bc2e2072
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
cuDNN 7.0 CUDA 9.1
- **GPU model and memory**:
Geforce 1080 Ti 12Gb
- **Exact command to reproduce**:
See given code snippets


### Describe the problem
Calling tf.layers.conv3d with ""channels_first"" on a tf.placeholder with dynamic shapes produces an error, whereas it does not for ""channels_last"" (see source code below). Cause is usage of reshape operation [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py#L203-L207) as outlined in a similar [issue](https://github.com/tensorflow/tensorflow/issues/15655). Unfortunately, no general solution supporting dynamic shapes was found in aforementioned issue, and shape inference does not allow for >1 dynamic shape.

### Source code / logs
this produces error:
```
import tensorflow as tf
x = tf.placeholder(dtype=tf.float32, shape=[None, 1, None, None, None])
y = tf.layers.conv3d(x, 32, 9, data_format='channels_first')
```

this works fine:
```
import tensorflow as tf
x = tf.placeholder(dtype=tf.float32, shape=[None, None, None, None, 1])
y = tf.layers.conv3d(x, 32, 9, data_format='channels_last')
```
"
20524,Explanation for why tf.gradients() no longer propagates through integer tensors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX 10.13.4
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0-dev20180628 (nightly)
- **Python version**: 2.7.15
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
For performance reasons, we sometimes work with integer tensors like `tf.uint8`.  We have custom ops that operate on these tensors, and in our unit tests we verify the correctness of the gradients using `tf.gradients()`.  However, it seems as of version 1.9, the behavior of `tf.gradients()` has been changed so that it no longer backpropagates through integer tensors.

This was causing `tf.gradients()` to always return `None`.

This seems like a significant change that introduces inconsistent behavior (or at least special cases that need to be accounted for).  I assume there's a good rationale, but could additional documentation be added to explain why this change was necessary (or deemed desirable)?  The only documntation I'm currently aware of is this line from the release notes: 

> Prevent `tf.gradients()` from backpropagating through integer tensors.

And the following comment in `gradients_impl.py`:

> All integer tensors are considered constant with respect to all `xs`, as if they were included in `stop_gradients`.

Thanks.

### Source code / logs

        with self.test_session(config=self.config) as session:
            t = tf.ones(5, dtype=tf.float32) * 2
            t2 = t**2

            grad_ys = tf.ones(5)
            grad = tf.gradients(t2, t, grad_ys)[0]
            grad_out = session.run(grad)
            print(""grad_out: "", grad_out)

Prints `[4. 4. 4. 4. 4.]` as expected

        with self.test_session(config=self.config) as session:
            t = tf.ones(5, dtype=tf.int32) * 2
            t2 = t**2

            grad_ys = tf.ones(5)
            grad = tf.gradients(t2, t, grad_ys)[0]
            grad_out = session.run(grad)
            print(""grad_out: "", grad_out)

Raises `TypeError: Fetch argument None has invalid type <type 'NoneType'>` "
20523,ImportError: cannot import name gen_collective_ops,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:centos-release-7-3.1611.el7.centos.x86_64
- **TensorFlow installed from (source or binary)**:Source
- **TensorFlow version (use command below)**:1.7
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**:0.11.0
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**:No (CPU only)
- **GPU model and memory**: No
- **Exact command to reproduce**: 


### Describe the problem
I am working on distributed tensorflow using horovod in CPU cluster mode.
Below is the run command used:
mpirun -x LD_LIBRARY_PATH -x OMP_NUM_THREADS -cpus-per-proc 20 --map-by socket --overscribe --report-bindings  -n 2 python  $python_script      --mkl=True --forward_only=False --num_batches=200 --kmp_blocktime=0 --num_warmup_batches=50 --num_inter_threads=$inter_op --distortions=False --optimizer=sgd --batch_size=$batch_size --num_intra_threads=$intra_op --data_format=NCHW --model=$MODEL --variable_update horovod --horovod_device cpu --data_dir=XXX


Error Stack:
 Traceback (most recent call last):
  File ""/tensorflow/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py"", line 27, in <module>
    import benchmark_cnn
  File ""/tensorflow/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py"", line 53, in <module>
    import variable_mgr
  File ""/tensorflow/benchmarks/scripts/tf_cnn_benchmarks/variable_mgr.py"", line 25, in <module>
    import allreduce
  File ""/tensorflow/benchmarks/scripts/tf_cnn_benchmarks/allreduce.py"", line 28, in <module>
    from tensorflow.python.ops import collective_ops
  File ""/.local/lib/python2.7/site-packages/tensorflow/python/ops/collective_ops.py"", line 21, in <module>
    from tensorflow.python.ops import gen_collective_ops
ImportError: cannot import name gen_collective_ops


"
20522,Creating variables folder for hosting model on server,"I'm using [this](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/image_retraining/retrain.py) image_retraining script for image classification and I got a .pb file. For hosting this model on a server, I need to have a `variables` folder as mentioned [here](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/serving_basic.md). Can I generate `variables` folder from .pb file or I need to modify [retrain script](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/image_retraining/retrain.py) and do training again? There's some discussion [here](https://github.com/tensorflow/serving/issues/467)."
20521,prefetch_to_device not working with SparseTensor when fetching to GPU,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Linux Ubuntu 14.04
- **TensorFlow installed from**: source
- **TensorFlow version**: 1.9
- **Python version**: 2.7
- **Bazel version**: 0.12.0
- **GCC/Compiler version**: 4.8.4
- **CUDA/cuDNN version**: 9.1
- **GPU model and memory**: GTX 1080, 8114MiB
- **Exact command to reproduce**:
(Any sparse tensor being prefetched to GPU will work here, this is just one short example)
```
import tensorflow as tf

# Set up simple sparse tensor
ds = tf.data.Dataset.from_tensors(tf.contrib.layers.dense_to_sparse([1], 0))

# prefetch to GPU
gpu_ds = ds.apply(tf.contrib.data.prefetch_to_device('/gpu:0'))

# Get a value
gpu_iter = gpu_ds.make_one_shot_iterator()
val = gpu_iter.get_next()

# Barf
with tf.Session() as sess:
    sess.run(val)
```
### Describe the problem
Bug resulting from prefetch_to_device with the following conditions: 
* device is GPU
* dataset element contains a sparse tensor

Iterator functions appropriately, only when you try to _use_ the data do you run into the error. (see log for error)

Also, as a side note which may or may not be relevant, I've found that even a single sparse tensor in an element will spoil any other nice, kind, and dense tensors in the pack. So, if, in the previous example, I were to do the following, I would still run into an error:
```
import tensorflow as tf
# Make a sparse tensor
ds = tf.data.Dataset.from_tensors(tf.contrib.layers.dense_to_sparse([1], 0))

# Add in a non-sparse component
ds = ds.map(lambda x: (x,1))

# Prefetch to device
gpu_ds = ds.apply(tf.contrib.data.prefetch_to_device('/gpu:0'))

# Pull out vals
gpu_iter = gpu_ds.make_one_shot_iterator()
val_sparse, val_normal = gpu_iter.get_next()

# Uh oh!
with tf.Session() as sess:
    sess.run(val_normal)
```

### Log
Traceback (most recent call last):
  File ""sparse_bunk.py"", line 17, in <module>
    sess.run(val_normal)
  File ""/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: No unary variant device copy function found for direction: 1 and Variant type_name: tensorflow::Tensor
     [[Node: FunctionBufferingResourceGetNext = FunctionBufferingResourceGetNext[output_types=[DT_VARIANT, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FunctionBufferingResource)]]

Thank you"
20519,[Feature Request] Graph Transform Tool support SavedModel as Input,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:n/a
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

### Describe the problem
Currently [Graph Transform Tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md) only support GraphDef as input. People normally save model using SavedModel or ckpt file. Maybe it would be better to support SavedModel as input.
"
20518,how to use tensorboard debugger with tf.slim?,"I am using tf.slim to train a network, I hope to debug the net and check out the inner variables with tensorboard debugger. how can I do that? the session  wraped by slim API."
20517,Python 3.7 compatibility,"I'm sure developers are working hard to catch up with Python 3.7.
Is there any timeline?

pip3 install tensorflow - apparently does not work, building from source:

OS Platform and Distribution: Mac OS X 10.13.5
Python: Python 3.7.0 (Homebrew)
TensorFlow installed from: source (https://github.com/tensorflow/tensorflow.git)
TensorFlow version: TensorFlow 1.9.0-rc2
Bazel version:
```
Build label: 0.15.0-homebrew
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jun 26 12:42:27 2018 (1530016947)
Build timestamp: 1530016947
Build timestamp as int: 1530016947
```
CUDA/cuDNN version: None
GPU model and memory: None
Exact command to reproduce:
`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`
```
Starting local Bazel server and connecting to it...
...........................
WARNING: /private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:356:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/kfac/python/ops/BUILD:80:1: in py_library rule //tensorflow/contrib/kfac/python/ops:loss_functions: target '//tensorflow/contrib/kfac/python/ops:loss_functions' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/BUILD:14:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (303 packages loaded).
INFO: Found 1 target...
INFO: From Linking external/grpc/libgrpc_base_c.a [for host]:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(endpoint_pair_uv.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(endpoint_pair_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(ev_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(fork_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(gethostname_fallback.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(gethostname_host_name_max.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(iocp_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(iomgr_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(pollset_set_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(pollset_uv.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(pollset_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(resolve_address_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(socket_utils_linux.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(socket_utils_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(socket_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_client_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_server_utils_posix_noifaddrs.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_server_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_uv.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(timer_uv.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(unix_sockets_posix_noop.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(wakeup_fd_eventfd.o) has no symbols
INFO: From Linking external/grpc/libalts_util.a [for host]:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libalts_util.a(check_gcp_environment_linux.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libalts_util.a(check_gcp_environment_windows.o) has no symbols
INFO: From Linking external/grpc/libtsi.a [for host]:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libtsi.a(ssl_session_openssl.o) has no symbols
INFO: From Linking external/grpc/libgrpc++_base.a [for host]:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc++_base.a(rpc_method.o) has no symbols
INFO: From Linking external/grpc/libgpr_base.a [for host]:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(cpu_iphone.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(cpu_linux.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(cpu_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(env_linux.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(env_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(log_android.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(log_linux.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(log_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(string_util_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(string_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(sync_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(time_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(tls_pthread.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(tmpfile_msys.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(tmpfile_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(wrap_memcpy.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(thd_windows.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(stap_timers.o) has no symbols
INFO: From Linking external/grpc/third_party/address_sorting/libaddress_sorting.a [for host]:
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/third_party/address_sorting/libaddress_sorting.a(address_sorting_windows.o) has no symbols
ERROR: /Users/zardoz/Projects/tensorflow/tensorflow/python/BUILD:5315:1: Executing genrule //tensorflow/python:framework/fast_tensor_util.pyx_cython_translation failed (Exit 1)
Traceback (most recent call last):
  File ""/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/execroot/org_tensorflow/bazel-out/host/bin/external/cython/cython_binary.runfiles/cython/cython.py"", line 17, in <module>
    main(command_line = 1)
  File ""/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/Main.py"", line 720, in main
    result = compile(sources, options)
  File ""/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/Main.py"", line 695, in compile
    return compile_multiple(source, options)
  File ""/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/Main.py"", line 666, in compile_multiple
    context = options.create_context()
  File ""/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/Main.py"", line 590, in create_context
    self.cplus, self.language_level, options=self)
  File ""/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/Main.py"", line 75, in __init__
    from . import Builtin, CythonScope
  File ""/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/CythonScope.py"", line 5, in <module>
    from .UtilityCode import CythonUtilityCode
  File ""/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/UtilityCode.py"", line 3, in <module>
    from .TreeFragment import parse_from_strings, StringParseContext
  File ""/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/TreeFragment.py"", line 17, in <module>
    from .Visitor import VisitorTransform
  File ""/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/Visitor.py"", line 15, in <module>
    from . import ExprNodes
  File ""/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/ExprNodes.py"", line 2875
    await = None
          ^
SyntaxError: invalid syntax
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 179.318s, Critical Path: 6.38s
INFO: 413 processes: 413 local.
FAILED: Build did NOT complete successfully
```"
20516,Cannot restore variables with Checkpoint because keys do not align,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: tf-nightly==1.10.0.dev20180609
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**:

I get the error that is thrown here:
https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/checkpointable/util.py#L633

I cannot provide code to reproduce it. But basically what happens is I have a class that inherits from `Checkpointable`. It assigns all variables to itself to make them checkpointable. It also assigns an optimizer to itself. I then save the model and restore it. When calling `.assert_consumed()` on the `load_status` object of `restore(dir, session)` it throws an error because some key does not match. The variable it tries to restore is actually in the saved checkpoint it just has a different key then the one it gets from `enumerate(self._checkpoint.object_graph_proto.nodes)`.

This describes it as good as I can. Sorry for not being able to share the code. I tried to reproduce it but so far I cannot. I believe it is a bug, because I call `ckpt.save()` and immediately after it `ckpt.restore()` and I get the exception.

Output of `self._checkpoint.object_by_proto_id.keys()` at line 631.
You can see that some keys are missing (idk why) but theses are the ones I need to restore.
```
[0, 1, 2, 3, 4, 5, 6, 7, 14, 19, 10, 17, 22, 11, 18, 23, 12, 13]
```

Output of `util._serialize_object_graph(self.checkpoint, None)` after `restore` before `assert_consumed`
```json
nodes {
  children {
    node_id: 1
    local_name: ""model""
  }
  children {
    node_id: 2
    local_name: ""save_counter""
  }
}
nodes {
  children {
    node_id: 3
    local_name: ""_global_step_pretrain""
  }
  children {
    node_id: 4
    local_name: ""embedding""
  }
  children {
    node_id: 5
    local_name: ""cell""
  }
  children {
    node_id: 6
    local_name: ""dense""
  }
  children {
    node_id: 7
    local_name: ""_optimizer""
  }
  children {
    local_name: ""_checkpoint""
  }
}
nodes {
  attributes {
    name: ""VARIABLE_VALUE""
    full_name: ""initialize_or_restore/save_counter""
    checkpoint_key: ""save_counter/.ATTRIBUTES/VARIABLE_VALUE""
  }
}
nodes {
  attributes {
    name: ""VARIABLE_VALUE""
    full_name: ""ConditionedLSTMGenerator2/CONDITIONED_LSTM/pretrain/global_step""
    checkpoint_key: ""model/_global_step_pretrain/.ATTRIBUTES/VARIABLE_VALUE""
  }
}
nodes {
  attributes {
    name: ""VARIABLE_VALUE""
    full_name: ""ConditionedLSTMGenerator2/CONDITIONED_LSTM/embedding""
    checkpoint_key: ""model/embedding/.ATTRIBUTES/VARIABLE_VALUE""
  }
}
nodes {
  children {
    node_id: 8
    local_name: ""kernel""
  }
  children {
    node_id: 9
    local_name: ""bias""
  }
  attributes {
    name: ""OBJECT_CONFIG_JSON""
    checkpoint_key: ""model/cell/.ATTRIBUTES/OBJECT_CONFIG_JSON""
  }
}
nodes {
  children {
    node_id: 10
    local_name: ""kernel""
  }
  children {
    node_id: 11
    local_name: ""bias""
  }
  attributes {
    name: ""OBJECT_CONFIG_JSON""
    checkpoint_key: ""model/dense/.ATTRIBUTES/OBJECT_CONFIG_JSON""
  }
}
nodes {
  children {
    node_id: 12
    local_name: ""beta1_power""
  }
  children {
    node_id: 13
    local_name: ""beta2_power""
  }
  slot_variables {
    original_variable_node_id: 4
    slot_name: ""m""
    slot_variable_node_id: 14
  }
  slot_variables {
    original_variable_node_id: 8
    slot_name: ""m""
    slot_variable_node_id: 15
  }
  slot_variables {
    original_variable_node_id: 9
    slot_name: ""m""
    slot_variable_node_id: 16
  }
  slot_variables {
    original_variable_node_id: 10
    slot_name: ""m""
    slot_variable_node_id: 17
  }
  slot_variables {
    original_variable_node_id: 11
    slot_name: ""m""
    slot_variable_node_id: 18
  }
  slot_variables {
    original_variable_node_id: 4
    slot_name: ""v""
    slot_variable_node_id: 19
  }
  slot_variables {
    original_variable_node_id: 8
    slot_name: ""v""
    slot_variable_node_id: 20
  }
  slot_variables {
    original_variable_node_id: 9
    slot_name: ""v""
    slot_variable_node_id: 21
  }
  slot_variables {
    original_variable_node_id: 10
    slot_name: ""v""
    slot_variable_node_id: 22
  }
  slot_variables {
    original_variable_node_id: 11
    slot_name: ""v""
    slot_variable_node_id: 23
  }
}
```
"
20515, TFLite Demo mobilenet_ssd ,"How are mobilenet_ssd.tflite and [detect.tflite](https://storage.googleapis.com/download.tensorflow.org/models/tflite/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip)  converted  in the newest [TFLite Demo ](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android)?

Previously, I converted that model by following [this instructions](https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-377652630),  but now,  I see that there are 4 outputs in TFLiteObjectDetectionAPIModel in the source code [TFLiteObjectDetectionAPIModel.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java), 

`    d.tfLite.setNumThreads(NUM_THREADS);
    d.outputLocations = new float[1][NUM_DETECTIONS][4];
    d.outputClasses = new float[1][NUM_DETECTIONS];
    d.outputScores = new float[1][NUM_DETECTIONS];
    d.numDetections = new float[1];`

the previously converted model can't be used.

I tried the following instructions, but the error occurred.

`bazel run --config=opt \
  //tensorflow/contrib/lite/toco:toco -- \
  --input_file=${MODEL_PATH}/frozen_inference_graph_stripped.pb \
  --output_file=${MODEL_PATH}/ssd_mobilenet_v1_float.tflite \
  --input_format=TENSORFLOW_GRAPHDEF \
  --output_format=TFLITE \
  --inference_type=FLOAT \
  --input_shapes=1,300,300,3 \
  --input_arrays=Preprocessor/sub \
  --output_arrays=detection_boxes,detection_scores,detection_classes,num_detections \
  --dump_graphviz=${MODEL_PATH} `

.......
2018-07-03 19:35:14.798049: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: TensorArrayGatherV3
2018-07-03 19:35:14.798066: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: TensorArrayGatherV3
2018-07-03 19:35:14.798080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: TensorArrayGatherV3
2018-07-03 19:35:15.248275: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 4096 operators, 6814 arrays (0 quantized)
2018-07-03 19:35:15.563598: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 4053 operators, 6719 arrays (0 quantized)
2018-07-03 19:35:15.977287: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 4053 operators, 6719 arrays (0 quantized)
2018-07-03 19:35:16.225122: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:95] Check failed: other_op->type == OperatorType::kMerge 
Aborted (core dumped)

Who can help ?

Thanks!
"
20514,No OpKernel was registered to support Op 'QuantizedReluX' with these attrs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary using conda
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.5.2
- **CUDA/cuDNN version**: 8.0/7.0
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **GPU model and memory**: Quadro M2000M/ 4GB
- **Exact command to reproduce**:
Tried to use quantized_relu_x and it doesn't work
I have already used quantized_conv2d and quantized_max_pool, both of them works fine, but have only problem when I use quantized_relu_x

### Describe the problem
Other two quantized kernels QuantizedRelu and QuantizedRelu6 seem to have defined but QuantizedReluX appears to be missing, although three of them are registered together in core/ops/nn_ops.cc file

### Source code / logs
Error message that I am getting is as follows:
InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'QuantizedReluX' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
  >no registered kernels
"
20511,How to use pre-train cnn model with tensorflow-go,"Hi all,

I had train a cnn model with python, it has 57 inputs, one hidden layer with 12nodes, 3 output, and save the model as follow:

```py
        builder = tf.saved_model.builder.SavedModelBuilder('shield')
        builder.add_meta_graph_and_variables(self.sess, ['login3'])
        builder.save()
```

then load it:

```go
	model, err := tf.LoadSavedModel(""shield"", []string{""login3""}, nil)
```

but I got error like:

```sh
2018-07-03 18:23:14.476427: I tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { login3 }; from: shield
2018-07-03 18:23:14.481530: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA
2018-07-03 18:23:14.486676: I tensorflow/cc/saved_model/loader.cc:161] Restoring SavedModel bundle.
2018-07-03 18:23:14.500925: I tensorflow/cc/saved_model/loader.cc:196] Running LegacyInitOp on SavedModel bundle.
2018-07-03 18:23:14.501650: I tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { login3 }; Status: success. Took 25441 microseconds.
2 In[0] is not a matrix
	 [[Node: MatMul = MatMul[T=DT_FLOAT, _output_shapes=[[?,12]], transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_input_0_0, Variable/read)]]
```

How to make it work?

here is my all code:

```py
import os
import sys
import tensorflow as tf
import pandas as pd
import numpy as np

class NeturalNetwork:

    def __init__(self, model_path, input_nodes, hidden_layer_nodes, output_nodes, learning_rate):
        self.input_nodes = input_nodes
        self.hidden_layer_nodes = hidden_layer_nodes
        self.output_nodes = output_nodes
        self.learning_rate = learning_rate
        self.model_path = model_path
        self.x_data = tf.placeholder(shape=[None, input_nodes], dtype=tf.float32, name=""input"")
        self.y_target = tf.placeholder(shape=[None, output_nodes], dtype=tf.float32, name=""target"")

        l1 = add_layer(self.x_data, input_nodes, self.hidden_layer_nodes, activation_function=tf.nn.relu)
        self.prediction = add_layer(l1, self.hidden_layer_nodes, output_nodes, activation_function=tf.nn.softmax)

        y_clipped = tf.clip_by_value(self.prediction, 1e-10, 1)
        self.loss = -tf.reduce_mean(tf.reduce_sum(self.y_target * tf.log(y_clipped), axis=1))

        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)
    
        # GO
        infer = tf.argmax(self.prediction, 1, name=""infer"")
        correct_prediction = tf.equal(infer, tf.argmax(self.y_target, 1))
        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

        self.saver = tf.train.Saver()

        self.sess = tf.Session()
        init = tf.global_variables_initializer()
        self.sess.run(init)

        meta_path = model_path + '/shield.nn.meta'
        if os.path.exists(meta_path):
            nn_path = model_path + '/shield.nn'
            self.saver.restore(self.sess, nn_path)
        else:
            print 'ERROR: no model or restore model fail'

    def Train(self, epoch, filenames):
        interval = 5

        x, y = load_dataset(filenames, 'ShieldResult')
        x_values, y_values = shuffle(x, y)

        x_train = x_values
        y_train = y_values

        test_size  = 10000
        x_test = x_values[-test_size:]
        y_test = y_values[-test_size:]

        print('Training the model...', len(x_train))
        for i in range(1, (epoch + 1)):
            self.sess.run(self.optimizer, feed_dict={self.x_data: x_train, self.y_target: y_train})
            if i % interval == 0:
                print('Epoch', i, '|', 'Loss:', self.sess.run(self.loss, feed_dict={self.x_data: x_train, self.y_target: y_train}))

        print(self.sess.run(self.accuracy, feed_dict={self.x_data: x_test, self.y_target: y_test}))

        builder = tf.saved_model.builder.SavedModelBuilder('shield')
        builder.add_meta_graph_and_variables(self.sess, ['login3'])
        builder.save()

        self.saver.save(self.sess, self.model_path + '/shield.nn')

    def Predict(self, filenames):
        x, y = load_dataset(filenames, 'ShieldResult')
        x_values, y_values = shuffle(x, y)

        test_size  = 10000
        x_test = x_values[-test_size:]
        y_test = y_values[-test_size:]

        yes = 0 
        for i in range(len(x_test)):
            pred = np.rint(self.sess.run(self.prediction, feed_dict={self.x_data: [x_test[i]]}))
            if (y_test[i] == pred[0]).all():
                print('=== Actual:', y_test[i], 'Predicted:', pred[0])
                yes = yes + 1
            else:
                print('!!! Actual:', y_test[i], 'Predicted:', pred[0])

        print(yes)
        print(yes / (len(x_test) * 1.0))

def random_seed():
    seed = 1234
    np.random.seed(seed)
    tf.set_random_seed(seed)

def load_dataset(filenames, resultField):
    dataset = pd.read_csv(filenames[0])
    dataset = pd.get_dummies(dataset, columns=[resultField]) # One Hot Encoding
    values = list(dataset.columns.values)

    for i in range(len(filenames)):
        if i == 0:
            continue

        tmpdataset = pd.read_csv(filenames[i])
        tmpdataset = pd.get_dummies(tmpdataset, columns=[resultField]) # One Hot Encoding
        dataset = dataset.append(tmpdataset)

    y = dataset[values[-3:]]
    x = dataset[values[0:-3]]

    return np.array(x, dtype='float32'), np.array(y, dtype='float32')

def shuffle(x, y):
    indices = np.random.choice(len(x), len(x), replace=False)
    
    return x[indices], y[indices]

def add_layer(inputs, in_size, out_size, activation_function=None):
    weights = tf.Variable(tf.random_normal(shape = [in_size, out_size]))
    biases = tf.Variable(tf.random_normal(shape = [out_size]))

    w_plus_b = tf.matmul(inputs, weights) + biases

    if activation_function is None:
        outputs = w_plus_b
    else:
        outputs = activation_function(w_plus_b)

    return outputs

if __name__ == ""__main__"":
    epoch = int(sys.argv[1])
    node = int(sys.argv[2])
    rate = float(sys.argv[3])
    csv_num = sys.argv[4]

    print(epoch)
    print(node)
    print(rate)
    print(csv_num)

    random_seed()

    ann = NeturalNetwork('./shield-ai-engine', 57, node, 3, rate)
    csvs = [
        '../data/deal/11_' + csv_num + '.csv', 
        '../data/deal/12_' + csv_num + '.csv',
    ]

    ann.Train(epoch, csvs)

```


```go
package main

import (
	""fmt""

	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
)

func main() {
	model, err := tf.LoadSavedModel(""shield"", []string{""login3""}, nil)
	if err != nil {
		fmt.Println(1, err)
		return
	}

	defer model.Session.Close()

	output := tf.Output{
		Op:    model.Graph.Operation(""input""),
		Index: 0,
	}

	tensor, err := tf.NewTensor([]float32{
		12, 41, 41, 41, 41, 41, 70, 137, 259, 305, 344, 81, 148, 271, 370, 397, 81, 148, 271, 370, 397, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 122, 233, 295, 391, 1, 58, 195, 1, 6, 4, 195, 1, 6, 4, 165, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
	})

	target, err := tf.NewTensor([]float32{})

	if err != nil {
		fmt.Println(3, err)
	}

	feeds := map[tf.Output]*tf.Tensor{
		output: tensor,
		model.Graph.Operation(""target"").Output(0): target,
	}

	fetches := []tf.Output{
		{
			Op:    model.Graph.Operation(""infer""),
			Index: 0,
		},
	}

	result, err := model.Session.Run(
		feeds,
		fetches,
		nil,
	)

	if err != nil {
		fmt.Println(2, err)
		return
	}

	fmt.Println(result[0].Value())
}

```"
20510,Memory Leak,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7
- **TensorFlow installed from (source or binary)**:
Anaconda
- **TensorFlow version (use command below)**:
b'unknown' 1.8.0
CuDNN - None
GPU-Integrated
### Describe the problem
Memory Leak when running my code 

### Source code / logs
`import numpy as np`
`import os`
`import tensorflow as tf`
`tf.enable_eager_execution()`
`import tensorflow.contrib.eager as tfe`
`import csv as csv`

`import matplotlib.pyplot as plt`
`from tensorflow.contrib.opt.python.training.elastic_average_optimizer import *`
`import keras as keras`



`def parse_csv(line):`
`  example_defaults = [[0.], [0.], [0.], [0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.], [0]] # sets field types`
  
`  parsed_line = tf.decode_csv(line, example_defaults)`
  
`  # First 4 fields are features, combine into single tensor`
`  features = tf.reshape(parsed_line[:-1], shape=(50,))`
`  # Last field is the label`
`  label = tf.reshape(parsed_line[-1], shape=())`
`  return features, label`
   
`train_dataset = tf.data.TextLineDataset(fileid + ""train"")`
`train_dataset = train_dataset.map(parse_csv)`
`train_dataset = train_dataset.shuffle(buffer_size=1000)`
`train_dataset = train_dataset.batch(16)`

`model = tf.keras.Sequential([`
 ` tf.keras.layers.Dense(100, input_shape=(50,)),  # input shape required`
 ` tf.keras.layers.Dense(2000),`
`  tf.keras.layers.Dense(2000),`
`  tf.keras.layers.Dense(200,activation = 'sigmoid'),`
`  tf.keras.layers.Dense(200),`
 ` tf.keras.layers.Dense(2000),`
  `tf.keras.layers.Dense(200),`
 ` tf.keras.layers.Dense(2)`
  
`])`
`print (model.summary)`


`def loss(model, x, y):`
`  y_ = model(x)`
`  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)`

`def grad(model, inputs, targets):`

`    with tf.GradientTape() as tape:`
    
 `       loss_value = loss(model, inputs, targets)`
  
 `   return tape.gradient(loss_value, model.variables)    `

`optimizer = tf.train.AdamOptimizer(learning_rate = 0.0010)`
`train_loss_results = []`
`train_accuracy_results = []`
`test_loss_results = []`
`test_accuracy_results = []`
`num_epochs = 2000`

`for epoch in range(num_epochs):`
`  epoch_loss_avg = tfe.metrics.Mean()`
`  epoch_accuracy = tfe.metrics.Accuracy()`
  

`  train_dataset = train_dataset.shuffle(16)`
` #  Training loop - using batches of 32`
` for x, y in train_dataset:`
`  #   Optimize the model`
  `  grads = grad(model, x, y)`
  `  optimizer.apply_gradients(zip(grads, model.variables),`
           `                   global_step=tf.train.get_or_create_global_step())`

`  #   Track progress`
`    epoch_loss_avg(loss(model, x, y))  # add current batch loss`
`     #compare predicted label to actual label`
`    epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y)`
 
`  # end epoch`
`  train_loss_results.append(epoch_loss_avg.result())`
`  train_accuracy_results.append(epoch_accuracy.result())`
  
` if epoch % 1 == 0:
    print(""Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}"".format(epoch,
                                                                epoch_loss_avg.result(),
                                                                epoch_accuracy.result()))`
  
`  test_dataset = tf.data.TextLineDataset(fileid + ""test"")
  test_dataset = test_dataset.map(parse_csv)
  test_dataset = test_dataset.shuffle(buffer_size=1000)
  test_dataset = test_dataset.batch(32)`
  
 ` test_accuracy = tfe.metrics.Accuracy()`
  
 ` for (x, y) in test_dataset:
      prediction = tf.argmax(model(x), axis=1, output_type=tf.int32)
      test_accuracy(prediction, y)`
      
`  print(""Test set accuracy: {:.3%}"".format(test_accuracy.result()))
  train_dataset = tf.data.TextLineDataset(fileid + ""train"")
  train_dataset = train_dataset.map(parse_csv)
  train_dataset = train_dataset.batch(32)`
  


`del optimizer`

`test_dataset = tf.data.TextLineDataset(fileid + ""test"")`
`test_dataset = test_dataset.map(parse_csv)`
`test_dataset = test_dataset.shuffle(buffer_size=1000)`
`test_dataset = test_dataset.batch(32)`

`test_accuracy = tfe.metrics.Accuracy()`

`for (x, y) in test_dataset:`
 ` prediction = tf.argmax(model(x), axis=1, output_type=tf.int32)`
`  test_accuracy(prediction, y)`

`print(""Test set accuracy: {:.3%}"".format(test_accuracy.result()))`



"
20509,Cannot use tf.metrics with MirroredStrategy,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: using nvidia containter: https://docs.nvidia.com/deeplearning/dgx/tensorflow-release-notes/rel_18.06.html#rel_18.06 
- **TensorFlow version (use command below)**:1.8.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0.176
- **GPU model and memory**: nvidia tesla v100
- **Exact command to reproduce**: N/A



### Describe the problem
I'm training a network using a custom tf.estimator. To monitor its training i used several metrics from tf. metrics: accuracy, auc, true positives, ... When running this on a single GPU, This works as expected, however when using `tf.contrib.distribute.MirroredStrategy` i get an exception. Apparently all metrics suffer from the same bug (tested by iteratively leaving the metric out).

### Source code / logs
I tried to extract the relevant part from my code:
```
def configure_trainer(self):
        if self.configuration.get(""multi_gpu""):
            distribute = tf.contrib.distribute.MirroredStrategy(num_gpus=2)
        else:
            distribute = None

        run_config = tf.estimator.RunConfig(model_dir=out_dir, tf_random_seed=self.configuration.get(""random_seed""),
                                                save_summary_steps=400,
                                                save_checkpoints_steps=1000,
                                                log_step_count_stepss400,
                                                train_distribute=distribute)

        self.trainer = tf.estimator.Estimator(
            model_fn=self.get_model_fn,  
            config=run_config
        )

def get_model_fn(self, mode, features, labels, params):
        self.configure_network(input_tensor=features, output_tensor=labels, mode=mode)
        eval_metrics = self.get_accuracy(self.network.get_output_tensor(),labels)
        return tf.estimator.EstimatorSpec(mode=mode, predictions=self.predictions,
                                              loss=self.loss, train_op=self.trainer_conf.get_train_op_fn(self.loss,mode),
                                              eval_metric_ops=eval_metrics
                                              )
...
def get_accuracy(self, output_tensor,  ground_truth, name=""""):
        metric_name = 'Accuracy_' + name
        accuracy = tf.metrics.accuracy(
                labels=ground_truth,
                predictions=tf.argmax(output_tensor, axis=-1),
                name=metric_name)
        tf.summary.scalar(metric_name , accuracy[1])
        return {
            metric_name : accuracy
        }
```

This results in the following error trace:

```
Traceback (most recent call last):
  File ""train.py"", line 27, in <module>
    experimenter.run_training_experiment(config)
  File ""/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py"", line 39, in run_training_experiment
    tf.estimator.train_and_evaluate(self.trainer, self.training_specs, self.eval_specs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 439, in train_and_evaluate
    executor.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 518, in run
    self.run_local()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 650, in run_local
    hooks=train_hooks)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 363, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 841, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 977, in _train_model_distributed
    saving_listeners)
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5265, in get_controller
    yield g
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5060, in get_controller
    yield default
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5265, in get_controller
    yield g
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 977, in _train_model_distributed
    saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py"", line 304, in __exit__
    self._var_creator_scope.__exit__(exception_type, exception_value, traceback)
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py"", line 2283, in variable_creator_scope
    yield
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2939, in _variable_creator_scope
    yield
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py"", line 2283, in variable_creator_scope
    yield
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 884, in _train_model_distributed
    self.config)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py"", line 756, in call_for_each_tower
    return self._call_for_each_tower(fn, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 254, in _call_for_each_tower
    coord.join(threads)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/lib/python3.5/dist-packages/six.py"", line 693, in reraise
    raise value
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 466, in run
    self.done = True
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py"", line 295, in _mode
    yield
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 466, in run
    self.done = True
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py"", line 514, in device_policy
    yield
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 466, in run
    self.done = True
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5265, in get_controller
    yield g
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5060, in get_controller
    yield default
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5265, in get_controller
    yield g
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 466, in run
    self.done = True
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 4338, in device
    yield
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 466, in run
    self.done = True
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5991, in __exit__
    self._name_scope.__exit__(type_arg, value_arg, traceback_arg)
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 4115, in name_scope
    yield """" if new_stack is None else new_stack + ""/""
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 466, in run
    self.done = True
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py"", line 2097, in __exit__
    self._graph_context_manager.__exit__(type_arg, value_arg, traceback_arg)
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5265, in get_controller
    yield g
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5060, in get_controller
    yield default
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5265, in get_controller
    yield g
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 466, in run
    self.done = True
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py"", line 2283, in variable_creator_scope
    yield
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2939, in _variable_creator_scope
    yield
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py"", line 2283, in variable_creator_scope
    yield
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 465, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 831, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py"", line 116, in get_model_fn
    eval_metrics = self.logger.get_eval_metrics(self.predictions, labels)
  File ""/media/local/myfiles/Mytraininglogger.py"", line 26, in get_eval_metrics
    metrics.update(self.get_accuracy(output_tensor, ground_truth, name=name))
  File ""/media/local/BDA_tf_framework/neuralnetwork/traininglogger.py"", line 60, in get_accuracy
    name=metric_name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/metrics_impl.py"", line 409, in accuracy
    name or 'accuracy')
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/metrics_impl.py"", line 345, in mean
    return mean_t, update_op
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py"", line 2095, in __exit__
    self._current_name_scope.__exit__(type_arg, value_arg, traceback_arg)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5991, in __exit__
    self._name_scope.__exit__(type_arg, value_arg, traceback_arg)
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 4115, in name_scope
    yield """" if new_stack is None else new_stack + ""/""
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/metrics_impl.py"", line 332, in mean
    update_total_op = state_ops.assign_add(total, math_ops.reduce_sum(values))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py"", line 251, in assign_add
    return ref.assign_add(value)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py"", line 311, in assign_add
    return self.get(device=_get_update_device()).assign_add(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py"", line 283, in _get_update_device
    ""Use DistributionStrategy.update() to modify a MirroredVariable."")
RuntimeError: Use DistributionStrategy.update() to modify a MirroredVariable.
Exception ignored in: <generator object get_controller at 0x7f2dec1a2fc0>
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5267, in get_controller
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py"", line 136, in pop
IndexError: pop from empty list
Exception ignored in: <generator object get_controller at 0x7f2dec1f0830>
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5267, in get_controller
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py"", line 136, in pop
IndexError: pop from empty list
Exception ignored in: <generator object get_controller at 0x7f2dec17d9e8>
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 5267, in get_controller
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py"", line 136, in pop
IndexError: pop from empty list
Makefile:19: recipe for target 'train_gpu' failed
make: *** [train_gpu] Error 1
```"
20506,Pop from empty context_switches when take outputs of one estimator.predict as inputs of another,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: y 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux  4.14.8-gentoo-r1
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.5.5
- **Bazel version (if compiling from source)**: n
- **GCC/Compiler version (if compiling from source)**: n
- **CUDA/cuDNN version**: 7.1.4
- **GPU model and memory**:  GeForce GTX 1080 Ti / 11171MiB
- **Exact command to reproduce**:

### Describe the problem
Currently I am working on making predictions of two models (outputs of model1 as inputs of model2) with large amount of inputs. I tried to use tf.data.Dataset.from_generator, while it seems there are some problems about context stack.

### Source code / logs
Here is my example
``` python
import tensorflow as tf

def model_fn(features, labels, mode):
    x = features[""inputs""]
    W = tf.get_variable(
        name=""weight"",
        shape=[1],
        dtype=tf.float32,
        initializer=tf.initializers.constant(value=0.0),
        trainable=(mode == tf.estimator.ModeKeys.TRAIN),
    )
    b = tf.get_variable(
        name=""bias"",
        shape=[1],
        dtype=tf.float32,
        initializer=tf.initializers.constant(value=0.0),
        trainable=(mode == tf.estimator.ModeKeys.TRAIN),
    )
    hypothesis = W * x + b
    if mode == tf.estimator.ModeKeys.TRAIN:
        y = labels[""labels""]
        cost = tf.square(hypothesis - y)
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
        train_op = optimizer.minimize(
            loss=cost, global_step=tf.train.get_global_step())
    else:
        train_op = None
        cost = None
    return tf.estimator.EstimatorSpec(
        mode=mode, predictions=hypothesis, loss=cost, train_op=train_op)


def get_input_fn(inputs, num_epochs=1):
    def gen():
        for item in inputs:
            if isinstance(item, tuple) or isinstance(item, list):
                yield item
            else:
                yield item, 0

    def input_fn():
        dataset = tf.data.Dataset.from_generator(
            gen, (tf.float32, tf.float32),
            (tf.TensorShape([]), tf.TensorShape([])))
        dataset = dataset.repeat(num_epochs)
        dataset = dataset.batch(1)
        iterator = dataset.make_one_shot_iterator()
        inputs, labels = iterator.get_next()
        return {""inputs"": inputs}, {""labels"": labels}

    return input_fn


model1 = tf.estimator.Estimator(model_fn=model_fn, model_dir=""model1"")
model2 = tf.estimator.Estimator(model_fn=model_fn, model_dir=""model2"")
# model1.train(input_fn=get_input_fn([(1, 1), (2, 2), (3, 3)], num_epochs=100))
# model2.train(input_fn=get_input_fn([(1, 2), (2, 4), (3, 6)], num_epochs=100))

# ok
for item in model1.predict(input_fn=get_input_fn([1, 2, 3, 4])):
    print(item)
# ok
for item in model2.predict(input_fn=get_input_fn([1, 2, 3, 4])):
    print(item)

model1_output = model1.predict(input_fn=get_input_fn([1, 2, 3, 4]))
model2_input_fn = get_input_fn(model1_output)
# IndexError: pop from empty list
# tensorflow/python/framework/ops.py"", line 5267, in get_controller
#    context.context().context_switches.pop()
for item in model2.predict(model2_input_fn):
    print(item)
```

Error log
```
Traceback (most recent call last):

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/ops/script_ops.py"", line 157, in __call__
    ret = func(*args)

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 382, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""test.py"", line 36, in gen
    for item in inputs:

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 519, in predict
    for key, value in six.iteritems(preds_evaluated)

  File ""/usr/lib64/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 5267, in get_controller
    context.context().context_switches.pop()

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/eager/context.py"", line 136, in pop
    self.stack.pop()

IndexError: pop from empty list


Traceback (most recent call last):
  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
    return fn(*args)
  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: IndexError: pop from empty list
Traceback (most recent call last):

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/ops/script_ops.py"", line 157, in __call__
    ret = func(*args)

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 382, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""test.py"", line 36, in gen
    for item in inputs:

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 519, in predict
    for key, value in six.iteritems(preds_evaluated)

  File ""/usr/lib64/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 5267, in get_controller
    context.context().context_switches.pop()

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/eager/context.py"", line 136, in pop
    self.stack.pop()

IndexError: pop from empty list


         [[Node: PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_FLOAT, DT_FLOAT], token=""pyfunc_7""](arg0)]]
         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?]], output_types=[DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]
         [[Node: IteratorGetNext/_19 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_6_IteratorGetNext"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 72, in <module>
    for item in model2.predict(model2_input_fn):
  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 509, in predict
    preds_evaluated = mon_sess.run(predictions)
  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 567, in run
    run_metadata=run_metadata)
  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1043, in run
    run_metadata=run_metadata)
  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1134, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/lib64/python3.5/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1119, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 1191, in run
    run_metadata=run_metadata)
  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/training/monitored_session.py"", line 971, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: IndexError: pop from empty list
Traceback (most recent call last):

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/ops/script_ops.py"", line 157, in __call__
    ret = func(*args)

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 382, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""test.py"", line 36, in gen
    for item in inputs:

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 519, in predict
    for key, value in six.iteritems(preds_evaluated)

  File ""/usr/lib64/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 5267, in get_controller
    context.context().context_switches.pop()

  File ""/home/miacro/.local/lib64/python3.5/site-packages/tensorflow/python/eager/context.py"", line 136, in pop
    self.stack.pop()

IndexError: pop from empty list


         [[Node: PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_FLOAT, DT_FLOAT], token=""pyfunc_7""](arg0)]]
         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?]], output_types=[DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]
         [[Node: IteratorGetNext/_19 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_6_IteratorGetNext"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

```"
20505,[feature request] allclose,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GTX 1070
- **Exact command to reproduce**: N/A

### Describe the problem
As far as I can see, tensorflow does not have a function like `allclose` in numpy [1]. It's not something very tricky to implement, but it would be very convenient if it's already included in tensorflow. See the code below for a possible implementation. The default values for `rtol` and `atol` are chosen as they are for the numpy function.

[1] https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html

### Source code / logs
```
def allclose(x, y, rtol=1e-5, atol=1e-8):
    return tf.reduce_all(tf.abs(x - y) <= tf.abs(y) * rtol + atol)
```
"
20504,freeze_graph failed with core dump,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:0.11.1
- **GCC/Compiler version (if compiling from source)**: 4.9.2
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**:
`bazel-bin/tensorflow/python/tools/freeze_graph --input_saved_model_dir ~/tmp/model/test.model --saved_model_tags=serve --output_node_names=Test/Sigmoid --output_graph ~/tmp/model/test_model.pb`

### Describe the problem
Core dump occurred when I tried to freeze a 7.6G saved model.

    $ du -sh test.model/
    7.6G	test.model/

2018-07-03 16:15:26.895400: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
Converted 16 variables to const ops.

[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/wire_format.cc:830] CHECK failed: (output->ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected.  Perhaps it was modified by another thread during serialization?
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: (output->ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected.  Perhaps it was modified by another thread during serialization?
Aborted (core dumped)





### Source code / logs
```
(gdb) bt
#0  0x00007f7a6cd575f7 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007f7a6cd58ce8 in __GI_abort () at abort.c:90
#2  0x00007f7a61d356bd in __gnu_cxx::__verbose_terminate_handler () at ../../.././libstdc++-v3/libsupc++/vterminate.cc:95
#3  0x00007f7a61d33726 in __cxxabiv1::__terminate (handler=<optimized out>) at ../../.././libstdc++-v3/libsupc++/eh_terminate.cc:47
#4  0x00007f7a61d33771 in std::terminate () at ../../.././libstdc++-v3/libsupc++/eh_terminate.cc:57
#5  0x00007f7a61d33988 in __cxxabiv1::__cxa_throw (obj=0xd10f070, tinfo=0x7f7a61aad2f0 <typeinfo for google::protobuf::FatalException>, dest=0x7f7a61a1e8f0 <google::protobuf::FatalException::~FatalException()>)
    at ../../.././libstdc++-v3/libsupc++/eh_throw.cc:87
#6  0x00007f7a61a1f71c in google::protobuf::internal::LogMessage::Finish() ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
#7  0x00007f7a619ff402 in google::protobuf::internal::WireFormat::SerializeWithCachedSizes(google::protobuf::Message const&, int, google::protobuf::io::CodedOutputStream*) ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
#8  0x00007f7a619fe0d8 in google::protobuf::internal::WireFormat::SerializeFieldWithCachedSizes(google::protobuf::FieldDescriptor const*, google::protobuf::Message const&, google::protobuf::io::CodedOutputStream*) ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
#9  0x00007f7a619ff35f in google::protobuf::internal::WireFormat::SerializeWithCachedSizes(google::protobuf::Message const&, int, google::protobuf::io::CodedOutputStream*) ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
#10 0x00007f7a619fe0d8 in google::protobuf::internal::WireFormat::SerializeFieldWithCachedSizes(google::protobuf::FieldDescriptor const*, google::protobuf::Message const&, google::protobuf::io::CodedOutputStream*) ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
#11 0x00007f7a619ff35f in google::protobuf::internal::WireFormat::SerializeWithCachedSizes(google::protobuf::Message const&, int, google::protobuf::io::CodedOutputStream*) ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
#12 0x00007f7a619fe0d8 in google::protobuf::internal::WireFormat::SerializeFieldWithCachedSizes(google::protobuf::FieldDescriptor const*, google::protobuf::Message const&, google::protobuf::io::CodedOutputStream*) ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
#13 0x00007f7a619ff35f in google::protobuf::internal::WireFormat::SerializeWithCachedSizes(google::protobuf::Message const&, int, google::protobuf::io::CodedOutputStream*) ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
#14 0x00007f7a619fe0d8 in google::protobuf::internal::WireFormat::SerializeFieldWithCachedSizes(google::protobuf::FieldDescriptor const*, google::protobuf::Message const&, google::protobuf::io::CodedOutputStream*) ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
#15 0x00007f7a619ff35f in google::protobuf::internal::WireFormat::SerializeWithCachedSizes(google::protobuf::Message const&, int, google::protobuf::io::CodedOutputStream*) ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
#16 0x00007f7a619fe0d8 in google::protobuf::internal::WireFormat::SerializeFieldWithCachedSizes(google::protobuf::FieldDescriptor const*, google::protobuf::Message const&, google::protobuf::io::CodedOutputStream*) ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
#17 0x00007f7a619ff35f in google::protobuf::internal::WireFormat::SerializeWithCachedSizes(google::protobuf::Message const&, int, google::protobuf::io::CodedOutputStream*) ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
#18 0x00007f7a619fe0d8 in google::protobuf::internal::WireFormat::SerializeFieldWithCachedSizes(google::protobuf::FieldDescriptor const*, google::protobuf::Message const&, google::protobuf::io::CodedOutputStream*) ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
---Type <return> to continue, or q <return> to quit---
#19 0x00007f7a619ff35f in google::protobuf::internal::WireFormat::SerializeWithCachedSizes(google::protobuf::Message const&, int, google::protobuf::io::CodedOutputStream*) ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
#20 0x00007f7a618ccd40 in google::protobuf::python::cmessage::InternalSerializeToString(google::protobuf::python::CMessage*, _object*, _object*, bool) ()
   from /home/user/.cache/bazel/_bazel_user/c06ab0b1cffa9a52f3e96df3f3fdd0d6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/tools/freeze_graph.runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so
#21 0x00007f7a6dae8aa4 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0
#22 0x00007f7a6daea0bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0
#23 0x00007f7a6dae876f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0
#24 0x00007f7a6daea0bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0
#25 0x00007f7a6dae876f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0
#26 0x00007f7a6daea0bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0
#27 0x00007f7a6dae876f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0
#28 0x00007f7a6daea0bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0
#29 0x00007f7a6dae876f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0
#30 0x00007f7a6daea0bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0
#31 0x00007f7a6dae876f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0
#32 0x00007f7a6daea0bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0
#33 0x00007f7a6dae876f in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0
#34 0x00007f7a6daea0bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0
#35 0x00007f7a6daea1c2 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0
#36 0x00007f7a6db035ff in run_mod () from /lib64/libpython2.7.so.1.0
#37 0x00007f7a6db047be in PyRun_FileExFlags () from /lib64/libpython2.7.so.1.0
#38 0x00007f7a6db05a49 in PyRun_SimpleFileExFlags () from /lib64/libpython2.7.so.1.0
#39 0x00007f7a6db16b9f in Py_Main () from /lib64/libpython2.7.so.1.0
#40 0x00007f7a6cd43b15 in __libc_start_main (main=0x4006f0 <main>, argc=8, ubp_av=0x7fffc3fae458, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffc3fae448)
    at libc-start.c:274
#41 0x0000000000400721 in _start ()
```"
20502,TensorFlow master build failure on s390x,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: s390x Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.12.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
TensorFlow Master build on s390x failed due to BoringSSL again.
Changes causing build failure can be checked [here](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/179/)

@gunan , Could you please have a look?

### Source code / logs
```
ERROR: /home/jenkins/.cache/bazel/_bazel_jenkins/14d9bef57f8e4d2a0eef0de174c4144b/external/boringssl/BUILD:128:1: C++ compilation of rule '@boringssl//:ssl' failed (Exit 1)
In file included from external/boringssl/src/include/openssl/ssl.h:145:0,
                 from external/boringssl/src/ssl/s3_lib.cc:149:
external/boringssl/src/include/openssl/base.h:114:2: error: #error ""Unknown target CPU""
 #error ""Unknown target CPU""
```
"
20500,problem while compiling as c++ lib (libtensorflow_cc.so),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.8
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0/7.1.2
- **GPU model and memory**: GTX850M 2GB
- **Exact command to reproduce**:
sudo apt-get purge eigen* && sudo apt-get install libeigen3-dev
(move eigen3 to /usr/local/ )

git clone https://github.com/google/protobuf.git
./autogen.sh && ./configure && make && sudo make install

git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git -b r1.8
./configure
bazel build //tensorflow:libtensorflow_cc.so
(make soft link to libs and includes)

### Describe the problem
firstly, shows that lost lots of *.pb.h,
and I generate files like 'protoc --cpp_out=./  tensorflow/core/protobuf/\*.proto', so on other folders.

after that I got lots of error like image shows below:

![2018-07-03 13-34-24](https://user-images.githubusercontent.com/12708080/42200462-ee7205a2-7ec5-11e8-9ae6-f77cbca15e7d.png)

after adding `-std=c++11` to `CFLAGS` in Makefile...

![2018-07-03 13-36-55](https://user-images.githubusercontent.com/12708080/42200504-2ac3d666-7ec6-11e8-8b72-abfd1e592e7d.png)

comment the `template<>` line...

and... now I cant understand that why highest() and lowest() can passed, but infinity() and quiet_NaN() failed. whats the difference between them in coding layer?
![2018-07-03 14-22-52](https://user-images.githubusercontent.com/12708080/42201994-9738cc56-7ecc-11e8-881c-b4629e30a46d.png)

comment them too...
now it cant find c++5 files
![2018-07-03 15-09-20](https://user-images.githubusercontent.com/12708080/42204043-24109112-7ed3-11e8-9ec0-6a1a078015a4.png)


and here's my Makefile

```
GPU=1
CUDNN=1
CUDNN_HALF=0
OPENCV=1
AVX=0
OPENMP=0
LIBSO=1
EXECT=1

# set GPU=1 and CUDNN=1 to speedup on GPU
# set CUDNN_HALF=1 to further speedup 3 x times (Mixed-precision using Tensor Cores) on GPU Tesla V100, Titan V, DGX-2
# set AVX=1 and OPENMP=1 to speedup on CPU (if error occurs then set AVX=0)

DEBUG=0

ARCH= -gencode arch=compute_50,code=[sm_50,compute_50]

OS := $(shell uname)

# Tesla V100
# ARCH= -gencode arch=compute_70,code=[sm_70,compute_70]

# GTX 1080, GTX 1070, GTX 1060, GTX 1050, GTX 1030, Titan Xp, Tesla P40, Tesla P4
# ARCH= -gencode arch=compute_61,code=sm_61 -gencode arch=compute_61,code=compute_61

# GP100/Tesla P100 \96 DGX-1
# ARCH= -gencode arch=compute_60,code=sm_60

# For Jetson Tx1 uncomment:
# ARCH= -gencode arch=compute_51,code=[sm_51,compute_51]

# For Jetson Tx2 or Drive-PX2 uncomment:
# ARCH= -gencode arch=compute_62,code=[sm_62,compute_62]


VPATH=./darkSrc/ ./feature/ ./matching/ ./thirdPart/ ./thirdPart/munkres/ ./thirdPart/munkres/adapters/
EXEC=deepsort
OBJDIR=./obj/

ifeq ($(LIBSO), 1)
LIBNAMESO=deepsort.so
APPNAMESO=uselib
endif

CC=gcc
CPP=g++
NVCC=/usr/local/cuda/bin/nvcc
OPTS=-Ofast
LDFLAGS= -lm -pthread
COMMON=
CFLAGS=-w -std=c++11
#-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas

ifeq ($(DEBUG), 1)
OPTS= -O0 -g
else
ifeq ($(AVX), 1)
CFLAGS+= -ffp-contract=fast -mavx -msse4.1 -msse4a
endif
endif

CFLAGS+=$(OPTS)

ifeq ($(OPENCV), 1)
COMMON+= -DOPENCV
CFLAGS+= -DOPENCV
LDFLAGS+= `pkg-config --libs tensorflow opencv`
COMMON+= `pkg-config --cflags tensorflow opencv`
endif

ifeq ($(OPENMP), 1)
CFLAGS+= -fopenmp
LDFLAGS+= -lgomp
endif

ifeq ($(GPU), 1)
COMMON+= -DGPU -I/usr/local/cuda/include/
CFLAGS+= -DGPU
ifeq ($(OS),Darwin) #MAC
LDFLAGS+= -L/usr/local/cuda/lib -lcuda -lcudart -lcublas -lcurand
else
LDFLAGS+= -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -lcurand
endif
endif

ifeq ($(CUDNN), 1)
COMMON+= -DCUDNN
ifeq ($(OS),Darwin) #MAC
CFLAGS+= -DCUDNN -I/usr/local/cuda/include
LDFLAGS+= -L/usr/local/cuda/lib -lcudnn
else
CFLAGS+= -DCUDNN -I/usr/local/cudnn/include
LDFLAGS+= -L/usr/local/cudnn/lib64 -lcudnn
endif
endif

ifeq ($(CUDNN_HALF), 1)
COMMON+= -DCUDNN_HALF
CFLAGS+= -DCUDNN_HALF
ARCH+= -gencode arch=compute_70,code=[sm_70,compute_70]
endif

OBJ= activation_layer.o activations.o avgpool_layer.o batchnorm_layer.o blas.o box.o col2im.o connected_layer.o convolutional_layer.o cost_layer.o crnn_layer.o crop_layer.o cuda.o data.o deconvolutional_layer.o demo.o detection_layer.o dropout_layer.o gemm.o gettimeofday.o gru_layer.o im2col.o image.o layer.o list.o local_layer.o matrix.o maxpool_layer.o network.o normalization_layer.o option_list.o parser.o region_layer.o reorg_layer.o rnn_layer.o route_layer.o shortcut_layer.o softmax_layer.o tree.o utils.o
ifeq ($(GPU), 1)
LDFLAGS+= -lstdc++
OBJ+= convolutional_kernels.o activation_kernels.o im2col_kernels.o col2im_kernels.o blas_kernels.o crop_layer_kernels.o dropout_layer_kernels.o maxpool_layer_kernels.o network_kernels.o avgpool_layer_kernels.o
endif

OBJS = $(addprefix $(OBJDIR), $(OBJ))
DEPS = $(wildcard darkSrc/*.h) Makefile

all: obj backup results $(EXEC) $(LIBNAMESO) $(APPNAMESO)

ifeq ($(LIBSO), 1)
CFLAGS+= -fPIC

$(LIBNAMESO): $(OBJS) ./errmsg.hpp ./VideoTracker.hpp ./errmsg.cpp ./VideoTracker.cpp ./feature/dataType.hpp ./feature/FeatureTensor.hpp ./feature/model.hpp ./matching/kalmanfilter.hpp ./matching/linear_assignment.hpp ./matching/nn_matching.hpp ./matching/tracker.hpp ./matching/track.hpp ./feature/FeatureTensor.cpp ./feature/model.cpp ./matching/kalmanfilter.cpp ./matching/linear_assignment.cpp ./matching/nn_matching.cpp ./matching/track.cpp ./matching/tracker.cpp ./thirdPart/hungarianoper.cpp
	$(CPP) -shared -std=c++11 -fvisibility=hidden -DYOLODLL_EXPORTS $(COMMON) $(CFLAGS) $(OBJS) ./errmsg.cpp ./VideoTracker.cpp ./feature/FeatureTensor.cpp ./feature/model.cpp ./matching/kalmanfilter.cpp ./matching/linear_assignment.cpp ./matching/nn_matching.cpp ./matching/track.cpp ./matching/tracker.cpp ./thirdPart/hungarianoper.cpp -o $@ $(LDFLAGS)

$(APPNAMESO): $(LIBNAMESO) ./*.hpp ./*/*.hpp ./main.cpp
	$(CPP) -std=c++11 $(COMMON) $(CFLAGS) -o $@  ./main.cpp $(LDFLAGS) -L ./ -l:$(LIBNAMESO)
endif

ifeq ($(EXECT), 1)
$(EXEC): $(OBJS)
	$(CPP) $(COMMON) $(CFLAGS) $^ -o $@ $(LDFLAGS)
endif

$(OBJDIR)%.o: %.c $(DEPS)
	$(CC) $(COMMON) $(CFLAGS) -c $< -o $@

$(OBJDIR)%.o: %.cpp $(DEPS)
	$(CPP) $(COMMON) $(CFLAGS) -c $< -o $@

$(OBJDIR)%.o: %.cu $(DEPS)
	$(NVCC) $(ARCH) $(COMMON) --compiler-options ""$(CFLAGS)"" -c $< -o $@

obj:
	mkdir -p obj
backup:
	mkdir -p backup
results:
	mkdir -p results

.PHONY: clean

valgrind:
	VALGRIND=""valgrind --log-file=valgrind-%p.log"" $(MAKE)

clean:
	rm -rf $(OBJS) $(EXEC) $(LIBNAMESO) $(APPNAMESO)
```
"
20499,AttributeError: module 'tensorflow.python.training.checkpointable' has no attribute 'CheckpointableBase',"from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
import seaborn as sns

import tensorflow as tf                            # importing Tensorflow
import tensorflow_probability as tfp               # and Tensorflow probability
from tensorflow_probability import edward2 as ed   # Edwardlib extension

tfd = tfp.distributions             # Basic probability distribution toolkit
tfb = tfp.distributions.bijectors   # and their modifiers

# Eager Execution
# tfe = tf.contrib.eager
# tfe.enable_eager_execution()

%matplotlib inline
plt.style.use(""fivethirtyeight"")        # Styling plots like FiveThirtyEight

import warnings
warnings.filterwarnings('ignore')
%config InlineBackend.figure_format=""retina"" # improves resolution of plots

> ### it's my source.
> but i have a problem. I installed tensorflow and my tensorflow version is 1.8.0.
> But that source has a error. I don't know what my source is error.
> 

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-6-86bc64cca59d> in <module>()
      8 import seaborn as sns
      9 
---> 10 import tensorflow as tf                            # importing Tensorflow
     11 import tensorflow_probability as tfp               # and Tensorflow probability
     12 from tensorflow_probability import edward2 as ed   # Edwardlib extension

~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 # pylint: disable=wildcard-import
     26 from tensorflow.tools.api.generator.api import *  # pylint: disable=redefined-builtin

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()
     61 
     62 # Framework
---> 63 from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
     64 from tensorflow.python.framework.versions import *
     65 from tensorflow.python.framework import errors

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py in <module>()
    102 from tensorflow.python.framework.random_seed import set_random_seed
    103 from tensorflow.python.framework.sparse_tensor import convert_to_tensor_or_sparse_tensor
--> 104 from tensorflow.python.framework.importer import import_graph_def
    105 
    106 # Utilities for working with Tensors

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py in <module>()
     30 from tensorflow.python.framework import dtypes
     31 from tensorflow.python.framework import errors
---> 32 from tensorflow.python.framework import function
     33 from tensorflow.python.framework import op_def_registry
     34 from tensorflow.python.framework import ops

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py in <module>()
     34 from tensorflow.python.framework import ops
     35 from tensorflow.python.ops import array_ops
---> 36 from tensorflow.python.ops import resource_variable_ops
     37 from tensorflow.python.ops import variable_scope as vs
     38 from tensorflow.python.util import compat

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py in <module>()
     33 from tensorflow.python.ops import gen_state_ops
     34 from tensorflow.python.ops import math_ops
---> 35 from tensorflow.python.ops import variables
     36 # go/tf-wildcard-import
     37 # pylint: disable=wildcard-import

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in <module>()
     38 
     39 @tf_export(""Variable"")
---> 40 class Variable(checkpointable.CheckpointableBase):
     41   """"""See the @{$variables$Variables How To} for a high level overview.
     42 

AttributeError: module 'tensorflow.python.training.checkpointable' has no attribute 'CheckpointableBase'


> it's my error...
> 
> help me pleazzzzzzzzzzzzzz
> "
20498,Discrepency between build of  tensorflow in --config=mkl vs --config=cuda,"I am building the tensorflow from source for cpu and gpu.


For GPU 
As mentioned here https://www.tensorflow.org/install/install_sources
--config = cuda 
In this case cuda library link is happening through run time


for CPU
As mentioned here https://www.tensorflow.org/performance/performance_guide
--config = mkl  

In this case if I have MKL-DNN installed its linking at compile time and generating fat binary. 

Why there is discrepancy here or is my understanding incorrect ?

"
20496,Is between-graph mode in MultiWorkerMirroredStrategy being developed ?,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem

I found MultiWorkerMirroredStrategy has been supported in Estimator. However, it seems that only works with in-graph mode. And the comments here shows it will support between-graph mode in the future. 

`# TODO(yuefengz): support between-graph replication.`

I want to know the progress of the between-graph development. Whether it is in progress or even has not started yetï¼Ÿ If it has been in progress, when does it release in master branchï¼Ÿ 

Thank you.  

"
20494,Why dense layer cannot be speed up in tf.contrib.trt ?,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**:  2.7.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 / 7.0.5
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem

When using tf.contrib.trt.create_inference_graph, I meet the following error, it seems that dense layer is not support because the input tensor is not rank 4 ? Why has this request on dense layer ?

> subgraph conversion error for subgraph_index:9 due to: ""Unimplemented: Require 4 dimensional input. Got 2 dense/MatMul"" SKIPPING

### Source code / logs

Need not to code, clear above ...
"
20493,Huge whl file for 1.9 compared to 1.6,"After building  Tensorflow 1.9 and 1.6 for GPU, noticed that file sizes are 7x larger for 1.9.   
TF 1.9 :  Ubuntu 16.04, Python 3.6.6,  Cuda 9.0 , Cudnn 7.0
TF 1.6:   Ubuntu 16.04, Python 3.5.3   Cuda 9.0 , Cudnn 7.0

-rw-rw-r-- 1     75302881 Jul  2 00:16 tensorflow-1.6.1-cp35-cp35m-linux_x86_64.whl
-rw-rw-r-- 1   512505346 Jul  2 18:51 tensorflow-1.9.0rc2-cp36-cp36m-linux_x86_64.whl"
20492,How to replace the model in TensorFlow Android Demo,"I would like to replace the model of the TensorFlowAndroidDemo. There are two files in the asset folder: one is ""model.pb"", file and one "" label.txt"".

Now I already have a pretrained pb model. How can I get the corresponding label. txt file?

Thanks"
20487,Validate variable dtype before restoring checkpoint,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: [Yes](https://stackoverflow.com/questions/51137417/wrong-output-for-restored-variable-in-tensorflow-graph)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0-dev20180620
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0 / 7
- **GPU model and memory**: GeForce GTX 1080 / 8 Gb
- **Exact command to reproduce**: Run the two scripts below in ""Source code/logs""

### Describe the problem
A warning/error should be raised when a variable is loaded with a different dtype as that which was saved. 

In the code below, the variable `global_step` is saved as a `tf.int32`, but treated as a `tf.float32` when restored. When this happens, the `global_step` variable appears to have a value of 7e-45 instead of the expected value of 5. 

### Source code / logs
 [Link to Stackoverflow Post with the code/question](https://stackoverflow.com/questions/51137417/wrong-output-for-restored-variable-in-tensorflow-graph)

[Original post copied here]
**Script that saves the checkpoints:**

    import tensorflow as tf

    a = tf.Variable(3.0, name='a')
    b = tf.Variable(5.0, name='b')
    
    b = tf.assign_add(b, a)
    
    n_steps = 5
    
    global_step = tf.Variable(0, name='global_step', trainable=False)
    
    saver = tf.train.Saver()
    
    with tf.Session() as sess:
    
        sess.run(tf.global_variables_initializer())
        
        for step in range(n_steps):
            print(sess.run(b))
    
            global_step.assign_add(1).eval()
            print(global_step.eval())
    
            saver.save(sess, './my_test_model', global_step=global_step)

**Script that restores checkpoint:**

    import tensorflow as tf
    
    from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file
    
    # List ALL tensors.
    print_tensors_in_checkpoint_file(tf.train.latest_checkpoint('./'), all_tensors=True, tensor_name='')
    
    tf.reset_default_graph()
    
    a = tf.get_variable('a', shape=[])
    b = tf.get_variable('b', shape=[])
    global_step = tf.get_variable('global_step', shape=[])
    
    saver = tf.train.Saver()
    
    with tf.Session() as sess:
            
        ckpt = tf.train.latest_checkpoint('./')
        if ckpt:
            print(ckpt)
        
            saver.restore(sess, ckpt)
        
        else:
            print('Nothing restored')
        
        print(a.eval())
        print(b.eval())
        print(global_step.eval())

**Output**
```
tensor_name:  a
3.0
tensor_name:  b
20.0
tensor_name:  global_step
5
./my_test_model-5
INFO:tensorflow:Restoring parameters from ./my_test_model-5
3.0
20.0
7e-45
```

"
20485,tflite label_image not showing any labels,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.3
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: Python 3.6.0 :: Anaconda 4.3.1 (x86_64)
- **Bazel version (if compiling from source)**: Build label: 0.14.1-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.1.0 (clang-902.0.39.2)
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NVIDIA GeForce GT 650M 1 GB
- **Exact command to reproduce**:  bazel-bin/tensorflow/contrib/lite/examples/label_image/label_image --tflite_model=../new_training_dir/retrainedMNetV1_7_8000.tflite --image=../tf_files/test_data_BITMAP/sidewalk/IMG_1913.bmp --labels=testLabels.txt


### Describe the problem
I have managed to compile the tflite label_image but when I use it with a retrained mobilenet I get the results without any labels next to it like following:

### Source code / logs
```
Loaded model ../new_training_dir/retrainedMNetV1_7_8000.tflite
resolved reporter
invoked
average time: 70.195 ms
8.15717: 544
6.8344: 732
6.73561: 734
6.2833: 423
5.93283: 420
```"
20484,Unable to import tensorflow with tf-nightly-gpu,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip install tensorflow-nightly-gpu
- **TensorFlow version (use command below)**: 1.10
- **Python version**:  2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
import tensorflow as tf
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

ttTraceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 81, in <module>
    from tensorflow.python import keras
  File ""/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/__init__.py"", line 24, in <module>
    from tensorflow.python.keras import activations
  File ""/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/activations/__init__.py"", line 22, in <module>
    from tensorflow.python.keras._impl.keras.activations import elu
  File ""/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/__init__.py"", line 21, in <module>
    from tensorflow.python.keras._impl.keras import activations
  File ""/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/activations.py"", line 23, in <module>
    from tensorflow.python.keras._impl.keras import backend as K
  File ""/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/backend.py"", line 38, in <module>
    from tensorflow.python.layers import base as tf_base_layers
  File ""/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 25, in <module>
    from tensorflow.python.keras.engine import base_layer
  File ""/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/__init__.py"", line 21, in <module>
    from tensorflow.python.keras.engine.base_layer import InputSpec
  File ""/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 33, in <module>
    from tensorflow.python.keras import backend
  File ""/home/dhingratul/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/keras/backend/__init__.py"", line 22, in <module>
    from tensorflow.python.keras._impl.keras.backend import abs
ImportError: cannot import name abs
"
20482,Trouble replicating results using capi," I have written minimal code in both python and using the capi. This code loads a premade graph and runs a test image through it. I cannot seem to get the same results using the capi as I get running through python.
Linux Ubuntu 16.04
I have both installed tf and built the library using bazel.
I haqve tried various versions but for this experiment am fixing on 1.9.
Python 3.5.2
bazel 0.13.0
gcc 5.4
no cuda

I am trying to run the graph ""ssd_mobilenet_v1_android_export.pb"" given with the android mobile examples.

I am using this test image https://github.com/pjreddie/darknet/raw/master/data/dog.jpg

I have simple python code (code below), that loads the graph, feeds the image to it and reads back various tensors. This graph has an op at the end called ""num_detections"", I am reading the tensor at this point to check for success.
With the dog.jpg I get a ""3.0"" result. Other test images give appropriate results. 

I am repeating this experiment with the capi (code below), but the only result I ever get is zero. 
The first obvious place I looked is my image input. The second graph op is a ""ToFloat"" operation. I have been using that to check the values of the pixel data going into the feed. They are identical. I checked the image is not flipped and the color channels are in the same order after decoding. I then started looking further up the graph, sampling values of the tensors at each op. I find they are mostly the same until they get to a long list of convolution ops where it seems the values slowly diverge - like they are the same the first couple of ops then start to show differences getting further and further apart.

Not working capi code:

````

#include <png.hpp>
#include <stdlib.h>
#include <stdio.h>
#include <tensorflow/c/c_api.h>

typedef unsigned char byte; 

const int num_channels = 3;

struct Image {
  byte* data;
  int width, height;
  int dataSize() {
    return width * height * num_channels;
  }
};

Image open_image(const char* filename) {

  Image image;
  int dataSize = image.width * image.height * num_channels;

  png::image< png::rgb_pixel > png_import(filename);
  image.width = png_import.get_width();
  image.height = png_import.get_height();
  image.data = (byte*)malloc(image.dataSize());
  for(int y=0; y<image.height; y++) {
    for(int x=0; x<image.width; x++) {
      png::rgb_pixel pixel = png_import.get_pixel(x,y);
      byte* data = &image.data[(y*image.width+x)*num_channels];
      *data++ = pixel.red;
      *data++ = pixel.green;
      *data++ = pixel.blue;
    }
  }

  printf(""%s loaded, %dx%d\n"", filename, image.width, image.height);
  return image;
}

TF_Graph* open_graph(const char* filename, TF_Status* status) {

  TF_Graph* graph = TF_NewGraph();

  TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();

  FILE* fp = fopen(filename, ""r"");

  fseek(fp, 0L, SEEK_END);
  int file_size = ftell(fp);
  rewind(fp);

  char* graph_content = (char*)malloc(file_size);

  fread(graph_content, file_size, 1, fp);
  fclose(fp);

  TF_Buffer tfBuffer;
  tfBuffer.data = graph_content;
  tfBuffer.length = file_size;
  
  TF_GraphImportGraphDef(graph, &tfBuffer, opts, status);

  free(graph_content);

  if(TF_GetCode(status) != TF_OK) {

    printf(""%s\n"", TF_Message(status));
    return 0;
  }

  printf(""Successfully loaded graph\n"");

  return graph;
}
void TensorDeallocator(void* data, size_t len, void* arg)
{

}


int main(int argi, char** argc) {

  printf(""TensorFlow C library version %s\n"", TF_Version());

  TF_Status* status = TF_NewStatus();
  TF_Graph* graph = open_graph(argc[1], status);

  Image image = open_image(argc[2]);

  int64_t dims[] = {1, image.width, image.height, num_channels};
  
  TF_Tensor* image_tensor = TF_NewTensor(
    TF_UINT8, dims, 4, 
    image.data, image.dataSize(), 
    TensorDeallocator, 0
  );

  TF_Operation* input_image_op = TF_GraphOperationByName(graph, ""image_tensor"");
  if(!input_image_op) {
    printf(""Failed to find Op '%s'"", ""image_tensor"");
    return 1;
  }

  TF_Output image_input;
  image_input.oper = input_image_op;
  image_input.index = 0;
  TF_Output inputs[1] = {image_input};

  TF_Operation* num_detection_op = TF_GraphOperationByName(graph, ""num_detections"");
  if(!num_detection_op) {
    printf(""Failed to find Op '%s'"", ""num_detections"");
    return 1;
  }
  TF_Output num_detection_output;
  num_detection_output.oper = num_detection_op;
  num_detection_output.index = 0;

  TF_Output outputs[1] = {num_detection_output};
  TF_Tensor* output_tensors[1];

  TF_SessionOptions * options = TF_NewSessionOptions();
  TF_Session* session = TF_NewSession( graph, options, status );

  TF_SessionRun(session, 0, 
                &image_input, &image_tensor, 1,
                outputs, output_tensors, 1,
                0, 0,
                0, status
                );

  if(TF_GetCode(status) != TF_OK) {

    printf(""%s\n"", TF_Message(status));
  }
  else {
      
    printf(""Ran successfully\n"");

    float* f = (float*)TF_TensorData(output_tensors[0]);

    printf(""TF data %f\n"", f[0]);

  }
  
  free(image.data);

  return 0;
}


```


Working python code:


```
import data_helpers
import sys
import tensorflow as tf
import numpy as np


from tensorflow.python.platform import gfile
from tensorflow.python.util import compat
from tensorflow.core.protobuf import saved_model_pb2

def read_graph(model_filename):

    with gfile.FastGFile(model_filename, 'rb') as f:

        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        g_in = tf.import_graph_def(graph_def)

        return g_in


def printOps():

    for op in tf.get_default_graph().get_operations():

        print(op.name)

if __name__ == ""__main__"":
    
    if len(sys.argv) < 3:
        print(""Usage: run_image_on_graph.py [input graph filepath] [output image filepath]"")
        exit(1)

    graph_file = sys.argv[1]
    image_file = sys.argv[2]

    image = data_helpers.import_image(image_file)

    with tf.Session() as sess:

        read_graph(graph_file)

        graph = tf.get_default_graph()

        image_tensor = graph.get_tensor_by_name(""import/image_tensor:0"")
        num_detections = graph.get_tensor_by_name(""import/num_detections:0"")
        detection_scores = graph.get_tensor_by_name(""import/detection_scores:0"")
        detection_boxes = graph.get_tensor_by_name(""import/detection_boxes:0"")
        debug = graph.get_tensor_by_name(""import/ToFloat:0"")

        image = np.array([data_helpers.import_image(image_file)])

        print(""sess = "" + str(sess.run([num_detections], feed_dict={
            image_tensor: image})
        ))
      
```
   

Is there anything obviously wrong with my approach?"
20481,tf.data.Dataset.from_tensor_slices incompatible with tuples?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 and 18.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: `1.8.0`
- **Python version**: `2.7`
- **CUDA/cuDNN version**: 9.0 / 7.5 but also failing in CPU-only mode
- **GPU model and memory**: P100
- **Exact command to reproduce**:
``` python
>>> import tensorflow as tf
>>> 
>>> data_list = 'tensors: A nested structure of tensors, each having the same size in the 0th dimension.'.split()
>>> data_tuple = tuple(data_list)
>>> print('list: {}'.format(data_list))
list: ['tensors:', 'A', 'nested', 'structure', 'of', 'tensors,', 'each', 'having', 'the', 'same', 'size', 'in', 'the', '0th', 'dimension.']
>>> print('tuple: {}'.format(data_tuple))
tuple: ('tensors:', 'A', 'nested', 'structure', 'of', 'tensors,', 'each', 'having', 'the', 'same', 'size', 'in', 'the', '0th', 'dimension.')
>>> 
>>> ds_l = tf.data.Dataset.from_tensor_slices(data_list)
>>> try:
...     ds_t = tf.data.Dataset.from_tensor_slices(data_tuple)
... except IndexError as e:
...     print(e.message)
... 
list index out of range
>>> ds_l = tf.data.Dataset.from_tensors(data_list)
>>> ds_t = tf.data.Dataset.from_tensors(data_tuple)
>>> 
>>> with tf.Session() as session:
...     for ds in [ds_l, ds_t]:
...         it = ds.make_one_shot_iterator().get_next()
...         while True:
...             try:
...                 print(session.run(it))
...             except tf.errors.OutOfRangeError:
...                 break
... 
['tensors:' 'A' 'nested' 'structure' 'of' 'tensors,' 'each' 'having' 'the'
'same' 'size' 'in' 'the' '0th' 'dimension.']
('tensors:', 'A', 'nested', 'structure', 'of', 'tensors,', 'each', 'having', 'the', 'same', 'size', 'in', 'the', '0th', 'dimension.')


```

### Describe the problem
Apparently one can not use `tf.data.Dataset.from_tensor_slices` with tuples. This is very counter-intuitive as they almost everywhere have the same behavior as lists.
Also using `tf.data.Dataset.from_tensors` is no option. Even though this seems to handle tuples properly, one only gets a single element instead of `n` elements. This is in alignment with the documentation but does not fulfill the same functionality as `tf.data.Dataset.from_tensor_slices`.

Am I just using it wrong, is the documentation to ambiguous or should this be fixed?"
20478,Support dense tensors in sequence_numeric_column,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
Tensorflow 1.8.0
- **Python version**: 
3.8.3
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
CUDA 9.0/cuDNN 7.0
- **GPU model and memory**:
1080TI 11GB
- **Exact command to reproduce**:
N/A

### Describe the problem

I am training a time series using an RNN and my input data has a fixed sequence length per batch.

I am using [`parse_single_example`](https://www.tensorflow.org/api_docs/python/tf/parse_single_example) with [`regressor_parse_example_spec`](https://www.tensorflow.org/api_docs/python/tf/estimator/regressor_parse_example_spec) and [`sequence_numeric_column`](https://www.tensorflow.org/api_docs/python/tf/contrib/feature_column/sequence_numeric_column) which parses the features as sparse tensors because [`_parse_example_spec` returns `VarLenFeature`](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/feature_column/python/feature_column/sequence_feature_column.py#L419).

I want to parse our data as dense tensors because the underlying data is dense, and using sparse tensors in our graph requires extra unnecessary complexity for our calculations. I want to continue to use `feature_columns` to describe our data and `sequence_input_layer` but I propose adding a new feature column type `sequence_fixed_len_numeric_column` that parses as a dense tensor.

[Here is the implementation](https://gist.github.com/jperl/245c414793a5271da72183bada93c55c#file-sequence_fixed_len_numeric_column-py-L35) and I would be happy to submit a PR + add tests. But first I wanted to confirm this a idiomatic approach.

Alternatively I could add a parameter to `sequence_numeric_column`, `is_fixed` to accomplish the same things."
20475,Add a dictionary to a collection,"### System information
OS Platform and Distribution Mac OS X latest
TensorFlow installed from pip
TensorFlow version 1.8
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce see code below

### Describe the problem
Dear All,

I am looking for a way to store and retrieve a dictionary of tensors. If I try to pass a dict to `ft.add_to_collection` I get a warning and when I load the graph again the collection does not exist. This is a simple example:

```
import tensorflow as tf
tf.reset_default_graph()

foo  = tf.Variable([1,2,3], dtype=tf.float32)
boo  = tf.Variable([1,2,3], dtype=tf.float32)

tf.add_to_collection('test', {'foo' : foo, 'boo': [boo] })
tf.add_to_collection('test2', foo)

with tf.Session() as sess:
    saver = tf.train.Saver()
    sess.run(tf.global_variables_initializer())
    saver.save(sess,'save/model.ckpt')

tf.reset_default_graph() # reset so we are sure to load a new graph

with tf.Session() as sess:
    saver = tf.train.import_meta_graph(""save/model.ckpt.meta"")
    saver.restore(sess,'save/model.ckpt')
    graph = tf.get_default_graph()
    print(graph.get_collection('test'))
    print(graph.get_collection('test2'))
```

Output:

```
WARNING:tensorflow:Error encountered when serializing test.
Type is unsupported, or the types of the items don't match field type in CollectionDef.
'dict' object has no attribute 'name'
INFO:tensorflow:Restoring parameters from save/model.ckpt
[]
[<tf.Tensor 'Variable:0' shape=(3,) dtype=float32_ref>]
```

As you can see, after loading the graph, the `test` collection is empty.

Is there any way to properly store/retrieve a dictionary that maps tensors? 
"
20473,I want to build tensorflow-gpu 1.4.0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04.4 LTS
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:tensorflow-gpu 1.4.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:0.5.4
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:8.0/6.0.21
- **GPU model and memory**:GeForce GTX 980 4GB
- **Exact command to reproduce**:


### Describe the problem

I am trying to install TensorFlow from Sources. I did it with reference to https://www.tensorflow.org/install/install_sources.But I accidentally built a CPU version.

I want to build tensorflow-gpu 1.4.0 from source. But I cannot find the GPU version of tensorflow.

Any suggestions would help. Thanks.
"
20471,ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04.4 LTS
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**:0.5.4
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:8.0/6.0.21
- **GPU model and memory**:GeForce GTX 980 4GB
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I am trying to install  TensorFlow from Sources. I did it with reference to https://www.tensorflow.org/install/install_sources.

To build a pip package for TensorFlow with GPU support, I invoke the following command:

bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

But getting following error.(My log is shown at the bottom)

Any suggestions would help. Thanks.

### Source code / logs
root@nakadake:/mnt/fs4/nagata/tensorflow-1.4.0# ./configure 
WARNING: Running Bazel server needs to be killed, because the startup options are different.
You have bazel 0.5.4 installed.
Traceback (most recent call last):
  File ""configure.py"", line 1039, in <module>
    main()
  File ""configure.py"", line 973, in main
    reset_tf_configure_bazelrc()
  File ""configure.py"", line 249, in reset_tf_configure_bazelrc
    open(_TF_BAZELRC, 'w').close()
IOError: [Errno 13] Permission denied: '/mnt/fs4/nagata/tensorflow-1.4.0/.tf_configure.bazelrc'
"
20470,Error while converting .pb model into tensorflowlite model,"Command:
 toco --input_file=/home/p/Downloads/tensorflow/output_graph.pb --output_file=/home/p/Downloads/tensorflow/optimized.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_arrays=""dense_1_input"" --output_arrays=""act_4/Softmax"" --inference_type=FLOAT --input_data_type=FLOAT --input_shapes=1,40

- output_graph.pb is a frozen model.

Error:
2018-06-29 16:02:00.398756: F tensorflow/contrib/lite/toco/model_cmdline_flags.cc:260] Check failed: uses_single_input_flags 
Aborted (core dumped)

Any help will be highly appreciated.
"
20469,total_float_ops is 0 by tf.profiler.profile,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: CUDA 9 / cuDNN 7
- **GPU model and memory**: GeForce GTX 1080Ti
- **Exact command to reproduce**:
```
import cv2
import time
import tensorflow as tf
from tensorflow.python.framework import graph_util

ModelFile = ""OX_Predict_frozen.pb""

def load_pb(pb):
    with tf.gfile.GFile(pb, ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
    with tf.Graph().as_default() as graph:
        tf.import_graph_def(graph_def, name='')
        return graph
        
import math
M = math.pow( 10, 6 )
print(M)

def log_FLOP():
    # ***** (3) Load frozen graph *****
    g2 = load_pb(ModelFile)
    with g2.as_default():
        flops = tf.profiler.profile(g2, options = tf.profiler.ProfileOptionBuilder.float_operation())
        print('FLOP after freezing(M): ', flops.total_float_ops/ M)

def main():
    log_FLOP()

if __name__ == ""__main__"":
    main()
```


### Describe the problem
I'm trying to log the number of multiply-add operations (MAC) in my network by ""tf.profiler.profile"".

here is the [model](https://drive.google.com/file/d/12-7TI7EGLuN8JgcafGPJKeugp263gVYD/view?usp=sharing)(.pb)

the model work perfect when predict, but it always return 0 flops with ""tf.profiler.profile""

any suggestion ?

### Source code / logs
the sample code to get .pb is following:
https://github.com/ChiFang/TensorFlow_XO_example

it takes only few sec~~~~
"
20468,Failing to load h5 model using tf-gpu?,"```
Traceback (most recent call last):
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1305, in _run_fn
    self._extend_graph()
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1340, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=""un
idirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=
""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa
ndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 264, in load_model
    load_weights_from_hdf5_group(f['model_weights'], model.layers)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 929, in load_weights_from_hdf5_group
    K.batch_set_value(weight_value_tuples)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 2435, in batch_set_value
    get_session().run(assign_ops, feed_dict=feed_dict)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 196, in get_session
    [tf.is_variable_initialized(v) for v in candidate_vars])
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=""un
idirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=
""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa
ndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

Caused by op 'bidirectional_1/CudnnRNN_1', defined at:
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 261, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 335, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\__init__.py"", line 55, in deserialize
    printable_module_name='layer')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ut
ils\generic_utils.py"", line 145, in deserialize_keras_object
    list(custom_objects.items())))
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 293, in from_config
    model.add(layer)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 166, in add
    layer(x)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 426, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\base_layer.py"", line 460, in __call__
    output = self.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 505, in call
    y_rev = self.backward_layer.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 90, in call
    output, states = self._process_batch(inputs, initial_state)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 297, in _process_batch
    is_training=True)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1623, in __call__
    seed=self._seed)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1012, in _cudnn_rnn_no_i
nput_c
    direction, dropout, seed, name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 926, in _cudnn_rnn
    name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\ops\gen_cudnn_rnn_ops.py"", line 143, in cudnn_rnn
    is_training=is_training, name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-
access

InvalidArgumentError (see above for traceback): No OpKernel was registered to su
pport Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered ker
nels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=""un
idirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=
""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa
ndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]


(tensorflow-gpu) C:\Users\xion>python Z:\trader_connect.py --csv Y:\EURUSD,5.c
sv
Using TensorFlow backend.
2018-07-01 20:58:02.203507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:1356] Found device 0 with properties:
name: GeForce GT 530 major: 2 minor: 1 memoryClockRate(GHz): 1.399
pciBusID: 0000:01:00.0
totalMemory: 2.00GiB freeMemory: 1.87GiB
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:1406] Ignoring visible gpu device (device: 0, name: GeFo
rce GT 530, pci bus id: 0000:01:00.0, compute capability: 2.1) with Cuda compute
 capability 2.1. The minimum required Cuda capability is 3.0.
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1
edge matrix:
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:929]      0
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:942] 0:   N
Traceback (most recent call last):
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1305, in _run_fn
    self._extend_graph()
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1340, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=""u
nidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode
=""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp
andDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 264, in load_model
    load_weights_from_hdf5_group(f['model_weights'], model.layers)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 929, in load_weights_from_hdf5_group
    K.batch_set_value(weight_value_tuples)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 2435, in batch_set_value
    get_session().run(assign_ops, feed_dict=feed_dict)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 196, in get_session
    [tf.is_variable_initialized(v) for v in candidate_vars])
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=""u
nidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode
=""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp
andDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

Caused by op 'bidirectional_1/CudnnRNN_1', defined at:
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 261, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 335, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\__init__.py"", line 55, in deserialize
    printable_module_name='layer')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ut
ils\generic_utils.py"", line 145, in deserialize_keras_object
    list(custom_objects.items())))
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 293, in from_config
    model.add(layer)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 166, in add
    layer(x)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 426, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\base_layer.py"", line 460, in __call__
    output = self.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 505, in call
    y_rev = self.backward_layer.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 90, in call
    output, states = self._process_batch(inputs, initial_state)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 297, in _process_batch
    is_training=True)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1623, in __call__
    seed=self._seed)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1012, in _cudnn_rnn_no_i
nput_c
    direction, dropout, seed, name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 926, in _cudnn_rnn
    name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\ops\gen_cudnn_rnn_ops.py"", line 143, in cudnn_rnn
    is_training=is_training, name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-
access

InvalidArgumentError (see above for traceback): No OpKernel was registered to su
pport Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered ker
nels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=""u
nidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode
=""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp
andDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

```




I've tried--fresh reinstall, change float

Any fixes? No idea why Im getting this. P.S: I trained the model on a titan v and I am trying to now open it on a computer with a GeForce GT 530 Gpu."
20467,maybe a bug? different gradients of x**2 and x*x?,"```
import tensorflow as tf

sess = tf.Session()

lr = 0.1

x = tf.Variable(1.)
y = 0.5 * x * x
# y = 0.5 * x**2

P = tf.global_variables()

grads = tf.gradients(y, P)

ops = []
for p,g in zip(P, grads):
    ops.append(tf.assign(p, p - lr * g))

with tf.control_dependencies(ops):
    ops2 = []
    grads = tf.gradients(y, P)
    for p,g in zip(P, grads):
        ops2.append(tf.assign(p, p - lr * g))


sess.run(tf.global_variables_initializer())

sess.run(ops2)
```

if `y = 0.5 * x * x` it return `[0.80499995] ` but `y = 0.5 * x**2` return `[0.81]`, why?

how can I do if I want to run two steps of GD in one sess.run ?"
20466,InternalError (see above for traceback): CUB segmented reduce errorinvalid device function,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: running training step from [here](https://github.com/lengstrom/fast-style-transfer)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64 bit
- **TensorFlow installed from (source or binary)**: installed using conda
- **TensorFlow version (use command below)**: b'unknown' 1.8.0
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**:  CUDA 9.0 cuDNN 7.1.4
- **GPU model and memory**: name: GeForce GTX 650 Ti major: 3 minor: 0 memoryClockRate(GHz): 0.928
- **Exact command to reproduce**: python style.py --style examples\poem.jpg --checkpoint-dir checkpoint


### Describe the problem
Running a CNN training on fast style transfer, and using tensorflow-gpu. training crushed at moments/mean, is this a bug in tensorflow?

### Source code / logs

> Traceback (most recent call last):
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\tensorflow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\tensorflow\python\client\session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\tensorflow\python\client\session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: CUB segmented reduce errorinvalid device function
         [[Node: moments/mean = Mean[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](Conv2D_16, moments/mean-1-LayoutOptimizer)]]

"
20465,Tensorflow Import Error,"I have just downloaded Anaconda 1.8.7. , using Py 3.6. Donwloaded Tensorflow pkgs. But I have importing issues. Using windows. Any help will be appraciated.Thank you. Here is the error message on Jupyter Notebook: 
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

C:\ProgramData\Anaconda3\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

C:\ProgramData\Anaconda3\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 # pylint: disable=wildcard-import
     26 from tensorflow.tools.api.generator.api import *  # pylint: disable=redefined-builtin

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

"
20464,[Feature request] Allow `tf.estimator.train_and_evaluate` to evaluate on multiple datasets,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

### Describe the problem

Currently `tf.estimator.train_and_evaluate` makes it easy to use an `Estimator` to perform both training and evaluation, possibly in a distributed environment.  However, this function only supports a single evaluation dataset.  This makes the function suboptimal because we oftentimes want to evaluate on both the training and the validation set in order to get a sense for the amount of overfitting that is happening.  It would be ideal if we could perhaps pass a list of `EvalSpec` objects to `train_and_evaluate`."
20463,C++ Code,"Hello, thank you very much for answer me !

This is the question:

There are 2 tensors, tensor A and tensor  B with same shape and data-type,  how can I put B into A with C++ code. I know in Python, I can use ""append()"" function. In C++, which function is same  with ""append()""?

Python's code for example :
```
TrainImageDataArray = []
ImageData = mpl.image.imread(strImageInfoArray[i][0])
ImageData = ImageData.reshape(-1)
TrainImageDataArray.append(ImageData)
```
And how can I put ""ImageData"" into ""TrainImageDataArray"" in C++."
20462,[Feature Request] Exponential Integral function Ei,"No form because it's not a bug. -- Well, I assume it isn't a bug. As it happens, while Tensorflow does implement the log-gamma and regularized incomplete gamma functions, as far as I know no combination of provided functions can be used to obtain the exponential integral function Ei (it's a 0/0 problem for the most part). As far as I know, my only recourse at this point is writing a custom op and gradient with SciPy's expi function. It'd be cool if that were addressed in the next version.

Thanks!"
20461,[Feature Request] Inverse Wishart Distribution,"Mathematically, the InvWishart(X) = Wishart(X^-1)

Since there is already InverseGamma in tf.contrib.distributions, it should be straightforward to implement InverseWishart. 

"
20459,How to link Tensorflow C++ API code with the original c++ source code?,"I use TensorFlow to build an ML model and now want to use it in my application. My plan is to use the TF to implement the ML reference using my ML model.
Is it possible to use TF API within my application and link TF as a library when building my application?
My application is a c++ code."
20457,Krylov subspace matrix solvers?,"Hi @langmore @ebrevdo,

I'm interested in performing differentiable matrix solves with TensorFlow using Krylov subspace methods (for sparse or structured matrices). I was considering making a conjugate gradient solver, since that's what I need at the moment, and was wondering if that would be worthy of contributing somewhere? Or if it's already been done? I was planning to use `tf.while_loop` and `tf.linalg.LinearOperator`, and to implement the gradient using the standard $dA^{-1}/dx = -A^{-1} dA/dx A^{-1}$ formula.

Any advice/pointers would be appreciated. Thanks!"
20456,add sticky version flag to https://www.tensorflow.org/,"The https://www.tensorflow.org/ site would be greatly improved by a sticky flag (stored in a cookie in the browser) for the version you're using.  This would be used (as a default) for the landing page and search results.  People could choose to set it, or use have some such value as 'stable' for the latest stable release.

I waste a lot of time repeatedly navigating from 1.8 to the right version for the environment I'm working in.  When reading the docs, I prefer to read them for the right version of the API ..."
20453,Tensorflow Lite adds unnecessary permissions.,"When using Tensorflow Lite 0.17.0, it adds `android.permission.READ_PHONE_STATE` and `android.permission.READ_EXTERNAL_STORAGE` because no min sdk was specified.

I suggest to hotfix and release this ASAP because it's adding these permissions to apps that don't need them.

Here is the log from the manifest merger:
```
uses-permission#android.permission.READ_PHONE_STATE
IMPLIED from /home/ph1b/Dev/Yazio/app/src/main/AndroidManifest.xml:2:1-165:12 reason: org.tensorflow.lite has a targetSdkVersion < 4
uses-permission#android.permission.READ_EXTERNAL_STORAGE
IMPLIED from /home/ph1b/Dev/Yazio/app/src/main/AndroidManifest.xml:2:1-165:12 reason: org.tensorflow.lite requested WRITE_EXTERNAL_STORAGE
```"
20452,Keras TimeDistributed with Input and no batch_size fails,"OS: Windows 10 & Ubuntu 16.04 tested
Tensorflow 1.8
Python 3.6.5

The following code shows the problem.
When using `Input` with a `batch_size` everything works fine, but without it it fails.
I assumed that when setting the `batch_size` in `batch_shape` to `None` or using `shape` instead that the `batch_size` would then be dynamic.

```python
from tensorflow.python.keras.layers import Input, Lambda, TimeDistributed
from tensorflow.python.keras import backend as K


i_batch = Input(batch_shape=(32, 18, 64, 512))
i_batch_none_error = Input(batch_shape=(None, 18, 64, 512))
i_error = Input(shape=(18, 64, 512))


def reduce_sum(x):
  return K.sum(x, axis=-2)


sumpool = Lambda(reduce_sum, output_shape=(512,))
# Computes correct shape (?, 18, 512)
print(TimeDistributed(sumpool, name='t')(i_batch))

# Both throw warnings
# Computes incorrect shape (?, 18, 64, 512)
print(TimeDistributed(sumpool, name='t')(i_batch_none_error))
# Computes incorrect shape (?, 18, 64, 512)
print(TimeDistributed(sumpool, name='t')(i_error))
```"
20451,"with quantized-training model,PC ok but tf-lite failed.","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux  4.17.2-1-ARCH SMP
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:b'v1.8.0-3238-g52bf2fe0f6' 1.9.0-rc0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**:0.14.1- (@non-git)
- **GCC/Compiler version (if compiling from source)**:gcc-7.1
- **CUDA/cuDNN version**:cuda-9.2 cuDNN-7.1
- **GPU model and memory**:16G
- **Exact command to reproduce**:

I trained **label_image** on mobilenetv2 backbone with quantization and everything works well on PC.
Then I tried to convert it to tf-lite,even the converting processing is well-down(**no error,no unsupported ops**),but when I finally ran it I got **tensorflow/contrib/lite/kernels/conv.cc:260 real_multiplier < 1.0 was not true**
This the log:
`
tensors size: 174
nodes size: 66
inputs: 1
input(0) name: input
0: MobilenetV2/Conv/Conv2D_Fold_bias, 128, 2, 0.0408043, 0
1: MobilenetV2/Conv/Relu6, 100352, 3, 0.0235285, 0
2: MobilenetV2/Conv/weights_quant/FakeQuantWithMinMaxVars, 864, 3, 0.0408043, 121
3: MobilenetV2/Conv_1/Conv2D_Fold_bias, 5120, 2, 0.00108845, 0
4: MobilenetV2/Conv_1/Relu6, 20480, 3, 0.0235285, 0
5: MobilenetV2/Conv_1/weights_quant/FakeQuantWithMinMaxVars, 409600, 3, 0.00680513, 119
6: MobilenetV2/Logits/AvgPool, 1280, 3, 0.0235285, 0
7: MobilenetV2/Logits/Conv2d_1c_1x1/BiasAdd, 3, 3, 0.174401, 120
8: MobilenetV2/Logits/Conv2d_1c_1x1/Conv2D_bias, 12, 2, 2.91482e-05, 0
9: MobilenetV2/Logits/Conv2d_1c_1x1/weights_quant/FakeQuantWithMinMaxVars, 3840, 3, 0.00123885, 125
10: MobilenetV2/Logits/Squeeze, 3, 3, 0.174401, 120
11: MobilenetV2/Logits/Squeeze_shape, 8, 2, 0, 0
12: MobilenetV2/Predictions/Reshape_1, 3, 3, 0.00390625, 0
13: MobilenetV2/expanded_conv/depthwise/Relu6, 100352, 3, 0.0235285, 0
14: MobilenetV2/expanded_conv/depthwise/depthwise_Fold_bias, 128, 2, 0.00805492, 0
15: MobilenetV2/expanded_conv/depthwise/weights_quant/FakeQuantWithMinMaxVars, 288, 3, 0.342348, 165
16: MobilenetV2/expanded_conv/project/Conv2D_Fold_bias, 64, 2, 0.00091254, 0
17: MobilenetV2/expanded_conv/project/add_fold, 50176, 3, 0.354141, 130
18: MobilenetV2/expanded_conv/project/weights_quant/FakeQuantWithMinMaxVars, 512, 3, 0.0387845, 150
19: MobilenetV2/expanded_conv_1/depthwise/Relu6, 75264, 3, 0.0235285, 0
20: MobilenetV2/expanded_conv_1/depthwise/depthwise_Fold_bias, 384, 2, 0.00060018, 0
21: MobilenetV2/expanded_conv_1/depthwise/weights_quant/FakeQuantWithMinMaxVars, 864, 3, 0.0255087, 109
22: MobilenetV2/expanded_conv_1/expand/Conv2D_Fold_bias, 384, 2, 0.00352435, 0
23: MobilenetV2/expanded_conv_1/expand/Relu6, 301056, 3, 0.0235285, 0
24: MobilenetV2/expanded_conv_1/expand/weights_quant/FakeQuantWithMinMaxVars, 1536, 3, 0.00995183, 126
25: MobilenetV2/expanded_conv_1/project/Conv2D_Fold_bias, 96, 2, 0.000607076, 0
26: MobilenetV2/expanded_conv_1/project/add_fold, 18816, 3, 0.294347, 131
27: MobilenetV2/expanded_conv_1/project/weights_quant/FakeQuantWithMinMaxVars, 2304, 3, 0.0258018, 151
28: MobilenetV2/expanded_conv_10/depthwise/Relu6, 18816, 3, 0.0235285, 0
29: MobilenetV2/expanded_conv_10/depthwise/depthwise_Fold_bias, 1536, 2, 0.000711485, 0
30: MobilenetV2/expanded_conv_10/depthwise/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0302393, 140
31: MobilenetV2/expanded_conv_10/expand/Conv2D_Fold_bias, 1536, 2, 0.000350018, 0
32: MobilenetV2/expanded_conv_10/expand/Relu6, 18816, 3, 0.0235285, 0
33: MobilenetV2/expanded_conv_10/expand/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.00174141, 149
34: MobilenetV2/expanded_conv_10/project/Conv2D_Fold_bias, 384, 2, 0.000179579, 0
35: MobilenetV2/expanded_conv_10/project/add_fold, 4704, 3, 0.150007, 128
36: MobilenetV2/expanded_conv_10/project/weights_quant/FakeQuantWithMinMaxVars, 36864, 3, 0.00763239, 134
37: MobilenetV2/expanded_conv_11/add, 4704, 3, 0.149921, 125
38: MobilenetV2/expanded_conv_11/depthwise/Relu6, 28224, 3, 0.0235285, 0
39: MobilenetV2/expanded_conv_11/depthwise/depthwise_Fold_bias, 2304, 2, 0.0013777, 0
40: MobilenetV2/expanded_conv_11/depthwise/weights_quant/FakeQuantWithMinMaxVars, 5184, 3, 0.0585545, 92
41: MobilenetV2/expanded_conv_11/expand/Conv2D_Fold_bias, 2304, 2, 0.000252474, 0
42: MobilenetV2/expanded_conv_11/expand/Relu6, 28224, 3, 0.0235285, 0
43: MobilenetV2/expanded_conv_11/expand/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.00168308, 133
44: MobilenetV2/expanded_conv_11/project/Conv2D_Fold_bias, 384, 2, 0.000203408, 0
45: MobilenetV2/expanded_conv_11/project/add_fold, 4704, 3, 0.102331, 126
46: MobilenetV2/expanded_conv_11/project/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.00864519, 141
47: MobilenetV2/expanded_conv_12/add, 4704, 3, 0.213277, 145
48: MobilenetV2/expanded_conv_12/depthwise/Relu6, 28224, 3, 0.0235285, 0
49: MobilenetV2/expanded_conv_12/depthwise/depthwise_Fold_bias, 2304, 2, 0.00214226, 0
50: MobilenetV2/expanded_conv_12/depthwise/weights_quant/FakeQuantWithMinMaxVars, 5184, 3, 0.0910496, 173
51: MobilenetV2/expanded_conv_12/expand/Conv2D_Fold_bias, 2304, 2, 0.000217147, 0
52: MobilenetV2/expanded_conv_12/expand/Relu6, 28224, 3, 0.0235285, 0
53: MobilenetV2/expanded_conv_12/expand/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.00144841, 139
54: MobilenetV2/expanded_conv_12/project/Conv2D_Fold_bias, 384, 2, 0.000596996, 0
55: MobilenetV2/expanded_conv_12/project/add_fold, 4704, 3, 0.170068, 144
56: MobilenetV2/expanded_conv_12/project/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.0253734, 149
57: MobilenetV2/expanded_conv_13/depthwise/Relu6, 9216, 3, 0.0235285, 0
58: MobilenetV2/expanded_conv_13/depthwise/depthwise_Fold_bias, 2304, 2, 0.00034101, 0
59: MobilenetV2/expanded_conv_13/depthwise/weights_quant/FakeQuantWithMinMaxVars, 5184, 3, 0.0144935, 90
60: MobilenetV2/expanded_conv_13/expand/Conv2D_Fold_bias, 2304, 2, 0.000297714, 0
61: MobilenetV2/expanded_conv_13/expand/Relu6, 28224, 3, 0.0235285, 0
62: MobilenetV2/expanded_conv_13/expand/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.0013959, 122
63: MobilenetV2/expanded_conv_13/project/Conv2D_Fold_bias, 640, 2, 0.000194787, 0
64: MobilenetV2/expanded_conv_13/project/add_fold, 2560, 3, 0.144367, 122
65: MobilenetV2/expanded_conv_13/project/weights_quant/FakeQuantWithMinMaxVars, 92160, 3, 0.00827876, 139
66: MobilenetV2/expanded_conv_14/add, 2560, 3, 0.150496, 130
67: MobilenetV2/expanded_conv_14/depthwise/Relu6, 15360, 3, 0.0235285, 0
68: MobilenetV2/expanded_conv_14/depthwise/depthwise_Fold_bias, 3840, 2, 0.0010313, 0
69: MobilenetV2/expanded_conv_14/depthwise/weights_quant/FakeQuantWithMinMaxVars, 8640, 3, 0.0438318, 148
70: MobilenetV2/expanded_conv_14/expand/Conv2D_Fold_bias, 3840, 2, 0.000355036, 0
71: MobilenetV2/expanded_conv_14/expand/Relu6, 15360, 3, 0.0235285, 0
72: MobilenetV2/expanded_conv_14/expand/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.00245926, 111
73: MobilenetV2/expanded_conv_14/project/Conv2D_Fold_bias, 640, 2, 0.000174049, 0
74: MobilenetV2/expanded_conv_14/project/add_fold, 2560, 3, 0.0903757, 130
75: MobilenetV2/expanded_conv_14/project/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.00739738, 137
76: MobilenetV2/expanded_conv_15/add, 2560, 3, 0.300834, 122
77: MobilenetV2/expanded_conv_15/depthwise/Relu6, 15360, 3, 0.0235285, 0
78: MobilenetV2/expanded_conv_15/depthwise/depthwise_Fold_bias, 3840, 2, 0.00129889, 0
79: MobilenetV2/expanded_conv_15/depthwise/weights_quant/FakeQuantWithMinMaxVars, 8640, 3, 0.0552048, 110
80: MobilenetV2/expanded_conv_15/expand/Conv2D_Fold_bias, 3840, 2, 0.000226284, 0
81: MobilenetV2/expanded_conv_15/expand/Relu6, 15360, 3, 0.0235285, 0
82: MobilenetV2/expanded_conv_15/expand/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.00150359, 99
83: MobilenetV2/expanded_conv_15/project/Conv2D_Fold_bias, 640, 2, 0.000805016, 0
84: MobilenetV2/expanded_conv_15/project/add_fold, 2560, 3, 0.226103, 131
85: MobilenetV2/expanded_conv_15/project/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.0342145, 139
86: MobilenetV2/expanded_conv_16/depthwise/Relu6, 15360, 3, 0.0235285, 0
87: MobilenetV2/expanded_conv_16/depthwise/depthwise_Fold_bias, 3840, 2, 0.0040495, 0
88: MobilenetV2/expanded_conv_16/depthwise/weights_quant/FakeQuantWithMinMaxVars, 8640, 3, 0.17211, 201
89: MobilenetV2/expanded_conv_16/expand/Conv2D_Fold_bias, 3840, 2, 0.000576843, 0
90: MobilenetV2/expanded_conv_16/expand/Relu6, 15360, 3, 0.0235285, 0
91: MobilenetV2/expanded_conv_16/expand/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.00191748, 125
92: MobilenetV2/expanded_conv_16/project/Conv2D_Fold_bias, 1280, 2, 0.000119162, 0
93: MobilenetV2/expanded_conv_16/project/add_fold, 5120, 3, 0.159945, 146
94: MobilenetV2/expanded_conv_16/project/weights_quant/FakeQuantWithMinMaxVars, 307200, 3, 0.0050646, 130
95: MobilenetV2/expanded_conv_2/add, 18816, 3, 0.376629, 129
96: MobilenetV2/expanded_conv_2/depthwise/Relu6, 112896, 3, 0.0235285, 0
97: MobilenetV2/expanded_conv_2/depthwise/depthwise_Fold_bias, 576, 2, 0.00397992, 0
98: MobilenetV2/expanded_conv_2/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1296, 3, 0.169153, 51
99: MobilenetV2/expanded_conv_2/expand/Conv2D_Fold_bias, 576, 2, 0.00106746, 0
100: MobilenetV2/expanded_conv_2/expand/Relu6, 112896, 3, 0.0235285, 0
101: MobilenetV2/expanded_conv_2/expand/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.00362654, 142
102: MobilenetV2/expanded_conv_2/project/Conv2D_Fold_bias, 96, 2, 0.000610138, 0
103: MobilenetV2/expanded_conv_2/project/add_fold, 18816, 3, 0.342911, 133
104: MobilenetV2/expanded_conv_2/project/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0259319, 129
105: MobilenetV2/expanded_conv_3/depthwise/Relu6, 28224, 3, 0.0235285, 0
106: MobilenetV2/expanded_conv_3/depthwise/depthwise_Fold_bias, 576, 2, 0.000397524, 0
107: MobilenetV2/expanded_conv_3/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1296, 3, 0.0168954, 126
108: MobilenetV2/expanded_conv_3/expand/Conv2D_Fold_bias, 576, 2, 0.00108431, 0
109: MobilenetV2/expanded_conv_3/expand/Relu6, 112896, 3, 0.0235285, 0
110: MobilenetV2/expanded_conv_3/expand/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.00287898, 107
111: MobilenetV2/expanded_conv_3/project/Conv2D_Fold_bias, 128, 2, 0.000396253, 0
112: MobilenetV2/expanded_conv_3/project/add_fold, 6272, 3, 0.20811, 126
113: MobilenetV2/expanded_conv_3/project/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.0168414, 109
114: MobilenetV2/expanded_conv_4/add, 6272, 3, 0.250818, 134
115: MobilenetV2/expanded_conv_4/depthwise/Relu6, 37632, 3, 0.0235285, 0
116: MobilenetV2/expanded_conv_4/depthwise/depthwise_Fold_bias, 768, 2, 0.00236945, 0
117: MobilenetV2/expanded_conv_4/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1728, 3, 0.100705, 79
118: MobilenetV2/expanded_conv_4/expand/Conv2D_Fold_bias, 768, 2, 0.000398864, 0
119: MobilenetV2/expanded_conv_4/expand/Relu6, 37632, 3, 0.0235285, 0
120: MobilenetV2/expanded_conv_4/expand/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.0019166, 151
121: MobilenetV2/expanded_conv_4/project/Conv2D_Fold_bias, 128, 2, 0.000489459, 0
122: MobilenetV2/expanded_conv_4/project/add_fold, 6272, 3, 0.200968, 132
123: MobilenetV2/expanded_conv_4/project/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.0208029, 147
124: MobilenetV2/expanded_conv_5/add, 6272, 3, 0.276968, 127
125: MobilenetV2/expanded_conv_5/depthwise/Relu6, 37632, 3, 0.0235285, 0
126: MobilenetV2/expanded_conv_5/depthwise/depthwise_Fold_bias, 768, 2, 0.00202895, 0
127: MobilenetV2/expanded_conv_5/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1728, 3, 0.086234, 63
128: MobilenetV2/expanded_conv_5/expand/Conv2D_Fold_bias, 768, 2, 0.000362468, 0
129: MobilenetV2/expanded_conv_5/expand/Relu6, 37632, 3, 0.0235285, 0
130: MobilenetV2/expanded_conv_5/expand/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.00144514, 120
131: MobilenetV2/expanded_conv_5/project/Conv2D_Fold_bias, 128, 2, 0.000435147, 0
132: MobilenetV2/expanded_conv_5/project/add_fold, 6272, 3, 0.205061, 128
133: MobilenetV2/expanded_conv_5/project/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.0184945, 128
134: MobilenetV2/expanded_conv_6/depthwise/Relu6, 9408, 3, 0.0235285, 0
135: MobilenetV2/expanded_conv_6/depthwise/depthwise_Fold_bias, 768, 2, 0.000267618, 0
136: MobilenetV2/expanded_conv_6/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1728, 3, 0.0113742, 126
137: MobilenetV2/expanded_conv_6/expand/Conv2D_Fold_bias, 768, 2, 0.000528109, 0
138: MobilenetV2/expanded_conv_6/expand/Relu6, 37632, 3, 0.0235285, 0
139: MobilenetV2/expanded_conv_6/expand/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.00190675, 128
140: MobilenetV2/expanded_conv_6/project/Conv2D_Fold_bias, 256, 2, 0.00033262, 0
141: MobilenetV2/expanded_conv_6/project/add_fold, 3136, 3, 0.182997, 123
142: MobilenetV2/expanded_conv_6/project/weights_quant/FakeQuantWithMinMaxVars, 12288, 3, 0.0141369, 135
143: MobilenetV2/expanded_conv_7/add, 3136, 3, 0.182534, 120
144: MobilenetV2/expanded_conv_7/depthwise/Relu6, 18816, 3, 0.0235285, 0
145: MobilenetV2/expanded_conv_7/depthwise/depthwise_Fold_bias, 1536, 2, 0.00131166, 0
146: MobilenetV2/expanded_conv_7/depthwise/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0557479, 130
147: MobilenetV2/expanded_conv_7/expand/Conv2D_Fold_bias, 1536, 2, 0.000259114, 0
148: MobilenetV2/expanded_conv_7/expand/Relu6, 18816, 3, 0.0235285, 0
149: MobilenetV2/expanded_conv_7/expand/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.00141595, 134
150: MobilenetV2/expanded_conv_7/project/Conv2D_Fold_bias, 256, 2, 0.000434401, 0
151: MobilenetV2/expanded_conv_7/project/add_fold, 3136, 3, 0.150338, 112
152: MobilenetV2/expanded_conv_7/project/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.0184628, 129
153: MobilenetV2/expanded_conv_8/add, 3136, 3, 0.182103, 121
154: MobilenetV2/expanded_conv_8/depthwise/Relu6, 18816, 3, 0.0235285, 0
155: MobilenetV2/expanded_conv_8/depthwise/depthwise_Fold_bias, 1536, 2, 0.000993556, 0
156: MobilenetV2/expanded_conv_8/depthwise/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0422278, 135
157: MobilenetV2/expanded_conv_8/expand/Conv2D_Fold_bias, 1536, 2, 0.000266144, 0
158: MobilenetV2/expanded_conv_8/expand/Relu6, 18816, 3, 0.0235285, 0
159: MobilenetV2/expanded_conv_8/expand/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.00145805, 128
160: MobilenetV2/expanded_conv_8/project/Conv2D_Fold_bias, 256, 2, 0.000281633, 0
161: MobilenetV2/expanded_conv_8/project/add_fold, 3136, 3, 0.122043, 130
162: MobilenetV2/expanded_conv_8/project/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.0119699, 127
163: MobilenetV2/expanded_conv_9/add, 3136, 3, 0.200997, 130
164: MobilenetV2/expanded_conv_9/depthwise/Relu6, 18816, 3, 0.0235285, 0
165: MobilenetV2/expanded_conv_9/depthwise/depthwise_Fold_bias, 1536, 2, 0.000984803, 0
166: MobilenetV2/expanded_conv_9/depthwise/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0418558, 151
167: MobilenetV2/expanded_conv_9/expand/Conv2D_Fold_bias, 1536, 2, 0.000224455, 0
168: MobilenetV2/expanded_conv_9/expand/Relu6, 18816, 3, 0.0235285, 0
169: MobilenetV2/expanded_conv_9/expand/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.00123257, 123
170: MobilenetV2/expanded_conv_9/project/Conv2D_Fold_bias, 256, 2, 0.000418117, 0
171: MobilenetV2/expanded_conv_9/project/add_fold, 3136, 3, 0.157535, 127
172: MobilenetV2/expanded_conv_9/project/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.0177707, 145
173: input, 37632, 3, 1, 128
number of inputs: 1
number of outputs: 1
tensorflow/contrib/lite/kernels/conv.cc:260 real_multiplier < 1.0 was not true.
`

**I just add** `tf.contrib.quantize.create_training_graph( input_graph=tf.get_default_graph(),         quant_delay=FLAGS.quan_delay)` in training and ` tf.contrib.quantize.create_eval_graph()`
in evaluation compared to the official version.
Is there any extra processes I need to care? Anyone help ?  "
20450,"ImportError: dlopen(/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib   Referenced from: /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so   Reason: image not found",This is my issue https://stackoverflow.com/questions/51121273/tensorflow-error-reason-image-not-found
20449,    .pb Model in android is not detecting images,"Using Window 10
Tensoflow installed from its main site.
2.0 version
Using Android studio
No Bazel
I am using following code to train my dataset. The code first downloads the requested model and then create .pb file with label text file. The link is as follows:
https://github.com/loicmarie/sign-language-alphabet-recognizer
After converting my model into .pb file and getting labels, I loaded my model into following android code:
https://github.com/Nilhcem/tensorflow-classifier-android
Then I make little changes to code according to my model.
![screenshot 92](https://user-images.githubusercontent.com/32578887/42205475-0617f714-7ebe-11e8-9390-a5248d8362fd.png)
Please, where I am wrong what I need to make image detection possible. As it is not giving labels...
"
20448,sess.run a list with assign ?,"```
import tensorflow as tf

sess = tf.Session()

x = tf.Variable(0.)
y = tf.assign(x, 1)
z = x**2
g = tf.gradients(z, [x])[0]

sess.run(tf.global_variables_initializer())

sess.run([g, y, g])
```

the code return [2, 1, 2]. but what I expect is [0, 1, 2]. 

if I run `[sess.run(i) for i in [g, y, g]]`, it return [0, 1, 2], but I want to get [0, 1, 2] in one `sess.run`.

is it possible?"
20447,Support NCCL2,"Current TF use nccl1, however nccl2 can support inter-node aggregation, which is desirable for distributed computation, is there a simple way to modify default settings for using nccl2 ops?"
20445,tensorflow installation issue,"I am trying to install tensorflow on ubuntu 16.04 and it always gives me the error when i want to import it regardlesss of the face that it shows it is installed successfully.
I installed it as follows:
```
alireza@ochoa2:~$ sudo -H pip3 install tensorflow-gpu
Collecting tensorflow-gpu
  Downloading https://files.pythonhosted.org/packages/f2/fa/01883fee1cdb4682bbd188edc26da5982c459e681543bb7f99299fca8800/tensorflow_gpu-1.8.0-cp35-cp35m-manylinux1_x86_64.whl (216.3MB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 216.3MB 228kB/s 
Collecting astor>=0.6.0 (from tensorflow-gpu)
  Downloading https://files.pythonhosted.org/packages/b2/91/cc9805f1ff7b49f620136b3a7ca26f6a1be2ed424606804b0fbcf499f712/astor-0.6.2-py2.py3-none-any.whl
Collecting gast>=0.2.0 (from tensorflow-gpu)
  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz
Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.5/dist-packages (from tensorflow-gpu) (1.13.3)
Collecting tensorboard<1.9.0,>=1.8.0 (from tensorflow-gpu)
  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.1MB 9.7MB/s 
Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.5/dist-packages (from tensorflow-gpu) (0.30.0)
Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-gpu) (3.4.0)
Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-gpu) (1.11.0)
Collecting termcolor>=1.1.0 (from tensorflow-gpu)
  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz
Collecting absl-py>=0.1.6 (from tensorflow-gpu)
  Downloading https://files.pythonhosted.org/packages/57/8d/6664518f9b6ced0aa41cf50b989740909261d4c212557400c48e5cda0804/absl-py-0.2.2.tar.gz (82kB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92kB 17.6MB/s 
Collecting grpcio>=1.8.6 (from tensorflow-gpu)
  Downloading https://files.pythonhosted.org/packages/8a/c1/d5af6bd4d0d90804a1c48f8e52e428fba587d23880d66ad47eaf62b1adc7/grpcio-1.13.0-cp35-cp35m-manylinux1_x86_64.whl (9.1MB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.1MB 5.0MB/s 
Collecting html5lib==0.9999999 (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu)
  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 23.7MB/s 
Collecting werkzeug>=0.11.10 (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu)
  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 327kB 22.5MB/s 
Collecting markdown>=2.6.8 (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu)
  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (78kB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 19.1MB/s 
Collecting bleach==1.5.0 (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu)
  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl
Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from protobuf>=3.4.0->tensorflow-gpu) (36.6.0)
Building wheels for collected packages: gast, termcolor, absl-py, html5lib
  Running setup.py bdist_wheel for gast ... done
  Stored in directory: /root/.cache/pip/wheels/9a/1f/0e/3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f
  Running setup.py bdist_wheel for termcolor ... done
  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6
  Running setup.py bdist_wheel for absl-py ... done
  Stored in directory: /root/.cache/pip/wheels/a0/f8/e9/1933dbb3447ea6ef57062fd5461cb118deb8c2ed074e8344bf
  Running setup.py bdist_wheel for html5lib ... done
  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29
Successfully built gast termcolor absl-py html5lib
Installing collected packages: astor, gast, html5lib, werkzeug, markdown, bleach, tensorboard, termcolor, absl-py, grpcio, tensorflow-gpu
  Found existing installation: html5lib 0.999
    Uninstalling html5lib-0.999:
      Successfully uninstalled html5lib-0.999
Successfully installed absl-py-0.2.2 astor-0.6.2 bleach-1.5.0 gast-0.2.0 grpcio-1.13.0 html5lib-0.9999999 markdown-2.6.11 tensorboard-1.8.0 tensorflow-gpu-1.8.0 termcolor-1.1.0 werkzeug-0.14.1
```

but then when i do `python -c 'import tensorflow'`


it shows this:
```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/alireza/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/alireza/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/alireza/anaconda3/lib/python3.6/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""/home/alireza/anaconda3/lib/python3.6/site-packages/google/protobuf/descriptor.py"", line 46, in <module>
    from google.protobuf.pyext import _message
ImportError: /home/alireza/anaconda3/lib/python3.6/site-packages/google/protobuf/pyext/_message.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZNK6google8protobuf10TextFormat17FieldValuePrinter9PrintBoolB5cxx11Eb
```

Have I written custom code: No
OS Platform and Distribution: Ubuntu 16.04
TensorFlow installed from: pip
TensorFlow version:latest available 
Bazel version: no bazel
CUDA/cuDNN version 9.0/
GPU model and memory geforce gtx 1080
Exact command to reproduce: mentioned above"
20444,Unable to install TensorFlow on Python3.7 with pip,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `pip install tensorflow`

### Describe the problem
Installing TensorFlow on **Python3.7** with pip failed. Please see the failure log below.


### Source code / logs
> Could not find a version that satisfies the requirement tensorflow (from versions: )
> No matching distribution found for tensorflow

"
20441,Convert keras model from pb file to tflite file,"

I am trying to convert a trained keras model from .pb file to .tflite file. 

tflite_model = tf.contrib.lite.toco_convert(frozen_graph_def, input_tensors, output_tensors)

However, when I run this code, there is an error that
AttributeError: module 'tensorflow.contrib.lite' has no attribute 'toco_convert'
How to solve this problem ,thanks."
20440,Unable to import Tensorflow after a successful installation on Windows 7 64bit,"System information
-------------------------------------------------------------------
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): >>> import tensorflow as tf  (from example script)
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 7 64bit
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.8
Python version: 3.6.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: import tensorflow as tf 

After much fruitless effort to install the java version of Tensorflow on my windows machine, i decide to give the python a trial.
System Information
OS: Windows 7 64bit,
Graphics:   Intel HD Graphics 
-------------------------------
First i download Python 3.6.6 and install. After setting the path, i tested and everything works fine:
C:\Users\Luke>python
Python 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>>
Next, run the Tensorflow installation command from the Terminal window:
C:\Windows\system32>cd\
C:\>pip3 install --upgrade tensorflow
Collecting tensorflow
  Downloading https://files.pythonhosted.org/packages/f4/88/980d7032b7408fcca5b0b8d420fcd97919197a9e7acf280ab74fc7db6993/tensorflow-1.8.0-cp36-cp36m-win_amd64.whl (34.4MB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34.4MB 23kB/s
Collecting termcolor>=1.1.0 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz
Collecting gast>=0.2.0 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz
Collecting numpy>=1.13.3 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/0d/b7/0c804e0bcba6505f8392d042d5e333a5e06f308e019517111fbc7767a0bc/numpy-1.14.5-cp36-none-win_amd64.whl (13.4MB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.4MB 82kB/s
Collecting absl-py>=0.1.6 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/57/8d/6664518f9b6ced0aa41cf50b989740909261d4c212557400c48e5cda0804/absl-py-0.2.2.tar.gz (82kB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92kB 55kB/s
Collecting tensorboard<1.9.0,>=1.8.0 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.1MB 32kB/s
Collecting grpcio>=1.8.6 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/d5/c6/15728549704f9c03db7179b7f99303b91b7703e18a50f5e7b47e59b289ea/grpcio-1.13.0-cp36-cp36m-win_amd64.whl (1.4MB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4MB 42kB/s
Collecting wheel>=0.26 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/81/30/e935244ca6165187ae8be876b6316ae201b71485538ffac1d718843025a9/wheel-0.31.1-py2.py3-none-any.whl (41kB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 60kB/s
Collecting protobuf>=3.4.0 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/75/7a/0dba607e50b97f6a89fa3f96e23bf56922fa59d748238b30507bfe361bbc/protobuf-3.6.0-cp36-cp36m-win_amd64.whl (1.1MB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 45kB/s
Collecting six>=1.10.0 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl
Collecting astor>=0.6.0 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/b2/91/cc9805f1ff7b49f620136b3a7ca26f6a1be2ed424606804b0fbcf499f712/astor-0.6.2-py2.py3-none-any.whl
Collecting markdown>=2.6.8 (from tensorboard<1.9.0,>=1.8.0->tensorflow)
  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (78kB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 37kB/s
Collecting html5lib==0.9999999 (from tensorboard<1.9.0,>=1.8.0->tensorflow)
  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 43kB/s
Collecting werkzeug>=0.11.10 (from tensorboard<1.9.0,>=1.8.0->tensorflow)
  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 327kB 29kB/s
Collecting bleach==1.5.0 (from tensorboard<1.9.0,>=1.8.0->tensorflow)
  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl
Requirement not upgraded as not directly required: setuptools in c:\users\luke\appdata\local\programs\python\python36\lib\site-packages (from protobuf>=3.4.0->tensorflow) (39.0.1)
Installing collected packages: termcolor, gast, numpy, six, absl-py, markdown, html5lib, werkzeug, bleach, wheel, protobuf, tensorboard, grpcio, astor, tensorflow
  Running setup.py install for termcolor ... done
  Running setup.py install for gast ... done
  Running setup.py install for absl-py ... done
  The script markdown_py.exe is installed in 'c:\users\luke\appdata\local\programs\python\python36\Scripts' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  Running setup.py install for html5lib ... done
  The script wheel.exe is installed in 'c:\users\luke\appdata\local\programs\python\python36\Scripts' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  The script tensorboard.exe is installed in 'c:\users\luke\appdata\local\programs\python\python36\Scripts' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  The scripts freeze_graph.exe, saved_model_cli.exe, tensorboard.exe, toco.exe and toco_from_protos.exe are installed in 'c:\users\luke\appdata\local\programs\python\python36\Scrip
ts' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed absl-py-0.2.2 astor-0.6.2 bleach-1.5.0 gast-0.2.0 grpcio-1.13.0 html5lib-0.9999999 markdown-2.6.11 numpy-1.14.5 protobuf-3.6.0 six-1.11.0 tensorboard-1.8.0 t
ensorflow-1.8.0 termcolor-1.1.0 werkzeug-0.14.1 wheel-0.31.1
---------------------------------------------------------------------------------
From what i observed after the installation, i then added C:\users\luke\appdata\local\programs\python\python36\Scripts in my path ENV directly.
After that i now open another Terminal and run the sample test.
I first import the Tensorflow with this command:
C:\Users\Luke>cd\

C:\>python
Python 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\Lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\Lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\Lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Luke\AppData\Local\Programs\Python\Python36\Lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>
----------------------------------------------------------------------
The error code i have: -1073741795 does not match with any of those listed in the documentation table.
[I followed this official documentation](https://www.tensorflow.org/install/install_windows#Common installation problems)
I need community assistance please."
20439,Import Error for Tensorflow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 - 1803
- **TensorFlow installed from (source or binary)**:  Binary
- **TensorFlow version (use command below)**:  1.8.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**:N/A
- **Exact command to reproduce**:  import tensorflow

### Describe the problem
**I am consistently getting an **import error** when I enter - import tensorflow. I have tried reinstalling. But I am unfortunately limited by my experience and expertise, so I request help.**

**After some research -** 
I have found that **AVX** hardware support is necessary #17386 
So,
**System Specs**

- AMD Athlon X2 250
- Nvidia GT 710 _(doesn't support Cuda)_

Can you confirm on this? I can't find enough to confirm. 
### Source code / logs

`>>> import tensorflow`

```
Traceback (most recent call last):
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\mishr\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
20438,import tensorflow error in windows 10 ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
windows 10
cuda 9.0
cudnn 7
tensorflow 1.8.0

### Describe the problem
I got the error message when I'm typing import tensorflow on windows.
How can I solve this problem?

### Source code / logs



ImportError: DLL load failed:"
20436,Memory leaking in tf.data.Dataset in eager mode,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**:  3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 9.0
- **GPU model and memory**: 
- **Exact command to reproduce**:

### Describe the problem

`tf.data.Dataset` doesn't seem to free its own memory usage even it is out of scope in eager execution mode.

The memory I'm mentioning is the main memory not GPU's memory.

### Source code / logs

The following codes are run in Jupyterlab.

Block 1: 
```
import tensorflow as tf
import numpy as np

tf.enable_eager_execution()
```

Block 2: 
```
def run():
    data = tf.data.Dataset.from_tensor_slices(np.zeros((60000, 32, 32, 3))) 
    data = data.shuffle(10000)
    data = data.batch(128)
    x = next(iter(data))

run()
```

I repeat Block 2 many times and the memory usage grows each time.

Note: `gc.collect()` doesn't have any effect."
20435,Feature request: can you add formula of LSTM to docs?,"Have I written custom codeï¼š No
OS Platform and Distribution: Linux 16.04
TensorFlow installed from: pip install tensorflow-gpu
TensorFlow version: 1.8.0
CUDA/cuDNN version: 9.0

### Describe the problem
Hi,
I have trained a bidirectional lstm(256 hidden units) model in pytorch and I want to transfer it to tensorflow. 
In pytorch,  each LSTM has two weights and two bias:
```
rnn.0.rnn.weight_ih_l0 torch.Size([1024, 512])
rnn.0.rnn.weight_hh_l0 torch.Size([1024, 256])
rnn.0.rnn.bias_ih_l0 torch.Size([1024])
rnn.0.rnn.bias_hh_l0 torch.Size([1024])

weight_ih_l[k] â€“ the learnable input-hidden weights of the kth layer (W_ii|W_if|W_ig|W_io), of shape (4*hidden_size x input_size)
weight_hh_l[k] â€“ the learnable hidden-hidden weights of the kth layer (W_hi|W_hf|W_hg|W_ho), of shape (4*hidden_size x hidden_size)
bias_ih_l[k] â€“ the learnable input-hidden bias of the kth layer (b_ii|b_if|b_ig|b_io), of shape (4*hidden_size)
bias_hh_l[k] â€“ the learnable hidden-hidden bias of the kth layer (b_hi|b_hf|b_hg|b_ho), of shape (4*hidden_size)
```
However in tensorflow, each LSTM has only one weight and one bias:
```
<tf.Variable 'RNN/BiLSTM1/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/basic_lstm_cell/kernel:0' shape=(768, 1024) dtype=float32_ref>,
 <tf.Variable 'RNN/BiLSTM1/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/basic_lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>,
```
I do not know how to convert the parameters in pytorch into tensorflow. Can you add some formula and weight structures of LSTM to docs as what they did in pytorch?
Thanks!

"
20434,something wrong with  tf.image.draw_bounding_boxes,"```
import matplotlib.pyplot as plt
import tensorflow as tf   
import numpy as np
image_raw_data = tf.gfile.FastGFile(""./datasets/cat.jpg"",'rb').read()

with tf.Session() as sess:
    img_data = tf.image.decode_jpeg(image_raw_data)
    boxes = tf.constant([[[0.05, 0.05, 0.9, 0.7], [0.3, 0.4, 0.5, 0.6]]])
    batched = tf.expand_dims(tf.image.convert_image_dtype(img_data, tf.float32), 0)
    image_with_box = tf.image.draw_bounding_boxes(batched, boxes)
    #print (image_with_box[0].eval())
    plt.imshow(image_with_box[0].eval()),plt.title(""result"")
    plt.show()

```
ç»˜åˆ¶çš„ç»“æžœæ˜¯ï¼š
![image](https://user-images.githubusercontent.com/7501074/42121467-536e4792-7c62-11e8-92da-040141d562d3.png)
no matter in windows or linux, it get the same result. i think it is not the issue of the version of tensorflow and the environment, but something very amazing which i do not know. 
Who can help me? Thanks very much"
20433,Support for Multiple Inputs with tf.lite for quantization,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**: N/A
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

`bazel build tensorflow/contrib/lite/toco:toco && \
  ./bazel-bin/third_party/tensorflow/contrib/lite/toco/toco \
  --input_file=frozen_eval_graph.pb \
  --output_file=tflite_model.tflite \
  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \
  --inference_type=QUANTIZED_UINT8 \
  --input_shape=""1,224, 224,3"" \
  --input_array=input \
  --output_array=outputs \
  --std_value=127.5 --mean_value=127.5`

How can i use this framework with a model with multiple inputs and outputs, as input_shape is defined only once here? 

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20431,'VBN' object has no attribute 'dtype',"Its broken or I do wrong something?

version 1.9.0-rc1

i = tf.layers.conv2d_transpose(i, filters, 4, strides=2, activation=tf.nn.leaky_relu, padding='same')
i = tf.contrib.gan.features.VBN(i)"
20428,comparison between signed and unsigned integer,"### System information
No need. It is about comparison warning happening in cc/framework/ops.h
ops.h:153:27: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
       if (t.NumElements() != v.size()) {
                            
cc1plus: all warnings being treated as errors

Unfortunately, due to policy out of my control, 
I cannot change the compiler option to treat warning as an error. 
I hope this to be fixed so that I can compile.

### Describe the problem
Same as above.
### Source code / logs
tensorflow/cc/framework/ops.h
"
20427,Installing tensorflow with both conda and source,"Hi everyone i'm new at tensorflow i have recently installed it with the package manager conda but i was having a warning message saying it was better install it from the source , so after install it from the source here is ths situation 
1/ when I use import tensorflow without using conda (from the source) it doesn't work saying that there is no mudule called tensorflow 
2/ when i use conda (source activate project_name) tensorflow work fine and without any warning anymore
Is it possible to use tensorflow from the source (without conda ) with conda still installed or should i remove conda and how ? 
when i try sudo pip3 install /tmp/tensorflow_pkg/tensorflow-1.8.0-py2-none-any.whl consol says that tensorflow is perfectly installed 
my configuration:
 ubuntu 18.04
tensorflow 1.8.0 for conda and 1.9.0cr0 for the source
pyhton 3.6.5
and i use cpu configuration not the gpu one 

"
20426,`tf.keras.layers` does not work probably when using `variable_scope` in `tf.keras.Model`,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:  pip install
- **TensorFlow version (use command below)**: v1.9.0-rc0-35-g17d6639b55 1.9.0-rc1
- **Python version**:  3.6
- **CUDA/cuDNN version**:  CUDA 9.0 / cuDNN 7
- **GPU model and memory**: GTX 1080
- **Exact command to reproduce**:
```python
import tensorflow as tf

def conv_block(inputs, filters, kernel_size, strides, scope):
  '''Create a simple Conv --> BN --> ReLU6 block'''

  with tf.variable_scope(scope):
    x = tf.keras.layers.Conv2D(filters, kernel_size, strides, name='conv2d')(inputs)
    x = tf.keras.layers.BatchNormalization(name='BN')(x)
    x = tf.keras.layers.Activation(tf.nn.relu6)(x)
    return x

def reproduce_keras_variable_scope_error():

  # Construct a simple model
  inputs = tf.keras.Input(shape=[224, 224, 3], batch_size=1, name='inputs')
  hidden = conv_block(inputs, 32, 3, 2, scope='block_1')
  outputs = conv_block(hidden, 64, 3, 2, scope='block_2')
  
  # This is fine, the tensor scopes are matched as expected
  for v in tf.trainable_variables():
    print('{:20} {}'.format(v.name, v.shape))

  # Problem happens here. Please consult the error output below.
  model = tf.keras.Model(inputs, outputs)
  model.summary()

if __name__ =='__main__':
  reproduce_keras_variable_scope_error()
```
### Describe the problem

The problem is Keras layers do not probably name its variables according the `variable_scope` . Therefore, I cannot call `conv_block` multiple times. 
  * This problem occurs when calling `tf.layers` as well.
  * I would like to use `tf.keras.Model` because I might convert to `Estimator` later.

### Source code / logs
```shell
block_1/conv2d/kernel:0 (3, 3, 3, 32)
block_1/conv2d/bias:0 (32,)
block_1/BN/gamma:0   (32,)
block_1/BN/beta:0    (32,)
block_2/conv2d/kernel:0 (3, 3, 32, 64)
block_2/conv2d/bias:0 (64,)
block_2/BN/gamma:0   (64,)
block_2/BN/beta:0    (64,)
Traceback (most recent call last):
  File ""tf_keras_layer.py"", line 24, in <module>
    reproduce_variable_scope_error()
  File ""tf_keras_layer.py"", line 20, in reproduce_variable_scope_error
    model = tf.keras.Model(inputs, outputs)
  File ""/home/dat/miniconda2/envs/portrait/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 112, in __init__
    super(Model, self).__init__(*args, **kwargs)
  File ""/home/dat/miniconda2/envs/portrait/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 78, in __init__
    self._init_graph_network(*args, **kwargs)
  File ""/home/dat/miniconda2/envs/portrait/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 244, in _init_graph_network
    self.inputs, self.outputs)
  File ""/home/dat/miniconda2/envs/portrait/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1689, in _map_graph_network
    str(all_names.count(name)) + ' times in the model. '
ValueError: The name ""conv2d"" is used 2 times in the model. All layer names should be unique.
```"
20425,"Documentation Suggestion: mention and clarification of `tf.contrib.data.prefetch_to_device()` in ""Input Pipeline Performance Guide""","
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: n/a
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: n/a
- **TensorFlow installed from (source or binary)**: n/a
- **TensorFlow version (use command below)**: n/a
- **Python version**:  n/a
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

Documentation Suggestion: In `https://www.tensorflow.org/performance/datasets_performance` I think it would be worth:

- Adding some mention of `tf.contrib.data.prefetch_to_device()`. This method isn't presented in this document, nor in the basic programming guide at `https://www.tensorflow.org/programmers_guide/datasets`. It's hard to know about this potentially useful method if it's not mentioned. I only learned of it when watching a Youtube video by Derek Murray from the TF Dev Summit 2018.

- In `https://www.tensorflow.org/performance/datasets_performance`, in addition to mentioning `.prefetch_to_device()` I think it would be useful to some to clarify how it's different from `.prefetch()`, perhaps with a useful diagram like the one presented to clarify `.prefetch()`. I've seen several questions online about this point. I think it's clear when one carefully reads the documentation, but some extra clarity wouldn't hurt.

- Also, more examples of how to use these tools would be nice...for example, some before (using `feed_dict` for all data into a model) vs after (using `tf.data.Dataset` and associated tools) would be helpful.

Thank you.
"
20421,Guidelines to have a working C API (libtensorflow.so) on FreeBSD 11.1,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: FreeBSD-11.1
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: github branch r1.9
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: clang (llvm40)
- **Exact command to reproduce**: bazel build //tensorflow:libtensorflow.so

### Describe the problem
Tensorflow does not compile out of the box on FreeBSD-11.1 systems. I provide here the workarounds and adjustements i've done for compiling it. This is not perfect but I can run TensorFlow C programs on FreeBSD without problem. 

I hope it can be useful for someone.

### Source code / logs

The following `static_cast<int64>` generates an error at compile time in tensorflow/core/platform/env.cc, line 350: `int32 tid = static_cast<int32>(static_cast<int64>(pthread_self()));`

static_cast from 'pthread_t' (aka 'pthread *') to 'int64' (aka 'long long') is not allowed

As a workaround I used:
`int32 tid = (int32)((int64)(pthread_self()));` 

A non-existing header file on FreeBSD is included in tensorflow/core/platform/posix/posix_file_system.cc, line 22:
```
#if !defined(__APPLE__)
#include <sys/sendfile.h>
#endif
```
I replaced this code with:
```
#if defined(__linux__) && !defined(__ANDROID__)
#include <sys/sendfile.h>
#endif
```

At link time there are undefined references to backtrace() and backtrace_symbols_fd() functions in tensorflow/core/platform/*. This is because the **LD flag `-lexecinfo` is missing in linker options**

At last, under FreeBSD, the ""libdl"" library does not exists. So the **LD flag `-ldl` should be removed from linker options**.

"
20420,ld: unknown option: -no-as-needed on macOS,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.5
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: b7300de4ef75bdd9373bccc4ae5b98135a70287b
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.15.0-homebrew
- **GCC/Compiler version (if compiling from source)**: LLVM version 9.1.0 (clang-902.0.39.2)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:  see below

### Describe the problem

The compilation fails because Bazel passes an unsupported linker flag:

```
$ bazel build @protobuf_archive//:js_embed
INFO: Analysed target @protobuf_archive//:js_embed (0 packages loaded).
INFO: Found 1 target...
ERROR: [...]/external/protobuf_archive/BUILD:260:1: Linking of rule '@protobuf_archive//:js_embed' failed (Exit 1)
ld: unknown option: -no-as-needed
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target @protobuf_archive//:js_embed failed to build
```
"
20418,Error when using tf.contrib.metrics.count in eval_metric_ops of tf.estimator.EstimatorSpec,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac Os 10.13.5
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**:  tested on v1.8.0-0-g93bc2e2072 1.8.0 and v1.9.0-rc0-35-g17d6639b55 1.9.0-rc1
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: no gpu
- **Exact command to reproduce**: see description

### Describe the problem

Hello,

It seems that tf.contrib.metrics.count does not work into
eval_metric_ops of tf.estimator.EstimatorSpec during validation
(I just wanted to check the size of my validation set when debugging)

It raises
```
~/.virtualenvs/python3/lib/python3.5/site-packages/tensorflow/python/estimator/model_fn.py in _check_is_tensor_or_operation(x, name)
    388 def _check_is_tensor_or_operation(x, name):
    389   if not (isinstance(x, ops.Operation) or isinstance(x, ops.Tensor)):
--> 390     raise TypeError('{} must be Operation or Tensor, given: {}'.format(name, x))
    391 
    392 

TypeError: eval_metric_ops[count] must be Operation or Tensor, given: <tf.Variable 'count/count:0' shape=() dtype=float32_ref>
```

It seems that the first returned argument (count_) in 
https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3722

is type `<tf.Variable 'count/count:0' shape=() dtype=float32_ref>`

Shoud not it be converted into 

`<tf.Tensor 'count/count/read:0' shape=() dtype=float32>`

using math_ops.float32() (or something else ?) like in other metrics definitions ?

### Source code / logs

Somewhere in my generate_model_fn():

```
        if mode == tf.estimator.ModeKeys.EVAL:
            eval_metrics = _create_evaluation_metrics(outputs, labels)
            return tf.estimator.EstimatorSpec(mode, loss=loss, 
                                              eval_metric_ops=eval_metrics)
```

with 

```
def _create_evaluation_metrics(outputs, labels):
    count = tf.contrib.metrics.count(outputs)
    return {
        ""count"": count
    }
```

raises the `TypeError: eval_metric_ops[count] must be Operation or Tensor, given: <tf.Variable 'count/count:0' shape=() dtype=float32_ref>` during validation phase

but works with

```
from tensorflow.python.ops import math_ops

def _create_evaluation_metrics(outputs, labels):
    count, op = tf.contrib.metrics.count(outputs)
    return {
        ""count"": (math_ops.to_float(count), op)
    }
```"
20416,"How to uninstall tensorflow or reset pip installs, to do a fresh install?","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20415,"nnapi error: unable to open library libneuralnetworks.so Model provided has model identifier 'Plac', should be 'TFL3'  2018-06-29 14:52:30.475235+0200 tflite_simple_example[3723:177396] Failed to mmap model ","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Mac OS Sierra)**: Mac OS Sierra
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: Run it as an ios application



### Describe the problem
I try to run the demo application for ios with tensorflow lite. With your model it works, but when I put my model it can not load it.

### Source code / logs
```
2018-06-29 14:52:20.618818+0200 tflite_simple_example[3723:177396] [MC] Lazy loading NSBundle MobileCoreServices.framework
2018-06-29 14:52:20.620767+0200 tflite_simple_example[3723:177396] [MC] Loaded MobileCoreServices.framework
2018-06-29 14:52:20.883003+0200 tflite_simple_example[3723:177396] [MC] System group container for systemgroup.com.apple.configurationprofiles path is /Users/macbook13/Library/Developer/CoreSimulator/Devices/25BA8841-5579-4FBB-ABE4-CEAEF108661B/data/Containers/Shared/SystemGroup/systemgroup.com.apple.configurationprofiles
**nnapi error: unable to open library libneuralnetworks.so**
**Model provided has model identifier 'Plac', should be 'TFL3'**

**2018-06-29 14:52:30.475235+0200 tflite_simple_example[3723:177396] Failed to mmap model my_model_tflite_graph.**
```
Please anyone who can tell me how to fix this."
20414,cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE ... Failed to create session,"Hello everyone, when I execute this code :

```
import tensorflow as tf
hello = tf.constant('hi,tensorflow')
sess = tf.Session()
```

I get each time the same error :

```
2018-06-29 15:14:01.587077: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow bhis actinary was not compiled to use: AVX2 FMA
2018-06-29 15:14:01.694678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-06-29 15:14:01.695507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: Quadro P400 major: 6 minor: 1 memoryClockRate(GHz): 1.2525
pciBusID: 0000:05:00.0
totalMemory: 1.95GiB freeMemory: 1.92GiB
2018-06-29 15:14:01.715820: E tensorflow/core/common_runtime/direct_session.cc:154] Internal: failed initializing StreamExecutor for CUDA device ordinal 1: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE
Traceback (most recent call last):
  File ""/home/mounir/PycharmProjects/ssd_keras-master/ssd512_inference.py"", line 6, in <module>
    sess = tf.Session()
  File ""/home/mounir/anaconda3/envs/gpukeras/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1560, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/mounir/anaconda3/envs/gpukeras/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 633, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.

```
I have two GPUs in my computer, the P400 is the most powerful one.
I tried to troubleshoot it by adding to .bashrc export CUDA_VISIBLE_DEVICES with alternatively these values =0, =1, =0,1, but it outputs always the same error


**Have I written custom code :** 
```
import tensorflow as tf
hello = tf.constant('hi,tensorflow')
sess = tf.Session()
```
**OS Platform and Distribution** :  Linux, Ubuntu 18.04 
**TensorFlow installed from** : PyPI
**TensorFlow version** 1.8.0
**Bazel version** Seems not installed
**CUDA/cuDNN version** Cudatoolkit installed within the virtual env is 9.0 and Cuda installed within the system is 9.1
**GPU model and memory** P400 Quadro, 2 GB
**Exact command to reproduce** 
```
import tensorflow as tf
hello = tf.constant('hi,tensorflow')
sess = tf.Session()
```
"
20411,Boosted trees classifier fails with segmentation fault,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
pip install
- **TensorFlow version (use command below)**:
v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
cuda 9.0, cudnn 7
- **GPU model and memory**:
NVidia V100, aws p3 instance
- **Exact command to reproduce**:
python tf_trees.txt
[tf_trees.txt](https://github.com/tensorflow/tensorflow/files/2149208/tf_trees.txt)


### Describe the problem
I try to train a boosted trees model and it seems to work. When I try to predict on new data, it crashes.

Logs
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Graph was finalized.
2018-06-29 11:27:47.891292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-06-29 11:27:47.891344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-06-29 11:27:47.891359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0
2018-06-29 11:27:47.891373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N
2018-06-29 11:27:47.891523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14867 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)
INFO:tensorflow:Restoring parameters from /data/model.ckpt-110
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
Segmentation fault (core dumped)

"
20408,LoadLibrary crashed at LoadLibraryExW when load custom ops dll on Windows,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7

- **TensorFlow installed from (source or binary)**:
source

- **TensorFlow version (use command below)**:
1.8.0

- **Python version**: 

- **Bazel version (if compiling from source)**:

- **GCC/Compiler version (if compiling from source)**:
VS 2017

- **CUDA/cuDNN version**:

- **GPU model and memory**:

- **Exact command to reproduce**:

write op 
```c++
#if defined(_MSC_VER) && !defined(COMPILER_MSVC)
#define COMPILER_MSVC // Set MSVC visibility of exported symbols in the shared library.
#ifndef NOMINMAX
# define NOMINMAX
#endif
#endif

#include ""tensorflow/core/framework/types.h""
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""

using namespace tensorflow;

REGISTER_OP(""ZeroOut"")
.Input(""to_zero: int32"")
.Output(""zeroed: int32"")
.SetShapeFn([](shape_inference::InferenceContext* c) {
    c->set_output(0, c->input(0));
    return Status::OK();
});

class ZeroOutOp : public OpKernel {
public:
    explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

    void Compute(OpKernelContext* context) override {
        // Grab the input tensor
        const Tensor& input_tensor = context->input(0);
        auto input = input_tensor.flat<int32>();

        // Create an output tensor
        Tensor* output_tensor = NULL;
        OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
            &output_tensor));
        auto output_flat = output_tensor->flat<int32>();

        // Set all but the first element of the output tensor to 0.
        const int N = input.size();
        for (int i = 1; i < N; i++) {
            output_flat(i) = 0;
        }

        // Preserve the first input value if possible.
        if (N > 0) output_flat(0) = input(0);
    }
};

REGISTER_KERNEL_BUILDER(Name(""ZeroOut"").Device(DEVICE_CPU), ZeroOutOp);
```

load ops in C++
```
Env* env = Env::Default();
env->LoadLibrary(library_filename, &library.handle);
```
LoadLibraryExW crashed when load the op.dll

### Describe the problem
LoadLibraryExW crashed when load the op.dll
The above code works fine under Linux, but crashed under windows 7 compiled by VS 2017

### Source code / logs
"
20407,fused_batch_norm's gradient implementation is incomplete,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:N/A
- **TensorFlow installed from (source or binary)**:N/A
- **TensorFlow version (use command below)**:N/A
- **Python version**: N/A
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:N/A
- **Exact command to reproduce**:N/A

When `is_training=True`, the op looks like:
```python
y, batch_mean, batch_var = tf.nn.fused_batch_norm(x, scale, offset)
```
where `y`, `batch_mean` , and `batch_var` all depend on `x`. However the gradients of `batch_mean` and `batch_var` on `x` are currently None.

When `is_training=False`, the op looks like:
```python
y, _, _ = tf.nn.fused_batch_norm(x, scale, offset, mean, variance)
```
where `y` depend on all the inputs. However currently the gradients of `y` on `mean` and `variance` is None.

Both cases are not commonly used in practice, but mathematically there should be gradients."
20405, only supports 'NHWC' format under the GPU mode ,"I use tensorflow 1.8 version.
we konw the gpu should support both 'NHWC' and 'NCHW' but i get bellow errors under GPU mode:

Conv2DCustomBackpropInputOp only supports NHWC.
	 [[Node: model_1/inference/conv2d_transpose/conv2d_transpose = Conv2DBackpropInput[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 16], **use_cudnn_on_gpu=true**, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](model_1/inference/conv2d_transpose/stack, model/inference/conv2d_transpose/kernel/read, model_1/inference/ExpandDims_1)]]"
20404,Where the data is converted into tensor,"Like this:

```
ta = tf.convert_to_tensor(np.array([1,2,3]))
with tf.Session() as sess:
         result= sess.run(ta)
```

I want to know where the list is converted to tensor. 
Is in GPU, or in CPU and then being feeded to GPU ?"
20403,[tflite][operator: cast] How to quantize MobileNetV2 for deeplabV3+?,"**System information**
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:Source
- **TensorFlow version (use command below)**:1.9.0
- **Python version**:2.7.12
- **Bazel version (if compiling from source)**:0.12.0
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:cuda-9.0/7.0
- **GPU model and memory**:GeForce GTX 1080/8105MiB
- **Phone**:xiaomi5 (Snapdragon 820)
- **Exact command to reproduce**:
bazel run --config=opt //tensorflow/contrib/lite/toco:toco --
--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb
--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite
--inference_type=QUANTIZED_UINT8
--input_shape=1,513,513,3
--input_array=sub_7
--output_array=logits/semantic/BiasAdd

**Describe the problem**
I have tried to quantize MobileNetV2 for deeplabV3+ with TFlite. But I fail to convert the model.
From the following issue, I saw that the operations were not supported for the option of quantization.

https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md
Checkpoint name: mobilenetv2_coco_voc_trainaug

Who can explain and support to resolve the issue?

**Source code / logs**
bazel run --config=opt //tensorflow/contrib/lite/toco:toco --
--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb
--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite
--inference_type=QUANTIZED_UINT8
--input_shape=1,513,513,3
--input_array=sub_7
--output_array=logits/semantic/BiasAdd
--default_ranges_min=0  
--default_ranges_max=6

Unimplemented: this graph contains an operator of type Cast for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).


**Source code / logs**
bazel run --config=opt //tensorflow/contrib/lite/toco:toco --
--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb
--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite
--inference_type=QUANTIZED_UINT8
--input_shape=1,513,513,3
--input_array=sub_7
--output_array=logits/semantic/BiasAdd

tensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:105] Tweaking the MinMax of array ResizeBilinear_1, which is an input to {Concatenation operator with output concat}, because we want all inputs and outputs of a Concatenation operator to have the same MinMax so that it can be implemented as a pure byte-copy, no arithmetic.

tensorflow/contrib/lite/toco/tflite/export.cc:367] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which  you will need custom implementations: Stack."
20402,Feature request: non-stateful scatter_nd_min and scatter_nd_max,"Currently tf.scatter_min and tf.scatter_max, unlike tf.scatter_nd, accept a variable ref input and are stateful op. It would be great if there are a non-stateful version of tf.scatter_min and tf.scatter_max that can back-propagate gradients to their inputs.

@facaiy @drpngx"
20401,Combine two models in tensorflow,"I have checkpoints of two different models in tensorflow.

How to combine them both and save them as one model in one checkpoint file and the resulting computation graph looks like the earlier ones were merged?

What I intend to do is -

    Load model 1 weights and configuration
    Load model 2 weights and configuration

    combine model1 and model2 to get model3

    save model3

Also the models 1 and 2 have identical graphs (they are CNNs and have same variable names for tensors)

"
20399,Feature request: A Layer object wrapping multiple Layer objects,"`tf.contrib.seq2seq.BasicDecoder` module allows provision for a single output layer, to modify the output of the RNN before proceeding to further sample.

In my particular application, I require application of multiple layers.
This is not possible as far as I have checked.

A `MultiLayer` class could be implemented, similar to the `tf.contrib.rnn.MultiRNNCell`.
This class could wrap multiple `Layer` objects and apply them on `call`."
20398,Why dense layer cannot be speed up in tf.contrib.trt ?,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu14.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**:  2.7.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 / 7.0.5
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem

When using tf.contrib.trt.create_inference_graph, I meet the following error, it seems that dense layer is not support because the input tensor is not rank 4 ? Why has this request on dense layer ?

> subgraph conversion error for subgraph_index:9 due to: ""Unimplemented: Require 4 dimensional input. Got 2 dense/MatMul"" SKIPPING

### Source code / logs

Need not to code, clear above ...

"
20397,tf.image.decode_png bug,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- official docker image , Linux Ubuntu 16.04
- tf1.8.0  v1.8.0-0-g93bc2e2072 1.8.0
- python3.5.2
- Cuda compilation tools, release 9.0, V9.0.176
- GPU:1080Ti 

### Describe the problem
tf.image.decode_png can not decode a gray png which has size 60064

### Source code / logs
pp = 'SRAD2018_TRAIN_004/RAD_296582494212544/RAD_296582494212544_004.png'
im = io.imread(pp)
print('haha:  ',  im.shape)
content = tf.read_file(pp)
img = tf.image.decode_png(content, channels=1)
with tf.Session() as sess:
     ig = sess.run(img)
     print(ig.shape)
picture can be downloaded here https://tianchi.aliyun.com/competition/information.htm?spm=5176.100067.5678.2.451d5147iiIyoD&raceId=231662
it is a competition hosted by aliyun,  the competition name is ""IEEE ICDM 2018 å…¨çƒæ°”è±¡AIæŒ‘æˆ˜èµ›""

picture name is 'SRAD2018_TRAIN_004/RAD_296582494212544/RAD_296582494212544_004.png'

I met this problem several times. maybe there are several pictures can repeat this bug. i just give an example above. at everytime the bug occurs with image having size 65564.

only images having size 65504 can not be decoded, and skimage.io can read the picture correctly.
### Logs
haha:  (501, 501)
2018-06-29 02:57:24.012213: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-06-29 02:57:24.128019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-06-29 02:57:24.128635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 222.81MiB
2018-06-29 02:57:24.328695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-06-29 02:57:24.329699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:04:00.0
totalMemory: 10.91GiB freeMemory: 9.93GiB
2018-06-29 02:57:24.527217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-06-29 02:57:24.528368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 2 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:06:00.0
totalMemory: 10.91GiB freeMemory: 10.74GiB
2018-06-29 02:57:24.539737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2
2018-06-29 02:57:25.708418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-06-29 02:57:25.708454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 
2018-06-29 02:57:25.708465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y 
2018-06-29 02:57:25.708475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y 
2018-06-29 02:57:25.708486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N 
2018-06-29 02:57:25.709161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 166 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-06-29 02:57:25.710945: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 166.81M (174915584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2018-06-29 02:57:25.715583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9604 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)
2018-06-29 02:57:25.827449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10397 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid PNG data, size 65564
	 [[Node: DecodePng = DecodePng[channels=1, dtype=DT_UINT8, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""model.py"", line 452, in <module>
    ig = sess.run(img)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid PNG data, size 65564
	 [[Node: DecodePng = DecodePng[channels=1, dtype=DT_UINT8, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]

Caused by op 'DecodePng', defined at:
  File ""model.py"", line 450, in <module>
    img = tf.image.decode_png(content, channels=1)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_image_ops.py"", line 1062, in decode_png
    name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Invalid PNG data, size 65564
	 [[Node: DecodePng = DecodePng[channels=1, dtype=DT_UINT8, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile)]]

"
20396,Feature Request: .string_handle() support for iterator based on Dataset to which .prefetch_to_device() has been applied,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**: AWS p3.2xlarge instance
- **Exact command to reproduce**:

Request:

As stated in the title, it seems that a Dataset object upon which .prefect_to_device has been applied and an iterator defined from results in an iterator for which .string_handle() gives an error. In my case, my dataset and iterator are defined as
```
training_dataset_shuffle_batch = tf.data.Dataset.from_tensor_slices(training_data)
training_dataset_shuffle_batch = training_dataset_shuffle_batch.apply(tf.contrib.data.shuffle_and_repeat(buffer_size = dataset_size))
training_dataset_shuffle_batch = training_dataset_shuffle_batch.batch(minibatch_size)
training_dataset_shuffle_batch = training_dataset_shuffle_batch.apply(tf.contrib.data.prefetch_to_device(gpu_names[0]))
training_shuffle_batch_iterator = training_dataset_shuffle_batch.make_initializable_iterator()
```
and I'd like to use this with a feedable iterator defined as
```
handle = tf.placeholder(tf.string, shape=[])
iterator = tf.data.Iterator.from_string_handle(handle, training_dataset_shuffle_batch.output_types, training_dataset_shuffle_batch.output_shapes) 
next_input_data_element = iterator.get_next()
```
However, 
`training_shuffle_batch_handle = sess.run(training_shuffle_batch_iterator.string_handle())`
results in an 
`AttributeError: '_PrefetchToDeviceIterator' object has no attribute 'string_handle'`

This unfortunately prevents us from using feedable iterators and simultaneous benefit from prefetching data to the GPU.

Thank you."
20392,Feature Request: n-dimensinal extract_glimpse ,"Currently there exists a method [`tf.image.extract_glimpse`](https://www.tensorflow.org/versions/master/api_docs/python/tf/image/extract_glimpse), but this is specific for 2D images (and `tf.extract_image_patches` as well).

Since I work mostly with 3D data it would be nice to have a similar function which produces *glimpses* for 3D data. One could generalise this even more by computing glimpses for n-dimensinal input.

I am happy to look into this, although I am not quite sure where the actual source code is located for the `tf.image.extract_glimpse` method.
There is a class `ExtractGlimpseOp ` in [tensorflow/tensorflow/core/kernels/attention_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/attention_ops.cc), but the name `attention_ops` is a bit confusing.
And of course the op registration in [tensorflow/tensorflow/core/ops/image_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops.cc#L539).

What is a good way to start?


#### System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A
- TensorFlow installed from (source or binary): N/A
- TensorFlow version (use command below): N/A
- Python version: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
- Exact command to reproduce: N/A"
20391,Feature Request: tf.data.Dataset.truncated_batch(),"The `Dataset` api currently provides a `padded_batch` transformation, but for dense sequential data (audio, video) it might be better to drop a few frames to make the batched examples even. Can we have a new function to `Dataset` like this?
```python
truncated_batch(
    batch_size,
    truncating_shapes
)
```

The semantic would be very similar to `padded_batch` but in reverse. The `truncating_shapes` argument determines the resulting shape for each dimension of each component in an output element:
* If the dimension is a constant (e.g. `tf.Dimension(37)`), the component will be truncated to that length in that dimension.
* If the dimension is unknown (e.g. `tf.Dimension(None)`), the component will be truncated to the minimum length of all elements in that dimension.

Thoughts? @mrry "
20387,Tensorflow C++ not releasing GPU resources after closing the session,"### System information
- **Have I written custom code:**: No
- **OS Platform and Distribution: **: Windows 10
- **TensorFlow installed from**: source
- **TensorFlow version (use command below)**: tensorflow r1.7
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: MSVC v140
- **CUDA/cuDNN version**:  CUDA 9.0 and cuDNN v7.0.5
- **GPU model and memory**:  NVIDIA GeForce GTX 1050 Ti 4095 MB
- **Exact command to reproduce**: See code below

### Describe the problem
Compiled tensorflow C++ with GPU support from source (branch r1.7) on Windows 10. Upon creating a new session 3GBs of memory are allocated on the GPU. Closing the session does not seem to result in the memory being released from the GPU as confirmed by the nvidia-smi command. Resources are only released when the C++ program exits.

It would be desirable to have the option of releasing all the resources that tensorflow allocated since in the application I am developing the GPU computation is needed only very infrequently and if tensorflow keeps the GPU memory it is not available for rendering etc.

### Source code / logs
```
int main()
{
    SessionOptions options;
    Session* session;
    tensorflow::Status status = NewSession(SessionOptions(), &session); // returns ok

    status = session->Close(); // returns ok
    delete session;

    // GPU memory is still occupied at this point
    std::string s;
    std::cin >> s;

    return 0;
}
// GPU memory is released when process exits
```

"
20386,html5lib requirement ,"Is there any way the html5lib version can be bumped to at least  0.99999999 (which supports 'html5lib[datrie]').

Thanks a lot

Carlos"
20385,return ordered list of operations in Go API or provide sort Operation function,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.5
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

Reading tensorflow Go APIs I see that there is no way to find which order of operations used in a graph. The current APIs only return list of Operations, and if I want to deduce the graph structure I need to know the order of operations. I can extract names, types but I don't know how to order operations. For instance, if I'll be given a model I want to see its structure. I can loop over graph operations and I'll get all operations, but I don't know their order and therefore can't reproduce the graph model structure.

### Source code / logs
```
   // assume I'll be give a TF graph
    sGraph := """"
   // loop over graph operations and print them out
    for _, op := range graph.Operations() {
        fmt.Println(op.Name(), op.Type(), op.NumOutputs())
        for i := 0; i < op.NumOutputs(); i++ {
            if sGraph != """" {
                sGraph = fmt.Sprintf(""%s -> %s(%s)"", sGraph, op.Name(), op.Output(i).Shape())
            } else {
                sGraph = fmt.Sprintf(""%s(%s)"", op.Name(), op.Output(i).Shape())
            }
        }
    }
```"
20382,TypeError: 'InvalidArgumentError' object is not iterable,"Occasionally encounter   issues like the above during training.  Tensorflow does not provide any information about  where is it is happening. For example  take a look at the following ?  

```
Traceback (most recent call last):
  File ""networks.py"", line 586, in <module>
    main()
  File ""networks.py"", line 572, in main
    x, y, z = train_dataset_batch ()
  File ""networks.py"", line 505, in train_dataset_batch
    loss_f, train_op_f, summary_str = sess.run([ loss, train_op, summaries_op], feed_dict=fd)
TypeError: 'InvalidArgumentError' object is not iterable
```

 How can anyone debug this ?
"
20381,"Help......I just installed tf, i keep getting this error","ImportError                               Traceback (most recent call last)
~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     13         try:
---> 14             return importlib.import_module(mname)
     15         except ImportError:

~\Anaconda3\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

~\Anaconda3\lib\importlib\_bootstrap.py in _gcd_import(name, package, level)

~\Anaconda3\lib\importlib\_bootstrap.py in _find_and_load(name, import_)

~\Anaconda3\lib\importlib\_bootstrap.py in _find_and_load_unlocked(name, import_)

~\Anaconda3\lib\importlib\_bootstrap.py in _load_unlocked(spec)

~\Anaconda3\lib\importlib\_bootstrap.py in module_from_spec(spec)

~\Anaconda3\lib\importlib\_bootstrap_external.py in create_module(self, spec)

~\Anaconda3\lib\importlib\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ModuleNotFoundError                       Traceback (most recent call last)
~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     16             return importlib.import_module('_pywrap_tensorflow_internal')
---> 17     _pywrap_tensorflow_internal = swig_import_helper()
     18     del swig_import_helper

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     15         except ImportError:
---> 16             return importlib.import_module('_pywrap_tensorflow_internal')
     17     _pywrap_tensorflow_internal = swig_import_helper()

~\Anaconda3\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-6b7c506c9114> in <module>()
      1 get_ipython().magic('matplotlib inline')
      2 import matplotlib.pyplot as plt
----> 3 import tensorflow as tf
      4 import numpy as np
      5 from sklearn.metrics import confusion_matrix

~\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 # pylint: disable=wildcard-import
     26 from tensorflow.tools.api.generator.api import *  # pylint: disable=redefined-builtin

~\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\mo.yosiwealth\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\mo.yosiwealth\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\mo.yosiwealth\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\mo.yosiwealth\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\mo.yosiwealth\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\mo.yosiwealth\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
20380,Dockerfile should assign LANG=C_ALL,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:official Dockerfile
- **TensorFlow version (use command below)**:1.8.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:from Docker
- **GCC/Compiler version (if compiling from source)**:from Docker
- **CUDA/cuDNN version**:not relevant
- **GPU model and memory**:not relevant
- **Exact command to reproduce**:see link

### Describe the problem

Python 3 has a serious issue on LANG=C environment as described in [Python Issue 19846](https://bugs.python.org/issue19846).
In fact, official python or Anaconda Dockerfile assigned LANG=C.UTF-8 like [Dockerfile](https://github.com/docker-library/python/blob/a652f35d6ce77f02ca268d67f39e7dfa24cf08c5/3.5/jessie/Dockerfile)

Can this be fixed in future releases?
Thank you so much.
"
20379,tf.layers.conv3d throws an error when using 'channels_first' and 'None' size for the input shape,"### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Windows 10 Home version 1803
- **TensorFlow installed from**: Installed by running ""pip install tensorflow-gpu""
- **TensorFlow version (use command below)**: b'v1.8.0-0-g93bc2e2072' 1.8.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA v9.0, cuDNN v7.1
- **GPU model and memory**: GeForce GTX1080, 8GB
- **Exact command to reproduce**: see the *Code To Reproduce The Bug* section


### Bug Description
When calling ""tf.layers.conv3d"" with ""data_format='channels_first'"" and the
input shape (None, 1, 3, None, None), tensorflow throws the error:
""TypeError(""unsupported operand type(s) for *: 'int' and 'NoneType'"",)""

This error appears to be thrown from [this line](https://github.com/tensorflow/tensorflow/blob/f202958ee2d5177a474e3d107fdbf0c83174d099/tensorflow/python/keras/layers/convolutional.py#L205) according to Visual Studio's python debugger.

### Code To Reproduce The Bug
```python
import tensorflow as tf

def WorkingCode():
    InputTensor = tf.placeholder(dtype=tf.float32, shape=(None, 3, None, None, 1))
    OutputTensor = tf.layers.conv3d(inputs=InputTensor, filters=16, kernel_size=(1, 3, 3), data_format='channels_last')

def BuggedCode():
    InputTensor = tf.placeholder(dtype=tf.float32, shape=(None, 1, 3, None, None))
    OutputTensor = tf.layers.conv3d(inputs=InputTensor, filters=16, kernel_size=(1, 3, 3), data_format='channels_first')
```
Calling the `BuggedCode` function will throw the error, while calling `WorkingCode` will work perfectly fine.

### Full Traceback
```
Traceback (most recent call last):
  File "".\TensorflowBugTest.py"", line 11, in <module>
    BuggedCode()
  File "".\TensorflowBugTest.py"", line 9, in BuggedCode
    OutputTensor = tf.layers.conv3d(inputs=InputTensor, filters=16, kernel_size=(1, 3, 3), data_format='channels_first')
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\layers\convolutional.py"", line 828, in conv3d
    return layer.apply(inputs)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\layers\base.py"", line 828, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\layers\base.py"", line 717, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\layers\convolutional.py"", line 187, in call
    outputs_shape[2] * outputs_shape[3],
TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'
```"
20377,keras model to estimator in eager mode gives ValueError,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.4
- **TensorFlow installed from (source or binary)**: binary 
- **TensorFlow version (use command below)**:  1.9.0-rc1
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
After enabling eager mode with `tf.enable_eager_execution()` if I convert my `tf.keras.Model` to an estimator via: `tf.keras.estimator.model_to_estimator(model)` I get:

```
INFO:tensorflow:Using the Keras model provided.
INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: /var/folders/yg/t3pf0vds6c14zlhmq1jwwvtnsz8fsr/T/tmp197t8ju4
INFO:tensorflow:Using config: {'_model_dir': '/var/folders/yg/t3pf0vds6c14zlhmq1jwwvtnsz8fsr/T/tmp197t8ju4', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1227bcdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-8-6cc0c716ca8a> in <module>()
----> 1 estimator = tf.keras.estimator.model_to_estimator(model)

/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config)
    512 
    513   # Check if we need to call get_weights:
--> 514   if _any_variable_initialized():
    515     keras_weights = keras_model.get_weights()
    516     # Warn if config passed to estimator tries to update GPUOptions. If a

/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py in _any_variable_initialized()
     77     boolean, True if at least one variable has been initialized, else False.
     78   """"""
---> 79   variables = variables_module.global_variables()
     80   for v in variables:
     81     if getattr(v, '_keras_initialized', False):

/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in global_variables(scope)
   1442     A list of `Variable` objects.
   1443   """"""
-> 1444   return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope)
   1445 
   1446 

/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in get_collection(key, scope)
   5905   @end_compatibility
   5906   """"""
-> 5907   return get_default_graph().get_collection(key, scope)
   5908 
   5909 

/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in get_collection(self, name, scope)
   3961       collected.
   3962     """"""  # pylint: disable=g-doc-exception
-> 3963     _assert_collection_is_ok(name)
   3964     with self._lock:
   3965       collection = self._collections.get(name, None)

/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _assert_collection_is_ok(collection_name)
   6146     if collection_name in GraphKeys._VARIABLE_COLLECTIONS:  # pylint: disable=protected-access
   6147       raise ValueError(
-> 6148           ""variable collections are not supported when eager execution is enabled.""
   6149       )
   6150 

ValueError: variable collections are not supported when eager execution is enabled.
```"
20376,tensorboard doesn't work well on Windows,"Hey guys,
I use TensorFlow on Windows. When I invoked this command:
`tensorboard --logdir=output`
The **tensorboard** program didn't visualize my trained model as expected. Instead, it simply executed my model's python file.
Anyway, I tried this fully-qualified version:
`C:\Python36\Scripts\tensorboard --logdir=output`
It did work, and I got my model visualized successfully.
This is very strange. Invoking the same program **tensorboard** in different ways produce totally different results.
Can this be fixed in future releases?
Thank you so much.
"
20375,Feature Request: CheckpointSaverHook should allow writing graph in binary mode,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**:  3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
First, I found that the **MonitoredTrainingSession** is slow to start due to saving the `graph.pbtxt` file when the graph is huge. I could see that if it was saved in a binary format (`.pb`), it would be much faster (and also smaller).

Now, I see that **MonitoredTrainingSession** utilizes the **CheckpointSaverHook** to save the graph. However, I see no way to make **CheckpointSaverHook** save in binary format. 

See: https://github.com/tensorflow/tensorflow/blob/51ddb66cdf1444998827e03c4b5f592841ee6255/tensorflow/python/training/basic_session_run_hooks.py#L433

Should **CheckpointSaverHook** has an option to save in binary format?

PS. I also doubt the usefulness of `graph.pbtxt` file where the `.meta` file is also present.
"
20373,kernel_tests:cwise_ops_test fails on AVX512 systems,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.8.0-4021-g4292085', '1.9.0-rc0')
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**:  0.15.0
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: bazel test --config=opt -- //tensorflow/python/kernel_tests:cwise_ops_test

### Describe the problem

The //tensorflow/python/kernel_tests:cwise_ops_test test fails on AVX512 machines.  Reproducing the problem is simple.  Just run the unit test.

```
bazel test --config=opt -- //tensorflow/python/kernel_tests:cwise_ops_test
```

The test is failing as there's a bug in Eigen's AVX512 implementation of the psqrt functions, which incorrectly compute the sqrt of negative numbers as 0 instead of NaN.  There's a pull request pending on Eigen that fixes the issue.

https://bitbucket.org/eigen/eigen/pull-requests/412/fix-avx512-implementations-of-psqrt/diff

A similar issue was fixed in Eigen's AVX2 implementations of psqrt a couple of years ago.

### Source code / logs

```
cwise_ops_test.py:1920: RuntimeWarning: invalid value encountered in sqrt
  np_y = np.sqrt(x)
F
======================================================================
FAIL: testSqrt (__main__.IsFiniteInfNanTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""$HOME/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/cwise_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/cwise_ops_test.py"", line 1926, in testSqrt
    self.assertAllEqual(np_nan, tf_nan.eval())
  File ""$HOME/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/cwise_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1401, in assertAllEqual
    np.testing.assert_array_equal(a, b, err_msg=msg)
  File ""$HOME/.local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py"", line 855, in assert_array_equal
    verbose=verbose, header='Arrays are not equal')
  File ""$HOME/.local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py"", line 779, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Arrays are not equal

(mismatch 76.1904761905%)
 x: array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,...
 y: array([False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,...

----------------------------------------------------------------------
Ran 3 tests in 4.787s

FAILED (failures=1)
not equal where =  (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,
       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]),)
not equal lhs =  [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True]
not equal rhs =  [False False False False False False False False False False False False
 False False False False False False False False False False False False
 False False False False False False False False False False False False
 False False False False False False False False False False False False]
```"
20372,Keras Tensorboard callback fails with tf.eager,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: Via pip
- **TensorFlow version (use command below)**: 1.9.0-rc1
- **Python version**:  3.6.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: GTX 1080Ti/11GB
- **Exact command to reproduce**: Code given below

### Describe the problem
I'm using tf.eager, Keras model subclassing and tf.Dataset on TF 1.9.0-rc1. When I try to add the Tensorboard callback to the Keras `.fit` method I get the following error:
```
  File ""/home/ekami/workspace/src/main.py"", line 112, in <module>
    main(args)
  File ""/home/ekami/workspace/src/main.py"", line 63, in main
    validation_steps=3)
  File ""/home/ekami/workspace/src/models/base_model.py"", line 75, in fit
    sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
  File ""/home/ekami/Programs/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1328, in fit
    validation_steps=validation_steps)
  File ""/home/ekami/Programs/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_eager.py"", line 979, in fit_loop
    callbacks.set_model(callback_model)
  File ""/home/ekami/Programs/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 70, in set_model
    callback.set_model(model)
  File ""/home/ekami/Programs/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 763, in set_model
    self.merged = tf_summary.merge_all()
  File ""/home/ekami/Programs/anaconda3/envs/dl/lib/python3.6/site-packages/tensorflow/python/summary/summary.py"", line 311, in merge_all
    'Merging tf.summary.* ops is not compatible with eager execution. '
RuntimeError: Merging tf.summary.* ops is not compatible with eager execution. Use tf.contrib.summary instead.
```

Here is a pseudo code to reproduce the error:
```
train_ds, train_slides, val_ds, val_slides, lookup_table = Dataset(logger, input_dir, output_dir, crop_size,
                                           cache_slide_size=cache_slide_size).get_dataset() # Returns a tf.Dataset object

patches_len = len(train_slides[0].patches)
model = SimpleCNNModel(num_classes=len(lookup_table))
model.compile(optimizer=tf.train.AdamOptimizer(),
                          loss='categorical_crossentropy',
                          metrics=['accuracy'])
model.fit(train_ds, epochs=args.epochs,
                callbacks=[tf.keras.callbacks.TensorBoard()],  # Will fail with the above error
                batch_size=batch_size,
                steps_per_epoch=patches_len // batch_size,
                shuffle=True,
                validation_data=val_ds,
                validation_steps=3)
model.save_weights(weights_pth)
logger.info(""Done training!"")
```

### Source code / logs

Given above
"
20370,"MirroredStrategy fails with ""no supported kernel for GPU devices is available""","- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ML Engine default
- **TensorFlow installed from (source or binary)**: ML Engine default
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 2.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: ML Engine default
- **GPU model and memory**: NVIDIA Tesla K80
- **Exact command to reproduce**: N/A

MirroredStrategy fails with ""no supported kernel for GPU devices is available"". The same code works on a single GPU.

Traceback:
Traceback (most recent call last): File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main ""__main__"", fname, loader, pkg_name) File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code exec code in run_globals File ""/root/.local/lib/python2.7/site-packages/trainer_yolo/main.py"", line 188, in <module> main(sys.argv) File ""/root/.local/lib/python2.7/site-packages/trainer_yolo/main.py"", line 184, in main start_training(output_dir, hparams, **otherargs) File ""/root/.local/lib/python2.7/site-packages/trainer_yolo/main.py"", line 131, in start_training tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 439, in train_and_evaluate executor.run() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 546, in run getattr(self, task_to_run)() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 601, in run_master self._start_distributed_training(saving_listeners=saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py"", line 739, in _start_distributed_training saving_listeners=saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 363, in train loss = self._train_model(input_fn, hooks, saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 841, in _train_model return self._train_model_distributed(input_fn, hooks, saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 977, in _train_model_distributed saving_listeners) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1056, in _train_with_estimator_spec log_step_count_steps=self._config.log_step_count_steps) as mon_sess: File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 405, in MonitoredTrainingSession stop_grace_period_secs=stop_grace_period_secs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 816, in __init__ stop_grace_period_secs=stop_grace_period_secs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 539, in __init__ self._sess = _RecoverableSession(self._coordinated_creator) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1002, in __init__ _WrappedSession.__init__(self, self._create_session()) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1007, in _create_session return self._sess_creator.create_session() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 696, in create_session self.tf_sess = self._session_creator.create_session() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 467, in create_session init_fn=self._scaffold.init_fn) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 285, in prepare_session sess.run(init_op, feed_dict=init_feed_dict) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 900, in run run_metadata_ptr) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1135, in _run feed_dict_tensor, options, run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1316, in _do_run run_metadata) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1335, in _do_call raise type(e)(node_def, op, message) InvalidArgumentError: Cannot assign a device for operation 'tower_3/Tile_7/input': Could not satisfy explicit device specification '/device:GPU:3' because **no supported kernel for GPU devices is available**. Registered kernels: device='CPU'; T in [DT_QINT32] device='CPU'; T in [DT_QUINT8] device='CPU'; T in [DT_QINT8] device='CPU'; T in [DT_VARIANT] device='CPU'; T in [DT_RESOURCE] device='CPU'; T in [DT_STRING] device='CPU'; T in [DT_BOOL] device='CPU'; T in [DT_COMPLEX128] device='CPU'; T in [DT_COMPLEX64] device='CPU'; T in [DT_DOUBLE] device='CPU'; T in [DT_FLOAT] device='CPU'; T in [DT_BFLOAT16] device='CPU'; T in [DT_HALF] device='CPU'; T in [DT_INT8] device='CPU'; T in [DT_UINT8] device='CPU'; T in [DT_INT16] device='CPU'; T in [DT_UINT16] device='CPU'; T in [DT_INT32] device='CPU'; T in [DT_INT64] device='GPU'; T in [DT_INT32] device='GPU'; T in [DT_BOOL] device='GPU'; T in [DT_INT64] device='GPU'; T in [DT_BFLOAT16] device='GPU'; T in [DT_DOUBLE] device='GPU'; T in [DT_FLOAT] device='GPU'; T in [DT_HALF] [[Node: tower_3/Tile_7/input = Pack[N=1, T=DT_INT16, axis=0, _device=""/device:GPU:3""](tower_3/Cast_4)]] Caused by op u'tower_3/Tile_7/input', defined at: File ""/usr/lib/python2.7/threading.py"", line 774, in __bootstrap self.__bootstrap_inner() File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner self.run() File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 465, in run self.main_result = self.main_fn(*self.main_args, **self.main_kwargs) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 831, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File ""/root/.local/lib/python2.7/site-packages/trainer_yolo/model.py"", line 230, in model_fn iou_accuracy = box.compute_safe_IOU(target_rois, detected_rois, detected_rois_overflow, settings.TILE_SIZE) File ""/root/.local/lib/python2.7/site-packages/trainer_yolo/utils_box.py"", line 521, in compute_safe_IOU iou_accuracy = IOUCalculator.batch_intersection_over_union(detected_rois * tile_size, target_rois * tile_size, tile_size=tile_size) File ""/root/.local/lib/python2.7/site-packages/trainer_yolo/utils_box.py"", line 476, in batch_intersection_over_union linmap2 = cls.__iou_gen_linmap(batch, n2, tile_size) File ""/root/.local/lib/python2.7/site-packages/trainer_yolo/utils_box.py"", **line 428, in __iou_gen_linmap linmap = tf.tile([row], [tile_size, 1])** File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 8430, in tile ""Tile"", input=input, multiples=multiples, name=name) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper preferred_dtype=default_dtype) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1104, in internal_convert_to_tensor ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1034, in _autopacking_conversion_function return _autopacking_helper(v, inferred_dtype, name or ""packed"") File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 997, in _autopacking_helper return gen_array_ops.pack(elems_as_tensors, name=scope) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 4517, in pack ""Pack"", values=values, axis=axis, name=name) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper op_def=op_def) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op op_def=op_def) File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__ self._traceback = self._graph._extract_stack() # pylint: disable=protected-access InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'tower_3/Tile_7/input': Could not satisfy explicit device specification '/device:GPU:3' because **no supported kernel for GPU devices is available**. Registered kernels: device='CPU'; T in [DT_QINT32] device='CPU'; T in [DT_QUINT8] device='CPU'; T in [DT_QINT8] device='CPU'; T in [DT_VARIANT] device='CPU'; T in [DT_RESOURCE] device='CPU'; T in [DT_STRING] device='CPU'; T in [DT_BOOL] device='CPU'; T in [DT_COMPLEX128] device='CPU'; T in [DT_COMPLEX64] device='CPU'; T in [DT_DOUBLE] device='CPU'; T in [DT_FLOAT] device='CPU'; T in [DT_BFLOAT16] device='CPU'; T in [DT_HALF] device='CPU'; T in [DT_INT8] device='CPU'; T in [DT_UINT8] device='CPU'; T in [DT_INT16] device='CPU'; T in [DT_UINT16] device='CPU'; T in [DT_INT32] device='CPU'; T in [DT_INT64] device='GPU'; T in [DT_INT32] device='GPU'; T in [DT_BOOL] device='GPU'; T in [DT_INT64] device='GPU'; T in [DT_BFLOAT16] device='GPU'; T in [DT_DOUBLE] device='GPU'; T in [DT_FLOAT] device='GPU'; T in [DT_HALF] [[Node: tower_3/Tile_7/input = Pack[N=1, T=DT_INT16, axis=0, _device=""/device:GPU:3""](tower_3/Cast_4)]]
"
20369,No registered 'NotEqual' OpKernel for GPU for INT32 type (but other types are fine).,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux 4.13.0-45-generic #50-Ubuntu SMP Wed May 30 08:23:18 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""17.10 (Artful Aardvark)""
VERSION_ID=""17.10""
VERSION_CODENAME=artful

- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.8
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
CUDA8.0 cuDNN9.0
- **GPU model and memory**:
Tesla K80 10762 MB
- **Exact command to reproduce**:
See below


### Describe the problem
------------------------

When I run tf.not_equal under GPU, I got this error msg:

> tensorflow.python.framework.errors_impl.NotFoundError: No registered 'NotEqual' OpKernel for GPU devices compatible with node NotEqual = NotEqual[T=DT_INT32](dummy_input, dummy_input)
> 	 (OpKernel was found, but attributes didn't match)
> 	.  Registered:  device='CPU'; T in [DT_BOOL]
>   device='CPU'; T in [DT_STRING]
>   device='CPU'; T in [DT_COMPLEX128]
>   device='CPU'; T in [DT_COMPLEX64]
>   device='CPU'; T in [DT_INT64]
>   device='CPU'; T in [DT_INT32]
>   device='CPU'; T in [DT_INT16]
>   device='CPU'; T in [DT_INT8]
>   device='CPU'; T in [DT_UINT8]
>   device='CPU'; T in [DT_DOUBLE]
>   device='CPU'; T in [DT_HALF]
>   device='CPU'; T in [DT_FLOAT]
>   device='GPU'; T in [DT_BOOL]
>   device='GPU'; T in [DT_COMPLEX128]
>   device='GPU'; T in [DT_COMPLEX64]
>   device='GPU'; T in [DT_INT64]
>   device='GPU'; T in [DT_INT16]
>   device='GPU'; T in [DT_INT8]
>   device='GPU'; T in [DT_UINT8]
>   device='GPU'; T in [DT_DOUBLE]
>   device='GPU'; T in [DT_HALF]
>   device='GPU'; T in [DT_FLOAT]
>  [Op:NotEqual]
>
It seems that INT32 is omitted forgetfully (jump from INT16 to INT64 directly)?"
20368,Using new tensorflow op for matrix exponential in a c++ library that already uses tensorflow as third party,"### System information
- **Have I written custom code**: I'm using ZeroOut CPU versionfrom https://github.com/MatteoRagni/tf.ZeroOut.gpu
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 LTS
- **TensorFlow installed from**: source
- **TensorFlow version** : 1.8.0
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**: gcc 5.4.0,  cmake 3.5.1
- **CUDA/cuDNN version**: release 9.0, V9.0.176
- **GPU model and memory**: GeForce GT 730/PCIe/SSE2
- **Exact command to reproduce**: N/A


Hallo to everyone! That's my first time asking a question in tensorflow. I will try my best to formulate my question properly.

My plan is: 

 - implement a new tensorflow GPU op for the matrix exponential,using Eigen unsupported MatrixFunctions or the already existing tensorflow matrix exponential op
 - add the gradient
 - use the new op in a c++ library, which already uses tensorflow as third party.

I have started from the basics, and I realized that I don't know how to use my custom operation in c++. I registered the ZeroOut op for cpu from tensorflow c++ tutorial as in https://github.com/MatteoRagni/tf.ZeroOut.gpu but now I don't know how to use that in my c++ code. 

I tried to add the ZeroOut.so file to my lib as shared library, but it didn't work. Maybe I'm doing something wrong? My CMakeList.txt is attached. And including  ZeroOut.cpp in my c++ files hasn't make any difference until now.
I looked in tensorflow documentation, stackoverflow and the internet but I couln't find an answer to my questions. Hopefully I didn't miss anything.

Can you help me? Maybe giving an example of the required CMakeList.txt, even if not related to mine?

Speaking about my general plan, I would also like to have some advices from more experienced programmers. I know tensorflow has a matrix exponential op, but as far as I know it doesn't work for GPU (see #15465) and has no gradient implementation. Should I add this features to the existing op rather than registering a new one? And what about using Eigen unsupported MatrixFunctions in a new user op?

### Source code / logs
Here is my CMakeList.txt, which also creates the whole library I'm working with:

    cmake_minimum_required(VERSION 2.8)
    project(Project1)

    set(CMAKE_BUILD_TYPE ""Release"") # Debug Release
    set(CMAKE_CXX_FLAGS_RELEASE ""$ENV{CXXFLAGS} -std=c++14 -O3 -Wall                 -fopenmp"")
    SET(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)

    #-------------path of 3rd party libraries-------------
    # special libs.
    find_package(Boost COMPONENTS filesystem iostreams regex)
    find_package(FFTW)
    find_package(NLopt)
    find_package(HDF5 COMPONENTS CXX)

    set(EXTERN_LIB_ROOT ${PROJECT_SOURCE_DIR}/3rd-party)

    set(TENSORFLOW_ROOT /.../tensorflow)
    set(TF_INCLUDE_DIRS ""${TENSORFLOW_ROOT}"" ""${TENSORFLOW_ROOT}/bazel-  genfiles"" ""${TENSORFLOW_ROOT}/bazel-tensorflow/external/protobuf_archive/src"")

    # lib dirs.
    set(LUA_LIBRARIES ""${EXTERN_LIB_ROOT}/lua/liblua53.so"") #5.3.4
    set(LINENOISE_LIBRARIES ""${EXTERN_LIB_ROOT}/linenoise-ng/build/liblinenoise.so"")
    set(YACAS_LIBRARIES ""${EXTERN_LIB_ROOT}/yacas/cyacas/libyacas/build/libyacas.so"")


    set(TF_LIBRARIES ${TENSORFLOW_ROOT}/bazel-bin/tensorflow/libtensorflow_cc.so
        ${TENSORFLOW_ROOT}/tensorflow/core/user_ops/tf.ZeroOut.gpu-master/zero_out.so) 
    #-------------ssl headers-------------
    include_directories(${PROJECT_SOURCE_DIR}/src
        ${EXTERN_LIB_ROOT}/eigen
        ${EXTERN_LIB_ROOT}/gnuplot-iostream
        ${EXTERN_LIB_ROOT}/
        ${EXTERN_LIB_ROOT}/linenoise-ng/include
        ${EXTERN_LIB_ROOT}/yacas/cyacas/libyacas/include
        ${EXTERN_LIB_ROOT}/lua/src
        ${NLOPT_INCLUDE_DIRS}
        ${FFTW_INCLUDES}
        ${TF_INCLUDE_DIRS}
        ${Boost_INCLUDE_DIRS}
        ${HDF5_INCLUDE_DIRS}
        ${TENSORFLOW_ROOT}) 

    option(BUILD_SHARED_LIBS ""build shared library"" ON)
    set(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/lib)

    #-------------ssl kernel lib-------------
    file(GLOB_RECURSE _src_list
        LIST_DIRECTORIES false
        RELATIVE ""${CMAKE_CURRENT_SOURCE_DIR}"" ""${PROJECT_SOURCE_DIR}/src/*.h"" ""${PROJECT_SOURCE_DIR}/src/*.cpp"" """")

    add_library(ssl SHARED ${_src_list})

    set(SSL_LIBRARIES ${TF_LIBRARIES} ${LUA_LIBRARIES} ${Boost_LIBRARIES}     ${NLOPT_LIBRARIES} ${FFTW_LIBRARIES} ${LINENOISE_LIBRARIES} ${YACAS_LIBRARIES} ${HDF5_CXX_LIBRARIES}) #${TF_LIBRARIES}

    target_link_libraries(ssl ${SSL_LIBRARIES} dl)

    add_executable(Project1 main.cpp)
    target_link_libraries(Project1 ssl)
"
20367,Using new tensorflow op for matrix exponential in a c++ library that already uses tensorflow as third party,"### System information
- **Have I written custom code**: I'm using ZeroOut CPU versionfrom https://github.com/MatteoRagni/tf.ZeroOut.gpu
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 LTS
- **TensorFlow installed from**: source
- **TensorFlow version** : 1.8.0
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**: gcc 5.4.0,  cmake 3.5.1
- **CUDA/cuDNN version**: release 9.0, V9.0.176
- **GPU model and memory**: GeForce GT 730/PCIe/SSE2


Hallo to everyone! That's my first time asking a question in tensorflow. I will try my best to formulate my question properly.

My plan is: 

 - implement a new tensorflow GPU op for the matrix exponential,using Eigen unsupported MatrixFunctions or the already existing tensorflow matrix exponential op
 - add the gradient
 - use the new op in a c++ library, which already uses tensorflow as third party.

I have started from the basics, and I realized that I don't know how to use my custom operation in c++. I registered the ZeroOut op for cpu from tensorflow c++ tutorial as in https://github.com/MatteoRagni/tf.ZeroOut.gpu but now I don't know how to use that in my c++ code. 

I tried to add the ZeroOut.so file to my lib as shared library, but it didn't work. Maybe I'm doing something wrong? My CMakeList.txt is attached. And including  ZeroOut.cpp in my c++ files hasn't make any difference until now.
I looked in tensorflow documentation, stackoverflow and the internet but I couln't find an answer to my questions. Hopefully I didn't miss anything.

Can you help me? Maybe giving an example of the required CMakeList.txt, even if not related to mine?

Speaking about my general plan, I would also like to have some advices from more experienced programmers. I know tensorflow has a matrix exponential op, but as far as I know it doesn't work for GPU (see #15465) and has no gradient implementation. Should I add this features to the existing op rather than registering a new one? And what about using Eigen unsupported MatrixFunctions in a new user op?

### Source code / logs
Here is my CMakeList.txt, which also creates the whole library I'm working with:

    cmake_minimum_required(VERSION 2.8)
    project(Project1)

    set(CMAKE_BUILD_TYPE ""Release"") # Debug Release
    set(CMAKE_CXX_FLAGS_RELEASE ""$ENV{CXXFLAGS} -std=c++14 -O3 -Wall                 -fopenmp"")
    SET(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)

    #-------------path of 3rd party libraries-------------
    # special libs.
    find_package(Boost COMPONENTS filesystem iostreams regex)
    find_package(FFTW)
    find_package(NLopt)
    find_package(HDF5 COMPONENTS CXX)

    set(EXTERN_LIB_ROOT ${PROJECT_SOURCE_DIR}/3rd-party)

    set(TENSORFLOW_ROOT /.../tensorflow)
    set(TF_INCLUDE_DIRS ""${TENSORFLOW_ROOT}"" ""${TENSORFLOW_ROOT}/bazel-  genfiles"" ""${TENSORFLOW_ROOT}/bazel-tensorflow/external/protobuf_archive/src"")

    # lib dirs.
    set(LUA_LIBRARIES ""${EXTERN_LIB_ROOT}/lua/liblua53.so"") #5.3.4
    set(LINENOISE_LIBRARIES ""${EXTERN_LIB_ROOT}/linenoise-ng/build/liblinenoise.so"")
    set(YACAS_LIBRARIES ""${EXTERN_LIB_ROOT}/yacas/cyacas/libyacas/build/libyacas.so"")


    set(TF_LIBRARIES ${TENSORFLOW_ROOT}/bazel-bin/tensorflow/libtensorflow_cc.so
        ${TENSORFLOW_ROOT}/tensorflow/core/user_ops/tf.ZeroOut.gpu-master/zero_out.so) 
    #-------------ssl headers-------------
    include_directories(${PROJECT_SOURCE_DIR}/src
        ${EXTERN_LIB_ROOT}/eigen
        ${EXTERN_LIB_ROOT}/gnuplot-iostream
        ${EXTERN_LIB_ROOT}/
        ${EXTERN_LIB_ROOT}/linenoise-ng/include
        ${EXTERN_LIB_ROOT}/yacas/cyacas/libyacas/include
        ${EXTERN_LIB_ROOT}/lua/src
        ${NLOPT_INCLUDE_DIRS}
        ${FFTW_INCLUDES}
        ${TF_INCLUDE_DIRS}
        ${Boost_INCLUDE_DIRS}
        ${HDF5_INCLUDE_DIRS}
        ${TENSORFLOW_ROOT}) 

    option(BUILD_SHARED_LIBS ""build shared library"" ON)
    set(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/lib)

    #-------------ssl kernel lib-------------
    file(GLOB_RECURSE _src_list
        LIST_DIRECTORIES false
        RELATIVE ""${CMAKE_CURRENT_SOURCE_DIR}"" ""${PROJECT_SOURCE_DIR}/src/*.h"" ""${PROJECT_SOURCE_DIR}/src/*.cpp"" """")

    add_library(ssl SHARED ${_src_list})

    set(SSL_LIBRARIES ${TF_LIBRARIES} ${LUA_LIBRARIES} ${Boost_LIBRARIES}     ${NLOPT_LIBRARIES} ${FFTW_LIBRARIES} ${LINENOISE_LIBRARIES} ${YACAS_LIBRARIES} ${HDF5_CXX_LIBRARIES}) #${TF_LIBRARIES}

    target_link_libraries(ssl ${SSL_LIBRARIES} dl)

    add_executable(Project1 main.cpp)
    target_link_libraries(Project1 ssl)
"
20363,'DNNBoostedTreeCombinedClassifier' default argument setting would lead to error,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

In `DNNBoostedTreeCombinedClassifier`, the default setting for `tree_feature_columns` is None and the default setting for `dnn_input_layer_to_tree` is True. In `dnn_tree_combined_estimator.py`:  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/boosted_trees/estimator_batch/dnn_tree_combined_estimator.py#L228 there is an append operation for `tree_feature_columns` which would lead to a `AttributeError: 'NoneType' object has no attribute 'append'`. Current boston_combined example for `DNNBoostedTreeCombinedClassifier` also failed due to the same reason.


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20362,Does Tensorflow install on 32-bit Windows 7 ? ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I could not install Tensorflow on my Win 7 32-bit machine through Anaconda or Pycharm, but I did
on my Win 7 64-bit machine.  

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20361,ppc64le: //tensorflow/contrib/lite/kernels:resize_bilinear_test test fails CPU test,"Please assign this issue to me and add the tag: stat:community support

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Ubuntu 16.04.4
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: master from June 27th
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
bazel test -c opt --cache_test_results=no //tensorflow/contrib/lite/kernels:resize_bilinear_test


### Describe the problem
```
FAIL: //tensorflow/contrib/lite/kernels:resize_bilinear_test (see /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/testlogs/tensorflow/contrib/lite/kernels/resize_bilinear_test/test.log)
INFO: From Testing //tensorflow/contrib/lite/kernels:resize_bilinear_test:
==================== Test output for //tensorflow/contrib/lite/kernels:resize_bilinear_test:
[==========] Running 10 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 10 tests from ResizeBilinearOpTest
[ RUN      ] ResizeBilinearOpTest.HorizontalResize
[       OK ] ResizeBilinearOpTest.HorizontalResize (0 ms)
[ RUN      ] ResizeBilinearOpTest.HorizontalResize8Bit
[       OK ] ResizeBilinearOpTest.HorizontalResize8Bit (0 ms)
[ RUN      ] ResizeBilinearOpTest.VerticalResize
[       OK ] ResizeBilinearOpTest.VerticalResize (0 ms)
[ RUN      ] ResizeBilinearOpTest.VerticalResize8Bit
[       OK ] ResizeBilinearOpTest.VerticalResize8Bit (0 ms)
[ RUN      ] ResizeBilinearOpTest.TwoDimensionalResize
[       OK ] ResizeBilinearOpTest.TwoDimensionalResize (0 ms)
[ RUN      ] ResizeBilinearOpTest.TwoDimensionalResize8Bit
[       OK ] ResizeBilinearOpTest.TwoDimensionalResize8Bit (1 ms)
[ RUN      ] ResizeBilinearOpTest.TwoDimensionalResizeWithTwoBatches
[       OK ] ResizeBilinearOpTest.TwoDimensionalResizeWithTwoBatches (0 ms)
[ RUN      ] ResizeBilinearOpTest.ThreeDimensionalResize
[       OK ] ResizeBilinearOpTest.ThreeDimensionalResize (0 ms)
[ RUN      ] ResizeBilinearOpTest.TwoDimensionalResizeWithTwoBatches8Bit
tensorflow/contrib/lite/kernels/resize_bilinear_test.cc:261: Failure
Value of: m.GetOutput<uint8>()
Expected: has 18 elements where
element #0 is approximately 3 (absolute error <= 9.9999997e-06),
element #1 is approximately 5 (absolute error <= 9.9999997e-06),
element #2 is approximately 6 (absolute error <= 9.9999997e-06),
element #3 is approximately 7 (absolute error <= 9.9999997e-06),
element #4 is approximately 9 (absolute error <= 9.9999997e-06),
element #5 is approximately 10 (absolute error <= 9.9999997e-06),
element #6 is approximately 9 (absolute error <= 9.9999997e-06),
element #7 is approximately 11 (absolute error <= 9.9999997e-06),
element #8 is approximately 12 (absolute error <= 9.9999997e-06),
element #9 is approximately 4 (absolute error <= 9.9999997e-06),
element #10 is approximately 8 (absolute error <= 9.9999997e-06),
element #11 is approximately 10 (absolute error <= 9.9999997e-06),
element #12 is approximately 8 (absolute error <= 9.9999997e-06),
element #13 is approximately 12 (absolute error <= 9.9999997e-06),
element #14 is approximately 14 (absolute error <= 9.9999997e-06),
element #15 is approximately 10 (absolute error <= 9.9999997e-06),
element #16 is approximately 13 (absolute error <= 9.9999997e-06),
element #17 is approximately 16 (absolute error <= 9.9999997e-06)
  Actual: { '\x3' (3), '\x5' (5), '\x6' (6), '\a' (7), '\t' (9), '\n' (10, 0xA), '\t' (9), '\v' (11, 0xB), '\f' (12, 0xC), '\x4' (4), '\b' (8), '\n' (10, 0xA), '\b' (8), '\f' (12, 0xC), '\xE' (14), '\n' (10, 0xA), '\xE' (14), '\x10' (16) }, whose element #16 doesn't match, which is 1 from 13
tensorflow/contrib/lite/kernels/resize_bilinear_test.cc:278: Failure
Value of: const_m.GetOutput<uint8>()
Expected: has 18 elements where
element #0 is approximately 3 (absolute error <= 9.9999997e-06),
element #1 is approximately 5 (absolute error <= 9.9999997e-06),
element #2 is approximately 6 (absolute error <= 9.9999997e-06),
element #3 is approximately 7 (absolute error <= 9.9999997e-06),
element #4 is approximately 9 (absolute error <= 9.9999997e-06),
element #5 is approximately 10 (absolute error <= 9.9999997e-06),
element #6 is approximately 9 (absolute error <= 9.9999997e-06),
element #7 is approximately 11 (absolute error <= 9.9999997e-06),
element #8 is approximately 12 (absolute error <= 9.9999997e-06),
element #9 is approximately 4 (absolute error <= 9.9999997e-06),
element #10 is approximately 8 (absolute error <= 9.9999997e-06),
element #11 is approximately 10 (absolute error <= 9.9999997e-06),
element #12 is approximately 8 (absolute error <= 9.9999997e-06),
element #13 is approximately 12 (absolute error <= 9.9999997e-06),
element #14 is approximately 14 (absolute error <= 9.9999997e-06),
element #15 is approximately 10 (absolute error <= 9.9999997e-06),
element #16 is approximately 13 (absolute error <= 9.9999997e-06),
element #17 is approximately 16 (absolute error <= 9.9999997e-06)
  Actual: { '\x3' (3), '\x5' (5), '\x6' (6), '\a' (7), '\t' (9), '\n' (10, 0xA), '\t' (9), '\v' (11, 0xB), '\f' (12, 0xC), '\x4' (4), '\b' (8), '\n' (10, 0xA), '\b' (8), '\f' (12, 0xC), '\xE' (14), '\n' (10, 0xA), '\xE' (14), '          \x10' (16) }, whose element #16 doesn't match, which is 1 from 13
[  FAILED  ] ResizeBilinearOpTest.TwoDimensionalResizeWithTwoBatches8Bit (0 ms)
[ RUN      ] ResizeBilinearOpTest.ThreeDimensionalResize8Bit
tensorflow/contrib/lite/kernels/resize_bilinear_test.cc:293: Failure
Value of: m.GetOutput<uint8>()
Expected: has 18 elements where
element #0 is approximately 3 (absolute error <= 9.9999997e-06),
element #1 is approximately 4 (absolute error <= 9.9999997e-06),
element #2 is approximately 5 (absolute error <= 9.9999997e-06),
element #3 is approximately 8 (absolute error <= 9.9999997e-06),
element #4 is approximately 6 (absolute error <= 9.9999997e-06),
element #5 is approximately 10 (absolute error <= 9.9999997e-06),
element #6 is approximately 7 (absolute error <= 9.9999997e-06),
element #7 is approximately 8 (absolute error <= 9.9999997e-06),
element #8 is approximately 9 (absolute error <= 9.9999997e-06),
element #9 is approximately 12 (absolute error <= 9.9999997e-06),
element #10 is approximately 10 (absolute error <= 9.9999997e-06),
element #11 is approximately 14 (absolute error <= 9.9999997e-06),
element #12 is approximately 9 (absolute error <= 9.9999997e-06),
element #13 is approximately 10 (absolute error <= 9.9999997e-06),
element #14 is approximately 11 (absolute error <= 9.9999997e-06),
element #15 is approximately 13 (absolute error <= 9.9999997e-06),
element #16 is approximately 12 (absolute error <= 9.9999997e-06),
element #17 is approximately 16 (absolute error <= 9.9999997e-06)
  Actual: { '\x3' (3), '\x4' (4), '\x5' (5), '\b' (8), '\x6' (6), '\n' (10, 0xA), '\a' (7), '\b' (8), '\t' (9), '\f' (12, 0xC), '\n' (10, 0xA), '\xE' (14), '\t' (9), '\n' (10, 0xA), '\v' (11, 0xB), '\xE' (14), '\f' (12, 0xC), '          \x10' (16) }, whose element #15 doesn't match, which is 1 from 13
tensorflow/contrib/lite/kernels/resize_bilinear_test.cc:305: Failure
Value of: const_m.GetOutput<uint8>()
Expected: has 18 elements where
element #0 is approximately 3 (absolute error <= 9.9999997e-06),
element #1 is approximately 4 (absolute error <= 9.9999997e-06),
element #2 is approximately 5 (absolute error <= 9.9999997e-06),
element #3 is approximately 8 (absolute error <= 9.9999997e-06),
element #4 is approximately 6 (absolute error <= 9.9999997e-06),
element #5 is approximately 10 (absolute error <= 9.9999997e-06),
element #6 is approximately 7 (absolute error <= 9.9999997e-06),
element #7 is approximately 8 (absolute error <= 9.9999997e-06),
element #8 is approximately 9 (absolute error <= 9.9999997e-06),
element #9 is approximately 12 (absolute error <= 9.9999997e-06),
element #10 is approximately 10 (absolute error <= 9.9999997e-06),
element #11 is approximately 14 (absolute error <= 9.9999997e-06),
element #12 is approximately 9 (absolute error <= 9.9999997e-06),
element #13 is approximately 10 (absolute error <= 9.9999997e-06),
element #14 is approximately 11 (absolute error <= 9.9999997e-06),
element #15 is approximately 13 (absolute error <= 9.9999997e-06),
element #16 is approximately 12 (absolute error <= 9.9999997e-06),
element #17 is approximately 16 (absolute error <= 9.9999997e-06)
  Actual: { '\x3' (3), '\x4' (4), '\x5' (5), '\b' (8), '\x6' (6), '\n' (10, 0xA), '\a' (7), '\b' (8), '\t' (9), '\f' (12, 0xC), '\n' (10, 0xA), '\xE' (14), '\t' (9), '\n' (10, 0xA), '\v' (11, 0xB), '\xE' (14), '\f' (12, 0xC), '          \x10' (16) }, whose element #15 doesn't match, which is 1 from 13
[  FAILED  ] ResizeBilinearOpTest.ThreeDimensionalResize8Bit (1 ms)
[----------] 10 tests from ResizeBilinearOpTest (2 ms total)

[----------] Global test environment tear-down
[==========] 10 tests from 1 test case ran. (2 ms total)
[  PASSED  ] 8 tests.
[  FAILED  ] 2 tests, listed below:
[  FAILED  ] ResizeBilinearOpTest.TwoDimensionalResizeWithTwoBatches8Bit
[  FAILED  ] ResizeBilinearOpTest.ThreeDimensionalResize8Bit

 2 FAILED TESTS
================================================================================
```

### Source code / logs
see above
"
20358,Highlevel API do not well support float64 due to tf.feature_column.input_layer,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes. See the end.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Debian 9.4.

- **TensorFlow installed from (source or binary)**:
source.

- **TensorFlow version (use command below)**:
1.9.0-rc1

- **Python version**: 
3.5.3

- **Bazel version (if compiling from source)**:
0.11.1

- **GCC/Compiler version (if compiling from source)**:
6.3.0

- **CUDA/cuDNN version**:
NA (cpu only build)

- **GPU model and memory**:
NA (cpu only build)

- **Exact command to reproduce**:
See the python script in the end. Just run it like:
`python3 test_tf_float64.py`

### The problem
The function `tf.feature_column.input_layer` return tensor in dtype=float32 for input dtype=float64.

I know this is a [documented](https://github.com/tensorflow/tensorflow/blob/v1.9.0-rc1/tensorflow/python/feature_column/feature_column.py#L270) behavior, but this breaks things:

1. When the training data is in float64 precision, the user is (I am) expecting a float64 output, especially when the DNN written in high level API is used as a regressor. However, it returns float32, see the script in the end.

2. When building a custom estimator using high level API, seems that the input layer has to be created by `tf.feature_column.input_layer`. See the guide [here](https://www.tensorflow.org/get_started/custom_estimators#define_the_input_layer). This forbid the user from creating a network with dtype=float64 precision.

One may argue that dtype=float32 is already more than enough for DNN. But e.g. when doing theoretical work, the extra precision is desired.

A possible fix is:

Change line [2347](https://github.com/tensorflow/tensorflow/blob/v1.9.0-rc1/tensorflow/python/feature_column/feature_column.py#L2347) of function `_transform_feature` in file `tensorflow/python/feature_column/feature_column.py`, from

```python3
 return math_ops.to_float(input_tensor)
```

to

```python3
    if not input_tensor.dtype.is_floating:
      return math_ops.to_float(input_tensor)
    return input_tensor
```

Note: this fix could breaks old code that has float32 assumption...

If this is considered as non-bug, then consider this as a feature request that make it possible to pass dtype=float64 through.

### Source code
```python3
# Test tensorflow high level API.
# dtype=float64 will be silently transformed to float32.

import sys
import numpy as np
import tensorflow as tf

def load_data(dtype):
    l = 7
    x1 = np.linspace(-1.0, 1.0, l, dtype=dtype)
    z1 = np.sin(x1)
    return ({'x':x1}, {'z':z1})

def train_input_fn(features, labels, batch_size):
    dataset = tf.data.Dataset.from_tensor_slices((features, labels['z']))
    dataset = dataset.repeat().batch(batch_size)
    return dataset

def eval_input_fn(features):
    dataset = tf.data.Dataset.from_tensor_slices(features)
    dataset = dataset.batch(1)
    return dataset

def main(argv):
    dtype = np.float64   # specifies the data type

    batch_size = 7
    n_steps = 1

    (train_x, train_y) = load_data(dtype)
    my_feature_columns = [
        tf.feature_column.numeric_column(
            key   = 'x',
            shape = (1,),
            dtype = tf.as_dtype(train_x['x'].dtype))
    ]

    regressor = tf.estimator.DNNRegressor(   # or any other regressors
        hidden_units    = [3],
        feature_columns = my_feature_columns)

    regressor.train(
        input_fn = lambda:train_input_fn(train_x, train_y, batch_size),
        steps    = n_steps)

    predict_x = {'x': np.array([0.1, 0.2])}

    predictions = regressor.predict(
        input_fn=lambda:eval_input_fn(predict_x))

    print([i for i in predictions]) # outputs are float32 instead of float64

if __name__ == '__main__':
    tf.app.run(main)
```"
20356,CUDA_ERROR_LAUNCH_FAILED,"just wanna report this. it happened during the 60th call while I was calling a python script for about 100 times. (train 100 NNs).

system: gtx950m

at about the 30th call the neural network was not able to learn anymore. the accuracy and cost remained the same.

2018-06-27 21:10:37.421165: E tensorflow/stream_executor/cuda/cuda_driver.cc:1110] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED :: No stack trace available
2018-06-27 21:10:37.421173: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED
2018-06-27 21:10:37.421198: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1
Aborted (core dumped)

"
20355,Gradient of tf.where returns NaN when it should not.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Centos 7
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.8.0-3463-g39ea5a7044 1.10.0-dev20180620
- **Python version**: Python 3.6.5
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: Cuda 9.0
- **GPU model and memory**: NA
- **Exact command to reproduce**: 

```
from __future__ import absolute_import, division, print_function
import tensorflow as tf

transfer = lambda x: tf.where(x < 0, 1./(1.-x), tf.log(x+1)) 

def f(x):
    xt = tf.placeholder(tf.float32, shape=(1,))
    yt  = transfer(xt)
    dydx = tf.gradients(ys=yt, xs=xt)[0]
    with tf.Session() as sess:
        rval = sess.run([xt, yt, dydx], feed_dict={xt:[x]})
    return rval

print(f(1.))  # [1, 2, nan]
print(f(-1.)) # [-1, 0.5, nan]
print(f(0.)) # [0, 0, 1] (no nan here, interestingly)
```

### Problem
Gradients computed through the tf.where command erroneously returns nans. Presumably this is because the gradient computation computes the gradient through both conditions, which may result in a division by zero or log of zero."
20354,No instructions for NDK setup,"The setup steps for the NDK don't include how to install from an older version

Step 2 on https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android#bazel"
20353,convert frozon pb file to tflite but cannot find Output array which may be caused by that the frozen pb still have raining ops like Assign.,"I cannot generate tflite file which may be caused that frozon pb file still have training ops.

The operation like below:
use 
bazel-bin/tensorflow/python/tools/freeze_graph \
  --input_graph=/data/frankzhu/tmp/t2t_train/mnist/graph.pbtxt\
  --input_checkpoint=/data/frankzhu/tmp/t2t_train/mnist/model.ckpt-2000 \
  --input_binary=false \
  --output_graph=/data/frankzhu/tmp/mnist.pb \
  --output_node_names=save/restore_all

to get .tflite file

then use 

bazel run --config=opt \
  //tensorflow/contrib/lite/toco:toco -- \
  --input_file=/data/frankzhu/tmp/mnist.pb \
  --input_format=TENSORFLOW_GRAPHDEF \
  --output_format=TFLITE \
  --output_file=/data/frankzhu/tmp/mnist_q8.tflite \
  --inference_type=QUANTIZED_UINT8 \
  --input_type=FLOAT \
  --input_arrays=input \
  --output_arrays=save/restore_all/NoOp --input_shapes=1,28,28,1

But get error:
018-06-11 09:42:39.760033: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign
2018-06-11 09:42:39.760052: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Assign
2018-06-11 09:42:39.774575: F tensorflow/contrib/lite/toco/tooling_util.cc:806] Check failed: model.HasArray(output_array) Output array not found: save/restore_all/NoOp

Help from Andre Hentz' via TensorFlow Lite <tflite@tensorflow.org>, I was told that frozen graph may have training ops.

Basically I would like to freeze the graph and convert it to quantized int8 tflite. If you need more details, could you please let me know?


[mnist.tar.gz](https://github.com/tensorflow/tensorflow/files/2142432/mnist.tar.gz)





  "
20349,TF TRT Integration removes the input nodes for an Object Detection Model,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.8
- **Python version**:  3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: cuda 9, cudnn 7.1.3
- **GPU model and memory**: dual GTX 1080Ti
- **Exact command to reproduce**:
`from tensorflow.contrib import tensorrt as trt 
trt_graph = trt.create_inference_graph(
        input_graph_def=g.as_graph_def(),
        outputs=nodenames,
        max_batch_size=1,
        max_workspace_size_bytes=1 << 25,
        precision_mode=""FP32"",  # TRT Engine precision ""FP32"",""FP16"" or ""INT8""
        minimum_segment_size=2  # minimum number of nodes in an engine
        )`


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
With the TF-TRT integration for my object detection model, some of the input nodes are removed/fused. Is there a way to freeze some nodes from optimization?
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20348,tf.contrib.ffmpeg.decode_video error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.8.0-4015-g44a1f241bc', '1.9.0-rc0')
- **Python version**: 2.7.15rc1
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: 6.4.0
- **CUDA/cuDNN version**: 9.0.176/7.0.5
- **GPU model and memory**: GeForce GTX 1080 Ti (11GB)
- **Exact command to reproduce**:
```python
import tensorflow as tf
with tf.Session() as sess:
    movie_bin = tf.read_file('5205acf1-3d30b48d.mov')
    movie = tf.contrib.ffmpeg.decode_video(movie_bin)
    movie_ev = movie.eval()
    print(""****"", len(movie_ev))
```
### Describe the problem

The `tf.contrib.ffmpeg.decode_video` op throws an error and causes a core dump.

My ffmpeg version (`apt get install ffmpeg` on ubuntu 18.04):
```shell
ffmpeg version 3.4.2-2 Copyright (c) 2000-2018 the FFmpeg developers
  built with gcc 7 (Ubuntu 7.3.0-16ubuntu2)
  configuration: --prefix=/usr --extra-version=2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared
  libavutil      55. 78.100 / 55. 78.100
  libavcodec     57.107.100 / 57.107.100
  libavformat    57. 83.100 / 57. 83.100
  libavdevice    57. 10.100 / 57. 10.100
  libavfilter     6.107.100 /  6.107.100
  libavresample   3.  7.  0 /  3.  7.  0
  libswscale      4.  8.100 /  4.  8.100
  libswresample   2.  9.100 /  2.  9.100
  libpostproc    54.  7.100 / 54.  7.100
```
### Source code / logs
```shell
2018-06-27 09:15:22.489692: F tensorflow/contrib/ffmpeg/default/ffmpeg_lib.cc:405] Non-OK-status: ReadInfoFile(stderr_filename, width, height, frames) status: Unknown: Not enough video info returned by FFmpeg [0, 720, 1280, 3]Could not read FFmpeg stderr file: /tmp/tmp_file_tensorflow_3_ILW8Ht.err
Aborted (core dumped)
```"
20347,Bug on  create a Dataset from a DataFrame when  index its not ordered,"This code generate a  **segmentation fault**

```python
import pandas     as pd
import tensorflow as tf
import tensorflow.contrib.eager as tfe
tf.enable_eager_execution()

train = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=[2, 1, 4])
label = pd.Series([7, 8, 9], index=[2, 1, 4])

ds = tf.data.Dataset.from_tensor_slices((dict(train), label))
```

But this not,  (only index  it's changed) from **[2, 1, 4]** to **[0, 1, 4]**:
```python
import pandas     as pd
import tensorflow as tf
import tensorflow.contrib.eager as tfe
tf.enable_eager_execution()


train = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=[0, 1, 4])
label = pd.Series([7, 8, 9], index=[0, 1, 4])

ds = tf.data.Dataset.from_tensor_slices((dict(train), label))
```

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Kubuntu 17.10 (Artful Aardvark), Kernel 4.13.0-21-generic
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**:  3.6.3
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A


"
20346,Tensorflow Program Hangs on pthread_cond_wait,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS, xenial
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  tensorflow-gpu (1.8.0)
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0.176/7.1.3.16
- **GPU model and memory**: GeForce GTX 850M with 2GB of memory
- **Exact command to reproduce**:

### Describe the problem
I have a program using Tensorflow and it consistently hangs after a week or so, arbitrarily.  The GPU utilization goes to 0%, and the program stops training.  A stack trace shows that the program is stuck waiting on pthread_cond_wait.

### Source code / logs
I don't know that the source code will be of much use since it's not a concise example that reproduces the problem, but the code is available here: https://github.com/benbotto/bsy-dqn-atari/tree/breakout-best  My code is single threaded.

When last the program hung I took a stack trace.  That's available here: https://pastebin.com/LiPhz2CE

This looks similar to this report: https://github.com/tensorflow/tensorflow/issues/1947
 
Let me know if more information is needed."
20345,tf.train.init_from_checkpoint should add reshape parameter like the tf.train.Saver,"Have I written custom code . Yes, with Estimator API
OS Platform and Distribution. Ubuntu 16.04
TensorFlow installed from: pip
TensorFlow version: 1.8
Bazel version: N/A
CUDA/cuDNN version: 9.0
GPU model and memory: 1080Ti with 11G
Exact command to reproduce: N/A


Sometimes pretrained weights are squeezed while the weights for training have extra dimensions. For example, the shape of V1 in checkpoint is [128], while shape of V1 in target net is [1,1,1,128].  The reshape parameter in tf.train.Saver can solve this issue. But when I use Estimator to load pretrained weights, it will be very difficult to load weights with mismatch shapes.
"
20344,When I save a tensorflow session in windows the file(e.g 1.ckpt.data-00000-of-00001) size becomes around 1.9 GB whereas in linux it is around 270 MB. The file generated in windows gives 2% accuracy(supposed to be 82%) when restored in testing.  So the weights are not getting saved properly and many extra info gets stored in Windows I think.  Kindly help to solve the problem in Windows.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20343,"Please, invest into profiler documentation",After trying to figure out how to use tensorflow profiler to test my model I am giving up. Current `README.md`  does not only lack details but is actually quite confusing. 
20342,distributed training with SyncReplicasOptimizer got stuck after a number of iterations,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
I am running distributed training using SyncReplicasOptimizer, after about 10k iterations, the workers got stuck. CPU usage drops to 0 percent. 

The arguments for SyncReplicasOptimizer:
replicas_to_aggregate = 60, total_num_replicas = 64 (I have 64 workers)

It might also be worth noting that this happens after 27 workers finish their training data.

Connecting to one of the stuck worker processes using gdb I get the following backtraces:

#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38
#1  0x00007f5813609de4 in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007f58136095b1 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007f5813606af4 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007f5813607015 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007f58111a4b23 in tensorflow::(anonymous namespace)::WaitForNotification(tensorflow::CallOptions*, long long, tensorflow::Notification*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007f58111a54ab in tensorflow::LocalMaster::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007f5811187256 in tensorflow::GrpcSession::RunProto(tensorflow::CallOptions*, tensorflow::MutableRunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007f581118767d in tensorflow::GrpcSession::RunHelper(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, std::string const&) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007f5811187ceb in tensorflow::GrpcSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007f5811468dba in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007f58114699b6 in TF_SessionRun () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007f5811119256 in tensorflow::TF_SessionRun_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007f581111939a in tensorflow::TF_SessionRun_wrapper(TF_Session*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007f58110d5b3e in _wrap_TF_SessionRun_wrapper () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so

Any ideas ? Thanks!
"
20341,SWIGing tensorflow/contrib/lite/toco/python/toco.i failed,"
Hi There..
While installing tensorflow-1.4.0 usning bazel-0.9.0 getting the error as follows.Can any one help to solve this.
my environmet
-----------------
gcc-4.8.3 java-1.8.0 bazel-0.9.0


ERROR: /remote/us01home34/ids_cm/rgatti/temp/RHEL6/tensorflow/tensorflow/contrib/lite/toco/python/BUILD:22:1: SWIGing tensorflow/contrib/lite/toco/python/toco.i failed (Exit 1)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1172.854s, Critical Path: 45.81s
FAILED: Build did NOT complete successfully


Thanks in advance"
20340,reg. tensorflow-gpu in amd windows,"**I have Win 10/ AMD Raedon R5 M335
I am using Python 3.5 and tensorflow 1.0.0
I am also, working through Spyder as using Python Console would make handling the code very difficult**

Please guide me how should i use my gpu in the training of deep learning model i am working on. Currently it uses my CPU.

**PS: No solutions to previously made issues worked for me.**"
20339,"A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x30 in tid 28963 (flitecamerademo).I found this error while compiling the TfliteCameraDemo app","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20338,Keras Model functional API with custom submodel not working in eager execution?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Darwin localhost 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.8
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
See below


### Describe the problem
Consider the following keras model with a custom submodel:

```
tf.enable_eager_execution()

class SubModel(tf.keras.Model):
    def __init__(self):
        super(SubModel, self).__init__()
        self.layer = tf.keras.layers.Dense(3) 
    def call(self, inputs):
        return self.layer(inputs)

def MyModel():
    input = tf.keras.Input(shape=(3, 3))
    m = SubModel()
    output = m(input)
    return tf.keras.Model(input, output)

m = MyModel()
m(tf.constant(tf.ones([3, 3])))
```

where the SubModel is a custom model, and MyModel() uses it in the functional API. The code raises error:

> File ""/Library/Python/2.7/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py"", line 639, in compute_output_shape
>     raise NotImplementedError

I think it might be because the submodel cannot calculate the shape. So I added a compute_output_shape method for the SubModel class as if it's a custom layer:

```
class SubModel(tf.keras.Model):
    ...
    def compute_output_shape(self, input_shape):
        return (input_shape[0], 3)
```

Now the NotImplementedError disappeared, but we have a new error when running the model:

> AssertionError: Could not compute output DeferredTensor('None', shape=(3,), dtype=float32)

Now I don't know what to do. The code actually works in non-eager mode, so I guess it's a bug for keras functional API in eager execution?

"
20337,profiler add_step make import_meta_graph error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 
- **Python version**: 
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
- **GPU**ï¼š**as i test, ï¼ˆgpu-tf, gpu-sessï¼‰or (cpu-tf,cpu-sess)ï¼Œ    this error will always occur.**

### Produce
1. prepare data
cp ner_data /ceph_ai/xiahong/data/ -rf
2. python train.py -i example_configs/ner_config.py
notice MODEL_PATH
3. test follow 2 commands
-* python predict.py -i {MODEL_PATH}-0
-* python predict.py -i {MODEL_PATH}-34   (should fail load model if run add step, else works)

comments train.py 86 ""profiler.add_step(step=step, run_meta=run_metadata)"" and try 2,3 again.

### Describe the problem
after run profiler add_step, 
the saver.save file cannot import_meta_graph 

### Source code / logs
```
Traceback (most recent call last):
  File ""predict.py"", line 126, in <module>
    tag_model=Classifier(args.model_path)
  File ""predict.py"", line 50, in __init__
    self.tf_model=TFModel(sess,model_path,inputs,outputs)
  File ""/dockerdata/xiahong/tf_text_modeling-master/text_modeling/tf_utils/predict.py"", line 8, in __init__
    saver = tf.train.import_meta_graph(""{}.meta"".format(model_path), clear_devices=True)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1955, in import_meta_graph
    **kwargs)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py"", line 743, in import_scoped_meta_graph
    producer_op_list=producer_op_list)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 493, in import_graph_def
    raise ValueError(str(e))
ValueError: Can not squeeze dim[1], expected a dimension of 1, got 200 for 'cond_2/Squeeze' (op: 'Squeeze') with input shapes: [500,200,19].
```
 without train.py line 86, it works well.

[ner_data.zip](https://github.com/tensorflow/tensorflow/files/2140517/ner_data.zip)
[text_modeling.zip](https://github.com/tensorflow/tensorflow/files/2140519/text_modeling.zip)

"
20336,Tensorflow failed to build on x64 when build with MSVC,"System information
â€¢	Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
N/A
â€¢	OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows server 2016
â€¢	TensorFlow installed from (source or binary):
Source
â€¢	TensorFlow version (use command below):
Master branch latest revison
â€¢	Python version:
Anaconda 4.1.1 (Python 3.5 64-bit)
â€¢	Bazel version (if compiling from source):
N/A
â€¢	GCC/Compiler version (if compiling from source):
VS2017 15.5.7
â€¢	CUDA/cuDNN version:
NVidia CUDA Toolkit 8.0
NVidia CUDNN 5.1
â€¢	GPU model and memory:
N/A
â€¢	Exact command to reproduce:
N/A

**Describe the problem:**
Tensorflow failed to build on x64. This issue can be reproduced from the master revision 68bb435. This should be tensorflow source issue, could you please help take a look at this? Thanks!

**The failures like:**
The whole log file please see attachment.
[tensorflow_x64_build.log](https://github.com/tensorflow/tensorflow/files/2140452/tensorflow_x64_build.log)

**Repro steps:**
1. git clone https://github.com/tensorflow/tensorflow D:\Tensorflow\src
2. pushd D:\Tensorflow
3. set PreferredToolArchitecture=x64
4. set rel=Release
5. set CUDNN_HOME=""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\cuda""
6. set PY=C:\ProgramData\Anaconda3
7. set CL=/FS /permissive-
8. cmake D:\Tensorflow\src\tensorflow\contrib\cmake -A x64 -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=C:\ProgramData\Anaconda3\python.exe -DPYTHON_LIBRARIES=C:\ProgramData\Anaconda3\libs\python36.lib -DSWIG_EXECUTABLE=D:\Tensorflow\swigwin-3.0.12\swig.exe -Dtensorflow_BUILD_PYTHON_TESTS=ON -Dtensorflow_BUILD_SHARED_LIB=ON
9. MSBuild /m /p:Configuration=Release;Platform=x64 /p:WindowsTargetPlatformVersion=10.0.17134.0 tensorflow.sln /t:Rebuild"
20335,Keras Model functional API not working in eager execution?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Darwin localhost 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.8
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
See below


### Describe the problem
Consider the following keras model with a custom submodel:

```
tf.enable_eager_execution()

class SubModel(tf.keras.Model):
    def __init__(self):
        super(SubModel, self).__init__()
        self.layer = tf.keras.layers.Dense(3) 
    def call(self, inputs):
        return self.layer(inputs)

def MyModel():
    input = tf.keras.Input(shape=(3, 3))
    m = SubModel()
    output = m(input)
    return tf.keras.Model(input, output)

m = MyModel()
m(tf.constant(tf.ones([3, 3])))
```

where the SubModel is a custom model, and MyModel() uses it in the functional API. The code raises error:

> File ""/Library/Python/2.7/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py"", line 639, in compute_output_shape
>     raise NotImplementedError

I think it might be because the submodel cannot calculate the shape. So I added a compute_output_shape method for the SubModel class as if it's a custom layer:

```
class SubModel(tf.keras.Model):
    ...
    def compute_output_shape(self, input_shape):
        return (input_shape[0], 3)
```

Now the NotImplementedError disappeared, but we have a new error when running the model:

> AssertionError: Could not compute output DeferredTensor('None', shape=(3,), dtype=float32)

Now I don't know what to do. The code actually works in non-eager mode, so I guess it's a bug for keras functional API in eager execution?

"
20334,Incompatible shapes between op input and calculated input gradient.,"When I try to implement a SegNet with tensorflow, I met up with the following issue:  

```
Incompatible shapes between op input and calculated input gradient.
```

My net structure is here below:  

```
 def buildNet(self):
        #TODO: Implemente this method
        self.X = tf.placeholder(dtype=tf.float32, shape=(self.batchsize, self.width, self.height, self.channel))
        self.Y = tf.placeholder(dtype=tf.int32, shape=(self.batchsize, self.width, self.height))

        norm = tf.nn.lrn(self.X, depth_radius=5, bias=1.0, alpha=1e-4, beta=0.75)
        # Encoder of segnet
        self.enconv1 = batchnorm(conv2d(norm, [3,3,self.channel,64], [1,1,1,1]))
        self.enconv2 = batchnorm(conv2d(self.enconv1, [3,3,64,64], [1,1,1,1]))
        self.enpool1 = maxpooling2d(self.enconv2, [1,2,2,1], [1,2,2,1])

        self.enconv3 = batchnorm(conv2d(self.enpool1, [3,3,64,128], [1,1,1,1]))
        self.enconv4 = batchnorm(conv2d(self.enconv3, [3,3,128,128], [1,1,1,1]))
        self.enpool2 = maxpooling2d(self.enconv4, [1,2,2,1], [1,2,2,1])

        self.enconv5 = batchnorm(conv2d(self.enpool2, [3,3,128,256], [1,1,1,1]))
        self.enconv6 = batchnorm(conv2d(self.enconv5, [3,3,256,256], [1,1,1,1]))
        self.enpool3 = maxpooling2d(self.enconv6, [1,2,2,1], [1,2,2,1])

        self.enconv7 = batchnorm(conv2d(self.enpool3, [3,3,256,512], [1,1,1,1]))
        self.enconv8 = batchnorm(conv2d(self.enconv7, [3,3,512,512], [1,1,1,1]))
        self.enpool4 = maxpooling2d(self.enconv8, [1,2,2,1], [1,2,2,1])

        self.enconv9 = batchnorm(conv2d(self.enpool4, [3,3,512,512], [1,1,1,1]))
        self.enconv10 = batchnorm(conv2d(self.enconv9, [3,3,512,512], [1,1,1,1]))
        self.enpool5 = maxpooling2d(self.enconv10, [1,2,2,1], [1,2,2,1])

        # Decoder of segnet
        self.depool1 = deconv2d(self.enpool5, [3,3,512,512], self.enconv10.shape, [1,1,1,1])
        self.deconv1 = batchnorm(conv2d(self.depool1, [3,3,512,512], [1,1,1,1]))
        self.deconv2 = batchnorm(conv2d(self.deconv1, [3,3,512,512], [1,1,1,1]))

        self.depool2 = deconv2d(self.deconv2, [3,3,512,512], self.enconv8.shape, [1,1,1,1])
        self.deconv3 = batchnorm(conv2d(self.depool2, [3,3,512,256], [1,1,1,1]))
        self.deconv4 = batchnorm(conv2d(self.deconv3, [3,3,256,256], [1,1,1,1]))

        self.depool3 = deconv2d(self.deconv4, [3,3,256,256], self.enconv6.shape, [1,1,1,1])
        self.deconv5 = batchnorm(conv2d(self.depool3, [3,3,256,128], [1,1,1,1]))
        self.deconv6 = batchnorm(conv2d(self.deconv5, [3,3,128,128], [1,1,1,1]))

        self.depool4 = deconv2d(self.deconv6, [3,3,128,128], self.enconv4.shape, [1,1,1,1])
        self.deconv7 = batchnorm(conv2d(self.depool4, [3,3,128,64], [1,1,1,1]))
        self.deconv8 = batchnorm(conv2d(self.deconv7, [3,3,64,64], [1,1,1,1]))

        self.depool5 = deconv2d(self.deconv8, [3,3,64,64], self.enconv2.shape, [1,1,1,1])
        self.deconv9 = batchnorm(conv2d(self.depool5, [3,3,64,32], [1,1,1,1]))
        self.deconv10 = batchnorm(conv2d(self.deconv9, [3,3,32,self.classes], [1,1,1,1]))

        self.pred = tf.nn.softmax(self.deconv10)
        self.oneHotY = tf.one_hot(self.Y, depth=2, on_value=1.0, off_value=0.0)
        self.loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=self.oneHotY, logits=self.pred))
        print self.loss.shape
        self.optimizer = tf.train.AdamOptimizer(self.learningrate).minimize(self.loss)
```

I think it is the problem of **tf.conv2d_trainspose**. Does anyone have idea about it?
"
20333,Tensorflow build issue in RHEL6 machine,"-----------------------

### System information
- [root@abinaya-tensorflow ~]$ cat /etc/redhat-release
Red Hat Enterprise Linux Server release 6.8 (Santiago)

-TensorFlow installed from source
- TensorFlow version 
- Python 2.7.11 
- Bazel version :

[root@abinaya-tensorflow tensorflow-r1.6]$ bazel version
Build label: 0.14.1- (@non-git)
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Jun 27 06:45:12 2018 (1530081912)
Build timestamp: 1530081912
Build timestamp as int: 1530081912

- GCC/Compiler version :

[root@abinaya-tensorflow ~]$ gcc --version
gcc (GCC) 8.1.0
Copyright (C) 2018 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


### Describe the problem

We are trying to build Tensorflow 1.6 from source code as per the help of below guide:

https://www.tensorflow.org/install/install_sources

please find below steps which i tried:

***********************************************************************************************************
[root@abinaya-tensorflow tensorflow-r1.6]$ bazel version
**Build label: 0.14.1- (@non-git)**
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Jun 27 06:45:12 2018 (1530081912)
Build timestamp: 1530081912
Build timestamp as int: 1530081912

[root@abinaya-tensorflow tensorflow-r1.6]$ ./configure
Traceback (most recent call last):
  File ""configure.py"", line 1459, in <module>
    main()
  File ""configure.py"", line 1346, in main
    check_bazel_version('0.5.4')
  File ""configure.py"", line 459, in check_bazel_version
    curr_version = run_shell(['bazel', '--batch', 'version'])
  File ""configure.py"", line 154, in run_shell
    output = subprocess.check_output(cmd)
AttributeError: 'module' object has no attribute 'check_output'

Already available bazel version is 0.14.1 but i couldn't run the ""configure"" script of tensorflow.

Can you please help us to build the tensorflow from source code.


### Source code / logs

Available version of gcc and JAVA and bazel in our machine:

#################################################################
[root@abinaya-tensorflow ~]$ which gcc
/opt/gcc/x86_64/8.1.0/bin/gcc

[root@abinaya-tensorflow ~]$ echo $JAVA_HOME
/opt/j2se/x86_64/1.8.0_172

[root@abinaya-tensorflow ~]$ bazel version
WARNING: ignoring http_proxy in environment.
Build label: 0.14.1- (@non-git)
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Jun 27 06:45:12 2018 (1530081912)
Build timestamp: 1530081912
Build timestamp as int: 1530081912
#################################################################

Issue which we are facing while run the configure script of tensorflow is:

################################################################
[root@abinaya-tensorflow tensorflow-r1.6]$ ls
ACKNOWLEDGMENTS     AUTHORS             CODEOWNERS    CONTRIBUTING.md    models.BUILD  tensorflow   util
ADOPTERS.md         BUILD               configure     ISSUE_TEMPLATE.md  README.md     third_party  WORKSPACE
arm_compiler.BUILD  CODE_OF_CONDUCT.md  configure.py  LICENSE            RELEASE.md    tools
[root@abinaya-tensorflow tensorflow-r1.6]$ ./configure
Traceback (most recent call last):
  File ""configure.py"", line 1459, in <module>
    main()
  File ""configure.py"", line 1346, in main
    check_bazel_version('0.5.4')
  File ""configure.py"", line 459, in check_bazel_version
    curr_version = run_shell(['bazel', '--batch', 'version'])
  File ""configure.py"", line 154, in run_shell
    output = subprocess.check_output(cmd)
AttributeError: 'module' object has no attribute 'check_output'
####################################################################
"
20332,An attempt at Tensorflow and Bazel CPU/GPU build (v1.8.0),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Well, I added patches
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10, 64 bit
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**:  3.5, 3.6
- **Bazel version (if compiling from source)**: 1.12.0, 1.14.0
- **GCC/Compiler version (if compiling from source)**:  MSVC 2015 Update v3
- **CUDA/cuDNN version**: 9.0/7.1
- **GPU model and memory**: GeForce GT 430
- **Exact command to reproduce**: ;-)

### Description
I wanted to compile tensorflow with GPU support on Windows and I went through the ordeal (I was finally able to build it, but I'll come to that later). I always followed the stuff done in: https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/tools/ci_build/

This is how it went:

First, I attempted the CPU build.
Initial attempt was with CMake. It took ~3hours, all went fine upto the final linking phase, there link.exe died with:
```
(Lib target) ->
  tf_core_kernels.dir\Release\tf_core_kernels.lib : fatal error LNK1248: image size (100039B2C) exceeds maximum allowable size (FFFFFFFF) 
```
I was lost here, I tried tinkering with some CMake build options, but all resulted into the same error. So, I gave up CMake.

Then, I attempted to build with bazel 1.12.0.
I was using the mingw64 shell, so first, I had to patch `tensorflow/tools/pip_package/build_pip_package.sh` . I also sent a PR https://github.com/tensorflow/tensorflow/pull/18953 

Surprisingly, the build went fine after that, and then I tried to run the tests. All were successful, except `tensorflow/python/kernel_tests/boosted_trees:training_ops_test`,  probably because it was not being run serially. Even for running the tests, I had to patch `tensorflow/contrib/tensorboard/plugins/trace/trace.py` and I also sent a PR https://github.com/tensorflow/tensorflow/pull/18954

Now comes the real deal. The GPU build.

Again, I attempted the build with CMake. It took ~3hours and then link.exe failed with the same error as before. I spent quite some time reading about it and couldn't figure it out as most solutions said 'break your huge lib into smaller libs'. I am still baffled as to why the CI job at http://ci.tensorflow.org/job/tf-master-win-gpu-cmake/ doesn't die with the same problem. I am using the same compiler, same linker, 
and my machine is powerful enough. So, I just gave up again on CMake.

Going back to bazel now.  By this time, bazel 1.14.0 was was released. So I decided to use that instead. I had to apply the two patches as before, but life isn't so easy. 

Next big hurdle: https://github.com/tensorflow/tensorflow/issues/17067 .
-  So I got @dtrebbien 's patch from https://github.com/dtrebbien/protobuf/commit/50f552646ba1de79e07562b41f3999fe036b4fd0 and made changes to `tensorflow/workspace.bzl` to apply it to all protobuf checkouts.

Next big hurdle: NCCL doesn't seem to be officially supported on Windows. 
- But bazel is still trying to build nccl related stuff. I had to patch stuff again, to disable all references to nccl in the BUILD files all over the place. :crying_cat_face: 

Next big hurdle:
-  Shared libraries under `tensorflow/contrib/rnn/python/ops/` failed at link phase with the error:
```
Creating library bazel-out/host/bin/tensorflow/contrib/rnn/python/ops/_gru_ops.ifso and object bazel-out/host/bin/tensorflow/contrib/rnn/python/ops/_gru_ops.exp
blas_gemm.o : error LNK2019: unresolved external symbol ""public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenBlasGemm(enum perftools::gputools::blas::Transpose,enum perftools::gputools::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,float,class perftools::gputools::DeviceMemory<float> const &,int,class perftools::gputools::DeviceMemory<float> const &,int,float,class perftools::gputools::DeviceMemory<float> *,int)"" (?ThenBlasGemm@Stream@gputools@perftools@@QEAAAEAV123@W4Transpose@blas@23@0_K11MAEBV?$DeviceMemory@M@23@H2HMPEAV623@H@Z) referenced in function ""public: void __cdecl tensorflow::functor::TensorCuBlasGemm<float>::operator()(class tensorflow::OpKernelContext *,bool,bool,unsigned __int64,unsigned __int64,unsigned __int64,float,float const *,int,float const *,int,float,float *,int)"" (??R?$TensorCuBlasGemm@M@functor@tensorflow@@QEAAXPEAVOpKernelContext@2@_N1_K22MPEBMH3HMPEAMH@Z)
blas_gemm.o : error LNK2019: unresolved external symbol ""public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenBlasGemm(enum perftools::gputools::blas::Transpose,enum perftools::gputools::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,double,class perftools::gputools::DeviceMemory<double> const &,int,class perftools::gputools::DeviceMemory<double> const &,int,double,class perftools::gputools::DeviceMemory<double> *,int)"" (?ThenBlasGemm@Stream@gputools@perftools@@QEAAAEAV123@W4Transpose@blas@23@0_K11NAEBV?$DeviceMemory@N@23@H2HNPEAV623@H@Z) referenced in function ""public: void __cdecl tensorflow::functor::TensorCuBlasGemm<double>::operator()(class tensorflow::OpKernelContext *,bool,bool,unsigned __int64,unsigned __int64,unsigned __int64,double,double const *,int,double const *,int,double,double *,int)"" (??R?$TensorCuBlasGemm@N@functor@tensorflow@@QEAAXPEAVOpKernelContext@2@_N1_K22NPEBNH3HNPEANH@Z)
```
- Then I stumbled upon https://github.com/tensorflow/tensorflow/issues/15013 and saw that another person had faced the same issue quite some time ago and the OP's solution was `add all the .lib from tensorflow and cuda` . I can't do that. So, I went looking for where this symbol comes from, and where should it be. Took me a while to realize what's happening and that's when I came across the workaround 
of using intermediate interface shared object file at https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/tensorflow.bzl#L1228-L1237. Apparently, this didn't have enough symbols for the kernels in `tensorflow/contrib/rnn/python/ops/`. So I started looking for where I could find them and ended up adding:
`clean_dep(""//tensorflow/stream_executor:stream_executor_impl""),` to that list and the link error went away.

Next hurdle: https://github.com/tensorflow/tensorflow/issues/20088
-  The compiler doesn't like
https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc#L46 when it was already being done at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops_gpu.h#L189 . So I commented out the typdef in `fused_conv2d_bias_activation_op.cc` and tried building it. The error went away, but link.exe cried again at the end, because it didn't know where to get the symbol for `GetCudnnWorkspaceLimit` at https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc#L519 . After some searching, I found out that this symbol comes from https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/core/kernels/conv_ops.cc#L456-L471 . So I tried making it part of the intermediary additional_deps_impl.ifso import lib, but no matter what, it always ended up bloating it with symbols > 64k (it went upto ~72k, when I tried adding some deps which might have that symbol). So I gave up on this contrib kernel at this point and when I looked at the CMake build (I could be wrong here) I didn't find it building this kernel either. So I just ended up commenting out all references to `fused_conv` in the BUILD files.

Next hurdle: Bazel had made some breaking changes in 1.13.0
-  I came across at https://github.com/bazelbuild/bazel/issues/4583 and then applied two other patches locally to get around that problem.

Next hurdle: Tests won't run.
- Most of them want to import nccl. :sob: So I ended up faking it by creating a blank module and then the tests ran fine. I did have to tell bazel to ignore the `contrib/lite` subset `-//${PY_TEST_DIR}/tensorflow/contrib/lite/...`.

For the lost souls who attempt at building tensorflow with GPU support, I hope this issue might be helpful. All the patches and the entire build can be found here: https://github.com/AnacondaRecipes/aggregate/blob/459dee0989e0c7cc4ab66c49d3d7605cddbb1bc3/tensorflow-gpu-base-feedstock/recipe/meta.yaml"
20327,warning: no files found matching '*.dll' under directory '*',"Hi all, this problem has troubled me for a long time and I have not found a suitable solution.

My system informations:
- **Have I written custom code**: No, I just try to make a building wheel by Bazel.
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: source
- **TensorFlow version**: latest
- **Python version**: 3.5 
- **Bazel version**: 0.14.1
- **GCC/Compiler version**: 5.0+
- **CUDA/cuDNN version**: 8.0, 7.0.5
- **GPU model and memory**: GeForce GTX 1050 Ti (4GB)
- **Exact command to reproduce**:`bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`

When I make a building wheel:

`bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`

 I get some warnings like these:

```
2018å¹´ 06æœˆ 26æ—¥ æ˜ŸæœŸäºŒ 14:22:53 CST : === Preparing sources in dir: /tmp/tmp.DJDLaJRILD
~/tensorflow ~/tensorflow
~/tensorflow
2018å¹´ 06æœˆ 26æ—¥ æ˜ŸæœŸäºŒ 14:24:22 CST : === Building wheel
warning: no files found matching '*.dll' under directory '*'
warning: no files found matching '*.lib' under directory '*'
warning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'
warning: no files found matching '*' under directory 'tensorflow/include/Eigen'
warning: no files found matching '*.h' under directory 'tensorflow/include/google'
warning: no files found matching '*' under directory 'tensorflow/include/third_party'
warning: no files found matching '*' under directory 'tensorflow/include/unsupported'
2018å¹´ 06æœˆ 26æ—¥ æ˜ŸæœŸäºŒ 14:26:01 CST : === Output wheel file is in: /tmp/tensorflow_pkg
```

Then, I check the **Output wheel files**, **but I did not find any tmp/tensorflow folder or .whl file.** 
I think many people are also facing the same confusion, anyone can help me?

Thanks in advance!
"
20326,codelabs tutorial poets doesn't work,"Your tutorial does not work on a variety of fronts. You cannot complete the tutorial from start to end just trying to use your instructions. I dare you to do so. 

I tried this using Anaconda AND Pycharm

https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0"
20320,_dataset_ops.so not found,"Hi, 

Trying to run this code https://towardsdatascience.com/implementing-a-generative-adversarial-network-gan-dcgan-to-draw-human-faces-8291616904a

However, I keep running into an error:

---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-8-c79e12159a4e> in <module>()
----> 1 g = generator(noise, keep_prob, is_training)
      2 print(g)
      3 d_real = discriminator(X_in)
      4 d_fake = discriminator(g, reuse=True)
      5 

<ipython-input-7-61000d187a2b> in generator(z, keep_prob, is_training)
     42         x = tf.layers.dense(x, units=d1 * d1 * d2, activation=activation)
     43         x = tf.layers.dropout(x, keep_prob)
---> 44         x = tf.contrib.layers.batch_norm(x, is_training=is_training, decay=momentum)
     45 
     46         x = tf.reshape(x, shape=[-1, d1, d1, d2])

D:\anaconda\lib\site-packages\tensorflow\python\util\lazy_loader.py in __getattr__(self, item)
     51 
     52   def __getattr__(self, item):
---> 53     module = self._load()
     54     return getattr(module, item)
     55 

D:\anaconda\lib\site-packages\tensorflow\python\util\lazy_loader.py in _load(self)
     40   def _load(self):
     41     # Import the target module and insert it into the parent's namespace
---> 42     module = importlib.import_module(self.__name__)
     43     self._parent_module_globals[self._local_name] = module
     44 

D:\anaconda\lib\importlib\__init__.py in import_module(name, package)
    124                 break
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 
    128 

D:\anaconda\lib\importlib\_bootstrap.py in _gcd_import(name, package, level)

D:\anaconda\lib\importlib\_bootstrap.py in _find_and_load(name, import_)

D:\anaconda\lib\importlib\_bootstrap.py in _find_and_load_unlocked(name, import_)

D:\anaconda\lib\importlib\_bootstrap.py in _load_unlocked(spec)

D:\anaconda\lib\importlib\_bootstrap_external.py in exec_module(self, module)

D:\anaconda\lib\importlib\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

D:\anaconda\lib\site-packages\tensorflow\contrib\__init__.py in <module>()
     32 from tensorflow.contrib import crf
     33 from tensorflow.contrib import cudnn_rnn
---> 34 from tensorflow.contrib import data
     35 from tensorflow.contrib import deprecated
     36 from tensorflow.contrib import distribute

D:\anaconda\lib\site-packages\tensorflow\contrib\data\__init__.py in <module>()
     65 from tensorflow.contrib.data.python.ops.counter import Counter
     66 from tensorflow.contrib.data.python.ops.enumerate_ops import enumerate_dataset
---> 67 from tensorflow.contrib.data.python.ops.error_ops import ignore_errors
     68 from tensorflow.contrib.data.python.ops.get_single_element import get_single_element
     69 from tensorflow.contrib.data.python.ops.grouping import bucket_by_sequence_length

D:\anaconda\lib\site-packages\tensorflow\contrib\data\python\ops\error_ops.py in <module>()
     18 from __future__ import print_function
     19 
---> 20 from tensorflow.contrib.data.python.ops import contrib_op_loader  # pylint: disable=unused-import
     21 from tensorflow.contrib.data.python.ops import gen_dataset_ops
     22 from tensorflow.python.data.ops import dataset_ops

D:\anaconda\lib\site-packages\tensorflow\contrib\data\python\ops\contrib_op_loader.py in <module>()
     22 
     23 _dataset_ops = loader.load_op_library(
---> 24     resource_loader.get_path_to_datafile(""../../_dataset_ops.so""))

D:\anaconda\lib\site-packages\tensorflow\contrib\util\loader.py in load_op_library(path)
     54       return None
     55   path = resource_loader.get_path_to_datafile(path)
---> 56   ret = load_library.load_op_library(path)
     57   assert ret, 'Could not load %s' % path
     58   return ret

D:\anaconda\lib\site-packages\tensorflow\python\framework\load_library.py in load_op_library(library_filename)
     54     RuntimeError: when unable to load the library or get the python wrappers.
     55   """"""
---> 56   lib_handle = py_tf.TF_LoadLibrary(library_filename)
     57 
     58   op_list_str = py_tf.TF_GetOpList(lib_handle)

NotFoundError: D:\anaconda\lib\site-packages\tensorflow\contrib\data\python\ops\..\..\_dataset_ops.so not found

I'm running Python 3.6 and have same problem on Windows 10 and Mac Osx. Tensorflow is latest available through Anaconda. Cuda 9.0. Tensorflow 1.8.0

This error also affects my efforts to look at MNIST tutorials for commands such as:

from tensorflow.examples.tutorials.mnist import input_data"
20313,Can't build tf-lite benchmark_model,"

Hi all. I am trying to run tflite models on my desktop and for that trying to build benchmark models as given in the benchmark tools github page https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/benchmark . However when I run the command

`bazel build -c opt tensorflow/contrib/lite/tools/benchmark:benchmark_model`

Please help me with this :) Thanks in advance.
I encounter the following error -
System information

    OS Platform and Distribution (Linux Ubuntu):
    TensorFlow installed from (source): Yes
    **TensorFlow version **: 1.9.0rc1
    Python version: 2.7
    Bazel version (if compiling from source):
    GCC/Compiler version (if compiling from source):
    CUDA/cuDNN version: No CUDA
    GPU model and memory:
    Exact command to reproduce:

Describe the problem

The command I ran

bazel build -c opt tensorflow/contrib/lite/tools/benchmark:benchmark_model

**Source code / logs**

**tensorflow/contrib/lite/profiling/profile_summarizer.cc: In function 'tflite::profiling::{anonymous}::OperatorDetails tflite::profiling::{anonymous}::GetOperatorDetails(const tflite::Interpreter&, int)':
tensorflow/contrib/lite/profiling/profile_summarizer.cc:86:27: error: 'string' was not declared in this scope
details.name += "":"" + string(profiling_string);
^~~~~~
tensorflow/contrib/lite/profiling/profile_summarizer.cc:86:27: note: suggested alternatives:
In file included from /usr/include/c++/7/iosfwd:39:0,
from /usr/include/c++/7/memory:72,
from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
from ./tensorflow/contrib/lite/allocation.h:25,
from ./tensorflow/contrib/lite/interpreter.h:24,
from ./tensorflow/contrib/lite/profiling/profile_summarizer.h:21,
from tensorflow/contrib/lite/profiling/profile_summarizer.cc:16:
/usr/include/c++/7/bits/stringfwd.h:74:33: note: 'std::__cxx11::string'
typedef basic_string string;
^~~~~~
/usr/include/c++/7/bits/stringfwd.h:74:33: note: 'std::__cxx11::string'
Target //tensorflow/contrib/lite/tools/benchmark:benchmark_model failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2.069s, Critical Path: 1.77s
INFO: 0 processes.
FAILED: Build did NOT complete successfully**
"
20311,"Could not find 'nvcuda.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Typically it is installed in 'C:\Windows\System32'. If it is not present, ensure that you have a CUDA-capable GPU with the correct driver installed.","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20309,Very high discrepancy in memory usage and computation time in convolution operation between tf versions,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

    I attach a small code example that should explain the issue,  but in principle it is nothing but multiple 1x1 convolutions with a conv1D layer.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

    VERSION=""16.04.3 LTS (Xenial Xerus)""
    VERSION_ID=""16.04""
    VERSION_CODENAME=xenial


- **TensorFlow installed from (source or binary)**:

    pip install tensorflow-gpu==1.3.0
    and
    pip install tensorflow-gpu==1.8.0

- **TensorFlow version (use command below)**:
The following tf versions were used to recreate the issue
    
    tf.GIT_VERSION = v1.8.0-0-g93bc2e2072 
    tf.VERSION = 1.8.0
    and
    tf.GIT_VERSION = b'unknown' 
    tf.VERSION = 1.3.0

- **Python version**:
Python 3.5.5 :: Anaconda, Inc.
- **CUDA/cuDNN version**:
    For tf 1.3.0:
    nvcc: NVIDIA (R) Cuda compiler driver
    Copyright (c) 2005-2016 NVIDIA Corporation
    Built on Tue_Jan_10_13:22:03_CST_2017
    Cuda compilation tools, release 8.0, V8.0.61
    
    For tf 1.8.0
    nvcc: NVIDIA (R) Cuda compiler driver
    Copyright (c) 2005-2017 NVIDIA Corporation
    Built on Fri_Sep__1_21:08:03_CDT_2017
    Cuda compilation tools, release 9.0, V9.0.176

- **GPU model and memory**:
    GeForce GTX 1080 Ti
    total memory shown as 10.91GiB
- **Exact command to reproduce**:
    python 'script shown below'

### Describe the problem
When trying to update from tensorflow 1.3.0 to 1.8.0, we noticed that memory consumption and computation time increased significantly for our networks (both >10%). 
We tried to find out what was causing this issue and realized that there is a large discrepancy for memory and computation time in our 1D convolutional layers, both increasing roughly by a factor of two when going from tf 1.3 to tf 1.8. 
Something similar happens in the tf 1.3 version when changing the data format from NWC to NCW, but tf 1.8 is slow regardless of the data format.
I attach a small code snippet, which creates a network of 30 1x1 1D convolutinal layers with some arbitrary parameters for batch size, width, and number of filters. 
The network output is evaluated 10,000 times and the time spent for this is written to stdout, for different data formats and two different libraries for the 1D convolution (nn and layers). 
The following output is generated for the different tf versions. 
~~~~
tf version 1.3.0 	 order NWC 	 library layers	 time 5.840178s
tf version 1.3.0 	 order NWC 	 library nn	 time 5.809642s
tf version 1.3.0 	 order NCW 	 library layers	 time 10.344764s
tf version 1.3.0 	 order NCW 	 library nn	 time 9.630965s

tf version 1.8.0 	 order NWC 	 library layers	 time 13.982041s
tf version 1.8.0 	 order NWC 	 library nn	 time 13.616668s
tf version 1.8.0 	 order NCW 	 library layers	 time 11.336001s
tf version 1.8.0 	 order NCW 	 library nn	 time 10.919789s

~~~~

Tracking memory consumption for this network with nvidia-smi showed that tf 1.8 allocates around 467MiB for all different configurations, whereas tf 1.3 allocates 223MiB for NWC data format and up to 449MiB for the NCW format.

Moreover, we compared the outputs of the two different data formats. They appear to be the same.

### Source code / logs
~~~~
#############################################
## Created by Moritz Boehle moritz@audatic.ai
#############################################
import time
import tensorflow as tf
import numpy as np
import os

# Exclude tf logs in output for better overview
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# Some arbitrary parameters for the network
num_filters = 16
num_layers = 30
width = 100
batch_size = 20
steps = 10000

# Allow GPU growth
config = tf.ConfigProto()
config.gpu_options.allow_growth = True

# Below, we will compare performance of the two different data formats.
formats = [""NWC"", ""NCW""]

# Tested with 1.3.0 and 1.8.0
tf_v = tf.__version__

# Different libs for performing 1D convolution.
libs = [""layers"", ""nn""]

# Options for the two different data formats.
ncw_opts = {""shape"": [batch_size, num_filters, width],
            ""data_format_layers"": ""channels_first"",
            ""data_format_nn"": ""NCW"" if tf_v == ""1.8.0"" else ""NCHW""}
nwc_opts = {""shape"": [batch_size, width, num_filters],
            ""data_format_layers"": ""channels_last"",
            ""data_format_nn"": ""NWC"" if tf_v == ""1.8.0"" else ""NHWC""}
data_format_opts = {""NCW"": ncw_opts, ""NWC"": nwc_opts}

for data_format in formats:
    opts = data_format_opts[data_format]
    for lib in libs:
        with tf.Session(config=config) as sess:
            with tf.device(""/gpu:0""):
                shape = opts[""shape""]
                format_str = opts[""data_format_""+lib]

                # Create arbitrary input.
                layer = tf.constant(np.random.random(shape), dtype=tf.float32)

                # Create num_layers of 1D convolutions.
                for _ in range(num_layers):
                    if lib == ""layers"":
                        layer = tf.layers.conv1d(layer, filters=num_filters,
                                                 kernel_size=[1], strides=[1],
                                                 data_format=format_str)
                    elif lib == ""nn"":
                        filters = tf.Variable(tf.random_normal(
                            [1, num_filters, num_filters]))
                        bias = tf.Variable(tf.random_normal(shape))
                        layer = tf.nn.conv1d(layer, filters, 1, ""VALID"",
                                             data_format=format_str)
                        layer += bias
                sess.run(tf.global_variables_initializer())

                # Measure time to run 'steps' steps.
                start = time.time()
                for i in range(steps):
                    sess.run(layer)
                total = time.time() - start
                print(""tf version {tf} \t order {order} \t library {lib}""
                      ""\t time {total:f}s"".format(tf=tf_v, total=total,
                                                  order=data_format, lib=lib))


~~~~
"
20308,Results are different on MacOS for fixed data,"Hello.

We've encountered strange behaviour of TF (tested on versions 1.7.0 and 1.8.0) on MacOS (tested on 10.11.6) with Python 2.7 (tested on 2.7.10). While running this code:

```python
import tensorflow as tf

class Model(object):
    def __init__(self):
        one_hot = tf.constant([[1, 0, 0, 1], [0, 1, 1, 2], [1, 1, 1, 3]])
        logits = tf.constant([[0.3, 0.2, 0], [0.1, 0.0004, 0], [0, 0.02, 0.2]])

        self.one_hot = one_hot
        self.logits = logits
        self.rec_at_k = self._recall_at_k()
        self.prec_at_k = self._precision_at_k()

        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        self.sess = tf.Session(config=config)
        self.sess.run(tf.global_variables_initializer())
        self.sess.run(tf.local_variables_initializer())


    def _recall_at_k(self, top=3):
        recalls = []
        for k in range(1,top+1):
            recalls.append(tf.metrics.recall_at_k(tf.cast(self.one_hot, tf.int64), tf.cast(self.logits, tf.float32), k=k))
        return recalls

    def _precision_at_k(self, top=3):
        precs = []
        for k in range(1,top+1):
            precs.append(tf.metrics.recall_at_k(tf.cast(self.one_hot, tf.int64), self.logits, k=k))
        return precs

    def fit(self):
        recalls, precs = self.sess.run([self.rec_at_k, self.prec_at_k])
        for rec,prec in zip(recalls,precs):
            print rec[0],rec, prec
        l,o = self.sess.run([self.logits, self.one_hot])
        print l
        print o, o[:,:-1]


model = Model()
model.fit()
```


we receive results which are different from run to run, like:


```
0.2857142857142857 ('0.2857142857142857', '0.2857142857142857') ('1.0', '0.2857142857142857')
0.7142857142857143 ('0.7142857142857143', '0.7142857142857143') ('1.0', '0.7142857142857143')
0.0 ('0.0', '0.8571428571428571') ('1.0', '0.8571428571428571')
[[0.3    0.2    0.    ]
 [0.1    0.0004 0.    ]
 [0.     0.02   0.2   ]]
[[1 0 0 1]
 [0 1 1 2]
 [1 1 1 3]] [[1 0 0]
 [0 1 1]
 [1 1 1]]
```


and


```
0.4 ('0.4', '0.2857142857142857') ('1.0', '0.2857142857142857')
1.0 ('1.0', '0.7142857142857143') ('0.7142857142857143', '0.7142857142857143')
0.0 ('0.0', '0.8571428571428571') ('1.0', '0.8571428571428571')
[[0.3    0.2    0.    ]
 [0.1    0.0004 0.    ]
 [0.     0.02   0.2   ]]
[[1 0 0 1]
 [0 1 1 2]
 [1 1 1 3]] [[1 0 0]
 [0 1 1]
 [1 1 1]]
```


On linux (Ubuntu 16.04 4.13.0-45-generic) and Python (2.7.14) results are stable."
20306,How to install 0.9 or 1.0.0 in 2018?,"i have posted this issue on stackoverflow as well, i have searched all over internet but my issue is still unresolved. I want to use tensorflow 0.9 or 1.0.0 or earlier versions on windows 10. a project needs older version of tensorflow to run. can somebody please guide me? and sorry if this question does not follow guidelines, i just need to install tensorflow earlier version."
20305,Compiling TF from source on Debian9 does not work as documented - partial fix,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Debian 9, with nvidia-driver 390, and nvidia-cuda-toolkit 9.1.85 from the repository ""stretch-backports""

- **TensorFlow installed from (source or binary)**:

trying to install from source - but this fails (see description below) 

- **TensorFlow version (use command below)**:
1.8
- **Python version**: 
3.5.2
- **Bazel version (if compiling from source)**:
0.14.1
- **GCC/Compiler version (if compiling from source)**:
6.3
- **CUDA/cuDNN version**:
9.1/7.1
- **GPU model and memory**:
GTX1080Ti
- **Exact command to reproduce**:
```

TF_NEED_CUDA=1 \
GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
CUDA_TOOLKIT_PATH=/usr \
TF_CUDA_VERSION=9.1 \
TF_CUDNN_VERSION=7.1 \
CUDNN_INSTALL_PATH=/mnt/nfs/clustersw/shared/cuda/cudnn-9.1-linux-x64-v7.1/ \
TF_CUDA_COMPUTE_CAPABILITIES=5.2,6.1,6.2 \
./configure 

bazel build 
bazel build //tensorflow/tools/pip_package:build_pip_package 
```
### Describe the problem
I was trying to install TF 1.8 on a  Debian 9 system with python/3.5.2, Cuda 9.1, cudnn 7.1. Because all attempts to used some some precomiled TF wheels failed, I'm trying to install TF from source. Cuda, and libcupti are installed from stretch-backports repository 
    apt-get install -t stretch-backports nvidia-cuda-toolkit libcupti-dev nvidia-driver

In order to compile TF from source, the following changes to the build system where necessary.

```
diff --git a/configure.py b/configure.py
index ad585fa52e..af5ead70da 100644
--- a/configure.py
+++ b/configure.py
@@ -33,12 +33,12 @@ except ImportError:
   from distutils.spawn import find_executable as which
 # pylint: enable=g-import-not-at-top
 
-_DEFAULT_CUDA_VERSION = '9.0'
-_DEFAULT_CUDNN_VERSION = '7'
+_DEFAULT_CUDA_VERSION = '9.1'
+_DEFAULT_CUDNN_VERSION = '7.1'
 _DEFAULT_NCCL_VERSION = '1.3'
-_DEFAULT_CUDA_COMPUTE_CAPABILITIES = '3.5,5.2'
-_DEFAULT_CUDA_PATH = '/usr/local/cuda'
-_DEFAULT_CUDA_PATH_LINUX = '/opt/cuda'
+_DEFAULT_CUDA_COMPUTE_CAPABILITIES = '5.2,6.1,6.2'
+_DEFAULT_CUDA_PATH = '/usr'
+_DEFAULT_CUDA_PATH_LINUX = '/usr'
 _DEFAULT_CUDA_PATH_WIN = ('C:/Program Files/NVIDIA GPU Computing '
                           'Toolkit/CUDA/v%s' % _DEFAULT_CUDA_VERSION)
 _DEFAULT_TENSORRT_PATH_LINUX = '/usr/lib/%s-linux-gnu' % platform.machine()
@@ -839,7 +839,7 @@ def set_tf_cuda_version(environ_cp):
     if is_windows():
       cuda_rt_lib_path = 'lib/x64/cudart.lib'
     elif is_linux():
-      cuda_rt_lib_path = 'lib64/libcudart.so.%s' % tf_cuda_version
+      cuda_rt_lib_path = 'lib/x86_64-linux-gnu/libcudart.so.%s' % tf_cuda_version
     elif is_macos():
       cuda_rt_lib_path = 'lib/libcudart.%s.dylib' % tf_cuda_version
 
diff --git a/third_party/gpus/cuda_configure.bzl b/third_party/gpus/cuda_configure.bzl
index c90c66912d..f55d0a455e 100644
--- a/third_party/gpus/cuda_configure.bzl
+++ b/third_party/gpus/cuda_configure.bzl
@@ -59,6 +59,7 @@ CUDA_LIB_PATHS = [
 CUPTI_HEADER_PATHS = [
   ""extras/CUPTI/include/"",
   ""include/cuda/CUPTI/"",
+  ""include/""
 ]
 
 # Lookup paths for the cupti library, relative to the
@@ -67,7 +68,7 @@ CUPTI_HEADER_PATHS = [
 # the other CUDA libraries but rather in a special extras/CUPTI directory.
 CUPTI_LIB_PATHS = [
   ""extras/CUPTI/lib64/"",
-  ""lib/x86_64-linux-gnu"",
+  ""lib/x86_64-linux-gnu/"",
   ""lib64/"",
   ""extras/CUPTI/libx64/"",
   ""extras/CUPTI/lib/"",
@@ -94,6 +95,7 @@ CUDNN_INCLUDE_PATHS = [
 NVVM_LIBDEVICE_PATHS = [
   ""nvvm/libdevice/"",
   ""share/cuda/"",
+  ""lib/nvidia-cuda-toolkit/libdevice/""
 ]
 
 load(""//third_party/clang_toolchain:download_clang.bzl"", ""download_clang"")`
```


With these changes, ""bazel build"" was sucessful. However, the command 
   bazel build //tensorflow/tools/pip_package:build_pip_package
would still fail with 

`bazel build //tensorflow/tools/pip_package:build_pip_package 
WARNING: Output base '/nfs/scistore12/jonasgrp/schloegl/.cache/bazel/_bazel_schloegl/8678486dd934320ead8a15f39d2403d9' is on NFS. This may lead to surprising failures and undetermined behavior.
Starting local Bazel server and connecting to it...
..........................
ERROR: /nfs/scistore_grp/schloegl/.cache/bazel/_bazel_schloegl/8678486dd934320ead8a15f39d2403d9/external/local_config_cuda/cuda/BUILD:48526:11: in cmd attribute of genrule rule @local_config_cuda//cuda:cuda-include: '$SocketInputStream' syntax is not supported; use '$(SocketInputStream)' instead for ""Make"" variables, or escape the '$' as '$$' if you intended this for the shell
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_cuda//cuda:cuda-include' failed; build aborted
INFO: Elapsed time: 7.423s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (186 packages loaded)
`

"
20304,d,#20303 close #20303 
20303,#24,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20302,Error while importing tensorflow in ubuntu,"Hi all,
I have installed tensorflow=1.5.0 in ubuntu 14.04. while importing tensorflow I got the below error. Please let me know how to handle this issue.

**>>> import tensorflow as tf**
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/__init__.py"", line 22,                          in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im                         port
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py"", l                         ine 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/core/framework/graph_p                         b2.py"", line 15, in <module>
    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_do                         t_framework_dot_node__def__pb2
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/core/framework/node_de                         f_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_                         dot_framework_dot_attr__value__pb2
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/core/framework/attr_va                         lue_pb2.py"", line 15, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_                         framework_dot_tensor__pb2
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/core/framework/tensor_                         pb2.py"", line 15, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_                         core_dot_framework_dot_resource__handle__pb2
  File ""/usr/local/lib/python3.4/dist-packages/tensorflow/core/framework/resourc                         e_handle_pb2.py"", line 22, in <module>
    serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\nte                         nsorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x                         12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\                         thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x1                         8org.tensorflow.frameworkB\x0eResourceHandleP\x01Z=github.com/tensorflow/tensorf                         low/tensorflow/go/core/framework\xf8\x01\x01\x62\x06proto3')
**TypeError: __init__() got an unexpected keyword argument 'serialized_options'**
"
20301,InternalError: Dst tensor is not initialized.,"`2018-06-26 09:22:30.314633: W tensorflow/core/common_runtime/bfc_allocator.cc:277] **************xx***x******************************************xx********xx***********_____**********
Traceback (most recent call last):
  File ""/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1323, in _do_call
    return fn(*args)
  File ""/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1302, in _run_fn
    status, run_metadata)
  File ""/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
         [[Node: dynamic_seq2seq/decoder/embedding_lookup/_61 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_50_dynamic_seq2seq/decoder/embedding_lookup"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 21, in <module>
    tf.app.run(main=nmt.main, argv=[os.getcwd() + '\nmt\nmt\nmt.py'] + unparsed)
  File ""/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/datadrived/jai/nmt-chatbot-v1/nmt/nmt/nmt.py"", line 529, in main
    run_main(FLAGS, default_hparams, train_fn, inference_fn)
  File ""/datadrived/jai/nmt-chatbot-v1/nmt/nmt/nmt.py"", line 522, in run_main
    train_fn(hparams, target_session=target_session)
  File ""/datadrived/jai/nmt-chatbot-v1/nmt/nmt/train.py"", line 251, in train
    sample_tgt_data)
  File ""/datadrived/jai/nmt-chatbot-v1/nmt/nmt/train.py"", line 125, in run_full_eval
    eval_model, eval_sess, model_dir, hparams, summary_writer)
  File ""/datadrived/jai/nmt-chatbot-v1/nmt/nmt/train.py"", line 56, in run_internal_eval
    summary_writer, ""dev"")
  File ""/datadrived/jai/nmt-chatbot-v1/nmt/nmt/train.py"", line 405, in _internal_eval
    ppl = model_helper.compute_perplexity(model, sess, label)
  File ""/datadrived/jai/nmt-chatbot-v1/nmt/nmt/model_helper.py"", line 414, in compute_perplexity
    loss, predict_count, batch_size = model.eval(sess)
  File ""/datadrived/jai/nmt-chatbot-v1/nmt/nmt/model.py"", line 221, in eval
    self.batch_size])
  File ""/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
         [[Node: dynamic_seq2seq/decoder/embedding_lookup/_61 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_50_dynamic_seq2seq/decoder/embedding_lookup"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
`

I get the above error when i try to run the train.py using the gpu server"
20300,W T:\src\github\tensorflow\tensorflow\core\framework\op_kernel.cc:1318] OP_REQUIRES failed at save_restore_tensor.cc:170 : ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No. I am trying to run the Speech_Commands example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**:  3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: 

### Describe the problem
The training of 18000 steps is over successfully and I have got both the confusion matrix and the accuracy. I am trying to freeze the model by running freeze.py. I am getting errors. 

### Source code / logs
The command I typed is as follows: as given in the tutorial.
`C:\Program Files\Python35\Lib\site-packages\tensorflow>python ""C:\Program Files\python35\Lib\site-packages\tensorflow\examples\speech_commands\freeze.py"" \`


The output I got is as follows:

```
2018-06-26 12:55:44.437915: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-06-26 12:55:45.011031: W T:\src\github\tensorflow\tensorflow\core\framework\op_kernel.cc:1318] OP_REQUIRES failed at save_restore_tensor.cc:170 : Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for
Traceback (most recent call last):
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\python35\Lib\site-packages\tensorflow\examples\speech_commands\freeze.py"", line 180, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""C:\Program Files\python35\Lib\site-packages\tensorflow\examples\speech_commands\freeze.py"", line 117, in main
    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)
  File ""C:\Program Files\python35\Lib\site-packages\tensorflow\examples\speech_commands\models.py"", line 123, in load_variables_from_checkpoint
    saver.restore(sess, start_checkpoint)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\training\saver.py"", line 1802, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

Caused by op 'save/RestoreV2', defined at:
  File ""C:\Program Files\python35\Lib\site-packages\tensorflow\examples\speech_commands\freeze.py"", line 180, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""C:\Program Files\python35\Lib\site-packages\tensorflow\examples\speech_commands\freeze.py"", line 117, in main
    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)
  File ""C:\Program Files\python35\Lib\site-packages\tensorflow\examples\speech_commands\models.py"", line 122, in load_variables_from_checkpoint
    saver = tf.train.Saver(tf.global_variables())
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\training\saver.py"", line 1338, in __init__
    self.build()
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\training\saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\training\saver.py"", line 1384, in _build
    build_save=build_save, build_restore=build_restore)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\training\saver.py"", line 835, in _build_internal
    restore_sequentially, reshape)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\training\saver.py"", line 472, in _AddRestoreOps
    restore_sequentially)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\training\saver.py"", line 886, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 1546, in restore_v2
    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for
         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
```
I found a similar issue here [#15426](https://github.com/tensorflow/tensorflow/issues/15426)

Doesn't mention how they solved the problem though. I have commented on that as well. Still no response. 

So I am posting a new thread here. "
20299,Layer norm for cudnnlstm?,"I am training a very large and deep lstm-RNN. Using cudnnlstm can save memories and speed up the training process. However, I need some extra functions such as layer norm, adding attention wrapper, etc.
Is there a way to achieve this? Should I rewrite the c++ code and recompile? If so, which file of the source code contains the cudnnlstm implementation?

PS: In the documentation, there is only one bit mentioning topics related to the layer norm of cudnn. It is in the performance guides. It only tells developers there is no layer norm for cudnnlstm. However, layer norm is very important for a decent lstm network. I would like to know the method to adding the layer norm algorithm similar to the LayerNormBasicLstm class. I explored a bit via trying to scoop out the params (weights and biases) of cudnn. It seems the shape of those two are not specific in the process of building the graph.

**Have I written custom code**
Not yet
**OS Platform and Distribution**
Ubuntu 16.04
**TensorFlow installed from**
pip 
**TensorFlow version**
1.8.0
**Bazel version**
N/A
**CUDA/cuDNN version**
Cuda 8.0  
**GPU model and memory**
Nvidia GTX 1080
**Exact command to reproduce**
N/A

"
20298,Memory leak when run multi-times in C++,"I build tensorflow dll in Win10 and call it in my project. I get correct results, but when running serveral times it will leak CPU memory, is there any resource I do not release ? Here is some C++ code
```
for (int i = 0; i < 1000; i++)
{
  NewSession(SessionOptions(), &_session);
  ReadBinaryProto(Env::Default(), model_path, &_graphdef);
  _session->Create(_graphdef);
  _session->Run(inputs, _output_tensor_names, {}, &outputs);
  _session->Close();
}
```
if I change to the following code, there is no CPU memery leak, but I hope to release GPU memory when not using GPU
```
NewSession(SessionOptions(), &_session);
ReadBinaryProto(Env::Default(), model_path, &_graphdef);
_session->Create(_graphdef);
for (int i = 0; i < 1000; i++)
{
  _session->Run(inputs, _output_tensor_names, {}, &outputs);
  _session->Close();
}
```
make code simple , only newsession and close multi-times, it will leak CPU memory
```
for (int i = 0; i < 1000; i++)
{
  NewSession(SessionOptions(), &_session);
  _session->Close();
}
```
"
20297,Why does tf.placeholder() change the random state?,"![image](https://user-images.githubusercontent.com/22118890/41892870-4e437440-794c-11e8-9267-31f600c34181.png)
"
20293,"transfer learning: /scripts/label_image.py"", line 137, in <module>     print(template.format(labels[i], results[i])) IndexError: list index out of range","In Tensorflow 1.8, Ubuntu 16.04, Anaconda Python 3.6.5, I have:

```
mona@Mona:~/code/alternative-controllers/style_transfer$ python -m scripts.label_image     --graph=dataset/retrained_graph.pb      --image=dataset/charmander/charmander30.jpg 
/home/mona/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-06-25 18:51:53.814201: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-06-25 18:51:53.934034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 11.90GiB freeMemory: 10.57GiB
2018-06-25 18:51:53.934061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-06-25 18:51:54.128693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-06-25 18:51:54.128723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-06-25 18:51:54.128728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-06-25 18:51:54.128937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10233 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-06-25 18:51:54.240643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-06-25 18:51:54.240673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-06-25 18:51:54.240678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-06-25 18:51:54.240682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-06-25 18:51:54.240766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10233 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)

Evaluation time (1-image): 0.752s

Traceback (most recent call last):
  File ""/home/mona/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/mona/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/mona/code/alternative-controllers/style_transfer/scripts/label_image.py"", line 137, in <module>
    print(template.format(labels[i], results[i]))
IndexError: list index out of range
```


I was successfully able to follow the tutorial for transfer learning for flowers from Google Collab, but then I tried it on Pokemon images of 200*200 and set the image size to 224 (because it did not accept the 200), I was able to pretrain but inference does not work. Can you please guide how to fix?

I got the pokemon images from:
https://github.com/AravindVasudev/Pokedex

and I followed this codelab tutorial for transfer learning:
https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#8

As you can see the retrained_labels.txt and retrained_graph.pb are already made:

```
mona@Mona:~/code/alternative-controllers/style_transfer$ ls -ltra dataset/
total 5524
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 alakazam
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 abra
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 bulbasaur
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 blastoise
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 articuno
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 magikarp
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 ivysaur
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 gengar
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 charmeleon
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 charmander
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 charizard
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 squirtle
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 pikachu
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 moltres
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 mewtwo
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 mew
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 meowth
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 zapdos
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 wartortle
drwxrwxr-x  2 mona mona    4096 Jun 25 18:41 venusaur
drwxr-xr-x  4 mona mona    4096 Jun 25 18:45 training_summaries
drwxrwxr-x  3 mona mona    4096 Jun 25 18:45 models
drwxrwxr-x 22 mona mona    4096 Jun 25 18:45 bottlenecks
-rw-rw-r--  1 mona mona     168 Jun 25 18:45 retrained_labels.txt
-rw-rw-r--  1 mona mona 5548864 Jun 25 18:45 retrained_graph.pb
drwxrwxr-x 25 mona mona    4096 Jun 25 18:45 .
drwxrwxr-x  7 mona mona    4096 Jun 25 18:51 ..

```"
20292,Error:Execution failed for task ':buildNativeBazel'. > A problem occurred starting process 'command '/usr/local/bin/bazel'',"So, I downloaded whole repository and I tried to run the 'android' application:
tensorflow-master/tensorflow/examples/android
but I get the runtime error from title.

there is this line of code in build.gradle:
def nativeBuild='bazel';
and when I change it, for example, to 'none', I get problem with camera.
However, I would like to solve this bazel buildSystem because it seems that is default.

**Top-level directory of the model I am using**
This is path to application:
home/AndroidStudioProjects/tensorflow-master/tensorflow/examples/android
Everything is set to default

**Have I written custom code**
No

**OS Platform and Distribution**
Ubuntu 16.04

**TensorFlow installed from**
I installed TensorFlow by issuing pip commands

**TensorFlow version**
1.5.0

**Bazel version**
Build label: 0.13.0

**CUDA/cuDNN version**
Cuda compilation tools, release 7.5, V7.5.17

**GPU model and memory**
Intel Corporation 4 Series Chipset Integrated Graphics Controller (rev 03) (prog-if 00 [VGA Controller])
Subsystem: Dell 4 Series Chipset Integrated Graphics Controller
4 GB

Thanks in advance"
20286,java.lang.RuntimeException: Fail to connect to camera service,"So, I downloaded whole repository and I tried to run the 'android' application:
tensorflow-master/tensorflow/examples/android
but I get the runtime error from title, because the old camera API is deprecated.

However, I couldn't find elegant solution to this problem.
Implementing Camera2 API seems very complicated, because modifying all existing code to match Camera2 seems like quite complicated solution.
Is there a fix?

**Top-level directory of the model I am using**
This is path to application:
home/AndroidStudioProjects/tensorflow-master/tensorflow/examples/android
Everything is set to default

**Have I written custom code**
No

**OS Platform and Distribution**
Ubuntu 16.04

**TensorFlow installed from**
I installed TensorFlow by issuing pip commands

**TensorFlow version**
1.5.0

**Bazel version**
Build label: 0.13.0

**CUDA/cuDNN version**
Cuda compilation tools, release 7.5, V7.5.17

**GPU model and memory**
Intel Corporation 4 Series Chipset Integrated Graphics Controller (rev 03) (prog-if 00 [VGA Controller])
Subsystem: Dell 4 Series Chipset Integrated Graphics Controller
4 GB

"
20283,Truncated Distributions in TensorFlow,"At this point, only [tf.truncated_normal](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) is implemented within TensorFlow. Are there any plans for implementing others aswell? ex. Truncated Gamma, Truncated Exponential etc. Truncated distributions come up often in probabilistic programming, a branch in which TensorFlow is becoming prominent. I for one do most of my probabilistic work and sampling in TensorFlow, due to the GPU support and the control it offers me.

Each truncated distribution offers a different method of efficiently sampling from it, so distribution specific algorithms would be ideal.

In the mean time, I implemented a general method for personal use. I attach the specifications bellow, in case there is any interest in this general solution (the implementation, documentation and examples are available [here](https://github.com/ZigaSajovic/truncatedDistribution)). Note that I am willing to work on improving it, in case interest is present.

**TruncatedDistribution**

The class [TruncatedDistribution](https://github.com/ZigaSajovic/truncatedDistribution) extends any existing TensorFlow distribution, i.e. classes inheriting from [tf.distribution](https://www.tensorflow.org/api_docs/python/tf/distributions/Distribution), to enable their truncated counterparts, with full support of broadcasting.

**Methods:**

* \_\_init\_\_(disttribution,left,right, n_points=1000)
* sample(sample_shape=())
* cdf(X)
* log_cdf(X)
* survival_function(X)
* log_survival_function(X)
* prob(X)
* log_prob(X)
* mean(n_samples=1000)
* variance(n_samples=1000)
* stddev(n_samples=1000)"
20280,Provide a way to build a Tensorflow wheel without a dependency on Tensorboard,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**: 6.3.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

When building a custom Tensorflow wheel from source there is currently no way to disable the resulting wheels dependency on `tensorboard`. Tensorboard is great during development but is not useful when running in a headless server environment. It brings with it a large number of dependencies, and as I believe a custom tensorflow build is likely to be used in this kind of environment and it would be great to be able to disable the `install_requires` dependency on tensorboard in this case.
"
20279,"the ""body_inputs"" variable of Whilecontext class should be std::vector<InputTensor> ","Class tensorflow::Whilecontext private body_inputs_ is in the form of std::vector< Outputvector > now, which should be std::vector< Inputvector >. Although this bug may not be effect the function."
20276,tflite_convert raises TypeError: object of type 'zip' has no len(),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: docker image tensorflow/tensorflow:nightly-py3
- **TensorFlow installed from (source or binary)**: -
- **TensorFlow version (use command below)**: latest (1.9.0 rc1)
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: `tflite_convert --output_file=out.tflite --saved_model_dir=1529923978/ --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_dev_values=127 --input_array=input`

### Describe the problem
Trying to run the script above in Python 3 raises a `TypeError: object of type 'zip' has no len()`.
This is caused by [line 107 of the `tflite_converter` script](https://github.com/tensorflow/tensorflow/blob/f202958ee2d5177a474e3d107fdbf0c83174d099/tensorflow/contrib/lite/python/tflite_convert.py#L107), more specifically by `len(quant_stats)`. `quant stats` is defined in [line 105](https://github.com/tensorflow/tensorflow/blob/f202958ee2d5177a474e3d107fdbf0c83174d099/tensorflow/contrib/lite/python/tflite_convert.py#L105) as `zip(mean_values, std_dev_values)`.

"
20275,`tf.profiler.profile` outputs negative number of flops,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See below

### Describe the problem
`tf.profiler.profile` outputs negative number of flops for `tf.nn.conv2d` ops on large inputs because of np.int32 overflow.

### Source code / logs
```py
import tensorflow as tf
from tensorflow.python.framework.ops import get_stats_for_node_def

test_conv2d = tf.nn.conv2d(tf.zeros((1, 1024, 1024, 16)), tf.zeros((4, 4, 16, 4)), strides=[1, 1, 1, 1], padding='SAME')
# This function is used by tf.profiler.profile
stats = get_stats_for_node_def(tf.get_default_graph(), test_conv2d.op.node_def, 'flops')
# Compare the results: one is negative, the other is the actual result
# The size is exactly 2^31
print(stats.value, 1 * 1024 * 1024 * 4 * 16 * 4 * 4 * 2)
```

The reason is:
https://github.com/tensorflow/tensorflow/blob/cfebbbc94f3edd1622a9a42379dd2ccc956ea52c/tensorflow/python/ops/nn_ops.py#L2169
here `np.prod` is used which returns an `np.int32`. This enforces the next product line 
https://github.com/tensorflow/tensorflow/blob/cfebbbc94f3edd1622a9a42379dd2ccc956ea52c/tensorflow/python/ops/nn_ops.py#L2172
to compute everything as `np.int32`. Actually, the protobuf supports int64:
https://github.com/tensorflow/tensorflow/blob/cf375f06747b7be998f0df329772390f545577a1/tensorflow/core/profiler/tfprof_output.proto#L105
so this line could be replaced by:
```py
np.prod(output_shape.as_list(), dtype=np.int64)
```
There are more ops affected by this issue, as they also use `np.prod`.


PS: Sorry that I cannot submit the fix straight away, everything needed to fix this issue is mentioned above."
20274,Error:Execution failed for task ':buildNativeBazel'. > A problem occurred starting process 'command '/usr/local/bin/bazel'',"When I download whole repository and try to run android application:
tensorflow-master/tensorflow/examples/android
I get the error from the title.

This error happens because of this code from build.gradle :
// set to 'bazel', 'cmake', 'makefile', 'none'
def nativeBuildSystem = 'bazel'

When I try to change to some other build system, I get different errors. For example, when I put to 'none', I get the camera problem, but I would like to solve this 'bazel' problem, because it looks like it is default build system.

I use Ubuntu 16.04,  
I have TensorFlow version 1.5.0
Bazel version: Build label: 0.13.0,
CUDA version: Cuda compilation tools, release 7.5, V7.5.17

GPU model and memory
Intel Corporation 4 Series Chipset Integrated Graphics Controller (rev 03) (prog-if 00 [VGA Controller])
Subsystem: Dell 4 Series Chipset Integrated Graphics Controller
4 GB

Thank you in advance
"
20273,Add examples to documentation for CudnnRNN,"Have I written custom code: N/A
OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: 1.8.0
Bazel version: N/A
CUDA/cuDNN version: 9.0/7.0
GPU model and memory: NVIDIA GTX 1080 Ti
Exact command to reproduce: N/A

It would be very helpful to add usage examples on how to use `cudnn_rnn`, particularly on cross-compatibility between non-CUDA and CUDA-supporting devices. Users should be able to figure out how to save/restore weights to run their models with, say, `tf.nn.rnn_cell.LSTMCell` or `tf.contrib.cudnn_rnn.CudnnLSTM`.

There are classes that seem to do this (e.g. `tf.contrib.cudnn_rnn.CudnnLSTMSaveable`) but there are no easily accessible code examples showing how they should be used.

*A possible feature request*: would it be possible to have a high-level wrapper that makes this choice automatically based on the availability of a CUDA device, saving and restoring weights accordingly? This should be possible with `tf.test.is_gpu_available(cuda_only=True)`."
20272,Custom implementation for Dequantize when converting .pb to .tflite,"I'm trying to convert tensorflow quantised .pb file to .lite using toco. The command for creating .pb file is :
retrain.py is [here](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/scripts/retrain.py) and [here](https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py).
```
python retrain.py \
--bottleneck_dir=/mobilenet_q/bottlenecks \
--how_many_training_steps=4000 \
--output_graph=/mobilenet_q/retrained_graph_mobilenet_q_1_224.pb \
--output_labels=/mobilenet_q/retrained_labels_mobilenet_q_1_224.txt \
--image_dir=/data \
--architecture=mobilenet_1.0_224_quantized
```
When I'm trying to convert the .pb file to .tflite using toco command:
```
bazel run --config=opt //tensorflow/contrib/lite/toco:toco \
  -- --input_file= retrained_graph_mobilenet_q_1_224.pb \
  --output_file= retrained_graph_mobilenet_q_1_224.lite \
  --input_format=TENSORFLOW_GRAPHDEF \
  --output_format=TFLITE \
  --input_shape=1,224,224,3 \
  --input_array=input \
  --output_array=final_result \
  --inference_type=FLOAT \
  --input_data_type=FLOAT
```
I'm getting the error:
`Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which  you will need custom implementations: Dequantize.`"
20271,ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory,"I have recently installed tensorflow-gpu using pip. But when I am importing it it is giving the following error:

    ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory

I have gone through all the answers of stackoverflow related to this issue but none of them worked for me.

libcudnn.so.7 is present in both the following directories /usr/local/cuda/lib64 and /usr/local/cuda-9.0/lib64 .

Also, I have added the following path in my .bashrc file:

    export PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}
    export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
    
    export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}
    export LD_LIBRARY_PATH=/usr/local/cuda/lib64\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}


Please help me in resolving this
"
20270,Could the site refer change?,"Hi ,in https://www.tensorflow.org/mobile/tflite/demo_android 
the demo site (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo/app)
actually is direct,but i found i could not successful build it cuz various problem
And  occasionally i found this site:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo
it is much better and i can easily deploy on my phone without any change,Great!"
20269,PIP user-op tests-on-install FAILED,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.9, cpu
- **Python version**: 
Python 3.5
- **Bazel version (if compiling from source)**:
0.11.0
- **GCC/Compiler version (if compiling from source)**:
gcc version 5.4.0 20160609
- **CUDA/cuDNN version**:
NA
- **GPU model and memory**:
NA
- **Exact command to reproduce**:
ci_parameterized_build.sh with TF_BUILD_IS_PIP=PIP

### Describe the problem
I am using ci_parameterized_build.sh to build tensorflow whl in our CI system.
After https://github.com/tensorflow/tensorflow/commit/fdbb80f217d3a153b4eda66c766df921b3f73ab4, i found do_test_user_ops kept failing due the the following error.

Just let you guys know, @case540 as well.
Thanks.

[1m=== PIP test step 4 of 7: do_test_user_ops (User ops test) ===[0m

=== Testing user ops ===
PYTHON_BIN_PATH: /workspace/pip_test/venv/bin/python
/tmp/tmp.o7UMG4on4E /workspace
TensorFlow compile flags: -I/workspace/pip_test/venv/lib/python3.5/site-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=1
TensorFlow link flags: -L/workspace/pip_test/venv/lib/python3.5/site-packages/tensorflow -ltensorflow_framework

g++ version:
Using built-in specs.
COLLECT_GCC=g++
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/5/lto-wrapper
Target: x86_64-linux-gnu
lib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-5-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-5-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-5-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9) 

Extra GPP flag: 
Testing user ops in CPU environment
ero_out_op_kernel_1.cc
In file included from /workspace/pip_test/venv/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:20:0,
			  from /workspace/pip_test/venv/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/framework/allocator.h:23,
			  from /workspace/pip_test/venv/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:23,
ero_out_op_kernel_1.cc:17:
/workspace/pip_test/venv/lib/python3.5/site-packages/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory
compilation terminated.
ero_out_op_kernel_1.cc FAILED
PIP user-op tests-on-install FAILED


"
20268,HI what is the difference between examples and java folder's demo,"Can anyone tell me which tensorflow lite demo is good,cuz i met problem with both
1.https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo/app
and the problem is Plugin with id 'com.android.application' not found.
2.https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android
and this is more weird ,BUILD\android-profile 
thx
as 3.0 
"
20267,TensorRT - KeyError: 'shape',"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution: Linux Ubuntu 16.04**
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
> tensorflow (1.8.0)
> tensorrt (4.0.1.6)

- **Python version**: 2.7.14 
- **Bazel version (if compiling from source)**:None
- **CUDA/cuDNN version**: 9.0
- **Exact command to reproduce**:


### Describe the problem
Getting the following error while trying to optimize the predict node in from a frozen graph of an estimator-based transformer model. This is the line where I get the issue : 
`uff_model = uff.from_tensorflow_frozen_model(frozen_file, [""train/enc_0""])`

### Source code / logs
```
Warning: No conversion function registered for layer: IteratorGetNext yet.
Converting as custom op IteratorGetNext train/IteratorGetNext
name: ""train/IteratorGetNext""
op: ""IteratorGetNext""
input: ""train/Iterator""
attr {
  key: ""output_shapes""
  value {
    list {
      shape {
        dim {
          size: -1
        }
        dim {
          size: -1
        }
      }
      shape {
        dim {
          size: -1
        }
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: ""output_types""
  value {
    list {
      type: DT_INT32
      type: DT_INT32
    }
  }
}

Traceback (most recent call last):
  File ""tensorRT_optimize.py"", line 25, in <module>
    uff_model = uff.from_tensorflow(""logs/check_tiny/frozen_model_new.pb"", ['train/pred_0'])
  File ""/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/conversion_helpers.py"", line 120, in from_tensorflow
    name=""main"")
  File ""/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/converter.py"", line 76, in convert_tf2uff_graph
    uff_graph, input_replacements)
  File ""/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/converter.py"", line 63, in convert_tf2uff_node
    op, name, tf_node, inputs, uff_graph, tf_nodes=tf_nodes)
  File ""/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/converter.py"", line 38, in convert_layer
    fields = cls.parse_tf_attrs(tf_node.attr)
  File ""/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/converter.py"", line 209, in parse_tf_attrs
    for key, val in attrs.items()}
  File ""/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/converter.py"", line 209, in <dictcomp>
    for key, val in attrs.items()}
  File ""/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/converter.py"", line 204, in parse_tf_attr_value
    return cls.convert_tf2uff_field(code, val)
  File ""/home/user/.local/lib/python2.7/site-packages/uff/converters/tensorflow/converter.py"", line 189, in convert_tf2uff_field
    'type': 'dtype', 'list': 'list'}[code]
KeyError: 'shape'
```
"
20266,tensor flow does not install on windows 10,"Windows 10, TensorFlow 1.8.1, NVIDIA 1060
Was working.  Had to resinstall OS.  Now I always get error below with
CUDA 8.0, CUDA 9.0, CUDA 9.2, Visual Studio 2015, Visual Studio 2017 (have tried everything I can think of).  Any pointers for how to diagnose/solve?

(tensorflow) v:\deeplearning>python -W ignore test.py
Traceback (most recent call last):
  File ""C:\Users\jdm\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\jdm\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\jdm\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\jdm\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\jdm\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\jdm\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\jdm\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\jdm\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\jdm\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\jdm\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\jdm\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
20265,dataset.map function needs better documentation,"Can the documentation for dataset.map be fleshed out a bit? Most importantly, what are the input and output arguments for the mapping function? The current documentation implies there is only one return, but all the code examples show (input,output).  What is the meaning of the input and output args?"
20263,Computing gradients in extracted subgraph which contains a 'while_loop',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 17.10
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
v1.8.0-0-g93bc2e2072 
- **Python version**: 
3.6.5
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
9.0/7.0.5
- **GPU model and memory**:
Titan XP
- **Exact command to reproduce**:
```
import tensorflow as tf
g1=tf.Graph()
sess1=tf.Session(graph=g1)
with g1.as_default():
    with sess1.as_default():
        i=tf.constant(0, name=""input"")
        out=tf.while_loop(lambda i: tf.less(i,5), lambda i: [tf.add(i,1)], [i], name=""output"")
        loss=tf.square(out,name='loss')
        graph_def = tf.graph_util.convert_variables_to_constants(sess1,g1.as_graph_def(),['output/Exit'])

g2 = tf.Graph()
with g2.as_default():
    tf.import_graph_def(graph_def,name='')
    i_imported = g2.get_tensor_by_name(""input:0"")
    out_imported = g2.get_tensor_by_name(""output/Exit:0"")
    tf.gradients(out_imported, i_imported)

```
output:
```
INFO:tensorflow:Froze 0 variables.
Converted 0 variables to const ops.
Traceback (most recent call last):

  File ""<ipython-input-1-908dc1dee750>"", line 23, in <module>
    tf.gradients(out_imported, i_imported)

  File ""/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 494, in gradients
    gate_gradients, aggregation_method, stop_gradients)

  File ""/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 543, in _GradientsHelper
    ops.get_default_graph(), to_ops, from_ops, colocate_gradients_with_ops)

  File ""/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 195, in _PendingCount
    between_op_list, between_ops, colocate_gradients_with_ops)

  File ""/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1456, in MaybeCreateControlFlowState
    loop_state.AddWhileContext(op, between_op_list, between_ops)

  File ""/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1262, in AddWhileContext
    outer_forward_ctxt = forward_ctxt.outer_context

AttributeError: 'NoneType' object has no attribute 'outer_context'
```
### Describe the problem
TF  [issue #7404](https://github.com/tensorflow/tensorflow/issues/7404) describes that when trying to form a gradient op in an imported (sub)graph, a 'No attribute 'outer_context'' error occurs. This issue was closed with the recommendation to use `tf.train.import_meta_graph` instead, so the outer context related to the while op is included.

However, this does not fully solve the problem. In some deep-learning related settings, one might want to train a model, extract a subgraph (i.e., remove training related ops), connect the extracted subgraph into a larger graph to serve as part of an ensemble, GAN and so on, and then retrain the larger graph. When there is no dependence on outer context, one can easily use graph editing tools such as `tf.graph_util.convert_variables_to_constants` or `tf.graph_util.extract_sub_graph` to achieve that, exporting and importing subgraphs and then forming new tf.gradients operations. 

The minimal example above, adapted from issue 7404, shows how this approach fails when a tf.while is used and the outer context is missing. Importing and exporting the metagraph instead would leave the tf.square op in the graph. While for this minimal example being forced to save also the loss tensor looks like a very minor limitation it is easy to conceive actual applications in which there are large parts of the graph which we might really want to exclude (e.g., a decoder network in Capsule-network training). 

Right now, the dependence on outer context for computing gradients for tf.while is incompatible with tf.graph_util.extract_sub_graph and similar operations that operate on graphdefs. I believe that this is not a negligible functionality limitation.  A [related StackOverflow question](https://stackoverflow.com/questions/50663594/computing-gradients-in-extracted-tensorflow-subgraph-which-contains-a-while-loo) was upvoted but left unanswered.

In general, from the perspective of an API user who is ignorant of the internal implementation of TF, any dependence of operations on information stored out of the graphdef is not expected, hinders graph editing (as I try to convey above) and seems patchy."
20261,MultivariateNormalDiag Bugs,"MultivariateNormalDiag Has different output with scipy.stats.multivariate_normal
Code like that
```
loc=[0., 0., 0.]
scale_diag=[0.1, 0.1, 0.1]

# method1
tfd = tf.contrib.distributions
mvn = tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag)
sess = tf.Session()
print(""Tensorflow MultivariateNormalDiag:"")
print(mvn.prob(loc).eval(session=sess))  

# method2
from scipy.stats import multivariate_normal
p = multivariate_normal.pdf(loc, mean=loc, cov=scale_diag)
print(""\nScipy multivariate_normal:\n"", p)
```
Output:
```
Tensorflow MultivariateNormalDiag:
63.49365

Scipy multivariate_normal:
 2.0078450647771446
```
Why that?"
20260,Unknown argument output_layer=final_result,"when i execute this code:
bazel-bin/tensorflow/examples/label_image/label_image \
> output_layer=final_result \
> labels=/tf_files/retrained_labels.txt \
> image=/tf_files/flower_photos/daisy/5547758_eea9edfd54_n.jpg \
> graph=/tf_files/retrained_graph.pb

i got the error:
E tensorflow/examples/label_image/main.cc:319] Unknown argument output_layer=final_result
usage: bazel-bin/tensorflow/examples/label_image/label_image"
20259,How to predict on a new image given a trained tensorflow CNN graph?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  I build a CNN with 2 convolutional layers(tf.nn.conv2d) and a fully connected layer (tf.contrib.layers.fully_connected). I saved the model in a checkpoint path and I can see the weights in each of the layers. However, I wanted to see how I can use the restored model to predict on a new image that I have without explicitly putting all weights. Remember, the restored graph already has weights and I just want to be able to pass a new image data and be able to throw out a prediction. 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.0.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem."
20258,"Batched tf.linspace; documentaton says its possible, but returns error","Following documentation [here](https://www.tensorflow.org/api_docs/python/tf/lin_space) it says that in `tf.linspace(start=A,stop=B, n)`, `A` and `B` are to be Tensors. But


    A=tf.constant(np.array([1.,2.,3.]))

    B=tf.constant(np.array([3.,4.,5.]))

    tf.linspace(A,B,3)


returns an error, that A should have shape 0, which contradicts the documentation. Is there some other way to achieve this? The desired output is

`array([[1,2,3],[2,3,4],[3,4,5]])`"
20257,`transpose` in `tf.contrib.tensorrt` cause `dimension error`,
20255,[Feature Request] iterator.has_next() for dataset api,"Hi, I'd need some type of `iterator.has_next()` to loop through a dataset:

```python3
dataset = tf.data.Dataset[...]

iterator = dataset.make_initializable_iterator()
with tf.control_dependencies([iterator.initializer]):
    retval = tf.while_loop(lambda _: iterator.has_next(), body_fn, init_vals)
```

At the moment I add some sample index to the dataset and check if the index value is lower than the number of sampels.
However, this is a very ugly solution which might not be possible in every situation.

Is there any reason that there exists an iterator without a has_next() method?

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: none
- **GCC/Compiler version (if compiling from source)**: none
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**: none
"
20253,Contrib Loss Function Bug,"### System information
I've reproduced this problem on multiple systems including two with CPU only and three with GPU. The specs below represent one such system.

Have I written custom code
Yes

OS Platform and Distribution
Mac OS X 10.13.2
17.3.0 Darwin Kernel 17.3.0: Thu Nov  9 18:09:22 PST 2017; root:xnu-4570.31.3~1/RELEASE_X86_64 x86_64

TensorFlow installed from
Pip (python3)

TensorFlow version
1.8.0

Bazel version
N/A

CUDA/cuDNN version
N/A

GPU model and memory
N/A

Exact command to reproduce
Please see code below

### Describe the problem
There are several conditions that cause triplet_semihard_loss from tf.contrib to return nan as the loss. This totally messes up the gradient and causes divergence (at least in my build). Below is a minimal reproduction of one way to cause nan which is to have a batch of all different classes which causes there to be no positive for any chosen anchor. 

The lack of a negative does not result in nan, just a super high loss which while not as bad, is still not what I'd expect.

When running backprop, I would expect to have no gradient if there are no valid triplets to train off of.

I'm happy to contribute to fixes if you guys agree this is a bug.

### Source code / logs
```python
import numpy as np
import tensorflow as tf
from tensorflow.contrib.losses import metric_learning

labels = tf.placeholder(tf.int32, [None], name='labels')
embeddings = tf.placeholder(tf.float32, [None, 128], name='embeddings')

loss_ph = metric_learning.triplet_semihard_loss(labels, embeddings)

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())

loss = sess.run(loss_ph, feed_dict={
    embeddings: np.random.rand(2, 128),
    labels: np.array([1, 2])
})

print(""loss"", loss)
# loss nan
```"
20252,tensorflow/core/framework/op_kernel.cc:1318] Not found ,"What is the reason this error is happening ?

WARNING:tensorflow:From /home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2018-06-23 21:43:50.471573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-06-23 21:43:50.471640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-06-23 21:43:50.471665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0
2018-06-23 21:43:50.471684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N
2018-06-23 21:43:50.471857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10654 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2018-06-23 21:43:55.304510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-06-23 21:43:55.304589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-06-23 21:43:55.304618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0
2018-06-23 21:43:55.304636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N
2018-06-23 21:43:55.304812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10654 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
Epoch 1/25
2018-06-23 21:44:00.405493: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at resource_variable_ops.cc:389 : Not found: Resource localhost/RMSprop/iterations/N10tensorflow3VarE does not exist.
Traceback (most recent call last):
  File ""/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
    return fn(*args)
  File ""/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable RMSprop/lr from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/RMSprop/lr/N10tensorflow3VarE does not exist.
         [[Node: training/RMSprop/ReadVariableOp_4 = ReadVariableOp[dtype=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](RMSprop/lr)]]
         [[Node: training/RMSprop/gradients/add_9/add_grad/Shape_1/_1445 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2193_training/RMSprop/gradients/add_9/add_grad/Shape_1"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 161, in <module>
    callbacks=[TensorBoard(log_dir='./log')])
  File ""/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py"", line 1598, in fit_generator
    initial_epoch=initial_epoch)
  File ""/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training_generator.py"", line 191, in fit_generator
    x, y, sample_weight=sample_weight, class_weight=class_weight)
  File ""/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py"", line 1390, in train_on_batch
    outputs = self.train_function(ins)
  File ""/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py"", line 2824, in __call__
    fetches=fetches, feed_dict=feed_dict, **self.session_kwargs)
  File ""/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable RMSprop/lr from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/RMSprop/lr/N10tensorflow3VarE does not exist.
         [[Node: training/RMSprop/ReadVariableOp_4 = ReadVariableOp[dtype=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](RMSprop/lr)]]
         [[Node: training/RMSprop/gradients/add_9/add_grad/Shape_1/_1445 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2193_training/RMSprop/gradients/add_9/add_grad/Shape_1"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
"
20251,"Tensorflow recognized my GPU which is GTX 1060, but is using my CPU to train","Stuck on this
INFO:tensorflow:Starting Queues.
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0
INFO:tensorflow:global_step/sec: 0

![capture](https://user-images.githubusercontent.com/28718783/41813713-073d9444-7709-11e8-89cd-faeadedc74e7.JPG)
![dddd](https://user-images.githubusercontent.com/28718783/41813731-3fc754e4-7709-11e8-83df-f781949e9e57.JPG)
"
20248,Tensorflow-gpu requires AVX instruction to import; even if using gpu for all calculations,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04 Bionic Beaver
AMD Phenom x4 II 965
- **TensorFlow installed from (source or binary)**: binary; docker image tensorflow-1.8.0-gpu-py3
- **TensorFlow version (use command below)**: 1.18.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:
CUDA 9.0
CuDNN 7.0.5
- **GPU model and memory**:
Geforce GTX 760 (compute capability 3.0) 2GB
- **Exact command to reproduce**:
`import tensorflow as tf` on tensorflow-gpu on system without AVX

### Describe the problem

Jupyter Kernel crashes when doing `import tensorflow as tf`using a system that does not have AVX.

Similarly if i enter a python shell and do `import tensorflow as tf` i get an `Illegal Instruction (core dumped)` error and it exits the shell.

I have confirmed this is due to the AVX instruction because when i use the docker image Tensorflow-1.15.0-gpu-py3 I am able to successfully import tensorflow.

*Unfortunately*, tensorflow 1.15.0 requires compute capability 3.5 or higher which is extremely frusterating because i cannot find a build of tensorflow online that supports compute capaibility 3.0 but does not use AVX instructions.

Why do i need a CPU that supports AVX just to import tensorflow when i dont actually need the AVX instruction since i have a supported GPU (at least it is supported on all tensorflow versions other than 1.15.0).

Is there a way around this or do i need to build tensorflow from source to not use AVX but to allow compute capability 3.0? I tried using tensorflow-gpu-1.18.0-devel-py to build a new version from source but it did not even ask me what compute capabilities i wanted to build for and so i just let it build but it took over 24 hours and it was still going. 

Can someone either provide a tensorflow-gpu release with CUDA 9.0 and CuDNN 7 support that does not use AVX and supports compute capability 3.0 or at least tell me the best way to obtain such a build (since apparently building using the docker-devel image wont work)."
20243,Feature Request: Let tf.contrib.image.transform supports half type,"Currently tf.contrib.image.transform doesn't support half type. It is great if it can support. Thanks.

@ringw "
20241,[Tensorflow C++ API]Multiple session on single GPU,"We are using tensorflow 1.4 c++ for run-time prediction.  Our 4 models are loaded by 4 session instances within one project on one GPU. The project breaks down after a few predictions. 

The output error message is:  [tensorflow\core\common_runtime\bfc_allocator.cc:464] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum) 

Is this memory problem? "
20240,Bazel build Failed in compiling libtensorflow.so from source using the mkl config in windows 10,"### System information
- **OS Platform and Distribution )**: win10 x64
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: R1.9
- **Python version**:  Anaconda3 Python3.6.2
- **Bazel version (if compiling from source)**: 0.14.0
- **GCC/Compiler version (if compiling from source)**: VS2015 Update3
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**: 
bazel build -c opt --copt=/arch:AVX  --config=mkl\
  tensorflow:libtensorflow.so \
  tensorflow/tools/lib_package:clicenses_generate \
  tensorflow/java:libtensorflow_jni.so \
  tensorflow/tools/lib_package:jnilicenses_generate

### Describe the problem
I want to build libtensorflow.so with ""cpu + mkl""  in win10 os.

I had builded the libtensorflow.so using the script (run_libtensorflow.bat) in the directory: tensorflow/tools/ci_build/windows/cpu/bazel/.This script works very well,but then a link error occur 
if I add --config=mkl to the option of bazel build. 

I think this link error is about mkl library,and this link error exist a period of time.
I tried to build libtensorflow.so r1.8,after fixing some compilation error the same link error occur. 

### The verbose log are following 

INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Options provided by the client:
  'build' options: --python_path=C:/tools/Anaconda3/python.exe
INFO: Reading rc options for 'build' from d:\tensorflow_git_reps\tensorflow_r1.9\tools\bazel.rc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define=grpc_no_ares=true --spawn_strategy=standalone --genrule_strategy=standalone -c opt
INFO: Reading rc options for 'build' from d:\tensorflow_git_reps\tensorflow_r1.9\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/tools/Anaconda3/python.exe --action_env PYTHON_LIB_PATH=C:/tools/Anaconda3/lib/site-packages --python_path=C:/tools/Anaconda3/python.exe --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_CUDA=0 --action_env TF_DOWNLOAD_CLANG=0 --define grpc_no_ares=true --strip=always --config monolithic --copt=-w --host_copt=-w --verbose_failures
INFO: Found applicable config definition build:monolithic in file d:\tensorflow_git_reps\tensorflow_r1.9\tools\bazel.rc: --define framework_shared_object=false
INFO: Found applicable config definition build:mkl in file d:\tensorflow_git_reps\tensorflow_r1.9\tools\bazel.rc: --define=using_mkl=true -c opt --define=using_mkl=true
WARNING: G:/tf/_bazel_50219889/4o2fzunw/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in G:/tf/_bazel_50219889/4o2fzunw/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: G:/tf/_bazel_50219889/4o2fzunw/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in G:/tf/_bazel_50219889/4o2fzunw/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: G:/tf/_bazel_50219889/4o2fzunw/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in G:/tf/_bazel_50219889/4o2fzunw/external/grpc/bazel/grpc_build_system.bzl:172:12
INFO: Analysed target //tensorflow:libtensorflow.so (77 packages loaded).
INFO: Found 1 target...
ERROR: D:/tensorflow_git_reps/tensorflow_r1.9/tensorflow/BUILD:480:1: Linking of rule '//tensorflow:libtensorflow.so' failed (Exit 1169): link.exe failed: error executing command 
  cd G:/tf/_bazel_50219889/4o2fzunw/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\8.1\include\\shared;C:\Program Files (x86)\Windows Kits\8.1\include\\um;C:\Program Files (x86)\Windows Kits\8.1\include\\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.10240.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\8.1\lib\winv6.3\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\8.1\bin\x64;C:\Program Files (x86)\Windows Kits\8.1\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\Windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/tools/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/tools/Anaconda3/lib/site-packages
    SET TEMP=C:\Users\50219889\AppData\Local\Temp
    SET TF_DOWNLOAD_CLANG=0
    SET TF_NEED_CUDA=0
    SET TF_NEED_OPENCL_SYCL=0
    SET TMP=C:\Users\50219889\AppData\Local\Temp
    SET USE_LINKER=1
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /DLL /WHOLEARCHIVE:external/mkl_windows/lib/libiomp5md.lib /WHOLEARCHIVE:external/mkl_windows/lib/mklml.lib /SUBSYSTEM:CONSOLE -DEFAULTLIB:advapi32.lib -pthread /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/libtensorflow.so-2.params /DEFAULTLIB:msvcrt.lib
LINK : warning LNK4044: unrecognized option '/pthread'; ignored
LINK : warning LNK4044: unrecognized option '/ldl'; ignored
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/ldl'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
**mklml.lib(mklml.dll) : error LNK2005: __NULL_IMPORT_DESCRIPTOR already defined in libiomp5md.lib(libiomp5md.dll)**
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/libtensorflow.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/libtensorflow.exp
libbatch_kernels.lo(batch_kernels.o) : warning LNK4217: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported in function ""void __cdecl tensorflow::`dynamic initializer for 'registrar__body__0__object''(void)"" (??__Eregistrar__body__0__object@tensorflow@@YAXXZ)
libarithmetic_optimizer.a(arithmetic_optimizer.o) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported
libarithmetic_optimizer.a(arithmetic_optimizer.o) : warning LNK4217: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported in function ""private: bool __cdecl tensorflow::grappler::`anonymous namespace'::ReorderCastAndTranspose::NodeIsOnCpuOrGpu(class tensorflow::NodeDef const *)const "" (?NodeIsOnCpuOrGpu@ReorderCastAndTranspose@?A0x3f420f79@grappler@tensorflow@@AEBA_NPEBVNodeDef@4@@Z)
liblayout_optimizer.a(layout_optimizer.o) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported
bazel-out/x64_windows-opt/bin/tensorflow/libtensorflow.so : fatal error LNK1169: one or more multiply defined symbols found
Target //tensorflow:libtensorflow.so failed to build
INFO: Elapsed time: 150.103s, Critical Path: 34.36s
INFO: 2 processes, local.
FAILED: Build did NOT complete successfully"
20233,"Build fails while creating Python API: ""TypeError: argument of type 'NoneType' is not iterable""","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Gentoo Linux
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.9.0-rc1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**:  7.3.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: None
- **Exact command to reproduce**: building using Gentoo ebuild


### Describe the problem
Build fails while creating Python API.

### Source code / logs
```
""/var/tmp/portage/sci-libs/tensorflow-1.9.0_rc1/work/bazel-base-python3_6/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py"", line 182, in get_api_init_text
    package not in module.__name__):
TypeError: argument of type 'NoneType' is not iterable
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3202.457s, Critical Path: 83.98s
INFO: 4901 processes, local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```

Fixed for me with:
```
--- tensorflow-1.9.0-rc1.orig/tensorflow/tools/api/generator/create_python_api.py
+++ tensorflow-1.9.0-rc1/tensorflow/tools/api/generator/create_python_api.py
@@ -179,7 +179,7 @@
   for module in list(sys.modules.values()):
     # Only look at tensorflow modules.
     if (not module or not hasattr(module, '__name__') or
-        package not in module.__name__):
+        module.__name__ is None or package not in module.__name__):
       continue
     # Do not generate __init__.py files for contrib modules for now.
     if '.contrib.' in module.__name__ or module.__name__.endswith('.contrib'):
```

because `module.__name__` can be None.
"
20231,Feature request: float16 support for sparse_softmax_cross_entropy,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9
- **GPU model and memory**: K80
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The Tensorflow documentation for tf.losses.sparse_softmax_cross_entropy seems to say that only `tf.float32` or `tf.float64` types are supported for `logits`. Is that true? Is `tf.float16` not supported? 
https://www.tensorflow.org/api_docs/python/tf/losses/sparse_softmax_cross_entropy
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20225,"Windows Make: Don't (re)build dependencies, use prebuilt ones","### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **GCC/Compiler version (if compiling from source)**: 8.0.1
- **Exact command to reproduce**: mingw32-make

### Describe the problem
The tool builds the dependencies instead of asking the user to provide location of prebuilt ones. Building dependencies when building TF causes problems: if a dependency build fails, I have to start the build from scratch, this consumes LOT OF TIME. Also I cannot fix an error in the dependencies because it fetches the latest version from git on each insfallation, this also consumes LOT OF TIME. Please provide the way to use prebuilt dependencies. "
20222,Use shallow clones from git repos in CMake build,"Please add

```cmake
GIT_SHALLOW 1
GIT_PROGRESS 1
```

to all cmake files cloning git repos in ``contrib/cmake/external`"
20219,[FEATURE REQUEST] tf.scatter_nd doesn't support half types,"I try to use tf.scatter_nd and tf.scatter_max for tf.float16 input, but it reports that it doesn't support half types. Can tf.scatter_nd  and tf.scatter_max add support for half types? Thanks.

@kosklain @dantkz"
20218,ran out of memory in eager execution,"### System information
- **Have I written custom code**:Yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**:source
- **TensorFlow version**:1.8.0-gpu
- **Python version**: 3.5
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**:GeForce GTX 1060 / 6G
- **Bazel version (if compiling from source)**: N/A
- **Exact command to reproduce**:
General training model steps

### Describe the problem
I wrote a Neural Language Model using TF eager mode, which has only one LSTM cell.
Some of these hyperparameters are:

hidden_dim : 128
vocab_size : 7393
seq_len : <=50
batch_size : 32

In particular, I did not use the inherited keras base class to write the model, but instead used tfe.Variable to gradually implement some of the details of the model. The model was normal during the early training and performed well.  However, a memory exhaustion error occurs every time the model runs to 960 steps or so.  I think it's because some variables haven't released this error that has been accumulated during training.  Because I still have this error in 960 steps after adjusting batch-size. 
The following is the error log, I think this is a TF eager mode memory usage problem, however the custom model is a necessary choice for some attempts.
How can we avoid this problem and let the model train smoothly?
Thanks a lot.

### Source code / logs
loss at epoch 0 step 954: 6.665831
loss at epoch 0 step 955: 6.716213
loss at epoch 0 step 956: 6.761374
loss at epoch 0 step 957: 6.712702
loss at epoch 0 step 958: 6.753476
loss at epoch 0 step 959: 6.481436
loss at epoch 0 step 960: 6.544363
loss at epoch 0 step 961: 6.562449
loss at epoch 0 step 962: 6.701943
loss at epoch 0 step 963: 6.415178
loss at epoch 0 step 964: 6.630089
loss at epoch 0 step 965: 6.580114
2018-06-22 11:53:38.547278: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 92.05MiB.  Current allocation summary follows.
2018-06-22 11:53:38.547428: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 12B client-requested in
 use in bin.
2018-06-22 11:53:38.547428: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 12B client-r[1767/1952]
 use in bin.
2018-06-22 11:53:38.547459: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 17, Chunks in use: 16. 8.5KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-req
uested in use in bin.
2018-06-22 11:53:38.547482: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-reque
sted in use in bin.
2018-06-22 11:53:38.547504: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use 
in bin.
2018-06-22 11:53:38.547525: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 1, Chunks in use: 0. 4.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in 
use in bin.
2018-06-22 11:53:38.547544: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use 
in bin.
2018-06-22 11:53:38.547568: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 5, Chunks in use: 4. 135.5KiB allocated for chunks. 110.0KiB in use in bin. 108.8Ki
B client-requested in use in bin.
2018-06-22 11:53:38.547593: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 920, Chunks in use: 918. 28.87MiB allocated for chunks. 28.81MiB in use in bin. 28.
69MiB client-requested in use in bin.
2018-06-22 11:53:38.547617: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 410, Chunks in use: 410. 26.12MiB allocated for chunks. 26.12MiB in use in bin. 25.
66MiB client-requested in use in bin.
2018-06-22 11:53:38.547640: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 20, Chunks in use: 20. 2.69MiB allocated for chunks. 2.69MiB in use in bin. 2.69MiB
 client-requested in use in bin.
.
.
.
.
.
.
.
2018-06-22 11:53:38.548461: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089bc700 of size 512                                                                         [1691/1952]
2018-06-22 11:53:38.548478: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089bc900 of size 131072
2018-06-22 11:53:38.548494: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089dc900 of size 512
2018-06-22 11:53:38.548510: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089dcb00 of size 131072
2018-06-22 11:53:38.548526: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089fcb00 of size 512
2018-06-22 11:53:38.548542: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089fcd00 of size 131072
2018-06-22 11:53:38.548558: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a1cd00 of size 512
2018-06-22 11:53:38.548574: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a1cf00 of size 131072
2018-06-22 11:53:38.548589: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a3cf00 of size 512
2018-06-22 11:53:38.548605: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a3d100 of size 131072
2018-06-22 11:53:38.548622: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a5d100 of size 512
2018-06-22 11:53:38.548638: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a5d300 of size 131072
2018-06-22 11:53:38.548654: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a7d300 of size 512
2018-06-22 11:53:38.548669: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a7d500 of size 131072
2018-06-22 11:53:38.548685: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a9d500 of size 512
2018-06-22 11:53:38.548702: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a9d700 of size 7570432
2018-06-22 11:53:38.548718: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091d5b00 of size 29696
2018-06-22 11:53:38.548735: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091dcf00 of size 32768
2018-06-22 11:53:38.548752: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091e4f00 of size 32768
2018-06-22 11:53:38.548768: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091ecf00 of size 32768
2018-06-22 11:53:38.548783: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091f4f00 of size 32768
.
.
.
.
.
.
.
.
.
2018-06-22 11:53:38.796751: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats: 
Limit:                  5272961024
2018-06-22 11:53:38.796751: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:                                                                                                       [19/1952]
Limit:                  5272961024
InUse:                  3087996928
MaxInUse:               3174426368
NumAllocs:                 2214674
MaxAllocSize:            140052992

2018-06-22 11:53:38.796832: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *************_************************************_***********_**_**_**_**_**_******_**_***_**_*****
2018-06-22 11:53:38.796866: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at bias_op.cc:341 : Resource exhausted: OOM when allocating tensor with shape[3200,7393] and type float on 
/job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
"
20217,Compile Error: tensorflow/tensorflow/examples/label_image/main.cc ,"I have compiled c++ lib tf_r1.3 with cuda 8.0 ,cudnn 6, vs2015 update3  on Windows10 successfullly.

When I try to compile example ""tensorflow/tensorflow/examples/label_image/main.cc"" on WIndows10, it reports error:
Error	C3861	'EndsWith': identifier not found	

And after I replace 'EndsWith(X,Y)' with '*Split(X,'.').end()==Y' to bypass the first error, it reports another error:
LNK2001	unresolved external symbol ""public: static unsigned __int64 const tensorflow::StringPiece::npos"" (?npos@StringPiece@tensorflow@@2_KB)	

please help to debug.
Thanks!


"
20216,Question about tf.unsorted_segment_max,"I'ven seen the tf r1.8 has add some segment ops such as tf.unsorted_segment_max. But the output dimention doesn't keep the same with the input. I wonder if there is any op just do the same thing as tf.unsorted_segment_max, but output Tensor with the same dim as input. For example, if I put the parameter keep_dims=True to tf.reduce_sum, the output dim is as my wish, but without id segmentation. 
"
20215,Tensorflow does not work,"### System information
- **I used stock example script provided in TensorFlow)**:
- **Windows -7 64 bit**:
- **installed tensorflow ""pip install tensorflow --upgrade""**:
- **TensorFlow version 1-8-0**:
- **Python version 3.6.5**: 
- **Exact command to reproduce**:
import tensorflow

### Describe the problem
When i run in python
_import tensorflow_,
i receive
""failed to load native tensorflow runtime""

### Source code / logs
Source:
import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))

Full traceback:
Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""K:\Python\testTensorflow.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>> 


"
20214,tf.flags not compatible with old versions,"The new `tf.flags` APIs (>1.4.1) perform badly, not compatible with old ones, even raise weird exception like: 

```
absl.flags._exceptions.IllegalFlagValueError: flag --env_worker_samples=100000.0: Expect argument to be a string or int, found <class 'float'>
```

In fact, I use '--env_worker_samples 100000' in command line.

Why does the official not provide any instruction on migrating `tf.flags` to new version TF?"
20213,Resource Exhausted  Error,"```
ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[98384,15003]
         [[Node: dynamic_seq2seq/decoder/output_projection/Tensordot/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](dynamic_seq2seq/decoder/output_projection/Tensordot/Reshape, dynamic_seq2seq/decoder/output_projection/Tensordot/Reshape_1)]]
         [[Node: dynamic_seq2seq/truediv/_217 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_783_dynamic_seq2seq/truediv"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```

i get the above error when i try to train my model using the tensorflow gpu"
20212,key ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20211,C++ compilation of rule '@nsync//:nsync_cpp' failed,"### System information
- **Have I written custom code**: No
- **OS Platform and Distribution**: ubuntu18.04
- **TensorFlow installed from**: anaconda3
- **TensorFlow version**: 1.8.0
- **Python version**:  3.6.4
- **Bazel version**: 0.14.1
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**: bazel build -c opt //tensorflow/examples/android:tensorflow_demo --verbose_failures

```
$ bazel build -c opt //tensorflow/examples/android:tensorflow_demo --verbose_failures
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/DataType.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Graph.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/NativeLibrary.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Operand.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Operation.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/OperationBuilder.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Output.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/SavedModelBundle.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Session.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Shape.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Tensor.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/TensorFlow.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/TensorFlowException.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Tensors.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/package-info.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/types/UInt8.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/types/package-info.java' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:batch_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_volume_patch.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there
INFO: Analysed target //tensorflow/examples/android:tensorflow_demo (2 packages loaded).
INFO: Found 1 target...
ERROR: /home/tech1/.cache/bazel/_bazel_tech1/1de281b1fba5f525d1ddad248a9536d4/external/nsync/BUILD:462:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): false failed: error executing command 
  (cd /home/tech1/.cache/bazel/_bazel_tech1/1de281b1fba5f525d1ddad248a9536d4/execroot/org_tensorflow && \
  exec env - \
    PATH='/home/tech1/anaconda3/envs/tensorflow/bin:~/anaconda3/bin:/home/tech1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin:$/home/tech1/android/android-sdk/platform-tools:$/home/tech1/android/android-sdk/tools:/home/tech1/android/android-sdk/tools:/home/tech1/android/android-sdk/platform-tools:/home/tech1/android/android-ndk' \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  /bin/false -MD -MF bazel-out/android-armeabi-v7a-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/sem_wait.pic.d -fPIC -iquote external/nsync -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/android-armeabi-v7a-opt/genfiles/external/nsync/public -isystem bazel-out/android-armeabi-v7a-opt/bin/external/nsync/public -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/arm -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -c external/nsync/internal/sem_wait.c -o bazel-out/android-armeabi-v7a-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/sem_wait.pic.o)
Target //tensorflow/examples/android:tensorflow_demo failed to build
INFO: Elapsed time: 151.160s, Critical Path: 18.85s
INFO: 196 processes, local.
FAILED: Build did NOT complete successfully
```
could you help me?"
20210,Support openmp in tflite for operations like depthwise convolution,"In tflite, some operations like convolution have openmp support from Eigen library, while for others like depthwise convolution they don't have openmp optimization yet.

Do we have any plans to add openmp to all operations in tflite?"
20209,tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match:,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
20208,"tf.distributions.Gamma quantile not implemented, yet exists in documentation","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.6

Atempting to run

`tf.distributions.Gamma(concentration=10.,rate=0.2).quantile(0)`
we see that

> NotImplementedError: quantile is not implemented

But in the [documentation](https://www.tensorflow.org/api_docs/python/tf/distributions/Gamma) there is no indication of this function not being implemented, as the function is documented as all others are.

Is there something missing from my installation and the function should be present, or is it wrongly documented as existing?"
20206,Reproducing the exact tensorflow_inception_v3_stripped_optimized_quantized.pb file,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 14:04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.9
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: 0.13
- **GCC/Compiler version (if compiling from source)**: 4.8
- **CUDA/cuDNN version**: 7/6
- **GPU model and memory**: 1060
- **Exact command to reproduce**:

Inspecting the available quantized/frozen model of inception-v3 to be used with HVX (`tensorflow_inception_v3_stripped_optimized_quantized.pb`), I see that the spatial_squeeze and some other operations are removed (stripped) and the input and output layers are 'Mul' and 'Softmax' (as they are not defined in nnlib). I tried to reproduce this graph using a pretrained model of my own, but still, end up with spatial_squeeze and thus, it fails to execute on HVX. Also, my input and output layers are called `input` and `InceptionV3/Predictions/Reshape_1`. 

Could you provide the steps to reproduce the exact `tensorflow_inception_v3_stripped_optimized_quantized.pb` using the `graph_transform`/`Optimize_for_inference`/`Quantize_graph` tools, so others can follow these steps to build their own model for HVX? Thanks, @satok16 @petewarden "
20205,F-RCN protobuf issue decoding string when reading in label_map,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Stock
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 LTS
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.6.0 (GPU)
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 (This is what I deduced from the help script provided)
- **GPU model and memory**:Tesla P100-PCIE-16GB
- **Exact command to reproduce**:python /home/_myhomedir_/tensorflow/models/research/object_detection/train.py --pipeline_config_path=/home/_myhomedir_/test_rfcn/rfcn_resnet101-1.config --train_dir=gs://mygcsbucket/mydir --ps_tasks=1
Config File: [rfcn_resnet101-1.txt](https://github.com/tensorflow/tensorflow/files/2125622/rfcn_resnet101-1.txt)

### Describe the problem
The bug occurs when trying to read in the label map (stack trace below). I am using the exact same setup when I test with ssd_mobilenet config file (points to ssd_mobilenet initial checkpoint) and training runs fine. Same for F-RCNN. Only when using the R-FCN config file is when this error is thrown. It seems to be related to protobuf and I have tried some various fixes found in the issues section. None seem to work, and I am unsure how/why that is a problem if it works for ssd and frcnn. I also have attache dmy config file (as .txt due to file type restrictions). Thanks for the help.

Zach

### Source code / logs
```
WARNING:tensorflow:From /home/zachary_mostowsky/tensorflow/models/research/object_detection/trainer.py:257: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
Traceback (most recent call last):
  File ""/home/zachary_mostowsky/tensorflow/models/research/object_detection/train.py"", line 184, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/home/zachary_mostowsky/tensorflow/models/research/object_detection/train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/home/zachary_mostowsky/tensorflow/models/research/object_detection/trainer.py"", line 264, in train
    train_config.prefetch_queue_capacity, data_augmentation_options)
  File ""/home/zachary_mostowsky/tensorflow/models/research/object_detection/trainer.py"", line 59, in create_input_queue
    tensor_dict = create_tensor_dict_fn()
  File ""/home/zachary_mostowsky/tensorflow/models/research/object_detection/train.py"", line 121, in get_next
    dataset_builder.build(config)).get_next()
  File ""/home/zachary_mostowsky/tensorflow/models/research/object_detection/builders/dataset_builder.py"", line 155, in build
    label_map_proto_file=label_map_proto_file)
  File ""/home/zachary_mostowsky/tensorflow/models/research/object_detection/data_decoders/tf_example_decoder.py"", line 245, in __init__
    use_display_name)
  File ""/home/zachary_mostowsky/tensorflow/models/research/object_detection/utils/label_map_util.py"", line 152, in get_label_map_dict
    label_map = load_labelmap(label_map_path)
  File ""/home/zachary_mostowsky/tensorflow/models/research/object_detection/utils/label_map_util.py"", line 137, in load_labelmap
    label_map.ParseFromString(label_map_string)
google.protobuf.message.DecodeError: Error parsing message

```"
20201,[FEATURE REQUEST] decode_csv - optionally skip records with empty required fields,"The function `decode_csv` has a `record_defaults` parameter used to specify the default value for each field. An empty value indicates that the field is required. Currently, if a required field is empty in a record, an InvalidArgumentError is raised with a message indicating the offending record and field numbers.

It would be useful to have a new parameter in `decode_csv` that allows you to optionally skip input records with empty required fields. Some examples of why this feature would be helpful:

- existing data files in which fields that you require are occasionally empty
- data files that are used for multiple purposes that have different required fields

Obligatory issue template requested by tensorflowbutler (I assume that most are irrelevant, since this is a feature request):

Have I written custom code: Not yet
OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: N/A
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A"
20200,Documentation and code discrepancy in tf.scatter_add,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: 1.8
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

The code for tf.scatter_add has argument use_locking=False, but the documentation says:

> use_locking: An optional bool. Defaults to **True**. If True, the assignment will be protected by a lock; otherwise the behavior is undefined, but may exhibit less contention.

"
20199,No float 16 support for NonMaxSuppressionV2,"Hi , 

While running the object detector in keras and tensorflow in float 16 mode,  

I get the following error -

TypeError: Input 'boxes' of 'NonMaxSuppressionV2' Op has type float16 that does not match expected type of float32.

Is there a way to add float 16 support for this operation here -

https://github.com/tensorflow/tensorflow/blob/9d2abd2ace95e6e352ba1292cc38c77b7bd1adc7/tensorflow/core/ops/image_ops.cc#L653

Thanks  :) 



Have I written custom code
OS Platform and Distribution --- Ubuntu 16.04
TensorFlow installed from --- pip 
TensorFlow version --- 1.8.0
Bazel version --- bazel release 0.13.1
CUDA/cuDNN version ---  9.2, V9.2.88
GPU model and memory - Titan X 
Exact command to reproduce --- training keras retinanet but NonMaximumSupression doesnt support float 16.
"
20194,Assigning new values to a protocol buffer frozen model in TensorFlow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 14.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.8
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: bazel release 0.13.0
- **GCC/Compiler version (if compiling from source)**: 4.8
- **CUDA/cuDNN version**: 8/7
- **GPU model and memory**: NVIDIA-1060
- **Exact command to reproduce**:

I am able to assign new values to a tensor by using `tf.assign` and `tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)` when I restore a pretrained model:

```
 >>>sess.run(tf.assign([v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) if v.name == 'LAYER_NAME'][0], New_Value_FOR_LAYER_NAME))`
```

However, in the case of a frozen model (`.pb`) file, the equivalent using `tf.assign` and `tf.get_default_graph().get_operations()` does not yeild any results:

```
>>>sess.run(tf.assign([v for v in tf.get_default_graph().get_operations() if v.name == LAYER_NAME'][0], New_Value_FOR_LAYER_NAME)). 
```
I guess the underlying reason is that pretrained models have [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor) and frozen models have [tf.Operation](https://www.tensorflow.org/api_docs/python/tf/Operation):



```
>>>[v for v in tf.get_default_graph().get_operations() if v.name == 'softmax/weights_quint8_const']
    [<tf.Operation 'softmax/weights_quint8_const' type=Const>]
```

Then, what is the correct way of assigning new values to a tf.Operation in a frozen model. 
"
20192,tensorflow lite with ndk 17 compilatoin failed,"Hi ,  I took latest tensorflow sources from github and try to build tensorflow lite.
There was error compilation.

We upgraded our NDK version from 16 to 17 couple weeks ago and tensorflow lite compiled **without any error**. But latest sources from today failed to compile.



```
INFO: Found 1 target...
ERROR: XXX/tensorflow/contrib/lite/kernels/BUILD:57:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:gemm_support' failed (Exit 1)
In file included from tensorflow/contrib/lite/kernels/gemm_support.cc:15:
In file included from ./tensorflow/contrib/lite/kernels/gemm_support.h:18:
In file included from external/gemmlowp/public/gemmlowp.h:19:
In file included from external/gemmlowp/public/../internal/dispatch_gemm_shape.h:20:
In file included from external/gemmlowp/public/../internal/../internal/kernel_default.h:22:
In file included from external/gemmlowp/public/../internal/common.h:26:
In file included from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cmath:305:
In file included from external/androidndk/ndk/sources/android/support/include/math.h:32:
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:1302:93: error: no member named 'log2f' in the global namespace
inline _LIBCPP_INLINE_VISIBILITY float       log2(float __lcpp_x) _NOEXCEPT       {return ::log2f(__lcpp_x);}
                                                                                          ~~^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:1303:93: error: no member named 'log2l' in the global namespace
inline _LIBCPP_INLINE_VISIBILITY long double log2(long double __lcpp_x) _NOEXCEPT {return ::log2l(__lcpp_x);}
                                                                                          ~~^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:1308:38: error: call to 'log2' is ambiguous
log2(_A1 __lcpp_x) _NOEXCEPT {return ::log2((double)__lcpp_x);}
                                     ^~~~~~
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:1302:46: note: candidate function
inline _LIBCPP_INLINE_VISIBILITY float       log2(float __lcpp_x) _NOEXCEPT       {return ::log2f(__lcpp_x);}
                                             ^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:1303:46: note: candidate function
inline _LIBCPP_INLINE_VISIBILITY long double log2(long double __lcpp_x) _NOEXCEPT {return ::log2l(__lcpp_x);}
                                             ^
3 errors generated.
Target //tensorflow/contrib/lite:libtensorflowLite.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 60.727s, Critical Path: 13.01s
INFO: 19 processes, local.
FAILED: Build did NOT complete successfully
```

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu
- **TensorFlow installed from (source or binary)**:
sources 
- **TensorFlow version (use command below)**:
master
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.14
- **GCC/Compiler version (if compiling from source)**:
NDK 17
- **CUDA/cuDNN version**:
No
- **GPU model and memory**:
No
- **Exact command to reproduce**:
bazel build //tensorflow/contrib/lite:framework --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=""-std=c++14""

Thanks for help
"
20191,`tf.contrib.data.sliding_window_batch` cannot slide further than the window size,"### System information
- **Have I written custom code: yes**:
- **OS Platform and Distribution: macOS High Sierra 10.13.4**:
- **TensorFlow installed from: binary**:
- **TensorFlow version: 1.8.0**:
- **Python version: 3.6.5**: 
- **CUDA/cuDNN version, GPU info: N/A; not using GPU support**:
- **Exact command to reproduce**:

### Problem
`tf.contrib.data.sliding_window_batch` requires that the `stride` argument lie in the range `[1, window_size)`. This limits potential uses for this function. For example; consider the following scenario where the model is fed an element and must predict the previous and next element. Here is the description of the task more precisely:

We have data elements `(e0, e1, e2, ...)`. The model has to learn to predict `e_{i-1}, e_{i+1}` given `e_i`. To give the model triples of batches from the original sequence `(e0, e1, e2, ...)`, we could use `tf.contrib.data.sliding_window_batch` as follows, assuming `dataset` yields `(e0, e1, e_2, ...)`:

```python
dataset = dataset.apply(window_size=batch_size, stride=1)
# yields ([e_0, ..., e_{b-1}], [e_1, ..., e_b], [e_2, ..., e_{b+1}], ...)

dataset = dataset.apply(window_size=3, stride=batch_size)
# yields:
# [[e_0, ..., e_{b-1}],     [[  e_b  , ..., e_{2b-1}],
#  [e_1, ...,   e_b  ],  ;   [e_{b+1}, ...,   e_2b  ],  ;  ...
#  [e_2, ..., e_{b+1}]]      [e_{b+2}, ..., e_{2b+1}]]
```

Notice how the training inputs would be the middle row, and if the stride size were smaller, we would feed the same input-labels triples to the model twice. So the stride size in this case needs to be larger than the window size, but the existing implementation does not allow that.

### Source code / logs
As soon as one attempts to run the iterator within a session, one gets an error log with this relevant line:
```
...
tensorflow.python.framework.errors_impl.InvalidArgumentError: Stride must be in [1, window_size).
```
"
