Issue Number,Issue Title,Issue Body
44044,[tinyML Book] `hello_world` example not running on Ardunio Nano 33 BLE,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **masOS.Catalina.10.15.5**
- TensorFlow installed from (source or binary): **binary**
- Tensorflow version (commit SHA if source):  **2.1.0-ALPHA-precompiled**
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): **Arduino Nano 33**

**Describe the problem**

After following the book and the instructions in the [video screencast](https://youtu.be/AfAyHheBk6Y?t=1671) I am able to compile the program and apparently load it onto the board, but nothing seems to happen.

Then when I try to inspect the serial port logger, I am always faced with `Board at /dev/cu.usbmodem14301 is not available` and I am forced to reset the board and start again, but the same problem arises.

**Please provide the exact sequence of commands/steps when you ran into the problem**

For details, see below.

I face the same situation when I **only** use the code provided in the examples folder **and** when I modify it for my own model (following instructions in the book).

1. Open the `Examples>Arduino_TensorFlowLite>hello_world` example and upload gives this output:
```
Library Arduino_TensorFlowLite has been declared precompiled:
Using precompiled library in /Users/tallamjr/Documents/Arduino/libraries/Arduino_TensorFlowLite/src/cortex-m4/fpv4-sp-d16-softfp
Sketch uses 231536 bytes (23%) of program storage space. Maximum is 983040 bytes.
Global variables use 58272 bytes (22%) of dynamic memory, leaving 203872 bytes for local variables. Maximum is 262144 bytes.
Device       : nRF52840-QIAA
Version      : Arduino Bootloader (SAM-BA extended) 2.0 [Arduino:IKXYZ]
Address      : 0x0
Pages        : 256
Page Size    : 4096 bytes
Total Size   : 1024KB
Planes       : 1
Lock Regions : 0
Locked       : none
Security     : false
Erase flash

Done in 0.001 seconds
Write 231544 bytes to flash (57 pages)
[==============================] 100% (57/57 pages)
Done in 9.084 seconds
```
This immediately seems fine, but I notice that the orange LED goes from slowly blinking (in a bootloader state) to a solid always on light.

I have tried to change the `const int kInferencesPerCycle` as I initially thought that I am just not able to see the flicker, but this did not alter the solid light.

I also get this same problem when running a modified model (using the steps in the book and this notebook: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb)

Each time as well, when I try to inspect using the Serial Plotter, I get the following error in the console:
```
Board at /dev/cu.usbmodem14301 is not available
```
Any help with this would be appreciated. I feel very stuck at the moment on what to try as it seems to compiling fine. Thanks"
44043,[TFLite] Built-in way to run the reference kernels with the Python API,"Hello,

When accelerating inference (via optimized software or hardware implementations) we often want to compare the result accuracy with the reference result defined by TFLite reference kernels (`kReference`) instead of the default kernels (which often are `kGenericOptimized`).

With the C++ API it is easy to pass the `tflite::ops::builtin::BuiltinRefOpResolver` instead of the `tflite::ops::builtin::BuiltinOpResolver` to the `tflite::InterpreterBuilder` constructor. Unfortunately with the Python API we need to recompile TensorFlow to expose the experimental `tf.lite.InterpreterWithCustomOps` API and manually export the symbol of a C function which adds the `tflite::ops::builtin::BuiltinRefOpResolver` to the current `tflite::MutableOpResolver`.

As manually compiling TensorFlow instead of using the public binaries is quite cumbersome for some of our users, we would like to see if it would be possible to add a built-in way to run a model with the reference kernels using the publicly distributed binaries.


Thanks
Thibaut"
44042,UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfa in position 3: invalid start byte,"
While converting my trained model from saved.pb and checkpoint file to frozen.pb getting the below error :

Traceback (most recent call last):
  File ""/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 491, in <module>
    run_main()
  File ""/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 487, in run_main
    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)
  File ""/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/absl_py/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/absl_py/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 486, in <lambda>
    my_main = lambda unused_args: main(unused_args, flags)
  File ""/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 372, in main
    freeze_graph(flags.input_graph, flags.input_saver, flags.input_binary,
  File ""/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 338, in freeze_graph
    input_graph_def = _parse_input_graph_proto(input_graph, input_binary)
  File ""/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 253, in _parse_input_graph_proto
    text_format.Merge(f.read(), input_graph_def)
  File ""/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py"", line 122, in read
    return self._prepare_value(self._read_buf.read(length))
  File ""/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py"", line 94, in _prepare_value
    return compat.as_str_any(val)
  File ""/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/util/compat.py"", line 139, in as_str_any
    return as_str(value)
  File ""/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/util/compat.py"", line 118, in as_str
    return as_text(bytes_or_text, encoding)
  File ""/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/util/compat.py"", line 109, in as_text
    return bytes_or_text.decode(encoding)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfa in position 3: invalid start byte


Below are my system details :

OS: Ubuntu  - 20.04.1
Python Version : 3.8.5

I found couple of threads detailing the use of 'rb' instead of 'r', but I'm not sure of its working here. "
44041,AttributeError: module 'tensorflow' has no attribute 'matrix_determinant',"when i write in jupyter

```
import tensorflow as tf
matrix_det = tf.matrix_determinant(matrix_3)
```

the error will be:```

AttributeError                            Traceback (most recent call last)
<ipython-input-16-b52dc5929b23> in <module>
----> 1 matrix_det = tf.matrix_determinant(matrix_3)

AttributeError: module 'tensorflow' has no attribute 'matrix_determinant'
```"
44040,not able to find libcudnn.so.7,"**System information**
Operating System: 20.04.4 LTS
TensorFlow version: 2.3.1
Python version: 3.7.4
GCC/Compiler: 7.3.0
CUDA: 10.1.243
cuDNN: 8.0.4
GPU model and memory: GeForce GTX 960m, 4 Gb
NVIDIA-SMI: 450.80.02


2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

2020-10-15 13:01:11.746116: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
v2.3.0-54-gfcc4b966f1 2.3.1

>>> import tensorflow as tf

2020-10-15 12:44:04.961178: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
>>> 
>>> tf.config.list_physical_devices(""GPU"")
2020-10-15 12:44:11.883542: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-10-15 12:44:11.915652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-15 12:44:11.916045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: GeForce GTX 960M computeCapability: 5.0
coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 74.65GiB/s
2020-10-15 12:44:11.916111: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-15 12:44:11.918107: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-15 12:44:11.919968: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-15 12:44:11.920310: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-15 12:44:11.922495: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-15 12:44:11.923927: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
**2020-10-15 12:44:11.924102: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:/home/nitin/catkin_ws/devel/lib:/usr/lib/cuda/include:/usr/lib/cuda/lib64
2020-10-15 12:44:11.924120: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries.** 
Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]



"
44038,different result from tf.image.resize in tensorflow 2,"I use tensorflow 2.2 and python 3.6.9

I tried to convert tf 1.12 version model to tf 2.2 version model

but in tf 2.2
tf.image.resize(image, [299, 299], method=""bilinear"", antialias=True)
tf.image.resize(image, [299, 299], method=""bilinear"", antialias=False)

are different results with tf.compat.v1.image.resize(image, [299,299], method='bilinear')

for the same image.
"
44037,How to pass mutable tensors to a Custom Op in Python?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, I've written custom ops and import it on Python side.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1
- Python version: 3.5


**Describe the current behavior**
I'm trying custom op with mutable inputs. Therefore I got my own custom op with:
```
REGISTER_OP(""MyCustomOp"").Input(""input1: Ref(float)"") ....
```

I was able to call this custom ops from my Python keras layer:
```
class MyKerasLayer(Layer):
...
 def build(self, input_shape):
    ...
    self.moving_mean = self.add_weight(shape=(5,), trainable=False, name=""moving_mean"")

 def call(self, inputs):
    return self.custom_op.MyCustomOp(input1=self.moving_mean, ...)
```

However, when running the custom layer in graph mode, I got errors like:
```
TypeError: 'MyCustomOp' Op requires that input 'input1' be a mutable tensor (e.g.: a tf.Variable)
```

**I'm trying to find a way you could feed a layer's weights into custom ops, in a way these inputs are mutable.**

**Describe the expected behavior**

By passing `self.moving_mean` to my `MyCustomOp`, the Op accept it as a valid mutable tensor.


"
44036,Disable mkl inside eigen,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version:latest 
- Python version:3.7.3
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source):3.3.0
- GCC/Compiler version (if compiling from source): gcc 8.3
- I did **NOT** use the --config=mkl flag while building
- The image is a call graph of 1024X1024 Matmul done 100 times, if you look carefully mkldnn_sgemm is called internally by Eigen, this is what I want to disable.
 
![internal_eigen](https://user-images.githubusercontent.com/44555985/96214609-7444a200-0f99-11eb-988b-c4f9b32c6d2d.jpg)


After some amount of reading I found out that MKL/mkl_dnn can be called internally by Eigen and after reading the Bazel documentation and seeing how TensorFlow is strucured.I want to disable mkl/mkl_dnn completely if eigen is to be used.
The Eigen[ build files loads the ifmkl symbol](https://github.com/tensorflow/tensorflow/blob/bd35e19edbf55bf3f6b5d10545725d41a99aa9ba/third_party/eigen3/BUILD#L5) from the mkl directory.
My next step was to look at the [if_mkl function inside the build_defs.bzl](https://github.com/tensorflow/tensorflow/blob/c63b59ce648b525c4bbb2d85f04f5b71f1849ff2/third_party/mkl/build_defs.bzl#L17)   

I changed the [includes attribute in the cclibrary rule in the BUILD file of eigen](https://github.com/tensorflow/tensorflow/blob/44aace846df438bf55e88dad1e047b7c81f9efb9/third_party/eigen3/BUILD#L36) to [""//conditions:default""]

which game me an error saying `ModuleNotFoundError: No module named 'portpicker' `

So pip installed portpicker `pip install portpicker`
And the build completed sucessfully, but the profile literally shows no difference mkldnn_sgemm is still bieng called the same number of time by eigen internally

NOTE: **The Ultimate aim is to disable mkldnn which is bieng called from inside eigen**"
44033,triggered tf.function retracing warning,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**
- TensorFlow installed from (source or binary): **python3 pip**
- TensorFlow version (use command below): **2.3.1**
- Python version: **python 3.6**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
WARNING:tensorflow: triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, 
(2) passing tensors with different shapes, 
(3) passing Python objects instead of tensors. 
For (1), please define your @tf.function outside of the loop. 
For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. 
For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.

I add experimental_relax_shapes=True option in my code but not work.

**Describe the expected behavior**
I would like not to have retracing if there is no need.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

#### example code
```
import tensorflow as tf
from random import randint
from tensorflow.keras.layers import Conv1D
from tensorflow.keras.models import Sequential

model = Sequential()
model.add(Conv1D(8, 3))
model.build([None, 12, 1])

predict_tensors = [
    tf.random.normal([randint(1, 8), randint(4, 40), 1])
    for _ in range(10)
]
for t in predict_tensors:
    print(model.predict(t))
```"
44032,Densenet (Floating point models) reported accuracy is much lower than the original paper,"## URL(s) with the issue:
https://www.tensorflow.org/lite/guide/hosted_models#floating_point_models

## Description of issue (what needs changing):
The accuracy of Densenet reported in the URL is much lower than the one reported in the original paper.

### Clear description
In the original paper (https://arxiv.org/abs/1608.06993), the performance is as follows:
Top-1: 74.98%; Top-5: 92.29% (Densenet-121)
However, the performance reported in the URL is as follows:
Top-1: 64.2%; Top-5: 85.6% (Densenet-?)
And there is no information about the depth of the provided densenet model.

Is there any modification for this model?
Or they are typos?
"
44030,Multi-gpus call failed by  tf.distribute.MirroredStrategy(),"**System information**
- Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):pip install tf-nightly-gpu
- TensorFlow version (use command below):2.4.0-dev20201014
- Python version:3.7.8
- CUDA/cuDNN version:CUDA11.1+CUDNN8.0.4.30
- Multi GPUs: RTX3080*2

**Describe the current behavior**
Code
```sh
import tensorflow as tf
import tensorflow_datasets as tfds

num_epochs = 5
batch_size_per_replica = 64
learning_rate = 0.001
strategy = tf.distribute.MirroredStrategy()
print('Number of devices: %d' % strategy.num_replicas_in_sync)  
batch_size = batch_size_per_replica * strategy.num_replicas_in_sync


def resize(image, label):
    image = tf.image.resize(image, [224, 224]) / 255.0
    return image, label

dataset = tfds.load(""cats_vs_dogs"", split=tfds.Split.TRAIN, as_supervised=True)
dataset = dataset.map(resize).shuffle(1024).batch(batch_size)

with strategy.scope():
    model = tf.keras.applications.MobileNetV2(weights=None, classes=2)
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        metrics=[tf.keras.metrics.sparse_categorical_accuracy]
    )

model.fit(dataset, epochs=num_epochs)
```
<details>
<summary>The error log is as follows</summary>
<pre><code>
Epoch 1/5
INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-5-ca3f8dc7254f> in <module>
----> 1 model.fit(dataset, epochs=num_epochs)

~/anaconda3/envs/sshtf24/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1083                 _r=1):
   1084               callbacks.on_train_batch_begin(step)
-> 1085               tmp_logs = self.train_function(iterator)
   1086               if data_handler.should_sync:
   1087                 context.async_wait()

~/anaconda3/envs/sshtf24/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--> 828       result = self._call(*args, **kwds)
    829       compiler = ""xla"" if self._experimental_compile else ""nonXla""
    830       new_tracing_count = self.experimental_get_tracing_count()

~/anaconda3/envs/sshtf24/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    886         # Lifting succeeded, so variables are initialized and we can run the
    887         # stateless function.
--> 888         return self._stateless_fn(*args, **kwds)
    889     else:
    890       _, _, _, filtered_flat_args = \

~/anaconda3/envs/sshtf24/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   2948        filtered_flat_args) = self._maybe_define_function(args, kwargs)
   2949     return graph_function._call_flat(
-> 2950         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
   2951 
   2952   @property

~/anaconda3/envs/sshtf24/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1926       # No tape is watching; skip to running the function.
   1927       return self._build_call_outputs(self._inference_function.call(
-> 1928           ctx, args, cancellation_manager=cancellation_manager))
   1929     forward_backward = self._select_forward_and_backward_functions(
   1930         args,

~/anaconda3/envs/sshtf24/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    559               inputs=args,
    560               attrs=attrs,
--> 561               ctx=ctx)
    562         else:
    563           outputs = execute.execute_with_cancellation(

~/anaconda3/envs/sshtf24/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

NotFoundError: 3 root error(s) found.
  (0) Not found:  No algorithm worked!
	 [[node mobilenetv2_1.00_224/Conv1/Conv2D (defined at /home/librazxc/anaconda3/envs/sshtf24/lib/python3.7/threading.py:926) ]]
	 [[div_no_nan_1/ReadVariableOp_1/_264]]
  (1) Not found:  No algorithm worked!
	 [[node mobilenetv2_1.00_224/Conv1/Conv2D (defined at /home/librazxc/anaconda3/envs/sshtf24/lib/python3.7/threading.py:926) ]]
  (2) Not found:  No algorithm worked!
	 [[node mobilenetv2_1.00_224/Conv1/Conv2D (defined at /home/librazxc/anaconda3/envs/sshtf24/lib/python3.7/threading.py:926) ]]
	 [[Adam/Adam/group_deps/NoOp/_307]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_44264]

Errors may have originated from an input operation.
Input Source operations connected to node mobilenetv2_1.00_224/Conv1/Conv2D:
 cond_2/Identity (defined at <ipython-input-5-ca3f8dc7254f>:1)

Input Source operations connected to node mobilenetv2_1.00_224/Conv1/Conv2D:
 cond_2/Identity (defined at <ipython-input-5-ca3f8dc7254f>:1)

Input Source operations connected to node mobilenetv2_1.00_224/Conv1/Conv2D:
 cond_2/Identity (defined at <ipython-input-5-ca3f8dc7254f>:1)

Function call stack:
train_function -> train_function -> train_function
</code></pre>
</details>

Can you give me some suggestions on how to use multiple gpus in tf2.4?"
44028,Unable to build TensorFlow targeting Raspberry Pi,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version: v2.3.0
- Python version: N/A
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
I am following the instructions on https://www.tensorflow.org/install/source_rpi to build a `.whl` package that I can install on a Raspberry Pi 3B, but the build process is failing. I can clone TensorFlow, check out a revision, and begin the build in the Docker container without issue, but the build fails with the following output at the end:

```
Removing intermediate container 8f1ac04cb8a3
 ---> 9c723ba9afae
Step 14/14 : COPY install/.bazelrc /etc/bazel.bazelrc
 ---> 8dba22ce896f
Successfully built 8dba22ce896f
Successfully tagged tf_ci.pi-python37:latest
Running 'tensorflow/tools/ci_build/pi/build_raspberry_pi.sh' inside tf_ci.pi-python37...
Reading package lists...
Building dependency tree...
Reading state information...
sudo is already the newest version (1.8.16-0ubuntu1.9).
0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.
addgroup: Please enter a username matching the regular expression configured
via the NAME_REGEX[_SYSTEM] configuration variable.  Use the `--force-badname'
option to relax this check or reconfigure NAME_REGEX.
```

If I instead try to build `v2.3.1`, I get the following output:
```
Step 15/15 : ENV TF_ENABLE_XLA=0
 ---> Using cache
 ---> e7647e308801
Successfully built e7647e308801
Successfully tagged tf_ci.pi-python37:latest
Running 'tensorflow/tools/ci_build/pi/build_raspberry_pi.sh' inside tf_ci.pi-python37...
Reading package lists...
Building dependency tree...
Reading state information...
sudo is already the newest version (1.8.16-0ubuntu1.9).
0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.
addgroup: To avoid problems, the username should consist only of
letters, digits, underscores, periods, at signs and dashes, and not start with
a dash (as defined by IEEE Std 1003.1-2001). For compatibility with Samba
machine accounts $ is also supported at the end of the username
```


This happens well before the 30 minutes the documentation suggests the process should take, and there is no `.whl` in the `tensorflow/output-artifacts` directory.

I am experiencing this issue when trying to compile v2.3.0, but I have encountered when trying to build v1.15.0 as well. I would expect the use of containers and Bazel to make the build reproducible, but it would seem that something has changed that broke the build process. Please let me know if there are any additional logs I could provide to help diagnose this.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

> Note: you must have docker installed for the following steps

```shell
$ git clone https://github.com/tensorflow/tensorflow.git
$ cd tensorflow
$ git checkout v2.3.0
$ tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \
    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh
```

At this point the build will fail, with no output `.whl` generated.
"
44026,Everything is as expected when batch size <= 64 but having nan loss after few epochs if batch size > 64,"<em>When I was training my image dataset with InceptionResNetV2 everything works fine if the batch size is less than or equal to 64, however, if I set the batch size larger than 64 (For example, 128), after a few epochs the loss becomes nan</em>

**System information**
- OS Platform and Distribution: Linux-4.15.0-112-generic-x86_64-with-Ubuntu-18.04-bionic
- TensorFlow installed from (source or binary): NGC Container
- TensorFlow version (use command below): 2.2.0+nv
- Python version: 3.6.9
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): c++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
- CUDA/cuDNN version: 11.0  
- GPU model and memory: Tesla V100 32510MiB

**Code**
##
## Check multiple GPUs and model selection
##
    my_strategy = tf.distribute.MirroredStrategy()
    with my_strategy.scope():
        if args.model == 'usDL_InceptionResNetV2':
            print(""train usDL_InceptionResNetV2...!"")
            logging.info(""train usDL_InceptionResNetV2 with {} GPUs..."".format(args.gpus))
            model = models.usDL_InceptionResNetV2(args)

        elif args.model == 'usDL_InceptionV3':
            print(""train usDL_InceptionV3...!"")
            logging.info(""train usDL_InceptionV3 with {} GPUs..."".format(args.gpus))
            model = models.models.usDL_InceptionV3(args)

        else:
            print(""No model is available!"")
            sys.exit()

            # Replicates the model on multiple GPUs.
            #model = multi_gpu_model(model, gpus=args.gpus)

        ##
        ## compile the model
        ##

        if (args.model == ""usDL_InceptionResNetV2""):

            # 2020-05-01
            logging.info(""loss: categorical_crossentropy, optimizer: Adam()"")
            model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])

            # model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['acc'])

        else:
            logging.info(""optimizer: Adam, loss: ssim_loss"")
            model.compile(optimizer=Adam(), loss=ssim_loss, metrics=[ssim_loss, 'accuracy'])

        ##
        ## use call back functions
        ##

        ModelCheckpointFileName = os.path.join(model_save_dir, 'model_{epoch:02d}.h5')
        ckpt = ModelCheckpoint(ModelCheckpointFileName, monitor='val_loss', verbose=0, period=args.save_every)
        csvlogFileName = os.path.join(model_save_dir, 'log.csv')
        csv_logger = CSVLogger(csvlogFileName, append=True, separator=',')
        lr_sched = step_decay_schedule(initial_lr=args.lr)

    ##
    ## train
    ##
    ## The ""steps_per_epoch"" is not provoded here since the ""__len__()"" function
    ## has been provoded in the ""My_Custom_Generator""
    ##

    print(""train: start..."")

    if (args.model == ""usDL_InceptionResNetV2""):
        history = model.fit_generator(generator=train_generator,
                                      # steps_per_epoch=len(data)//args.batch_size,
                                      epochs=args.epoch,
                                      verbose=1,
                                      callbacks=[ckpt, csv_logger, lr_sched])"
44024,Validation accuracy stuck at 0.6 in my CNN model,"I've been creating a CNN to map facial emotions to certain emojis, but after certain epochs I cant seem to get the validation accuracy past the 0.61 mark, while the training accuracy keeps increasing smoothly as required. I can recognize this as an overfitting problem since the validation loss also dips and then keeps increasing, but I cant seem to find methods like regularization or dropouts showing any change in the validation accuracy. So I'm wondering if something is wrong with my model itself, like it being having too many parameters or such? Here is the model:

model = tf.keras.Sequential()

model.add(Conv2D(32, kernel_size = (3,3), input_shape = (48,48,1)))
model.add(Activation('relu'))
model.add(Conv2D(64, kernel_size=(3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(128, kernel_size=(3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(128, kernel_size=(3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(1024))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(7))
model.add(Activation('softmax'))

Data consists of 48x48 pixel images which has 28701 training instances and 7178 validation instances "
44023,"In TFnighty TFLite  Interpreter missing setUseNNAPI ""wrapper"" method  to call Interpreter.Options.setUseNNAPI","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WIndows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Android emulator
- TensorFlow installed from (source or binary):  org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'
- TensorFlow version (use command below): nightly
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Making call to Interpreter's setUseNNAPI method which no longer exists in the tfnightly version as of Oct. 12.   The method it should call Interpreter.Options.setUseNNAPI still exists but, not in the Interperter itself.

**Describe the expected behavior**
Either change the documentation to reflect this method does not exist or fix this.


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
tfLite = new Interpreter(loadModelFile(assetManager, modelFilename));

if (tfLite != null) tfLite.setUseNNAPI(isChecked);

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44021,Different output for custom tf.keras.Model within tf.function,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): Colab Pre-Installed
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: 
- GPU model and memory: 


**Describe the current behavior**
I'm trying to obtain reproducible results between graph and eager execution. While doing that I observed that calling my tf.keras.Model within a tf.function wouldn't exactly give me the same results as in eager execution. I was wondering if that's expected ? If yes, what's the reason for it ?

**Describe the expected behavior**
I would expect to get exactly the same value.

**Standalone code to reproduce the issue**
Here is a Colab : https://colab.research.google.com/drive/1n4p-Iq_g7dT-Vv5cb-s6nyeqjuYjyyGL?usp=sharing

**Other info / logs** 
"
44020,Problem to import keras from tensorflow 2.3.1,"I succesfully upgraded my python files to tensorflow 2. with tf_upgrade_2.0
**System information**
- I have written custom code 
- OS Platform : windows 10
- TensorFlow : 2.3.1
- TensorFlow-gpu 2.3.1
- Keras version : 2.3.1
- Python version: 3.7.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0.130 / 7.6.5
- GPU model and memory: NVIDIA 1060 6GB

When I start importing various libriaries, the python file bugs rapidly. I used to run the following imports :
```
import os
import warnings
import numpy as np
import numpy_financial as npf
import pandas as pd
import sqlite3 as sq
import time
from itertools import chain
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras.layers import Dropout, Activation, Dense, LSTM, Flatten
from tensorflow.keras.layers import Concatenate
from tensorflow.keras.layers import CuDNNLSTM, CuDNNGRU
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import confusion_matrix

from util_data import *
from util_prepa import *
from util_model import *
from util_invest import *
from util_RESNET import *
```

I automaticaly get an error message  : 
`ModuleNotFoundError: No module named 'tensorflow.python.keras.activations'`
preceeded with 
```
File ""D:\GD Navagne\AI\tf2\ResNet1D3D_list.py"", line 27, in <module>
    from tensorflow import keras

  File ""D:\Anaconda3\envs\tf2\lib\site-packages\tensorflow\keras\__init__.py"", line 14, in <module>
    from . import activations

  File ""D:\Anaconda3\envs\tf2\lib\site-packages\tensorflow\keras\activations\__init__.py"", line 10, in <module>
    from tensorflow.python.keras.activations import deserialize
```
I don't know what to do... Thanks in advance for the help."
44018,"I've created a DCGAN model and when i run that in tensorflow ver 1.x and 2.x, it gives different result.What is the the reason for this?Thanks","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44016,TFLite not respecting ByteBuffer's limit,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android (Fire OS 5)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Kindle Fire HD 8
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): newest tflite 0.0.0-nightly as of 14.10.20
- Python version: n/a
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I have a lot of data in ByteBuffer that I want to run in chunks (for streaming the model). However, when passing a slice of ByteBuffer with limit set to less than the underlying array, TFLite errors out here:
https://github.com/tensorflow/tensorflow/blob/76c685252469e800aa4486c50f6a390f26b806a7/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Tensor.java#L443

**Describe the expected behavior**
I expect TFLite to take the limit() of the buffer as its size, not just the capacity():
https://github.com/tensorflow/tensorflow/blob/76c685252469e800aa4486c50f6a390f26b806a7/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Tensor.java#L441

**Standalone code to reproduce the issue**
```java
ByteBuffer bb = getBuffer();
Buffer sliced = bb.slice().limit(8);
tflite.run(sliced, output);
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.xxx.xxx/com.xxx.xxx.XXX}: java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (xxx) with 16000 bytes from a Java Buffer with 650880 bytes.
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2473)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2541)
        at android.app.ActivityThread.access$800(ActivityThread.java:160)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1321)
        at android.os.Handler.dispatchMessage(Handler.java:102)
        at android.os.Looper.loop(Looper.java:135)
        at android.app.ActivityThread.main(ActivityThread.java:5597)
        at java.lang.reflect.Method.invoke(Native Method)
        at java.lang.reflect.Method.invoke(Method.java:372)
        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:984)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:779)
     Caused by: java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (xxx) with 16000 bytes from a Java Buffer with 650880 bytes.
        at org.tensorflow.lite.Tensor.throwIfSrcShapeIsIncompatible(Tensor.java:444)
        at org.tensorflow.lite.Tensor.setTo(Tensor.java:189)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:159)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:360)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:319)
        at com.xxx.xxx.XXX.xxx(XXX.java:75)
        at com.xxx.xxx.XXX.onCreate(XXX.java:30)
        at android.app.Activity.performCreate(Activity.java:6010)
        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1122)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2426)
```"
44013,Patching CMSIS source file takes a lot of time,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Source
- Tensorflow version (commit SHA if source): 261bc3aba4e5c1611a417cf9d916c916996afad2
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Any 

**Describe the problem**
The download and extract script patches CMSIS source files to use qualified paths in order to be compatible with Arduino IDE build system. Currently it takes a significant amount of time, since there are many include files that need to be patched.

Ideally, there would be no need for patching CMSIS at all, but that would require to change the entire CMSIS repo to using qualified paths. Another solution could be to do the patching only when generating Arduino projects. However, that would most likely require us to patch the TFLu optimized op code (kernels/cmsis-nn/*) instead. 

A short term solution is to optimize the patching algorithm.

**Please provide the exact sequence of commands/steps when you ran into the problem**
Any command running using TAGS=""cmsis-nn"". For example:
`make -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn TARGET=sparkfun_edge person_detection_int8_bin`
"
44012,CUDA issues with 'tf-2.4.0-dev20201012',"**System information**
- OS Platform and Distribution : Ubuntu 18.04.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): `pip install tensorflow-2.3.0` and then `pip install --quiet --upgrade tf-nightly` in
a `virtualenv`

- TensorFlow version: `2.4.0-dev20201012`
- Python version: `3.7.5`
- Installed using virtualenv? pip? conda?: `virtualenv` and `pip`
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: `Cuda compilation tools, release 10.1, V10.1.243` and 
```
$apt list --installed | grep -i 'libcudnn'
libcudnn7/unknown,now 7.6.5.32-1+cuda10.1 amd64 [installed,upgradable to: 7.6.5.32-1+cuda10.2]
libcudnn7-dev/unknown,now 7.6.5.32-1+cuda10.1 amd64 [installed,upgradable to: 7.6.5.32-1+cuda10.2]
libcudnn7-doc/now 7.6.5.32-1+cuda10.1 amd64 [installed,local]
```
- GPU model and memory: RTX 2080 Ti (12GB) and RTX 2080 SUPER (8 GB)




**I am trying to test the new tensorflow NumPy API so I installed `tf-2.3.0` in a `virtualenv` and then pip upgraded it to nightly. Within `tf-2.3.0` I didn't have any issues with **
```
assert tf.test.is_gpu_available()
assert tf.test.is_built_with_cuda()
tf.config.list_physical_devices('GPU')
```

** When I upgraded the tensorflow to nightly I get the following errors **
```
(tf2.3_env) ruthvik@ruthvik:~$ pythonPython 3.7.5 (default, Nov  7 2019, 10:50:52) 
[GCC 8.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2020-10-13 06:51:35.053420: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib
2020-10-13 06:51:35.053436: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
INFO:tensorflow:Using local port 19355
INFO:tensorflow:Using local port 18622
INFO:tensorflow:Using local port 24757
INFO:tensorflow:Using local port 15110
INFO:tensorflow:Using local port 23483
INFO:tensorflow:Using local port 24017
INFO:tensorflow:Using local port 15691
INFO:tensorflow:Using local port 19679
INFO:tensorflow:Using local port 16397
INFO:tensorflow:Using local port 15952
>>> tf.__version__'2.4.0-dev20201012'
>>> assert tf.test.is_gpu_available()
WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2020-10-13 06:52:14.678269: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-13 06:52:14.680128: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-10-13 06:52:14.683285: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2020-10-13 06:52:14.995295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-13 06:52:14.995671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-10-13 06:52:14.995713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-13 06:52:14.996014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:04:00.0 name: GeForce RTX 2080 SUPER computeCapability: 7.5
coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 462.00GiB/s
2020-10-13 06:52:14.996088: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib
2020-10-13 06:52:14.996156: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib
2020-10-13 06:52:14.997073: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2020-10-13 06:52:14.997223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2020-10-13 06:52:14.998150: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2020-10-13 06:52:14.998217: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib
2020-10-13 06:52:14.998270: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib
2020-10-13 06:52:14.998278: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-10-13 06:52:14.998291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-13 06:52:14.998296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 
2020-10-13 06:52:14.998301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N 
2020-10-13 06:52:14.998304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N 
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AssertionError
>>> tf.config.list_physical_devices('GPU')
2020-10-13 06:52:57.975881: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-10-13 06:52:57.976201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-13 06:52:57.978121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-10-13 06:52:57.978335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-13 06:52:57.979850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: 
pciBusID: 0000:04:00.0 name: GeForce RTX 2080 SUPER computeCapability: 7.5
coreClock: 1.815GHz coreCount: 48 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 462.00GiB/s
2020-10-13 06:52:57.980178: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib
2020-10-13 06:52:57.980437: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib
2020-10-13 06:52:57.980506: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2020-10-13 06:52:57.980552: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2020-10-13 06:52:57.980591: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2020-10-13 06:52:57.980798: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib
2020-10-13 06:52:57.981034: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib
2020-10-13 06:52:57.981075: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
```
From what it looks like it's looking for `*.so*` files that are both `*.so.10*` and `*.so.11*` and the cuDNN file `libcudnn.so.8` isn't that not supposed to happen if say I have just CUDA-10.1? Does this mean that I have to have multiple CUDA versions in my `$PATH` ? Why is it looking for multiple versions of libraries? In `tf-2.3.0` I noticed that it looks for all files that are only `*.so.10*`  and `libcudnn.so.7`"
44011,ops defined inside tf.while_loop's cond/body or tf.cond's true_fn/false_fn functions ignore their enclosed tf.device if the tf.while_loop/tf.cond itself is inside a tf.device,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: Python 3.7.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

`tf.print` doesn't print on specified device (using `tf.device`) if it is inside a for loop in graph mode.

Initially I though it's an autograph issue. But if you trace the graph and inspect it in tensorboard (color by device), the graph does have that PrintV2 placed where it's supposed to.

Now, I'm not sure why it's happening.
I'm yet to verify if this happens across all the ops, or just the `tf.print`.

Waiting for the team to let me know what to check next.

**Describe the expected behavior**

`tf.print` should print on specified device (using `tf.device`). ~Everything works fine if I'm using tf.while_loop instead.
Can be seen from sample code.~

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**THIS SNIPPET IS NOT CORRECT ANYMORE, PLEASE READ A COUPLE OF COMMENTS BELOW.**

```python
import tensorflow as tf

# assume there's a tf.distribute.Server running on rpi.local:2222
tf.config.experimental_connect_to_cluster(
    tf.train.ClusterSpec({'worker': ['dhruvins-macbook-air.local:2222', 'rpi.local:2222']}),
    job_name='worker',
    task_index=0
)

laptop = '/job:worker/task:0'
rpi = '/job:worker/task:1'


@tf.function
def test_for():
    with tf.device(rpi):
        for i in tf.range(3):
            tf.print('rpi', 'for', i)
            with tf.device(laptop):
                tf.print('laptop', 'for', i)


@tf.function
def test_while():
    def cond(i):
        return i < 3

    def body(i):
        with tf.device(rpi):
            tf.print('rpi', 'while', i)
            with tf.device(laptop):
                tf.print('laptop', 'while', i)
        return [i + 1]

    tf.while_loop(cond, body, [0], parallel_iterations=1)


test_for()
test_while()

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44010,ImportError: cannot import name 'MomentumParameters',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Its on COLAB



**Describe the problem**
I find your beautiful code and run the very second line of code and was thrilled to get the error and wasted my one full day to figure out the issue

Line of Code - [from tflite_model_maker import configs]
Error  - ImportError: cannot import name 'MomentumParameters'

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I just ran your code from this - https://www.tensorflow.org/lite/tutorials/model_maker_text_classification, and it fails at second line of code

**Any other info / logs**
Following imports fail as well
from tflite_model_maker import ExportFormat
from tflite_model_maker import model_spec
from tflite_model_maker import text_classifier
from tflite_model_maker import TextClassifierDataLoader

Thanks So much
Happy Coding

"
44009,ValueError: Unable to create group (Name already exists),"```
def create_group(self, name, track_order=None):
        #Create and return a new subgroup.

        #Name may be absolute or relative.  Fails if the target name already
        #exists.

        #track_order
          #  Track dataset/group/attribute creation order under this group
          #  if True. If None use global default h5.get_config().track_order.
        
        if track_order is None:
            track_order = h5.get_config().track_order

        with phil:
            name, lcpl = self._e(name, lcpl=True)
            gcpl = Group._gcpl_crt_order if track_order else None
            gid = h5g.create(self.id, name, lcpl=lcpl, gcpl=gcpl)
            return Group(gid)

```

I am training a ssd-mobilenet model, after the end of 1st epoch, I am facing the issue. 
Any ideas here ? 

python2.7 "
44008,Using skip() of tf.data.Dataset is slow for big datasets,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.9
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**

When using _skip()_ to skip most of a big _tf.data.Dataset_ (e.g. to create a validation set from a training split by just taking the last 10% of the training data), iteration over the dataset takes very long to begin. In the standalone code below skipping samples takes about 1.5 minutes to return the first sample. Another test with imagenet took ~25 minutes before returning the first sample.

**Describe the expected behavior**

1. I would expect the first sample from a dataset to be available in a shorter time (seconds not minutes, <10 seconds). 
2. I would also expect the time to get the first sample when skipping to depend on the size of the shards not on the size of the dataset.

**Standalone code to reproduce the issue**

Below is a small script using COCO from tfds to demonstrate the problem.

For me the output is something like this:
> First sample for complete set: 0.49 sec
> First sample for skipped set: 89.66 sec
> 

Standalone code:
```python
import tensorflow_datasets as tfds
import time

def test_tfds(dataset_name='coco'):
    print('### Test tfds ###')

    # Download dataset
    print(""Downloading dataset '{}'..."".format(dataset_name))
    builder = tfds.builder(dataset_name)
    download_dir = dataset_name
    builder.download_and_prepare(download_dir=download_dir)
    print('...dataset downloaded:\n{}'.format(builder.info.splits))

    # Get train split 
    split = 'train'
    dataset = tfds.load(dataset_name, split=split)
    
    test(dataset, builder.info.splits[split].num_examples)

def test(dataset, num_samples):
    # Get first sample of complete dataset: this should be ok
    t = time.time()
    _ = next(iter(dataset))
    print('First sample for complete set: {:.2f} sec'.format(time.time() - t))

    # Now skip 90% of the train split
    num_skip_samples = int(0.9 * num_samples)
    skipped_dataset = dataset.skip(num_skip_samples)

    # Get first sample of skipped dataset: this takes extremely long
    t = time.time()
    _ = next(iter(skipped_dataset))
    print('First sample for skipped set: {:.2f} sec'.format(time.time() - t))

if __name__ == '__main__':
    test_tfds('coco')
```"
44007,Output of Regression Quantized Aware Training Model is always restricted between 0 and 6,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.15


I have successfully QAT a regression model for company purposes. However, when I run it on edge_tpu model, the output inference is limited between 0 and 6. This is ridiculous when my output should be in a bigger range: From the label data, the output is from (0-95) and (0-2pi).

![min-max limit between 0-6](https://user-images.githubusercontent.com/27914179/95973523-bd3c0f80-0e4e-11eb-9fc7-a8e2a2b0a761.png)

I have also tried normalize the output to prevent the outcome exceed 0-255 for unit8 model. However, it still be the same range (0-6) no matter what

I have also tried with a different structure but always get the same result. Because this is private code so I cant share out, but I attached some frozen file for people viewing.

Any recommendation? 

In addition, this problem does not happen if I post quantize my model with a representative dataset.

```
https://drive.google.com/drive/folders/14L4Yrsk_9rB6UOgJn8La7n1epanAFI0B?usp=sharing
```
"
44006,tf.keras.utils.Generator doesn't get called after first batch.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution : Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8.5
- CUDA/cuDNN version: 10.1 / 7.6.5
- GPU model and memory: GTX 1060 (6gb)

**Describe the current behavior**
I've created a generator class which inherits from `tf.keras.utils.Sequence` that reads from directory and returns a tuple of `X, y`. I use it with a custom model and pass it to the fit method. After using logging to debug, I found out that it returns the very first batch, passes it to the model and trains, but after that it doesn't get called again, I have a logging statement in its `__getitem__` method which should get called each batch but only gets called in the first one, and so the model then gets an array of shape `(None, None, None)`.

**Describe the expected behavior**
The `__getitem__` should get called after each epoch.

**Standalone code to reproduce the issue**
Colab: https://colab.research.google.com/drive/14Pz5z65Cb5V8J_IsOJQ1Kk7SCU-ubwfy#scrollTo=iZm4LeXuUfja

**Other info / logs**
This is the logfile I used
```
2020-10-14 10:42:01,329:train.DataGenerator:Preparing batch #0
2020-10-14 10:42:04,687:train.CustomLSTM:Epoch #1
2020-10-14 10:42:04,688:train.CustomLSTM:Xe shape: (32, 1, 160, 40)
2020-10-14 10:42:05,851:train.CustomLSTM:Epoch #2
2020-10-14 10:42:06,333:train.CustomLSTM:Xe shape: (None, None, None, None)
```
As you can see the generator the first batch before logging the first epoch and passes it to the model, and then doesn't return the second batch and the model doesn't find anything to train on.
"
44005,ValueError: The same saveable will be restored with two names: layer_with_weights-1/_table/.ATTRIBUTES/table,"tensorflow 2.3.1

**Describe the current behavior**

When I run ```! pip install -U tensorflow```  at the begin and run ```! saved_model_cli show --dir 'my_pet_classifier' --all``` at the end of this [document](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers)'s colab , it raises error :

```
2020-10-14 08:20:39.291886: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1

MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID
        shape: unknown_rank
        name: NoOp
  Method name is: 

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['Age'] tensor_info:
        dtype: DT_INT64
        shape: (-1, 1)
        name: serving_default_Age:0
    inputs['Breed1'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: serving_default_Breed1:0
    inputs['Color1'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: serving_default_Color1:0
    inputs['Color2'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: serving_default_Color2:0
    inputs['Fee'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: serving_default_Fee:0
    inputs['FurLength'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: serving_default_FurLength:0
    inputs['Gender'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: serving_default_Gender:0
    inputs['Health'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: serving_default_Health:0
    inputs['MaturitySize'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: serving_default_MaturitySize:0
    inputs['PhotoAmt'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: serving_default_PhotoAmt:0
    inputs['Sterilized'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: serving_default_Sterilized:0
    inputs['Type'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: serving_default_Type:0
    inputs['Vaccinated'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: serving_default_Vaccinated:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['dense_1'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: StatefulPartitionedCall:0
  Method name is: tensorflow/serving/predict
2020-10-14 08:20:41.728805: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-10-14 08:20:41.738107: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-10-14 08:20:41.738155: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (1a8174b158ac): /proc/driver/nvidia/version does not exist
Traceback (most recent call last):
  File ""/usr/local/bin/saved_model_cli"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py"", line 1185, in main
    args.func(args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py"", line 715, in show
    _show_all(args.dir)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py"", line 307, in _show_all
    _show_defined_functions(saved_model_dir)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py"", line 187, in _show_defined_functions
    trackable_object = load.load(saved_model_dir)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py"", line 603, in load
    return load_internal(export_dir, tags, options)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py"", line 633, in load_internal
    ckpt_options)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py"", line 131, in __init__
    self._restore_checkpoint()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py"", line 330, in _restore_checkpoint
    load_status = saver.restore(variables_path, self._checkpoint_options)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py"", line 1320, in restore
    checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py"", line 209, in restore
    restore_ops = trackable._restore_from_checkpoint_position(self)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py"", line 914, in _restore_from_checkpoint_position
    tensor_saveables, python_saveables))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py"", line 290, in restore_saveables
    tensor_saveables)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 361, in validate_and_slice_inputs
    _add_saveable(saveables, seen_ops, converted_saveable_object)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 331, in _add_saveable
    saveable.name)
ValueError: The same saveable will be restored with two names: layer_with_weights-1/_table/.ATTRIBUTES/table
```"
44004,Cannot import Tensorflow due to DLL import error,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 OS Build: 19041.508
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Installed from pip
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Intel(R) HD Graphics Full Display Device 2160 MB, VRAM: 112 MB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Not able to import Tensorflow due to some DLL

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Prerak\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Prerak\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Prerak\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\Prerak\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\Users\Prerak\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Prerak\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Prerak\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
44003,segmentation fault using detection postprocess for ssd_mobilenet_v3     ,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18
- TensorFlow installed from (source or binary): binary
- Tensorflow version (commit SHA if source):  tf 1.15
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm Mbed OS

**Describe the problem**
I have to run the object detection model on Mbed OS. Currently while testing directly on Linux, we ran into a custom op unsupported issue. following is the error:  

     ` Failed to get registration from op code CUSTOM`

So, we tried to use [detection_postprocess](https://github.com/mansnils/tensorflow/tree/detection_postprocess). we ran into a segmentation fault.

`Error Stack :: SIGSEGV,
#0  0x000000000807a31c in tflite::ops::micro::custom::detection_postprocess::ComputeIntersectionOverUnion(float const*, int, int) ()
#1  0x000000000807a6a6 in tflite::ops::micro::custom::detection_postprocess::NonMaxSuppressionSingleClassHelper(TfLiteContext*, TfLiteNode*, tflite::ops::micro::custom::detection_postprocess::OpData*, float const*, int*, int*, int) ()
#2  0x000000000807b062 in tflite::ops::micro::custom::detection_postprocess::NonMaxSuppressionMultiClassFastHelper(TfLiteContext*, TfLiteNode*, tflite::ops::micro::custom::detection_postprocess::OpData*, float const*) ()
#3  0x000000000800f4e6 in tflite::MicroInterpreter::Invoke() ()
#4  0x0000000008007da1 in main ()`


When can we expect  [detection_postprocess](https://github.com/mansnils/tensorflow/tree/detection_postprocess) to be merged?.
 

**Steps to reproduce the issue**


1.Transfer Learning 
Trained the model with a custom dataset using the following weights-
[ssd_mobilenet_v3](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_small_coco_2020_01_14.tar.gz)


2.Freeze graph::

python /content/models/research/object_detection/export_tflite_ssd_graph.py \
--pipeline_config_path 'exported-model/ssd_mobilenet_v3_small_coco_2020_01_14/output_inference_graph_v1.pb/pipeline.config' \
--trained_checkpoint_prefix 'exported-model/ssd_mobilenet_v3_small_coco_2020_01_14/output_inference_graph_v1.pb/model.ckpt'  \
--output_directory 'exported-model/ssd_mobilenet_v3_small_coco_2020_01_14/tflite' 

3.Conversion to tflite ::

```
input_arrays = [""normalized_input_image_tensor""]
output_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']
input_shapes = {'normalized_input_image_tensor': [1, 320, 320, 3]}

image_list = glob.glob('/content/drive/My Drive/Colab Notebooks/colddrink/tf1/ssd_mobilenet_v3/images/train/*.bmp')
def representative_dataset_gen():
    for i in image_list:
        img = cv2.imread(i)
        img = img.astype(np.float32)
        img = cv2.resize( img,(320,320), interpolation = cv2.INTER_NEAREST)
        img = img
        img = img.reshape(1,320,320,3)
        # Get sample input data as a numpy array in a method of your choosing.
        yield [img]


graph_def_file = PATH_TO_CKPT
converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shapes)
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
converter.representative_dataset = representative_dataset_gen
converter.allow_custom_ops = True
converter.experimental_new_converter = True
tflite_model = converter.convert()
open(""exported-model/ssd_mobilenet_v3_small_coco_2020_01_14/tflite/ssd_mobilenet_v3_small_int8.tflite"", ""wb"").write(tflite_model)

```

4. Converting the model and image to cc and header files

```
xxd -i ssd.tflite object_detection.cc
xxd -s 54 -i img.bmp > img.h
```

5.  Test
[image_recognition_test.txt](https://github.com/tensorflow/tensorflow/files/5376363/image_recognition_test.txt)
[original](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/image_recognition_experimental/image_recognition_test.cc)


command
`make -f tensorflow/lite/micro/tools/make/Makefile image_recognition_test`"
44002,How to serve a model containing ```tensorflow.keras.layers.experimental.preprocessing``` layers ?,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
[https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers)

## Description of issue (what needs changing):

### Clear description

How to serve the model in that document by tf_serving ? Is there any example ?  I have read the document [https://www.tensorflow.org/guide/keras/preprocessing_layers#benefits_of_doing_preprocessing_inside_the_model_at_inference_time](https://www.tensorflow.org/guide/keras/preprocessing_layers#benefits_of_doing_preprocessing_inside_the_model_at_inference_time) , but it doesn't give an example , either .
"
44000,Comparison of gpu acceleration between gtx 1660ti(6g video memory) and quadro gv100(32g video memory),"I use the same mnist data set code to classify on gtx 1660ti graphics card and Quadro gv100 graphics card, and test that GPU acceleration is available.
Why is the speed on Quadro gv100 slower than gtx 1660ti, and what conditions should be met to turn on gpu acceleration on Quadro gv100? But I have verified that gpu acceleration is available.
![image](https://user-images.githubusercontent.com/64268860/95950710-a635f700-0e27-11eb-9116-5585afaf5f65.png)
Fig. 1 is the experimental result (37s) on gtx 1660ti
![image](https://user-images.githubusercontent.com/64268860/95955723-ded9ce80-0e2f-11eb-9bd5-2f145a1214c2.png)
![image](https://user-images.githubusercontent.com/64268860/95955752-ec8f5400-0e2f-11eb-8073-0f92795295c6.png)
Fig. 2 and 3 is the result of the experiment on Quadro gv100
![image](https://user-images.githubusercontent.com/64268860/95955860-12b4f400-0e30-11eb-84fe-0972181df3a9.png)

Fig. 4 shows the memory usage of Quadro gv100
Because of this problem, I trained very slowly when I used a lot of data to do semantic segmentation experiments. I don't think this Quadro gv100 graphics card should be trained so slowly.
Thank you very much for helping me!"
43999,Why is loading resnet50 keras model in TF 1.15 on TPU does not work?,"### System information

-   **Have I written custom code ( No , I am using tf.keras.application.resnet50 ):
-   **OS Platform and Distribution (e.g., Linux Ubuntu 20.04)**:
-   **TensorFlow version (1.15)**:
-   **Python version**: (3.7)
-   **TPU *: V2 using TPU estimator API


###
         I am trying to initialize resnet50 as backbone for a model in TF 1.15, and the model is run on google TPU V2. My code is this:


### Source code 

import tensorflow.keras as keras
from MBCONV import MBConvBlock
import yaml
import tensorflow.keras.backend as K
import tensorflow as tf

print(tf.__version__)
tf.compat.v1.enable_eager_execution()

def model():

    
    mode=resnet_backbone() // error is in this fuction 

    // adding some more layers etc 

    return tf.contrib.tpu.TPUEstimatorSpec(mode, loss=loss, train_op=train_op, host_call=host_call,
                                           predictions={""emb"": embeddings_layer})
    

def resent_backbone():
 
    backbone_model=tf.keras.applications.ResNet50(include_top=False, weights='imagenet',pooling=None)
    return backbone_model


def main(unused_argv):

    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(FLAGS.tpu)

    run_config = tf.contrib.tpu.RunConfig(
        model_dir=FLAGS.model_dir,
        cluster=tpu_cluster_resolver,
        session_config=tf.ConfigProto(
            allow_soft_placement=True, log_device_placement=True),
        tpu_config=tf.contrib.tpu.TPUConfig(FLAGS.iterations),
    )

    classifier = tf.contrib.tpu.TPUEstimator(
        model_fn=model,
        use_tpu=FLAGS.use_tpu,
        train_batch_size=FLAGS.batch_size,
        eval_batch_size=FLAGS.batch_size,
        predict_batch_size=FLAGS.batch_size,
        config=run_config,
        params={

            ""use_tpu"": FLAGS.use_tpu,
        })

    classifier.train(
        input_fn=lambda params: train_input_fn(params[""batch_size""]),
        # input_fn=lambda params: train_input_fn(params[""batch_size""]),
        max_steps=FLAGS.train_steps)


def build_prediction_network():
    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
        FLAGS.tpu)

    run_config = tf.contrib.tpu.RunConfig(
        model_dir=FLAGS.model_dir,
        cluster=tpu_cluster_resolver,
        session_config=tf.ConfigProto(
            allow_soft_placement=True, log_device_placement=True),
        tpu_config=tf.contrib.tpu.TPUConfig(FLAGS.iterations),
    )
    classifier = tf.contrib.tpu.TPUEstimator(
        model_fn=model,
        use_tpu=FLAGS.use_tpu,
        train_batch_size=FLAGS.batch_size,
        eval_batch_size=FLAGS.batch_size,
        predict_batch_size=FLAGS.batch_size,
        config=run_config,
        params={

            ""use_tpu"": FLAGS.use_tpu, })
    return classifier





### I get the following errors

























Operation of type Placeholder (input_1) is not supported on the TPU. Execution will fail if this op is used in the graph. 

tensorflow.python.framework.errors_impl.InaccessibleTensorError: Operation 'VarIsInitializedOp' has been marked as not fetchable. Typically this happens when it is defined in another function or code block. Use return values,explicit Python locals or TensorFlow collections to access it.


"
43998,Update BatchNormalization documentation,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue: 
https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization

## Description of issue (what needs changing):

### Clear description
The document should mention that the axis can take in a list of integers, not just an integer. I tested it and it is already implemented in the TensorFlow. I was not aware of it and used reshape and transpose, which is inefficient.

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

The axis can be a list of integers.

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
43997,Why TF loss function gives different results than Pytorch?,"I'm trying to figure out why TF binary_cross_entropy gives different results than Pytorch's binary_cross_entropy.

https://colab.research.google.com/drive/1hzFEI05_hKNRV4gE1FFDumd1ygN3LVsQ?usp=sharing

Anyone?"
43996,Build options,"is it possible to build a lite version from the source code? I only need Cuda support and an Intel processor. As I see during the build there is a compilation for arm including many other things. What commands can I use to restrict build options that I don't need?
"
43995,Regularization Loss is getting added twice if model is encapsulated in another model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Pre-installed TF on Colab
- TensorFlow version (use command below): 2.3
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

```
import tensorflow as tf
tfk = tf.keras
tfkl = tf.keras.layers

def test_model(input_shape):
  input_x = tfkl.Input(shape=input_shape)
  h = tfkl.Flatten()(input_x)
  h = tfkl.Dense(10, activation='relu', activity_regularizer=tf.keras.regularizers.l1())(h)

  return tf.keras.models.Model(inputs=input_x, outputs=h)

def another_model(input_shape, tm):
  input_x = tfkl.Input(shape=input_shape)
  m = tm(input_x)
  return tf.keras.models.Model(inputs=input_x, outputs=m)

tm = test_model(input_shape)
print(another_model(input_shape, tm).losses)

```
Above prints

```
[<tf.Tensor 'dense_27/ActivityRegularizer/truediv:0' shape=() dtype=float32>,
 <tf.Tensor 'functional_20/dense_27/ActivityRegularizer/truediv:0' shape=() dtype=float32>]
```

This pattern of creating simple models (modules) and then assembling them as part of the big model is widely used and as shown in the above code snippets and its outcome this ends up adding the regularization loss twice.

I noticed it while adding KLDivergenceRegularization from tensorflow_probability and then reproduced it using a simple example as shown above.

Please suggest if there is a workaround as I said earlier I have many models that I assemble.

Regards & thanks
Kapil





"
43994,How to use custom tensorflow op for CUDA?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.3
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1 7.6.5
- GPU model and memory: GTX 1060 6G

**Describe the problem**
Recently I am researching deformable convolution, because i can't find a tf2.3 version to use, i use a tensorflow python api build, it is too slow. I want to learn how to use a cuda version to get better performance.
thanks for advice.
"
43993,How to print the tensor values in tensorflow 2.x version?,"I have a small issue, in tensorflow 2.x version, we don't need session to run the graph.

Currently I have a tensor object, calculated by tf.image.ssim(), but I have no way to print the value in the tensor. 

Most of the tutorials I found is about keras models, but what is the way to print tensor values in simple tensor?

"
43992,Passing structs by value makes C API hard to wrap,"The API has a number of functions that pass `TF_Output` argument by value. For example:

```c
TF_CAPI_EXPORT extern int TF_GraphGetTensorNumDims(TF_Graph* graph, TF_Output output, TF_Status* status);
```

This creates problems with different calling conventions. In my case, I'm trying to call these functions from Go through the syscall API instead of cgo and have no idea how to pass the struct correctly. I realize that existing functions cannot be changed, but would suggest deprecating them and adding new ones that only use struct pointers."
43986,installing tflite-model-maker encounters errors in regards to tf-nightly==2.4.0.dev20200810,"**System information**
- OS Platform and Distribution (e.g., windows):
- TensorFlow installed from both source and binary: tried on both
- TensorFlow version:  source 2.2.0 , binary 2.0.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 3.2.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: N/A



**Describe the problem**
I am trying to pip install tflite-model-maker, (tried from the source too) but I keep getting this error:
ERROR: No matching distribution found for tf-nightly==2.4.0.dev2.4.0.dev20201012
It seems that there is no version of tf-nightly 2.4.0.dev2.4.0.dev20201012
I tried to install another version of tf-nightly but that also encounters other error

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_internal\cli\base_command.py"", line 228, in _main
    status = self.run(options, args)
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_internal\cli\req_command.py"", line 182, in wrapper
    return func(self, options, args)
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_internal\commands\install.py"", line 324, in run
    reqs, check_supported_wheels=not options.target_dir
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_internal\resolution\legacy\resolver.py"", line 183, in resolve
    discovered_reqs.extend(self._resolve_one(requirement_set, req))
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_internal\resolution\legacy\resolver.py"", line 388, in _resolve_one
    abstract_dist = self._get_abstract_dist_for(req_to_install)
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_internal\resolution\legacy\resolver.py"", line 340, in _get_abstract_dist_for
    abstract_dist = self.preparer.prepare_linked_requirement(req)
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_internal\operations\prepare.py"", line 469, in prepare_linked_requirement
    hashes=self._get_linked_req_hashes(req)
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_internal\operations\prepare.py"", line 259, in unpack_url
    hashes=hashes,
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_internal\operations\prepare.py"", line 130, in get_http_url
    link, downloader, temp_dir.path, hashes
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_internal\operations\prepare.py"", line 282, in _download_http_url
    for chunk in download.chunks:
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_internal\cli\progress_bars.py"", line 168, in iter
    for x in it:
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_internal\network\utils.py"", line 88, in response_chunks
    decode_content=False,
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_vendor\urllib3\response.py"", line 576, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_vendor\urllib3\response.py"", line 541, in read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
  File ""c:\users\chingool\anaconda3\lib\contextlib.py"", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File ""c:\users\chingool\anaconda3\lib\site-packages\pip\_vendor\urllib3\response.py"", line 442, in _error_catcher
    raise ReadTimeoutError(self._pool, None, ""Read timed out."")
pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.

I feel like I will never be able to install tflite-model-maker, though it looks to be very useful.
Is there any other alternative to that?
"
43985,"TF to TF Lite Int 8 resulting in error: ""Quantization not yet supported for op: %""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes (a mix)**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- TensorFlow installed from (source or binary): **Source**
- TensorFlow version (use command below): **2.3**
- Python version: **3**

**Describe the current behavior**
Conversion to TF Lite Float 16 works and model runs well
Conversion to TF Lite Int 8 does NOT convert 

**Describe the expected behavior**
Conversion to TF Lite Int 8 works

**Standalone code to reproduce the issue**

`    
# TFLite model export
    try:
        print('\nStarting TFLite export with TensorFlow %s...' % tf.__version__)
        if opt.no_tfl_detect:
            print(""Don't export Detect module"")
            m.training = True
            keras_model = keras.Model(inputs=inputs, outputs=tf_model.predict(inputs))

        # fp16 TFLite model export
        converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float16]
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
        converter.allow_custom_ops = False
        converter.experimental_new_converter = True
        tflite_model = converter.convert()
        f = opt.weights.replace('.pt', '.tflite')  # filename
        open(f, ""wb"").write(tflite_model)
        print('\nTFLite export success, saved as %s' % f)

        # int8 TFLite model export
        if opt.tfl_int8:

            dataset = LoadImages(opt.source, img_size=opt.img_size, auto=False)
                
            def representative_data_gen():
                n = 0
                for path, img, im0s, vid_cap in dataset:
                    # Get sample input data as a numpy array in a method of your choosing.
                    n += 1
                    input = np.transpose(img, [1, 2, 0])
                    input = np.expand_dims(input, axis=0).astype(np.float32)
                    input /= 255.0
                    yield [input]
                    if n >= opt.ncalib:
                        break

            converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
            # This enables quantization
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            # This sets the representative dataset for quantization
            converter.representative_dataset = representative_data_gen
            # This ensures that if any ops can't be quantized, the converter throws an error
            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
            # For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.
            converter.target_spec.supported_types = [tf.int8]
            # These set the input and output tensors to uint8 (added in r2.3)
            converter.inference_input_type = tf.uint8
            converter.inference_output_type = tf.uint8
            tflite_model = converter.convert()

            with open('mobilenet_v2_1.0_224_quant.tflite', 'wb') as f:
              f.write(tflite_model)

`

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
[logs.pdf](https://github.com/tensorflow/tensorflow/files/5373243/logs.pdf)
"
43984,TF to TF Lite Int 8 resulting in error: ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43983,remote devices in cluster not visible in TF 2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.7
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3
- Python version: 3.6.12
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**
I am not able to simply put a tensor on a specific device in a TF cluster.

This example works for TF1.14:
```python
# try_worker_tensor_tf1.py
import json
import os
import time
import sys

import tensorflow as tf
from tensorflow.distribute.cluster_resolver import TFConfigClusterResolver

id_ = int(sys.argv[1])

os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': ['localhost:8081', 'localhost:8082']
    },
    'task': {
        'type': 'worker',
        'index': id_
    }
})
resolver = TFConfigClusterResolver()
cluster = resolver.cluster_spec()

with tf.device(""/job:worker/task:0""):
    v1 = tf.constant(1)

experimental_config = tf.ConfigProto.Experimental(
    share_cluster_devices_in_session=False,
    share_session_state_in_clusterspec_propagation=False)
config = tf.ConfigProto(experimental=experimental_config)
server = tf.train.Server(
    cluster, job_name=""worker"", task_index=id_, config=config)
sess = tf.Session(target=server.target, config=config)
print(sess.run(v1))
time.sleep(5)
```
```console
> python try_worker_tensor_tf1.py 0&
> python try_worker_tensor_tf1.py 1&
```
outputs
```console
1
1
```

But this example in TF2.3 does not work:
```python
import json
import os
import time
import sys

import tensorflow as tf

id_ = int(sys.argv[1])

os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': ['localhost:8081', 'localhost:8082']
    },
    'task': {
        'type': 'worker',
        'index': id_
    }
})
# Initializes tf server internally.
dist = tf.distribute.experimental.MultiWorkerMirroredStrategy()

print('Devices', tf.config.list_logical_devices())

with tf.device(""/job:worker/task:0""):
    v1 = tf.constant(1)

print(v1.numpy())
time.sleep(5)
```

```console
python try_worker_tensor_tf2.py 0
```
outputs:
```console
2020-10-13 16:56:49.658744: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> localhost:8081, 1 -> localhost:8082}
2020-10-13 16:56:49.658987: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://localhost:8081
Devices [LogicalDevice(name='/job:worker/replica:0/task:0/device:CPU:0', device_type='CPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:XLA_CPU:0', device_type='XLA_CPU')]
1
```
without blocking on the other server to start. In a separate terminal in parallel:
```console
python try_worker_tensor_tf2.py 1
```
outputs:
```console
2020-10-13 17:00:11.617132: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://localhost:8082
Devices [LogicalDevice(name='/job:worker/replica:0/task:1/device:CPU:0', device_type='CPU'), LogicalDevice(name='/job:worker/replica:0/task:1/device:XLA_CPU:0', device_type='XLA_CPU')]
Traceback (most recent call last):
  File ""try_worker_tensor_tf2.py"", line 25, in <module>
    v1 = tf.constant(1)
  File ""/Users/fgranqvist/opt/anaconda3/envs/py36-tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 264, in constant
    allow_broadcast=True)
  File ""/Users/fgranqvist/opt/anaconda3/envs/py36-tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 275, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File ""/Users/fgranqvist/opt/anaconda3/envs/py36-tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 300, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/Users/fgranqvist/opt/anaconda3/envs/py36-tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 98, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
tensorflow.python.framework.errors_impl.InvalidArgumentError: /job:worker/replica:0/task:0/device:CPU:0 unknown device.
```

**Describe the expected behavior**
I am expecting the TF2 example to also output `1` for both processes.

The reason why I am trying to do this is because I need to use `tf.queue.FIFOQueue` in the same way as `v1` above (as a distributed queue). One worker in the cluster enqueues items and all workers dequeues items and processes it."
43982,AttributeError: module 'tensorflow._api.v2.sets' has no attribute 'set_intersection',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Hello there,
I must admit that I am quite new to this, so please excuse me if I raise a trivial problem.
I have installed tensorflow and tried to implement Mask R-CNN just to make sure that everything runs smoothly before I try it on my own images. The programme and all necessary modules appears run without a problem until it is confronted by an error. I have copied all the commands and the error below. I would be appreciated if you could help.


**Provide the exact sequence of commands / steps that you executed before running into the problem**

import os
import sys
import random
import math
import numpy as np
import skimage.io
import matplotlib
import matplotlib.pyplot as plt

# Root directory of the project
ROOT_DIR = os.path.abspath(""../"")

import warnings
warnings.filterwarnings(""ignore"")

# Import Mask RCNN
sys.path.append(ROOT_DIR)  # To find local version of the library
from mrcnn import utils
import mrcnn.model as modellib
from mrcnn import visualize
# Import COCO config
sys.path.append(os.path.join(ROOT_DIR, ""samples/coco/""))  # To find local version
import coco

%matplotlib inline

# Directory to save logs and trained model
MODEL_DIR = os.path.join(ROOT_DIR, ""logs"")

# Local path to trained weights file
COCO_MODEL_PATH = os.path.join('', ""mask_rcnn_coco.h5"")

# Download COCO trained weights from Releases if needed
if not os.path.exists(COCO_MODEL_PATH):
    utils.download_trained_weights(COCO_MODEL_PATH)

# Directory of images to run detection on
IMAGE_DIR = os.path.join(ROOT_DIR, ""images"")

class InferenceConfig(coco.CocoConfig):
    # Set batch size to 1 since we'll be running inference on
    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1

config = InferenceConfig()
config.display()

# Create model object in inference mode.
model = modellib.MaskRCNN(mode=""inference"", model_dir='mask_rcnn_coco.hy', config=config)

# Load weights trained on MS-COCO
model.load_weights('mask_rcnn_coco.h5', by_name=True)

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-3-a2b43923c698> in <module>
     48 
     49 # Create model object in inference mode.
---> 50 model = modellib.MaskRCNN(mode=""inference"", model_dir='mask_rcnn_coco.hy', config=config)
     51 
     52 # Load weights trained on MS-COCO

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/mask_rcnn-2.1-py3.8.egg/mrcnn/model.py in __init__(self, mode, config, model_dir)
   1835         self.model_dir = model_dir
   1836         self.set_log_dir()
-> 1837         self.keras_model = self.build(mode=mode, config=config)
   1838 
   1839     def build(self, mode, config):

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/mask_rcnn-2.1-py3.8.egg/mrcnn/model.py in build(self, mode, config)
   2041             # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in
   2042             # normalized coordinates
-> 2043             detections = DetectionLayer(config, name=""mrcnn_detection"")(
   2044                 [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta])
   2045 

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/backend/tensorflow_backend.py in symbolic_fn_wrapper(*args, **kwargs)
     73         if _SYMBOLIC_SCOPE.value:
     74             with get_graph().as_default():
---> 75                 return func(*args, **kwargs)
     76         else:
     77             return func(*args, **kwargs)

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)
    487             # Actually call the layer,
    488             # collecting output(s), mask(s), and shape(s).
--> 489             output = self.call(inputs, **kwargs)
    490             output_mask = self.compute_mask(inputs, previous_mask)
    491 

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/mask_rcnn-2.1-py3.8.egg/mrcnn/model.py in call(self, inputs)
    808 
    809         # Run detection refinement graph on each item in the batch
--> 810         detections_batch = utils.batch_slice(
    811             [rois, mrcnn_class, mrcnn_bbox, window],
    812             lambda x, y, w, z: refine_detections_graph(x, y, w, z, self.config),

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/mask_rcnn-2.1-py3.8.egg/mrcnn/utils.py in batch_slice(inputs, graph_fn, batch_size, names)
    818     for i in range(batch_size):
    819         inputs_slice = [x[i] for x in inputs]
--> 820         output_slice = graph_fn(*inputs_slice)
    821         if not isinstance(output_slice, (tuple, list)):
    822             output_slice = [output_slice]

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/mask_rcnn-2.1-py3.8.egg/mrcnn/model.py in <lambda>(x, y, w, z)
    810         detections_batch = utils.batch_slice(
    811             [rois, mrcnn_class, mrcnn_bbox, window],
--> 812             lambda x, y, w, z: refine_detections_graph(x, y, w, z, self.config),
    813             self.config.IMAGES_PER_GPU)
    814 

/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/mask_rcnn-2.1-py3.8.egg/mrcnn/model.py in refine_detections_graph(rois, probs, deltas, window, config)
    718     if config.DETECTION_MIN_CONFIDENCE:
    719         conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]
--> 720         keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),
    721                                         tf.expand_dims(conf_keep, 0))
    722         keep = tf.sparse_tensor_to_dense(keep)[0]

AttributeError: module 'tensorflow._api.v2.sets' has no attribute 'set_intersection' 

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43979,ERROR: An error occurred during the fetch of repository 'local_config_cc':,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  windows 10 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):     source 
- TensorFlow version:   2.3
- Python version: 3.8.5
- Bazel version (if compiling from source):    3.6.0
- GCC/Compiler version (if compiling from source):   MSVC
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Repository rule cc_autoconf defined at:
  C:/users/10184590/_bazel_10184590/z5zmmi7q/external/bazel_tools/tools/cpp/cc_configure.bzl:143:30: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cc':
   Traceback (most recent call last):
        File ""C:/users/10184590/_bazel_10184590/z5zmmi7q/external/bazel_tools/tools/cpp/cc_configure.bzl"", line 120, column 36, in cc_autoconf_impl
                configure_windows_toolchain(repository_ctx)
        File ""C:/users/10184590/_bazel_10184590/z5zmmi7q/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 783, column 35, in configure_windows_toolchain
                msvc_vars_x64 = _get_msvc_vars(repository_ctx, paths, ""x64"")
        File ""C:/users/10184590/_bazel_10184590/z5zmmi7q/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 631, column 32, in _get_msvc_vars
                env = setup_vc_env_vars(repository_ctx, vc_path)
        File ""C:/users/10184590/_bazel_10184590/z5zmmi7q/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 387, column 24, in setup_vc_env_vars
                _check_env_vars(env_map, cmd, expected = envvars)
        File ""C:/users/10184590/_bazel_10184590/z5zmmi7q/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 393, column 32, in _check_env_vars
                auto_configure_fail(
        File ""C:/users/10184590/_bazel_10184590/z5zmmi7q/external/bazel_tools/tools/cpp/lib_cc_configure.bzl"", line 112, column 9, in auto_configure_fail
                fail(""\n%sAuto-Configuration Error:%s %s\n"" % (red, no_color, msg))
Error in fail:
Auto-Configuration Error: Setting up VC environment variables failed, WINDOWSSDKDIR is not set by the following command:
    ""C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Auxiliary\Build\VCVARSALL.BAT"" amd64  -vcvars_ver=14.27.29110
INFO: Repository icu instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule third_party_http_archive defined at:
  F:/git/tensorflow/tensorflow/third_party/repo.bzl:216:43: in <toplevel>
INFO: Repository 'icu' used the following cache hits instead of downloading the corresponding file.
 * Hash 'dfc62618aa4bd3ca14a3df548cd65fe393155edd213e49c39f3a30ccd618fc27' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/unicode-org/icu/archive/release-64-2.zip
If the definition of 'icu' was updated, verify that the hashes were also updated.
ERROR: F:/git/tensorflow/tensorflow/tensorflow/tools/build_info/BUILD:9:10: //tensorflow/tools/build_info:gen_build_info depends on @local_config_cc//:cc-compiler-x64_windows in repository @local_config_cc which failed to fetch. no such package '@local_config_cc//':
Auto-Configuration Error: Setting up VC environment variables failed, WINDOWSSDKDIR is not set by the following command:
    ""C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Auxiliary\Build\VCVARSALL.BAT"" amd64  -vcvars_ver=14.27.29110
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
INFO: Elapsed time: 31.340s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (201 packages loaded, 3847 targets configured)
    Fetching C:/users/10184590/_bazel_10184590/z5zmmi7q/external/icu; Extracting C:/users/10184590/_bazel_10184590/z5zmmi7q/external/icu/release-64-2.zip 24s
    Fetching @llvm-project; fetching 15s
    Fetching @boringssl; fetching 14s
    Fetching .../_bazel_10184590/z5zmmi7q/external/llvm-project; Extracting C:/users/10184590/_bazel_10184590/z5zmmi7q/external/llvm-project/a2291a58bf1c860d026581fee6fe96019dc25440.tar.gz 14s
    Fetching ...184590/_bazel_10184590/z5zmmi7q/external/boringssl; Extracting C:/users/10184590/_bazel_10184590/z5zmmi7q/external/boringssl/80ca9f9f6ece29ab132cce4cf807a9465a18cfac.tar.gz 14s


"
43978,Cannot save mixed precision model when using activity regularizer,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.1
- Python version: Python 3.6.9
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.1/7.6.5.32-1+cuda10.1
- GPU model and memory: RTX 2080Ti 11Gb

**Describe the current behavior**
When using activity_regularizer in a subclassed keras model, it is not possible to save the model if mixed_float16 policy is set.
If mixed_float16 is not set, the example below does work.

**Describe the expected behavior**
The example below should save model.

**Standalone code to reproduce the issue**

``` python:
import numpy as np

import tensorflow as tf
from tensorflow.keras.mixed_precision import experimental as mixed_precision


policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_policy(policy)


class TestModel(tf.keras.models.Model):
    def __init__(self, **kwargs):
        super(TestModel, self).__init__(**kwargs)
        self.conv_0 = tf.keras.layers.Conv2D(1, (1, 1), activity_regularizer='l1')
    
    def call(self, x):
        return self.conv_0(x)
    
    def get_config(self):
        return {}


model = TestModel()
_ = model(np.zeros((1, 16, 16, 3)))
model.save(""test_model"")
```

**Other info / logs** 
```
ValueError: Python inputs incompatible with input_signature:
  inputs: (
    Tensor(""StatefulPartitionedCall:0"", shape=(None, 16, 16, 1), dtype=float16))
  input_signature: (
    TensorSpec(shape=<unknown>, dtype=tf.float32, name=None))
```
"
43977,Method to get k smallest elements from a tensor,"[`tf.math.top_k`](https://www.tensorflow.org/api_docs/python/tf/math/top_k) only returns `k` largest elements from the input `tensor` along a specified `axis`. However, there is no direct way to obtain `k` smallest elements from a `tensor`. The equivalent method [`topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) in `pytorch` has an `bool` flag `largest` to toggle the output from `k` largest elements to `k` smallest elements and vice versa. It would be useful to have similar option  (or an alternative method like `bottom_k`) for this purpose so that hacky solution like the following can be avoided.

```python
tf.negative(tf.math.top_k(tf.negative(A)))
```
Finding `k` minimum values can be potentially very useful in many cases especially for writing custom `ops`. This does not seem to be too hard to implement or drastically change the API.

**System information**
- TensorFlow version: 2.3

Related discussion in [Stackoverflow](https://stackoverflow.com/questions/44548227/minimum-k-values-of-a-tensor)."
43976,run-time error,"internal/modules/cjs/loader.js:1122
  return process.dlopen(module, path.toNamespacedPath(filename));
"
43974,about RuntimeError: Use `_distributed_apply()` instead of `apply_gradients()` in a cross-replica context.,"the tf version is 1.14
when i use mirrored_strategy = tf.distribute.MirroredStrategy(), there are a error:
    self.opt_op = self.optimizer.minimize(self.loss)
  File ""miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 413, in minimize
    name=name)
  File ""miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 564, in apply_gradients
    raise RuntimeError(""Use `_distributed_apply()` instead of ""
RuntimeError: Use `_distributed_apply()` instead of `apply_gradients()` in a cross-replica context.

how can i fix it?"
43972,Error when bazel-building tensorflow from source,"**System information**
- OS Platform and Distribution : Ubuntu 18.04.5 LTS
- TensorFlow installed from: source
- TensorFlow version: TF 2.1
- Python version: Python 3.7.5
- Installed using: venv
- Bazel version (if compiling from source): bazel 3.1.0
- GCC/Compiler version (if compiling from source): gcc 7.5.0
- CUDA/cuDNN version: -
- GPU model and memory: - 


Dear all,
I am facing a problem when building tensorflow using bazel.
As a first remark, I have to say that I tried to install TF using pip; it worked but unfortunately my CPU does not support the AVX instruction set, so I couldn't really import TF after installation. I read here: [https://github.com/tensorflow/tensorflow/issues/30114](url) that I should build it from source, which is not working. Here's what I did and the error which prompts on the bash:

1) I git-cloned:

git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow

2) I run the configuration:

(venv) fabio@fabio-HP-620:~/tensorflow$ ./configure
You have bazel 3.1.0 installed.
Please specify the location of python. [Default is /home/fabio/venv/bin/python3]: 


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /home/fabio/venv/lib/python3.7/site-packages
  /home/fabio/root-6.20.00_builddir/lib
  /usr/local/lib/python3.7/dist-packages
  /home/fabio/fastjet-install/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished

3) I built:
bazel build //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=192
INFO: Reading rc options for 'build' from /home/fabio/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/fabio/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /home/fabio/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/fabio/venv/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/home/fabio/venv/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /home/fabio/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/fabio/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /home/fabio/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:linux in file /home/fabio/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/fabio/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Repository local_config_python instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule python_configure defined at:
  /home/fabio/tensorflow/third_party/py/python_configure.bzl:294:20: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_python':
   Traceback (most recent call last):
        File ""/home/fabio/tensorflow/third_party/py/python_configure.bzl"", line 267
                _create_local_python_repository(<1 more arguments>)
        File ""/home/fabio/tensorflow/third_party/py/python_configure.bzl"", line 214, in _create_local_python_repository
                _symlink_genrule_for_dir(<4 more arguments>)
        File ""/home/fabio/tensorflow/third_party/py/python_configure.bzl"", line 66, in _symlink_genrule_for_dir
                ""\n"".join(<1 more arguments>)
        File ""/home/fabio/tensorflow/third_party/py/python_configure.bzl"", line 66, in ""\n"".join
                read_dir(repository_ctx, <1 more arguments>)
        File ""/home/fabio/tensorflow/third_party/remote_config/common.bzl"", line 101, in read_dir
                execute(repository_ctx, <2 more arguments>)
        File ""/home/fabio/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
find: ‘/usr/include/python3.7m’: No such file or directory
INFO: Repository aws-c-event-stream instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule third_party_http_archive defined at:
  /home/fabio/tensorflow/third_party/repo.bzl:216:28: in <toplevel>
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Traceback (most recent call last):
        File ""/home/fabio/tensorflow/third_party/py/python_configure.bzl"", line 267
                _create_local_python_repository(<1 more arguments>)
        File ""/home/fabio/tensorflow/third_party/py/python_configure.bzl"", line 214, in _create_local_python_repository
                _symlink_genrule_for_dir(<4 more arguments>)
        File ""/home/fabio/tensorflow/third_party/py/python_configure.bzl"", line 66, in _symlink_genrule_for_dir
                ""\n"".join(<1 more arguments>)
        File ""/home/fabio/tensorflow/third_party/py/python_configure.bzl"", line 66, in ""\n"".join
                read_dir(repository_ctx, <1 more arguments>)
        File ""/home/fabio/tensorflow/third_party/remote_config/common.bzl"", line 101, in read_dir
                execute(repository_ctx, <2 more arguments>)
        File ""/home/fabio/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
find: ‘/usr/include/python3.7m’: No such file or directory
INFO: Elapsed time: 20.364s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (70 packages loaded, 37 targets configured)

I don't really know how to solve it. Thanks for your help, it's really appreciated.

Best,
Fabio."
43969,Memory leak with tf.data.Dataset.map,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18-04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nigthtly
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**
Test is failing if line with dataset.map is not commented.
It shows the garbage collection is not working when using even a simple function with tf.data.Dataset.map

```
import tensorflow as tf
from tensorflow.python.framework.test_util import assert_no_garbage_created

class TestStandardTrainer(tf.test.TestCase):
  @assert_no_garbage_created
  def test_dataset_map(self):
      data_tensor = {'x': tf.constant([1, 2, 3], dtype=tf.float32),
                     'y': tf.constant([4, 5, 6], dtype=tf.float32)}
      dataset = tf.data.Dataset.from_tensor_slices(data_tensor)

      # We split the data in 2
      dataset = dataset.map(lambda x: (x, x))

import unittest
unittest.main(argv=['first-arg-is-ignored'], exit=False)

```

**Describe the expected behavior**

Test should pass

**Standalone code to reproduce the issue**
See above.

**Other info / logs**
None
"
43968,Adam Optimizer not working in migration of custom model,"I'm performing a migration to my custom model from TensorFlow 1.x to TensorFlow 2.x, so parts of my custom model (Class Model) are wrapped in a function that decorated with tf.function so before I use session.run with feed_dict, but now I use a function that decorated with tf.function, the problem is that the accuracy is small and the loss is big, so I implement Adam optimizer inside the decorated function with tf.function (def new_function), but it shows error creating variable on non-first call, however, it runs when I declare Adam optimizer in forecast function(def forecast) where I call the Model to predict, but it doesn't affect the accuracy that should be increasing and the loss should be minimized, it just remains the same like without using adam optimizer but slower. Is something missing? or how should be the syntax written in the code and where should it be placed?

note : Adam are implemented with 3 lines of code with comment in def forecast, see the difference of speed with or without Adam, and why the accuracy is not increasing and loss is also still big.

https://colab.research.google.com/drive/1VpahdrELXkMk29670yxLksOJMoR7uHKa
this is the colab pushed file

kindly need help for this problem
"
43967,could tf.feature_column.shared_embeddings support sequence_categorical_column_with_* in the future?,"
"
43966,"AssertionError: Could not compute output Tensor(""ctc/Identity:0"", shape=(None, 1), dtype=float32)","- I'm trying out OCR projects using CRNN models. This error occurred when the program is executed to model.fit. I tried to use several methods to solve the problem, but nothing seemed to work. 
I am using 
Win10 OS, 
Pycharm IDE 2020-02, 
TF version 2.3.0. 
Keras version 2.4.3 
python version 3.7.7
and executed on my GPU-2080ti. 
How can i fix it? Many thanks.

- Due to the large amount of data, I only selected a small amount of data for the test. The details of the data are as follows.
**image type is: <class 'numpy.ndarray'>
label type is: <class 'numpy.ndarray'>
image shape is: (9, 32, 320)
label shape is: (9, 21713)
image dtype is: float64
label dtype is: float64**

- The error detail as below.
_________________________________________________________________
Model: ""model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
image_input (InputLayer)        [(None, 32, 320, 1)] 0                                            
__________________________________________________________________________________________________
block1_conv1 (Conv2D)           (None, 32, 320, 64)  640         image_input[0][0]                
__________________________________________________________________________________________________
block1_conv2 (Conv2D)           (None, 32, 320, 64)  36928       block1_conv1[0][0]               
__________________________________________________________________________________________________
block1_pool (MaxPooling2D)      (None, 16, 160, 64)  0           block1_conv2[0][0]               
__________________________________________________________________________________________________
block2_conv1 (Conv2D)           (None, 16, 160, 128) 73856       block1_pool[0][0]                
__________________________________________________________________________________________________
block2_conv2 (Conv2D)           (None, 16, 160, 128) 147584      block2_conv1[0][0]               
__________________________________________________________________________________________________
block2_pool (MaxPooling2D)      (None, 8, 80, 128)   0           block2_conv2[0][0]               
__________________________________________________________________________________________________
block3_conv1 (Conv2D)           (None, 8, 80, 256)   295168      block2_pool[0][0]                
__________________________________________________________________________________________________
block3_conv2 (Conv2D)           (None, 8, 80, 256)   590080      block3_conv1[0][0]               
__________________________________________________________________________________________________
block3_conv3 (Conv2D)           (None, 8, 80, 256)   590080      block3_conv2[0][0]               
__________________________________________________________________________________________________
block3_conv4 (Conv2D)           (None, 8, 80, 256)   590080      block3_conv3[0][0]               
__________________________________________________________________________________________________
block3_pool (MaxPooling2D)      (None, 4, 40, 256)   0           block3_conv4[0][0]               
__________________________________________________________________________________________________
block4_conv1 (Conv2D)           (None, 4, 40, 512)   1180160     block3_pool[0][0]                
__________________________________________________________________________________________________
block4_conv2 (Conv2D)           (None, 4, 40, 512)   2359808     block4_conv1[0][0]               
__________________________________________________________________________________________________
block4_conv3 (Conv2D)           (None, 4, 40, 512)   2359808     block4_conv2[0][0]               
__________________________________________________________________________________________________
block4_conv4 (Conv2D)           (None, 4, 40, 512)   2359808     block4_conv3[0][0]               
__________________________________________________________________________________________________
block4_pool (MaxPooling2D)      (None, 2, 20, 512)   0           block4_conv4[0][0]               
__________________________________________________________________________________________________
block5_conv1 (Conv2D)           (None, 2, 20, 512)   2359808     block4_pool[0][0]                
__________________________________________________________________________________________________
block5_conv2 (Conv2D)           (None, 2, 20, 512)   2359808     block5_conv1[0][0]               
__________________________________________________________________________________________________
block5_conv3 (Conv2D)           (None, 2, 20, 512)   2359808     block5_conv2[0][0]               
__________________________________________________________________________________________________
block5_conv4 (Conv2D)           (None, 2, 20, 512)   2359808     block5_conv3[0][0]               
__________________________________________________________________________________________________
block5_pool (MaxPooling2D)      (None, 1, 10, 512)   0           block5_conv4[0][0]               
__________________________________________________________________________________________________
permute (Permute)               (None, 10, 1, 512)   0           block5_pool[0][0]                
__________________________________________________________________________________________________
timedistrib (TimeDistributed)   (None, 10, 512)      0           permute[0][0]                    
__________________________________________________________________________________________________
bidirectional (Bidirectional)   (None, 10, 1024)     4198400     timedistrib[0][0]                
__________________________________________________________________________________________________
dense (Dense)                   (None, 10, 512)      524800      bidirectional[0][0]              
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 10, 1024)     4198400     dense[0][0]                      
__________________________________________________________________________________________________
orc_out (Dense)                 (None, 10, 21713)    22255825    bidirectional_1[0][0]            
__________________________________________________________________________________________________
the_labels (InputLayer)         [(None, None)]       0                                            
__________________________________________________________________________________________________
input_length (InputLayer)       [(None, 1)]          0                                            
__________________________________________________________________________________________________
label_length (InputLayer)       [(None, 1)]          0                                            
__________________________________________________________________________________________________
ctc (Lambda)                    (None, 1)            0           orc_out[0][0]                    
                                                                              the_labels[0][0]                 
                                                                              input_length[0][0]               
                                                                              label_length[0][0]               
==================================================================================================
Total params: 51,200,657
Trainable params: 51,200,657
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
Traceback (most recent call last):
  File ""C:/Users/Administrator/Desktop/Python代码/test4.py"", line 89, in <module>
    model_result = model.fit(train_images, train_labels, steps_per_epoch=10000, epochs=50, callbacks=callback_list, validation_data=(test_images, test_labels), validation_steps=50)
  File ""C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 848, in fit
    tmp_logs = train_function(iterator)
  File ""C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\eager\def_function.py"", line 580, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\eager\def_function.py"", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\eager\def_function.py"", line 506, in _initialize
    *args, **kwds))
  File ""C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\eager\function.py"", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\eager\function.py"", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\eager\function.py"", line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\eager\def_function.py"", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AssertionError: in user code:

    C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\keras\engine\training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\keras\engine\training.py:531 train_step  **
        y_pred = self(x, training=True)
    C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\keras\engine\base_layer.py:927 __call__
        outputs = call_fn(cast_inputs, *args, **kwargs)
    C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\keras\engine\network.py:719 call
        convert_kwargs_to_constants=base_layer_utils.call_context().saving)
    C:\Users\Administrator\Pythonproject\venv\lib\site-packages\tensorflow\python\keras\engine\network.py:899 _run_internal_graph
        assert str(id(x)) in tensor_dict, 'Could not compute output ' + str(x)

    AssertionError: Could not compute output Tensor(""ctc/Identity:0"", shape=(None, 1), dtype=float32)"
43965, Error in tflite model deployment on android：Cannot copy to a TensorFlowLite tensor with 270000 bytes from a Java Buffer with 1228800 bytes.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 20.04.1 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:OnePlus 5T
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):2.2.0
- Python version:3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


I downloaded the latest example project on github and used the object detection android project.
We have trained a detection model and want to transfer it to the object detection project.
Then when we run app, we encountered this error: java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (normalized_input_image_tensor) with 270000 bytes from a Java Buffer with 1228800 bytes.
The input of the original model is 300×300×3 int data, and the buffer length is 270000.
The input of our model is 320×320×3 float32 data, and the buffer length is 1228800.
We have tried our best to modify the parameters in the code, but still did not solve this problem.
Does anyone have the same problem? How do you solve similar problems."
43963,'tf.Conv2D' op is neither a custom op nor a flex op error while converting ONNX model to TF Lite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): Conda binary 2.2.0
- TensorFlow version (or github SHA if from source):


**Command used to run the converter or code if you’re using the Python API**
My goal is to run a model trained in PyTroch on Edge TPU. I am getting this issue with a custom as well as standard Mobilenet V2 model.

Here is Colab Link

https://colab.research.google.com/drive/1giZg-g7PkLkGWb3bbMV890IFVWf_Es9f?usp=sharing

Steps
1. Get Mobilenet V2 model from torchvision
2. Convert to ONNX
    PyTorch: 1.6.0
    ONNX: 1.7.0

```
import torch
import torchvision.models as models
import torchvision.transforms as transforms

mobilenet = models.mobilenet_v2()

normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])

dummy_input = torch.rand((3, 224, 224))
dummy_input = normalize(dummy_input)
dummy_input = torch.unsqueeze(dummy_input, 0)
out = mobilenet(dummy_input)

torch.onnx.export(mobilenet, dummy_input, ""pytorch_mnet.onnx"", verbose=True, input_names=['input'], output_names=['output'])
```
3. Convert ONNX model to TF

`onnx-tf convert -i ./pytorch_mnet.onnx -o onnx_to_tf_mnet`

4. Convert to TFLite

```
import tensorflow as tf
from tensorflow import keras
import numpy as np

quant = True
im_h = 224
im_w = 224
random_data = np.random.uniform(size=(100, im_h, im_w, 3))
random_data = random_data.astype(np.float32)

def representative_dataset_gen():
  for i in range(100):
    # Get sample input data as a numpy array in a method of your choosing.
    temp = random_data[i, :, :, :]
    temp_shape = np.shape(temp)
    temp = np.reshape(temp, (1, temp_shape[0], temp_shape[1], temp_shape[2]))
    yield [temp]

converter = tf.lite.TFLiteConverter.from_saved_model(""./onnx_to_tf_mnet"")
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True
if not quant:
    converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
    ]
else:
    converter.representative_dataset = representative_dataset_gen
    converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS_INT8
    ]
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8

tflite_model = converter.convert()
open(""./mnet_quant.tflite"", ""wb"").write(tflite_model)

```
This throws error,
`<unknown>:0: error: loc(fused[""convolution_7169@__inference___call___23399"", ""PartitionedCall/convolution_7169""]): 'tf.Conv2D' op is neither a custom op nor a flex op`

However, when I set quant=False and just get normal tflite model, it seems to work."
43962,Cannot use seq_len other than 128 with BertNLClassifier – Model Maker Text Classification example,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): openSUSE Leap 15.2 for training and Ubuntu 18.10 with Android Studio
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  Sony Xperia XZ1 compact (Lineage 16.0)
- TensorFlow installed from (source or binary): from binary (pip) 
- TensorFlow version (use command below): v1.12.1-38915-gfe968502a9 2.4.0-dev20200810
- Python version: 3.6.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.1.74
- GPU model and memory: Tesla V100-PCIE-32GB

I was trying out the [Text classification with TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/tutorials/model_maker_text_classification) Tutorial. I would like to fine-tune MobileBERT with `seq_len=512`, convert to tflite and run the tflite model on Android.

I can fine-tune MobileBERT with `seq_len=512` and convert to tflite just fine. When I try to use the tflite model on Android with the Java [`BertNLClassifier`](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_nl_classifier) and try to classify text, the app crashes. If I use a model fine-tuned with `seq_len=128`, it works just fine. In the tutorial, it says that 128 is the default sequence length, but that it can be adjusted. I tried `seq_len` = 16, 64, 256, 512 but they all produce the same error, as given below.

**Describe the current behavior**
1. Fine-tune MobileBERT using Model Maker with `seq_len` other than 128, e.g. with
```python
model_spec = model_spec.get('mobilebert_classifier')
model_spec.seq_len = 512
```
2. Convert the model to tflite
3. Run the following Java Code on Android:
```Java
Context context = getApplicationContext();
BertNLClassifier classifier = BertNLClassifier.createFromFile(context, ""model.tflite"");

Log.v(TAG, ""created classifier "" + classifier.toString());
// Run inference
List<Category> res = classifier.classify(""good""); /* produces an error */
Log.v(TAG, ""Successfully did an inference!"");
```
4. Run App
5. App crashes
```
2020-10-11 18:41:31.045 9441-9441/org.tensorflow.lite.examples.textclassification A/libc: Fatal signal 6 (SIGABRT), code -6 (SI_TKILL) in tid 9441 (tclassification), pid 9441 (tclassification)
```

**Describe the expected behavior**
Using `seq_len` other than 128 should not produce an error

**Standalone code to reproduce the issue**
- For the python fine-tuning code, see the [tutorial colab notebook](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb)
- Slightly modified `MainActivity.java` from the [text classification android app example](https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification/android):
 https://gist.github.com/matthias-vogt/74880d85e4a2f1e69fb9e45b893d534f

**Other info / logs**
<details>
<summary>
Full traceback
</summary>
<code><pre>10-12 19:30:03.424 23376 23376 F libc    : Fatal signal 6 (SIGABRT), code -6 (SI_TKILL) in tid 23376 (tclassification), pid 23376 (tclassification)
10-12 19:30:03.485 23419 23419 F DEBUG   : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
10-12 19:30:03.485 23419 23419 F DEBUG   : LineageOS Version: '16.0-20190813-UNOFFICIAL-lilac'
10-12 19:30:03.485 23419 23419 F DEBUG   : Build fingerprint: 'Sony/lilac/lilac:9/YOSHINO-2.2.0-190725-0908/1:user/dev-keys'
10-12 19:30:03.485 23419 23419 F DEBUG   : Revision: '0'
10-12 19:30:03.485 23419 23419 F DEBUG   : ABI: 'arm64'
10-12 19:30:03.485 23419 23419 F DEBUG   : pid: 23376, tid: 23376, name: tclassification  >>> org.tensorflow.lite.examples.textclassification <<<
10-12 19:30:03.485 23419 23419 F DEBUG   : signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------
10-12 19:30:03.485 23419 23419 F DEBUG   :     x0  0000000000000000  x1  0000000000005b50  x2  0000000000000006  x3  0000000000000008
10-12 19:30:03.485 23419 23419 F DEBUG   :     x4  00000000646f6f67  x5  00000000646f6f67  x6  00000000646f6f67  x7  00000000646f6f67
10-12 19:30:03.485 23419 23419 F DEBUG   :     x8  0000000000000083  x9  0000007c91bfbb40  x10 fffffff87ffffbdf  x11 0000000000000001
10-12 19:30:03.485 23419 23419 F DEBUG   :     x12 fefefefefefefeff  x13 1e1e1e1e1e1e1e1e  x14 0000000000000018  x15 0000000000000000
10-12 19:30:03.485 23419 23419 F DEBUG   :     x16 0000007c91c332a8  x17 0000007c91b71360  x18 0000007beb7d3600  x19 0000000000005b50
10-12 19:30:03.485 23419 23419 F DEBUG   :     x20 0000000000005b50  x21 0000000000000083  x22 0000007c07a6e400  x23 aaaaaaaaaaaaaaab
10-12 19:30:03.485 23419 23419 F DEBUG   :     x24 0000007c07a6e470  x25 0000007c117ffe00  x26 0000007c0955f600  x27 0000007c117ffe0c
10-12 19:30:03.485 23419 23419 F DEBUG   :     x28 0000000000000001  x29 0000007fd32858a0
10-12 19:30:03.485 23419 23419 F DEBUG   :     sp  0000007fd3285860  lr  0000007c91b65b18  pc  0000007c91b65b44
10-12 19:30:03.658 23419 23419 F DEBUG   : 
10-12 19:30:03.658 23419 23419 F DEBUG   : backtrace:
10-12 19:30:03.658 23419 23419 F DEBUG   :     #00 pc 0000000000021b44  /system/lib64/libc.so (abort+124)
10-12 19:30:03.658 23419 23419 F DEBUG   :     #01 pc 0000000000039848  /data/app/org.tensorflow.lite.examples.textclassification-MOts0RndnPfbNq09b2Opyg==/lib/arm64/libtask_text_jni.so
10-12 19:30:03.658 23419 23419 F DEBUG   :     #02 pc 0000000000073ea8  /data/app/org.tensorflow.lite.examples.textclassification-MOts0RndnPfbNq09b2Opyg==/lib/arm64/libtask_text_jni.so
10-12 19:30:03.658 23419 23419 F DEBUG   :     #03 pc 0000000000073d38  /data/app/org.tensorflow.lite.examples.textclassification-MOts0RndnPfbNq09b2Opyg==/lib/arm64/libtask_text_jni.so
10-12 19:30:03.658 23419 23419 F DEBUG   :     #04 pc 00000000000733d4  /data/app/org.tensorflow.lite.examples.textclassification-MOts0RndnPfbNq09b2Opyg==/lib/arm64/libtask_text_jni.so
10-12 19:30:03.658 23419 23419 F DEBUG   :     #05 pc 0000000000565be0  /system/lib64/libart.so (art_quick_generic_jni_trampoline+144)
10-12 19:30:03.658 23419 23419 F DEBUG   :     #06 pc 000000000055ce4c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)
10-12 19:30:03.658 23419 23419 F DEBUG   :     #07 pc 00000000000cf760  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+232)
10-12 19:30:03.658 23419 23419 F DEBUG   :     #08 pc 00000000002823f0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)
10-12 19:30:03.658 23419 23419 F DEBUG   :     #09 pc 000000000027c3ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+948)
10-12 19:30:03.658 23419 23419 F DEBUG   :     #10 pc 000000000052d95c  /system/lib64/libart.so (MterpInvokeStatic+204)
10-12 19:30:03.658 23419 23419 F DEBUG   :     #11 pc 000000000054f294  /system/lib64/libart.so (ExecuteMterpImpl+14612)
10-12 19:30:03.658 23419 23419 F DEBUG   :     #12 pc 0000000000163e18  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/org.tensorflow.lite.examples.textclassification-MOts0RndnPfbNq09b2Opyg==/base.apk (deleted) (org.tensorflow.lite.task.text.nlclassifier.BertNLClassifier.classify+8)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #13 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #14 pc 000000000025ba28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #15 pc 000000000027c390  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #16 pc 000000000052c458  /system/lib64/libart.so (MterpInvokeVirtual+584)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #17 pc 000000000054f114  /system/lib64/libart.so (ExecuteMterpImpl+14228)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #18 pc 0000000000144bd8  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/org.tensorflow.lite.examples.textclassification-MOts0RndnPfbNq09b2Opyg==/base.apk (deleted) (org.tensorflow.lite.examples.textclassification.MainActivity.onCreate+104)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #19 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #20 pc 000000000025ba28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #21 pc 000000000027c390  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #22 pc 000000000052c458  /system/lib64/libart.so (MterpInvokeVirtual+584)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #23 pc 000000000054f114  /system/lib64/libart.so (ExecuteMterpImpl+14228)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #24 pc 0000000000372386  /system/framework/boot-framework.vdex (android.app.Activity.performCreate+24)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #25 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #26 pc 000000000025ba28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #27 pc 000000000027c390  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #28 pc 000000000052f4a8  /system/lib64/libart.so (MterpInvokeVirtualQuick+584)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #29 pc 0000000000552e94  /system/lib64/libart.so (ExecuteMterpImpl+29972)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #30 pc 00000000004a1d50  /system/framework/boot-framework.vdex (android.app.Activity.performCreate+2)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #31 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #32 pc 000000000025ba28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #33 pc 000000000027c390  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #34 pc 000000000052c458  /system/lib64/libart.so (MterpInvokeVirtual+584)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #35 pc 000000000054f114  /system/lib64/libart.so (ExecuteMterpImpl+14228)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #36 pc 00000000003910d4  /system/framework/boot-framework.vdex (android.app.Instrumentation.callActivityOnCreate+6)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #37 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #38 pc 000000000025ba28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #39 pc 000000000027c390  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #40 pc 000000000052c458  /system/lib64/libart.so (MterpInvokeVirtual+584)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #41 pc 000000000054f114  /system/lib64/libart.so (ExecuteMterpImpl+14228)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #42 pc 0000000000369cb0  /system/framework/boot-framework.vdex (android.app.ActivityThread.performLaunchActivity+642)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #43 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #44 pc 000000000025ba28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #45 pc 000000000027c390  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #46 pc 000000000052d798  /system/lib64/libart.so (MterpInvokeDirect+296)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #47 pc 000000000054f214  /system/lib64/libart.so (ExecuteMterpImpl+14484)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #48 pc 00000000003699a8  /system/framework/boot-framework.vdex (android.app.ActivityThread.handleLaunchActivity+72)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #49 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #50 pc 000000000025ba28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #51 pc 000000000027c390  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #52 pc 000000000052c458  /system/lib64/libart.so (MterpInvokeVirtual+584)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #53 pc 000000000054f114  /system/lib64/libart.so (ExecuteMterpImpl+14228)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #54 pc 00000000003af078  /system/framework/boot-framework.vdex (android.app.servertransaction.LaunchActivityItem.execute+112)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #55 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #56 pc 000000000025ba28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #57 pc 000000000027c390  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #58 pc 000000000052c458  /system/lib64/libart.so (MterpInvokeVirtual+584)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #59 pc 000000000054f114  /system/lib64/libart.so (ExecuteMterpImpl+14228)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #60 pc 00000000003afce8  /system/framework/boot-framework.vdex (android.app.servertransaction.TransactionExecutor.executeCallbacks+196)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #61 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #62 pc 000000000025ba28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #63 pc 000000000027c390  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #64 pc 000000000052c458  /system/lib64/libart.so (MterpInvokeVirtual+584)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #65 pc 000000000054f114  /system/lib64/libart.so (ExecuteMterpImpl+14228)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #66 pc 00000000003afbfe  /system/framework/boot-framework.vdex (android.app.servertransaction.TransactionExecutor.execute+68)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #67 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #68 pc 000000000025ba28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #69 pc 000000000027c390  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #70 pc 000000000052c458  /system/lib64/libart.so (MterpInvokeVirtual+584)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #71 pc 000000000054f114  /system/lib64/libart.so (ExecuteMterpImpl+14228)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #72 pc 000000000036917e  /system/framework/boot-framework.vdex (android.app.ActivityThread$H.handleMessage+78)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #73 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #74 pc 000000000025ba28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #75 pc 000000000027c390  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #76 pc 000000000052c458  /system/lib64/libart.so (MterpInvokeVirtual+584)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #77 pc 000000000054f114  /system/lib64/libart.so (ExecuteMterpImpl+14228)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #78 pc 0000000000a25b9a  /system/framework/boot-framework.vdex (android.os.Handler.dispatchMessage+42)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #79 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #80 pc 000000000025ba28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #81 pc 000000000027c390  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #82 pc 000000000052c458  /system/lib64/libart.so (MterpInvokeVirtual+584)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #83 pc 000000000054f114  /system/lib64/libart.so (ExecuteMterpImpl+14228)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #84 pc 0000000000a2cb42  /system/framework/boot-framework.vdex (android.os.Looper.loop+414)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #85 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #86 pc 000000000025ba28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #87 pc 000000000027c390  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #88 pc 000000000052d95c  /system/lib64/libart.so (MterpInvokeStatic+204)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #89 pc 000000000054f294  /system/lib64/libart.so (ExecuteMterpImpl+14612)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #90 pc 000000000036e9a4  /system/framework/boot-framework.vdex (android.app.ActivityThread.main+216)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #91 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #92 pc 000000000051cb98  /system/lib64/libart.so (artQuickToInterpreterBridge+1032)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #93 pc 0000000000565cfc  /system/lib64/libart.so (art_quick_to_interpreter_bridge+92)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #94 pc 000000000055ce4c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #95 pc 00000000000cf760  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+232)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #96 pc 00000000004633f8  /system/lib64/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+104)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #97 pc 0000000000464e50  /system/lib64/libart.so (art::InvokeMethod(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jobject*, _jobject*, unsigned long)+1440)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #98 pc 00000000003f43f0  /system/lib64/libart.so (art::Method_invoke(_JNIEnv*, _jobject*, _jobject*, _jobjectArray*)+48)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #99 pc 00000000001176d4  /system/framework/arm64/boot.oat (offset 0x10d000) (java.lang.Class.getDeclaredMethodInternal [DEDUPED]+180)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #100 pc 000000000055cb88  /system/lib64/libart.so (art_quick_invoke_stub+584)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #101 pc 00000000000cf740  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+200)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #102 pc 00000000002823f0  /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #103 pc 000000000027c3ac  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+948)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #104 pc 000000000052c458  /system/lib64/libart.so (MterpInvokeVirtual+584)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #105 pc 000000000054f114  /system/lib64/libart.so (ExecuteMterpImpl+14228)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #106 pc 0000000000b4650c  /system/framework/boot-framework.vdex (com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run+22)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #107 pc 0000000000255ea8  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1271626068+496)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #108 pc 000000000051cb98  /system/lib64/libart.so (artQuickToInterpreterBridge+1032)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #109 pc 0000000000565cfc  /system/lib64/libart.so (art_quick_to_interpreter_bridge+92)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #110 pc 0000000000bcb274  /system/framework/arm64/boot-framework.oat (offset 0x39f000) (com.android.internal.os.ZygoteInit.main+3092)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #111 pc 000000000055ce4c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #112 pc 00000000000cf760  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+232)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #113 pc 00000000004633f8  /system/lib64/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+104)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #114 pc 0000000000463050  /system/lib64/libart.so (art::InvokeWithVarArgs(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, std::__va_list)+416)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #115 pc 0000000000366894  /system/lib64/libart.so (art::JNI::CallStaticVoidMethodV(_JNIEnv*, _jclass*, _jmethodID*, std::__va_list)+644)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #116 pc 00000000000b20b0  /system/lib64/libandroid_runtime.so (_JNIEnv::CallStaticVoidMethod(_jclass*, _jmethodID*, ...)+120)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #117 pc 00000000000b4a30  /system/lib64/libandroid_runtime.so (android::AndroidRuntime::start(char const*, android::Vector<android::String8> const&, bool)+760)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #118 pc 00000000000021a0  /system/bin/app_process64 (main+1200)
10-12 19:30:03.659 23419 23419 F DEBUG   :     #119 pc 00000000000af8d0  /system/lib64/libc.so (__libc_init+88)
</pre></code>
</details>

Thank you for your help!!!"
43961,Model Initialization take too much time,"**System information**
- OS Platform and Distribution : Linux Ubuntu 20.04 LTS
- TensorFlow installed from : Conda
- TensorFlow version : 2.2 GPU
- Python version: 3.7.8
- CUDA/cuDNN version: 10.1/7.6.5
- GPU model and memory: Geforce 940M & 4GB

I implemented simple feed forward NN for classfication task. which consists 5 dense layer. model weight size is 11.2 mb, but model initialization took almost 3 minutes. But training the model ( 50 epochs ) took almost 40 seconds. How can I fix this?
"
43960,the number of parameters of a GRU layer is different on keras and tensorflow 2.X,"calculating the number of parameters of a GRU layer : 
- nums = 3 * [ dh * (dh + dx) + dh ]
- 150 = 3 * [5*(5+4)+5]

why the total of GRU params in  tensorflow 2.X is different?

## Keras GRU

```py
from keras import Model, Input
from keras.layers import GRU

input = Input(shape=[10, 4])
gru = GRU(5)(input)
model = Model(input, gru)
model.summary()
```

```
Model: ""model_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)        [(None, 10, 4)]           0         
_________________________________________________________________
gru_1 (GRU)                 (None, 5)                 150       
=================================================================
Total params: 150
Trainable params: 150
Non-trainable params: 0
_________________________________________________________________
```

## Tensorflow2.X GRU
```py
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import GRU
 
input = Input(shape=[10, 4])
gru = GRU(5)(input)
model = Model(input, gru)
model.summary()
```

```
Model: ""model_2""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)        [(None, 10, 4)]           0         
_________________________________________________________________
gru_2 (GRU)                 (None, 5)                 165       
=================================================================
Total params: 165
Trainable params: 165
Non-trainable params: 0
_________________________________________________________________
```

> Modify on 2020/10/13 11:19 AM CST
"
43959,After converting to TFLite the network lost structure and return zero ,"### System information
-   **OS Platform and Distribution  - Linux Ubuntu 20.04 LTS, 64bit**:
-   **TensorFlow installed from conda**:
-   **TensorFlow version 2.2.0**:
-   **Python version 3.8.5**:
-   **GCC/Compiler version 9.3.0**:
-   **CUDA/cuDNN version 11.0**:
-   **GPU model and memory GTX 1080, 12GB**:


### Problem
I did a model training for my own data set. I used the model from the Tensorflow V2 Zoo repository: ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8.  

```
python model_main_tf2.py --model_dir=trening_demo/models/my_ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8 \
			 --pipeline_config_path=trening_demo/pre-trained-models/ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8/pipeline.config
```

I did training the model for the dataset and I exported the model using this command:
```
python export_tflite_graph_tf2.py --pipeline_config_path trening_demo/pre-trained-models/ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8/pipeline.config --trained_checkpoint_dir trening_demo/models/my_ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8/ --output_directory trening_demo/exported-models-tflite/ex_my_ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8
```
Then I run the detector for the saved model and the results are satisfactory.  Then I wanted to convert the saved model to TFLite.
I use:
```
import tensorflow as tf

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory
converter.optimizations = [lite.Optimize.DEFAULT]
tflite_model = converter.convert().

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```
Then I run the detector for tflite_model and the results are zeros. This line of code return zero:
```
# Retrieve detection results
boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects
classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects
scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence of detected objects
#num = interpreter.get_tensor(output_details[3]['index'])[0]  # Total number of detected objects (inaccurate and not needed)
```
Error:
```
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-20-ca031052c9d2> in <module>()
     23     print(interpreter.get_tensor(output_details[3]['index']))
     24     # Retrieve detection results
---> 25     boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects
     26     classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects
     27     scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence of detected objects

IndexError: invalid index to scalar variable.
```



So, I compared the network structures with the Netron program. The structure of pre-trained model form tf model Zoo:
![p1](https://user-images.githubusercontent.com/28406311/95753600-24689100-0ca2-11eb-8ec1-4fd4336a8fd9.png)

The structure of the network after training:
![p2](https://user-images.githubusercontent.com/28406311/95753615-29c5db80-0ca2-11eb-9fdd-8cd8ce9c3de5.png)

The structuer after convert to TFLite
![p3](https://user-images.githubusercontent.com/28406311/95753751-57128980-0ca2-11eb-93e5-c06df4b9a6d4.png)


Why has the network been so reduced? How to solve problems?

Best,

"
43958,Enable profiling in release builds,"@tensorflow/micro

Profiler is turned on/off depending on if BUILD_TYPE is debug or any of the release builds.
It would be useful to be able to decouple profiling from the NDEBUG-define, so that it's possible to enable profiling for a release build. The reason for this is that the reference kernels runs slower in debug mode in release mode, so the measurements are not really representative 


One idea is to add a build type ""release_with_profiling"", which works in a similar way as ""release_with_logs""."
43956,Could not find any cudnn.h matching version '8.0.4' in any subdirectory,"**System information**
- OS Platform and Distribution: Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version:1.15.4
- Python version:3.7.9
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):0.26.1
- GCC/Compiler version (if compiling from source):8.4.0
- CUDA/cuDNN version:CUDA:10.2 cuDNN:8.0.4
- GPU model and memory:Tesla P100



**Describe the problem**
I've installed cuDNN 8.0.4 in `/usr/local/cuda10-2`, and` libcudnn.so.8 libcudnn.so.8.0.4 `can be found in` /usr/local/cuda-10.2/lib64`. But after running `. /configure` command, I get the error ""Could not find any cudnn.h matching version '8.0.4' in any subdirectory"", and I don't know how to handle this error.


**Any other info / logs**
Terminal output:
```
**(base) atr1506@atr1506-PR1763GYW:~/tensorflow$ ./configure
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.26.1 installed.
Please specify the location of python. [Default is /home/atr1506/anaconda3/bin/python]:


Found possible Python library paths:
  /home/atr1506/anaconda3/lib/python3.8/site-packages
Please input the desired Python library path to use.  Default is [/home/atr1506/anaconda3/lib/python3.8/site-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]:
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]:
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: y
TensorRT support will be enabled for TensorFlow.

Could not find any NvInferVersion.h matching version '' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
of:
        '/lib'
        '/lib/x86_64-linux-gnu'
        '/usr'
        '/usr/lib/x86_64-linux-gnu/libfakeroot'
        '/usr/local/cuda'
Asking for detailed CUDA configuration...

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.2]:


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 8]: 8.0.4


Please specify the TensorRT version you want to use. [Leave empty to  default to TensorRT 5]:


Please specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]:


Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]:


Could not find any cudnn.h matching version '8.0.4' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
of:
        '/lib'
        '/lib/x86_64-linux-gnu'
        '/usr'
        '/usr/lib/x86_64-linux-gnu/libfakeroot'
        '/usr/local/cuda'
        '/usr/local/cuda-10.2'
Asking for detailed CUDA configuration...

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.2]:
```
"
43955,How to build tflite C++ library for ios from tf 2.3 onwards?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos 10.15.3 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.3.1
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: NA
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): Apple clang version 11.0.0 (clang-1100.0.33.17)
- CUDA/cuDNN version: NA
- GPU model and memory: NA


My workflow involved building tflite C++ library and using it on my app.

Steps that I used to take earlier:
```
> cd $TENSORFLOW_ROOT/tensorflow/lite/tools/make/
> ./build_ios_universal_lib.sh
```
This gives me a fat static library that I linked with my app. This was with tensorflow 2.0.0

Now I want to upgrade to tf 2.3.1, and when I use the same method, I get this warning:
```
=========================================================================
WARNING: This build script is deprecated and no longer maintained. Please
         refer to the iOS build guide to learn how to build the latest   
         version of TFLite static framework for iOS using bazel.         
         https://www.tensorflow.org/lite/guide/build_ios                 
=========================================================================
```
But on https://www.tensorflow.org/lite/guide/build_ios, there is no mention on how to build tflite C++ library. It only mentions about C library.

How do I continue to build tflite ios C++ library for future tf releases?
Or is this use-case deprecated, and I need to use C API instead?"
43954,tf.keras.models.model_load outputs random evaluation values ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11
- GPU model and memory: Nvidia Tesla v100 32 Gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


After loading a model the evaluation outputs random values:

```
model.evaluate(source_gen)
model.evaluate(target_gen)
```

> 2984/2984 [==============================] - 5s 2ms/step - loss: 0.1515 - accuracy: 0.9811
>4634/4634 [==============================] - 8s 2ms/step - loss: 1.4460 - accuracy: 0.7262

```
model.save_weights(""models/weights.h5"")
model.save(""models/test.h5"")

model_2 = tf.keras.models.load_model(""models/test.h5"")
model_2.load_weights(""models/weights.h5"")
model_2.evaluate(source_gen)
model_2.evaluate(target_gen)
```
>2984/2984 [==============================] - 5s 2ms/step - loss: 0.1515 - accuracy: 0.2070
>4634/4634 [==============================] - 8s 2ms/step - loss: 1.4460 - accuracy: 0.2473

also tried:

```
model.save(""models/test.h5"")
model_2 = tf.keras.models.load_model(""models/test.h5"")
model_2.evaluate(source_gen)
model_2.evaluate(target_gen)
```
>2984/2984 [==============================] - 5s 2ms/step - loss: 0.1515 - accuracy: 0.2070
>4634/4634 [==============================] - 8s 2ms/step - loss: 1.4460 - accuracy: 0.2473"
43953,"When compile keras model with multi losses, regularizers are not added properly","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
2.3.0
- Python version:
3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
10.1/7
- GPU model and memory:
rtx2080ti, 11G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


I have a piece code like below:
```
model.compile(
        optimizer=Adam(lr),
        loss={
            'loss1': loss1
            'loss2': loss2
        },
    )
...
model.fit(
        train_dataset,
        steps_per_epoch=train_steps,
        validation_data=val_dataset,
        validation_steps=val_steps,
        epochs=epochs,
        initial_epoch=initial_epoch,
        callbacks=callbacks,
        verbose=verbose,
        workers=0
    )
```
My model had only one loss before but recently I modified cus I need to do two tasks simultaneously. When it targeted one loss, during training, i can see the outputted loss to be much larger than the metrics (metrics is the same as loss) cus i added l2 regularizer on the model. However when i added the two losses together, the outputted loss becomes the sum of the two losses outputted. it seems my regularizers are gone while training with multi-loss. Is this behavior intended or this is a bug for multi-loss training? Thanks. "
43952,PyCharms: Unresolved reference 'config'  for some of the TensorFlow imports. For example: from tensorflow.python.autograph.core import config from tensorflow.python.autograph.core import converter from tensorflow.python.autograph.impl import api,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43948,tf.function and PerReplica tf.Variable does not work together.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Ubuntu 16.04 
- TensorFlow installed from (source or binary): binary from pip
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.7.9
- GPU model and memory: 2080Ti

**Describe the current behavior**
```python
import tensorflow as tf
from tensorflow.python.distribute import values as value_lib

strategy = tf.distribute.MirroredStrategy()

shape = (3, 3)

def create_value(ctx):
    return tf.Variable(tf.random.normal(shape=shape))
    # return tf.random.normal(shape=shape)

v = strategy.experimental_distribute_values_from_function(create_value)
assert isinstance(v, value_lib.PerReplica)

@tf.function
def my_print(v):
    tf.print(v)

strategy.run(my_print, args=(v,))
```
results:
```
Traceback (most recent call last):
  File ""4_tape_and_distribute/issue_report.py"", line 19, in <module>
    strategy.run(my_print, args=(v,))
  File ""/home/cloudhan/miniconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 1211, in run
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""/home/cloudhan/miniconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 2585, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""/home/cloudhan/miniconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 585, in _call_for_each_replica
    self._container_strategy(), fn, args, kwargs)
  File ""/home/cloudhan/miniconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_run.py"", line 78, in call_for_each_replica
    return wrapped(args, kwargs)
  File ""/home/cloudhan/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/home/cloudhan/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 846, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""/home/cloudhan/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File ""/home/cloudhan/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/cloudhan/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 550, in call
    ctx=ctx)
  File ""/home/cloudhan/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run __inference_my_print_79: Can't copy Tensor with type resource to device /job:localhost/replica:0/task:0/device:GPU:0. [Op:__inference_my_print_79]
```

**Describe the expected behavior**
The above code should work. Currently, the only way to make it work is that:

in `create_value`, I create Tensor instead of Variable.

or
 
comment out `@tf.function` decorator of `my_print`


**Standalone code to reproduce the issue**
Colab notebook for additional information:
https://colab.research.google.com/drive/1YfbT43chs_whz2iPQOCLzqLsreJQvV8D?usp=sharing
"
43947,CUDA 11.1 error on tf-nightly - libcusolver.so.10 not found,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): pip install tf-nightly
- TensorFlow version: 2.4.0-dev20201011
- Python version: 3.8.3 (default, May 14 2020, 23:52:17)
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 11.1 / 8.0.4.30
- GPU model and memory: RTX 2080 8GB Driver 455.23.05

**Describe the problem**

I'm trying to install tensorflow on a Linux machine with CUDA 11.1. I'm using tf-nightly, which supposedly supports CUDA 11 . It can find all libraries, except `libcusolver.so.10`

`Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
`

Any idea if CUDA 11.1 should work with nightly, or is this still not supported?

I also tried to manually install libcusolver.so.10 from CUDA 10.0, reload ldconfig cache, etc, but still didn't work; same error.

Thanks in advance!

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Sample code:

```python
import tensorflow as tf
import numpy as np
from tensorflow import keras
model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
```

**Any other info / logs**

Output:

```
2020-10-11 21:31:34.848630: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-10-11 21:31:34.849049: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2020-10-11 21:31:34.870960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-11 21:31:34.871193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.8GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s
2020-10-11 21:31:34.871206: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2020-10-11 21:31:34.872541: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2020-10-11 21:31:34.873082: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2020-10-11 21:31:34.873266: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2020-10-11 21:31:34.873367: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-10-11 21:31:34.873738: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2020-10-11 21:31:34.873855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2020-10-11 21:31:34.873863: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-10-11 21:31:34.874073: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-11 21:31:34.874568: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-10-11 21:31:34.874580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-11 21:31:34.874584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]
```


```
$ dir /usr/local/cuda/lib64/libcusolver*
lrwxrwxrwx 1 root root   19 Oct 11 19:31 lib64/libcusolverMg.so -> libcusolverMg.so.11
lrwxrwxrwx 1 root root   26 Oct 11 19:31 lib64/libcusolverMg.so.11 -> libcusolverMg.so.11.0.0.74
-rw-r--r-- 1 root root 383M Sep 16 13:57 lib64/libcusolverMg.so.11.0.0.74
lrwxrwxrwx 1 root root   17 Oct 11 19:31 lib64/libcusolver.so -> libcusolver.so.11
lrwxrwxrwx 1 root root   24 Oct 11 19:31 lib64/libcusolver.so.11 -> libcusolver.so.11.0.0.74
-rw-r--r-- 1 root root 664M Sep 16 13:57 lib64/libcusolver.so.11.0.0.74
-rw-r--r-- 1 root root 187M Sep 16 13:57 lib64/libcusolver_static.a
```

```
Sun Oct 11 21:39:27 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce RTX 2080    On   | 00000000:01:00.0  On |                  N/A |
|  0%   39C    P0    44W / 225W |    517MiB /  7979MiB |      2%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1920      G   /usr/lib/xorg/Xorg                292MiB |
|    0   N/A  N/A      2668      G   /usr/bin/gnome-shell              115MiB |
|    0   N/A  N/A      3067      G   ...gAAAAAAAAA --shared-files        7MiB |
|    0   N/A  N/A      3773      G   ...AAAAAAAAA= --shared-files       74MiB |
+-----------------------------------------------------------------------------+
```"
43946,Tensorflow Import Error: DLL load failed....,"
```

### Describe the problem
I installed tensorflow 2.3.1 (pip install) and get the following error when I try to import it. I'm using windows 10, 64 bit. Python 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]

code:
import tensorflow as tf
print(tf.__version__)

###Error  log:
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\anaconda3-new\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     63   try:
---> 64     from tensorflow.python._pywrap_tensorflow_internal import *
     65   # This try catch logic is because there is no bazel equivalent for py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-12-f83c6d50081b> in <module>
----> 1 import tensorflow as tf
      2 print(tf.__version__)

~\anaconda3-new\lib\site-packages\tensorflow\__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

~\anaconda3-new\lib\site-packages\tensorflow\python\__init__.py in <module>
     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
     39 
---> 40 from tensorflow.python.eager import context
     41 
     42 # pylint: enable=wildcard-import

~\anaconda3-new\lib\site-packages\tensorflow\python\eager\context.py in <module>
     33 from tensorflow.core.protobuf import config_pb2
     34 from tensorflow.core.protobuf import rewriter_config_pb2
---> 35 from tensorflow.python import pywrap_tfe
     36 from tensorflow.python import tf2
     37 from tensorflow.python.client import pywrap_tf_session

~\anaconda3-new\lib\site-packages\tensorflow\python\pywrap_tfe.py in <module>
     26 
     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import
---> 28 from tensorflow.python import pywrap_tensorflow
     29 from tensorflow.python._pywrap_tfe import *

~\anaconda3-new\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     81 for some common reasons and solutions.  Include the entire stack trace
     82 above this error message when asking for help."""""" % traceback.format_exc()
---> 83   raise ImportError(msg)
     84 
     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\Obichi\anaconda3-new\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.
"
43945,GPU support does not install libcublas.so.10 on TF 2.3,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

https://www.tensorflow.org/install/gpu

Using TF 2.3.1

## Description of issue (what needs changing):

Followed the instructions verbatim on Ubuntu 18.04 on Google Compute Engine.

Does not properly install all required libraries.  When running code, GPU fails to be used with the following warnings:

```
2020-10-11 21:16:44.435380: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-11 21:16:44.435770: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2020-10-11 21:16:44.437908: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-11 21:16:44.438497: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-11 21:16:44.440939: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-11 21:16:44.441954: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-11 21:16:44.448346: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-11 21:16:44.448376: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
```
"
43944,Keras: removing layers by creating new model not working,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Colab
- TensorFlow version (use command below):2.4.2
- Python version:3.8
- Bazel version (if compiling from source): Colab
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Google Colab
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
**Problem facing**
Unable to remove batch normalization layers from Keras DenseNet121 model
**Describe the expected behavior**
I am trying to remove batch normalization layer from DenseNet121 model by creating a new model and transferring layers, except Batch normalization layer, from DenseNet121 model to the new one. 
[code provided below]
**Describe the current behavior**
Error was ""ValueError: Input 0 of layer conv1/bn is incompatible with the layer: expected axis 3 of input shape to have value 64 but received input with shape [None, 512, 512, 3]""


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
https://colab.research.google.com/drive/1Efi-rVRZRzVWCJuPy06rm4doK_C2J2J_?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
//code
model1=Sequential()
for layer in model.layers:
    if(layer.name.find('bn')!=-1):
      model1.add(layer)
    
input_shape = (None,512,512,3)
model1.build(input_shape)



"
43943,Cannot load data from imdb_reviews datasets.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 2.2.0
- Python version: 3.8.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1, 7.6.5
- GPU model and memory: RTX2060, VRAM6GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When I run 
```python
import tensorflow as tf
import tensorflow_datasets as tfds

imdb, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)

```

the only output is 

```
Downloading and preparing dataset imdb_reviews (80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/0.1.0...
HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…
HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…
```
and loading takes forever to end(won't end).
Note: There are no errors or warnings shown, I just see these three lines, and the process never goes on.

The process succeeded once for some reason after many times of trials, and the output came with a warning. I forgot the contents but it ended with '...migrate to tensorflow_text'.
Of course this is the only time when I successfully got the loaded data so that's why I am here.

I tried to import data from keras.datasets and there seems no problems with it(was able to get the data imported with no warning and error).

I just want to know what this import problem is. Also I am wondering why there are no people facing the same problems.

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43942,Connecting to invalid output 1 of source node PartitionedCall_7 which has 1 outputs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): tf-gpu-1.15
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0,7.4
- GPU model and memory: Nvidia GTX 1080 Ti, 11GB


I am training a slightly complicated network (tf-gpu1.15,keras), and it was running fine till I used a new function. This function is a modification of an addon available for tf2 ([dense_image_warp](https://github.com/tensorflow/addons/blob/v0.11.2/tensorflow_addons/image/dense_image_warp.py)) and I am trying to get it working with Tf-gpu1.15. Everything seems to work fine except while training I get this Invalid argument error. The error occurs at `model.train_on_batch` line, and has something to do with Adam optimiser, so a wild guess is the problem is with optimisation/finding gradients part. Since everything was running fine earlier, I wonder if there is something in the addon function which doesn't work in tf1, but everything seems to be compatible.`PartitionedCall_7` seems to be one of the outputs of using the addon function, why is it invalid? The function returns a tensor. Any clue with what the error is trying to say will be of great help, I just cannot figure out what is causing this.

I will try to get a minimal code to reproduce the error as soon as possible, putting this here just in case some one has a clue.


Error:

    ---------------------------------------------------------------------------
    InvalidArgumentError                      Traceback (most recent call last)
    ~/anaconda3/envs/myenv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)
       1364     try:
    -> 1365       return fn(*args)
       1366     except errors.OpError as e:

    ~/anaconda3/envs/myenv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
       1347       # Ensure any changes to the graph are reflected in the runtime.
    -> 1348       self._extend_graph()
       1349       return self._call_tf_sessionrun(options, feed_dict, fetch_list,

    ~/anaconda3/envs/myenv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py in _extend_graph(self)
       1387     with self._graph._session_run_lock():  # pylint: disable=protected-access
    -> 1388       tf_session.ExtendSession(self._session)
       1389 

    InvalidArgumentError: Node 'training/Adam/gradients/gradients/PartitionedCall_7_grad/PartitionedCall': Connecting to invalid output 1 of source node PartitionedCall_7 which has 1 outputs.

    During handling of the above exception, another exception occurred:

    InvalidArgumentError                      Traceback (most recent call last)
   

    ~/anaconda3/envs/myenv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
       1015       self._update_sample_weight_modes(sample_weights=sample_weights)
       1016       self._make_train_function()
    -> 1017       outputs = self.train_function(ins)  # pylint: disable=not-callable
       1018 
       1019     if reset_metrics:

    ~/anaconda3/envs/myenv/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py in __call__(self, inputs)
       3439     inputs = nest.flatten(inputs, expand_composites=True)
       3440 
    -> 3441     session = get_session(inputs)
       3442     feed_arrays = []
       3443     array_vals = []

    ~/anaconda3/envs/myenv/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py in get_session(op_input_list)
        484   if not _MANUAL_VAR_INIT:
        485     with session.graph.as_default():
    --> 486       _initialize_variables(session)
        487   return session
        488 

    ~/anaconda3/envs/myenv/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py in _initialize_variables(session)
        901     # marked as initialized.
        902     is_initialized = session.run(
    --> 903         [variables_module.is_variable_initialized(v) for v in candidate_vars])
        904     uninitialized_vars = []
        905     for flag, v in zip(is_initialized, candidate_vars):

    ~/anaconda3/envs/myenv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
        954     try:
        955       result = self._run(None, fetches, feed_dict, options_ptr,
    --> 956                          run_metadata_ptr)
        957       if run_metadata:
        958         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

    ~/anaconda3/envs/myenv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
       1178     if final_fetches or final_targets or (handle and feed_dict_tensor):
       1179       results = self._do_run(handle, final_targets, final_fetches,
    -> 1180                              feed_dict_tensor, options, run_metadata)
       1181     else:
       1182       results = []

    ~/anaconda3/envs/myenv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
       1357     if handle is None:
       1358       return self._do_call(_run_fn, feeds, fetches, targets, options,
    -> 1359                            run_metadata)
       1360     else:
       1361       return self._do_call(_prun_fn, handle, feeds, fetches)

    ~/anaconda3/envs/myenv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)
       1382                     '\nsession_config.graph_options.rewrite_options.'
       1383                     'disable_meta_optimizer = True')
    -> 1384       raise type(e)(node_def, op, message)
       1385 
       1386   def _extend_graph(self):

    InvalidArgumentError: Node 'training/Adam/gradients/gradients/PartitionedCall_7_grad/PartitionedCall': Connecting to invalid output 1 of source node PartitionedCall_7 which has 1 outputs.

​
Same thing with SGD:

    InvalidArgumentError: Node 'training/SGD/gradients/gradients/PartitionedCall_7_grad/PartitionedCall': Connecting to invalid output 1 of source node PartitionedCall_7 which has 1 outputs."
43941,Masking scores before Softmax in Attention discussed here - https://www.tensorflow.org/tutorials/text/nmt_with_attention,"Hi - 

In the Attention mechanism here: [Neural Machine Translation](https://www.tensorflow.org/tutorials/text/nmt_with_attention) - see related code included below (from `class BahdanauAttention`), we did not mask the scores before applying softmax, so that we only sum the attention distribution to 1 for only the actual sequence length, just like it is discussed here: [masking scores before Softmax](https://github.com/juditacs/snippets/blob/master/deep_learning/masked_softmax.ipynb)

Is there a reason why we did not mask by setting previously padded elements to '-inf' so that their Softmax score would be 0?

Thanks!

`


    # score shape == (batch_size, max_length, 1)
    # we get 1 at the last axis because we are applying score to self.V
    # the shape of the tensor before applying self.V is (batch_size, max_length, units)
    score = self.V(tf.nn.tanh(
        self.W1(query_with_time_axis) + self.W2(values)))

    # attention_weights shape == (batch_size, max_length, 1)
    attention_weights = tf.nn.softmax(score, axis=1)

    # context_vector shape after sum == (batch_size, hidden_size)
    context_vector = attention_weights * values
    context_vector = tf.reduce_sum(context_vector, axis=1)

    return context_vector, attention_weights`
"
43939,support Quantization Aware Training for Conv2dTranspose->BathcNorm->Activation,"**System information**
- TensorFlow version (you are using): 2.3.1

**Describe the feature and the current behavior/state.**
the sequence ""Conv2dTranspose->BathcNorm->Activation"" is not defined in
tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_layout_transform.py

**Who will benefit with this feature?**
anyone with this sequence in the network

"
43938,[EfficientNetB7] validation not getting correct accuracy,"[Problem]: EfficientNetB7 gets very low val_accuracy (compared to efn.EfficientNetB7)
[System] : Kaggle.com with GPU on
[Tensorflow] : 2.3
[Model]: EfficientNetB7
[Dataset]: Food11

[Version 1](https://www.kaggle.com/arikuo/food11-classification?scriptVersionId=44424467). 
from tensorflow.keras.applications import EfficientNetB7
net = EfficientNetB7(input_shape=input_shape, weights='imagenet', include_top=False)
epochs = 20, accuracy = 92.49%, val_accuracy = **6.57%**

[Version 3](https://www.kaggle.com/arikuo/food11-classification?scriptVersionId=44436912) 
net = efn.EfficientNetB7(input_shape=input_shape, weights='imagenet', include_top=False)
epochs = 20, accuracy =91.87%, val_accuracy = **87.15%**
"
43937,please expose uniform_full_int on tf.random,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.2
- Are you willing to contribute it (Yes/No): no



**Describe the feature and the current behavior/state.**
This request is about adding `tf.random.uniform_full_int` similarly to how `tf.random.uniform` exists

it's currently not trivial to get the full range of ints using `tf.random.uniform` (specifically because maxval is expected to be of the int type but is exluded from the range)

**Will this change the current api? How?**
Yes, adding `tf.random.uniform_full_int` similarly to how `tf.random.uniform` exists

(note that there is `tf.random.Generator.uniform_full_int`, but this request applies to `tf.random`)"
43936,"In the custom model, keras.layers.Embedding Output Shape is multiple ","python 3.7.3
tensorflow 2.3.0
I want to use keras.layers.Embedding in a customized sub-model. But output shape is 'multiple'.Then I try to write a demo and test it

The results of the two methods are different, and I'm not sure if it's my problem.

model1 return me a clear output shape

but model2 give me a 'multiple'

here is the full demo code:
```python
import tensorflow as tf
import tensorflow.keras as keras
import numpy as np

myPath = ""./data/""
embedding_matrix = np.load(myPath + ""embeddings_matrix.npy"",allow_pickle=True)

model1=keras.Sequential([
    keras.layers.InputLayer(5),
    keras.layers.Embedding(len(embedding_matrix), 100,weights=[embedding_matrix],trainable=False,mask_zero=True,name=""embedding_layer""),
    keras.layers.Dense(200)
],name=""model1"")

model1.build((None,5))
model1.summary()

class myModel(keras.Model):
    def __init__(self,
                 embedding_matrix,
                 embedding_out=100,
                 **kwargs):
        super(myModel, self).__init__(name='model2')

        self.input_x = keras.layers.InputLayer(input_shape=5)
        self.embedding_layer = keras.layers.Embedding(len(embedding_matrix), embedding_out,weights=[embedding_matrix],trainable=False,mask_zero=True,name=""embedding"",input_length=5)
        self.dense = keras.layers.Dense(200)

    def call(self,x):
        x = self.input_x(x)
        y = self.embedding_layer(x)
        y = self.dense(y)
        return y

model2 = myModel(embedding_matrix)
model2.build((None,5))
model2.summary()
```
Here is the model.summary() info:
```
**************************************
Model: ""model1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding_layer (Embedding)  (None, 5, 100)            5000400
_________________________________________________________________
dense (Dense)                (None, 5, 200)            20200
=================================================================
Total params: 5,020,600
Trainable params: 20,200
Non-trainable params: 5,000,400
_________________________________________________________________
Model: ""model2""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_2 (InputLayer)         [(None, 5)]               0
_________________________________________________________________
embedding (Embedding)        multiple                  5000400
_________________________________________________________________
dense_1 (Dense)              multiple                  20200
=================================================================
Total params: 5,020,600
Trainable params: 20,200
Non-trainable params: 5,000,400
_________________________________________________________________
```"
43934,TFLite Interpreter Crashes when running with TF SELECT op,"Hi. I have a similar issue when I try to Invoke the interpreter. 
![image](https://user-images.githubusercontent.com/3530663/95645424-a9ba2e80-0a73-11eb-8dab-30eee3b6f4eb.png)

Tflite conversion is passed without error:

converter = tf.lite.TFLiteConverter.from_saved_model('saved_model/')
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
converter.allow_custom_ops=True
converter.experimental_new_converter =True
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
interpreter = tf.lite.Interpreter(model_path=""converted_model.tflite"")
interpreter.allocate_tensors()



**model definition:**

#define LSTM model with skip connection
def LSTM_net():
    N_CLASSES=4

    i = Input(shape=(79, 40), name='input')
    x = Masking()(i)
    x = LayerNormalization(name='layer_norm')(x)
    s = TimeDistributed(Dense(64, activation='tanh'),
                        name='td_dense_tanh')(x)
    x = Bidirectional(LSTM(128, return_sequences=True),
                             name='bidirectional_lstm')(s)
    x = concatenate([s, x], axis=2, name='skip_connection')
    x = Dense(64, activation='relu', name='dense_1_relu')(x)
    x = MaxPooling1D(name='max_pool_1d')(x)
    x = Dense(32, activation='relu', name='dense_2_relu')(x)
    x = Flatten(name='flatten')(x)
    x = Dropout(rate=0.5, name='dropout')(x)
    x = Dense(32, activation='relu',
                         activity_regularizer=regularizers.l2(0.001),
                         name='dense_3_relu')(x)
    o = Dense(N_CLASSES, activation='softmax', name='softmax')(x)

    model = Model(inputs=i, outputs=o, name='long_short_term_memory')

    return model

However I dont get any error when I use BatchNormalization instead of LayerNormalization. Not sure if that makes any difference.  But Model performance is not as expected with the batchnormalization. I need to use LayerNormalization for my project. Please help as soon as possible.

_Originally posted by @Thaslim in https://github.com/tensorflow/tensorflow/issues/38113#issuecomment-706484223_"
43933,"GPU support Software requirements links to incompatible CUDA Toolkit, cuDNN version","The various links provided by the https://www.tensorflow.org/install/gpu#software_requirements section leads to the newest version of CUDA Toolkit, cuDNN, whereas, Tensorflow  is only compatible with Toolkit 10.1 and cuDNN 7.6

This leads to a lot of errors when finding certain .dll files, GPU devices, and even running Models. 

If the links can be updated to point to the Archival download section ( to the last version, compatible with TF), it would save a lot of troubleshooting for new users who are setting up GPU support."
43931,Slow inference with libopencm3 when compared to Mbed ,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.10
- TensorFlow installed from: source
- Tensorflow version: 2.3.0, commit b36436b
- Target platform: Building for Nucleo-F767ZI dev board, comparing performance between Mbed and libopencm3


**Describe the problem**

Hello,

I have been having this problem for a while now, but only recently made enough progress so I can ask for help.

I'm trying to make TensorFlow Lite for Microcontrollers (TFMicro) library to work with libopencm3, which is a open-source firmware library for various ARM Cortex-M microcontrollers. 
When this is done it should be quick to run TensorFlow on any micro that libopencm3 supports.
Right now I'm extensively testing this on Nucleo-F767ZI dev board which has STM32F767ZI micro, with 2MB of flash and 1M of SRAM, 216 MHz.

TFMicro port is working as it should, I tested it with several different models, everthing compiles and I also get same outputs as compared to TFLite python interpreter.

However, on device inference is **very slow**. It takes about 1465 ms for one inference, with -O3 flag
I managed to get the same setup working with a generated Mbed project. With -O3 flag I get inference time of 486 ms, almost a second faster.

Mbed has a option to export a makefile of the project, that you usually need to compile with mbed command line tool.
I exported the makefile and with some small changes the project compiled and inference time was again 486 ms.

I hoped that with a exported mbed makefile and my makefile I could compare the differences and come down to the core fo the problem, but so far I have not managed to so.

So far I can tell that the problem is not in the tensorflow code or in the model, both setups use the same things. I also copied exported mbed makefile into my project and started changing out pieces. I managed get to a setup where I am using all compile flags from mbed makefile and a libopencm archive file, linker file and startup file, but inference is stilll around 1465 ms.

Both setups set the clock frequency of a micro to 216MHz, I am timing inference in exactly the same way, with a DWT counter which increments for every clock cycle, to get to milliseconds I do a bit of calculation that is the same in both cases.

Both setups also use cmsis-nn kernel implementations, i had to make manually sure of that.

For now I can not tell what exactly is the problem, but the things that are different are:
* linker script file
* linker flags
* startup routine

I can see that libopencm needs linker with -nostartfiles, -specs=nano.specs, -specs=nosys.specs, while mbed makefile just passes many -Wl,--wrap flags. Judging by the look of it it uses crt0.s for some startup work, but this is way over my head.

What can make microcontroller run slower, even though the clock is the same in both examples? Can a incorrect linker script slow down a micro, or are there some settings that make loading a model from flash slow? 

Any help or a suggested path how to continue wiht this problem would be extremly appreciated.

**Please provide the exact sequence of commands/steps when you ran into the problem**
Both setups are available on my github.

**Project with fast mbed example:**
https://github.com/MarkoSagadin/tensorflow_mbed_test 
To load the dependencies this code requires, run:
`mbed config root .`
`mbed deploy`

To compile this for any platform supported by mbed:
`mbed compile -m auto -t GCC_ARM -f --profile my_profile.json -f`

To compile this for Nucleo_F767ZI dev board you can run makefile:
`make flash -j4`

or with mbed:
`mbed compile -m NUCLEO_F767ZI -t GCC_ARM -f --profile my_profile.json -f`

Inspect serial output with `minicom -b 9600`

Location of main file:
https://github.com/MarkoSagadin/tensorflow_mbed_test/tree/master/tensorflow/lite/micro/examples/hello_world

Linker file that is used can be found here:
https://github.com/ARMmbed/mbed-os/blob/master/targets/TARGET_STM/TARGET_STM32F7/TARGET_STM32F767xI/TOOLCHAIN_GCC_ARM/STM32F767xI.ld

Startup file that is used can be found here:
https://github.com/ARMmbed/mbed-os/blob/master/targets/TARGET_STM/TARGET_STM32F7/TARGET_STM32F767xI/TOOLCHAIN_GCC_ARM/startup_stm32f767xx.S

**Project with slow libopencm3 example:**
https://github.com/MarkoSagadin/MicroML/tree/mbed_makefile
The setup for my project takes more time and more commands to setup, as im using tensorflow and libopencm as submodules. Please note that im using branch mbed_makefile to showcase my problem. To get everthing setup you should just copy commands below. 
```
git clone --recurse-submodules https://github.com/SkobecSlo/MicroML.git
git checkout mbed_makefile
cd tensorflow
sudo make -f tensorflow/lite/micro/tools/make/Makefile hello_world
cd ..
make -C libopencm3
mbed config root .
mbed deploy
```

To compile and flash to Nucelo board:
`make flash -j4`

Inspect serial output with `minicom -b 115200`

Location of main file and linker script that is used:
https://github.com/MarkoSagadin/MicroML/tree/mbed_makefile/projects/mbed_test
"
43929,Missing motivation for TF hacking in Pix2Pix Generative tutorial,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
The note from
https://www.tensorflow.org/tutorials/generative/pix2pix#generate_images

## Description of issue (what needs changing):

Under the section pointed to above there is a note addressing a hack in the code where training mode is used for a piece of code serving as ""visual validation"". To me, as a person learning from this tutorial, it is insufficient. Although it mitigates the uncertainty around unexpected measures by saying that this is done for the sake of certain shapes of statistics, it also cuts abruptly by saying ""and this is what we [don't] want"". This leaves me puzzled with why do we want such thing, the more so that further on we take just a single sample which means that the batch norm degenerates to instance norm. 

### Clear description

Can someone please add motivation to why training statistics are preferred in this case and perhaps relate to the issue of BN in the case of a single example?
"
43928,Using Orthogonal initializer in dense layer with backend set to fp16 gets stuck,"**System information**
- OS Platform and Distribution: Windows 10 64 bit
- TensorFlow installed from binary
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7.6
- CUDA/cuDNN version: 10.1 / 7.6.5
- GPU model and memory: 2060 super, 8GB


**Describe the current behavior**

Using Orthogonal kernel initializer in dense layer with backend set to fp16 execution gets stuck during initialization. CPU runs at 100% and nothing happens. Changing initializer to Zeros fixes this. The same Orthogonal initializer in convolutional layer works fine.

**Standalone code to reproduce the issue**

```
import tensorflow as tf
import numpy as np

class SNetwork(tf.keras.Model):
    def __init__(self):
        super(SNetwork, self).__init__()

        self.conv1 = tf.keras.layers.Conv2D(filters=32,
                                            kernel_size=(8, 8),
                                            strides=(4, 4),
                                            padding=""same"",
                                            input_shape=(96, 96, 3),
                                            kernel_initializer=tf.keras.initializers.Orthogonal(np.sqrt(2)),
                                            bias_initializer=tf.keras.initializers.Zeros(),
                                            activation=""linear"")

        self.flatten = tf.keras.layers.Flatten()

        self.a_dense = tf.keras.layers.Dense(512,
                                             kernel_initializer=tf.keras.initializers.Orthogonal(np.sqrt(2)),
                                             bias_initializer=tf.keras.initializers.Zeros(),
                                             activation=""relu"")

        self.a_out = tf.keras.layers.Dense(9,
                                           bias_initializer=tf.keras.initializers.Zeros(),
                                           activation=""softmax"")

    def call(self, inputs):
        x = self.conv1(inputs)
        x = tf.keras.layers.LeakyReLU()(x)
        x = self.flatten(x)
        x = self.a_dense(x)
        x = self.a_out(x)
        return x

tf.keras.backend.set_floatx(""float16"")
tmp = np.zeros((1, 96, 96, 3), dtype=""float16"")
net = SNetwork()
print(net(tmp))
```"
43927,Docker Image : cuda:10.0-cudnn7-devel-ubuntu16.04 -Training performance low - GPU Util% keep changing - Process Id section empty,"**System information**
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Docker Image : cuda:10.0-cudnn7-devel-ubuntu16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): tensorflow-gpu ==1.15.0
Python version: python 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.2
GPU model and memory: 4 GPU - Model : V100 Memory: 16160 MB

collected some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)

[tf_env.txt](https://github.com/tensorflow/tensorflow/files/5359214/tf_env.txt)

Environment:
Docker Image : cuda:10.0-cudnn7-devel-ubuntu16.04
Total GPUs : 4 nos Tesla V100 - GPU Memory 16.2 GB
CUDA: 10.2
Tensorflow-gpu - 1.15
keras: 2.1.3

**Describe the current behavior**

nvidia-smi shows as below - that is all GPUs Utilisation is above 90% for few seconds 

![gpu all above 90 %](https://user-images.githubusercontent.com/2168986/95658306-00d50780-0b37-11eb-833b-a45d9214e6db.png)

Then nvidia-smi window shows as follows, that is all GPUs Utilisation is 0% for few seconds
![Gpu 0%](https://user-images.githubusercontent.com/2168986/95658335-472a6680-0b37-11eb-80ee-a817391f3d58.png)


Then nvidia-smi window shows as follows, that is  Utilisation% is random in all GPUs Utilisation for few seconds
![gpu random %](https://user-images.githubusercontent.com/2168986/95658620-6c1fd900-0b39-11eb-9d21-c6a5e43b9afd.png)

I have noticed Training performance is low and taking longer duration in Docker container mentioned here. Same code is working fine in Tesla K80 2 GPUs with CUDA 10.0 on a Dedicated server as shown below.

![Screen Shot 2020-10-10 at 9 34 06 PM](https://user-images.githubusercontent.com/2168986/95659895-d177c800-0b41-11eb-8c72-91dc83ef8d8a.png)


But in Docker container, Why GPU utilization% is keep changing unusually and why process list section is empty? Why CUDA version is shown a 10.2 in the cuda:10.0-cudnn7-devel-ubuntu16.04 Image refer first three nvidia-smi schreen shots? I have not installed CUDA 10.2 toolkit and CuDNN libraries in the Image. How can i solve this issue?

ps aux command shows processids but nvidia-smi doesn’t show
"
43926,To compile the code need portpicker. What for? is it safe?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): no
- TensorFlow version: master branch
- Python version: 3.8.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): bazelisk
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11/8
- GPU model and memory: 1070 ti 8 Gb



**Describe the problem**
To compile the code,  need portpicker. What for? is it safe?

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43925,Avoiding data copy when using tf.py_func in Python,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): b'v1.13.0-rc2-5-g6612da8' 1.13.1
- Python version: 3.7.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla V100 32GB

**Describe the current behavior**
There may exists data copy from numpy array to tensorflow tensor when using ``tf.py_func``, and it costs extra time.

**Describe the expected behavior**
Avoid the data copy.

**Standalone code to reproduce the issue**
```python
import time
import tensorflow as tf
import numpy as np

arr = np.random.rand(100000000)


def get_arr():
    return arr


get = tf.py_func(get_arr, [], arr.dtype)

sess = tf.Session()

while 1:
    t = time.time()
    sess.run(get)
    print(time.time() - t)  # it outputs ~0.5s in my case
```

As it is mentioned [here](https://github.com/tensorflow/docs/blob/r1.13/site/en/api_docs/python/tf/py_func.md#args), 

> Important Note: Input and output numpy ndarrays of func are not guaranteed to be copies. In some cases their underlying memory will be shared with the corresponding TensorFlow tensors.

so in what cases do the numpy arrays share memory with tensors?

Or how to avoid the data copy?

Thanks for help!!
"
43924,Build from source wheel can not be used in another cpu,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version:1.14.1
- Python version:3.6.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):0.26.1
- GCC/Compiler version (if compiling from source):7.4.0
- CUDA/cuDNN version:10.2
- GPU model and memory:2080ti



**Describe the problem**
I build the wheel on one CPU(Xeon) and can not be used in another CPU(i7-8700). I've tried with AVX enabled and disabled, but all lead to the same problem. 
CPU used to build:
```
Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
CPU(s):              48
On-line CPU(s) list: 0-47
Thread(s) per core:  2
Core(s) per socket:  12
Socket(s):           2
NUMA node(s):        2
Vendor ID:           GenuineIntel
CPU family:          6
Model:               85
Model name:          Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz
Stepping:            7
CPU MHz:             1199.554
BogoMIPS:            5400.00
Virtualization:      VT-x
L1d cache:           32K
L1i cache:           32K
L2 cache:            1024K
L3 cache:            19712K
NUMA node0 CPU(s):   0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46
NUMA node1 CPU(s):   1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke avx512_vnni md_clear flush_l1d arch_capabilities
```
CPU which caused the problem:
```
Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
CPU(s):              12
On-line CPU(s) list: 0-11
Thread(s) per core:  2
Core(s) per socket:  6
Socket(s):           1
NUMA node(s):        1
Vendor ID:           GenuineIntel
CPU family:          6
Model:               158
Model name:          Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz
Stepping:            10
CPU MHz:             899.941
CPU max MHz:         4600.0000
CPU min MHz:         800.0000
BogoMIPS:            6399.96
Virtualization:      VT-x
L1d cache:           32K
L1i cache:           32K
L2 cache:            256K
L3 cache:            12288K
NUMA node0 CPU(s):   0-11
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d
```
when `import tensorflow` caused `Illegal instruction (core dumped)`  

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1.  Build tensorflow as instructed by https://www.tensorflow.org/install/source, with `--config=opt --config=cuda`  
2. pip install built wheel   
3. `python -q -X faulthandler -c ""import tensorflow as tf""`

**Any other info / logs**
```
Fatal Python error: Illegal instruction
Current thread 0x00007f598de0f740 (most recent call first):
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 922 in create_module
  File ""<frozen importlib._bootstrap>"", line 571 in module_from_spec
  File ""<frozen importlib._bootstrap>"", line 658 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 684 in _load
  File ""/opt/conda/lib/python3.6/imp.py"", line 343 in load_dynamic
  File ""/opt/conda/lib/python3.6/imp.py"", line 243 in load_module
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24 in swig_import_helper
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 678 in exec_module
  File ""<frozen importlib._bootstrap>"", line 665 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 955 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 971 in _find_and_load
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 678 in exec_module
  File ""<frozen importlib._bootstrap>"", line 665 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 955 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 971 in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1023 in _handle_fromlist
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 678 in exec_module
  File ""<frozen importlib._bootstrap>"", line 665 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 955 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 971 in _find_and_load
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow/__init__.py"", line 28 in <module>
  File ""<frozen importlib._bootstrap>"", line 219 in _call_with_frames_removed
  File ""<frozen importlib._bootstrap_external>"", line 678 in exec_module
  File ""<frozen importlib._bootstrap>"", line 665 in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 955 in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 971 in _find_and_load
  File ""<string>"", line 1 in <module>
Illegal instruction (core dumped)
```
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43919,Docker cuda:10.0-cudnn7-devel-ubuntu16.04 - 3 out of 4 GPUs not utilized,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Docker Image : cuda:10.0-cudnn7-devel-ubuntu16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary):  binary
- TensorFlow version (use command below): tensorflow-gpu ==1.15.0
- Python version: python 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.2
- GPU model and memory: 4 GPU - Model : V100 Memory: 16160 MB

Environment:
Docker Image : cuda:10.0-cudnn7-devel-ubuntu16.04
Total GPUs : 4 nos Tesla V100 - GPU Memory 16.2 GB
CUDA: 10.2
Tensorflow-gpu - 1.15
keras: 2.2.5

**Describe the current behavior**

I have started the training. Logs shows all 4 GPU devices are prepared and allocated. but nvidia-smi command shows as below

![3 GPUs is not Utilized at all](https://user-images.githubusercontent.com/2168986/95613566-c90c8800-0a82-11eb-9b45-348bb7a144a3.png)

Please refer the screenshot, One GPU is 63% utilization and other three GPUs are 0 % Utilization. But There is no process id list.

What is the issue here? Why three GPUs are not utilized and empty process list?

"
43917,tensorflow/core/grappler/optimizers/meta_optimizer.cc:581,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

More details on bug :  function_optimizer failed: Invalid argument: Input 0 of node zeros_like_270 was passed float from StatefulPartitionedCall/sequential/lstm/PartitionedCall:6 incompatible with expected variant

**System information**
- Have I written custom code : yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  (with 1080Ti)
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 2.3.0 
- Python version: 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce GTX 1660 Ti computeCapability: 7.5

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
tf version : v2.3.0-rc2-23-gb36436b087 2.3.0

**Describe the current behavior**

Error messages 

2020-10-09 16:42:47.403851: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_270 was passed float from StatefulPartitionedCall/sequential/lstm/PartitionedCall:6 incompatible with expected variant.
2020-10-09 16:42:47.524417: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_240 was passed float from StatefulPartitionedCall/sequential/lstm_3/PartitionedCall:6 incompatible with expected variant.
2020-10-09 16:42:52.806817: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_190 was passed float from StatefulPartitionedCall/sequential/lstm/PartitionedCall:6 incompatible with expected variant.
2020-10-09 16:42:52.899388: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_160 was passed float from StatefulPartitionedCall/sequential/lstm_3/PartitionedCall:6 incompatible with expected variant.
2020-10-09 16:42:54.497530: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_150 was passed float from StatefulPartitionedCall/sequential/lstm/PartitionedCall:6 incompatible with expected variant.
2020-10-09 16:42:54.584999: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_120 was passed float from StatefulPartitionedCall/sequential/lstm_3/PartitionedCall:6 incompatible with expected variant.

**Describe the expected behavior**
I don't see why the errors are happening



**Other info / logs** 
I am using ipopt for optimization alongwith tensorflow (and tf.function and tf.gradTape). 
"
43916,Keras Functional API does not support tf.custom_gradient() with tf.numpy_function(),"**System information**

- I have written custom code (see below)
- Red Hat Enterprise Linux Server release 7.7 (Maipo)
- TensorFlow installed from: binary (pip)
- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0 AND v1.12.1-42951-ge8766a151d 2.4.0-dev20201005
- Python version: 3.6.9

**Describe the current behavior**

Functions with custom gradients (i.e. decorated with `tf.custom_gradient()`) using numpy functions internally (with `tf.numpy_function()`) cannot be used directly with the Keras Functional API (plain crash when calling these for example with `tf.keras.layers.Input(shape=(2,))` Tensor).

**Describe the expected behavior**

Calling these functions on Tensors originating from a `tf.keras.layers.Input()` Tensor should return another symbolic Tensor usable with the Keras Functional API.

**Standalone code to reproduce the issue**

```python
>>> import tensorflow as tf, numpy as np
>>> (tf.__version__, np.__version__)
('2.3.0', '1.18.5')

>>> x = tf.keras.layers.Input(shape=(2,))
>>> x
<tf.Tensor 'input_1:0' shape=(None, 2) dtype=float32>

>>> @tf.custom_gradient
... def my_sin_with_gradient_using_numpy_function(x):
...     def _sin_and_gradient_numpy(x):
...         return (np.sin(x), np.cos(x))
...     (y, dy_dx) = tf.numpy_function(_sin_and_gradient_numpy, [x], [tf.float32, tf.float32])
...     def grad_fn(grad_y):
...         return grad_y * dy_dx
...     return (y, grad_fn)

>>> my_sin_with_gradient_using_numpy_function(x)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py"", line 264, in __call__
    return self._d(self._f, a, k)
  File ""/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py"", line 218, in decorated
    return _eager_mode_decorator(wrapped, args, kwargs)
  File ""/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py"", line 412, in _eager_mode_decorator
    result, grad_fn = f(*args, **kwargs)
  File ""<stdin>"", line 5, in my_sin_with_gradient_using_numpy_function
  File ""/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 632, in numpy_function
    return py_func_common(func, inp, Tout, stateful=True, name=name)
  File ""/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 519, in py_func_common
    result = func(*[np.array(x) for x in inp])
  File ""/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 519, in <listcomp>
    result = func(*[np.array(x) for x in inp])
  File ""/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 848, in __array__
    "" a NumPy call, which is not supported"".format(self.name))
NotImplementedError: Cannot convert a symbolic Tensor (input_1:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
```

**Other info / logs**

This ticket is related to #43542 where I mention the fact that `tf.numpy_function()` does not support symbolic Tensors as of TF 2.3. It looks like this is going to be fixed in TF 2.4 (tested with tf_nightly), but the crash here still happens in a somewhat different fashion.

Below are some observations/tests with TF 2.3 and tf_nightly.

```python
>>> tf.__version__
'2.3.0' # TF 2.3
'2.4.0-dev20201005' # tf_nightly


# Let us define the following implementations of the sin() function:

# sin() with custom gradient, using tensorflow ops:
>>> @tf.custom_gradient
... def my_sin_with_gradient_using_tensorflow_ops(x):
...     def grad_fn(grad_y):
...         return grad_y * tf.cos(x)
...     return (tf.sin(x), grad_fn)

# sin() without custom gradient, using numpy function:
>>> def my_sin_without_gradient_using_numpy_function(x):
...     return tf.numpy_function(np.sin, [x], np.float32)

# sin() with custom gradient, using numpy function:
# This is the function of interest here.
>>> @tf.custom_gradient
... def my_sin_with_gradient_using_numpy_function(x):
...     def _sin_and_gradient_numpy(x):
...         return (np.sin(x), np.cos(x))
...     (y, dy_dx) = tf.numpy_function(_sin_and_gradient_numpy, [x], [tf.float32, tf.float32])
...     def grad_fn(grad_y):
...         return grad_y * dy_dx
...     return (y, grad_fn)


# All these variants return the expected values and gradients with eager Tensors:

>>> x = tf.constant([0.5])
>>> x
<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)> # TF 2.3 and tf_nightly

>>> my_sin_with_gradient_using_tensorflow_ops(x)
<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.47942555], dtype=float32)> # TF 2.3 and tf_nightly
>>> tf.test.compute_gradient(my_sin_with_gradient_using_tensorflow_ops, [x])
((array([[0.87758255]], dtype=float32),), (array([[0.877574]], dtype=float32),)) # TF 2.3 and tf_nightly

>>> my_sin_without_gradient_using_numpy_function(x)
<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.47942555], dtype=float32)> # TF 2.3 and tf_nightly
>>> tf.test.compute_gradient(my_sin_without_gradient_using_numpy_function, [x])
((array([[0.]], dtype=float32),), (array([[0.877574]], dtype=float32),)) # TF 2.3 and tf_nightly

>>> my_sin_with_gradient_using_numpy_function(x)
<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.47942555], dtype=float32)> # TF 2.3 and tf_nightly
>>> tf.test.compute_gradient(my_sin_with_gradient_using_numpy_function, [x])
((array([[0.87758255]], dtype=float32),), (array([[0.877574]], dtype=float32),)) # TF 2.3 and tf_nightly


# However, both fail with Keras Functional API:


>>> x = tf.keras.layers.Input(shape=(2,))
>>> x
<tf.Tensor 'input_1:0' shape=(None, 2) dtype=float32> # TF 2.3
<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'input_1')> # tf_nightly

# tf.custom_gradient() no longer works with Functional API with new TF version (new bug).
>>> my_sin_with_gradient_using_tensorflow_ops(x)
<tf.Tensor 'Identity_3:0' shape=(None, 2) dtype=float32> # TF 2.3 (ok)
TypeError: Cannot convert a symbolic Keras input/output to a numpy array. # tf_nightly

# tf.numpy_function() works with Functional API with new TF version (fixed bug, see issue #43542).
>>> my_sin_without_gradient_using_numpy_function(x)
NotImplementedError: Cannot convert a symbolic Tensor (input_1:0) to a numpy array # TF 2.3
<KerasTensor: shape=<unknown> dtype=float32 (created by layer 'tf.numpy_function_8')> # tf_nightly

# tf.custom_gradient() used with tf.numpy_function() does not work with Functional API.
>>> my_sin_with_gradient_using_numpy_function(x)
NotImplementedError: Cannot convert a symbolic Tensor (input_1:0) to a numpy array # TF 2.3
TypeError: Cannot convert a symbolic Keras input/output to a numpy array # tf_nightly

# As with issue #43542, wrapping this function into tf.keras.layers.Lambda() is a workaround but the root issue remains.
>>> tf.keras.layers.Lambda(my_sin_with_gradient_using_numpy_function)(x)
<tf.Tensor 'lambda/IdentityN:0' shape=<unknown> dtype=float32> # TF 2.3
<KerasTensor: shape=<unknown> dtype=float32 (created by layer 'lambda_1')> # tf_nightly
```



"
43915,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Nope
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: It didn't happen on mobile
- TensorFlow installed from (source or binary): I used pip
- TensorFlow version (use command below): tensorflow==2.3.1
- Python version: Python 3.8.2
- Bazel version (if compiling from source): I didn't use Bazel
- GCC/Compiler version (if compiling from source): I did not compile it from source
- CUDA/cuDNN version: I didn't install tensorflow for GPU, just tensorflow
- GPU model and memory: I don't have one

**Describe the current behavior**
 The current behavior is that I try to import tensorflow using `import tensorflow`
And I get
```
Traceback (most recent call last):
  File ""C:\Users\M.Sahal\AppData\Roaming\Python\Python38\site-packages\tensorflo
w\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dyna
mic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\M.Sahal\AppData\Roaming\Python\Python38\site-packages\tensorflo
w\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\M.Sahal\AppData\Roaming\Python\Python38\site-packages\tensorflo
w\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\M.Sahal\AppData\Roaming\Python\Python38\site-packages\tensorflo
w\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\Users\M.Sahal\AppData\Roaming\Python\Python38\site-packages\tensorflo
w\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\M.Sahal\AppData\Roaming\Python\Python38\site-packages\tensorflo
w\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\M.Sahal\AppData\Roaming\Python\Python38\site-packages\tensorflo
w\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dyna
mic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
 
**Describe the expected behavior**
I expected for it to load normally.

**Standalone code to reproduce the issue**
`import tensorflow`

**Other info / logs**
I tried to install the newest version of Microsoft Visual C++ but that didn't solve it. I tried installing Keras too but that didn't fix it either.

Stack overflow question: https://stackoverflow.com/questions/64282592/failed-to-load-the-native-tensorflow-runtime-windows-8-1"
43914,Internal launch timed out and terminated,"2020-10-09 20:00:22.213230: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-10-09 20:00:22.243996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-09 20:00:22.244526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce 920M computeCapability: 3.5
coreClock: 0.954GHz coreCount: 2 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 14.92GiB/s
2020-10-09 20:00:22.244886: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-10-09 20:00:22.246939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-10-09 20:00:22.248872: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-10-09 20:00:22.249439: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-10-09 20:00:22.251486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-10-09 20:00:22.252804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-10-09 20:00:22.257173: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-09 20:00:22.257405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-09 20:00:22.258014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-09 20:00:22.258479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
(4834, 21, 5)
(538, 21, 5)
2020-10-09 20:00:22.663431: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.
2020-10-09 20:00:22.663497: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1363] Profiler found 1 GPUs
2020-10-09 20:00:22.664240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1
2020-10-09 20:00:22.665489: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1479] CUPTI activity buffer flushed
2020-10-09 20:00:22.678966: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-10-09 20:00:22.684542: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2400500000 Hz
2020-10-09 20:00:22.684806: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558bf28f53e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-09 20:00:22.684835: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-09 20:00:22.685094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-09 20:00:22.685482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce 920M computeCapability: 3.5
coreClock: 0.954GHz coreCount: 2 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 14.92GiB/s
2020-10-09 20:00:22.685564: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-10-09 20:00:22.685607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-10-09 20:00:22.685631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-10-09 20:00:22.685653: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-10-09 20:00:22.685676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-10-09 20:00:22.685698: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-10-09 20:00:22.685721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-09 20:00:22.685813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-09 20:00:22.686221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-09 20:00:22.686534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-10-09 20:00:22.686597: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-10-09 20:00:22.765993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-09 20:00:22.766040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-10-09 20:00:22.766058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-10-09 20:00:22.766352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-09 20:00:22.766933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-09 20:00:22.767337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-09 20:00:22.767749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1757 MB memory) -> physical GPU (device: 0, name: GeForce 920M, pci bus id: 0000:01:00.0, compute capability: 3.5)
2020-10-09 20:00:22.769944: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558bf65bd9f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-10-09 20:00:22.769984: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce 920M, Compute Capability 3.5
2020-10-09 20:00:25.217767: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated
2020-10-09 20:00:25.217849: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1
Aborted (core dumped)

<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro 20.1.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.2
- Python version: 3.8.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1.243/7.6
- GPU model and memory: Nvidia 920m / 2GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43913,keras.utils.Sequence instance can't be deleted from memory after training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 x64
- TensorFlow version (use command below):2.3.1
- Python version: 3.8
- CUDA/cuDNN : 10.1, 7.6.5
- GPU model and memory:

**Describe the current behavior**
I have created a custom class that inherits from keras.utils.Sequence
This class is reading some images with opencv into memory when constructed
If i just instantiate this class and after this i call 
```
import gc

myObj = Myclass(imgpaths)
del myObj
gc.colect()
```

everything is ok, but, if i am creating an instance + performing training:
```
import gc

train= Myclass(imgpaths)
validation= Myclass(imgpaths)

model = MyCustomModel()#i just used the pretrained Xception with 2 custom layers at the top for my case
model.fit(train, validation,...)
del train
del validation
gc.collect()
#...
#here i am doing other things
#...

```
then i can see that memory is not freed when calling gc.collect() and is freed only when the process is stopped
**Describe the expected behavior**

model should free the Sequence class passed to it when training is finished

**Standalone code to reproduce the issue**

#create a simple Sequence instance that is reading some images when constructed
#create an Xception model and call fit() with that sequence
#import gc, call del train_sequence, call gc.collect()
#create a for loop so that you will be able to see that memory is not freed (for i in range(1,999999999999999999): a=5)
#inspect RAM consumption and observe that RAM use is not decreasing after gc.collect(), meaning that keras model stores internally a reference of the Sequence object and it is not allowing to free the memory
"
43912,Optimized cadence kernels need to use the new TfLiteEvalTensor API,"The TFLM team recently ported all kernels except a handful of externally maintained optimized kernels. The new API enables very low memory overhead for TFLM. The primary change is the `TfLiteTensor` C struct is only available during `TfLiteRegistration::Prepare` for a kernel. Those structs are served under temporary memory and all data is only available during the lifetime of that method. All TFLM kernels should request the `TfLiteEvalTensor` C struct during `TfLiteRegistration::Eval` calls. 

A sample change looks like this:
https://github.com/tensorflow/tensorflow/commit/e392050297b62772fb9e6aaf10cf1214cb5261e7

This issue tracks updating the Cadence hifi kernels to this new API.

@pnikam-cad @nyadla-sys

PTAL - happy to review any PRs as they come in.
"
43911,"""device kernel image is invalid"" when using pre-built linux+gpu c library","Hi,

I'm using FFmpeg which calls into tensorflow c library  with model file espcn.pb (see attached), the model is very simple with several layers.

[espcn.zip](https://github.com/tensorflow/tensorflow/files/5354977/espcn.zip)

it runs successfully based on cpu path (https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.3.0.tar.gz), but there's issue when just change the tf c library to be gpu path (https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-2.3.0.tar.gz)
```
...
2020-10-09 20:47:23.123617: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-09 20:50:38.630731: E tensorflow/core/common_runtime/session.cc:91] Failed to create session: Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid
2020-10-09 20:50:38.630816: E tensorflow/c/c_api.cc:2184] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid
```

https://forums.developer.nvidia.com/t/cuda-10-1-243-tensorflow-gpu-2-3-0rc0-cuda-runtime-error-device-kernel-image-invalid/145187 said that this issue  happens with tf version 2.3, there's no such issue with tf version 2.2.  Could you confirm it? thanks.

Is this issue still there with latest tf code? If no, i'll try to build from source code. 

How about to roll back to tf version 1.15 which is the previous pre-built c library before current 2.3 version, thanks.

"
43910,Optimized arc kernels need to use the new TfLiteEvalTensor API,"The TFLM team recently ported all kernels except a handful of externally maintained optimized kernels. The new API enables very low memory overhead for TFLM. The primary change is the `TfLiteTensor` C struct is only available during `TfLiteRegistration::Prepare` for a kernel. Those structs are served under temporary memory and all data is only available during the lifetime of that method. All TFLM kernels should request the `TfLiteEvalTensor` C struct during `TfLiteRegistration::Eval` calls. 

A sample change looks like this:
https://github.com/tensorflow/tensorflow/commit/e392050297b62772fb9e6aaf10cf1214cb5261e7

This issue tracks updating the Arc kernels to this new API.

@dzakhar 
@JaccovG

PTAL - happy to review any PRs as they come in.
"
43909,tf.signal.stft has side effects,"**Describe the current behavior**
**Describe the expected behavior**
**Standalone code to reproduce the issue**

The following script crashes if the designated line is **uncommented**, else it runs fine. Note that the output of tf.signal.stft is not used.

```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# uncomment the next line for serious side effects on the GPU (tf 2.1.x, OK on 2.3.x, 2.2.x)
# x = tf.signal.stft(tf.ones((16000,)), frame_length=512, frame_step=256)
x = tf.ones((61, 257), dtype=tf.float32)

# model
l0 = keras.Input(shape=x.shape)
l1 = layers.GRU(100)(l0)
l2 = layers.Dense(100, activation=""relu"")(l1)
model = keras.Model(inputs=l0, outputs=l2)
print(model.summary())

# call
x = tf.expand_dims(x, 0)
y = model(x)
```

**Other info / logs**
This is a partial crash log:
```
2020-10-09 12:45:02.659125: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-09 12:45:03.818397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-10-09 12:45:03.974618: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-10-09 12:45:03.982373: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-10-09 12:45:03.982410: W tensorflow/stream_executor/stream.cc:2041] attempting to perform BLAS operation using StreamExecutor without BLAS support
Traceback (most recent call last):
  File ""bug.py"", line 18, in <module>
    y = model(x)
  File ""/home/adev/.pyenv/versions/ndp120/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/home/adev/.pyenv/versions/ndp120/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 717, in call
    convert_kwargs_to_constants=base_layer_utils.call_context().saving)
  File ""/home/adev/.pyenv/versions/ndp120/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 891, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File ""/home/adev/.pyenv/versions/ndp120/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/home/adev/.pyenv/versions/ndp120/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/core.py"", line 1142, in call
    outputs = gen_math_ops.mat_mul(inputs, self.kernel)
  File ""/home/adev/.pyenv/versions/ndp120/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py"", line 5616, in mat_mul
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/adev/.pyenv/versions/ndp120/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(1, 100), b.shape=(100, 100), m=1, n=100, k=100 [Op:MatMul]
```

[tf_env.txt](https://github.com/tensorflow/tensorflow/files/5354967/tf_env.txt)
"
43908,which cuda version is required for the pre-built c library,"I'm trying to use linux gpu support pre-built c library at https://www.tensorflow.org/install/lang_c on a system which contains directory such as /usr/local/cuda-11.0/targets/x86_64-linux/lib/  (i want to use cuda instead of cpu)

```
# ls /usr/local/cuda-11.0/targets/x86_64-linux/lib/libcu
libcublas.so                 libcudart.so.11.0.221        libcuinj64.so.11.0           libcusolver.so
libcublas.so.11              libcudart_static.a           libcuinj64.so.11.0.221       libcusolver.so.10
libcublas.so.11.2.0.252      libcufft.so                  libculibos.a                 libcusolver.so.10.6.0.245
libcublasLt.so               libcufft.so.10               libcupti.so                  libcusolverMg.so
libcublasLt.so.11            libcufft.so.10.2.1.245       libcupti.so.11.0             libcusolverMg.so.10
libcublasLt.so.11.2.0.252    libcufftw.so                 libcupti.so.2020.1.1         libcusolverMg.so.10.6.0.245
libcudadevrt.a               libcufftw.so.10              libcurand.so                 libcusparse.so
libcudart.so                 libcufftw.so.10.2.1.245      libcurand.so.10              libcusparse.so.11
libcudart.so.11.0            libcuinj64.so                libcurand.so.10.2.1.245      libcusparse.so.11.1.1.245
```

I met the following issues when I tried to run my application based on the tf c library.
```
...
2020-10-09 10:13:05.274477: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-09 10:13:05.276781: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-10-09 10:13:05.557487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2020-10-09 10:13:05.558557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
pciBusID: 0000:82:00.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2020-10-09 10:13:05.559954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 2 with properties: 
pciBusID: 0000:02:00.0 name: Tesla P40 computeCapability: 6.1
coreClock: 1.531GHz coreCount: 30 deviceMemorySize: 22.38GiB deviceMemoryBandwidth: 323.21GiB/s
2020-10-09 10:13:05.560105: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/workspace/tf/lib/
2020-10-09 10:13:05.560239: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/workspace/tf/lib/
2020-10-09 10:13:05.561279: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-09 10:13:05.561567: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-09 10:13:05.563732: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-09 10:13:05.563897: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/workspace/tf/lib/
2020-10-09 10:13:05.564023: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/workspace/tf/lib/
2020-10-09 10:13:05.564037: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-10-09 10:13:05.564066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-09 10:13:05.564077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 2 
2020-10-09 10:13:05.564085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N N 
2020-10-09 10:13:05.564091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N N 
2020-10-09 10:13:05.564098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 2:   N N N 
```

i guess the pre-build c library depends on cuda 10? is it right? thanks.

btw, can I build the c library locally with cuda 11? thanks."
43905,"Prefetch to GPU: prefetch_to_device does not do anything (from_generator), tf.data.Dataset API unclear","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): Official 2.4-nightly docker (version 2.4.0-dev20201005)
- TensorFlow version (use command below): Official 2.4-nightly docker (version 2.4.0-dev20201005)
- Python version: 3.6.9
- CUDA/cuDNN version: CUDA: 11.1, cuDNN: 8.0.2
- GPU model and memory: Nvidia GTX 1080 Ti, 11170MiB

The API concerning moving CPU to GPU with prefetching extremely unclear using tf.data.Dataset. The function 'prefetch_to_device' simply does not work, even though it was stated that it should be fixed by TF 2.3 or TF 2.4 in the following issue:
[issue 35563](https://github.com/tensorflow/tensorflow/issues/35563)

In order to show the behavior, I have written a standalone test that goes over four options:
 - 0: only use 'prefetch_to_device'
 - 1: Use 'copy_to_device'
 - 2: Use 'copy_to_device' and prefetch
 - 3: Use 'copy_to_device' and 'prefetch_to_device'

I would expect options 0, 2, and 3 to be almost identical. However, only option 2 does actually the indented behavior. (Option 1 does not do prefetching, so in that sense it also does what it is intended for).

I am also not even 100% sure if option 2 is optimal. It does seem that the other GPU ops are blocked during the MemCpy. It could be that it only looks as if it is covered. But I do not have the expertise to judge this case. In terms of timings I did notice that option 2 was the fastest: 

~~~
Option prefetch_gpu
data/logs/pa_20201009-082628_prefetch_gpu
Time lapsed= 0:00:01.177912

Option copy
data/logs/pa_20201009-082628_copy
Time lapsed= 0:00:00.916617

Option copy_prefetch
data/logs/pa_20201009-082628_copy_prefetch
Time lapsed= 0:00:00.831833

Option copy_prefetch_gpu
data/logs/pa_20201009-082628_copy_prefetch_gpu
Time lapsed= 0:00:00.926764
~~~

**Standalone code to reproduce the issue**
```python
import os
import datetime
from tqdm import tqdm

import numpy as np

import tensorflow as tf
print('TF version', tf.__version__)


@tf.function
def do_stuff(wmat, tf_var):

    A = tf.matmul(wmat + 1, tf.transpose(wmat))
    error = tf.reduce_mean(tf_var)
    return error, A 

exp_uuid = datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")

n_batches = 20

weights = [None] * n_batches
for i in range(n_batches):
    weights[i] = tf.constant(np.random.rand(2000,10000), dtype=tf.float32)


def gen():
    for i in weights:
        yield i

option_names = ['prefetch_gpu', 'copy', 'copy_prefetch', 'copy_prefetch_gpu']
for option in range(4):

    dataset = tf.data.Dataset.from_generator(gen, output_types=(tf.float32))

    if option == 0:
        ## Option 1: prefetch_gpu
        #
        ## output:  
        ##          weights device /job:localhost/replica:0/task:0/device:CPU:0
        ##          weights device after identity /job:localhost/replica:0/task:0/device:GPU:0

        gpu_transform = tf.data.experimental.prefetch_to_device('/gpu:0')
        dataset.apply(gpu_transform)

    elif option == 1:
        ## Option 1: only copy
        #
        ## output:
        ##          weights device /job:localhost/replica:0/task:0/device:GPU:0
        dataset = dataset.apply(tf.data.experimental.copy_to_device(""/gpu:0""))

    elif option == 2:
        ## Option 2: copy + prefetch
        ## as suggested in https://github.com/tensorflow/tensorflow/issues/35563#issuecomment-602160568
        #
        ## output:
        ##          weights device /job:localhost/replica:0/task:0/device:GPU:0

        dataset = dataset.apply(tf.data.experimental.copy_to_device(""/gpu:0""))
        with tf.device(""/gpu:0""):
            dataset = dataset.prefetch(1)

    elif option == 3:
        ## Option 3: copy + prefetch_gpu
        #
        ## output:
        ##          weights device /job:localhost/replica:0/task:0/device:GPU:0
        dataset = dataset.apply(tf.data.experimental.copy_to_device(""/gpu:0""))
        gpu_transform = tf.data.experimental.prefetch_to_device('/gpu:0')
        dataset.apply(gpu_transform)


    tf_var = tf.Variable(np.zeros(3))
    adam = tf.keras.optimizers.Adam(1e-4) 
    logpath = os.path.join('data', 'logs', 'pa_' + exp_uuid + '_' + option_names[option])

    tf.profiler.experimental.start(logpath)
    start = datetime.datetime.now()
    for b, wmat in tqdm(enumerate(dataset)):
        with tf.GradientTape() as tape:

            if b == 0:
                print('\n weights device', wmat.device)
                print('')

            if option == 0:
                wmat = tf.identity(wmat, 'move_to_gpu')
                if b == 0:
                    print('weights device after identity', wmat.device)
                    print('')

            # Do some calculations
            result = do_stuff(wmat, tf_var)
        
        grads = tape.gradient(result[0], [tf_var])
        adam.apply_gradients(zip(grads, [tf_var]))
    stop = datetime.datetime.now()
    tf.profiler.experimental.stop()

    print(f'\nOption {option_names[option]}')
    print(logpath)
    print('Time lapsed=', stop - start)
```

Option 0: prefetch_to_device 
![prefetch_gpu_overview](https://user-images.githubusercontent.com/28840027/95562130-0984eb00-0a1c-11eb-89c0-0949ed49eeb8.png)
![prefetch_gpu_trace](https://user-images.githubusercontent.com/28840027/95562131-0a1d8180-0a1c-11eb-8840-b49acebe77ad.png)

Option 1: copy_to_device
![copy_overview](https://user-images.githubusercontent.com/28840027/95562121-07229100-0a1c-11eb-847e-ce5f0b0dd9e1.png)
![copy_trace](https://user-images.githubusercontent.com/28840027/95562129-08ec5480-0a1c-11eb-8eb9-fc4646f2ee3d.png)

Option 2: copy_to_device + prefetch
![copy_prefetch_overview](https://user-images.githubusercontent.com/28840027/95562125-0853be00-0a1c-11eb-9402-90584ba2360c.png)
![copy_prefetch_trace](https://user-images.githubusercontent.com/28840027/95562127-0853be00-0a1c-11eb-963e-4ef90664a366.png)

Option 3: copy_to_device + prefetch_to_device
![copy_prefetch_gpu_overview](https://user-images.githubusercontent.com/28840027/95562123-07bb2780-0a1c-11eb-947f-27aa82348e1f.png)
![copy_prefetch_gpu_trace](https://user-images.githubusercontent.com/28840027/95562124-07bb2780-0a1c-11eb-9254-47fdc7ab2e81.png)


Files:
[pa_20201009-082628_prefetch_gpu.zip](https://github.com/tensorflow/tensorflow/files/5353593/pa_20201009-082628_prefetch_gpu.zip)
[pa_20201009-082628_copy.zip](https://github.com/tensorflow/tensorflow/files/5353594/pa_20201009-082628_copy.zip)
[pa_20201009-082628_copy_prefetch.zip](https://github.com/tensorflow/tensorflow/files/5353596/pa_20201009-082628_copy_prefetch.zip)
[pa_20201009-082628_copy_prefetch_gpu.zip](https://github.com/tensorflow/tensorflow/files/5353597/pa_20201009-082628_copy_prefetch_gpu.zip)

"
43904,Per Example Gradients fail with LSTM,"**System information**
Colab with `tf-nightly-gpu==2.4.0-dev20201007`

**Issue**
Per example gradients via `tf.vectorized_map` with an LSTM model fails. It works if you set `unroll=True` for the LSTM.

**Standalone code to reproduce the issue**
[Here](https://colab.research.google.com/gist/n2cholas/4bf3e70aa2ec310f69fe90bb21cb1747/lstm-per-eg-grads.ipynb) is a colab gist with the error.
"
43903,Link error occurs due to MetaGraphDef,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4.0(master branch)
- Python version: 3.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0/8.0
- GPU model and memory:



**Describe the problem**
I am building tensorflow 2.4.0(master branch / lastest commit : 68a6fe0d984377b625b421d34f8c607d6ed73597) from source with additional link symbols


You have bazel 3.1.0 installed.
Please specify the location of python. [Default is C:\Anaconda3\envs\py37\python.exe]:


Found possible Python library paths:
  C:\Anaconda3\envs\py37\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Anaconda3\envs\py37\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Found CUDA 11.0 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/include
Found cuDNN 8 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 3.5, 5.0, 5.2, 6.1, 7.0, 7.5, 8.0


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.

bazel build -c opt --config=opt --config=cuda --config=v1 //tensorflow:libtensorflow_cc.so
(or bazel build -c opt --config=opt --config=cuda --config=v1 //tensorflow:tensorflow_cc.dll)

both way occurs same error messages.

I think this line causes error in c++
		MetaGraphDef graphDef;
                std::istream istream
		bool stat = graphDef.ParseFromIstream(istream);
 and the error message is 
libtensorflow_cc.so.2.4.0.if.exp : error LNK2001: 확인할 수 없는 외부 기호(maybe this phrase is same as 'unresolved external symbols') ""public: bool __cdecl google::protobuf::Message::ParseFromIstream(class std::basic_istream<char,struct std::char_traits<char> > *)"" (?ParseFromIstream@Message@protobuf@google@@QEAA_NPEAV?$basic_istream@DU?$char_traits@D@std@@@std@@@Z)


when i'm building tf 1.13 from source(CUDA10.0/cudnn7) using tf_exported_symbols_msvc.lds following this(https://github.com/tensorflow/tensorflow/issues/23542) i could successfully import additional symbols
However, when i'm building tf2.4.0, i could not import additional symbols via tf_exported_symbols_msvc.lds.
As described in (https://github.com/tensorflow/tensorflow/issues/23542), i think that tf do not support the way to add symbols by tf_exported_symbols_msvc.lds. So it tried to add symbols by explicitly add symbols on tensorflow/tools/def_file_filter/def_file_filter.py.tpl like following lines

      if DATA_EXCLUDE_RE.search(line):
        def_fp.write(""\t"" + decorated + ""\n"")
      else:
        def_fp.write(""\t"" + decorated + "" DATA\n"")
      taken.add(decorated)

    for sym in symbols_pybind:
      def_fp.write(""\t{}\n"".format(sym))
      taken.add(sym)
    # write additional symbols
    def_fp.write(""\t??$CreateMaybeMessage@VTensorShapeProto@tensorflow@@$$V@Arena@protobuf@google@@CAPEAVTensorShapeProto@tensorflow@@PEAV012@@Z\n"")
    def_fp.write(""\t?AppendToString@MessageLite@protobuf@google@@QEBA_NPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z\n"")
    def_fp.write(""\t?SerializePartialAsString@MessageLite@protobuf@google@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ\n"")



By this way, i think i could add additional symbols to tensorflow_cc.lib. and the log messsage says that lnk2001 unresolved symbols on these four symbols
    def_fp.write(""\t??0?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEAA@V?$Span@$$CB_J@absl@@@Z\n"")
    def_fp.write(""\t?ParseFromIstream@Message@protobuf@google@@QEAA_NPEAV?$basic_istream@DU?$char_traits@D@std@@@std@@@Z\n"")
    def_fp.write(""\t?find@string_view@absl@@QEBA_KD_K@Z\n"")
    def_fp.write(""\t?tensor_data@Tensor@tensorflow@@QEBA?AVstring_view@absl@@XZ\n"")
and when i erase these symbols from def_file_filter.py.tpl i could successfully add extra symbols on tensorflow_cc.lib(or libtensorflow_cc_.lib) by writing on def_file_filter.py.tpl

PROBLEMS
- I couldn't add above four extra symbols(Also, I couldn't find MetaGraphDef class.. that i couold found on tf1.13)

The question is, that i want to use those functions on c++, and i could use it when i build tensorflow on tf 1.13(cuda10.0, cudnn 7.0, bazel build -c opt --config=opt --config=cuda --config=noignite --config=nokafka //tensorflow:libtensorflow_cc.so)  by adding extra symbols from  tf_exported_symbols_msvc.lds. BUT when it comes to tf 2.4.0, i couldn't add some of the extra symbols that i could add in tf 1.13(above four symbols). It tooks almost 10 days to figure out that tf2.4.0 do not support extra symbols by tf_exported_mscvc.lds.... 
I think it's because while building tf1.13 they contains the codes from tensorflow/python/framework/meta_graph.py in c++ dll and lib. but tf2.4.0 does not..
[build logs.txt](https://github.com/tensorflow/tensorflow/files/5352426/build.logs.txt)

Therefore i think this problem does not related with exporting extra symbols(https://github.com/tensorflow/tensorflow/issues/23542), but it is related with how to build c++ with additional libraries...(I could find metagraphdef it tf.compat.v1.MetaGraphDef in tf2.3.0 python documentation. So i think they do support metagraphdef.. but i couldn't use it on c++ 2.4.0)

If anyone knows how to figure out this problems, i would appreciate your advice.


[log and def_file_filter.zip](https://github.com/tensorflow/tensorflow/files/5352429/log.and.def_file_filter.zip)

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43902,Supporting class_weight in train_step ?,"Hi, I have read more about overriding train_step here: https://keras.io/guides/customizing_what_happens_in_fit/. It helps me a lot. But, I still have a problem about support for class_weight in train_step? How to do it? My part code is as follows:

    def train_step(self, data):
        if isinstance(data, tuple):
            x,y,class_weight  = data
        with tf.GradientTape() as tape:

            Flair_encoder_out,T1_encoder_out,T1ce_encoder_out,T2_encoder_out,\
            Flair_decoder_out,T1_decoder_out,T1ce_decoder_out,T2_decoder_out,\
            grade_out = self.MMDR_model(x)

            z_mean_Flair, z_log_var_Flair, z_Flair, common_Flair,spec_Flair = Flair_encoder_out
            z_mean_T1, z_log_var_T1, z_T1, common_T1, spec_T1 =T1_encoder_out
            z_mean_T1ce, z_log_var_T1ce, z_T1ce,common_T1ce, spec_T1ce = T1ce_encoder_out
            z_mean_T2, z_log_var_T2, z_T2,common_T2, spec_T2 = T2_encoder_out

            reconstruction_loss = tf.reduce_mean(
                tf.keras.losses.mean_squared_error(x[0], Flair_decoder_out)
            )+tf.reduce_mean(
                tf.keras.losses.mean_squared_error(x[1], T1_decoder_out)
            )+tf.reduce_mean(
                tf.keras.losses.mean_squared_error(x[2], T1ce_decoder_out)
            )+tf.reduce_mean(
                tf.keras.losses.mean_squared_error(x[3], T2_decoder_out)
            )

            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var_Flair - tf.square(z_mean_Flair) - tf.exp(z_log_var_Flair))\
                      + (-0.5 * tf.reduce_mean(1 + z_log_var_T1 - tf.square(z_mean_T1) - tf.exp(z_log_var_T1)))\
                      + (-0.5 * tf.reduce_mean(1 + z_log_var_T1ce - tf.square(z_mean_T1ce) - tf.exp(z_log_var_T1ce)))\
                      + (-0.5 * tf.reduce_mean(1 + z_log_var_T2 - tf.square(z_mean_T2) - tf.exp(z_log_var_T2)))

            com_loss = tf.reduce_mean(mean_squared_error(common_Flair, common_T1)) + tf.reduce_mean(mean_squared_error(common_Flair,common_T1ce)) + \
                        tf.reduce_mean(mean_squared_error(common_Flair, common_T2)) + tf.reduce_mean(mean_squared_error(common_T1, common_T1ce)) + \
                        tf.reduce_mean(mean_squared_error(common_T1,common_T2)) + tf.reduce_mean(mean_squared_error(common_T1ce,common_T2))

            spec_loss = tf.reduce_mean(mean_squared_error(spec_Flair, spec_T1)) + tf.reduce_mean(mean_squared_error(spec_Flair, spec_T1ce)) + \
                        tf.reduce_mean(mean_squared_error(spec_Flair, spec_T2)) + tf.reduce_mean(mean_squared_error(spec_T1, spec_T1ce)) + \
                        tf.reduce_mean(mean_squared_error(spec_T1, spec_T2)) + tf.reduce_mean(mean_squared_error(spec_T1ce, spec_T2))
            com_spec_loss = tf.sqrt(com_loss / spec_loss)

            grade_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y,grade_out))

            # grade_loss = self.compiled_loss(
            #     y,
            #     grade_out,
            #     sample_weight=class_weight,
            #     regularization_losses=self.losses,
            # )
            #total_loss = 0.1 * reconstruction_loss + 0.01 * kl_loss + 0.25 * com_spec_loss + grade_loss
            total_loss =  0.1 * reconstruction_loss +  kl_loss + 0.1 * com_spec_loss + grade_loss
            # The loss function is configured in `compile()`.

        # The training happens here.
        grads = tape.gradient(total_loss, self.trainable_variables)

        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))
        print("">>>>>>>>>>>>>>metrics_names:"",self.metrics_names)

        # Update metrics (includes the metric that tracks the loss)
        self.compiled_metrics.update_state(y, grade_out)
        return {
            ""loss"": total_loss,
            ""rec_loss"": reconstruction_loss,
            ""kl_loss"": kl_loss,
            ""cs_loss"": com_spec_loss,
            ""grade_loss"": grade_loss,
            self.metrics[0].name: self.metrics[0].result(),
            self.metrics[1].name: self.metrics[1].result()
        }"
43898,TARGET command line option to the TFLM Makefile and the target-specific makefile have specific naming requirements,"With #43896 we are going to enforce that `TARGET=blah` will only include `micro/tools/make/targets/blah_makefile.inc`.

At minimum this means that checks such as https://github.com/tensorflow/tensorflow/blob/abaca545db62fd30c6caad021d55cbcacbf01060/tensorflow/lite/micro/tools/make/targets/cortex_m_gcc_generic_makefile.inc#L2 will no longer be necessary.

#43896 removes the checks for bluepill and apollo3evb.

This issue will remain open until the checks are removed for:
 * cortex_m_gcc_generic_makefile.inc
 * stm32f4_makefile.inc
 * hexagon_makefile.inc
 * xtensa_hifimini_makefile.inc

But the other targets might need updates beyond simply removing the no longer necessary if statements.

Tagging some of maintainers of targets that I am aware of: @yair-ehrenwald @dzakhar @JaccovG @mansnils "
43897,Custom Op Output Shape is Unknown in TF 1.15,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): SLES 12.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15.3
- Python version: 3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): g++ 4.8 (for custom op)
- CUDA/cuDNN version: 10.0 / 7.6
- GPU model and memory: Tesla P100 / 12198MiB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Custom Op compiled against TF 1.15 has unknown output shape (`>>>print(repr(t.shape))  # TensorShape(None)`) in following conditions:
- Input tensor has a known shape.
- Custom Op ShapeFn has been implemented to have the same output shape as input shape.

In TF 1.11, the output shape is identical to the input shape as expected.

**Describe the expected behavior**
Custom Op output shape should be identical to input shape (`>>>print(repr(t.shape))  #TensorShape([6])`)

**Standalone code to reproduce the issue**

Based on the ""Zero Out"" example from [create an op](https://www.tensorflow.org/guide/create_op). The following should get it up and running:
1. copy all files into the same directory
2. `sh build.sh`
3. `python zero_out_shape_test.py`

In TF 1.11 `as_list` will work and shape will be `TensorShape([6])`. In TF 1.15 `as_list` will fail (logs below) and shape will be `TensorShape(None)`.

_zero_out.cc_
```cpp
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""

using namespace tensorflow;

REGISTER_OP(""ZeroOut"")
    .Input(""to_zero: int32"")
    .Output(""zeroed: int32"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      c->set_output(0, c->input(0));
      return Status::OK();
    });

class ZeroOutOp : public OpKernel {
 public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);
    auto input = input_tensor.flat<int32>();

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));
    auto output_flat = output_tensor->flat<int32>();

    // Set all but the first element of the output tensor to 0.
    const int N = input.size();
    for (int i = 1; i < N; i++) {
      output_flat(i) = 0;
    }

    // Preserve the first input value if possible.
    if (N > 0) output_flat(0) = input(0);
  }
};

REGISTER_KERNEL_BUILDER(Name(""ZeroOut"").Device(DEVICE_CPU), ZeroOutOp);
```

_build.sh_
```sh
TF_CFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') )
TF_LFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') )
g++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2
```

_zero_out_shape_test.py_
```py
import tensorflow as tf

_input = tf.constant([1, 2, 3, 4, 5, 6])
print('input shape:', _input.shape.as_list())

zero_out_module = tf.load_op_library('./zero_out.so')

zo_op = zero_out_module.zero_out(_input)
print(repr(zo_op.get_shape()))
print(zo_op.get_shape())
print(zo_op.get_shape().as_list())
```

**Other info / logs** Include any logs or source code that would be helpful to

TF 1.15 logs
```
input shape: [6]
TensorShape(None)
<unknown>
Traceback (most recent call last):
  File ""zero_out_shape_test.py"", line 11, in <module>
    print(zo_op.get_shape().as_list())
  File ""******/python3.5/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 1171, in as_list
    raise ValueError(""as_list() is not defined on an unknown TensorShape."")
ValueError: as_list() is not defined on an unknown TensorShape.
```

TF 1.11 logs
```
input shape: [6]
TensorShape([Dimension(6)])
(6,)
[6]
```
"
43894,"Error while building tensorflow from source, need it for C++ inference.","no such package '@icu//': java.io.IOException: **Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz**

**System information**
- Windows 10
- TensorFlow installed from source
- TensorFlow version: 1.13
- Python version: 3.7
- Bazel version 0.21
- GCC/Compiler version VS2015
- CUDA/cuDNN version: 10, 7.4.1
- GPU model and memory: Titan XP, 10Gb



**While building tensorflow for windows based on tensoflow website directed steps I got this error below after configur.py was ran successfully**

ERROR: C:/tensorflow/tensorflow/tools/pip_package/BUILD:149:1: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to C:/users/dsavaliya/_bazel_dsavaliya/xv6zejqw/external/icu/release-62-1.tar.gz: Checksum was 86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761 and referenced by '//tensorflow/tools/pip_package:licenses'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to C:/users/dsavaliya/_bazel_dsavaliya/xv6zejqw/external/icu/release-62-1.tar.gz: Checksum was 86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761

**Provide the exact sequence of commands / steps that you executed before running into the problem**

(base) **c:\>python tensorflow\configure.py**
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
nul
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: d6710a42-59b5-4bbb-9174-8bc67a08a8c7
You have bazel 0.21.0 installed.
Please specify the location of python. [Default is C:\ProgramData\Anaconda3\python.exe]:


Found possible Python library paths:
  \Users\dsavaliya\Repos\Pytools
  C:\ProgramData\Anaconda3\lib\site-packages
Please input the desired Python library path to use.  Default is [\Users\dsavaliya\Repos\Pytools]

Do you wish to build TensorFlow with XLA JIT support? [y/N]:
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]:
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apacha Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.


(base) c:\>cd c:\tensorflow
(base) **c:\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package**
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
c:\tensorflow/.bazelrc
Starting local Bazel server and connecting to it...
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
INFO: Invocation ID: 27545a6e-c366-4264-9cb3-eea8d0762600
**ERROR: C:/tensorflow/tensorflow/tools/pip_package/BUILD:149:1: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to C:/users/dsavaliya/_bazel_dsavaliya/xv6zejqw/external/icu/release-62-1.tar.gz: Checksum was 86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761 and referenced by '//tensorflow/tools/pip_package:licenses'**
**ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to C:/users/dsavaliya/_bazel_dsavaliya/xv6zejqw/external/icu/release-62-1.tar.gz: Checksum was 86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761**
INFO: Elapsed time: 22.941s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (316 packages loaded, 11908 targets configured)
    Fetching @icu; fetching 14s

**Also tried GPU supported virion with command as below, but do the same error as above.** 
(base) **c:\tensorflow>bazel build --config=monolithic --config=opt --config=cuda //tensorflow/tools/lib_package:libtensorflow**

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43893,Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
43892,Model size reduction in set_weights(),"I have used tf.Keras for training and then transferred weights to another model with the help of get_weights() and set_weights(). After saving the weights from the original to another model I have seen that there is the model size reduction of 3 times smaller. 
Any idea why this is happening. "
43891,No way to initialize Interpreter with Model reference in TensorFlowLiteSwift,"**System information**
- TensorFlow version (you are using):

TensorFlowLiteSwift 2.3.0

- Are you willing to contribute it (Yes/No):

Maybe

**Describe the feature and the current behavior/state.**

For inference you need to use Interpreter class. But all initializers uses model file parameters. This prevents sharing of the model in different threads.  

**Will this change the current api? How?**

Yes, Model class needs to be public and Interpreter class will need new initializer functions.

**Who will benefit with this feature?**

Who wants to use inference in separate threads.

"
43890,"pytorch sees my gpu, but tensorflow does not","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: no
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubunto 18.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: pip install tensorflow
-   **TensorFlow version (use command below)**: 2.3.1
-   **Python version**: 3.8.1
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 11.1
-   **GPU model and memory**: tesla p100
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem

i am not sure what is going on here. i set up a fresh gcloud instance, updated the nvidia drivers, downloaded anaconda, pytorch and tensorflow but tf can not seem to see the gpu.

my versions:


pytorch : '1.6.0'
torch.cuda.is_available() gives me True


tf: 2.3.1.
tf.config.list_physical_devices(""GPU"") gives me []


also when listing all devices:
tf.config.list_physical_devices()


i get 

[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),
 PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),
 PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]


which means tf sees the gpu but does not use it"
43887,Building on MacOS is broken for TFLite Micro,"Latest head doesn't build on MacOS:

```
make -f tensorflow/lite/micro/tools/make/Makefile test_micro_interpreter_test
```

Exception:
```
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/micro_interpreter_test tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_interpreter_test.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/lib/libtensorflow-microlite.a -Wl,--fatal-warnings -Wl,--gc-sections -lm -framework Foundation -framework AudioToolbox
ld: unknown option: --fatal-warnings
clang: error: linker command failed with exit code 1 (use -v to see invocation)
gmake: *** [tensorflow/lite/micro/tools/make/Makefile:460: tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/micro_interpreter_test] Error 1
```

Looks like the build cleanup added this flag.

"
43886,Update Keras Conv2D layer docs to specify padding value,"## URL(s) with the issue:
https://keras.io/api/layers/convolution_layers/convolution2d/

## Description of issue (what needs changing):
Specify what value is added as padding when padding argument is set to `same`. Is it zero values?

"
43885,Build Error with codes from TinyML Book,"#40170 # System information

-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.14.6
-   **TensorFlow installed from (source or binary)**: 2.0.0
-   **TensorFlow version (use command below)**:
-   **Python version**:3.7
-   **GCC/Compiler version (if compiling from source)**:4.3
-   **Exact command to reproduce**: ""make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test""

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Every time I build using the command from book, encounter error below
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *** [tensorflow/lite/micro/examples//hello_world/Makefile.inc:34: tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/hello_world_test] Error 1

### Source code / logs
Joonui-Mac-mini:tensorflow joonkim$ make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test
tensorflow/lite/micro/tools/make/Makefile:401: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
tensorflow/lite/micro/tools/make/Makefile:401: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip"" ""7e8191b24853d75de2af87622ad293ba"" tensorflow/lite/micro/tools/make/downloads/gemmlowp  
downloading https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.12.0.tar.gz"" ""c62ffefb3d4548b127cca14ce047f16c"" tensorflow/lite/micro/tools/make/downloads/flatbuffers  
downloading http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.12.0.tar.gz
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://github.com/google/ruy/archive/5bb02fbf90824c2eb6cd7418f766c593106a332b.zip"" ""c720b1743360259ac45809a321f8f26c"" tensorflow/lite/micro/tools/make/downloads/ruy  
downloading https://github.com/google/ruy/archive/5bb02fbf90824c2eb6cd7418f766c593106a332b.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2020_05_27.zip"" ""55b85f76e2995153e660391d4a209ef1"" tensorflow/lite/micro/tools/make/downloads/person_model_grayscale  
downloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2020_05_27.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_06_23.zip"" ""9b5b6d4677dd0a91b1bb992d1c4c0417"" tensorflow/lite/micro/tools/make/downloads/person_model_int8  
downloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_06_23.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://storage.googleapis.com/download.tensorflow.org/models/tflite/cifar_image_recognition_model_2020_05_27.zip"" ""1f4607b05ac45b8a6146fb883dbc2d7b"" tensorflow/lite/micro/tools/make/downloads/image_recognition_model  
downloading https://storage.googleapis.com/download.tensorflow.org/models/tflite/cifar_image_recognition_model_2020_05_27.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""http://mirror.tensorflow.org/www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz"" ""c32a1d4ab5d03f1284b67883e8d87530"" tensorflow/lite/micro/tools/make/downloads/cifar10 patch_cifar10_dataset 
downloading http://mirror.tensorflow.org/www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz
tensorflow/lite/micro/tools/make/download_and_extract.sh ""http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip"" ""438ba1fef5783cc5f5f201395cc477ca"" tensorflow/lite/micro/tools/make/downloads/kissfft patch_kissfft 
downloading http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip
Finished patching kissfft
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/examples/hello_world/hello_world_test.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/hello_world/hello_world_test.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/examples/hello_world/model.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/hello_world/model.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/all_ops_resolver.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/all_ops_resolver.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/debug_log.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/debug_log.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/memory_helpers.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/memory_helpers.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_allocator.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_allocator.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_error_reporter.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_error_reporter.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_interpreter.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_interpreter.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_profiler.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_profiler.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_string.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_string.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_time.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_time.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_utils.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_utils.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/recording_micro_allocator.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/recording_micro_allocator.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/recording_simple_memory_allocator.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/recording_simple_memory_allocator.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/simple_memory_allocator.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/simple_memory_allocator.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/test_helpers.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/test_helpers.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/benchmarks/keyword_scrambled_model_data.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/benchmarks/keyword_scrambled_model_data.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/activations.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/activations.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/add.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/add.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/arg_min_max.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/arg_min_max.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/ceil.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/ceil.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/circular_buffer.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/circular_buffer.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/comparisons.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/comparisons.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/concatenation.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/concatenation.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/conv.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/conv.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/depthwise_conv.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/depthwise_conv.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/dequantize.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/dequantize.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/elementwise.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/elementwise.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/ethosu.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/ethosu.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/floor.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/floor.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/fully_connected.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/fully_connected.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/hard_swish.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/hard_swish.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/kernel_runner.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/kernel_runner.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/kernel_util.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/kernel_util.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/l2norm.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/l2norm.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/logical.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/logical.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/logistic.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/logistic.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/maximum_minimum.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/maximum_minimum.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/mul.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/mul.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/neg.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/neg.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/pack.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/pack.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/pad.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/pad.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/pooling.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/pooling.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/prelu.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/prelu.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/quantize.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/quantize.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/reduce.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/reduce.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/reshape.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/reshape.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/resize_nearest_neighbor.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/resize_nearest_neighbor.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/round.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/round.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/shape.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/shape.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/softmax.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/softmax.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/split.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/split.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/split_v.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/split_v.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/strided_slice.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/strided_slice.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/sub.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/sub.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/svdf.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/svdf.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/tanh.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/tanh.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/unpack.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/unpack.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/memory_planner/greedy_memory_planner.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/memory_planner/greedy_memory_planner.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/memory_planner/linear_memory_planner.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/memory_planner/linear_memory_planner.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/testing/test_conv_model.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/testing/test_conv_model.o
gcc -std=c11 -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/c/common.c -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/c/common.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/core/api/error_reporter.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/error_reporter.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/core/api/flatbuffer_conversions.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/flatbuffer_conversions.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/core/api/op_resolver.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/op_resolver.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/core/api/tensor_utils.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/tensor_utils.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/kernels/internal/quantization_util.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/kernels/internal/quantization_util.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/kernels/kernel_util.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/kernels/kernel_util.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/testing/test_utils.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/testing/test_utils.o
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/schema/schema_utils.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/schema/schema_utils.o
ar -r tensorflow/lite/micro/tools/make/gen/osx_x86_64/lib/libtensorflow-microlite.a tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/all_ops_resolver.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/debug_log.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/memory_helpers.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_allocator.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_error_reporter.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_interpreter.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_profiler.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_string.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_time.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_utils.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/recording_micro_allocator.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/recording_simple_memory_allocator.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/simple_memory_allocator.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/test_helpers.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/benchmarks/keyword_scrambled_model_data.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/activations.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/add.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/arg_min_max.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/ceil.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/circular_buffer.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/comparisons.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/concatenation.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/conv.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/depthwise_conv.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/dequantize.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/elementwise.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/ethosu.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/floor.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/fully_connected.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/hard_swish.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/kernel_runner.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/kernel_util.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/l2norm.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/logical.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/logistic.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/maximum_minimum.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/mul.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/neg.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/pack.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/pad.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/pooling.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/prelu.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/quantize.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/reduce.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/reshape.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/resize_nearest_neighbor.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/round.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/shape.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/softmax.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/split.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/split_v.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/strided_slice.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/sub.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/svdf.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/tanh.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/unpack.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/memory_planner/greedy_memory_planner.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/memory_planner/linear_memory_planner.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/testing/test_conv_model.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/c/common.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/error_reporter.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/flatbuffer_conversions.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/op_resolver.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/tensor_utils.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/kernels/internal/quantization_util.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/kernels/kernel_util.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/testing/test_utils.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/schema/schema_utils.o
ar: creating archive tensorflow/lite/micro/tools/make/gen/osx_x86_64/lib/libtensorflow-microlite.a
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/hello_world_test tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/hello_world/hello_world_test.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/hello_world/model.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/lib/libtensorflow-microlite.a -Wl,--fatal-warnings -Wl,--gc-sections -lm -framework Foundation -framework AudioToolbox
ld: unknown option: --fatal-warnings
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *** [tensorflow/lite/micro/examples//hello_world/Makefile.inc:34: tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/hello_world_test] Error 1
"
43883,Embedding Projector - Unable to load different data set,"We want to use dimension reduction and clustering app of tensorflow. However we are unable to load data set to http://projector.tensorflow.org/ 
It gives us errors like Number of tensors (10000) do not match the number of lines in metadata (3492) or
it does not give any error and does not load the file

"
43882,folding batchnorm into conv in per-tensor weights quantization,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version (or github SHA if from source): 2.3.1


**Command used to run the converter or code if you’re using the Python API**
I took the keras qat tutorial and added a BatchNormalization layer in between the Conv2D and ReLU:

```
import tempfile
import os
import tensorflow as tf
from tensorflow import keras

mnist = keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images / 255.0
test_images = test_images / 255.0

model = keras.Sequential([
    keras.layers.InputLayer(input_shape=(28, 28)),
    keras.layers.Reshape(target_shape=(28, 28, 1)),
    keras.layers.Conv2D(filters=12, kernel_size=(3, 3)),
    keras.layers.BatchNormalization(),
    keras.layers.ReLU(),
    keras.layers.MaxPooling2D(pool_size=(2, 2)),
    keras.layers.Flatten(),
    keras.layers.Dense(10)
])
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.fit(
    train_images,
    train_labels,
    epochs=1,
    validation_split=0.1,
)
import tensorflow_model_optimization as tfmot
quantize_model = tfmot.quantization.keras.quantize_model
q_aware_model = quantize_model(model)
q_aware_model.compile(optimizer='adam',
                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                      metrics=['accuracy'])
train_images_subset = train_images[0:1000]  # out of 60000
train_labels_subset = train_labels[0:1000]
q_aware_model.fit(train_images_subset, train_labels_subset,
                  batch_size=500, epochs=1, validation_split=0.1)
converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

quantized_tflite_model = converter.convert()
with open('model.tflite', 'wb') as f:
    f.write(quantized_tflite_model)
```
When I quantize the weights per-channel, I can see (using Netron) that the BatchNorm was folded into Conv2D (as I expect it to be). When I changed [Default8BitConvWeightsQuantizer](https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantizers.py) to use per-tensor quantization (flipping per-axis flag and removing shape arguments in build), I saw that the BatchNorm was not folded.
Is this the way it should be? Or is it an issue? 

**The output from the converter invocation**

```
WARNING:tensorflow:From ***/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
2020-10-08 17:35:12.606042: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:tensorflow:From ***/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
2020-10-08 17:35:13.543657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-08 17:35:13.544090: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-10-08 17:35:13.544145: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-10-08 17:35:13.601942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-08 17:35:13.602363: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7313c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-10-08 17:35:13.602374: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-10-08 17:35:13.602525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-08 17:35:13.602870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2020-10-08 17:35:13.603313: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-08 17:35:13.603317: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-10-08 17:35:13.603326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-08 17:35:13.603330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-10-08 17:35:13.603333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-10-08 17:35:13.604761: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-10-08 17:35:13.604769: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-10-08 17:35:13.604772: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-10-08 17:35:13.650284: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-10-08 17:35:13.650302: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
2020-10-08 17:35:13.653368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-08 17:35:13.653747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2020-10-08 17:35:13.654132: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-08 17:35:13.654135: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-10-08 17:35:13.654145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-08 17:35:13.654148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-10-08 17:35:13.654151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 

Process finished with exit code 0

```

Thanks!"
43881,all_ops_resolver.h:4:10: fatal error: tensorflow/lite/micro/compatibility.h: No such file or directory,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution : Windows 10
- Tensorflow version : 2.1.0
- Target platform : Arduino Nano 33


Header are missing in hello_world sketches. So fails to build the deployment

Following contains the headers I used.
```

#include ""tensorflow/lite/micro/all_ops_resolver.h""
#include ""tensorflow/lite/micro/examples/hello_world/model.h""
#include ""tensorflow/lite/micro/micro_error_reporter.h""
#include ""tensorflow/lite/micro/micro_interpreter.h""
#include ""tensorflow/lite/micro/testing/micro_test.h""
#include ""tensorflow/lite/schema/schema_generated.h""
#include ""tensorflow/lite/version.h""
```"
43878,Using SYSTEM cURL (build against OpenSSL) conflicts with BoringSSL,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3.1

**Describe the problem**

Due to a hard-coded configure value of cURL (see https://github.com/tensorflow/tensorflow/issues/40065#issuecomment-667129086) we build TensorFlow with a system-installed cURL via `TF_SYSTEM_LIBS=curl`. This curl uses the systems OpenSSL library (for security reasons this has to be managed by the system)

Some parts of TF use BoringSSL, e.g. grpcio (build as a dependency) seems to contain/build a BoringSSL itself.
Now loading the TF lib (via an import in Python) loads cURL, which loads OpenSSL which causes conflicts with the BoringSSL used and ultimately leads to invalid free calls:

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- build TensorFlow with TF_SYSTEM_LIBS=curl
- Run any code using cURL through TF in Python. Example below:

```
import tensorflow_datasets as tfds

builder = tfds.builder('stanford_dogs')
builder.download_and_prepare()
```

**Any other info / logs**
```
*** Error in `python': free(): invalid pointer: 0x00000000046bfd08 ***
======= Backtrace: =========
/lib64/libc.so.6(+0x81299)[0x7f2b1dba9299]
<prefix>/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.2(ASN1_STRING_free+0x35)[0x7f2aac3d0205]
<prefix>/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.2(ASN1_primitive_free+0xbd)[0x7f2aac3d371d]
<prefix>/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.2(ASN1_template_free+0x8f)[0x7f2aac3d3b1f]
<prefix>/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.2(asn1_item_combine_free+0x2a0)[0x7f2aac3d39f0]
<prefix>/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.2(ASN1_item_free+0x17)[0x7f2aac3d3a77]
<prefix>/lib/libcurl.so.4(+0x57a20)[0x7f2aaae7ea20]
<prefix>/lib/libcurl.so.4(+0x593fb)[0x7f2aaae803fb]
<prefix>/lib/libcurl.so.4(+0x59fd7)[0x7f2aaae80fd7]
<prefix>/lib/libcurl.so.4(+0x110e2)[0x7f2aaae380e2]
<prefix>/lib/libcurl.so.4(+0x30215)[0x7f2aaae57215]
<prefix>/lib/libcurl.so.4(curl_multi_perform+0x83)[0x7f2aaae58473]
<prefix>/lib/libcurl.so.4(curl_easy_perform+0x11b)[0x7f2aaae50a6b]
<prefix>/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow15CurlHttpRequest4SendEv+0x2b9)[0x7f2ac88c34f9]
```

Is there any solution from the side of TF to avoid this? From reading the docs of BoringSSL it should be possible to define `BORINGSSL_PREFIX` to something, so the symbols from OpenSSL and BoringSSL do not clash. But I'm not sure if I understood that correctly as BoringSSL would need to use that when being built already"
43877,"TFLite: Getting error ""Expected bias tensor to be a vector"" when trying to convert and quantize a model","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): from pip
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7.9
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
TensorFlow Lite fails to convert a FC4 model when quantization is activated.
I am getting this error when trying to perform full integer quantization with mixed 16 bits activations and 8 bits weights ( tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8) and when trying to use 8 bit quantization method (tf.lite.OpsSet.TFLITE_BUILTINS_INT8).)
I did several tests. First it is working if i convert a MobileNet V1 network, so the error depends on the network architecture.
By dichotomy, i have found the layer which causes the error. I guess the error is raised when there is a ADD (AddV2 in my case) operation right after a Conv2D operation. The converter try to merge Add operation as a bias in Conv2D. In my case, the tensor dimension of the constant data to be added is [1,1,1,64]. Maybe the converter is expecting [1,64] which could explain the error raised.

Please note that  8-bit quantization is working when using TensorFlow 2.1.0, so it looks like a regression introduced between 2.1.0 and 2.3.1.

**Describe the expected behavior**
I expect the converter to successfully convert the model with desired quantization.
 
**Standalone code to reproduce the issue**
Here is the code to reproduce the issue:
```
import tensorflow as tf
import tensorflow_datasets as tfds

import os

def representative_dataset_gen():
  ds = tfds.load(""flic"",shuffle_files=True, split='train')
  assert isinstance(ds, tf.data.Dataset)
  print(ds)
  num_calibration_steps = 10;
  for _ in range(num_calibration_steps):
    example = ds.take(1)
    for i in example:
        image = i[""image""]
        name = i[""moviename""]
        print(name)
        print(""type: "" + str(type(image)))
        image = tf.image.resize(image,size=[256,256])
        image = tf.expand_dims(image, axis=0)
        yield [image]


# Convert onnx model to tflite format

# convert it first in tensorflow frozen graph
if not os.path.exists(""./convertedModels/tensorflow""):
    os.makedirs(""./convertedModels/tensorflow"")

#Then convert it in tflite format
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph('pretrained/model_frozen.pb', #TensorFlow freezegraph .pb model file
                                                      input_arrays=['FCN_1/AlexNet/mul'], # name of input arrays as defined in torch.onnx.export function before.
                                                      output_arrays=['FCN_1/Sum'],  # name of output arrays defined in torch.onnx.export function before.
                                                      input_shapes={ 'FCN_1/AlexNet/mul': [1,256,256,3]}
                                                      )

converter.optimizations = [tf.lite.Optimize.DEFAULT]

converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]

converter.inference_input_type = tf.int16  # or tf.uint8
converter.inference_output_type = tf.int16  # or tf.uint8


converter.representative_dataset = tf.lite.RepresentativeDataset(
    representative_dataset_gen)

tf_lite_model = converter.convert()
# save the converted model
open('fc4_quant.tflite', 'wb').write(tf_lite_model)

```
Here is the input model to convert:
[model_frozen.zip](https://github.com/tensorflow/tensorflow/files/5347801/model_frozen.zip)


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem.
Here is the traces i got:

```
Traceback (most recent call last):
  File ""convert_tflite.py"", line 61, in <module>
    tf_lite_model = converter.convert()
  File ""/home/arnaud/anaconda3/envs/FC4_CONV/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 1970, in convert
    return super(TFLiteConverter, self).convert()
  File ""/home/arnaud/anaconda3/envs/FC4_CONV/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 1339, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File ""/home/arnaud/anaconda3/envs/FC4_CONV/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 452, in _calibrate_quantize_model
    inference_output_type, allow_float, activations_type)
  File ""/home/arnaud/anaconda3/envs/FC4_CONV/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 98, in calibrate_and_quantize
    np.dtype(activations_type.as_numpy_dtype()).num)
RuntimeError: Expected bias tensor to be a vector.
```

"
43876,OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.,"import numpy as np 
import os
import tensorflow as tf
from tensorflow import keras
import tensorflow.keras.models as models
import tensorflow.keras.layers as layers
import tensorflow.keras.optimizers as optimizers
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import *
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler
from tensorflow.keras import backend


def unet(pretrained_weights = None,input_size = (256,256,1)):
    inputs = keras.Input(shape = input_size)
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)
    drop4 = Dropout(0.5)(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)

    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)
    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)
    drop5 = Dropout(0.5)(conv5)

    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))
    merge6 = concatenate([drop4,up6], axis = 3)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)

    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))
    merge7 = concatenate([conv3,up7], axis = 3)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)

    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))
    merge8 = concatenate([conv2,up8], axis = 3)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)

    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))
    merge9 = concatenate([conv1,up9], axis = 3)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)

    model = Model(inputs = inputs, outputs = conv10)

    def iou(y_pred, y_true):
        y_pred = tf.cast((y_pred > 0), dtype=tf.float32)
        i = tf.reduce_sum(y_true * y_pred)
        u = tf.reduce_sum(y_true + y_pred)
        return (i / u).item()if u != 0 else u.item()

    ssim1 = tf.image.ssim(inputs, conv10, max_val=255, filter_size=11,filter_sigma=1.5, k1=0.01, k2=0.03)
    
    model.compile(optimizer = Adam(lr = 1e-4), loss = 'ssim1', metrics = ['accuracy',iou])
    
    model.summary()


    if(pretrained_weights):
    	model.load_weights(pretrained_weights)

    return model

model = unet()"
43875,Unable to load Cats vs Dogs dataset,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No. I used the same code as is given in https://www.tensorflow.org/datasets/overview, but changed the dataset from mnist to cats_vs_dogs
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10.0.18362, 64-bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No. Using a PC
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
Tensorflow 2.3.0
Tensorflow datasets 3.2.1
- Python version:
Python 3.5
- Bazel version (if compiling from source):
NA
- GCC/Compiler version (if compiling from source):
NA
- CUDA/cuDNN version:
CUDA driver version   : 10020
- GPU model and memory:
NVIDIA GeForce 940MX 4GB

**Describe the current behavior**
I am unable to download the cats_vs_dogs dataset through the tensorflow datasets.

**Describe the expected behavior**
I should be able to load a variable with the dataset.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

ds = tfds.load('cats_vs_dogs', split = 'train', shuffle_files = True)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-10-45c82e970652> in <module>()
----> 1 ds, train = tfds.load('cats_vs_dogs', split = 'train', shuffle_files = True) #download cats vs dogs file for this projectb

~\AppData\Roaming\Python\Python35\site-packages\wrapt\wrappers.py in __call__(self, *args, **kwargs)
    565 
    566         return self._self_wrapper(self.__wrapped__, self._self_instance,
--> 567                 args, kwargs)
    568 
    569 class BoundFunctionWrapper(_FunctionWrapperBase):

C:\Anaconda\lib\site-packages\tensorflow_datasets\core\api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)
     67     _check_no_positional(fn, args, ismethod, allowed=allowed)
     68     _check_required(fn, kwargs)
---> 69     return fn(*args, **kwargs)
     70 
     71   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter

C:\Anaconda\lib\site-packages\tensorflow_datasets\core\registered.py in load(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)
    369   if download:
    370     download_and_prepare_kwargs = download_and_prepare_kwargs or {}
--> 371     dbuilder.download_and_prepare(**download_and_prepare_kwargs)
    372 
    373   if as_dataset_kwargs is None:

~\AppData\Roaming\Python\Python35\site-packages\wrapt\wrappers.py in __call__(self, *args, **kwargs)
    604 
    605             return self._self_wrapper(self.__wrapped__, self._self_instance,
--> 606                     args, kwargs)
    607 
    608         else:

C:\Anaconda\lib\site-packages\tensorflow_datasets\core\api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)
     67     _check_no_positional(fn, args, ismethod, allowed=allowed)
     68     _check_required(fn, kwargs)
---> 69     return fn(*args, **kwargs)
     70 
     71   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter

C:\Anaconda\lib\site-packages\tensorflow_datasets\core\dataset_builder.py in download_and_prepare(self, download_dir, download_config)
    374           self._download_and_prepare(
    375               dl_manager=dl_manager,
--> 376               download_config=download_config)
    377 
    378           # NOTE: If modifying the lines below to put additional information in

C:\Anaconda\lib\site-packages\tensorflow_datasets\core\dataset_builder.py in _download_and_prepare(self, dl_manager, download_config)
   1017     super(GeneratorBasedBuilder, self)._download_and_prepare(
   1018         dl_manager=dl_manager,
-> 1019         max_examples_per_split=download_config.max_examples_per_split,
   1020     )
   1021 

C:\Anaconda\lib\site-packages\tensorflow_datasets\core\dataset_builder.py in _download_and_prepare(self, dl_manager, **prepare_split_kwargs)
    949 
    950       # Prepare split will record examples associated to the split
--> 951       self._prepare_split(split_generator, **prepare_split_kwargs)
    952 
    953     # Update the info object with the splits.

C:\Anaconda\lib\site-packages\tensorflow_datasets\core\dataset_builder.py in _prepare_split(self, split_generator, max_examples_per_split)
   1032                                      hash_salt=split_generator.name)
   1033     for key, record in utils.tqdm(generator, unit="" examples"",
-> 1034                                   total=split_info.num_examples, leave=False):
   1035       example = self.info.features.encode_example(record)
   1036       writer.write(key, example)

C:\Anaconda\lib\site-packages\tqdm\notebook.py in __iter__(self, *args, **kwargs)
    232     def __iter__(self, *args, **kwargs):
    233         try:
--> 234             for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):
    235                 # return super(tqdm...) will not catch exception
    236                 yield obj

C:\Anaconda\lib\site-packages\tqdm\std.py in __iter__(self)
   1163 
   1164         try:
-> 1165             for obj in iterable:
   1166                 yield obj
   1167                 # Update and possibly print the progressbar.

C:\Anaconda\lib\site-packages\tensorflow_datasets\image_classification\cats_vs_dogs.py in _generate_examples(self, archive)
     87     """"""Generate Cats vs Dogs images and labels given a directory path.""""""
     88     num_skipped = 0
---> 89     for fname, fobj in archive:
     90       res = _NAME_RE.match(fname)
     91       if not res:  # README file, ...

C:\Anaconda\lib\site-packages\tensorflow_datasets\core\download\extractor.py in iter_zip(arch_f)
    197     for member in z.infolist():
    198       extract_file = z.open(member)
--> 199       if member.is_dir():  # Filter directories  # pytype: disable=attribute-error
    200         continue
    201       path = _normpath(member.filename)

AttributeError: 'ZipInfo' object has no attribute 'is_dir'
"
43872,[Encountered unresolved custom op: ResizeNearestNeighbor] when running tflite model ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (or github SHA if from source): 2.2.3

**Run Code**
interpreter = tf.lite.Interpreter(model_path=""model.tflite"")
interpreter.allocate_tensors()

**Error**
RuntimeError: Encountered unresolved custom op: ResizeNearestNeighbor.Node number 33 (ResizeNearestNeighbor) failed to prepare.

I looked into many things but could not find the solution.

"
43871,Inconsistent keras.layers.normalization version,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Docker image python 3.8 (debian distro) & Google Colab
- TensorFlow installed from pip
- TensorFlow version 2.3.0
- Python version: docker python 3.8.2 & Colab python 3.6.9

**Describe the current behavior**
When using `BatchNormalization `from different keras tree got a different result. In my case the model failed to converge.
`tensorflow.keras` & `tensorflow.python.keras` both using the same version of keras but if we dump the `BatchNormalization `layer the version is different. 

**Describe the expected behavior**
The same version of keras, must use the same version of normalization.

**Standalone code to reproduce the issue**
```
import sys

import tensorflow

from tensorflow import keras as k1
from tensorflow.python import keras as k2

from tensorflow.keras.layers import BatchNormalization as BN1
from tensorflow.python.keras.layers import BatchNormalization as BN2

print('python version:', sys.version)
print('tf version:', tensorflow.__version__)
print('keras version:', k1.__version__)
print('python.keras version:', k2.__version__)

batch_normalization_layer_keras = BN1()
print('keras:', batch_normalization_layer_keras)
batch_normalization_layer_python_keras = BN2()
print('python.keras:', batch_normalization_layer_python_keras)
```

**Output**
```
python version: 3.6.9 (default, Jul 17 2020, 12:50:27) 
[GCC 8.4.0]
tf version: 2.3.0
keras version: 2.4.0
python.keras version: 2.4.0
keras: <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f2aba63f390>
python.keras: <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x7f2aba63fb70>
```"
43870,Bump docker nightly cuda version to 11.1,"As the title says. docker nightly-gpu uses cuda 11.0 instead of the latest 11.1, should we bump the version to 11.1?"
43869,No zero values inside callback functions,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Jupyter macos
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the current behavior**
Inside tensorflow callback function on_epoch_end. Trying to use 0 value inside a str function like this str(0)
**Describe the expected behavior**
Should be able to capture it. print(str(0.0) does not print anything
**Standalone code to reproduce the issue**
    def on_epoch_end(self, epoch, logs=None):
        print(str(0.0))"
43866,got error while import keras libraries,"When i run the code to import the keras in anaconda 2020.07 and i am using python 3.8 and tensorflow  2.3.1 and keras 2.4.3  i got the error that is mention below: 

the solutions i tried from my side : i try to downgrade the version of python and tensorflow and keras but still i got the same error.
ImportError                               Traceback (most recent call last)
~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     63   try:
---> 64     from tensorflow.python._pywrap_tensorflow_internal import *
     65   # This try catch logic is because there is no bazel equivalent for py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
~\anaconda3\lib\site-packages\keras\__init__.py in <module>
      2 try:
----> 3     from tensorflow.keras.layers.experimental.preprocessing import RandomRotation
      4 except ImportError:

~\anaconda3\lib\site-packages\tensorflow\__init__.py in <module>
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader

~\anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>
     39 
---> 40 from tensorflow.python.eager import context
     41 

~\anaconda3\lib\site-packages\tensorflow\python\eager\context.py in <module>
     34 from tensorflow.core.protobuf import rewriter_config_pb2
---> 35 from tensorflow.python import pywrap_tfe
     36 from tensorflow.python import tf2

~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tfe.py in <module>
     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import
---> 28 from tensorflow.python import pywrap_tensorflow
     29 from tensorflow.python._pywrap_tfe import *

~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     82 above this error message when asking for help."""""" % traceback.format_exc()
---> 83   raise ImportError(msg)
     84 

ImportError: Traceback (most recent call last):
  File ""C:\Users\ankit\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-88d96843a926> in <module>
----> 1 import keras

~\anaconda3\lib\site-packages\keras\__init__.py in <module>
      3     from tensorflow.keras.layers.experimental.preprocessing import RandomRotation
      4 except ImportError:
----> 5     raise ImportError(
      6         'Keras requires TensorFlow 2.2 or higher. '
      7         'Install TensorFlow via `pip install tensorflow`')

ImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
43865,Wrong linker flags in Makefile for ARM Compiler,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 917882bfa2
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**
Recently, --fatal-warnings and --gc-sections was added as linker flags to the Makefile. These flags is not available for ARM Compiler (armclang), the corresponding flags are instead --diag_error=warning and --remove.

I propose changing the makefile to something like this, in order to use the correct flags when the ARM Compiler is used.
https://github.com/jenselofsson/tensorflow/commit/dba2406f6c3396b1d936c53f4dcd6f763b0dafbf

If I'm not mistaken, the --remove flag is the default, but could be added in order to be consistent with the gcc-flags for readability.
"
43863,Getting tensorflow.python.framework.errors_impl.NotFoundError,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No 
- OS Platform and Distribution (e.g., Windows 10):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  Not Applicable
- TensorFlow installed from (source or binary):  PyPI
- TensorFlow version (use command below): 2.3.1 (v2.3.0-54-gfcc4b966f1 2.3.1)
- Python version: 3.8.3
- Bazel version (if compiling from source): Not Applicable
- GCC/Compiler version (if compiling from source): Not Applicable
- CUDA/cuDNN version: Not applicable
- GPU model and memory: Not applicable

**Describe the current behavior**
When I try to save a tensorflow model to disk, it has recently started giving this error - ""tensorflow.python.framework.errors_impl.NotFoundError"". It used to work for over a month without issues, and nothing has changed as far as I know; but now, giving the error. I tried saving the model as a pickle file like this --> dnn_classifier.save(""/path/m.pkl"") and also without the .pkl extension like dnn_classifier.save(""/path/m"").    I observe that the ""assets"" folder and the ""saved_model.pb"" files are not getting created.

**Describe the expected behavior**
The ""assets"" folder and the ""saved_model.pb"" files should get created.

** Log **
.venvnew\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57   try:
     58     ctx.ensure_initialized()
---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:

NotFoundError: Failed to create a NewWriteableFile: <modelName>\variables\variables_temp_85e1300e45534129922b22ad101f5b87/part-00000-of-00001.data-00000-of-00001.tempstate8682566087768856168 : The system cannot find the path specified.
; No such process [Op:SaveV2]
"
43862,Reading cuda array in c++ as tensor and reading that tensor in python,"I am getting frames from camera feed, which is basically a cuda array/vector (not OpenCV mat). I want to read this array periodically in tensoflow python for further operations. To do that, first step is to read this cuda array in the form of GPU tensor in tensoflow, without transferring the array from GPU->CPU->GPU, which is basically an overhead. SInce it is cuda array is already present in the GPU, it can be done GPU->GPU. 
The issue here is which function in tensorflow api supports this GPU->GPU operation? I could not find one, if any.

The second issue is, reading this array in array in python side of code. I have gone the https://www.tensorflow.org/guide/create_op, which tells how to create the custom operations in python. But, the template is you send some array, do the operation in cuda, and receive it back in tf. Here, I am trying to read the cuda array in c++ periodically, 
Please help what should be the next step forward. 

Basically my code looks like this:
``

    #include    <unistd.h>   
    #include    <iostream>
    #include    ""VideoReader.h""
    #include <cuda_profiler_api.h>

    int main (int argc, char *argv[])
    {
     void    *fReader = NULL;
 
    //// Starting the frameReader to read the camera feed 1. 
    fReader = frameReader ::Init (  ""rtsp://admin:........"",  //camera feed url
                                                    ......................);  //some more options to read the frame
    
       do 
       {
        pOutData        = 0;
     
        // get frame from camera feed 1
        fResult = fReader::GetVideoFrame (fReader1, &pOutData);
       
       ///// pOutData is the cuda array containing the cuda vector . This vector, I need in tensorflow python for further processing.
      ///// How can I get this vector?     
    
        }
   
    } while (1);
    return 0;
    }


``"
43861,Unable to Install TF in Conda,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10
- Python version:  3.8  (64-bit)
- Installed using virtualenv? pip? conda?: conda




**Describe the problem**
When I try to install tensorflow via conda, the following error message shows.

------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------
UnsatisfiableError: The following specifications were found
to be incompatible with the existing python installation in your environment:

**Specifications:** 

  **- tensorflow-base -> python[version='>=3.5,<3.6.0a0|>=3.6,<3.7.0a0|>=3.7,<3.8.0a0']**
 **- tensorflow-datasets -> python[version='3.6.*|>=2.7,<2.8.0a0|>=3.5,<3.6.0a0|3.7.*']**

**Your python: python=3.8**

If python is on the left-most side of the chain, that's the version you've asked for.
When python appears to the right, that indicates that the thing on the left is somehow
not available for the python version you are constrained to. Note that conda will not
change your python version to a different minor version unless you explicitly specify
that.

The following specifications were found to be incompatible with each other:

Output in format: Requested package -> Available versions

Package zlib conflicts for:
python=3.8 -> sqlite[version='>=3.33.0,<4.0a0'] -> zlib[version='>=1.2.11,<1.3.0a0']
tensorflow-base -> zlib[version='>=1.2.11,<1.3.0a0']

Package requests conflicts for:
python=3.8 -> pip -> requests
tensorflow-datasets -> requests[version='>=2.19.0']The following specifications were found to be incompatible with your system:

  - feature:/win-64::__cuda==10.2=0

Your installed version is: 10.2

------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------

As far as I know, TF should support Python 3.8. But why does it show the bold error message? That makes me really confused. Anyone can help? Thanks!

_A stupid update: After creating a new environment in which I set up the Python version to be 3.7, everything works perfectly. Does it mean that TF currently only supports for (up to) Python 3.7? But indeed, I saw online that many ppl mentioned that it does support for Python 3.8 (some even mentioned the newly released Python 3.9). So why? I get even more confused ..._

"
43860,CUDA driver is not load in docker container,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **Linuix Ubuntu 20.04 LTS**:
-   **TensorFlow installed from tensorflow/tensorflow:latest-devel-gpu**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

### Describe the problem
CUDA ERROR

### Source code / logs

in my docker container
root@jupyter:/# nvidia-smi
Thu Oct  8 01:46:15 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: ERR!     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:65:00.0  On |                  N/A |
|  0%   46C    P8    16W / 250W |   2082MiB / 11177MiB |      2%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

in my PC
adminn@adminn-System-Product-Name:~$ nvidia-smi
Thu Oct  8 10:54:08 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:65:00.0  On |                  N/A |
|  0%   51C    P0    64W / 250W |   2089MiB / 11177MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1449      G   /usr/lib/xorg/Xorg                102MiB |
|    0   N/A  N/A      2786      G   /usr/lib/xorg/Xorg               1073MiB |
|    0   N/A  N/A      2916      G   /usr/bin/gnome-shell              301MiB |
|    0   N/A  N/A      4045      G   ...AAAAAAAAA= --shared-files      166MiB |
|    0   N/A  N/A      4172      G   ...AAAAAAAAA= --shared-files      429MiB |
+-----------------------------------------------------------------------------+"
43854,Issue when convert to tf lite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): 1.x


**Provide the text output from tflite_convert**

```
INFO:tensorflow:Restoring parameters from models/saved_model/variables/variables
INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}
INFO:tensorflow:input tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: input
INFO:tensorflow: tensor name: Reshape_1:0, shape: (1, 1960), type: DT_FLOAT
INFO:tensorflow:output tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: output
INFO:tensorflow: tensor name: labels_softmax:0, shape: (1, 4), type: DT_FLOAT
INFO:tensorflow:Restoring parameters from models/saved_model/variables/variables
INFO:tensorflow:Froze 4 variables.
INFO:tensorflow:Converted 4 variables to const ops.
Float model is 68048 bytes
INFO:tensorflow:Restoring parameters from models/saved_model/variables/variables
INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}
INFO:tensorflow:input tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: input
INFO:tensorflow: tensor name: Reshape_1:0, shape: (1, 1960), type: DT_FLOAT
INFO:tensorflow:output tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: output
INFO:tensorflow: tensor name: labels_softmax:0, shape: (1, 4), type: DT_FLOAT
INFO:tensorflow:Restoring parameters from models/saved_model/variables/variables
INFO:tensorflow:Froze 4 variables.
INFO:tensorflow:Converted 4 variables to const ops.
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)
   1364     try:
-> 1365       return fn(*args)
   1366     except errors.OpError as e:

11 frames
InvalidArgumentError: You must feed a value for placeholder tensor 'data_2/wav_filename' with dtype string
	 [[{{node data_2/wav_filename}}]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)
   1382                     '\nsession_config.graph_options.rewrite_options.'
   1383                     'disable_meta_optimizer = True')
-> 1384       raise type(e)(node_def, op, message)
   1385 
   1386   def _extend_graph(self):

InvalidArgumentError: You must feed a value for placeholder tensor 'data_2/wav_filename' with dtype string
	 [[node data_2/wav_filename (defined at /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'data_2/wav_filename':
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py"", line 664, in launch_instance
    app.start()
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py"", line 499, in start
    self.io_loop.start()
  File ""/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py"", line 132, in start
    self.asyncio_loop.run_forever()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 438, in run_forever
    self._run_once()
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 1451, in _run_once
    handle._run()
  File ""/usr/lib/python3.6/asyncio/events.py"", line 145, in _run
    self._callback(*self._args)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py"", line 122, in _handle_events
    handler_func(fileobj, events)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 300, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 462, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 492, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 444, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 300, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py"", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2718, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2822, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2882, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-40-582b489634f7>"", line 9, in <module>
    TESTING_PERCENTAGE, model_settings, LOGS_DIR)
  File ""/content/tensorflow/tensorflow/examples/speech_commands/input_data.py"", line 203, in __init__
    self.prepare_processing_graph(model_settings, summaries_dir)
  File ""/content/tensorflow/tensorflow/examples/speech_commands/input_data.py"", line 398, in prepare_processing_graph
    tf.string, [], name='wav_filename')
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/array_ops.py"", line 2619, in placeholder
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/gen_array_ops.py"", line 6669, in placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
43853,"getting these errors in tensorflow.contrib, TfPoseEstimator etc. while running the command ""python run_webcam.py --model=mobilenet_thin --resize=432x368 --camera=0"" . I have build the pafprocess c++ files swig successfully, but also I'm getting the errors","Traceback (most recent call last):
  File ""run.py"", line 6, in <module>
    from tf_pose import common
  File ""D:\tf-pose-estimation\tf_pose\__init__.py"", line 5, in <module>
    from tf_pose.runner import infer, Estimator, get_estimator
  File ""D:\tf-pose-estimation\tf_pose\runner.py"", line 8, in <module>
    from tf_pose import eval
  File ""D:\tf-pose-estimation\tf_pose\eval.py"", line 14, in <module>
    from tf_pose.networks import model_wh, get_graph_path
  File ""D:\tf-pose-estimation\tf_pose\networks.py"", line 6, in <module>
    from tf_pose.network_mobilenet import MobilenetNetwork
  File ""D:\tf-pose-estimation\tf_pose\network_mobilenet.py"", line 5, in <module>
    from tf_pose import network_base
  File ""D:\tf-pose-estimation\tf_pose\network_base.py"", line 8, in <module>
    import tensorflow.contrib.slim as slim
ModuleNotFoundError: No module named 'tensorflow.contrib'"
43851,AttributeError: 'Tensor' object has no attribute '_lazy_read' inside tf.while_loop containing tf.scatter_nd_update,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
--
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
v1.15.0-rc3-22-g590d6ee 1.15.0
- Python version:
Python 3.6.10
- Bazel version (if compiling from source): --
- GCC/Compiler version (if compiling from source): --
- CUDA/cuDNN version:
#define CUDNN_MAJOR 7
#define CUDNN_MINOR 6
#define CUDNN_PATCHLEVEL 0
Cuda compilation tools, release 10.0, V10.0.130
- GPU model and memory:
Asus Cerberus GTX-1070TI-A8G 8GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Implementing a simple `tf.while_loop` which contains a `tf.scatter_nd_update` function throws an error:

> AttributeError: 'Tensor' object has no attribute '_lazy_read'

The behaviour is present only in lazy mode (non-eagar execution). Also, it is not appearing when used outside of `tf.while_loop` with a fixed `j`.

**Describe the expected behavior**
I should be able to implement this fixed iteration loop without an error. In a [similar issue](https://github.com/tensorflow/tensorflow/issues/21957) the solution suggested converting Tensor to Variable which does not work in my case though.

**Standalone code to reproduce the issue**
```
import tensorflow as tf

ref = tf.Variable([[0, 1, 0, 2],
                   [0, 1, 2, 2],
                   [1, 2, 1, 3]], dtype=tf.int32)
true_array = tf.Variable([[1, 1, 1, 1]])
false_array = tf.Variable([[1, 0, 1, 0]])
num_iters = tf.Variable(3, dtype=tf.int32)  # 3


def body(ref, true_array, false_array, j, num_iters):
    samples = tf.cond(tf.equal(tf.reduce_sum(ref[j, :], axis=0), 1), lambda: true_array, lambda: false_array)
    ref = tf.scatter_nd_update(ref, [[j]], samples)
     j = tf.add(j, 1)
    return ref, true_array, false_array, j, num_iters


cond = lambda ref, true_array, false_array, j, num_iters: tf.less(j, num_iters)
j = tf.Variable(0, dtype=tf.int32)  # tf.constant(0)
ref, true_array, false_array, j, num_iters = tf.while_loop(cond, body, [ref, true_array, false_array, j, num_iters])
init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    print('ref', sess.run(ref))
    print('j', sess.run(j))
```
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43849,TFLite ConverterError: Node has inputs from different frames,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): with pip
- TensorFlow version (or github SHA if from source):  2.3.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
import os
import tensorflow as tf

export_dir = os.path.join('export_dir', 'out')

if not os.path.exists('export_dir'):
    os.mkdir('export_dir')

tf.compat.v1.enable_control_flow_v2()
tf.compat.v1.enable_v2_tensorshape()

def wrap_frozen_graph(graph_def, inputs, outputs):
    def _imports_graph_def():
        tf.compat.v1.import_graph_def(graph_def, name="""")
    wrapped_import = tf.compat.v1.wrap_function(_imports_graph_def, [])
    import_graph = wrapped_import.graph
    return wrapped_import.prune(
             tf.nest.map_structure(import_graph.as_graph_element, inputs),
             tf.nest.map_structure(import_graph.as_graph_element, outputs))

graph_def = tf.compat.v1.GraphDef()
loaded = graph_def.ParseFromString(open(os.path.join(export_dir, 'saved_model.pb'),'rb').read())

concrete_func = wrap_frozen_graph(
        graph_def, inputs=['extern_data/placeholders/data/data:0', 'extern_data/placeholders/data/data_dim0_size:0'],
        outputs=['output/output_batch_major:0'])
concrete_func.inputs[0].set_shape([1, 50])
concrete_func.inputs[1].set_shape([1])
concrete_func.outputs[0].set_shape([1, 100])

converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
converter.experimental_new_converter = True
converter.post_training_quantize=True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                               tf.lite.OpsSet.SELECT_TF_OPS]
converter.allow_custom_ops=True

tflite_model = converter.convert()

# Save the model.
if not os.path.exists('tflite'):
    os.mkdir('tflite')
output_model = os.path.join('tflite', 'model.tflite')
with open(output_model, 'wb') as f:
     f.write(tflite_model)

```

**The output from the converter invocation**

```
Traceback (most recent call last):
  File ""/home/mdigangi/environments/venv-tflite2/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 199, in toco_convert_protos
    enable_mlir_converter)
  File ""/home/mdigangi/environments/venv-tflite2/lib/python3.6/site-packages/tensorflow/lite/python/wrap_toco.py"", line 38, in wrapped_toco_convert
    enable_mlir_converter)
Exception: {{node output/rec/target0_embed/linear/embedding_lookup}} has inputs from different frames. The input {{node output/rec/target0_embed/linear/embedding_lookup/Enter}} is in frame 'output/rec/while/while_context'. The input {{node output/rec/target0_embed/linear/embedding_lookup/axis}} is in frame ''.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/home/mdigangi/bin/tflite-toolkit/tools/ckpt2pb.py"", line 52, in <module>
    tflite_model = converter.convert()
  File ""/home/mdigangi/environments/venv-tflite2/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1076, in convert
    return super(TFLiteConverterV2, self).convert()
  File ""/home/mdigangi/environments/venv-tflite2/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 900, in convert
    self).convert(graph_def, input_tensors, output_tensors)
  File ""/home/mdigangi/environments/venv-tflite2/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 633, in convert
    **converter_kwargs)
  File ""/home/mdigangi/environments/venv-tflite2/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 574, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/home/mdigangi/environments/venv-tflite2/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 202, in toco_convert_protos
    raise ConverterError(str(e))
tensorflow.lite.python.convert.ConverterError: {{node output/rec/target0_embed/linear/embedding_lookup}} has inputs from different frames. The input {{node output/rec/target0_embed/linear/embedding_lookup/Enter}} is in frame 'output/rec/while/while_context'. The input {{node output/rec/target0_embed/linear/embedding_lookup/axis}} is in frame ''.
```

**Also, please include a link to the saved model or GraphDef**

```
https://drive.google.com/file/d/1DUyxmwA4eIl5SqZ_R5jMJ1ACH3_0jOFK/view?usp=sharing
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:

- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I trained this Tranformer for NMT using [Returnn](https://github.com/rwth-i6/returnn/), which creates a TF model. It uses tf.compat.v1 for retrocompatibility. After creating the model, I freezed it using 
```
freeze_graph.py --output_graph export_dir/out/checkpoint_FROZEN.pb --clear_devices --output_node_names output/output_batch_major --input_meta_graph export_dir/0/params.003.compiled.meta --input_checkpoint export_dir/0/params.003.compiled --input_binary True
```
this script succeeds but adds the node output/rec/target0_embed/linear/embedding_lookup/axis, which creates the error. The node is added [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/convert_to_constants.py#L404-L420).  The node creating the error is the embedding layer of the decoder, which must be called recursively. I tried some quick hacks like remove the ""frame_name"" from one node, adding it to the other, or setting the first to '', but nothing of this works. 

Added 9 October 2020: if I use the concrete function for inference, it also doesn't work. Maybe it is a problem of TF2 in interpreting frozen graph produced with style of TF1

Added 12 October 2020: I tried to write a minimal code that can reproduce the issue and I've found out that maybe the problem is not due to the last node not being in the frame but the second one. This is the interesting part of the GraphDef of the graph that generates the error:
```
[name: ""source_embed_raw/linear/embedding_lookup/axis""
op: ""Const""
attr {
  key: ""_class""
  value {
    list {
      s: ""loc:@source_embed_raw/W""
    }
  }
}
attr {
  key: ""_output_shapes""
  value {
    list {
      shape {
      }
    }
  }
}
attr {
  key: ""dtype""
  value {
    type: DT_INT32
  }
}
attr {
  key: ""value""
  value {
    tensor {
      dtype: DT_INT32
      tensor_shape {
      }
      int_val: 0
    }
  }
}
, name: ""source_embed_raw/linear/embedding_lookup""
op: ""GatherV2""
input: ""source_embed_raw/W/read""
input: ""extern_data/placeholders/data/data""
input: ""source_embed_raw/linear/embedding_lookup/axis""
attr {
  key: ""Taxis""
  value {
    type: DT_INT32
  }
}
attr {
  key: ""Tindices""
  value {
    type: DT_INT32
  }
}
attr {
  key: ""Tparams""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""_class""
  value {
    list {
      s: ""loc:@source_embed_raw/W""
    }
  }
}
attr {
  key: ""_output_shapes""
  value {
    list {
      shape {
        dim {
          size: -1
        }
        dim {
          size: -1
        }
        dim {
          size: 512
        }
      }
    }
  }
}
attr {
  key: ""batch_dims""
  value {
    i: 0
  }
}
, name: ""source_embed_raw/linear/embedding_lookup/Identity""
op: ""Identity""
input: ""source_embed_raw/linear/embedding_lookup""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""_output_shapes""
  value {
    list {
      shape {
        dim {
          size: -1
        }
        dim {
          size: -1
        }
        dim {
          size: 512
        }
      }
    }
  }
}
, name: ""output/rec/target_embed_raw/linear/embedding_lookup/axis""
op: ""Const""
input: ""^output/rec/while/Identity""
attr {
  key: ""_class""
  value {
    list {
      s: ""loc:@output/rec/target_embed_raw/W""
    }
  }
}
attr {
  key: ""_output_shapes""
  value {
    list {
      shape {
      }
    }
  }
}
attr {
  key: ""dtype""
  value {
    type: DT_INT32
  }
}
attr {
  key: ""value""
  value {
    tensor {
      dtype: DT_INT32
      tensor_shape {
      }
      int_val: 0
    }
  }
}
, name: ""output/rec/target_embed_raw/linear/embedding_lookup""
op: ""GatherV2""
input: ""output/rec/target_embed_raw/linear/embedding_lookup/Enter""
input: ""output/rec/output/Reshape_2""
input: ""output/rec/target_embed_raw/linear/embedding_lookup/axis""
attr {
  key: ""Taxis""
  value {
    type: DT_INT32
  }
}
attr {
  key: ""Tindices""
  value {
    type: DT_INT32
  }
}
attr {
  key: ""Tparams""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""_class""
  value {
    list {
      s: ""loc:@output/rec/target_embed_raw/W""
    }
  }
}
attr {
  key: ""_output_shapes""
  value {
    list {
      shape {
        dim {
          size: -1
        }
        dim {
          size: 512
        }
      }
    }
  }
}
attr {
  key: ""batch_dims""
  value {
    i: 0
  }
}
, name: ""output/rec/target_embed_raw/linear/embedding_lookup/Enter""
op: ""Enter""
input: ""output/rec/target_embed_raw/W/read""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""_class""
  value {
    list {
      s: ""loc:@output/rec/target_embed_raw/W""
    }
  }
}
attr {
  key: ""_output_shapes""
  value {
    list {
      shape {
        dim {
          size: 22526
        }
        dim {
          size: 512
        }
      }
    }
  }
}
attr {
  key: ""frame_name""
  value {
    s: ""output/rec/while/while_context""
  }
}
attr {
  key: ""is_constant""
  value {
    b: true
  }
}
attr {
  key: ""parallel_iterations""
  value {
    i: 10
  }
}
, name: ""output/rec/target_embed_raw/linear/embedding_lookup/Identity""
op: ""Identity""
input: ""output/rec/target_embed_raw/linear/embedding_lookup""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""_output_shapes""
  value {
    list {
      shape {
        dim {
          size: -1
        }
        dim {
          size: 512
        }
      }
    }
  }
}
]
```
The error is about the /Axis node not being in the frame but also the Reshape child is not in the frame. 
In this small example instead:
```
import tensorflow as tf
import os
import numpy as np

def loop(m):
    x = tf.compat.v1.get_variable(""x"", dtype=tf.float32, shape=[5, 5], initializer=tf.random_uniform_initializer())
    l = tf.constant(0.005)
    m = tf.math.abs(tf.nn.embedding_lookup(x, y)) + tf.math.abs(m) + l
    return m

def cond(m):
    return tf.less(tf.math.reduce_sum(m), tf.Variable(1.0))

graph = tf.Graph()
with graph.as_default():
    with tf.compat.v1.variable_scope(""init"", reuse=tf.compat.v1.AUTO_REUSE) as scope:
        y = tf.compat.v1.placeholder(name=""y"", shape=[5], dtype=tf.int32)
        m = tf.Variable([[0.0]*5]*5, shape=[5, 5], dtype=tf.float32, name=""m"")
        z = tf.while_loop(body=loop, cond=cond, loop_vars=[m], name=""z"")[0]
    output = tf.math.reduce_sum(z, name=""output"")

nodes = [n for n in graph.as_graph_def().node if ""embedding_lookup"" in n.name]
print(nodes)
with tf.compat.v1.Session(graph=graph) as sess:
    sess.run(tf.compat.v1.global_variables_initializer())
    print(sess.run(output, feed_dict={y: [0, 1, 2, 3, 4]}))
    tf.compat.v1.saved_model.simple_save(sess, os.path.join(""export_dir"", ""0""), inputs={""y"": y}, outputs={""output"": output})
```
```
[name: ""init/z/embedding_lookup/axis""
op: ""Const""
input: ""^init/z/Identity""
attr {
  key: ""_class""
  value {
    list {
      s: ""loc:@init/x""
    }
  }
}
attr {
  key: ""dtype""
  value {
    type: DT_INT32
  }
}
attr {
  key: ""value""
  value {
    tensor {
      dtype: DT_INT32
      tensor_shape {
      }
      int_val: 0
    }
  }
}
, name: ""init/z/embedding_lookup""
op: ""GatherV2""
input: ""init/z/embedding_lookup/Enter""
input: ""init/z/embedding_lookup/Enter_1""
input: ""init/z/embedding_lookup/axis""
attr {
  key: ""Taxis""
  value {
    type: DT_INT32
  }
}
attr {
  key: ""Tindices""
  value {
    type: DT_INT32
  }
}
attr {
  key: ""Tparams""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""_class""
  value {
    list {
      s: ""loc:@init/x""
    }
  }
}
attr {
  key: ""batch_dims""
  value {
    i: 0
  }
}
, name: ""init/z/embedding_lookup/Enter""
op: ""Enter""
input: ""init/x/read""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""_class""
  value {
    list {
      s: ""loc:@init/x""
    }
  }
}
attr {
  key: ""frame_name""
  value {
    s: ""init/z/while_context""
  }
}
attr {
  key: ""is_constant""
  value {
    b: true
  }
}
attr {
  key: ""parallel_iterations""
  value {
    i: 10
  }
}
, name: ""init/z/embedding_lookup/Enter_1""
op: ""Enter""
input: ""init/y""
attr {
  key: ""T""
  value {
    type: DT_INT32
  }
}
attr {
  key: ""_class""
  value {
    list {
      s: ""loc:@init/x""
    }
  }
}
attr {
  key: ""frame_name""
  value {
    s: ""init/z/while_context""
  }
}
attr {
  key: ""is_constant""
  value {
    b: true
  }
}
attr {
  key: ""parallel_iterations""
  value {
    i: 10
  }
}
, name: ""init/z/embedding_lookup/Identity""
op: ""Identity""
input: ""init/z/embedding_lookup""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
]
```
The GatherV2 Op has an Enter and a RefEnter input, both having the same frame, besides the Axis input. In this case it converts successfully to TFLite and the axis not being in the frame does not cause any errors. This works despite the lack of output shapes in this second GraphDef. Any ideas?

Update 13 October 2020: If I don't set tf.enable_control_flow_v2() the conversion succeeds but then the interpreter is not able to use the model."
43848,Building micro_speech for Arm Mbed OS using ARM Compiler (armclang) fails,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 917882bfa2
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm Mbed OS

**Describe the problem**
Building micro_speech with the commands below results in this error:
`./tensorflow/lite/micro/tools/make/downloads/kissfft/kiss_fft.h:45:10: fatal error: 'sys/types.h' file not found`

Switching out ARMC6 for GCC_ARM in the final command fixes the issue.
The issue comes from sys/types.h not being available in the ARM Compiler toolchain, while it is available when using gcc. 

Patching kissfft in the download_and_extract.sh-script to remove the `#include <sys/types.h>` fixes the issue (https://github.com/jenselofsson/tensorflow/commit/080bc19079f25a2f2151e67d303f1f1949bcfc3e), and it compiles using both GCC_ARM and ARMC6.

**Please provide the exact sequence of commands/steps when you ran into the problem**
$ make -f tensorflow/lite/micro/tools/make/Makefile TAGS=""cmsis-nn disco_f746ng"" generate_micro_speech_mbed_project
$ cd tensorflow/lite/micro/tools/make/gen/linux_x86_64/prj/micro_speech/mbed/
$ mbed config root .
$ mbed deploy
$ mbed compile -m DISCO_F746NG -t ARMC6
"
43847,Go installation error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 32
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.3.0
- Python version:3.8.5
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 11.0
- GPU model and memory: 1660 gtx super (6GB)
- GO version: 1.15.2

Hello i'm trying to install the tensorflow go version to build a little application. So i installed the C library (2.3.0 version tested with classic hello.c example) and after that i run the following command:

`go get github.com/tensorflow/tensorflow/tensorflow/go`

but after this one i get this error:

```bash
go: found github.com/tensorflow/tensorflow/tensorflow/go in github.com/tensorflow/tensorflow v2.3.1+incompatible
go: finding module for package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto
go: finding module for package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto
../../../../golang/gopath/pkg/mod/github.com/tensorflow/tensorflow@v2.3.1+incompatible/tensorflow/go/saved_model.go:25:2: module github.com/tensorflow/tensorflow@latest found (v2.3.1+incompatible), but does not contain package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto
```

could tou help me?


Go ENVS:
```bash
GO111MODULE=""on""
GOARCH=""amd64""
GOBIN=""""
GOCACHE=""/home/fedyfausto/.cache/go-build""
GOENV=""/home/fedyfausto/.config/go/env""
GOEXE=""""
GOFLAGS=""""
GOHOSTARCH=""amd64""
GOHOSTOS=""linux""
GOINSECURE=""""
GOMODCACHE=""/home/fedyfausto/golang/gopath/pkg/mod""
GONOPROXY=""""
GONOSUMDB=""""
GOOS=""linux""
GOPATH=""/home/fedyfausto/golang/gopath""
GOPRIVATE=""""
GOPROXY=""https://proxy.golang.org,direct""
GOROOT=""/home/fedyfausto/golang/latest/go""
GOSUMDB=""sum.golang.org""
GOTMPDIR=""""
GOTOOLDIR=""/home/fedyfausto/golang/latest/go/pkg/tool/linux_amd64""
GCCGO=""gccgo""
AR=""ar""
CC=""gcc""
CXX=""g++""
CGO_ENABLED=""1""
GOMOD=""/home/fedyfausto/Lavoro/Project/go_predictor_2/project_name/go.mod""
CGO_CFLAGS=""-g -O2""
CGO_CPPFLAGS=""""
CGO_CXXFLAGS=""-g -O2""
CGO_FFLAGS=""-g -O2""
CGO_LDFLAGS=""-g -O2""
PKG_CONFIG=""pkg-config""
GOGCCFLAGS=""-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build398631669=/tmp/go-build -gno-record-gcc-switches""
```
"
43846,Access to GPU memory usage,"**System information**
- TensorFlow version (you are using): TensorFlow 2.3.1
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
I would like an easy way to access current/peak GPU memory usage. The TF profiler only allows detailed analysis of a single run, while accessing (and saving) the memory usage would allow users to track and compare the resource consumption of multiple different runs and models. Both PyTorch (`torch.cuda.max_memory_allocated()`) and TensorFlow 1 (`tf.contrib.memory_stats.MaxBytesInUse()`) allow easy access to this, only TensorFlow 2 doesn't.

**Will this change the current api? How?**
Yes, it would add a method that returns current/peak GPU memory consumption.

**Who will benefit with this feature?**
Anyone caring about memory consumption differences (which are probably a lot of people).

**Any Other info.**
This has been discussed in the past, e.g. in https://github.com/tensorflow/serving/issues/1407 or https://stackoverflow.com/questions/59652889/how-to-get-the-exact-gpu-memory-usage-for-keras.
"
43845,"""No gradients provided"" with GradientDescentOptimizer if not using float variable","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): pip
- Python version: 3.7.6
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version:  10.1
- GPU model and memory: Nvidia Geforce 940MX

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
The gradient descent optimizer gives ""No gradients provided"" error when minimizing it, if the variables used in the custom loss function are not explicitly defined as float

**Describe the expected behavior**
It should optimize the variable as expected.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import numpy as np
import tensorflow as tf
from tensorflow.python.training import gradient_descent
x = tf.Variable(np.array([[1,1],[2,2]]), trainable=True)
def my_loss():
    square = tf.math.square(x)
    return square
opt = gradient_descent.GradientDescentOptimizer(0.1)
for i in range(20):
    print([x.numpy(), my_loss().numpy()])
    train = opt.minimize(my_loss, var_list=[x])
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-165-a3994377e283> in <module>
      1 for i in range(20):
      2     print([x.numpy(), y.numpy(), my_loss().numpy()])
----> 3     train = opt.minimize(my_loss, var_list=[x,y])

/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)
    408           ""No gradients provided for any variable, check your graph for ops""
    409           "" that do not support gradients, between variables %s and loss %s."" %
--> 410           ([str(v) for _, v in grads_and_vars], loss))
    411 
    412     return self.apply_gradients(grads_and_vars, global_step=global_step,

ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [""<tf.Variable 'Variable:0' shape=(2, 2) dtype=int64, numpy=\narray([[1, 1],\n       [2, 2]])>"", ""<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\narray([[9.000003, 9.000003],\n       [9.000003, 9.000003]], dtype=float32)>""] and loss <function my_loss at 0x7f5f1c381680>.
```"
43843,Openpose installation issue ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- 
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version: 3.6.10
- Installed using virtualenv? pip? conda?:No
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: 11.1
- GPU model and memory: [10.18.10.4358]
![Anaconda error](https://user-images.githubusercontent.com/71690984/95325961-80f13800-08bf-11eb-9b28-ee40e005bb03.png)
![anaconda](https://user-images.githubusercontent.com/71690984/95325968-82226500-08bf-11eb-8ca8-21f5b1377ec1.png)





**Issue**

**I constantly encounter issues while installing openpose on windows using Anaconda using this link: https://www.youtube.com/watch?v=4FZrE3cmTPA**


****
In Anaconda, when running this command; python run_webcam.py camera --video.mp4,
error message comes. Attaching error message.
![Anaconda error](https://user-images.githubusercontent.com/71690984/95326026-98302580-08bf-11eb-85f4-760faf7c2260.png)
![anaconda](https://user-images.githubusercontent.com/71690984/95326030-99f9e900-08bf-11eb-99b7-92fbb013d339.png)


"
43842,"OP_REQUIRES failed at sparse_tensor_dense_matmul_op.cc:146 : Invalid argument: k (784) from index[4352,1] out of bounds (>=784)","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): [Referred code repository](https://github.com/WojciechMormul/deep-compression) 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Installed using conda 
- TensorFlow version (use command below): 1.15
- Python version: 3.6.12
- CUDA/cuDNN version: 11.1 
- GPU model and memory:

**Conda list of packages installed**
`# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                        main  
_tflow_select             2.3.0                       mkl  
absl-py                   0.10.0           py36h9f0ad1d_0    conda-forge
argon2-cffi               20.1.0           py36h7b6447c_1    anaconda
astor                     0.8.1              pyh9f0ad1d_0    conda-forge
async_generator           1.10             py36h28b3542_0    anaconda
attrs                     20.2.0                     py_0    anaconda
backcall                  0.2.0                      py_0    anaconda
bleach                    3.2.1                      py_0    anaconda
c-ares                    1.16.1               h516909a_3    conda-forge
ca-certificates           2020.7.22                     0    anaconda
certifi                   2020.6.20                py36_0    anaconda
cffi                      1.14.3           py36he30daa8_0    anaconda
dbus                      1.13.12              h746ee38_0    anaconda
decorator                 4.4.2                      py_0    anaconda
defusedxml                0.6.0                      py_0    anaconda
entrypoints               0.3                      py36_0    anaconda
expat                     2.2.9                he6710b0_2    anaconda
fontconfig                2.13.0               h9420a91_0    anaconda
freetype                  2.10.2               h5ab3b9f_0    anaconda
gast                      0.2.2                      py_0    conda-forge
glib                      2.56.2               hd408876_0    anaconda
google-pasta              0.2.0              pyh8c360ce_0    conda-forge
grpcio                    1.31.0           py36h769ab6c_0    conda-forge
gst-plugins-base          1.14.0               hbbd80ab_1    anaconda
gstreamer                 1.14.0               hb453b48_1    anaconda
h5py                      2.10.0          nompi_py36hecadee3_104    conda-forge
hdf5                      1.10.6          nompi_h3c11f04_101    conda-forge
icu                       58.2                 he6710b0_3    anaconda
importlib-metadata        2.0.0            py36h9f0ad1d_0    conda-forge
ipykernel                 5.3.4            py36h5ca1d4c_0    anaconda
ipython                   7.16.1           py36h5ca1d4c_0    anaconda
ipython_genutils          0.2.0                    py36_0    anaconda
ipywidgets                7.5.1                      py_0    anaconda
jedi                      0.17.2                   py36_0    anaconda
jinja2                    2.11.2                     py_0    anaconda
jpeg                      9b                   habf39ab_1    anaconda
jsonschema                3.0.2                    py36_0    anaconda
jupyter                   1.0.0                    py36_7    anaconda
jupyter_client            6.1.7                      py_0    anaconda
jupyter_console           6.2.0                      py_0    anaconda
jupyter_core              4.6.3                    py36_0    anaconda
jupyterlab_pygments       0.1.1                      py_0    anaconda
keras-applications        1.0.8                      py_1    conda-forge
keras-preprocessing       1.1.0                      py_0    conda-forge
ld_impl_linux-64          2.33.1               h53a641e_7  
libblas                   3.8.0               17_openblas    conda-forge
libcblas                  3.8.0               17_openblas    conda-forge
libedit                   3.1.20191231         h14c3975_1  
libffi                    3.3                  he6710b0_2  
libgcc-ng                 9.1.0                hdf63c60_0  
libgfortran-ng            7.5.0               hdf63c60_16    conda-forge
liblapack                 3.8.0               17_openblas    conda-forge
libopenblas               0.3.10          pthreads_hb3c22a3_4    conda-forge
libpng                    1.6.37               hbc83047_0    anaconda
libprotobuf               3.13.0               h8b12597_0    conda-forge
libsodium                 1.0.18               h7b6447c_0    anaconda
libstdcxx-ng              9.1.0                hdf63c60_0  
libuuid                   1.0.3                h1bed415_2    anaconda
libxcb                    1.14                 h7b6447c_0    anaconda
libxml2                   2.9.10               he19cac6_1    anaconda
markdown                  3.2.2                      py_0    conda-forge
markupsafe                1.1.1            py36h7b6447c_0    anaconda
mistune                   0.8.4            py36h7b6447c_0    anaconda
nbclient                  0.5.0                      py_0    anaconda
nbconvert                 6.0.6                    py36_0    anaconda
nbformat                  5.0.7                      py_0    anaconda
ncurses                   6.2                  he6710b0_1  
nest-asyncio              1.4.0                      py_1    anaconda
notebook                  6.1.1                    py36_0    anaconda
numpy                     1.19.1           py36h3849536_2    conda-forge
openssl                   1.1.1h               h7b6447c_0    anaconda
opt_einsum                3.3.0                      py_0    conda-forge
packaging                 20.4                       py_0    anaconda
pandoc                    2.10.1                        0    anaconda
pandocfilters             1.4.2                    py36_1    anaconda
parso                     0.7.0                      py_0    anaconda
pcre                      8.44                 he6710b0_0    anaconda
pexpect                   4.8.0                    py36_0    anaconda
pickleshare               0.7.5                    py36_0    anaconda
pip                       20.2.2                   py36_0  
prometheus_client         0.8.0                      py_0    anaconda
prompt-toolkit            3.0.7                      py_0    anaconda
prompt_toolkit            3.0.7                         0    anaconda
protobuf                  3.13.0           py36h831f99a_0    conda-forge
ptyprocess                0.6.0                    py36_0    anaconda
pycparser                 2.20                       py_2    anaconda
pygments                  2.7.1                      py_0    anaconda
pyparsing                 2.4.7                      py_0    anaconda
pyqt                      5.9.2            py36h22d08a2_1    anaconda
pyrsistent                0.17.3           py36h7b6447c_0    anaconda
python                    3.6.12               hcff3b4d_2  
python-dateutil           2.8.1                      py_0    anaconda
python_abi                3.6                     1_cp36m    conda-forge
pyzmq                     19.0.2           py36he6710b0_1    anaconda
qt                        5.9.7                h5867ecd_1    anaconda
qtconsole                 4.7.7                      py_0    anaconda
qtpy                      1.9.0                      py_0    anaconda
readline                  8.0                  h7b6447c_0  
scipy                     1.5.2            py36h3a855aa_0    conda-forge
send2trash                1.5.0                    py36_0    anaconda
setuptools                49.6.0                   py36_1  
sip                       4.19.24          py36he6710b0_0    anaconda
six                       1.15.0             pyh9f0ad1d_0    conda-forge
sqlite                    3.33.0               h62c20be_0  
tensorboard               1.15.0                   py36_0    conda-forge
tensorflow                1.15.0          mkl_py36h4920b83_0  
tensorflow-base           1.15.0          mkl_py36he1670d9_0  
tensorflow-estimator      1.15.1             pyh2649769_0  
termcolor                 1.1.0                      py_2    conda-forge
terminado                 0.8.3                    py36_0    anaconda
testpath                  0.4.4                      py_0    anaconda
tk                        8.6.10               hbc83047_0  
tornado                   6.0.4            py36h7b6447c_1    anaconda
traitlets                 4.3.3                    py36_0    anaconda
wcwidth                   0.2.5                      py_0    anaconda
webencodings              0.5.1                    py36_1    anaconda
werkzeug                  0.16.1                     py_0    conda-forge
wheel                     0.35.1                     py_0  
widgetsnbextension        3.5.1                    py36_0    anaconda
wrapt                     1.12.1           py36h8c4c3a4_1    conda-forge
xz                        5.2.5                h7b6447c_0  
zeromq                    4.3.2                he6710b0_3    anaconda
zipp                      3.2.0                      py_0    conda-forge
zlib                      1.2.11               h7b6447c_3`

**Describe the current behavior**
I am working on neural network pruning and the problem occurs when finding accuracy from the pruned weights. 

The code fails when it runs the `batch_acc = sess.run(accuracy,feed_dict={x_PH: batch_x, labels: batch_y})` in the deploy code. 

The training goes fine on the MNIST dataset. As I use deploy to find the accuracy from the pruned weights, I get an error on the multiplication between Sparse Tensor and Tensor. 

There are already issues on this. Found a stackoverflow thread on the same. 
https://stackoverflow.com/questions/34030140/is-sparse-tensor-multiplication-implemented-in-tensorflow

The error occurs at `tf.sparse.sparse_dense_matmul(w,x)` line.

`	def forward_matmul(self, x):
		
		if self.dense == False:
			w = tf.sparse.transpose(self.w_matrix, (1, 0))
			x = tf.transpose(x, (1, 0))
			x = tf.sparse.sparse_dense_matmul(w, x) # only left matrix can be sparse hence transpositions ### Changing tf.sparse.matmul() to tf.sparse.sparse_dense_matmul()
			x = tf.transpose(x, (1, 0))
		else:
			x = tf.matmul(x, self.w_matrix)
		
		return x`

**Describe the expected behavior**
The code should be able to predict the accuracy from the newly trained weights 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Please use the modified repository, which was initially failing because of the type conversion in the default code.
https://github.com/sachinkm308/deep-compression_v3/tree/v1.0 


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


`(mlp8) mohan:~/mlp/git/deep-compression_v3$ python deploy.py 
WARNING:tensorflow:From /home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
layer: conv1
	valid matrix weights: 800
	total tensor weights: 800
	total matrix weights: 4917248
Tensor(""Reshape:0"", shape=(?, 784), dtype=float32)
x -  Tensor(""Shape:0"", shape=(2,), dtype=int32)
Tensor(""transpose_1:0"", shape=(784, ?), dtype=float32)
layer: conv2
	valid matrix weights: 51200
	total tensor weights: 51200
	total matrix weights: 19668992
x -  Tensor(""Shape_1:0"", shape=(2,), dtype=int32)
Tensor(""transpose_5:0"", shape=(6272, ?), dtype=float32)
layer: fc1
	valid matrix weights: 3211264.0
	total matrix weights: 3211264
WARNING:tensorflow:From /home/mohan/mlp/git/deep-compression_v3/layers.py:223: The name tf.sparse.matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.

layer: fc2
	valid matrix weights: 10240.0
	total matrix weights: 10240
WARNING:tensorflow:From deploy.py:50: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From /home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST_data/train-images-idx3-ubyte.gz
WARNING:tensorflow:From /home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST_data/train-labels-idx1-ubyte.gz
WARNING:tensorflow:From /home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.one_hot on tensors.
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From /home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
2020-10-07 12:58:33.749107: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-07 12:58:33.770019: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3999980000 Hz
2020-10-07 12:58:33.770318: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55891ca32840 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-07 12:58:33.770345: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-7
OMP: Info #156: KMP_AFFINITY: 8 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 1 packages x 4 cores/pkg x 2 threads/core (4 total cores)
OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 0 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 1 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 2 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 3 thread 1 
OMP: Info #250: KMP_AFFINITY: pid 2931 tid 2931 thread 0 bound to OS proc set 0
2020-10-07 12:58:33.779852: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
OMP: Info #250: KMP_AFFINITY: pid 2931 tid 3127 thread 1 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 2931 tid 3147 thread 2 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 2931 tid 3148 thread 3 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 2931 tid 3149 thread 4 bound to OS proc set 4
2020-10-07 12:58:39.482282: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at sparse_tensor_dense_matmul_op.cc:146 : Invalid argument: k (784) from index[4352,1] out of bounds (>=784)
Traceback (most recent call last):
  File ""/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call
    return fn(*args)
  File ""/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn
    target_list, run_metadata)
  File ""/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: k (784) from index[4352,1] out of bounds (>=784)
	 [[{{node SparseTensorDenseMatMul/SparseTensorDenseMatMul}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""deploy.py"", line 62, in <module>
    batch_acc = sess.run(accuracy,feed_dict={x_PH: batch_x, labels: batch_y})
  File ""/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: k (784) from index[4352,1] out of bounds (>=784)
	 [[node SparseTensorDenseMatMul/SparseTensorDenseMatMul (defined at /home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'SparseTensorDenseMatMul/SparseTensorDenseMatMul':
  File ""deploy.py"", line 25, in <module>
    x = tf.nn.relu(L1.forward_matmul(x))
  File ""/home/mohan/mlp/git/deep-compression_v3/layers.py"", line 353, in forward_matmul
    x = tf.sparse.sparse_dense_matmul(w, x, adjoint_a=False, adjoint_b=False, name=None) # only left matrix can be sparse hence transpositions ### Changing tf.sparse.matmul() to tf.sparse.sparse_dense_matmul()
  File ""/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/ops/sparse_ops.py"", line 2405, in sparse_tensor_dense_matmul
    adjoint_b=adjoint_b)
  File ""/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_sparse_ops.py"", line 3063, in sparse_tensor_dense_mat_mul
    adjoint_b=adjoint_b, name=name)
  File ""/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()`"
43840,regression in layer names for tf-operation layers in current tf-nightly (2.4) ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.0-dev20201006
- Python version: 3.7

**Describe the current behavior**
In the last tf-nightly release, when attributing a name to a layer created by a tf operation, this naming does not seem anymore to be effective anymore. 

**Describe the expected behavior**
the naming of the layer shall work as expected, as for tf 2.3

**Standalone code to reproduce the issue**
```
import tensorflow as tf
print(tf.__version__)

test = tf.keras.Input((1,), name='input1')
test2 = tf.identity(test, name='abcd')
print(test.name)
print(test2.name)
```

**Other info / logs** 
```
2.4.0-dev20201005
input1
tf.identity/Identity:0
```
"
43839,Unexpected output shape when trying to convert to Frozen Graph using convert_variables_to_constants_v2,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary):  Using Deep Learning VM Instance on GCP. Not sure.
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7.0
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 11.0
- GPU model and memory: Tesla T4 16GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I am using this [answer](https://stackoverflow.com/a/61004639/5052482) by TF_Support on StackOverflow to convert a model to to a frozen graph. However, the output shape that I am getting for my custom model is is very different (weird) from the one that's there in my model. More information in the code snippet below.

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf
from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2

def blah():
    jumps = tf.keras.layers.Input(shape=(269, 8))
    loc_a = tf.keras.layers.Input(shape=(32))
    loc_b = tf.keras.layers.Input(shape=(256))
    loc_c = tf.keras.layers.Input(shape=(2))
    def conv_block(x):
        x = tf.keras.layers.Conv1D(filters=32, kernel_size=1, padding='same')(x)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Activation('linear')(x)
        return x
    
    x = jumps
    for i in range(1):
        x = conv_block(x)
    _loc_a = tf.keras.layers.Dense(269)(loc_a)
    _loc_b = tf.keras.layers.Dense(269)(loc_b)
    _loc_c = tf.keras.layers.Dense(269)(loc_c)
    _loc = tf.keras.layers.Concatenate()([_loc_a, _loc_b, _loc_c])
    _loc = tf.keras.layers.Dense(269)(_loc)
    _loc = tf.keras.layers.Reshape((269, 1))(_loc)
    
    x = tf.keras.layers.Concatenate()([_loc, x])
    x= tf.keras.layers.Dense(9)(x)
#     x= tf.keras.layers.Dense(9)(x)
#     x = tf.keras.layers.Flatten()(x)
    
    return tf.keras.Model(inputs=[jumps, loc_a, loc_b, loc_c], outputs=[x])

model = blah()

model.summary()

Model: ""model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 32)]         0                                            
__________________________________________________________________________________________________
input_3 (InputLayer)            [(None, 256)]        0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            [(None, 2)]          0                                            
__________________________________________________________________________________________________
dense (Dense)                   (None, 269)          8877        input_2[0][0]                    
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 269)          69133       input_3[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 269)          807         input_4[0][0]                    
__________________________________________________________________________________________________
input_1 (InputLayer)            [(None, 269, 8)]     0                                            
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 807)          0           dense[0][0]                      
                                                                 dense_1[0][0]                    
                                                                 dense_2[0][0]                    
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 269, 32)      288         input_1[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 269)          217352      concatenate[0][0]                
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 269, 32)      128         conv1d[0][0]                     
__________________________________________________________________________________________________
reshape (Reshape)               (None, 269, 1)       0           dense_3[0][0]                    
__________________________________________________________________________________________________
activation (Activation)         (None, 269, 32)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 269, 33)      0           reshape[0][0]                    
                                                                 activation[0][0]                 
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 269, 9)       306         concatenate_1[0][0]              
==================================================================================================
Total params: 296,891
Trainable params: 296,827
Non-trainable params: 64

# convert model to a frozen graph

full_model = tf.function(lambda x: model(x))

full_model = full_model.get_concrete_function([tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype, name=""input_0""),
                                              tf.TensorSpec(model.inputs[1].shape, model.inputs[0].dtype, name=""input_1""),
                                              tf.TensorSpec(model.inputs[2].shape, model.inputs[0].dtype, name=""input_2""),
                                              tf.TensorSpec(model.inputs[3].shape, model.inputs[0].dtype, name=""input_3"")])

frozen_func = convert_variables_to_constants_v2(full_model)
frozen_func.graph.as_graph_def(add_shapes=True)
layers = [op.name for op in frozen_func.graph.get_operations()]
print(""-"" * 50)
# print(""Frozen model layers: "")
# for layer in layers:
#     print(layer)
print(""-"" * 50)
print(""Frozen model inputs: "")
print(frozen_func.inputs)
print(""Frozen model outputs: "")
print(frozen_func.outputs)
# Save frozen graph from frozen ConcreteFunction to hard drive
tf.io.write_graph(graph_or_graph_def=frozen_func.graph,
                  logdir=""./frozen_models"",
                  name=""frozen_graph.pb"",
                  as_text=False)

Frozen model inputs: 
[<tf.Tensor 'input_0:0' shape=(None, 269, 8) dtype=float32>, <tf.Tensor 'input_1:0' shape=(None, 32) dtype=float32>, <tf.Tensor 'input_2:0' shape=(None, 256) dtype=float32>, <tf.Tensor 'input_3:0' shape=(None, 2) dtype=float32>]
Frozen model outputs: 
[<tf.Tensor 'Identity:0' shape=(None, None, 9) dtype=float32>]
'./frozen_models/frozen_graph.pb'

```

The output shape of the Frozen Model should match the output shape of the model created. However it does not. 

The output shape of the model should be `(None, 269, 9)` whereas the output shape of the Frozen Graph is `(None, None, 9)`

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43838,RPC retries do not work on Istio,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, but it fits on one screen
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): stock `tensorflow/tensorflow:2.2.0` docker image on k8s
- TensorFlow installed from (source or binary): binary (docker image)
- TensorFlow version (use command below): `v2.2.0-rc4-8-g2b96f3662b 2.2.0`
- Python version: `Python 3.6.9`

**Describe the current behavior**

When running a two-worker job *in an istio-enabled namespace* on kubernetes, tensorflow will crash in `tf.keras.layers.Dense()` with `tensorflow.python.framework.errors_impl.UnavailableError` with a very high probability *if worker `0` is not available yet at the moment where worker `1` sends an RPC to it*. Note that in a distributed environment perfect synchronization of tasks is impossible to achieve due to scheduling contraints, network issues, etc. In my experiments the crashed happend when worker `0` started executing more than a second later than worker `1`. The example code injects an artificial 10 second delay when running on task `1`.

**Describe the expected behavior**

Worker 1 should retry for a reasonable amount of time (a minute or so?) until worker 0 comes up. This is indeed what happens when *not* running on Istio (e.g. when istio sidecar injection is disabled for the pods by changing the `sidecar.istio.io/inject` annotation in attached manifest to `""false""`) then this example job succeeds every time.

My suspicion is that the retry code (either in gRPC layer or in tensorflow itself) gets confused in case there is an envoy transparent proxy in between the workers:
- when there is no istio, the process gets a `-ECONNREFUSED`, and knows how to retry this,
- where there is istio, the process establishes a connection (to the local transparent proxy) just fine, but then the proxy reports back a connection error in a higher layer, e.g.:
```
':status', '200'
'content-type', 'application/grpc'
'grpc-status', '14'
'grpc-message', 'upstream connect error or disconnect/reset before headers. reset reason: connection failure'
'server', 'envoy'
'x-envoy-upstream-service-time', '36'
```
and the resulting exception looks different enough for the retry code to not kick in, causing an immediate crash.

**Standalone code to reproduce the issue**

Here is a kubernetes manifest sufficient to reproduce the issue. This includes the code, which just creates a model and attempts to compile it.

**Note:** this needs to be applied against a namespace in which istio sidecar injection is enabled (with `kubectl apply -n YOUR_NS -f` or similar).

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: script
data:
  mnist-tensorflow.py: |
    from __future__ import absolute_import, division, print_function, unicode_literals

    import logging
    import time
    import sys

    import tensorflow as tf


    def main(pod_name):
        logging.info(""Started!"")
        tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.DEBUG)

        logging.info(""pod name: [%s]"", pod_name)
        if pod_name.endswith('0'):
            logging.warning(""sleeping 10 seconds"")
            time.sleep(10)

        logging.warning(""proceeding"")
        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
        logging.warning(""strategy created"")

        with strategy.scope():
            model = tf.keras.Sequential(
                [
                    tf.keras.layers.Conv2D(32, 3, activation=""relu"", input_shape=(28, 28, 1)),
                    tf.keras.layers.MaxPooling2D(),
                    tf.keras.layers.Flatten(),
                    tf.keras.layers.Dense(64, activation=""relu""),
                    tf.keras.layers.Dense(10),
                ]
            )
            model.compile(
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                optimizer=tf.keras.optimizers.SGD(learning_rate=0.5, momentum=0.1),
                metrics=[""accuracy""])

        logging.info(""Success!"")


    if __name__ == ""__main__"":
        logging.basicConfig(format='%(asctime)s.%(msecs)03d %(levelname)-8s %(message)s', level=logging.INFO,
                            datefmt='%Y-%m-%d %H:%M:%S')
        main(sys.argv[1])
---
apiVersion: kubeflow.org/v1
kind: TFJob
metadata:
  name: tf-job-mnist
spec:
  cleanPodPolicy: None  # For easy debugging
  tfReplicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: Never
      template:
        metadata:
          annotations:
            sidecar.istio.io/logLevel: ""debug""
            sidecar.istio.io/inject: ""true""
        spec:
          containers:
          - name: tensorflow
            image: tensorflow/tensorflow:2.2.0
            command: [""python"", ""-u"", ""/tmp/script/mnist-tensorflow.py"", ""$(POD_NAME)""]
            env:
            - name: TF_CPP_MIN_LOG_LEVEL
              value: ""0""
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            volumeMounts:
            - name: script
              mountPath: /tmp/script
          volumes:
          - name: script
            configMap:
              name: script
```


**Other info / logs**

Here is the python stack trace that I encounter. Sometimes the error message varies slightly.
```
Traceback (most recent call last):
  File ""/tmp/tests/jobs/mnist-tensorflow.py"", line 44, in <module>
    main(sys.argv[1])
  File ""/tmp/tests/jobs/mnist-tensorflow.py"", line 30, in main
    tf.keras.layers.Dense(10),
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 456, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py"", line 129, in __init__
    self.add(layer)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 456, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py"", line 198, in add
    layer(x)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 897, in __call__
    self._maybe_build(inputs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 2416, in _maybe_build
    self.build(input_shapes)  # pylint:disable=not-callable
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 163, in build
    dtype=self.dtype)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 577, in add_weight
    caching_device=caching_device)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 743, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 141, in make_variable
    shape=variable_shape if variable_shape else None)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py"", line 259, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py"", line 220, in _variable_v1_call
    shape=shape)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py"", line 66, in getter
    return captured_getter(captured_previous, **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 1767, in creator_with_resource_vars
    return self._create_variable(next_creator, **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 610, in _create_variable
    values.SyncOnReadVariable, **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/values.py"", line 694, in create_mirrored_variable
    value_list = real_mirrored_creator(**kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 602, in _real_mirrored_creator
    v = next_creator(**kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py"", line 198, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 2598, in default_variable_creator
    shape=shape)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py"", line 263, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1434, in __init__
    distribute_strategy=distribute_strategy)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1567, in _init_from_args
    initial_value() if init_from_fn else initial_value,
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 386, in initial_value_fn
    collective_instance_key)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/collective_ops.py"", line 176, in broadcast_recv
    communication_hint=communication_hint.lower())
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_collective_ops.py"", line 57, in collective_bcast_recv
    _ops.raise_from_not_ok_status(e, name)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 6653, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnavailableError: upstream connect error or disconnect/reset before headers. reset reason: connection failure
Additional GRPC error information:
{""created"":""@1601905369.391893191"",""description"":""Error received from peer ipv4:192.168.197.248:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""upstream connect error or disconnect/reset before headers. reset reason: connection failure"",""grpc_status"":14} [Op:CollectiveBcastRecv]
```

The interesting pieces of envoy logs are between `2020-10-05T13:42:49.336` (when TF makes the call) and `2020-10-05T13:42:49.391` (where it crashes):
- [fail-log-pod.tf-job-mnist-worker-1.istio-proxy.txt](https://github.com/tensorflow/tensorflow/files/5338766/fail-log-pod.tf-job-mnist-worker-1.istio-proxy.txt) lines 30923-31015
- [fail-log-pod.tf-job-mnist-worker-0.istio-proxy.txt](https://github.com/tensorflow/tensorflow/files/5338769/fail-log-pod.tf-job-mnist-worker-0.istio-proxy.txt) lines 31080-31423

TF logs are short:
- [fail-log-pod.tf-job-mnist-worker-1.tensorflow.txt](https://github.com/tensorflow/tensorflow/files/5338764/fail-log-pod.tf-job-mnist-worker-1.tensorflow.txt)
- [fail-log-pod.tf-job-mnist-worker-0.tensorflow.txt](https://github.com/tensorflow/tensorflow/files/5338767/fail-log-pod.tf-job-mnist-worker-0.tensorflow.txt)

For completeness, mostly irrelevant:
- [fail-log-pod.tf-job-mnist-worker-1.istio-validation.txt](https://github.com/tensorflow/tensorflow/files/5338765/fail-log-pod.tf-job-mnist-worker-1.istio-validation.txt)
- [fail-log-pod.tf-job-mnist-worker-0.istio-validation.txt](https://github.com/tensorflow/tensorflow/files/5338768/fail-log-pod.tf-job-mnist-worker-0.istio-validation.txt)

"
43835,tf_record issue while following tutorial ,"ValueError: Shape must be rank 0 but is rank 1 for '{{node DecodeJpeg}} = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method="""", fancy_upscaling=true, ratio=1, try_recover_truncated=false](ParseSingleExample/ParseExample/ParseExampleV2)' with input shapes: [?].


using this tutorial: https://keras.io/examples/keras_recipes/tfrecord/"
43834,Horovod fails to train Keras Preprocessing IntegerLookup Layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3
- Python version: 3.7.6
- GPU model and memory: K80, 15 GB of RAM

**Describe the current behavior**
In TensorFlow 2.3, Keras Preprocessing Layers were released.  Some of these cause Horovod to fail.  It is not clear if this is a Horovod or TensorFlow issue.

The model that fails is:
```python3
def make_model():
  import tensorflow.keras as keras
  
  vocabulary = range(1, 11)
  return keras.Sequential([
    keras.layers.experimental.preprocessing.IntegerLookup(vocabulary=vocabulary),
    keras.layers.Embedding(len(vocabulary) + 2, 8, input_length=1),
    tf.keras.layers.Dense(1, activation='sigmoid')
  ])
```

When run, the error that is reported is:
```pycon
[1,0]<stderr>:  File ""/databricks/python/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 973, in wrapper
[1,0]<stderr>:    raise e.ag_error_metadata.to_exception(e)
[1,0]<stderr>:AttributeError: in user code:
[1,0]<stderr>:
[1,0]<stderr>:    /databricks/python/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:138 broadcast_group  *
[1,0]<stderr>:        var.assign(broadcast(var, root_rank))
[1,0]<stderr>:
[1,0]<stderr>:    AttributeError: 'TrackableWeightHandler' object has no attribute 'assign'
```

After fitting the same model locally, we can look at the vairables and see:
```python
for x in fitted_model.variables:
  print(type(x))
```
```pycon
<class 'tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler'>
<class 'tensorflow.python.eager.def_function.UnliftedInitializerVariable'>
<class 'tensorflow.python.eager.def_function.UnliftedInitializerVariable'>
<class 'tensorflow.python.eager.def_function.UnliftedInitializerVariable'>
```

`TrackableWeightHandler` appears to not support the interface of https://www.tensorflow.org/api_docs/python/tf/Variable - in particular, the `assign` method.

Might be similar to https://github.com/keras-team/autokeras/issues/1225 .

**Describe the expected behavior**
Horovod should be able to train with Keras Preprocessing Layers.

I also expect all objects returned from `keras.Model.variables` to be `tf.Variables`.  I don't know if that expectation is correct.  The API is vague on that part.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import tensorflow as tf
import horovod.tensorflow.keras as hvd

from sparkdl import HorovodRunner

def make_dataset():
  import tensorflow as tf
  from random import randrange

  dataset = tf.data.Dataset.from_tensor_slices(
    ([randrange(1, 11) for p in range(0, 10000)], [randrange(0,2) for p in range(0, 10000)])
  )
  dataset = dataset.repeat().batch(128)
  return dataset

def train():
  import tensorflow as tf
  import tensorflow.keras as keras
  import horovod.tensorflow.keras as hvd
  
  # Initialize Horovod
  hvd.init()

  # Pin GPU to be used to process local rank (one GPU per process)
  gpus = tf.config.experimental.list_physical_devices('GPU')
  for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
  if gpus:
      tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')

  # Build model and dataset
  dataset = make_dataset()
  model = make_model()
  # Horovod: adjust learning rate based on number of GPUs.
  scaled_lr = 0.001 * hvd.size()
  opt = tf.optimizers.Adam(scaled_lr)

  # Horovod: add Horovod DistributedOptimizer.
  opt = hvd.DistributedOptimizer(opt)

  # Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow
  # uses hvd.DistributedOptimizer() to compute gradients.
  model.compile(
    loss=tf.losses.BinaryCrossentropy(from_logits=True),
    optimizer=opt,
    metrics=['AUC'],
    experimental_run_tf_function=False
  )

  callbacks = [
      # Horovod: broadcast initial variable states from rank 0 to all other processes.
      # This is necessary to ensure consistent initialization of all workers when
      # training is started with random weights or restored from a checkpoint.
      hvd.callbacks.BroadcastGlobalVariablesCallback(0),
  ]

  model.fit(
    dataset,
    steps_per_epoch=500 // hvd.size(),
    callbacks=callbacks,
    epochs=2,
    verbose=1 if hvd.rank() == 0 else 0
  )
  
  return model

hr = HorovodRunner(np=-1)
fitted_model = hr.run(train)

def train_local():
  # Build model and dataset
  dataset = make_dataset()
  model = make_model()
  
  opt = tf.optimizers.Adam(0.001)
  model.compile(
    loss=tf.losses.BinaryCrossentropy(from_logits=True),
    optimizer=opt,
    metrics=['AUC'],
    experimental_run_tf_function=False
  )

  model.fit(
    dataset,
    steps_per_epoch=100,
    epochs=2,
    verbose=1
  )
  
  return model
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43833,TF2.3 converter.convert ValueError: Input 0 of node StatefulPartitionedCall/functional_1/resnet50/conv1_bn/AssignNewValue was passed float from Func/StatefulPartitionedCall/input/_5:0 incompatible with expected resource.,"**System information**
- OS Platform and Distribution (Ubuntu 18.04, cuda 10.1):
- TensorFlow installed from (binary):
- TensorFlow version (TF 2.3):


Here is the part of my code. 
```
       export_path './test_model'
        tf.saved_model.save(model, export_path)
        converter = tf.lite.TFLiteConverter.from_saved_model(export_path)
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
        tflite_model = converter.convert()
        open(""./converted_model.tflite"", ""wb"").write(tflite_model)
```

Here is my model summary, which is a typical image classification model using pre_trained ResNet50 model for transfer-learning. 

```
Model: ""functional_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 160, 160, 3)]     0         
_________________________________________________________________
sequential (Sequential)      (None, 160, 160, 3)       0         
_________________________________________________________________
tf_op_layer_RealDiv (TensorF [(None, 160, 160, 3)]     0         
_________________________________________________________________
tf_op_layer_Sub (TensorFlowO [(None, 160, 160, 3)]     0         
_________________________________________________________________
resnet50 (Functional)        (None, 5, 5, 2048)        23587712  
_________________________________________________________________
global_average_pooling2d (Gl (None, 2048)              0         
_________________________________________________________________
dropout (Dropout)            (None, 2048)              0         
_________________________________________________________________
dense (Dense)                (None, 3)                 6147      
=================================================================
Total params: 23,593,859
Trainable params: 23,540,739
Non-trainable params: 53,120
_________________________________________________________________
```

Here is the failure errors. 
```
Traceback (most recent call last):
  File ""/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/framework/importer.py"", line 497, in _import_graph_def_internal
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input 0 of node StatefulPartitionedCall/functional_1/resnet50/conv1_bn/AssignNewValue was passed float from Func/StatefulPartitionedCall/input/_5:0 incompatible with expected resource.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""main.py"", line 321, in <module>
    main(train='lite')
  File ""main.py"", line 210, in main
    tflite_model = converter.convert()
  File ""/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 1076, in convert
    return super(TFLiteConverterV2, self).convert()
  File ""/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 878, in convert
    self._funcs[0], lower_control_flow=False))
  File ""/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 1109, in convert_variables_to_constants_v2_as_graph
    converted_input_indices)
  File ""/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 1001, in _construct_concrete_function
    new_output_names)
  File ""/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py"", line 650, in function_from_graph_def
    wrapped_import = wrap_function(_imports_graph_def, [])
  File ""/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py"", line 628, in wrap_function
    collections={}),
  File ""/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py"", line 87, in __call__
    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
  File ""/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py"", line 93, in wrapped
    return fn(*args, **kwargs)
  File ""/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py"", line 648, in _imports_graph_def
    importer.import_graph_def(graph_def, name="""")
  File ""/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/framework/importer.py"", line 405, in import_graph_def
    producer_op_list=producer_op_list)
  File ""/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/framework/importer.py"", line 501, in _import_graph_def_internal
    raise ValueError(str(e))
**ValueError: Input 0 of node StatefulPartitionedCall/functional_1/resnet50/conv1_bn/AssignNewValue was passed float from Func/StatefulPartitionedCall/input/_5:0 incompatible with expected resource.**

```

How can i solve it? I don't know the reason why these kind of error happen. please let me know in detail.
Thanks in advance. "
43832,"Enable multiple input pipelines with a single machine, MultiWorkerMirroredStrategy's experimental_distribute_datasets_from_function","**System information**
- TensorFlow version (you are using): TF 2.3
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
`tf.distribute.experimental.MultiWorkerMirroredStrategy` provides an easy way to perform multi-GPU training on a single instance.  `tf.distribute.experimental.MultiWorkerMirroredStrategy.experimental_distribute_datasets_from_function()` allows for manually sharding the input dataset.  This function gets passed a `tf.distribute.InputContext`.

On a single-machine, 8 GPU setup, the function gets called exactly once with the following `input_context`:
```python3
tf.distribute.InputContext(
    num_input_pipelines=1, input_pipeline_id=0, num_replicas_in_sync=8
)
```
The result is that on a single machine, we cannot shard the dataset manually.

Instead, it would be great if TensorFlow saw a single machine as having 8 differnet input pipelines --- one for each GPU.  The result would enable sharding differently for each GPU.

**Will this change the current api? How?**
Either the default behavior would change or a flag might need to be added to `MultiWorkerMirroredStrategy` or `experimental_distribute_datasets_from_function` to specify this new behavior.

**Who will benefit with this feature?**
Anyone who requires multiple GPUs to train their models, but doesn't want the extra sophistication of running on multiple machines.

**Any Other info.**
It would be great if each input pipeline ran as a seperate python application.  That would enable multi-CPU out of the box if the dataset was designed for single CPU initially."
43830,Tensorflow cannot run inference together with TensorRT on the same GPU,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No, for TF model there is no custom code.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
source
- TensorFlow version (use command below):
`1.15` / Command line: `unknown 1.15.2`
- Python version: N/A
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.2/7.6
- GPU model and memory:  2080Ti, 12GB

**Describe the current behavior**

I'm running C++ inference of TF and TensorRT on the same GPU. However, what I found is TF inference would be broken if I initialized TRT engine in any way.
Both TF and TRT are doing inference in their separated CPU threads, and creating separated `CUcontext` for these 2 inference engine returns the same errors from TF side.

The error is like this:
```
[Switching to Thread 0x7ffc1de50700 (LWP 26137)]
Cuda API error detected: cudaPointerGetAttributes returned (0x1)
(cuda-gdb) bt
#0  0x00007ffca7b41130 in cudbgReportDriverApiError () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
#1  0x00007ffca7b440ca in cudbgReportDriverInternalError () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
#2  0x00007ffca7b475a3 in cudbgApiDetach () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
#3  0x00007ffca7d8abe3 in cudbgMain () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
#4  0x00007ffca7d9d5a3 in cudbgMain () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
#5  0x00007fffcbc60d99 in cudaPointerGetAttributes () from /home/.cache/_solib_local/_U@tensorflow_S_S_Ctensorflow___Uexternal_Stensorflow_Slib/libtensorflow_cc.so.1
#6  0x00007fffccbe2c24 in void stream_executor::gpu::(anonymous namespace)::CheckPointerIsValid<void const*>(void const*, absl::string_view) [clone .constprop.161] ()
   from /home/.cache/_solib_local/_U@tensorflow_S_S_Ctensorflow___Uexternal_Stensorflow_Slib/libtensorflow_cc.so.1
#7 stream_executor::gpu::GpuDriver::AsynchronousMemcpyH2D(stream_executor::gpu::GpuContext*, unsigned long long, void const*, unsigned long long, CUstream_st*) ()
...
```

**Describe the expected behavior**

I would expect both inference engine could be ran without broken by the other.
"
43829,train.py: error: argument --silence_percentage: invalid float value: '{SILENT_PERCENTAGE}',I got this issue when I was trying to train on my custom data
43827,BinaryCrossentropy and binary_crossentropy in the. same `tf.keras.losses` module,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/keras/losses/binary_crossentropy
https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy

## Description of issue (what needs changing):
There are 2 versions of Binary Cross Entropy, it would be less confusing to have just one.

Also, only `tf.keras.losses.binary_crossentropy` (or alternatively `""binary_crossentropy""`) works in the below code:
```python
model.compile(optimizer = RMSprop(lr=0.0001), 
              loss = tf.keras.losses.binary_crossentropy, 
              metrics = [""accuracy""])
```
I wonder why `""BinaryCrossentropy""` or `tf.keras.losses.BinaryCrossentropy` don't work."
43825,"why does tensorflow import_pb_to_tensorboard.py tool require the pb file to be a fixed name ""saved_model.pb""?","I tried to use import_pb_to_tensorboard.py tool come with Tensorflow, but when I used the tool to process my pb file, it always failed, after check the source code of the tool, I found the name of pb file has to be ""saved_model.pb"". I am wondering why the tool requires a fixed .pb file instead of any name users preferred?"
43824,"Keras way of auto-naming layers triggers a naming issues when loading an existing model, and slicing its output","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7

**Describe the current behavior**
When adding a strided_slice layer using ""numpy-style"" syntax (eg [: , :....]), Keras auto-names this layer ""tf_op_layer_strided_slice_{i}"", incrementing i every time such layer is created.
However, when opening an already-existing model, without creating any layer in the session, i restarts at 0. Hence, when adding at strided_slice operation at the end of the model, an error is triggered as the model now has 2 layers with the same name ""tf_op_layer_strided_slice""

PS: i cannot use the explicit tf.strided_slice layer with naming, as it does not have the same ""shape-inference"" capabilities as numpy-style slicing

**Describe the expected behavior**
auto-naming shall carry on incrementing from all known layer in the session, whether they have been created or loaded from a model file.

**Standalone code to reproduce the issue**
Run this code first with initial_builing = True, then False

```
import tensorflow as tf

initial_builing = False
model_loading = True

if initial_builing:

    layer1 = tf.keras.Input((1,),)
    layer2 = tf.keras.layers.Dense(1)

    model_output = layer2(layer1)[:,:-1]

    model = tf.keras.Model(layer1, model_output)
    model.summary()
    model.save('testmodel')

if model_loading:
    model = tf.keras.models.load_model('testmodel')
    model = tf.keras.Model(model.inputs, model.layers[-1].output[:,:-1])
```


**Other info / logs** 
```
Exception has occurred: ValueError
The name ""tf_op_layer_strided_slice"" is used 2 times in the model. All layer names should be unique.
  File ""C:\tests_v2.py"", line 19, in <module>
    model = tf.keras.Model(model.inputs, model.layers[-1].output[:,:-1])
```
"
43822,tensorflow.vectorized_map cannot recive loop variable,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): ananconda
- TensorFlow version (use command below): 2.0.2
- Python version:3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.2
- GPU model and memory: GTX1080ti RTX2080ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
The tensorflow.vectorized_map function cannot receive the positional parameters in the loop variable.

**Describe the expected behavior**
The tensorflow.vectorized_map function can receive the positional parameters in the loop variable like its predecessor map_fn function.
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
arangelist = tf.cast(tf.range(start=0, limit=1000 - 100 + 1, delta=1), dtype=tf.float32)
def errfunc(Index):
    """"""
    generate input for tf.vectorized_map
    :param Index:
    :return:
    """"""
    Index = tf.cast(Index, dtype=tf.int32)
    return Index
c = tf.vectorized_map(errfunc, arangelist)


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43821,Serial printing breaks after updating Mbed OS,"@tensorflow/micro

**System information**
- Linux Ubuntu 20.04
- TensorFlow installed from source
- Target platform Arm Mbed OS for the NXP FRDM K66F board

**Describe the problem**
The version of Mbed OS used for this [example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech#deploy-to-nxp-frdm-k66f) is old and unsupported. After running the update command to get the latest version there is a problem in the mbed/DebugLog.cc file since it uses a deprecated API (Serial) that is no longer available on the latest version of Mbed OS (v6.3.0). 
**Please provide the exact sequence of commands/steps when you ran into the problem**
The steps to build the working application for the K66F micro speech example directly from the Tensor Flow repo are the following:
1. git clone https://github.com/tensorflow/tensorflow.git 
2. cd tensorflow 
3. make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=""nxp_k66f"" generate_micro_speech_mbed_project 
4. cd tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed 
5. mbed config root . 
6. mbed deploy 
7. mbed compile -m K66F -t GCC_ARM --flash 
8. sudo screen /dev/ttyACM0 9600

This builds and executes correctly.
 
To update to the Mbed 6.3 version after step 6 we have done the following: 


7. cd mbed-os/
8. mbed update mbed-os-6.3.0
9. cd ..
10. In /tensorflow/lite/micro/mbed/debug_log.cc I have changed the function to
```
extern ""C"" void DebugLog(const char* s) to:
{
  static BufferedSerial serial(USBTX,USBRX);
        printf(""%s"", s);
}
```
11. make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=""nxp_k66f"" generate_micro_speech_mbed_project 
12. mbed compile -m K66F -t GCC_ARM --flash
13. sudo screen /dev/ttyACM0 9600 
 
This compiles correctly but does not correctly display the serial output for the ""yes/no"" voice commands. Instead, we get a memory error which says ""8 bytes lost due to alignment. To avoid this loss, please make sure the tensor_arena is 16 bytes aligned.""
 


"
43818,"Exceptions not raised because the ""raise"" keyword is missing in a few places","
Hello,

While analyzing Tensorflow on SonarCloud I saw what looks like two errors in [`tensorflow/python/tpu/tpu_embedding.py`](https://github.com/tensorflow/tensorflow/blob/c6c6d90b29d4f9ccf501520ae691ccd58f0bbd85/tensorflow/python/tpu/tpu_embedding.py#L1639) and [`tensorflow/python/keras/losses.py`](https://github.com/tensorflow/tensorflow/blob/090f260aab3dab00bcdf0232962e753bb9fab696/tensorflow/python/keras/losses.py#L183):

<img width=""1846"" alt=""Screenshot 2020-10-06 at 14 44 31"" src=""https://user-images.githubusercontent.com/40498978/95203471-18cd2400-07e3-11eb-98f2-288b909ffe28.png"">


<img width=""1834"" alt=""Screenshot 2020-10-06 at 14 44 48"" src=""https://user-images.githubusercontent.com/40498978/95203495-1ff43200-07e3-11eb-9d15-b7830b794c7f.png"">


You can see both issues in SonarCloud [here](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60QzlBMD9OHnI8rBe&open=AXT60QzlBMD9OHnI8rBe) and [here](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60O-aBMD9OHnI8qI7&open=AXT60O-aBMD9OHnI8qI7).

The problem is pretty simple: exceptions are created but not raised because the `raise` keyword is missing. This is a pretty common mistake in python ;)

In case you have any question, suggestion or if you see a False Positive on SonarCloud you can reach out on [SonarSource community forum](https://community.sonarsource.com/).

A few notes in case you want to use SonarCloud:
* I am currently testing the python analyzer so [the project on SonarCloud](https://sonarcloud.io/dashboard?id=nicolas-harraudeau-sonarsource_tensorflow) will only show python issues, but SonarCloud can also analyze C/C++ code and other languages.
* SonarCloud can also import pylint issues in case you want to use a rule SonarCloud does not already provide. Note however that pylint rules and SonarCloud rules are implemented differently. You might see new issues with SonarCloud, or less issues in some cases (we try to avoid False Positives as much as possible).
* It is free for open-source projects."
43817,"Duplicate condition in ""is_square""","Hello,

While analyzing Tensorflow on SonarCloud I saw what looks like an error in [`tensorflow/python/ops/linalg/registrations_util.py`](https://github.com/tensorflow/tensorflow/blob/9648531c0bf5163de26e8688c017d58b3eb80405/tensorflow/python/ops/linalg/registrations_util.py#L59):

<img width=""1863"" alt=""Screenshot 2020-10-06 at 14 23 29"" src=""https://user-images.githubusercontent.com/40498978/95201004-9a22b780-07df-11eb-919e-180242560a57.png"">

You can see the issue in SonarCloud [here](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60PaoBMD9OHnI8qUK&open=AXT60PaoBMD9OHnI8qUK).

The condition `operator_a.is_square is not None and operator_a.is_square is not None` doesn't make sense as it checks twice the same thing. I guess what the developer intended was `operator_a.is_square is not None and operator_b.is_square is not None` but I can't be sure as I don't know this code base.

In case you have any question, suggestion or if you see a False Positive on SonarCloud you can reach out on [SonarSource community forum](https://community.sonarsource.com/).

A few notes in case you want to use SonarCloud:
* I am currently testing the python analyzer so [the project on SonarCloud](https://sonarcloud.io/dashboard?id=nicolas-harraudeau-sonarsource_tensorflow) will only show python issues, but SonarCloud can also analyze C/C++ code and other languages.
* SonarCloud can also import pylint issues in case you want to use a rule SonarCloud does not already provide. Note however that pylint rules and SonarCloud rules are implemented differently. You might see new issues with SonarCloud, or less issues in some cases (we try to avoid False Positives as much as possible).
* It is free for open-source projects."
43816,"TypeError in ""Bidirectional.compute_output_shape""","Hello,

While analyzing Tensorflow on SonarCloud I saw what looks like an error in [`tensorflow/python/keras/layers/wrappers.py`](https://github.com/tensorflow/tensorflow/blob/d6426459bca2971b611ca9773858e55f6cc1a9af/tensorflow/python/keras/layers/wrappers.py#L522):

<img width=""1899"" alt=""Screenshot 2020-10-06 at 14 11 27"" src=""https://user-images.githubusercontent.com/40498978/95199955-eb31ac00-07dd-11eb-8bee-2a0c006ed40f.png"">


You can see the issue in SonarCloud [here](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60Og4BMD9OHnI8p_E&open=AXT60Og4BMD9OHnI8p_E).

After reviewing the code myself I can indeed see that `state_shape` will have the type `tuple`, and using the operator `+` on a `list` and a `tuple` will fail with the exception:
> TypeError: can only concatenate list (not ""tuple"") to list

Here is the flow which leads to this issue:
```
output_shape = tuple(output_shape.as_list())  # output_shape is a tuple
...
if self.return_state:
      state_shape = output_shape[1:]  # state_shape is a tuple
...
return [output_shape] + state_shape + copy.copy(state_shape)  # the expression ""[output_shape] + state_shape"" will fail
```

In case you have any question, suggestion or if you see a False Positive on SonarCloud you can reach out on [SonarSource community forum](https://community.sonarsource.com/).

A few notes in case you want to use SonarCloud:
* I am currently testing the python analyzer so [the project on SonarCloud](https://sonarcloud.io/dashboard?id=nicolas-harraudeau-sonarsource_tensorflow) will only show python issues, but SonarCloud can also analyze C/C++ code and other languages.
* SonarCloud can also import pylint issues in case you want to use a rule SonarCloud does not already provide. Note however that pylint rules and SonarCloud rules are implemented differently. You might see new issues with SonarCloud, or less issues in some cases (we try to avoid False Positives as much as possible).
* It is free for open-source projects."
43815,Failed to load the native TensorFlow runtime,"Im on Windows10
Python 3.7.7
Tensorflow 2.2
Keras 2.4.3

when i run
`from tensorflow.keras.datasets import mnist`

i get 

![image](https://user-images.githubusercontent.com/28235664/95198114-98062c00-07d2-11eb-887b-4a787dafaf19.png)
![image](https://user-images.githubusercontent.com/28235664/95198139-a3f1ee00-07d2-11eb-9ab7-578ab22b99dc.png)
"
43813,"Buggy string concatenation in ""experimental_tpu_predict_loop"" and ""experimental_tpu_test_loop""","Hello,

While analyzing Tensorflow on SonarCloud I saw what looks like two errors in `tensorflow/python/keras/engine/training_distributed_v1.py` [here](https://github.com/tensorflow/tensorflow/blob/d6426459bca2971b611ca9773858e55f6cc1a9af/tensorflow/python/keras/engine/training_distributed_v1.py#L536-L537) and [here](https://github.com/tensorflow/tensorflow/blob/d6426459bca2971b611ca9773858e55f6cc1a9af/tensorflow/python/keras/engine/training_distributed_v1.py#L389-L390):

<img width=""1833"" alt=""Screenshot 2020-10-06 at 11 45 32"" src=""https://user-images.githubusercontent.com/40498978/95185842-8c623780-07c9-11eb-96b4-a128cb3fb09f.png"">
I put only one screenshot as the code is identical for both issues.

You can see the issues in SonarCloud [here](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60OurBMD9OHnI8qEF&open=AXT60OurBMD9OHnI8qEF) and [here](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60OurBMD9OHnI8qEI&open=AXT60OurBMD9OHnI8qEI).

The problem is quite simple: the developer expected both strings to concatenate automatically. However as there are no parentheses nor a backslash at the end of the first line, these are two separate statements: one assignment and one string formatting which is then discarded.

In case you have any question, suggestion or if you see a False Positive on SonarCloud you can reach out on [SonarSource community forum](https://community.sonarsource.com/).

A few notes in case you want to use SonarCloud:
* I am currently testing the python analyzer so [the project on SonarCloud](https://sonarcloud.io/dashboard?id=nicolas-harraudeau-sonarsource_tensorflow) will only show python issues, but SonarCloud can also analyze C/C++ code and other languages.
* SonarCloud can also import pylint issues in case you want to use a rule SonarCloud does not already provide. Note however that pylint rules and SonarCloud rules are implemented differently. You might see new issues with SonarCloud, or less issues in some cases (we try to avoid False Positives as much as possible).
* It is free for open-source projects."
43811,Unable to build tflite for aarch64,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): N/A
- TensorFlow version: master branch (latest)
- Python version: 3.8
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): Android NDK toolchain 21.1.6352462 and GCC aarch64 toolchain 8.3 (2019.3)
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
build_aarch64_lib.sh failed because it tries to build x86 cpuid source files

**Provide the exact sequence of commands / steps that you executed before running into the problem**
./build_aarch64_lib.sh

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

https://github.com/tensorflow/tensorflow/commit/81ddb09077702debafd61e8ef4312f5806348266

After the above patch is merged, aarch64 build failed.
if you need any further information please let me know."
43810,Support tf.keras.metrics.MeanIoU on TPU,Please add support for tf.keras.metrics.MeanIoU on TPU
43809,Exploding loss when converting tf1 to tf2,"I want to convert this notebook written in tf1 to tf2, more specifically the softmax reccommender (Section VI. Softmax model)  model in there:
https://colab.research.google.com/github/google/eng-edu/blob/master/ml/recommendation-systems/recommendation-systems.ipynb

This is from the ML Course, previous steps describe the logic: https://developers.google.com/machine-learning/recommendation/labs/movie-rec-softmax-programming-exercise

I have tensorflow version '2.3.1' on Mac OS X

```python
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt
tf.config.run_functions_eagerly(True)
# tf.random.set_seed(42)
# np.random.seed(42)

from urllib.request import urlretrieve
import zipfile
urlretrieve(""http://files.grouplens.org/datasets/movielens/ml-100k.zip"", ""movielens.zip"")
zip_ref = zipfile.ZipFile('movielens.zip', ""r"")
zip_ref.extractall()
print(""Done. Dataset contains:"")
print(zip_ref.read('ml-100k/u.info'))

# Load each data set (users, movies, and ratings).
users_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']
users = pd.read_csv(
    'ml-100k/u.user', sep='|', names=users_cols, encoding='latin-1')

ratings_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']
ratings = pd.read_csv(
    'ml-100k/u.data', sep='\t', names=ratings_cols, encoding='latin-1')

# The movies file contains a binary feature for each genre.
genre_cols = [
    ""genre_unknown"", ""Action"", ""Adventure"", ""Animation"", ""Children"", ""Comedy"",
    ""Crime"", ""Documentary"", ""Drama"", ""Fantasy"", ""Film-Noir"", ""Horror"",
    ""Musical"", ""Mystery"", ""Romance"", ""Sci-Fi"", ""Thriller"", ""War"", ""Western""
]
movies_cols = [
    'movie_id', 'title', 'release_date', ""video_release_date"", ""imdb_url""
] + genre_cols
movies = pd.read_csv(
    'ml-100k/u.item', sep='|', names=movies_cols, encoding='latin-1')

# Since the ids start at 1, we shift them to start at 0.
users[""user_id""] = users[""user_id""].apply(lambda x: str(x-1))
movies[""movie_id""] = movies[""movie_id""].apply(lambda x: str(x-1))
movies[""year""] = movies['release_date'].apply(lambda x: str(x).split('-')[-1])
ratings[""movie_id""] = ratings[""movie_id""].apply(lambda x: str(x-1))
ratings[""user_id""] = ratings[""user_id""].apply(lambda x: str(x-1))
ratings[""rating""] = ratings[""rating""].apply(lambda x: float(x))

# Compute the number of movies to which a genre is assigned.
genre_occurences = movies[genre_cols].sum().to_dict()

# Since some movies can belong to more than one genre, we create different
# 'genre' columns as follows:
# - all_genres: all the active genres of the movie.
# - genre: randomly sampled from the active genres.
def mark_genres(movies, genres):
  def get_random_genre(gs):
    active = [genre for genre, g in zip(genres, gs) if g==1]
    if len(active) == 0:
      return 'Other'
    return np.random.choice(active)
  def get_all_genres(gs):
    active = [genre for genre, g in zip(genres, gs) if g==1]
    if len(active) == 0:
      return 'Other'
    return '-'.join(active)
  movies['genre'] = [
      get_random_genre(gs) for gs in zip(*[movies[genre] for genre in genres])]
  movies['all_genres'] = [
      get_all_genres(gs) for gs in zip(*[movies[genre] for genre in genres])]

mark_genres(movies, genre_cols)

# Create one merged DataFrame containing all the movielens data.
movielens = ratings.merge(movies, on='movie_id').merge(users, on='user_id')

# Utility to split the data into training and test sets.
def split_dataframe(df, holdout_fraction=0.1):
  """"""Splits a DataFrame into training and test sets.
  Args:
    df: a dataframe.
    holdout_fraction: fraction of dataframe rows to use in the test set.
  Returns:
    train: dataframe for training
    test: dataframe for testing
  """"""
  test = df.sample(frac=holdout_fraction, replace=False, random_state=42)
  train = df[~df.index.isin(test.index)]
  return train, test

rated_movies = (ratings[[""user_id"", ""movie_id""]]
                .groupby(""user_id"", as_index=False)
                .aggregate(lambda x: list(x)))


BATCH_SIZE = 32
EMBEDDING_SIZE = 35

class SoftmaxRecommender(keras.Model):
    def __init__(self, feature_columns, **kwargs):
        super(SoftmaxRecommender, self).__init__(**kwargs)
        self.movie_embedding = layers.DenseFeatures(feature_columns['movie'])
        self.other_features = layers.DenseFeatures(feature_columns['other'])
        self.hidden_layer = layers.Dense(EMBEDDING_SIZE,
            kernel_initializer=keras.initializers.TruncatedNormal(
            stddev=1. / np.sqrt(EMBEDDING_SIZE) / 10.))
        # self.linear = layers.Dense(num_movies, use_bias=True, activation='softmax')
        # self.softmax_activation = keras.layers.Activation('softmax')


    def call(self, inputs, train=True, **kwargs):
        V = self.movie_embedding({'movie_id': inputs['movie_id']})
        inputs.pop('movie_id')
        other_fts = self.other_features(inputs)
        U = tf.concat([V,other_fts], axis=1)
        U = self.hidden_layer(U)
        logits = tf.matmul(U, self.movie_embedding.get_weights()[0], transpose_b=True)
        # logits = self.linear(logits)
        # logits = self.softmax_activation(logits)
        return logits


def custom_loss(labels, logits):
    # labels = tf.reshape(labels, [-1])
    # labels_org = labels.copy()
    labels = select_random(labels)
    # labels = tf.cast(labels, 'float32')
    labels = tf.reshape(labels, [-1, 1])
    # cce = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    # preds = tf.argmax(logits, axis=1)
    # preds = tf.cast(preds, 'float32')
    # loss = cce(y_true=labels, y_pred=logits)
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
        logits=logits, labels=labels))
    return loss




years_dict = {
    movie: year for movie, year in zip(movies[""movie_id""], movies[""year""])
}
genres_dict = {
    movie: genres.split('-')
    for movie, genres in zip(movies[""movie_id""], movies[""all_genres""])
}

def select_random(x):
  """"""Selectes a random elements from each row of x.""""""
  def to_float(x):
    return tf.cast(x, tf.float32)
  def to_int(x):
    return tf.cast(x, tf.int64)
  batch_size = tf.shape(x)[0]
  rn = tf.range(batch_size)
  nnz = to_float(tf.math.count_nonzero(x >= 0, axis=1))
  rnd = tf.random.uniform([batch_size])
  ids = tf.stack([to_int(rn), to_int(nnz * rnd)], axis=1)
  return to_int(tf.gather_nd(x, ids))

def make_embedding_col(key, embedding_dim):
  categorical_col = tf.feature_column.categorical_column_with_vocabulary_list(
      key=key, vocabulary_list=list(set(list(movies[key].values))), num_oov_buckets=0)
  return tf.feature_column.embedding_column(
      categorical_column=categorical_col, dimension=embedding_dim,
      initializer=keras.initializers.TruncatedNormal(
                                             stddev=1. / np.sqrt(embedding_dim) / 10.),
      # default initializer: trancated normal with stddev=1/sqrt(dimension)
      combiner='mean')

def make_dataset(ratings):
  """"""Creates a batch of examples.
  Args:
    ratings: A DataFrame of ratings such that examples[""movie_id""] is a list of
      movies rated by a user.
    batch_size: The batch size.
  """"""
  def pad(x, fill):
    return pd.DataFrame.from_dict(x).fillna(fill).values

  movie = []
  year = []
  genre = []
  label = []
  for movie_ids in ratings[""movie_id""].values:
    movie.append(movie_ids)
    genre.append([x for movie_id in movie_ids for x in genres_dict[movie_id]])
    year.append([years_dict[movie_id] for movie_id in movie_ids])
    label.append([int(movie_id) for movie_id in movie_ids])

  features = {
      ""movie_id"": pad(movie, """"),
      ""year"": pad(year, """"),
      ""genre"": pad(genre, """"),
  }

  y = pad(label, -1)
  # y = select_random(y)
  # print('y.nunique:', len(np.unique(y.numpy())))

  return features, y


train_rated_movies, test_rated_movies = split_dataframe(rated_movies)
x_train, y_train = make_dataset(train_rated_movies)
x_val, y_val = make_dataset(test_rated_movies)


# create two separate DenseFeatures in model to be able to access movie_embeddings
feature_cols = {
    'movie': [make_embedding_col(""movie_id"", 35)],
    'other': [make_embedding_col(""genre"", 3), make_embedding_col(""year"", 2)]
}

model = SoftmaxRecommender(feature_columns=feature_cols)
model.compile(
    loss=custom_loss, optimizer=keras.optimizers.SGD(.1, clipnorm=1)
    # loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=keras.optimizers.SGD(.1, clipnorm=1)
)

# batch_x1 = {
#     'movie_id':x_train['movie_id'][:32],
#     'year':x_train['year'][:32],
#     'genre':x_train['genre'][:32]
#            }
#
# batch_y1 = y_train[:32]
#
# logits = model(batch_x1)
# loss = custom_loss(batch_y1, logits)
#
# batch_x2 = {
#     'movie_id':x_train['movie_id'][32:64],
#     'year':x_train['year'][32:64],
#     'genre':x_train['genre'][32:64]
#            }
#
# batch_y2 = y_train[32:64]
#
# logits = model(batch_x2)
# loss = custom_loss(batch_y2, logits)

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=32,
    epochs=10,
    verbose=2,
    validation_data=(x_val, y_val),
)
```

The above model with custom_loss that uses tf.nn.softmax_cross_entropy_with_logits leads to exploding gradients.

I also tried adding softmax activation in the model class and using keras.losses.SparseCategoricalCrossentropy(from_logits=True) while doing select_random for labels in make_dataset function once and not in each batch. This approach lead to weights staying stable. If I use keras.losses.SparseCategoricalCrossentropy without adding softmax activation to the logits, loss explodes again.

My understanding of keras.Model and its call() method might be wrong, and/or my DenseFeatures layer application. I understand that with exponentially increasing loss it might be that there is something wrong with the dataset, but data-preparation steps are directly copied from the original code.

I highly appreciate any help!"
43808,Converting a LSTM model from Pytorch to Tensorflow using ONNX,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.9
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.1
- GPU model and memory:


**Describe the current behavior**
I am trying to convert a very simple LSTM model from Pytorch to Tensorflow using ONNX. The model conversion from Pytorch to ONNX is happening, but I am unable to convert that ONNX model to Tensorflow.

**Describe the expected behavior**
Successfully convert LSTM from .onnx to .pb.

**Standalone code to reproduce the issue**
import onnx
from onnx_tf.backend import prepare

onnx_model = onnx.load(""./SimpleLSTM.onnx"")
tf_rep = prepare(onnx_model)
tf_rep.export_graph('./SimpleLSTM.pb')


**Other info / logs** 
Traceback (most recent call last):
  File ""testLSTM.py"", line 14, in <module>
    tf_rep.export_graph('./SimpleLSTM.pb')
  File ""/workspace/ctg_nitya/onnx-tensorflow/onnx_tf/backend_rep.py"", line 93, in export_graph
    tf.saved_model.save(self.tf_module, path, signatures=self.tf_module.__call__.get_concrete_function(**self.signatures))
  File ""/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 1167, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 1074, in _get_concrete_function_garbage_collected
    self._initialize_uninitialized_variables(initializers)
  File ""/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 965, in _initialize_uninitialized_variables
    return initialize_variables.get_concrete_function()()
  File ""/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 2939, in get_concrete_function
    *args, **kwargs)
  File ""/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 2906, in _get_concrete_function_garbage_collected
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File ""/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 3213, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 3075, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 957, in initialize_variables
    inits, ops.get_default_graph(), op_map=op_map)
  File ""/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/lift_to_graph.py"", line 260, in lift_to_graph
    add_sources=add_sources))
  File ""/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/ops/op_selector.py"", line 413, in map_subgraph
    % (repr(init_tensor), repr(op), _path_from(op, init_tensor, sources)))
**tensorflow.python.ops.op_selector.UnliftableError: Unable to lift tensor <tf.Tensor 'concat_2:0' shape=(6, 12) dtype=float32> because it depends transitively on placeholder <tf.Operation 'Squeeze_1/60' type=Placeholder> via at least one path, e.g.: concat_2 (ConcatV2) <- transpose_1 (Transpose) <- concat_1 (ConcatV2) <- split_1 (Split) <- Squeeze_1 (Squeeze) <- Squeeze_1/60 (Placeholder)**

"
43807,is it possible tensorflow-gpu  on RK3399 mali t-860?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): LUbuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: RK3399 
- TensorFlow installed from (source or binary): pip3
- TensorFlow version: tensorflow 1.14(cpu)
- Python version: python3.5
- Installed using virtualenv? pip? conda?: pip3
- Bazel version (if compiling from source): none
- GCC/Compiler version (if compiling from source): gcc 5.5
- CUDA/cuDNN version: none
- GPU model and memory: mali t-860

hi.

My problem is running time is too long tensorflow(cpu version)
so, i try to install tensorflow-gpu on RK3399  GPU- mali t-860 (ARM)

but i can't install yet. 
is it possible? 

if possible how to install it?

Thank you




"
43806,Model containing EfficientNet cannot be restored,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.6 / Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below):v2.3.0-54-gfcc4b966f1 2.3.1 (Local),  v2.3.0-54-gfcc4b966f1 2.3.1 (Colab)
- Python version: 3.7.3 (Local), 3.6.9 (Colab)
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**
1. Trained a model containing pretrained EfficientNetB0 with GradientTape without compiling a model
2. Saved a model by ```model.save('path')```
3. Restoreing a model shows warning that gradient cannot be requested
4. Finetuning fails

**Describe the expected behavior**
Successfully save, restore and finetune.

**Standalone code to reproduce the issue**

[Colab](https://colab.research.google.com/drive/1gzOwSWJ1Kvwzo01SEpjqGq6Lb-OsI-ob?usp=sharing)
[StackOverflow](https://stackoverflow.com/questions/64218503/finetune-savedmodel-failure-due-to-no-gradient-loaded)

**Other info / logs** Include any logs or source code that would be helpful to
I followed this tutorial. How can I save and restore to train again with different learning rate for finetuning?
https://www.tensorflow.org/tutorials/quickstart/advanced

At first I posted on StackOverflow and then tried to reproduce on Colab to give more information, but error messages are different. It might be because of data loading difference, but I'm not sure.
Thank you for reading."
43805,Correct way to cast a RaggeredTensor to list of strings?,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: Titan V

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I am trying to incorporate a customized function into tensorflow via `tf.py_function`, which is something like the following

```
    def run(x):
        return tf.py_function(my_func, x, tf.float32, name=None)
    out = tf.keras.layers.Lambda(run)(raggedtensor)
```

The desired input of `my_func` is a list of strings, while the `raggedtensor` is `tf.RaggedTensor(values=Tensor(""StringSplit/StringSplitV2:1"", shape=(None,), dtype=string), row_splits=Tensor(""StringSplit/RaggedFromValueRowIds/concat:0"", shape=(None,), dtype=int64)).`

The current code will throw an error of `TypeError: Expected list for 'input' argument to 'EagerPyFunc' Op, not tf.RaggedTensor(values=Tensor...`, while if I changed it to the following

```
    def run(x):
        return tf.py_function(my_func, x.to_list(), tf.float32, name=None)
    out = tf.keras.layers.Lambda(run)(raggedtensor)
```

The error will change to `ValueError: RaggedTensor.to_list() is only supported in eager mode; in graph mode, evaluate the RaggedTensor first and then use RaggedTensorValue.to_list().`

Is there a way I could cast the tensor to my desired input?"
43802,ValueError: You are trying to load a weight file containing 0 layers into a model with 19 layers.,"hey this is my code i still have the same error 
from __future__ import absolute_import
from __future__ import print_function
import os
import numpy as np
from keras.layers import Input
from keras.layers.core import Activation, Flatten, Reshape
from keras.layers.convolutional import Convolution2D, MaxPooling2D, UpSampling2D, Conv2D
from keras.layers.normalization import BatchNormalization
from keras.models import Model
from keras.utils import np_utils
from keras.applications import imagenet_utils
from .Architecture import Architecture



class Ronneberger(Architecture):
    @staticmethod
    def get_model(config, crossval_id=None) :
        assert config.arch.num_dimensions is 2
        assert config.arch.patch_shape[0] % 16 == 0, ""Invalid patch shape""

        num_modalities = len(config.dataset.modalities)
        input_layer_shape = (num_modalities, ) + config.arch.patch_shape[:config.arch.num_dimensions]
        output_layer_shape = (config.train.num_classes, np.prod(config.arch.patch_shape[:config.arch.num_dimensions]))
        model = generate_unet_model(
            config.arch.num_dimensions,
            config.train.num_classes,
            input_layer_shape,
            output_layer_shape,
            config.train.activation,
            downsize_factor=2)

        return model

def generate_net_model(
    dimension, num_classes, input_shape, output_shape, activation, downsize_factor=2):
    img_input = Input(shape=input_shape)
    x = img_input
    # Encoder
    x = Conv2D(64, 3, 3, border_mode=""same"")(img_input)
    x = BatchNormalization()(x)
    x = Activation(""relu"")(x)
    x = MaxPooling2D(pool_size=(2, 2))(x)
    
    x = Convolution2D(128, 3, 3, border_mode=""same"")(x)
    x = BatchNormalization()(x)
    x = Activation(""relu"")(x)
    x = MaxPooling2D(pool_size=(2, 2))(x)
    
    x = Convolution2D(256, 3, 3, border_mode=""same"")(x)
    x = BatchNormalization()(x)
    x = Activation(""relu"")(x)
    x = MaxPooling2D(pool_size=(2, 2))(x)
    
    x = Convolution2D(512, 3, 3, border_mode=""same"")(x)
    x = BatchNormalization()(x)
    x = Activation(""relu"")(x)
    
    x = Convolution2D(1024, 3, 3, border_mode=""same"")(x)
    x = BatchNormalization()(x)
    x = Activation(""relu"")(x)
    
    # Decoder
    x = Convolution2D(512, 2, 2, border_mode=""same"")(x)
    x = BatchNormalization()(x)
    x = Activation(""relu"")(x)
    
    x = UpSampling2D(size=(2, 2))(x)
    x = Convolution2D(256, 2, 2, border_mode=""same"")(x)
    x = BatchNormalization()(x)
    x = Activation(""relu"")(x)
    
    x = UpSampling2D(size=(2, 2))(x)
    x = Convolution2D(128, 2, 2, border_mode=""same"")(x)
    x = BatchNormalization()(x)
    x = Activation(""relu"")(x)
    
    x = UpSampling2D(size=(2, 2))(x)
    x = Convolution2D(64, 2, 2, border_mode=""same"")(x)
    x = BatchNormalization()(x)
    x = Activation(""relu"")(x)
    
    x = Convolution2D(num_classes, 1, 1, border_mode=""valid"")(x)
    print(img_input.shape )
    x = Reshape((48*48, num_classes))(x)
    x = Activation(""softmax"")(x)
    model = Model(img_input, x)

    return model


and the error is 

load_weights_from_hdf5_group
    str(len(filtered_layers)) + ' layers.')

ValueError: You are trying to load a weight file containing 0 layers into a model with 19 layers."
43800,Possibility to compile python code when @tf.function is used in TensorFlow 2.x ,"**System information**
- TensorFlow version (you are using): **2.3.0**
- Are you willing to contribute it (Yes/No): **No**

**Describe the feature and the current behavior/state.**

With TensorFlow 2.x, eager mode is enabled by default, so it is essential to use @tf.function to speed up the performance.
tf.function decorator needs access to the python source code, therefore it makes it impossible to compile our python code and create an executable python build (binary) because the decorator (autograph) always needs access to the python source code. This issue makes it impossible to obfuscate the code and can create a security concern at some situations.  


**Will this change the current api? How?**

Probably yes, but if there is any workaround with the current api, please let me know!


**Who will benefit with this feature?**

Anyone who deploys a deep learning solution in the customer site or cloud or willing to create code obfuscation
"
43799,cuda_configure.bzl: wrong cuda_config.cudnn_version cause wrong cudnn_headers list,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source 
- TensorFlow version: master
- Python version: 3.6.8
- Bazel version (if compiling from source): 3.1.5
- GCC/Compiler version (if compiling from source): MSVC 2019
- CUDA/cuDNN version: 11.1 8.0.4
- GPU model and memory: RTX 2080 TI

**Describe the problem**

Unable to build.

But build fine if I comment out this line
in file tensorflow/third_party/gpus/cuda_configure.bzl 
```
    cudnn_headers = [""cudnn.h""]
###    if cuda_config.cudnn_version.rsplit(""_"", 1)[0] >= ""8"":
    cudnn_headers += [
```

**Any other info / logs**

I added debug line to print information in  `cuda_config.cudnn_version`

```
cudnn_headers = [""cudnn.h""]

fail(""cuda_config.cudnn_version = %s"" % cuda_config.cudnn_version )
    
if cuda_config.cudnn_version.rsplit(""_"", 1)[0] >= ""8"":
```

RESULT:
```
cuda_config.cudnn_version = 64_8
```"
43791,Include support for the Constant-Q transform.,"<em>A feature request for providing support for the Constant-Q transform.</em>


**System information**
- TensorFlow version (you are using): v2.1.0
- Are you willing to contribute : Yes, I have already worked on a sample code.

**Describe the feature and the current behavior/state.**
The constant Q transform is very closely related to the Fourier transform. Like the Fourier transform, a constant Q transform
is a bank of filters, but in contrast to the former it has geometrically spaced center frequencies. Currently, we have _tf.signal.fft_  for Fourier transforms, but no such functionality for the Constant Q transforms.

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
A nice feature of the constant Q transform is its increasing time resolution towards higher frequencies.

"
43790,Google Collab error for TPU - UnavailableError: {{function_node __inference_train_function_99378}} failed to connect to all addresses ,"No longer able to train model using google cloud TPU on my [gist](https://colab.research.google.com/gist/ScruffySilky/f681aca8130f40555b41f98323887c15/42038.ipynb?authuser=1&pli=1), it was training fine 2 months ago and now I get the following error:

```
UnavailableError: {{function_node __inference_train_function_99378}} failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{""created"":""@1601903304.230958587"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3948,""referenced_errors"":[{""created"":""@1601903304.089639211"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]}
	 [[{{node IteratorGetNext}}]]
```

Seems related to this issue: https://github.com/tensorflow/tensorflow/issues/43037
"
43789,Google Cloud Storage cannot be used in tf.keras.callbacks.experimental.BackupAndRestore as a back-up dir,"
**System information**
- Run in official container: tensorflow/tensorflow:2.3.1
- Host OS:  Debian GNU/Linux 10 (Cloud shell)

**Describe the current behavior**
On successful training, the checkpoints produced by `tf.keras.callbacks.experimental.BackupAndRestore` should be removed. However, if one provided a Cloud Storage location, the cleanup fails. This is not the case for specifying a local backup location.

**Describe the expected behavior**
`tf.keras.callbacks.experimental.BackupAndRestore` should be able to remove the checkpoint upon successful training. 

**Standalone code to reproduce the issue**
[Colab Notebook](https://colab.research.google.com/drive/1XocuDL8I8tivEa-DCgH6iWBcpV5WiJRJ?usp=sharing)
*Please change the Cloud Storage location to a bucket owned by you.*

**Other info / logs**
```
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/trainig/trainer/task.py"", line 69, in <module>
    main(args)
  File ""/trainig/trainer/task.py"", line 41, in main
    callbacks=callbacks
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py"", line 1145, in fit
    callbacks.on_train_end(logs=training_logs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py"", line 514, in on_train_end
    callback.on_train_end(logs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py"", line 1547, in on_train_end
    self._training_state.delete_backup()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/distribute/worker_training_state.py"", line 124, in delete_backup
    file_io.delete_recursively(pathname)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 586, in delete_recursively
    delete_recursively_v2(dirname)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 599, in delete_recursively_v2
    _pywrap_file_io.DeleteRecursively(compat.as_bytes(path))
tensorflow.python.framework.errors_impl.NotFoundError: gs://[bucket]/training/backup/ckpt-2.data-00000-of-00001 doesn't exist or not a directory.
```"
43788,TFLite C++ API outputs the same results all the time,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **HDK865 from QC**
- TensorFlow installed from (source or binary): **Python binary, C++ API from source**
- TensorFlow version (use command below): **2.3**
- Python version: **3.6**
- Bazel version (if compiling from source): **Build label: 3.5.0**
- GCC/Compiler version (if compiling from source): **7.5.0** 
- CUDA/cuDNN version:
- GPU model and memory:


I have an MLP model trained with **Keras** and **Tensorflow 2.3**, I train and predict correctly with that model as .tflite using Python API, but when I try to predict using C++ API output is always the same 1/model_classes, all the time.

I feed the model: 
```c++ 
void fillInputTenso(std::vector<float> input) {
        auto inputTensor = this->mInterpreter->typed_tensor<float>(this->mInterpreter->inputs()[0]);
        for(int i = 0; i < input.size(); i++) {
            inputTensor[i] = input[i];
        }
```
After Invoke I read the output: 
```c++ 
 void proceedOutput(void) {
        auto output = this->mInterpreter->typed_tensor<float>(this->mInterpreter->outputs()[0]);
        for(int i=0; i < this->mConfig.labels.size(); i++) {
            LOGI("" confidence = %f."", output[i]);
        }
    }
```
I also tried this from the C++ API but the results are the same: 
```c++
auto input = mInterpreter->typed_input_tensor<float>(0) 
```
**It doesn't matter what input I provide to model, its completely the same output always.
This model works completely fine with Python API.**

Model architecture:
```python
keras.Sequential([
            keras.layers.Flatten(input_shape=(self.dataset.features_num(),)),
            keras.layers.Dense(128, activation='relu'),
            keras.layers.Dense(nr_classes, activation='softmax')
        ])
```
Note: I added this node to this model since the accuracy was dropped drastically when converted to tflite:
```python
import tensorflow_model_optimization as tfmot
            quantize_model = tfmot.quantization.keras.quantize_model
            # q_aware stands for for quantization aware.
            q_aware_model = quantize_model(self.__model)
            # `quantize_model` requires a recompile.
            q_aware_model.compile(optimizer='adam',
                                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                                  metrics=['accuracy'])
            self.__model = q_aware_model
```"
43787,Interpreter takes too much time to  return results while similar iOS app makes it 10 times faster,"I use following code to create Interpreter, prepare image and execute task to get proper results:

    private var imageData: ByteBuffer
    private var outputBuffer = Array(1) { Array(256) { Array(256) { FloatArray(NUM_CLASSES) } } }
    private var gpuDelegate: GpuDelegate? = null
    private val interpreter: Interpreter
    private var intValues: IntArray

    init {
        interpreter = getInterpreter(context, tfLiteModel)
        interpreter.allocateTensors()
        imageData = ByteBuffer.allocateDirect(1 * inputWidth * inputHeight * 3 * 4)
        imageData.order(ByteOrder.nativeOrder())
        intValues = IntArray(inputWidth * inputHeight)

    }

    private fun getInterpreter(
        context: Context,
        tfLiteModel: String
    ): Interpreter {

        val compatList = CompatibilityList()
        val tfLiteOptions = Interpreter.Options().apply {
            if (compatList.isDelegateSupportedOnThisDevice) {
                val delegateOptions = compatList.bestOptionsForThisDevice
                delegateOptions.setPrecisionLossAllowed(true)
                this.addDelegate(GpuDelegate(delegateOptions))
            } else {
                setNumThreads(NUM_THREADS)
            }
        }
        return Interpreter(loadModelFile(context, tfLiteModel), tfLiteOptions)
    }

    @Throws(IOException::class)
    private fun loadModelFile(context: Context, modelFile: String): MappedByteBuffer {
        val fileDescriptor = context.assets.openFd(modelFile)
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        val retFile = fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
        fileDescriptor.close()
        return retFile
    }


    fun close() {
        interpreter?.close()
        gpuDelegate?.close()
    }

    fun prepareImage() {
        imageData.rewind()
        for (i in 0 until intValues.size) {
            val pixelValue = intValues[i]
            imageData.putFloat((((pixelValue shr 16) and 0xFF) - IMAGE_MEAN) / IMAGE_STD)
            imageData.putFloat((((pixelValue shr 8) and 0xFF) - IMAGE_MEAN) / IMAGE_STD)
            imageData.putFloat(((pixelValue and 0xFF) - IMAGE_MEAN) / IMAGE_STD)
        }
        imageData.rewind()
    }

    fun execute(bitmap: Bitmap): Array<Array<Array<FloatArray>>> {

        bitmap.getPixels(intValues, 0, bitmap.width, 0, 0, bitmap.width, bitmap.height)
        prepareImage()
        val timing = System.currentTimeMillis()
        interpreter.run(imageData, outputBuffer)
        Log.d(""executetiming"", ""run() - ${System.currentTimeMillis() - timing}"")
        return outputBuffer

    }

    companion object {
        private const val NUM_CLASSES: Int = 2
        private const val IMAGE_MEAN = 127.5f
        private const val IMAGE_STD = 127.5f
        private const val NUM_THREADS = 4
    }
}
The app is something between object recognition and image segmentation. I recognize certain objects with my camera and put colors on them pixel by pixel using Canvas.
The problem is that I am getting following timings:

> 2020-10-05 10:02:34.134 27093-27278/com.example.sampleappkt D/executetiming: run() - 550
2020-10-05 10:02:34.841 27093-27278/com.example.sampleappkt D/executetiming: run() - 625
2020-10-05 10:02:35.603 27093-27278/com.example.sampleappkt D/executetiming: run() - 645
2020-10-05 10:02:36.345 27093-27278/com.example.sampleappkt D/executetiming: run() - 608
2020-10-05 10:02:37.068 27093-27278/com.example.sampleappkt D/executetiming: run() - 628
2020-10-05 10:02:37.804 27093-27278/com.example.sampleappkt D/executetiming: run() - 640
2020-10-05 10:02:38.462 27093-27278/com.example.sampleappkt D/executetiming: run() - 591
2020-10-05 10:02:39.187 27093-27278/com.example.sampleappkt D/executetiming: run() - 632
2020-10-05 10:02:39.921 27093-27278/com.example.sampleappkt D/executetiming: run() - 650
2020-10-05 10:02:40.669 27093-27278/com.example.sampleappkt D/executetiming: run() - 627
2020-10-05 10:02:41.387 27093-27278/com.example.sampleappkt D/executetiming: run() - 621
2020-10-05 10:02:42.126 27093-27278/com.example.sampleappkt D/executetiming: run() - 648
2020-10-05 10:02:42.825 27093-27278/com.example.sampleappkt D/executetiming: run() - 635
2020-10-05 10:02:43.527 27093-27278/com.example.sampleappkt D/executetiming: run() - 636
2020-10-05 10:02:44.203 27093-27278/com.example.sampleappkt D/executetiming: run() - 576
2020-10-05 10:02:44.952 27093-27278/com.example.sampleappkt D/executetiming: run() - 654
2020-10-05 10:02:45.670 27093-27278/com.example.sampleappkt D/executetiming: run() - 649
2020-10-05 10:02:46.399 27093-27278/com.example.sampleappkt D/executetiming: run() - 667
2020-10-05 10:02:47.001 27093-27278/com.example.sampleappkt D/executetiming: run() - 509
2020-10-05 10:02:47.579 27093-27278/com.example.sampleappkt D/executetiming: run() - 495
2020-10-05 10:02:48.321 27093-27278/com.example.sampleappkt D/executetiming: run() - 660
2020-10-05 10:02:49.053 27093-27278/com.example.sampleappkt D/executetiming: run() - 624
2020-10-05 10:02:49.764 27093-27278/com.example.sampleappkt D/executetiming: run() - 617
2020-10-05 10:02:50.520 27093-27278/com.example.sampleappkt D/executetiming: run() - 659
2020-10-05 10:02:51.317 27093-27278/com.example.sampleappkt D/executetiming: run() - 694
2020-10-05 10:02:52.099 27093-27278/com.example.sampleappkt D/executetiming: run() - 658

I've checked performance using app from here: https://www.tensorflow.org/lite/performance/measurement
And I got this:
![deeplab_float32_converted](https://user-images.githubusercontent.com/55881948/95068369-81de6a00-0705-11eb-9933-34d46d9188e9.png)

I use Samsung Galaxy S7 Edge for my tests, but I think even if it isn't newest phone those timings shouldn't be that high.
Model used is Mobilenet.
Same model needs 80ms overal when used on iPhone 8 with 2 threads.
How can I get rid of ""Warmup"" process launched everytime?"
43785,Tensorflow 2.3.0 stopped detecting GPU suddenly while it used to detect in past,"**System information**
- OS Platform and Distribution : Windows 10 Pro
- TensorFlow installed from : `pip install tensorflow`
- TensorFlow version: 2.3.0
- Python version: 3.6.8
- Installed using : `pip`
- CUDA version : `Cuda compilation tools, release 10.1, V10.1.243`
- CUdnn version : V8.0.3.33
- GPU model and memory: Nvidia GTX 1050Ti, 4GB 


_I used to train my models locally and it's been a while(~1month) since I've trained some models and when I opened my jupyter notebook and wrote some code and tried to train my model, I noticed that the model is training really slow and so I wrote some code to check_ **if tensorflow is detecting my GPU and to my surprise it's not detecting.** 

![Capture](https://user-images.githubusercontent.com/30192967/95064765-d42b8500-071d-11eb-9d90-8f96e38f5029.JPG)

**After lots of brainstorming over the internet,** 
1) I re-installed tensorflow
2) I re-installed entire CUDA according to the version it supports for my gpu
3) I re-installed Python

I even checked if older versions of TF(1.15.4, 1.15,3, 2.0.3) could detect but no luck.
But still I tend to see the same problem. Please let me know down below if there are any solutions which I can follow.

**Things to note,**
1) I don't use anaconda.
2) I directly installed python and set up cuda support which used to work in past and suddenly stopped working.

**Logs**
**Note : Please don't get carried away with ETA, since this is a very lite model**
```
2020-10-05 16:39:40.085550: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-05 16:39:40.133725: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2135ad1a990 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-05 16:39:40.144525: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
[INFO] compiling model...
[INFO] training head...
Epoch 1/20
34/34 [==============================] - 32s 940ms/step - loss: 0.4717 - accuracy: 0.7959 - val_loss: 0.1328 - val_accuracy: 0.9746
Epoch 2/20
34/34 [==============================] - 42s 1s/step - loss: 0.1325 - accuracy: 0.9672 - val_loss: 0.0594 - val_accuracy: 0.9891
Epoch 3/20
18/34 [==============>...............] - ETA: 15s - loss: 0.0815 - accuracy: 0.9826
```"
43784,embedding_lookup cause ran out of memory ,"I am running the following code to test embedding_lookup.
```python

# command:
# python3 -m pdb embtest.py --features=1000 --nnz=30 --batch=128
#
# error:
# *** tensorflow.python.framework.errors_impl.ResourceExhaustedError: 
#     Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.
#
import tensorflow as tf
import numpy as np
import sys
import os
import time

def measure(params, sp_ids, steps, thr):
  res = tf.nn.embedding_lookup([params[0:thr],params[thr:]], sp_ids, None, name=""TEST1"")
  print(""Finished test"")
  return res

if __name__ == ""__main__"":

  import sys 
  import argparse

  parser = argparse.ArgumentParser(
     description=""Measure the performance of tensorflow embeddingbag using tf.nn.embedding"" )
  parser.add_argument(""--features"", type=int, default=10)
  parser.add_argument(""--em"", type=int, default=2)
  parser.add_argument(""--nnz"", type=int, default=2)
  parser.add_argument(""--batch"", type=int, default=4)
  parser.add_argument(""--steps"", type=int, default=1)
  parser.add_argument(""--warmups"", type=int, default=0)

  args = parser.parse_args()

  features     = args.features
  em           = args.em
  nnz          = args.nnz
  batch        = args.batch
  steps        = args.steps
  warmups      = args.warmups

  sp_ids = np.random.randint(0, features, (batch * nnz,))
  res = tf.zeros([batch, em])

  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=""grpc://""+os.environ[""TPU_IP""])
  tf.config.experimental_connect_to_cluster(resolver)
  tf.tpu.experimental.initialize_tpu_system(resolver)
  print(""   "")
  tpus = tf.config.list_logical_devices('TPU')
  print(""There are {} tpu logical devices"".format(len(tpus)))
  print(tpus[0])

  with tf.device('TPU:0'):
    params = tf.random.uniform([features, em])
    res = measure(params, sp_ids, tf.constant(steps), features//2)
 
  print(res)
```
But got the following error:

```bash
hongzhang@shan-tf1:~$ python  embtest.py --features=1000 --nnz=30 --batch=128
Eager execution :  True
2020-10-05 08:23:42.244623: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-05 08:23:42.250601: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz
2020-10-05 08:23:42.251595: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4c1dde0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-05 08:23:42.251631: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-05 08:23:42.263068: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.178.175.58:8470}
2020-10-05 08:23:42.263113: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:38651}
2020-10-05 08:23:42.279709: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.178.175.58:8470}
2020-10-05 08:23:42.279743: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:38651}
2020-10-05 08:23:42.280176: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://localhost:38651
   
There are 8 tpu logical devices
LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')
Traceback (most recent call last):
  File ""embtest.py"", line 84, in <module>
    t1 = measure(params, sp_ids, tf.constant(steps), features//2)
  File ""embtest.py"", line 15, in measure
    res = tf.nn.embedding_lookup([params[0:thr],params[thr:]], sp_ids, None, name=""TEST1"")
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/embedding_ops.py"", line 394, in embedding_lookup_v2
    return embedding_lookup(params, ids, ""div"", name, max_norm=max_norm)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/embedding_ops.py"", line 328, in embedding_lookup
    transform_fn=None)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/embedding_ops.py"", line 246, in _embedding_lookup_and_transform
    ret.set_shape(ids.get_shape().concatenate(element_shape_s))
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 1206, in set_shape
    if not self.shape.is_compatible_with(shape):
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 1167, in shape
    self._tensor_shape = tensor_shape.TensorShape(self._shape_tuple())
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.

Largest program allocations in vmem:

  XLA label: register allocator spill slots
  Allocation type: scoped

  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}
  Allocation type: scoped

  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}
  Allocation type: scoped

  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}
  Allocation type: scoped

  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}
  Allocation type: scoped

2020-10-05 08:23:59.826142: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.

Largest program allocations in vmem:

  XLA label: register allocator spill slots
  Allocation type: scoped

  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}
  Allocation type: scoped

  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}
  Allocation type: scoped

  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}
  Allocation type: scoped

  XLA label: %concatenate.724 = f32[3840,2]{0,1:T(2,128)} concatenate(f32[1,2]{0,1:T(2,128)}, f32[3,2]{0,1:T(2,128)}, f32[5,2]{0,1:T(2,128)}, f32[1,2]{0,1:T(2,128)}, ...(+2400)), dimensions={0}
  Allocation type: scoped
```

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
os: Linux
os kernel version: #1 SMP Debian 4.19.146-1 (2020-09-17)
os release version: 4.19.0-11-cloud-amd64
os platform: Linux-4.19.0-11-cloud-amd64-x86_64-with-debian-10.6
linux distribution: ('debian', '10.6', '')
linux os distribution: ('debian', '10.6', '')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='shan-tf1', release='4.19.0-11-cloud-amd64', version='#1 SMP Debian 4.19.146-1 (2020-09-17)', machine='x86_64', processor='')
architecture: ('64bit', 'ELF')
machine: x86_64

- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):

tf.version.VERSION = 2.3.0-dev20200620
tf.version.GIT_VERSION = v1.12.1-34769-gfd2d4cdb70
tf.version.COMPILER_VERSION = 7.3.1 20180303

- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

"
43783,bitwise op with 1 msb throws in tf.function,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
tf 2.2, windows 10 (can't install tf 2.3 because various open issues)
cuda 10.1

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
v2.2.0-57-g25fba035f3 2.2.1

**Describe the current behavior**

```
@tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.int32)])
def f(t):
    return tf.bitwise.bitwise_and(t, 0b10000000000000000000000000000000)

f(tf.constant(2, dtype=tf.int32))
```
throws `OverflowError: Python int too large to convert to C long`

(Works fine without `tf.function`)

**Describe the expected behavior**
should not throw (return 0 in this case)

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
Traceback (most recent call last):
  File ""<root>/t.py"", line 8, in <module>
    f(tf.constant(2, dtype=tf.int32))
  File ""<site-packages>\tensorflow\python\eager\def_function.py"", line 580, in __call__
    result = self._call(*args, **kwds)
  File ""<site-packages>\tensorflow\python\eager\def_function.py"", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""<site-packages>\tensorflow\python\eager\def_function.py"", line 506, in _initialize
    *args, **kwds))
  File ""<site-packages>\tensorflow\python\eager\function.py"", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""<site-packages>\tensorflow\python\eager\function.py"", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""<site-packages>\tensorflow\python\eager\function.py"", line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""<site-packages>\tensorflow\python\framework\func_graph.py"", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""<site-packages>\tensorflow\python\eager\def_function.py"", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""<site-packages>\tensorflow\python\framework\func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
tensorflow.python.autograph.impl.api.StagingError: in user code:

    <root>/t.py:6 f  *
        return tf.bitwise.bitwise_and(t, 0b10000000000000000000000000000000)
    <site-packages>\tensorflow\python\ops\gen_bitwise_ops.py:81 bitwise_and  **
        ""BitwiseAnd"", x=x, y=y, name=name)
    <site-packages>\tensorflow\python\framework\op_def_library.py:470 _apply_op_helper
        preferred_dtype=default_dtype)
    <site-packages>\tensorflow\python\framework\ops.py:1341 convert_to_tensor
        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    <site-packages>\tensorflow\python\framework\tensor_conversion_registry.py:52 _default_conversion_function
        return constant_op.constant(value, dtype, name=name)
    <site-packages>\tensorflow\python\framework\constant_op.py:262 constant
        allow_broadcast=True)
    <site-packages>\tensorflow\python\framework\constant_op.py:300 _constant_impl
        allow_broadcast=allow_broadcast))
    <site-packages>\tensorflow\python\framework\tensor_util.py:452 make_tensor_proto
        nparray = np.array(values, dtype=np_dt)

    OverflowError: Python int too large to convert to C long


Process finished with exit code 1
```
"
43782,can't compile tensorflow from source,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14 and 1.13
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip 
- Bazel version (if compiling from source): 0.24.1 
- GCC/Compiler version (if compiling from source): gcc-4.8.5 for bazel and gcc-8 for cuda libary
- CUDA/cuDNN version: 10.1 / 7
- GPU model and memory: 1070 ti 8gb



**Describe the problem**
I can't compile from tensorflow 1.14-1.13 source code (haven't tried other 1 tensorflow branches) and 2.4 has been successfully compiled with and without GPU. Originally the error was with newer versions of basel and gcc-8, but after reading the https://www.tensorflow.org/install/source table, I installed gcc-4.8, but the error persisted.


I have read the solutions
https://github.com/tensorflow/serving/issues/928 but adding the --cxxopt = -std = c ++ 11 flag didn't work for me
I also read that installing libc-ares-dev library might help, but that didn't help either

I thought the problem might be related to the fact that I am using cuda-10.1 and not 10.0 as in the table. I tried to set up the file without cuda, but I got exactly the same error, so I think this is not the problem
**Provide the exact sequence of commands / steps that you executed before running into the problem**

**writte config file**
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.24.1 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 


Found possible Python library paths:
  /usr/local/lib/python3.8/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.8/dist-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Found CUDA 10.1 in:
    /usr/local/cuda/lib64
    /usr/local/cuda/include
Found cuDNN 7 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: 


Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc-8


Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: -march=native -mno-avx


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apache Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished


**build command**
bazel build --cxxopt=-std=c++11 --config=cuda  //tensorflow/tools/pip_package:build_pip_package


**output error**
INFO: From ProtoCompile external/com_github_googleapis_googleapis/google/api/http.pb.h:
bazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.
INFO: From ProtoCompile external/com_github_googleapis_googleapis/google/rpc/status.pb.h:
bazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.
INFO: From Compiling external/mkl_dnn/src/cpu/simple_concat.cpp:
external/mkl_dnn/src/cpu/simple_concat.cpp:98: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
             PRAGMA_OMP_SIMD()
 
INFO: From Compiling external/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc:
external/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc: In function 'grpc_error* try_http_parsing(grpc_chttp2_transport*)':
external/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc:2466:40: warning: 'void* memset(void*, int, size_t)' clearing an object of non-trivial type 'grpc_http_response' {aka 'struct grpc_http_response'}; use assignment or value-initialization instead [-Wclass-memaccess]
   memset(&response, 0, sizeof(response));
                                        ^
In file included from external/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc:44:
external/grpc/src/core/lib/http/parser.h:71:16: note: 'grpc_http_response' {aka 'struct grpc_http_response'} declared here
 typedef struct grpc_http_response {
                ^~~~~~~~~~~~~~~~~~
ERROR: /home/dmitry/.cache/bazel/_bazel_dmitry/1319b3f3ad6252a51422db144992f79d/external/grpc/BUILD:507:1: C++ compilation of rule '@grpc//:gpr_base' failed (Exit 1)
external/grpc/src/core/lib/gpr/log_linux.cc:43:13: error: ambiguating new declaration of 'long int gettid()'
 static long gettid(void) { return syscall(__NR_gettid); }
             ^~~~~~
In file included from /usr/include/unistd.h:1170,
                 from external/grpc/src/core/lib/gpr/log_linux.cc:41:
/usr/include/x86_64-linux-gnu/bits/unistd_ext.h:34:16: note: old declaration '__pid_t gettid()'
 extern __pid_t gettid (void) __THROW;
                ^~~~~~
external/grpc/src/core/lib/gpr/log_linux.cc:43:13: warning: 'long int gettid()' defined but not used [-Wunused-function]
 static long gettid(void) { return syscall(__NR_gettid); }
             ^~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 281.090s, Critical Path: 57.54s
INFO: 1539 processes: 1539 local.
FAILED: Build did NOT complete successfully



(there was even more INFO output, but I did not copy everything, there really is a lot)
"
43781,[MLIR] TFlite contains unused file,"**tensorflow/compiler/mlir/lite/transforms/load_quantization_recipe.cc** is called via 
`tf-opt -allow-unregistered-dialect -tfl-load-recipe %s | FileCheck %s`

However, according to [this commit](https://github.com/tensorflow/tensorflow/commit/36167cb04c890f55f8abf1df1d79bf4bf6361d08), ""-allow-unregistered-dialect"" is disabled (the only way to call the file is deprecated).
Hence, in **tensorflow/compiler/mlir/lite/transforms/load_quantization_recipe.cc** can be removed as it is not involved in any mlir::tfl passes."
43778,Tensorflow does not work because of Cudnn library issue,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.15.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0 and 7.4
- GPU model and memory: RTX 2060 6GB VRAM

**Describe the problem**
I first installed tf 2.3.0 with CUDA version: 10.1 and CUDnn version: 7.6. I had to work with some old code of mine so I uninstalled the previous CUDA, deleted all the files, and did a fresh install of the CUDA and CUDnn versions mentioned above along with tf 1.15.0. When I try to run the TensorFlow python program I keep getting the same error. It has something to do with source compilation of the previous CUDnn version (7.6.0), I cannot seem to find how to undo this, I tried reinstalling anaconda as well and checked all the environment variable paths of my system, all of them seem to point to the right directories.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I ran the file via command line using python main.py. 

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

EPOCH 1 ...
2020-10-04 16:15:46.208813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-04 16:15:47.780566: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.2 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2020-10-04 16:15:47.793953: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.2 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
Traceback (most recent call last):
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\client\session.py"", line 1365, in _do_call
    return fn(*args)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\client\session.py"", line 1350, in _run_fn
    target_list, run_metadata)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\client\session.py"", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[{{node conv1_1/Conv2D}}]]
         [[Mean/_73]]
  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[{{node conv1_1/Conv2D}}]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main_test.py"", line 202, in <module>
    run()
  File ""main_test.py"", line 193, in run
    correct_label, keep_prob, learning_rate)
  File ""main_test.py"", line 146, in train_nn
    feed_dict={input_image: image, correct_label: label,                                keep_prob: 0.5, learning_rate: 0.0009})
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\client\session.py"", line 956, in run
    run_metadata_ptr)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\client\session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\client\session.py"", line 1359, in _do_run
    run_metadata)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\client\session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[node conv1_1/Conv2D (defined at C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\framework\ops.py:1748) ]]
         [[Mean/_73]]
  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[node conv1_1/Conv2D (defined at C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\framework\ops.py:1748) ]]
0 successful operations.
0 derived errors ignored.

Original stack trace for 'conv1_1/Conv2D':
  File ""main_test.py"", line 202, in <module>
    run()
  File ""main_test.py"", line 184, in run
    input_image, keep_prob, vgg_layer3_out, vgg_layer4_out, vgg_layer7_out = load_vgg(sess, vgg_path)
  File ""main_test.py"", line 36, in load_vgg
    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\saved_model\loader_impl.py"", line 269, in load
    return loader.load(sess, tags, import_scope, **saver_kwargs)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\saved_model\loader_impl.py"", line 422, in load
    **saver_kwargs)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\saved_model\loader_impl.py"", line 352, in load_graph
    meta_graph_def, import_scope=import_scope, **saver_kwargs)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\training\saver.py"", line 1477, in _import_meta_graph_with_return_elements
    **kwargs))
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\framework\meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements
    return_elements=return_elements)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\framework\importer.py"", line 405, in import_graph_def
    producer_op_list=producer_op_list)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\framework\importer.py"", line 517, in _import_graph_def_internal
    _ProcessNewOps(graph)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\framework\importer.py"", line 243, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 3561, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 3561, in <listcomp>
    for c_op in c_api_util.new_tf_operations(self)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 3451, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""C:\Users\manas\anaconda3\envs\tensorflow1-gpu\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()
"
43777,"Trivial example does not save/restore weights, prints warnings from TensorFlow/Keras implementation","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.7, colab
- TensorFlow installed from (source or binary): pip, colab
- TensorFlow version (use command below): 2.3.1 (MacOS), v2.3.0-0-gb36436b087 2.3.0 (colab)
- Python version: 3.8.5 (MacOS) 3.6.9 (colab)

**Describe the current behavior**
Trained weights are not restored, two warnings about the TensorFlow implementation are printed:
`WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.`

**Describe the expected behavior**
Weights are restored (test accuracy is consistent with saved model), no warnings printed.
I find several *closed* bug reports from others over that look like the same problems  (lack of saving and the bogus warnings), going back to the 2.0.0 days. It has clearly not been fixed.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')])
model.compile(optimizer='rmsprop',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(
    x_train,
    y_train,
    epochs=8,
    batch_size=128,
    validation_split=0.2)

test_loss, test_acc = model.evaluate(x_test, y_test)
print('model_test_acc:', test_acc)

model.save(""my_saved_path"")

saved_model = tf.keras.models.load_model(""my_saved_path"")

test_loss, test_acc = saved_model.evaluate(x_test, y_test)
print('saved_model_test_acc:', test_acc)
```"
43776,OSError: [WinError 126] The specified module could not be found even when following documentation,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- windows 10 home single language 
- TensorFlow installed from :cmd
- TensorFlow version:1.14
- Python version:3.7.9
- Installed using virtualenv? pip? conda?:pip



**Describe the problem**
i was following the guide to install the imageai documentation  and did this 
pip3 install tensorflow==1.14
pip3 install opencv-python
pip3 install keras==2.2.4
pip3 install numpy==1.18.5
pip3 install imageai --upgrade 
i changed from 1.13.1 to 1.14 because it wasnt working before, same thing with numpy from 1.16.1 to 1.18.5(the lasted accepted according to the cmd) 
but i cant run the image ai because every time i try to do something it says 
OSError: [WinError 126] The specified module could not be found 
heres the full error 
**
C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\__init__.py"", line 83, in <module>
    from tensorflow.python import keras
  File ""C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\keras\__init__.py"", line 26, in <module>
    from tensorflow.python.keras import activations
  File ""C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\keras\activations.py"", line 24, in <module>
    from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object
  File ""C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\keras\utils\__init__.py"", line 39, in <module>
    from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model
  File ""C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\keras\utils\multi_gpu_utils.py"", line 22, in <module>
    from tensorflow.python.keras.engine.training import Model
  File ""C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\keras\engine\training.py"", line 41, in <module>
    from tensorflow.python.keras.engine import training_arrays
  File ""C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 40, in <module>
    from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top
  File ""C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\scipy\__init__.py"", line 136, in <module>
    from . import _distributor_init
  File ""C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\scipy\_distributor_init.py"", line 61, in <module>
    WinDLL(os.path.abspath(filename))
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.2544.0_x64__qbz5n2kfra8p0\lib\ctypes\__init__.py"", line 364, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found 
** 
ive put 
C:\Users\Leoga\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\Scripts 
to path as well and it still does not work 
everytime i import tensorflow or imageai or even keras this error happens
and ive tried following some people with similar problems but none of them worked for me, like downloading a wheel a user found and so on 
ive been trying to slove this for 2 days please help me
"
43775,Add metadata in tensorflow lite model,How to add metadata in object detection model? In documentation only about image classifier model is given.
43773,Saving and loading Keras model with RNN layer that uses both multiple input and constants will result in ValueError when using model.predict(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.3.0
- Python version:3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
When using a RNN layer that has multiple inputs, as well as contants input, saving and loading it to disk using the H5 file format will produce the following when using model.predict:

`tensorflow\python\keras\engine\input_spec.py:155 assert_input_compatibility
        raise ValueError('Layer ' + layer_name + ' expects ' +

    ValueError: Layer rnn expects 3 inputs, but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 5, 5) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 5, 5) dtype=float32>]`

**Describe the expected behavior**
I expect equal results (and thus no ValueError) when calling model.predict() on a certain set of inputs, and doing it again but after saving and loading the model

**Standalone code to reproduce the issue**
```
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


class RNNCellWithConstants(keras.layers.Layer):

  def __init__(self, units, constant_size, **kwargs):
    self.units = units
    self.state_size = units
    self.constant_size = constant_size
    super(RNNCellWithConstants, self).__init__(**kwargs)

  def build(self, input_shape):
    self.input_kernel = self.add_weight(
        shape=(input_shape[0][1], self.units),
        initializer='uniform',
        name='kernel')
    self.recurrent_kernel = self.add_weight(
        shape=(self.units, self.units),
        initializer='uniform',
        name='recurrent_kernel')
    self.constant_kernel = self.add_weight(
        shape=(self.constant_size, self.units),
        initializer='uniform',
        name='constant_kernel')
    self.built = True

  def call(self, inputs, states, constants):
    [x1, _] = inputs
    [prev_output] = states
    [constant] = constants
    h_input = keras.backend.dot(x1, self.input_kernel)
    h_state = keras.backend.dot(prev_output, self.recurrent_kernel)
    h_const = keras.backend.dot(constant, self.constant_kernel)
    output = h_input + h_state + h_const
    return output, [output]

  def get_config(self):
    config = {'units': self.units, 'constant_size': self.constant_size}
    base_config = super(RNNCellWithConstants, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


x1 = keras.Input((None, 5))
x2 = keras.Input((None, 5))
c = keras.Input((3,))
cell = RNNCellWithConstants(32, constant_size=3)
layer = keras.layers.RNN(cell)
y = layer((x1,x2), constants=c)

model = keras.models.Model([x1, x2, c], y)
model.compile(
    optimizer='rmsprop',
    loss='mse')
model.train_on_batch(
    [np.zeros((6, 5, 5)), np.zeros((6, 5, 5)), np.zeros((6, 3))],
    np.zeros((6, 32))
)

# Test basic case 
x1_np = np.random.random((6, 5, 5))
x2_np = np.random.random((6, 5, 5))
c_np = np.random.random((6, 3))
y_np = model.predict([x1_np, x2_np, c_np]) 

model.save(""test.h5"")
loaded_model = keras.models.load_model(""test.h5"", custom_objects={""RNNCellWithConstants"":RNNCellWithConstants})
loaded_y_np = loaded_model.predict([x1_np, x2_np, c_np]) 
assert np.array_equal(y_np, loaded_y_np)
```

**Other info / logs**
I also figured out why this happens: Python's standard json encoder converts list and tuples into array, and the decoder always turns array's into list. When the RNN __call__ function gets called with a list instead of tuple, the following line
````
        full_input_spec = generic_utils.to_list(
            nest.map_structure(lambda _: None, inputs)) + additional_specs
````
Will map that input differently than if it were a tuple.

I will be submitting a pr to fix this
"
43772,third_party/gpus/find_cuda_config.py is unable to detect cudnn version and causing `./configure` to fail,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.1.2 (checked out at this release tag)
- Python version: Python 3.8.3
- Installed using virtualenv? pip? conda?: Conda (conda 4.8.5)
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-10ubuntu2) 9.3.0
- CUDA/cuDNN version: CUDA 11.1, cuDNN 8
- GPU model and memory: RTX 3080, 10G


**Describe the problem**
When I was trying to configure TensorFlow to build from source, I got the following error:

```
Traceback (most recent call last):
  File ""./configure.py"", line 1554, in <module>
    main()
  File ""./configure.py"", line 1437, in main
    if validate_cuda_config(environ_cp):
  File ""./configure.py"", line 1318, in validate_cuda_config
    config = dict(
ValueError: dictionary update sequence element #9 has length 1; 2 is required
```

and after debugging I noticed that the issue is that the cuDNN version isn't detected, and thus `./configure` fails.


third_party/gpus/find_cuda_config.py
cudnn_version.h

**Provide the exact sequence of commands / steps that you executed before running into the problem**

My `./configure` sequence:

```
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.29.1 installed.
Please specify the location of python. [Default is /home/madz/anaconda3/envs/tensorflow/bin/python]: 


Found possible Python library paths:
  /home/madz/anaconda3/envs/tensorflow/lib/python3.8/site-packages
Please input the desired Python library path to use.  Default is [/home/madz/anaconda3/envs/tensorflow/lib/python3.8/site-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: 
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: Y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Traceback (most recent call last):
  File ""./configure.py"", line 1554, in <module>
    main()
  File ""./configure.py"", line 1437, in main
    if validate_cuda_config(environ_cp):
  File ""./configure.py"", line 1318, in validate_cuda_config
    config = dict(
ValueError: dictionary update sequence element #9 has length 1; 2 is required
```

**Any other info / logs**

Then when I tried to run `python third_party/gpus/find_cuda_config.py cuda cudnn`, I got the following output:

```
cublas_include_dir: /usr/local/cuda/include
cublas_library_dir: /usr/local/cuda/lib64
cuda_binary_dir: /usr/local/cuda/bin
cuda_include_dir: /usr/local/cuda/include
cuda_library_dir: /usr/local/cuda/lib64
cuda_toolkit_path: /usr/local/cuda
cuda_version: 11.1
cudnn_include_dir: /usr/local/cuda/include
cudnn_library_dir: /usr/local/cuda/lib64
cudnn_version: 
cupti_include_dir: /usr/local/cuda/include
cupti_library_dir: /usr/local/cuda/lib64
nvvm_library_dir: /usr/local/cuda/nvvm/libdevice
```

It turns out for my cuDNN installation, the header file that should be checked is `cudnn_version.h` and not `cudnn.h`. I wanted to make a pull request, but I'm not really sure how to adjust this so that it can use either header file.


Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43771,Defined layers but received error:  __init__() missing 1 required positional argument: 'layer',"print(tf.__version__) 2.2.0
print(keras.__version__) 2.3.0-tf

Message:
 __init__() missing 1 required positional argument: 'layer'

I would like to better understand why I get this message with the following definition used in the example:

https://aihub.cloud.google.com/p/products%2F2290fc65-0041-4c87-a898-0289f59aa8ba

Model function definition:

def model_fn(features, labels, mode, params = params):
    # import the data and unpack the features
    # serving input_fn returns a dict, convert to multivalue obj
    if isinstance(features, dict):
        features = features['words'], features['length']
   
    words, length = features
   
    # Embedding
    embedding = tf.Variable(tf.random.normal([params['max_words'], params['dim']]))
    embedding_lookup_for_x = tf.nn.embedding_lookup(embedding, words)
   
    # LSTM
    lstm_cell_fw = tf.keras.layers.LSTMCell(params['lstm_size'])
    lstm_cell_bw = tf.keras.layers.LSTMCell(params['lstm_size'])
    states, final_state = tf.keras.layers.Bidirectional(
                                        cell_fw = lstm_cell_fw,
                                        cell_bw = lstm_cell_bw,
                                        inputs = embedding_lookup_for_x,
                                        dtype = tf.float32,
                                        time_major = False,
                                        sequence_length = length)
    lstm_out = tf.keras.layers.concat([states[0], states[1]], axis = 2)


The original definition was (no longer supported):

    # LSTM
#    lstm_cell_fw = tf.nn.rnn_cell.BasicLSTMCell(params['lstm_size'], state_is_tuple = True)
#    lstm_cell_bw = tf.nn.rnn_cell.BasicLSTMCell(params['lstm_size'], state_is_tuple = True)
#    states, final_state = tf.nn.bidirectional_dynamic_rnn(
#                                        cell_fw = lstm_cell_fw,
#                                        cell_bw = lstm_cell_bw,
#                                        inputs = embedding_lookup_for_x,
#                                        dtype = tf.float32,
#                                        time_major = False,
#                                        sequence_length = length)
#    lstm_out = tf.concat([states[0], states[1]], axis = 2)

Please advise."
43770,Wrong paragraph ordering in tutorial,"## URL(s) with the issue:

https://www.tensorflow.org/tutorials/keras/classification

## Description of issue (what needs changing):

When you open the above tutorial, the description of the dataset (Fashion MNIST) is placed at the bottom of the page.
However, in [the original .ipynb file](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/classification.ipynb), it is in the first section ""Import the Fashion MNIST dataset"".
It seems that the order of the paragraph is wrongly rearranged when building the web page."
43769,tensorflow is not running on my miniconda,"I have installed tensorflow using pip install, but it does not run.

When trying to run it, I receive the following error message:

(base) PS C:\WINDOWS\system32> python
Python 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)] :: Ana
conda, Inc. on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\pywra
p_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\pywra
p_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\pywra
p_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\ProgramData\Miniconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Miniconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routin
e failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\__init__.py"", lin
e 98, in <module>
    from tensorflow_core import *
  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\__init__.py""
, line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\__init__.py"", lin
e 50, in __getattr__
    module = self._load()
  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow\__init__.py"", lin
e 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\ProgramData\Miniconda3\lib\importlib\__init__.py"", line 127, in impor
t_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\__ini
t__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\pywra
p_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\pywra
p_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\pywra
p_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\pywra
p_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\ProgramData\Miniconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Miniconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routin
e failed.


Failed to load the native TensorFlow runtime.

Please help to fix it.

"
43768,Dataset.from_generator() returns a Dataset which freezes execution upon iteration,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Google Colab, running Ubuntu 18.04.5 LTS (Bionic Beaver)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **None**
- TensorFlow installed from (source or binary): **Source** (according to [this link](https://colab.research.google.com/notebooks/tensorflow_version.ipynb#scrollTo=8UvRkm1JGUrk))
- TensorFlow version (use command below): **v2.3.0-0-gb36436b087 2.3.0**
- Python version: **Python 3.6.9**

**running !bash tf_env_collect.sh threw an error, could not collect the following parameters:**
- Bazel version (if compiling from source): ?
- GCC/Compiler version (if compiling from source): ?
- CUDA/cuDNN version: ?
- GPU model and memory: ? (dynamically allocated by Google colab)

**Describe the current behavior**
According to the docs, tf.data.Dataset has a static method called from_generator(), constructing a dataset from a given iterable. I performed the following actions:

```
1) downloaded a text file from a link
2) parsed the file using a TextLineDataset (called raw_lines), then limited the size using dataset.take(100)

3) Created a new dataset (called after_generator_a) using an identity generator on the lines of raw_lines
4) used take(1) to obtain a subset dataset (from after_generator_a), containing a single record
5) iterated over the dataset to print the only record. The record was successfully printed

6) Created a new dataset (called after_generator_b) using an identity generator on the lines of after_generator_a
7) used take(1) to obtain a subset dataset (from after_generator_a), containing a single record
8) iterated over the dataset to print the only record. Nothing prints, and the Google Colab environment no longer responds
```

**Describe the expected behavior**
The code in 8) should print the value successfully, and not crash the Colab environment


**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/11ILZUYpb-8Solip6xTbdV8elvPGOpngo?usp=sharing

**Other info / logs** 
None
"
43767,Build error: concat_lib_gpu.cc(123): error C2908: explicit specialization,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: 3.6.8
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): MSVC 2019
- CUDA/cuDNN version: 10.1 / 7.6.0
- Building for CC: 3.5
- GPU model and memory: RTX 2080 TI

**Describe the problem**

Build error: concat_lib_gpu.cc(123): error C2908: explicit specialization 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

pre set env vars
```
env['TEST_TMPDIR'] = env['USERPROFILE']
env['PATH'] = ';'.join ( [str( cwd_path / 'tools' / ('bazel_'+version) ), str(msys_path), str(msys_usr_bin_path), 
                                  env['PATH']] )
env['BAZEL_SH'] = str ( msys_usr_bin_path / 'bash.exe' )
env['BAZEL_VC'] = env['VCINSTALLDIR']
env['BAZEL_POWERSHELL'] = r'C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe'
env['MSYS_NO_PATHCONV']  = '1'
env['MSYS2_ARG_CONV_EXCL']  = '""*""'

env['TF_NEED_CUDA']='1'

env['CUDA_TOOLKIT_PATH'] = str(cuda_toolkit_path).replace ('\\','\\\\' )
env['TF_CUDA_PATHS'] = env['CUDA_TOOLKIT_PATH']
env['CUDNN_INSTALL_PATH'] = str (cudnn_install_path).replace ('\\','\\\\' )
env['TF_CUDA_VERSION'] = cuda_ver
env['TF_CUDNN_VERSION'] = cudnn_ver[0]
env['TF_CUDA_COMPUTE_CAPABILITIES'] = cc
env['CUDA_BIN_PATH'] = str ( cuda_toolkit_path / 'bin' )
env['CUDNN_INCLUDE'] = str ( cudnn_install_path / 'include' )
env['CUDNN_LIBRARY'] = str ( cudnn_install_path / 'lib' / 'x64' )
env['CUDNN_ROOT'] = str ( cudnn_install_path )

env['PATH'] = ';'.join ( [ str(cudnn_install_path / 'bin'), env['CUDA_BIN_PATH'], 
       str(cuda_toolkit_path / 'extras' / 'CUPTI' / 'libx64'), env['PATH'] ] )

env['USE_DEFAULT_PYTHON_LIB_PATH']  = '1'
env['TF_NEED_TENSORRT']  = '0'
env['TF_NEED_IGNITE']  = '0'
env['TF_NEED_ROCM']  = '0'
env['TF_ENABLE_XLA']  = '0'
env['TF_OVERRIDE_EIGEN_STRONG_INLINE']  = '0'
env['TF_SET_ANDROID_WORKSPACE']  = '0'
env['CC_OPT_FLAGS'] = ""/W0 /DTHRUST_IGNORE_CUB_VERSION_CHECK""
```

run commands
```
bazel clean
python configure.py
bazel build --config=cuda --config=v2 --copt=-nvcc_options=disable-warnings 
      --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**

error log:

```
ERROR: D:/developpython/tools/tensorflow/tensorflow-master/tensorflow/core/kerne
ls/BUILD:258:1: C++ compilation of rule '//tensorflow/core/kernels:concat_lib' f
ailed (Exit 2): python.exe failed: error executing command
  cd D:/developpython/_internal/__z/u/_bazel_administrator/wtpap475/execroot/org
_tensorflow
  SET CUDA_TOOLKIT_PATH=D:/DevelopPython/tools/CUDA/v10.1
    SET CUDNN_INSTALL_PATH=D:\DevelopPython\tools\CUDNN\10.1-7.6.5
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC
\Tools\MSVC\14.27.29110\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6
.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C
:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Fil
es (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows
Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\includ
e\10.0.18362.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Too
ls\MSVC\14.27.29110\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\l
ib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Pr
ogram Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Commo
n7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Vi
sual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64;C:\Program
Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\
Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExt
ensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019
\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C
:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin
\Roslyn;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Too
ls\x64\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program F
iles (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studi
o\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.
30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\
;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C
:\Windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/DevelopPython/python/3.6.8/python.exe
    SET PYTHON_LIB_PATH=D:/DevelopPython/python/3.6.8/Lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=D:\DevelopPython\_internal\__z\t
    SET TF2_BEHAVIOR=1
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5
    SET TF_CUDA_PATHS=D:\DevelopPython\tools\CUDA\v10.1
    SET TF_CUDA_VERSION=10.1
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TMP=D:\DevelopPython\_internal\__z\t
  D:/DevelopPython/python/3.6.8/python.exe -B external/local_config_cuda/crossto
ol/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_
WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STD
EXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /w
d4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibaz
el-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ib
azel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-o
ut/x64_windows-opt/bin/external/nsync /Iexternal/gif /Ibazel-out/x64_windows-opt
/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/exter
nal/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin
/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x
64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archiv
e /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ib
azel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x
64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-
opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/b
in/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/
external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/ext
ernal/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows
-opt/bin/external/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external
/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_w
indows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers
 /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includ
es/cublas_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_
cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/externa
l/local_config_cuda/cuda/_virtual_includes/cufft_headers_virtual /Ibazel-out/x64
_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/curand_header
s_virtual /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eige
n_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync
/public /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/g
if/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_g
oogle_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/
src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/far
mhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib
/Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_con
version /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/extern
al/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel
-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal
/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/
local_config_cuda/cuda/cublas/include /Iexternal/local_config_cuda/cuda/cufft/in
clude /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cufft/incl
ude /Iexternal/local_config_cuda/cuda/curand/include /Ibazel-out/x64_windows-opt
/bin/external/local_config_cuda/cuda/curand/include /DTF_USE_SNAPPY /DEIGEN_MPL2
_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /D__CLANG_SUPPORT_DYN
_ANNOTATION__ /showIncludes /MD /O2 /DNDEBUG /w /D_USE_MATH_DEFINES -DWIN32_LEAN
_AND_MEAN -DNOGDI /experimental:preprocessor -nvcc_options=disable-warnings /std
:c++14 -DGOOGLE_CUDA=1 -DTENSORFLOW_USE_NVCC=1 -DTENSORFLOW_MONOLITHIC_BUILD /DP
LATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_A
VOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY
-DGOOGLE_CUDA=1 /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/c
oncat_lib/concat_lib_gpu.obj /c tensorflow/core/kernels/concat_lib_gpu.cc
Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been de
precated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental
:preprocessor'
tensorflow/core/kernels/concat_lib_gpu.cc(123): error C2908: explicit specializa
tion; 'void tensorflow::ConcatGPU<tensorflow::uint16>(tensorflow::OpKernelContex
t *,const std::vector<std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<const T,2,1
,IndexType>,16,Eigen::MakePointer>,std::default_delete<Eigen::TensorMap<Eigen::T
ensor<const T,2,1,IndexType>,16,Eigen::MakePointer>>>,std::allocator<std::unique
_ptr<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,Eigen::MakePointer
>,std::default_delete<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,E
igen::MakePointer>>>>> &,tensorflow::Tensor *,Eigen::TensorMap<Eigen::Tensor<T,2
,1,IndexType>,16,Eigen::MakePointer> *)' has already been instantiated
        with
        [
            T=tensorflow::uint16,
            IndexType=Eigen::DenseIndex
        ]
tensorflow/core/kernels/concat_lib_gpu.cc(124): error C2908: explicit specializa
tion; 'void tensorflow::ConcatGPU<tensorflow::int8>(tensorflow::OpKernelContext
*,const std::vector<std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<const T,2,1,I
ndexType>,16,Eigen::MakePointer>,std::default_delete<Eigen::TensorMap<Eigen::Ten
sor<const T,2,1,IndexType>,16,Eigen::MakePointer>>>,std::allocator<std::unique_p
tr<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,Eigen::MakePointer>,
std::default_delete<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,Eig
en::MakePointer>>>>> &,tensorflow::Tensor *,Eigen::TensorMap<Eigen::Tensor<T,2,1
,IndexType>,16,Eigen::MakePointer> *)' has already been instantiated
        with
        [
            T=tensorflow::int8,
            IndexType=Eigen::DenseIndex
        ]
tensorflow/core/kernels/concat_lib_gpu.cc(131): error C2908: explicit specializa
tion; 'void tensorflow::ConcatGPU<tensorflow::uint32>(tensorflow::OpKernelContex
t *,const std::vector<std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<const T,2,1
,IndexType>,16,Eigen::MakePointer>,std::default_delete<Eigen::TensorMap<Eigen::T
ensor<const T,2,1,IndexType>,16,Eigen::MakePointer>>>,std::allocator<std::unique
_ptr<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,Eigen::MakePointer
>,std::default_delete<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,E
igen::MakePointer>>>>> &,tensorflow::Tensor *,Eigen::TensorMap<Eigen::Tensor<T,2
,1,IndexType>,16,Eigen::MakePointer> *)' has already been instantiated
        with
        [
            T=tensorflow::uint32,
            IndexType=Eigen::DenseIndex
        ]
tensorflow/core/kernels/concat_lib_gpu.cc(132): error C2908: explicit specializa
tion; 'void tensorflow::ConcatGPU<tensorflow::uint64>(tensorflow::OpKernelContex
t *,const std::vector<std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<const T,2,1
,IndexType>,16,Eigen::MakePointer>,std::default_delete<Eigen::TensorMap<Eigen::T
ensor<const T,2,1,IndexType>,16,Eigen::MakePointer>>>,std::allocator<std::unique
_ptr<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,Eigen::MakePointer
>,std::default_delete<Eigen::TensorMap<Eigen::Tensor<const T,2,1,IndexType>,16,E
igen::MakePointer>>>>> &,tensorflow::Tensor *,Eigen::TensorMap<Eigen::Tensor<T,2
,1,IndexType>,16,Eigen::MakePointer> *)' has already been instantiated
        with
        [
            T=tensorflow::uint64,
            IndexType=Eigen::DenseIndex
        ]
```
"
43766,Error when creating model with LSTM layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 1909 OS build 18363.1016
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
I am trying to run the example from here:
https://www.tensorflow.org/guide/ragged_tensor

After adding LSTM layer to the model I get an error:

> NotImplementedError: Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported

**Describe the expected behavior**

LSTM layer should be added with no error.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
keras_model = tf.keras.Sequential([
    tf.keras.layers.Embedding(1000, 16),
    tf.keras.layers.LSTM(32),
])

```

**Other info / logs**
Call stack:

>	Tensor.__array__ in ops line 848	Python
 	_wrapreduction in fromnumeric line 90	Python
 	prod in fromnumeric line 2962	Python
 	prod in C:\Users\FA\Google Drive\Colab Notebooks\PoetryTransformer\Accentizer\<__array_function__ internals> line 6	Python
 	_constant_if_small in array_ops line 2732	Python
 	zeros in array_ops line 2794	Python
 	wrapped in _tag_zeros_tensor in array_ops line 2747	Python
 	wrapper in add_dispatch_support in dispatch line 201	Python
 	create_zeros in _generate_zero_filled_state in recurrent line 2981	Python
 	<listcomp> in nest line 635	Python
 	map_structure in nest line 635	Python
 	_generate_zero_filled_state in recurrent line 2984	Python
 	_generate_zero_filled_state_for_cell in recurrent line 2968	Python
 	LSTMCell.get_initial_state in recurrent line 2524	Python
 	RNN.get_initial_state in recurrent line 646	Python
 	RNN._process_inputs in recurrent line 862	Python
 	LSTM.call in recurrent_v2 line 1108	Python
 	Layer._functional_construction_call in base_layer line 1117	Python
 	Layer.__call__ in base_layer line 926	Python
 	RNN.__call__ in recurrent line 663	Python
 	Sequential.add in sequential line 221	Python
 	_method_wrapper in no_automatic_dependency_tracking in base line 457	Python
 	Sequential.__init__ in sequential line 142	Python
 	_method_wrapper in no_automatic_dependency_tracking in base line 457	Python
 	ragged module line 5	Python
"
43765,LookupError: gradient registry has no entry for: TensorScatterMax,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.9
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.1
- GPU model and memory: Tesla P100-PCIE, 16280MiB

**Describe the current behavior**

I'm making YOLACT and while creating semantic_segmentation_loss function, I've used `tensor_scatter_nd_max` function which was introduced in tf 2.3. The function is as follows:

```
def _loss_semantic_segmentation(pred_seg, mask_gt, classes):
    batch_size, mask_h, mask_w, num_classes = tf.shape(pred_seg)
    loss_s = 0

    for i in range(batch_size):
        cur_segment = pred_seg[i]
        cur_class_gt = classes[i]
        masks = mask_gt[i]

        masks = tf.expand_dims(masks, axis=-1)
        masks = tf.image.resize(masks, [mask_h, mask_w], method=tf.image.ResizeMethod.BILINEAR)
        masks = tf.cast(masks + 0.5, tf.int64)
        masks = tf.squeeze(tf.cast(masks, tf.float32))

        segment_gt = tf.zeros_like(cur_segment) # [height, width, num_cls]
        segment_gt = tf.transpose(segment_gt, perm=(2, 0, 1))

        obj_cls = tf.expand_dims(cur_class_gt, axis=-1)
        segment_gt = tf.tensor_scatter_nd_max(segment_gt, indices=obj_cls, updates=masks)
        segment_gt = tf.transpose(segment_gt, perm=(1, 2, 0))

        loss_s += tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(segment_gt, cur_segment))

    return loss_s / mask_h / mask_w
```

But it throws me following error:
```
Traceback (most recent call last):
  File ""train.py"", line 300, in <module>
    app.run(main)
  File ""/home/deploy/.local/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/deploy/.local/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""train.py"", line 208, in main
    labels)
  File ""/home/deploy/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 761, in __call__
    return self._python_function(*args, **kwds)
  File ""train.py"", line 54, in train_step
    grads = tape.gradient(total_loss, model.trainable_variables)
  File ""/home/deploy/.local/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File ""/home/deploy/.local/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py"", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File ""/home/deploy/.local/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 151, in _gradient_function
    grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access
  File ""/home/deploy/.local/lib/python3.6/site-packages/tensorflow/python/framework/registry.py"", line 97, in lookup
    ""%s registry has no entry for: %s"" % (self._name, name))
LookupError: gradient registry has no entry for: TensorScatterMax

```

**Describe the expected behavior**
No error should come

**Standalone code to reproduce the issue**
The link to my repository is [this](https://github.com/anshkumar/yolact). 
To run:

> python3 train.py -tfrecord_dir '/home/deploy/ved/data' -weights '/home/deploy/ved/ckpt'  -train_iter '100000'  -batch_size '2' 

If you need tfrecord let me know.
"
43764,UnknownError: Failed to get convolution algorithm. ,"**System information**

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
TensorFlow installed from (source or binary): binary **(no errors during installation)**
TensorFlow version: 2.3.1
Python version: 3.7.5
CUDA/cuDNN version: 10.1
GPU model and memory: RTX 1660 Ti 6.00GB
[full script output.txt](https://github.com/tensorflow/tensorflow/files/5322982/full.script.output.txt)


**Describe the current behavior**
The code below throws the following exception 

`tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]
`
Full script's output is attached.
The code is pretty standard, it's weird to get this error. In the script I also put code that trains model via Keras's model.fit() - and it works. **The error occurs when GradientTape-approach is used.**
Also the same code (version with training via Keras model.fit() and version with GradiantTape) works fine on GoogleColab https://colab.research.google.com/drive/1fjbvlpEEEm3yvyKhcGyel3vLguEkU9GN?usp=sharing

**Standalone code to reproduce the issue**

```

import tensorflow as tf

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = x_train[..., tf.newaxis].astype(""float32"")
x_test = x_test[..., tf.newaxis].astype(""float32"")
nb_classes = 10
y_train = tf.keras.utils.to_categorical(y_train, nb_classes)
y_test = tf.keras.utils.to_categorical(y_test, nb_classes)
dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)

# oversimplified model just for example
inputs = tf.keras.layers.Input(shape=(28, 28, 1))
x = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)
x = tf.keras.layers.MaxPool2D(2)(x)
x = tf.keras.layers.Flatten()(x)
outputs = tf.keras.layers.Dense(nb_classes, activation='softmax')(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)

loss_func = tf.keras.losses.CategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()

for i, (xx, yy) in enumerate(dataset):
    with tf.GradientTape() as tape:
        y_pred = model(xx)
        loss = loss_func(yy, y_pred)
    grad = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grad, model.trainable_variables))
    print('batch {} processed'.format(i))

# if used this approach to train - everything works
# model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam())
# model.fit(x_train, y_train, batch_size=128, validation_data=(x_test, y_test))
# model.evaluate(x_test, y_test, batch_size=128, verbose=1)
```



**Other info / logs** 
Full script output is attached.
"
43763,Loss and metric calculated differently on validation,"I'm using `keras.applications.vgg19` with some custom loss function (cross-entropy based). I'm using the loss function both as a loss and as a metric. While training, I've been getting the same values for both loss and metric, but for validation, those values are different.

![image](https://user-images.githubusercontent.com/59140738/95012520-ecc66d00-0641-11eb-99e0-370ff41af92f.png)

You can see that for training, the loss and loss metric are the same (orange and blue lines that merge into one line), and for validation the loss and loss metric look different.


```
Epoch 1/7
 - 111s - loss: 3.3320 - loss_metric: 3.3320 - val_loss: 3.0186 - val_loss_metric: 3.0993 
Epoch 2/7
 - 90s - loss: 3.0934 - loss_metric: 3.0934 - val_loss: 3.0394  - val_loss_metric: 3.0424 
Epoch 3/7
 - 91s - loss: 3.0643 - loss_metric: 3.0643 - val_loss: 3.0139  - val_loss_metric: 3.0031 
Epoch 4/7
 - 90s - loss: 3.0411 - loss_metric: 3.0411 - val_loss: 3.0588  - val_loss_metric: 2.9770 
Epoch 5/7
 - 90s - loss: 3.0170 - loss_metric: 3.0170 - val_loss: 2.8466  - val_loss_metric: 2.9625 
Epoch 6/7
 - 90s - loss: 2.9866 - loss_metric: 2.9866 - val_loss: 3.0236  - val_loss_metric: 2.9133 
Epoch 7/7
 - 91s - loss: 2.9447 - loss_metric: 2.9447 - val_loss: 2.9063  - val_loss_metric: 3.0723
```

Some notes:

- I know this might be due to the usage of regularization, however, I'm not finding any regularization in the  [Keras implementation of VGG19](https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg19.py)
- I know there a was a [bug ](https://github.com/tensorflow/tensorflow/issues/25970#issuecomment-470210090)which has been fixed for TF2. I'm using TensorFlow GPU 2.1.0 and Keras 2.3.1

Can anyone please tell me what am I missing?

"
43761,"Error : Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution : Ubuntu20.04
- TensorFlow installed from (source or binary): conda install tensorflow-gpu
- TensorFlow version (use command below): 2.2
- Python version: 3.7
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: I installed necessary tool things [here](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) and [here](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)
- GPU model and memory: RTX2060, 6GB

**Describe the current behavior**
I think I did everything to try to fix the bug that when I run model.fit_generator the error occurs saying as the tytle. Some information I need to add is I tried on windows, linux and finally docker environment, on each of which I got the same result. Below is my docker file to create a container;

```
FROM nvidia/cuda:11.1-base-ubuntu20.04
RUN apt-get update && apt-get install -y\
    sudo\
    wget\
    vim
WORKDIR /opt
RUN wget https://repo.continuum.io/archive/Anaconda3-2020.07-Linux-x86_64.sh &&\
    sh Anaconda3-2020.07-Linux-x86_64.sh -b -p /opt/anaconda3 &&\
    rm -f Anaconda3-2020.07-Linux-x86_64.sh

ENV PATH /opt/anaconda3/bin:$PATH

RUN conda upgrade conda && conda install \
    keras\
    tensorflow-gpu

WORKDIR /

CMD [""jupyter"", ""lab"", ""--ip=0.0.0.0"", ""--allow-root"", ""--LabApp.tokenh=''""]`

```

and after I built the file, I ran the image by;

`docker run --gpus all -v ~:/work -p 8888:8888 <Image ID>`

One thing I finally couldn't get is... is it even possible to use tensorflow and GPU in Anaconda environment?
I was able to run the same code as below for a few days on Windows, but one day the code suddenly did't work at all.

---

```python
import os
import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.preprocessing.image import ImageDataGenerator

base_dir = 'cats_and_dogs_filtered'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')

train_cats_dir = os.path.join(train_dir, 'cats')
train_dogs_dir = os.path.join(train_dir, 'dogs')

validation_cats_dir = os.path.join(validation_dir, 'cats')
validation_dogs_dir = os.path.join(validation_dir, 'dogs')

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy',
              optimizer=RMSprop(lr=1e-4),
              metrics=['accuracy'])

train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
        train_dir,  # This is the source directory for training images
        target_size=(150, 150),  # All images will be resized to 150x150
        batch_size=40,
        # Since we use binary_crossentropy loss, we need binary labels
        class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=100,
        class_mode='binary')

history = model.fit_generator(
      train_generator,
      steps_per_epoch=50,  # 2000 images = batch_size * steps
      epochs=100,
      validation_data=validation_generator,
      validation_steps=10,  # 1000 images = batch_size * steps
      verbose=1)'

```
---

and I've got same errors as below forever;
```
UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node sequential/conv2d/Conv2D (defined at :60) ]] [Op:__inference_train_function_1187]

Function call stack:
train_function

```

Without docker environment, I should have installed CUDA Driver, CUDA toolkit and tf of proper version manually right?
But they came with tensorflow when I install it on conda. Honestly, I don't get this(If they comes with tf, do I even need to mind what I need?)

Anyway, I checked tickets which mention this bug such as [#24828](https://github.com/tensorflow/tensorflow/issues/24828) but no answers were good medicine to my situation. So I issued this ticket again.

**Describe the expected behavior**
Successfully complete the process of learning.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

nouveau is ok, it doesn't work now. If there is any more information I should show, please let me know."
43760,creating segmentation,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
43757,creating tf.data.Dataset object from generator gives error for incorrect file name,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 **
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **conda**
- TensorFlow version (use command below): **2.3.0**
- Python version: **3.8 and 3.6**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **10.1.243/7.6.5**
- GPU model and memory: **Nvidia RTX2070 8 GB**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
On trying to consume batches from the dataset `ds` I get InvalidArgumentError
**Describe the expected behavior**

**Standalone code to reproduce the issue**
[official tfdata colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/data.ipynb)
In the above file. Go to Consuming Python Generators section and in the code cell
```
ds = tf.data.Dataset.from_generator(
    img_gen.flow_from_directory, args=[flowers], 
    output_types=(tf.float32, tf.float32), 
    output_shapes=([32,256,256,3], [32,5])
)

ds
```
add `next(iter(ds))` to consume a batch of `ds`
**Other info / logs** 
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)
   2101       ctx.executor = executor_new
-> 2102       yield
   2103     finally:

10 frames
InvalidArgumentError: TypeError: endswith first arg must be bytes or a tuple of bytes, not str
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 711, in get_iterator
    return self._iterators[iterator_id]

KeyError: 1


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py"", line 244, in __call__
    ret = func(*args)

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py"", line 302, in wrapper
    return func(*args, **kwargs)

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 827, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 713, in get_iterator
    iterator = iter(self._generator(*self._args.pop(iterator_id)))

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py"", line 959, in flow_from_directory
    interpolation=interpolation)

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py"", line 397, in __init__
    **kwargs)

  File ""/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py"", line 135, in __init__
    classes, filenames = res.get()

  File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 644, in get
    raise self._value

  File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 119, in worker
    result = (True, func(*args, **kwds))

  File ""/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py"", line 221, in _list_valid_filenames_in_directory
    for root, fname in valid_files:

  File ""/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py"", line 178, in _iter_valid_files
    if fname.lower().endswith('.tiff'):

TypeError: endswith first arg must be bytes or a tuple of bytes, not str


	 [[{{node PyFunc}}]] [Op:IteratorGetNext]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py in wait(self)
     65   def wait(self):
     66     """"""Waits for ops dispatched in this executor to finish.""""""
---> 67     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
     68 
     69   def clear_error(self):

InvalidArgumentError: TypeError: endswith first arg must be bytes or a tuple of bytes, not str
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 711, in get_iterator
    return self._iterators[iterator_id]

KeyError: 1


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py"", line 244, in __call__
    ret = func(*args)

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py"", line 302, in wrapper
    return func(*args, **kwargs)

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 827, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 713, in get_iterator
    iterator = iter(self._generator(*self._args.pop(iterator_id)))

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py"", line 959, in flow_from_directory
    interpolation=interpolation)

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py"", line 397, in __init__
    **kwargs)

  File ""/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py"", line 135, in __init__
    classes, filenames = res.get()

  File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 644, in get
    raise self._value

  File ""/usr/lib/python3.6/multiprocessing/pool.py"", line 119, in worker
    result = (True, func(*args, **kwds))

  File ""/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py"", line 221, in _list_valid_filenames_in_directory
    for root, fname in valid_files:

  File ""/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py"", line 178, in _iter_valid_files
    if fname.lower().endswith('.tiff'):

TypeError: endswith first arg must be bytes or a tuple of bytes, not str


	 [[{{node PyFunc}}]]
```
"
43793,"Fatal Exception: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: OpenCL library not loaded - dlopen failed: library ""libOpenCL-pixel.so"" not found","**System information**

Mobile Device - Android - across multiple Android version (6, 7, 8)

**Describe the current behavior**

Invoking `org.tensorflow.lite.Interpreter` causes crash on some cases

**Describe the expected behavior**

Should not crash

**Standalone code to reproduce the issue**

I am getting this on crash analysis tool. Since this is not reproducible on my devices I am not able to produce a standalone code.

**Other info / logs** 

```

Fatal Exception: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: OpenCL library not loaded - dlopen failed: library ""libOpenCL-pixel.so"" not found
Falling back to OpenGL
TfLiteGpuDelegate Init: No EGL error, but eglChooseConfig failed.
TfLiteGpuDelegate Prepare: delegate is not initialized
Node number 31 (TfLiteGpuDelegateV2) failed to prepare.

Restored previous execution plan after delegate application failure.
       at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(NativeInterpreterWrapper.java)
       at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:85)
       at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:61)
       at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)
       at a.b.c..data.Posenet.getInterpreter(Posenet.java:184)
       at a.b.c..data.Posenet.estimateSinglePose(Posenet.java:293)
       at a.b.c..call.ImageAnalyser.processImage(ImageAnalyser.java:157)
       at a.b.c..call.ImageAnalyser.access$processImage(ImageAnalyser.java:26)
       at a.b.c..call.ImageAnalyser$processImageForAnalysis$2.invokeSuspend(ImageAnalyser.java:127)
       at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(BaseContinuationImpl.java:33)
       at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.java:56)
       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:636)
       at java.lang.Thread.run(Thread.java:764)

```
"
43756,"Hi, am trying to visualize the loss function using tensorboard ","
Hi am trying to view loss function using tensorboard , am using google colab, i edit my code as following:
`import tensorflow as tf
import datetime
%load_ext tensorboard

sess = tf.Session()

file_writer = tf.summary.FileWriter('/path/to/logs', sess.graph)
logdir = os.path.join(
    ""logs"", datetime.datetime.now().strftime(""%Y%m%d-%H%M%S""))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)`

and while calling train() i added 

`custom_callbacks=[tensorboard_callback]`

it gives me the following error

`
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)
     59           ""allowed values: %s"" %
     60           (param_name, dtypes.as_dtype(dtype).name,
---> 61            "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
     62 
     63 

TypeError: Value passed to parameter 'values' has DataType bool not in list of allowed values: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, float16, uint32, uint64
`
"
43755,training.tracking.data_structures.List is not properly serialized / deserialized,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.6.9
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX 2080 Ti

**Describe the current behavior**
Weights loaded from a layer containing sublayers in a `tracking.data_structures.List` are not properly listed in the `.weights` attributes. This will not only cause confusion but will also sabotage any further optimization or modification of weights. They are properly restored though.

The issue is not present when `ListWrapper` is used.

**Describe the expected behavior**
Variables of sublayers contained in a `List` should be contained in `.weights` when the model is loaded again

**Standalone code to reproduce the issue**
```
import tensorflow as tf

class TestLayer(tf.keras.layers.Layer):

    def __init__(self, **kwargs):
        super(TestLayer, self).__init__(**kwargs)

        self.static_layer = tf.keras.layers.Dense(128)
        self.my_layers = tf.python.training.tracking.data_structures.List()
        for i in range(4):
            layer = tf.keras.layers.Dense(128)
            self.my_layers.append(layer)


    def call(self, x):
        x = self.static_layer(x)

        for layer in self.my_layers:
            x = layer(x)

        return x


    def get_config(self):
        return super(TestLayer, self).get_config()


model = tf.keras.Sequential([TestLayer()])

x = tf.constant(42.0, shape=[1,1])
y1 = model(x)

model.save('my_test_model', save_format='tf')

model_loaded = tf.keras.models.load_model('my_test_model')
y2 = model_loaded(x)

# output
model.summary()
model_loaded.summary()
print('n vars: ', len(model.weights), ' ', len(model_loaded.weights))
print('diff: ', tf.norm(y1-y2))   
```

**Output**
```
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
test_layer (TestLayer)       (None, 128)               66304     
=================================================================
Total params: 66,304
Trainable params: 66,304
Non-trainable params: 0
_________________________________________________________________
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
test_layer (TestLayer)       (None, 128)               256       
=================================================================
Total params: 256
Trainable params: 256
Non-trainable params: 0
_________________________________________________________________
n vars:  10   2
diff:  tf.Tensor(0.0, shape=(), dtype=float32)
```"
43753,Segmentation fault (core dumped),"**System information**
- OS Platform and Distribution :CentOS Linux release 7.7.1908
-TensorFlow version:2.3.0

I try to convert [the tensorflow offical image caption model ](https://www.tensorflow.org/tutorials/text/image_captioning?hl=en)to TFLite model 

And Now I have successfully convert the model using ```tf.lite.TFLiteConverter.from_concrete_functions```
as following:
```
@tf.function
def evaluate(img_tensor_val):
    temp_input = tf.expand_dims(img_tensor_val, 0)
    img_tensor_val = image_features_extract_model(temp_input)
    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))
    hidden = decoder.reset_states(batch_size=1)

    features = encoder(img_tensor_val)

    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)
    result = []

    for i in range(max_length):
        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)
        print(predictions.shape)
        # result.append(predictions)
        predicted_id = tf.random.categorical(predictions, 1)[0][0]
        #
        #
        result.append(predicted_id)
        #
        #
        # if predicted_id == 3:
        #     return result
        # # result.append(tf.gather(tokenizer.index_word, predicted_id))
        # #
        # # if tf.gather(tokenizer.index_word, predicted_id) == '<end>':
        # #     return result
        #
        dec_input = tf.expand_dims([predicted_id], 0)
    return result

export_dir = ""./""
tflite_enc_input = ''
ckpt.f = evaluate
to_save = evaluate.get_concrete_function(tf.TensorSpec(shape=(299, 299, 3),dtype=tf.dtypes.float32))

converter = tf.lite.TFLiteConverter.from_concrete_functions([to_save])
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```

And I Visualize the converted_model.tflite by [Netorn:](https://drive.google.com/file/d/1CXCJHVauGiccFpW8itIPUX4kgbnpCgLj/view?usp=sharing)
But when I invoke the interpreter the problem came:
**LOG:**

```
2020-10-03 12:11:24.049222: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-03 12:11:30.184705: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-10-03 12:11:30.213363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:af:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-10-03 12:11:30.213414: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-03 12:11:30.219666: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-03 12:11:30.223018: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-03 12:11:30.224419: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-03 12:11:30.227861: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-03 12:11:30.230195: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-03 12:11:30.236320: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-03 12:11:30.239374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-10-03 12:11:30.239829: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-03 12:11:30.248265: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2600000000 Hz
2020-10-03 12:11:30.249524: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5615faa7fa90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-03 12:11:30.249552: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-03 12:11:30.381691: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5615faaec0c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-10-03 12:11:30.381734: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2020-10-03 12:11:30.383860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:af:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-10-03 12:11:30.383900: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-03 12:11:30.383930: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-03 12:11:30.383944: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-03 12:11:30.383959: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-03 12:11:30.383973: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-03 12:11:30.383987: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-03 12:11:30.384002: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-03 12:11:30.387738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-10-03 12:11:30.387786: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-03 12:11:31.156790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-03 12:11:31.156840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-10-03 12:11:31.156853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-10-03 12:11:31.160006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30098 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:af:00.0, compute capability: 7.0)
**(299, 299, 3)**
INFO: Created TensorFlow Lite delegate for select TF ops.
2020-10-03 12:11:31.760387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:af:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-10-03 12:11:31.760470: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-03 12:11:31.760523: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-03 12:11:31.760551: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-03 12:11:31.760577: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-03 12:11:31.760601: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-03 12:11:31.760625: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-03 12:11:31.760647: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-03 12:11:31.763282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-10-03 12:11:31.763329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-03 12:11:31.763346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-10-03 12:11:31.763360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-10-03 12:11:31.766083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30098 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:af:00.0, compute capability: 7.0)
**INFO: TfLiteFlexDelegate delegate: 51 nodes delegated out of 2014 nodes with 51 partitions.**

**Segmentation fault (core dumped)**
```

**The invoke of Interpreter**
```
def load_image(image_path):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, (299,299))
    img = tf.keras.applications.inception_v3.preprocess_input(img)
    return img, image_path

image = load_image('./test.jpg')[0]
print(image.shape)


interpreter = tf.lite.Interpreter(model_path='./converted_model.tflite')
input_details = interpreter.get_input_details()
interpreter.allocate_tensors()
interpreter.set_tensor(input_details[0]['index'], image)
interpreter.invoke()

raw_prediction = interpreter.tensor(
    interpreter.get_output_details()[0]['index'])()
print(raw_prediction)
```

Please tell me what 's the problem of the program?What's the meaning of 'Segmentation fault (core dumped)' ?

"
43752,Documentation example for tf.keras.utils.Sequence is incorrect;  'fencepost' error in example code,"
on 
https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence

In the example generator there is code to return a batch:

    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_y = self.y[idx * self.batch_size:(idx + 1) *self.batch_size]

however this returns (batch_size + 1 ) results.   for example if batch_size = 1,  requesting idx 0 returns items of index  0, and 1, two items. 

This will cause an array overrun when the batch_size is an even multiple of the data size.

the correct code is:

    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size:((idx + 1) *self.batch_size)-1]
        batch_y = self.y[idx * self.batch_size:((idx + 1) *self.batch_size)-1]


I see the incorrect code copied all over the place."
43751,Win10+CUDA11.1+cudnn8.0.4+rtx3090,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Amd 3900x+ nvidia rtx 3090
- TensorFlow installed from (source or binary): source
- TensorFlow version:r2.3 v2.3 v2.3.1 v2.4
- Python version:3.8.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 3.5.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:CUDA11.1+cudnn8.0.4
- GPU model and memory:nvidia rtx 3090 24GB

**Describe the problem**

v2.3.1 error  below
ERROR: F:/tensorflow-2.3.1/tensorflow/core/kernels/BUILD:3180:18: C++ compilation of rule '//tensorflow/core/kernels:image_ops_gpu' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/itx/_bazel_itx/aorpfopj/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.27.29110\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.27.29110\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.27.29110\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.27.29110\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja

v2.4 error below
ERROR: C:/users/itx/_bazel_itx/vne2u2k5/external/nsync/BUILD:467:11: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/itx/_bazel_itx/vne2u2k5/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.27.29110\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.27.29110\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.27.29110\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.27.29110\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja


**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43750,Import issue with tensor flow Windows 10 python 3.7,"Hi 

I am trying to make this tutorial work : https://github.com/ildoonet/tf-pose-estimation/blob/master/.
I am having an error in file tf-pose-estimation-master\tf_pose\estimator.py"",
The import """"""import tensorflow.contrib.tensorrt as trt"""""" was causing an error, after having read about this issue, using 
from tensorflow.python.compiler.tensorrt import trt didnt fix it either, what must i do ?

Best regards
"
43749,Memory blowup converting from TFLite to C,"I am observig a blowup of around 6-7x in memory size when converting a model from tflite flatBuffer to C source. 

For easier reproducibility, I am reporting with the TFLite models obtained from the MNIST tutorial [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb)

I use the following method for converting from tflite model to C source:

`

	from tensorflow.lite.python.util import convert_bytes_to_c_source
	
	def convert_to_c(tflite_model,file_name):
	
	    source_text, header_text = convert_bytes_to_c_source(tflite_model,  file_name)
	
	    with  open(file_name + '.h',  'w')  as  file:
	
	        file.write(header_text)
	
	    with  open(file_name + '.cc',  'w')  as  file:
	
	        file.write(source_text)
 `
 
Here is a summary of the sizes before ('.tflite' file) and after converting to C source ('.cc' file):
1. Integer-only quantized model: before - 24.7 KB, after - 156.6 KB, blowup ~ 6.340x
2. Float fallback quantized model: before - 24.6 KB, after - 156.1 KB, blowup ~ 6.346x 
3. Dynamic range quantized model: before - 23.8 KB, after - 151.1 KB, blowup ~ 6.349x
4. Normal model: before - 84.5 KB, after - 533.4 KB, blowup ~ 6.312x
   
This blowup seems independent of the model architecture and parameters because I have tested with few other models and observed a similar trend there as well. 

I wanted to know what is possibly causing this blowup in memory size. In case of a small model (with initial flatbuffer size in kB) it is not a big issue, but for bigger models (with initial flatbuffer size in the order of MB) the final C source size is quite large and often exceeds the flash memory size of micro-controllers. "
43747,Cannot import tensorflow,"First time post on github.  trying to import tensorflow on local machine.  keep getting a runtime error.  please advise.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip install tensorflow on miniconda3
- TensorFlow version (use command below): 2.2.0
- Python version: 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cudnn-11.0-windows-x64-v8.0.3.33.zip   cuDNN8.0.4
- GPU model and memory: NVIDIA Quadro K1100M.  This has compute capability 3.0 which is less than the minimum required... see https://www.tensorflow.org/install/gpu#hardware_requirements 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

tensorflow is not imported.  instead i get a runtime warning... see warning below in Other info / logs

**Describe the expected behavior**

**Standalone code to reproduce the issue**

import tensorflow as tf

**Other info / logs** 
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\miniconda3\envs\quant\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59 

~\miniconda3\envs\quant\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\miniconda3\envs\quant\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\miniconda3\envs\quant\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

~\miniconda3\envs\quant\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-43-64156d691fe5> in <module>
----> 1 import tensorflow as tf

~\miniconda3\envs\quant\lib\site-packages\tensorflow\__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

~\miniconda3\envs\quant\lib\site-packages\tensorflow\python\__init__.py in <module>
     48 import numpy as np
     49 
---> 50 from tensorflow.python import pywrap_tensorflow
     51 
     52 # Protocol buffers

~\miniconda3\envs\quant\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     67 for some common reasons and solutions.  Include the entire stack trace
     68 above this error message when asking for help."""""" % traceback.format_exc()
---> 69   raise ImportError(msg)
     70 
     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\Jeff\miniconda3\envs\quant\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Jeff\miniconda3\envs\quant\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Jeff\miniconda3\envs\quant\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Jeff\miniconda3\envs\quant\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Jeff\miniconda3\envs\quant\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
43745,Keras Backend ones_like with Lambda is not serializable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Databricks Runtime 7.3
- TensorFlow installed from (source or binary):  binary
- TensorFlow version (use command below): 2.3
- Python version: 3
- CUDA/cuDNN version: 10.1
- GPU model and memory: AWS p3.xlarge

**Describe the current behavior**
Wrapping a `tf.keras.backend.ones_like` in a `tf.keras.layer.Lambda` fails serialization.

The following code creates the model that fails to serialize:
```python
x = keras.Input(shape=1, name=""x"")
ones_like_layer = keras.layers.Lambda(K.ones_like, name=""ones_like"")
ones_like_layer(x)
logits = keras.layers.Dense(1, activation=""sigmoid"")

model = keras.Sequential([x, ones_like_layer, logits], name=""ones_like_model"")
```

Errors:
```
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in wrapper(*args, **kwargs)
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
TypeError: 'str' object is not callable
During handling of the above exception, another exception occurred:
TypeError                                 Traceback (most recent call last)
11 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in wrapper(*args, **kwargs)
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a
    204       # TypeError, when given unexpected types.  So we need to catch both.
--> 205       result = dispatch(wrapper, args, kwargs)
    206       if result is not OpDispatcher.NOT_SUPPORTED:
    207         return result
TypeError: 'module' object is not callable
````

This happens on TF 2.3 and TF Nightly.  See https://colab.research.google.com/drive/1ih41e5b6jw_3iSm9pKSOcW_Kf5y8ktU5?usp=sharing .

**Describe the expected behavior**
The model should be serializable.

**Standalone code to reproduce the issue**
See above.

**Other info / logs** Include any logs or source code that would be helpful to
Workaround - instead of using the `Lambda`, just call `ones_like` directly.  This works but leads to the model being less interpretable.  This requires using the Functional model.

See also: https://github.com/tensorflow/tensorflow/issues/41244#issuecomment-698918718"
43744,Variable and Slow InfeedEnqueueTuple on TPUv3-8,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GCP VM with tf-version nightly
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): nightly
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: TPUv3-8

**Describe the current behavior**
[InfeedEnqueueTuple](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/tpu/kernels/infeed_ops.h) has variable performance and 33% slowdown. Infeed cannot keep TPUv3-8 saturated even with heavy caching due to stragglers in infeed operator. All input processing time is reported spent in ""while/body/_1/InfeedQueue/enqueue/*"".
I am attaching profiler results with infeed greater than 45ms. Note that each thread has variable time per Enqueue.
![slow_step](https://user-images.githubusercontent.com/12423239/94967869-ee6f2400-04cd-11eb-8c1c-e0a1c2df1a5e.png)
![infeed_slow](https://user-images.githubusercontent.com/12423239/94967461-232eab80-04cd-11eb-8e30-c291098dff33.png)

**Describe the expected behavior**
InfeedEnqueueTuple has consistent and fast performance (less than 45ms per step). I sometimes am able to observe periods of fast operation, where each infeed is consistently 45ms (image attached). However, infeed eventually regresses to ~60ms per step, as above.
![infeed_fast](https://user-images.githubusercontent.com/12423239/94967617-6be66480-04cd-11eb-828f-035298cd4fb0.png)


**Standalone code to reproduce the issue**
I am using MLPerfv0.7 ResNet [code](https://github.com/mlperf/training_results_v0.7/tree/master/Google/benchmarks/resnet/implementations/resnet-research-TF-tpu-v4-16) with ResNet-18. Issue persists even if dataset cache() (e.g., take(1).cache().repeat()) is placed right before prefetch().





"
43743,person_detection benchmarks do not build for STM32F4,"@tensorflow/micro

**System information**
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): #43509 
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): STM32F4

After #43509 is merged, remove the exclusion for the person_detection and person_deteciton_experimental benchmarks and then:

```
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=stm32f4 TAGS=cmsis-nn person_detection_benchmark
```
will give the following error:
```
/arm-none-eabi/7.3.1/../../../../arm-none-eabi/bin/ld: tensorflow/lite/micro/tools/make/gen/stm32f4_cortex-m4/bin/person_detection_benchmark section `.rodata' will not fit in region `FLASH'
/home/advaitjain/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/bin/ld: tensorflow/lite/micro/tools/make/gen/stm32f4_cortex-m4/bin/person_detection_benchmark section `.bss' will not fit in region `RAM'
/home/advaitjain/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/bin/ld: region `RAM' overflowed by 72568 bytes
/home/advaitjain/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/bin/ld: region `FLASH' overflowed by 158376 bytes
collect2: error: ld returned 1 exit status
make: *** [tensorflow/lite/micro/benchmarks/Makefile.inc:31: tensorflow/lite/micro/tools/make/gen/stm32f4_cortex-m4/bin/person_detection_benchmark] Error 1
```

The easy fix is to increase the numbers here https://github.com/tensorflow/tensorflow/blob/27d26a8d86bceda282ad9ba3e3116a00759d4ebc/tensorflow/lite/micro/tools/make/targets/stm32f4/stm32f4.lds#L34-L37 but will let the CMSIS-NN team weigh in on this.
"
43740,Assertion error in Keras model with two string inputs and single output,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina 10.15.5
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7.4

**Describe the current behavior**
I have created a simple test model to take two strings (base64 encoded images), convert those strings to
image-shaped tensors (299,299,3) and add the two image-shaped tensors (see Colab link below). The Add layer was
chosen as a simple of demo combining two inputs.

I am running into an assertion error when calling the model. Any ideas for how to fix this?

Reason for string conversion: when deployed, the model must accept string representations of images from a JSON payload.

Thank you!

**Describe the expected behavior**
I would expect the model to output a (1,299,299,3) float32 tensor

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1y7Z1W_DxigMYiYb4O4M2OVPScANXye_3?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```python
AssertionError                            Traceback (most recent call last)
~/tc/pixel-crunching/servitup.py in <module>
     97     im2 = get_and_resize(ANOTHER_URL)
     98     batch = tf.constant([im1, im2])
---> 99     y = model(batch)
    100 
    101 

~/tc/pixel-crunching/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    983 
    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):
--> 985           outputs = call_fn(inputs, *args, **kwargs)
    986 
    987         if self._activity_regularizer:

~/tc/pixel-crunching/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in call(self, inputs, training, mask)
    384     """"""
    385     return self._run_internal_graph(
--> 386         inputs, training=training, mask=mask)
    387 
    388   def compute_output_shape(self, input_shape):

~/tc/pixel-crunching/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in _run_internal_graph(self, inputs, training, mask)
    515     for x in self.outputs:
    516       x_id = str(id(x))
--> 517       assert x_id in tensor_dict, 'Could not compute output ' + str(x)
    518       output_tensors.append(tensor_dict[x_id].pop())
    519 

AssertionError: Could not compute output Tensor(""add/add:0"", shape=(None, 299, 299, 3), dtype=float32)
```"
43738,Evaluation produces OSError: [Errno 24] Too many open files,"I have a dataset of around 40,000 images. I load them into memory using opencv and I put them in a long python list. Then I use a TimeseriesGenerator, like this:

``
generator = TimeseriesGenerator(allframes, allframes, length=75, sampling_rate=5, stride=2, batch_size=8)
``

Where allframes is just a python list where each entry is an image as loaded with opencv.

I then have a custom generator:
```
def customGenerator(generator, indexes):

    while True:
        np.random.shuffle(indexes)
        for i in indexes:
            x,y = generator[i]
            yield (x, y)
```

and then my train function, im removing some non important lines:
```
def trainRNNmodel(model, generator):

    randomize = np.arange( len(generator) - 1 )
    np.random.shuffle(randomize)
    trainLimit = int( 0.9*len(randomize) )
    valsteps = int( 0.1*len(randomize) ) 

    workers = int(multiprocessing.cpu_count()/2)

    model.fit( x= customGenerator(generator, randomize[:trainLimit]), y=None, 
            validation_data = customGenerator(generator, randomize[trainLimit:]), 
            epochs=1000, steps_per_epoch=trainLimit, validation_steps=valsteps,
            use_multiprocessing = False, callbacks=callbacks_list)
```

it works ok, but around epoch 50 I get the same error 24 as the user says. and this happens during validation too.

I can increase ulimit but this only delays the error.

No idea why is it happening because I reckon it's not loading any files during training."
43736,Unable to train the model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Manjaro 5.4.64 kernel
- TensorFlow installed from (source or binary): Pycharm 
- TensorFlow version: 2.3.1
- Python version: 3.8
- Installed using virtualenv? pip? conda?: virtualenv
- GPU model and memory: No GPU


**Describe the problem**
I have built a model architecture that takes in multimodal inputs i.e. visual and textual. For visual inputs, I am using the embeddings generated by second to last layer of VGG-19 of 4096 dimension and passing them here. When I do start training the model, the following error shows up:

```
Epoch 1/300
Traceback (most recent call last):
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: z_log_var/BiasAdd:0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/caesar/PycharmProjects/fake-news-detector/mvae.py"", line 211, in <module>
    train(20, 4096, 64, 0.05, 0.3, 'models/vae_fnd_0.05_0.3')
  File ""/home/caesar/PycharmProjects/fake-news-detector/mvae.py"", line 158, in train
    model.autoencoder.fit(x=[text, im],
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1098, in fit
    tmp_logs = train_function(iterator)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 840, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1843, in _filtered_call
    return self._call_flat(
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1923, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 545, in call
    outputs = execute.execute(
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 72, in quick_execute
    raise core._SymbolicException(
tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'z_log_var/BiasAdd:0' shape=(None, 64) dtype=float32>, <tf.Tensor 'z_mean/BiasAdd:0' shape=(None, 64) dtype=float32>]

Process finished with exit code 1

```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
import pdb

import tensorflow
from keras import regularizers
from keras import objectives, backend as K
from keras.layers import Dropout, Reshape, Concatenate, Flatten, Bidirectional, Dense, Embedding, Input, Lambda, LSTM, \
    RepeatVector, TimeDistributed
from keras.models import Model
from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint, TensorBoard
from keras.optimizers import Adam, RMSprop
import keras
import numpy as np
import os
from sklearn.metrics import precision_score, accuracy_score, precision_recall_fscore_support

# tensorflow.config.experimental_run_functions_eagerly(True)


class MVAE(object):

    def create(self, max_length, image_embed_size, latent_dim, reg_lambda, fnd_lambda, embed_matrix):
        self.encoder = None
        self.decoder = None
        self.fnd = None
        self.autoencoder = None
        self.embedding_matrix = embed_matrix
        self.vocab_size = self.embedding_matrix.shape[0]
        self.max_length = max_length
        self.latent_dim = latent_dim
        self.reg_lambda = reg_lambda
        self.fnd_lambda = fnd_lambda
        self.image_embed_size = image_embed_size

        input_txt = Input(shape=(self.max_length,), name='input_txt')
        input_img = Input((image_embed_size,), name='input_img')

        vae_ce_loss, vae_mse_loss, encoded = self._build_encoder(input_txt, input_img)
        self.encoder = Model(inputs=[input_txt, input_img], outputs=encoded)

        encoded_input = Input(shape=(self.latent_dim,))
        predicted_outcome = self._build_fnd(encoded_input)
        self.fnd = Model(encoded_input, predicted_outcome)

        decoded_txt, decoded_img = self._build_decoder(encoded_input)
        self.decoder = Model(encoded_input, [decoded_txt, decoded_img])

        decoder_output = self._build_decoder(encoded)

        self.autoencoder = Model(inputs=[input_txt, input_img],
                                 outputs=[decoder_output[0], decoder_output[1], self._build_fnd(encoded)])
        self.autoencoder.compile(optimizer=Adam(1e-5),
                                 loss=['sparse_categorical_crossentropy', vae_mse_loss, 'binary_crossentropy'],
                                 metrics=['accuracy'])
        self.get_features = K.function([input_txt, input_img], [encoded])
        print(self.autoencoder.summary())

    def _build_encoder(self, input_txt, input_img, latent_dim=64):
        txt_embed = Embedding(self.vocab_size, 32, input_length=self.max_length, name='txt_embed', trainable=False,
                              weights=[self.embedding_matrix])(input_txt)
        lstm_txt_1 = Bidirectional(LSTM(32, return_sequences=True, name='lstm_txt_1', activation='tanh',
                                        kernel_regularizer=regularizers.l2(self.reg_lambda)), merge_mode='concat')(
            txt_embed)
        lstm_txt_2 = Bidirectional(LSTM(32, return_sequences=False, name='lstm_txt_2', activation='tanh',
                                        kernel_regularizer=regularizers.l2(self.reg_lambda)), merge_mode='concat')(
            lstm_txt_1)
        fc_txt = Dense(32, activation='tanh', name='dense_txt', kernel_regularizer=regularizers.l2(self.reg_lambda))(
            lstm_txt_2)

        fc_img_1 = Dense(1024, name='fc_img_1', activation='tanh', kernel_regularizer=regularizers.l2(self.reg_lambda))(
            input_img)
        fc_img_2 = Dense(32, name='fc_img_2', activation='tanh', kernel_regularizer=regularizers.l2(self.reg_lambda))(
            fc_img_1)

        h = Concatenate(axis=-1, name='concat')([fc_txt, fc_img_2])
        h = Dense(64, name='shared', activation='tanh', kernel_regularizer=regularizers.l2(self.reg_lambda))(h)

        def sampling(args):
            z_mean_, z_log_var_ = args
            batch_size = K.shape(z_mean_)[0]
            epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=0.01)
            return z_mean_ + K.exp(0.5 * z_log_var_) * epsilon

        z_mean = Dense(latent_dim, name='z_mean', activation='linear')(h)
        z_log_var = Dense(latent_dim, name='z_log_var', activation='linear')(h)

        def vae_mse_loss(x, x_decoded_mean):
            mse_loss = objectives.mse(x, x_decoded_mean)
            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
            return mse_loss + kl_loss

        def vae_ce_loss(x, x_decoded_mean):
            x = K.flatten(x)
            x_decoded_mean = K.flatten(x_decoded_mean)
            xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)
            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
            return xent_loss + kl_loss

        return (
        vae_ce_loss, vae_mse_loss, Lambda(sampling, output_shape=(latent_dim,), name='lambda')([z_mean, z_log_var]))

    def _build_decoder(self, encoded):
        dec_fc_txt = Dense(32, name='dec_fc_txt', activation='tanh',
                           kernel_regularizer=regularizers.l2(self.reg_lambda))(encoded)
        repeated_context = RepeatVector(self.max_length)(dec_fc_txt)
        dec_lstm_txt_1 = LSTM(32, return_sequences=True, activation='tanh', name='dec_lstm_txt_1',
                              kernel_regularizer=regularizers.l2(self.reg_lambda))(repeated_context)
        dec_lstm_txt_2 = LSTM(32, return_sequences=True, activation='tanh', name='dec_lstm_txt_2',
                              kernel_regularizer=regularizers.l2(self.reg_lambda))(dec_lstm_txt_1)
        decoded_txt = TimeDistributed(Dense(self.vocab_size, activation='softmax'), name='decoded_txt')(dec_lstm_txt_2)

        dec_fc_img_1 = Dense(32, name='dec_fc_img_1', activation='tanh',
                             kernel_regularizer=regularizers.l2(self.reg_lambda))(encoded)
        dec_fc_img_2 = Dense(1024, name='dec_fc_img_2', activation='tanh',
                             kernel_regularizer=regularizers.l2(self.reg_lambda))(dec_fc_img_1)
        decoded_img = Dense(4096, name='decoded_img', activation='sigmoid')(dec_fc_img_2)

        return decoded_txt, decoded_img

    def _build_fnd(self, encoded):
        h = Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(self.fnd_lambda))(encoded)
        h = Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(self.fnd_lambda))(h)
        return Dense(1, activation='sigmoid', name='fnd_output')(h)


def train(sequence_length, image_embed_size, latent_dim, reg_lambda, fnd_lambda, path):
    text = np.load('data/train_text.npy')
    im = np.load('data/train_image_embed.npy')
    label = np.load('data/train_label.npy')[:, 1]

    test_text = np.load('data/test_text.npy')
    test_im = np.load('data/test_image_embed.npy')
    test_label = np.load('data/test_label.npy')[:, 1]

    embed_matrix = np.load('data/embedding_matrix.npy')
    vocab_size = embed_matrix.shape[0]

    # temp = np.zeros((text.shape[0], sequence_length, vocab_size))
    # temp[np.expand_dims(np.arange(text.shape[0]), axis=0).reshape(text.shape[0], 1), np.repeat(np.array([np.arange(sequence_length)]), text.shape[0], axis=0), text] = 1
    # text_one_hot = temp
    #
    # temp = np.zeros((test_text.shape[0], sequence_length, vocab_size))
    # temp[np.expand_dims(np.arange(test_text.shape[0]), axis=0).reshape(test_text.shape[0], 1), np.repeat(np.array([np.arange(sequence_length)]), test_text.shape[0], axis=0), test_text] = 1
    # test_text_one_hot = temp

    if not os.path.exists(path):
        os.makedirs(path)
    if not os.path.exists(path + '/tb'):
        os.makedirs(path + '/tb')
    if not os.path.exists(path + '/weights'):
        os.makedirs(path + '/weights')
    tensorboard = TensorBoard(log_dir=path + '/tb', write_graph=True, write_images=True)
    checkpoint = ModelCheckpoint(path + '/weights/{epoch:02d}.hdf5', monitor='loss', verbose=1, save_best_only=True,
                                 mode='auto')
    reduce_lr = ReduceLROnPlateau(monitor='fnd_output_loss', factor=0.2, patience=6, min_lr=1e-7)

    model = MVAE()
    model.create(sequence_length, image_embed_size, latent_dim, reg_lambda, fnd_lambda, embed_matrix)
    model.autoencoder.fit(x=[text, im],
                          y={'decoded_txt': np.expand_dims(text, -1), 'decoded_img': im, 'fnd_output': label},
                          batch_size=128, epochs=300, callbacks=[checkpoint, tensorboard, reduce_lr], shuffle=True,
                          validation_data=([test_text, test_im],
                                           {'decoded_txt': np.expand_dims(test_text, -1), 'decoded_img': test_im,
                                            'fnd_output': test_label}))


def save_features(sequence_length, image_embed_size, latent_dim, reg_lambda, fnd_lambda, path):
    test_text = np.load('../data/test_text.npy')
    test_im = np.load('../data/test_image_embed.npy')

    embed_matrix = np.load('../data/embedding_matrix.npy')
    vocab_size = embed_matrix.shape[0]

    model = MVAE()
    model.create(sequence_length, image_embed_size, latent_dim, reg_lambda, fnd_lambda, embed_matrix)
    model.autoencoder.load_weights(path + '/weights/286.hdf5')

    if not os.path.exists(path + '/features'):
        os.makedirs(path + '/features')

    learnt_features = np.array([]).reshape(0, 64)
    for i in range(test_text.shape[0]):
        text_batch = test_text[i:i + 1]
        im_batch = test_im[i:i + 1]
        batch = model.get_features([text_batch, im_batch])[0]
        learnt_features = np.concatenate([learnt_features, batch])
    np.save(path + '/features/vae_fnd', learnt_features)


def test(sequence_length, image_embed_size, latent_dim, reg_lambda, fnd_lambda, path):
    test_text = np.load('data/test_text.npy')
    test_im = np.load('data/test_image_embed.npy')
    test_label = np.load('data/test_label.npy')[:, 1]

    embed_matrix = np.load('data/embedding_matrix.npy')
    vocab_size = embed_matrix.shape[0]

    model = MVAE()
    model.create(sequence_length, image_embed_size, latent_dim, reg_lambda, fnd_lambda, embed_matrix)
    model.autoencoder.load_weights(path + '/weights/224.hdf5')
    for i in range(10):
        pred = model.autoencoder.predict([test_text, test_im])[-1]
        pred[pred > 0.5] = 1
        pred[pred <= 0.5] = 0
        print(accuracy_score(test_label, pred))
        print(precision_recall_fscore_support(test_label, pred))

    pdb.set_trace()


if __name__ == '__main__':
    train(20, 4096, 64, 0.05, 0.3, 'models/vae_fnd_0.05_0.3')
    test(20, 4096, 64, 0.05, 0.3, 'models/vae_fnd_0.05_0.3')
    save_features(20, 4096, 64, 0.05, 0.3, '../models/vae_fnd_0.05_0.3')

```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
I tried adding `tensorflow.config.experimental_run_functions_eagerly(True)` to the code. This error went away but another error showed up later which I guess is because of adding this line only error - 
```
Epoch 1/300
2020-10-02 19:48:34.816657: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 130744320 exceeds 10% of free system memory.
2020-10-02 19:48:35.015006: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 130744320 exceeds 10% of free system memory.
2020-10-02 19:48:35.073321: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 130744320 exceeds 10% of free system memory.
Traceback (most recent call last):
  File ""/home/caesar/PycharmProjects/fake-news-detector/mvae.py"", line 211, in <module>
    train(20, 4096, 64, 0.05, 0.3, 'models/vae_fnd_0.05_0.3')
  File ""/home/caesar/PycharmProjects/fake-news-detector/mvae.py"", line 158, in train
    model.autoencoder.fit(x=[text, im],
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1098, in fit
    tmp_logs = train_function(iterator)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 806, in train_function
    return step_function(self, iterator)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 796, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 1211, in run
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 2585, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 2945, in _call_for_each_replica
    return fn(*args, **kwargs)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"", line 275, in wrapper
    return func(*args, **kwargs)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 789, in run_step
    outputs = model.train_step(data)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 756, in train_step
    _minimize(self.distribute_strategy, tape, self.optimizer, loss,
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 2743, in _minimize
    optimizer.apply_gradients(
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 545, in apply_gradients
    return distribute_ctx.get_replica_context().merge_call(
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 2715, in merge_call
    return self._merge_call(merge_fn, args, kwargs)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 2722, in _merge_call
    return merge_fn(self._strategy, *args, **kwargs)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"", line 275, in wrapper
    return func(*args, **kwargs)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 642, in _distributed_apply
    with ops.control_dependencies(update_ops):
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 5359, in control_dependencies
    return get_default_graph().control_dependencies(control_inputs)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 360, in control_dependencies
    return super(FuncGraph, self).control_dependencies(filtered_control_inputs)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 4749, in control_dependencies
    c = self.as_graph_element(c)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 3670, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/home/caesar/PycharmProjects/fake-news-detector/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 3758, in _as_graph_element_locked
    raise TypeError(""Can not convert a %s into a %s."" %
TypeError: Can not convert a NoneType into a Tensor or Operation.

Process finished with exit code 1

```
It is because of the some graph elements due to which previous error showed up. 
Thank you for your time!
"
43734,'@grpc//:gpr_base' failed (Exit 1),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14 and 1.13
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip 
- Bazel version (if compiling from source): 0.24.1 
- GCC/Compiler version (if compiling from source): gcc-4.8.5 for bazel and gcc-8 for cuda libary
- CUDA/cuDNN version: 10.1 / 7
- GPU model and memory: 1070 ti 8gb



**Describe the problem**
I can't compile from tensorflow 1.14-1.13 source code (haven't tried other 1 tensorflow branches) and 2.4 has been successfully compiled with and without GPU. Originally the error was with newer versions of basel and gcc-8, but after reading the https://www.tensorflow.org/install/source table, I installed gcc-4.8, but the error persisted.


I have read the solutions
https://github.com/tensorflow/serving/issues/928 but adding the --cxxopt = -std = c ++ 11 flag didn't work for me
I also read that installing libc-ares-dev library might help, but that didn't help either

I thought the problem might be related to the fact that I am using cuda-10.1 and not 10.0 as in the table. I tried to set up the file without cuda, but I got exactly the same error, so I think this is not the problem
**Provide the exact sequence of commands / steps that you executed before running into the problem**

**writte config file**
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.24.1 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 


Found possible Python library paths:
  /usr/local/lib/python3.8/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.8/dist-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Found CUDA 10.1 in:
    /usr/local/cuda/lib64
    /usr/local/cuda/include
Found cuDNN 7 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: 


Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc-8


Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: -march=native -mno-avx


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apache Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished


**build command**
bazel build --cxxopt=-std=c++11 --config=cuda  //tensorflow/tools/pip_package:build_pip_package


**output error**
INFO: From ProtoCompile external/com_github_googleapis_googleapis/google/api/http.pb.h:
bazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.
INFO: From ProtoCompile external/com_github_googleapis_googleapis/google/rpc/status.pb.h:
bazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.
INFO: From Compiling external/mkl_dnn/src/cpu/simple_concat.cpp:
external/mkl_dnn/src/cpu/simple_concat.cpp:98: warning: ignoring #pragma omp simd [-Wunknown-pragmas]
             PRAGMA_OMP_SIMD()
 
INFO: From Compiling external/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc:
external/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc: In function 'grpc_error* try_http_parsing(grpc_chttp2_transport*)':
external/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc:2466:40: warning: 'void* memset(void*, int, size_t)' clearing an object of non-trivial type 'grpc_http_response' {aka 'struct grpc_http_response'}; use assignment or value-initialization instead [-Wclass-memaccess]
   memset(&response, 0, sizeof(response));
                                        ^
In file included from external/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc:44:
external/grpc/src/core/lib/http/parser.h:71:16: note: 'grpc_http_response' {aka 'struct grpc_http_response'} declared here
 typedef struct grpc_http_response {
                ^~~~~~~~~~~~~~~~~~
ERROR: /home/dmitry/.cache/bazel/_bazel_dmitry/1319b3f3ad6252a51422db144992f79d/external/grpc/BUILD:507:1: C++ compilation of rule '@grpc//:gpr_base' failed (Exit 1)
external/grpc/src/core/lib/gpr/log_linux.cc:43:13: error: ambiguating new declaration of 'long int gettid()'
 static long gettid(void) { return syscall(__NR_gettid); }
             ^~~~~~
In file included from /usr/include/unistd.h:1170,
                 from external/grpc/src/core/lib/gpr/log_linux.cc:41:
/usr/include/x86_64-linux-gnu/bits/unistd_ext.h:34:16: note: old declaration '__pid_t gettid()'
 extern __pid_t gettid (void) __THROW;
                ^~~~~~
external/grpc/src/core/lib/gpr/log_linux.cc:43:13: warning: 'long int gettid()' defined but not used [-Wunused-function]
 static long gettid(void) { return syscall(__NR_gettid); }
             ^~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 281.090s, Critical Path: 57.54s
INFO: 1539 processes: 1539 local.
FAILED: Build did NOT complete successfully



(there was even more INFO output, but I did not copy everything, there really is a lot)
"
43733,CUDA 10.1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Linux CentOS 7.8
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n.a.
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.3.1
- Python version:3.6
- Installed using virtualenv? pip? conda?: n.a.
- Bazel version (if compiling from source): 3.1.0 (bazelisk)
- GCC/Compiler version (if compiling from source):7.3.1
- CUDA/cuDNN version:10.1/7.6
- GPU model and memory:n.a.



**Describe the problem**
Compilation error when compiling tensorflow 2.3.1. with CUDA 10.1:

ERROR: /build/tensorflow/tensorflow/stream_executor/cuda/BUILD:457:1: C++ compilation of rule '//tensorflow/stream_executor/cuda:cusparse_stub' failed (Exit 1)
In file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:59:0:


**Provide the exact sequence of commands / steps that you executed before running into the problem**
Tried to follow the build instructions from source with the recommend/tested configuration:
https://www.tensorflow.org/install/source#tested_build_configurations

Here is my configure log:
[configure.txt](https://github.com/tensorflow/tensorflow/files/5318180/configure.txt)

Here is a minimal build command to reproduce the issue:
BAZEL_LINKLIBS=-l%:libstdc++.a bazel build --config=nohdfs --config=noaws --config=nogcp --config=nonccl  --config=monolithic --config=cuda --config=v2 --config=opt  //tensorflow/stream_executor/cuda:cusparse_stub

Note: I had to add BAZEL_LINKLIBS=-l%:libstdc++.a for CentOS, see 
https://github.com/tensorflow/tensorflow/issues/35867

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

As you can see in the build log, the error occurs in tensorflow/stream_executor/cuda/cusparse_stub.cc:59:0. I assume this is because of wrong version of tensorflow/stream_executor/cuda/cusparse_10_1.inc.
The file is identical to cusparse_10_2.inc and it works with CUDA 10.2

[build.txt](https://github.com/tensorflow/tensorflow/files/5318181/build.txt)
"
43732,Batch Renormalization via Keras Layer not working with TF 2+ and tf.function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.15.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `v2.3.0-54-gfcc4b966f1 2.3.1`
- Python version: `3.6.2`
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

Passing a Tensor derived from a `tf.keras.Input` to the `renorm_clipping` parameter of `tf.keras.layers.BatchNormalization` fails with an error like this:

```
_SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'RealDiv_7:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'AddV2_2:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'RealDiv_8:0' shape=(None, 1) dtype=float32>]
```

**Describe the expected behavior**

Since `renorm_clipping` expects Tensors, I would expect any Tensor to work, including those derived from `tf.keras.Input`.

**Standalone code to reproduce the issue**

[This notebook](https://gist.github.com/georgwiese/12d341bd131e2b16c8d4bb1e275289a6) shows a small example with TF 2.3.1. It creates a model with a step as input, so that batch renorm parameters can be computed from it. Please let me know if this is not the way the step should be passed to the model.

The model is then used in a `tf.function` (originally as part of a custom training loop, but simply running the forward pass reproduces the issue).

**Other info / logs**
See attached notebook for traceback.
"
43730,NASNetLarge Keras Model - accuracy is very very low,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Issue happening on the Server while training
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow-gpu==2.0.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0 and cuDNN 7.6.5
- GPU model and memory: 2 GPU Tesla K80 and total memory 22 GB


**Describe the current behavior**
Keras version 2.3.0
Tensorflow-gpu version 2.0.0

Dataset: Imagenet cars dataset
Preprocessing : Resizing images to 331 * 331 pixel using custom code and saving as new files, no other preprocessing

I tested the NASNetLarge Keras model with parameters as below

```
from keras.applications.nasnet import NASNetLarge 
model=NASNetLarge(input_shape =  (331, 331, 3),include_top = False,  weights = 'imagenet')
model.trainable = False

```
and created combined model using Sequential
Training Data and valid data processed through flow_from_directory and fed fit_generator 

Loss : approx 5.0
val_acc: approx 0.00813

"
43729,AutoGraph could not transform,"WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000022F3CE53430> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert"
43728,multi-core support ,I have a question regarding the use of TensorFlow Microcontroller in the case of some multicore device (ARM CortexM cores). How to use TensorFlow Microcontroller within a multi-core device and switch between cores (heterogenous)?
43725,There is no way to build Cortex-M55,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source): 1f9328f3a4662085b2cb948b9ed2b03d7259f78b
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**

**Please provide the exact sequence of commands/steps when you ran into the problem**
There is a need to build for generic Cortex-M devices. There has been some PRs in the right directions lately, e.g. this one, which has good debug support: https://github.com/tensorflow/tensorflow/pull/41860
But it does not support, e.g. Cortex-M55.
"
43724,How it is done convolution calculation order in conv2d?,"I'm worried about cancellation of significant digits by convolution calculation order in conv2d.
I tried coding on my own on Python(using tf.math.add etc.), could not be reproduced Tensorflow API conv2d output.
Do you know detailed internal processing or original source code in API conv2d?

Development environment:
    Python 3.6
    TensorFlow 2.1.0
    JupyterNotebook

"
43723,XLA Compilation Bug,"**System information**
Colab with TensorFlow 2.4.0-dev20201001

**Describe the current behavior**
When using `experimental_compile=True`, I get the following error:

```
InvalidArgumentError: Output shapes of then and else branches do not match: (f32[100,1]) vs. (f32[100])
	 [[{{node gradient_tape/loop_body/logistic_loss/Select/pfor/cond_1}}]] [Op:__inference_train_step_2474]
```

**Describe the expected behavior**
The code should run without errors just like with `experimental_compile=False`

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1TqFDfhxeM6JueuNYbiVBKj1gdUMw03GG?usp=sharing
"
43721,TFBertMainLayer cannot be loaded from .h5,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 LTS
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.9
- CUDA/cuDNN version: 10.1 / 7.6.5
- GPU model and memory: GeForce RTX 2080ti 

**The Issue**
When I try to load the model according to the [documentation](https://www.tensorflow.org/tutorials/keras/save_and_load), I face with following issue:

Running code:
```python
new_model.model = tf.keras.models.load_model(os.path.join(load_folder_path, trans_model_name))
```

Error:
```python
ValueError: Unknown layer: Custom>TFBertMainLayer
```

Traceback (most recent call last):
```python
  File ""bert_nlu_basic_api.py"", line 105, in <module>
    initialize()
  File ""bert_nlu_basic_api.py"", line 46, in initialize
    model = JointTransBertModel.load(load_folder_path)
  File ""/home/hakan/Documents/HB/dialog-nlu/models/joint_trans_bert.py"", line 42, in load
    return BaseJointTransformerModel.load_model_by_class(BaseJointTransformerModel, load_folder_path, 'joint_bert_model.h5')
  File ""/home/hakan/Documents/HB/dialog-nlu/models/base_joint_trans.py"", line 121, in load_model_by_class
    new_model.model = tf.keras.models.load_model(os.path.join(load_folder_path, trans_model_name))
  File ""/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py"", line 182, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File ""/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 178, in load_model_from_hdf5
    custom_objects=custom_objects)
  File ""/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/saving/model_config.py"", line 55, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py"", line 175, in deserialize
    printable_module_name='layer')
  File ""/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 358, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py"", line 617, in from_config
    config, custom_objects)
  File ""/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py"", line 1204, in reconstruct_from_config
    process_layer(layer_data)
  File ""/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py"", line 1186, in process_layer
    layer = deserialize_layer(layer_data, custom_objects=custom_objects)
  File ""/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py"", line 175, in deserialize
    printable_module_name='layer')
  File ""/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 347, in deserialize_keras_object
    config, module_objects, custom_objects, printable_module_name)
  File ""/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 296, in class_and_config_for_serialized_keras_object
    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)
ValueError: Unknown layer: Custom>TFBertMainLayer
```"
43720,compatibility issue with cuda 11.1,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0 master
- Python version: 3.8.6
- Bazel version (if compiling from source): 3.5.1
- GCC/Compiler version (if compiling from source):  visual studio 2019
- CUDA/cuDNN version: 11.1/ 8.0.4
- GPU model and memory: RTX2070 super 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
cudart library not loaded
**Describe the expected behavior**
cudart dll should be loaded properly
**Standalone code to reproduce the issue**
```
python
import tensorflow as tf
```
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

cudart64 library name for cuda 11.1 is cudart64_110.dll,  not cudart64_111.dll"
43719,Float16 quantized DistilBERT model performs poorly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.3
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

I used the DistilBERT model for a text classification task with the SST-2 dataset. Now, when I use the float16 quantization recipe to convert the trained model to TensorFlow Lite the performance drops significantly (90% validation accuracy to ~50%). Upon investigating more I found out the TensorFlow Lite float16 quantized model always outputs `nan`. I am not sure why this is the case. When I convert the original model using dynamic-range quantization the performance is retained greatly. 

**Describe the expected behavior**

The float16 quantized model should not return `nan`.

**Standalone code to reproduce the issue**
- [Model training and conversion notebook](https://github.com/sayakpaul/BERT-for-Mobile/blob/master/DistilBERT_SST-2_TPU.ipynb)
- [TensorFlow Lite model evaluationn notebook](https://colab.research.google.com/gist/sayakpaul/705d53e72f0f5722461f892150fde8b0/evaluation_sst-2_distilbert.ipynb)

This is a part of a project called [BERT for Mobile](https://github.com/sayakpaul/BERT-for-Mobile/) where I plan to compare two different BERT models designed specifically for mobile deployments - DistilBERT and MobileBERT. Currently, I have only taken text classification as the NLP task. I plan to also include question answering in the future. 

Cc: @MeghnaNatraj @khanhlvg "
43718,RTX 3090 Cuda 10.1 Kernal image error,"**System information**
RTX3090, AMD 3950x

Nvidia Driver 455.23

Fri Oct  2 00:41:55 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce RTX 3090    On   | 00000000:0D:00.0  On |                  N/A |
| 30%   33C    P8    31W / 350W |    590MiB / 24265MiB |     24%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1881      G   /usr/lib/xorg/Xorg                350MiB |
|    0   N/A  N/A      2159      G   /usr/bin/gnome-shell               52MiB |
|    0   N/A  N/A      2560      G   ...AAAAAAAAA= --shared-files       47MiB |
|    0   N/A  N/A      2565      G   ...token=9020920654293169663       35MiB |
|    0   N/A  N/A      3061      G   ...mviewer/tv_bin/TeamViewer       35MiB |
|    0   N/A  N/A      5620      G   ...AAAAAAAAA= --shared-files       64MiB |
+-----------------------------------------------------------------------------+

NVCC:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243

Tensorflow version:
v2.3.0-54-gfcc4b966f1 2.3.1

**Describe the current behavior**
<pre>2020-10-02 00:44:35.590969: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Observed TF Version:  2.3.1
Observed Numpy Version:  1.18.5
Directory already exists!
Observation Length:  28693
Linear Length:  28693
Angular Length:  28693
Load all complete
Currently using multiple GPUs
2020-10-02 00:44:37.843072: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-10-02 00:44:37.876858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 00:44:37.877518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:0d:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2020-10-02 00:44:37.877531: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-02 00:44:37.878382: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-02 00:44:37.879285: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-02 00:44:37.879431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-02 00:44:37.880196: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-02 00:44:37.880602: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-02 00:44:37.882240: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-02 00:44:37.882327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 00:44:37.883011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 00:44:37.883625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
Selected MultiGPU but only observe single GPU. Cancel Multi GPU training!
2020-10-02 00:44:37.889428: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-02 00:44:37.894039: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3493600000 Hz
2020-10-02 00:44:37.894764: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5f88190 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-02 00:44:37.894780: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-02 00:44:37.954768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 00:44:37.955482: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5f6fd90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-10-02 00:44:37.955503: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2020-10-02 00:44:37.955670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 00:44:37.956825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:0d:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2020-10-02 00:44:37.956850: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-02 00:44:37.956877: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-02 00:44:37.956891: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-02 00:44:37.956905: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-02 00:44:37.956918: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-02 00:44:37.956931: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-02 00:44:37.956945: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-02 00:44:37.957008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 00:44:37.958197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 00:44:37.959374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-10-02 00:44:37.959401: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
</pre>

Halts for a minute then error:

<pre>Traceback (most recent call last):
  File &quot;train.py&quot;, line 94, in &lt;module&gt;
    model = FrankNet.build(200, 150)
  File &quot;/home/frank/Duckietown/AIDO5/challenge-aido_LF-baseline-behavior-cloning/duckieTrainer/frankModel.py&quot;, line 76, in build
    linearVelocity = FrankNet.build_linear_branch(inputs)
  File &quot;/home/frank/Duckietown/AIDO5/challenge-aido_LF-baseline-behavior-cloning/duckieTrainer/frankModel.py&quot;, line 12, in build_linear_branch
    x = Conv2D(24, (5, 5), strides=(2, 2), padding=&quot;valid&quot;)(x)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 925, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 1098, in _functional_construction_call
    self._maybe_build(inputs)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 2643, in _maybe_build
    self.build(input_shapes)  # pylint:disable=not-callable
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py&quot;, line 197, in build
    self.kernel = self.add_weight(
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 597, in add_weight
    variable = self._add_variable_with_custom_getter(
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py&quot;, line 745, in _add_variable_with_custom_getter
    new_variable = getter(
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_utils.py&quot;, line 133, in make_variable
    return tf_variables.VariableV1(
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py&quot;, line 260, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py&quot;, line 206, in _variable_v1_call
    return previous_getter(
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py&quot;, line 67, in getter
    return captured_getter(captured_previous, **kwargs)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py&quot;, line 2857, in creator
    return next_creator(**kwargs)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py&quot;, line 199, in &lt;lambda&gt;
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py&quot;, line 2583, in default_variable_creator
    return resource_variable_ops.ResourceVariable(
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py&quot;, line 264, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py&quot;, line 1507, in __init__
    self._init_from_args(
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py&quot;, line 1651, in _init_from_args
    initial_value() if init_from_fn else initial_value,
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/keras/initializers/initializers_v2.py&quot;, line 397, in __call__
    return super(VarianceScaling, self).__call__(shape, dtype=_get_dtype(dtype))
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/init_ops_v2.py&quot;, line 561, in __call__
    return self._random_generator.random_uniform(shape, -limit, limit, dtype)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/init_ops_v2.py&quot;, line 1043, in random_uniform
    return op(
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py&quot;, line 201, in wrapper
    return target(*args, **kwargs)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/random_ops.py&quot;, line 288, in random_uniform
    shape = tensor_util.shape_tensor(shape)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py&quot;, line 1029, in shape_tensor
    return ops.convert_to_tensor(shape, dtype=dtype, name=&quot;shape&quot;)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py&quot;, line 1499, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 338, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 263, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 275, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 300, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 97, in convert_to_eager_tensor
    ctx.ensure_initialized()
  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py&quot;, line 539, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid
</pre>

**Describe the expected behavior**
Code should execute fine.
"
43712,Docs: provide CUDA/CuDNN/TensorRT version information for PyPI packages,"The title says it all, but I'll explain nonetheless.

I have CUDA 10.1 Update 2, CuDNN 7.6.5 for CUDA 10.1, TensorRT 6.0.1.5 installed and run `tensorflow==2.3.1` from PyPI.

Today I tried `tf-nightly`, which suddenly does not support my GPU. The error message implies that some CUDA 11 is missing, but now I am at a loss what to ask my server admin to install. Obviously, I have to install what the PyPI packages were compiled against.

- CUDA 11.0? 11.1? 2 possibilities.
- CuDNN 7.6? 8.0? For CUDA 10.1? 11.0? 11.1? 6 possibilities.
- TensorRT 6? 7.0? 7.1? 7.2? 4 possibilities.

I would like to be able to find this out without going through several (up to 50?) iterations with my server admin, and without having to ask to install *everything*.

I understand that maybe this information is not available yet for 2.4.0, but 

- it should be available for current `tf-nightly`, and
- it should *definitely* be available for `tensorflow==2.3.1` (it may be that this is what https://www.tensorflow.org/install/gpu has, but that wording is awkward: ""TensorFlow supports"" should probably be ""TensorFlow requires"", and I don't think TensorRT is optional: I remember having had to install it; also, the top says ""See the pip install guide for available packages, *systems requirements*, and instructions"", and then that page does not say anything about CUDA/CuDNN/etc., but instead refers back to the first page for CUDA),
- it should be available for older versions, too."
43710,executing vectorized_map on batches triggers retracing,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): dockerhub container 'latest' Digest: c57fb9628d80
- TensorFlow version:tf.version.GIT_VERSION=v2.3.0-54-gfcc4b966f1 tf.version.VERSION=2.3.1
- Python version: 3.6.9

**Describe the current behavior**
Repeatedly calling vectorized_map on the same function with parameters of same shape and dtype triggers retracing.

It seems that vectorized_map allocates memory to run all iterations simultaneously and this is causing OOM. Therefore, I separated the execution on batches to reduce the memory allocation of the function.  If there is a better way to do this, please let me know.

**Describe the expected behavior**
No retracing

**Standalone code to reproduce the issue**

```python
import tensorflow as tf
import time

print('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))
print('tf.version.VERSION={}'.format(tf.version.VERSION))


def _jvp(f, primals, tangents):
    with tf.autodiff.ForwardAccumulator(primals, tangents) as acc:
        primals_out = f(*primals)
    return primals_out, acc.jvp(
        primals_out, unconnected_gradients=tf.UnconnectedGradients.ZERO)

def f(x, z, t, c, v1, v2, v3, v4):

    p = tf.concat([x, z, t], axis=1)
    pe = p[:, :, None]
    ce = tf.transpose(a=c[:, :, None], perm=[2, 1, 0])
    d = ce - pe
    r = tf.reduce_sum(input_tensor=tf.square(d), axis=1)
    G = tf.exp(-r / 2)

    p = tf.reduce_sum(input_tensor=G * v1, axis=1, keepdims=True)
    b = tf.reduce_sum(input_tensor=G * v2, axis=1, keepdims=True)
    u = tf.reduce_sum(input_tensor=G * v3, axis=1, keepdims=True)
    w = tf.reduce_sum(input_tensor=G * v4, axis=1, keepdims=True)

    return p, b, u, w

def g(x, z, t, c, v1, v2, v3, v4):

    tf.print('tf.print shapes', *[tf.shape(w) for w in (x, z, t, c, v1, v2, v3, v4)])
    tf.print('tf.print dtypes', *[w.dtype for w in (x, z, t, c, v1, v2, v3, v4)])
    #print('py.print x.shape', tf.shape(x))

    fx = lambda xi: f(xi, z, t, c, v1, v2, v3, v4)
    with tf.autodiff.ForwardAccumulator(primals=[x], tangents=[tf.ones_like(x)]) as fwd_outer:
        [dpdx, dbdx, dudx, dwdx] = _jvp(fx, [x], [tf.ones_like(x)])[1]
    [d2bd2x, d2ud2x, d2wd2x] = fwd_outer.jvp([dbdx, dudx, dwdx], tf.UnconnectedGradients.ZERO)

    fz = lambda zi: f(x, zi, t, c, v1, v2, v3, v4)
    with tf.autodiff.ForwardAccumulator(primals=[z], tangents=[tf.ones_like(z)]) as fwd_outer:
        [dpdz, dbdz, dudz, dwdz] = _jvp(fz, [z], [tf.ones_like(z)])[1]
    [d2bd2z, d2ud2z, d2wd2z] = fwd_outer.jvp([dbdz, dudz, dwdz], tf.UnconnectedGradients.ZERO)

    ft = lambda ti: f(x, z, ti, c, v1, v2, v3, v4)
    [p, b, u, w], [dpdt, dbdt, dudt, dwdt] = _jvp(ft, [t], [tf.ones_like(t)])

    return dudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz, d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z,


n = 7500
x = tf.random.uniform((n, 1), dtype=tf.float64)
z = tf.random.uniform((n, 1), dtype=tf.float64)
t = tf.random.uniform((n, 1), dtype=tf.float64)

c = tf.random.uniform((n,3), dtype=tf.float64)

v1 = tf.random.uniform((1, n), dtype=tf.float64)
v2 = tf.random.uniform((1, n), dtype=tf.float64)
v3 = tf.random.uniform((1, n), dtype=tf.float64)
v4 = tf.random.uniform((1, n), dtype=tf.float64)


def batched_execution(fb, args, batch_size):
    batch_size = tf.cast(batch_size, tf.int32)
    n = tf.shape(args[0])[0]
    i0 = tf.constant(0, dtype=tf.int32)
    souts = []
    while tf.less_equal(i0 + batch_size, n):
        tf.print('tf print batch iteration batch_size={}'.format(batch_size))
        il = i0 + batch_size
        bsargs = [a[i0:il] for a in args]
        bouts = fb(bsargs)
        souts.append(bouts)
        i0 = il
    if tf.less(i0, n):
        tf.print('tf print last batch iteration batch_size={}'.format(n-i0))
        bsargs = [a[i0:n] for a in args]
        bouts = fb(bsargs)
        souts.append(bouts)

    souts = [tf.concat([o[i] for o in souts], axis=0) for i in range(len(souts[0]))]
    return souts

def shaped_vectorized_map(fv, args, axis=2):
    sargs = [tf.expand_dims(a, axis=axis) for a in args]
    outs = batched_execution(lambda args: tf.vectorized_map(fv, args, fallback_to_while_loop=False), sargs, batch_size=1000)
    souts = [tf.squeeze(o, axis=[axis]) for o in outs]
    return souts

start_time = time.clock()
e2v = shaped_vectorized_map(lambda args: g(*args, c, v1, v2, v3, v4), (x, z, t))
delta_time = time.clock() - start_time
print('running with batched vectorized_map took {:f} seconds'.format(delta_time))
```

**Other info / logs** 

```
tf.version.GIT_VERSION=v2.3.0-54-gfcc4b966f1
tf.version.VERSION=2.3.1
tf print batch iteration batch_size=1000
tf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]
tf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64
tf print batch iteration batch_size=1000
tf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]
tf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64
tf print batch iteration batch_size=1000
tf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]
tf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64
tf print batch iteration batch_size=1000
tf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]
tf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64
tf print batch iteration batch_size=1000
tf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]
tf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64
WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x7f54481b3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
tf print batch iteration batch_size=1000
tf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]
tf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64
WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x7f54402c4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
tf print batch iteration batch_size=1000
tf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]
tf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64
WARNING:tensorflow:7 out of the last 7 calls to <function pfor.<locals>.f at 0x7f54407a37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
tf print last batch iteration batch_size=500
tf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]
tf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64
WARNING:tensorflow:8 out of the last 8 calls to <function pfor.<locals>.f at 0x7f54400c17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
running with batched vectorized_map took 119.087042 seconds
```

related issues: #42835 #43252"
43709,"ValueError: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build","model = tf.keras.Sequential([
layers.DenseFeatures(feature_columns),
layers.Dense(128,activation='relu'),
layers.Dropout(0.2),
layers.Dense(512,activation='relu'),
layers.Dropout(0.2),
layers.Dense(512, activation='relu'),
layers.Dense(31,activation='softmax')
])

This is my model.How do i solve this issue or add input shape to the first layer? Please help
When i try to save and load it with this piece of code 
model.save(""my_model_tf"",save_format=""tf"")
model_temp = keras.models.load_model('my_model_tf')
it works fine but when i write model.summary() it gives the above the title error.I have to deploy it on aws so it would be an issue there  so please help"
43708,Masking support for multiple values.,"Is there a way to mask more than one value in input before sending to Keras layers in TensorFlow? I have to pad in-between too with 0 to make the data balanced in addition to post padding. But my Seq2Seq model is unable to distinguish the in-between paddings and the paddings at the end and during testing, the output from decoder stops as soon as the first in-between padding occurs.

Consider an input tensor to be,

    [[1,2,3,4], [5,6,7]] 

Below is the way I am padding the above tensor. Each sub-list must be of size 6 and the total number of sub-lists should be 3.

    [ 3,4,5,6,0,0  , 7,8,9,0,0,0  , 0,0,0,0,0,0 ]

I  want my tensor to be masked this way, where 2 is also masked along with 0, before sending to subsequent layers like embedding and RNN layers.

    [ 3,4,5,6,2,2  , 7,8,9,2,2,2,  0,0,0,0,0,0 ]

Could some one also point out a possible solution for preventing the decoder output from stopping at the end of processing the first sub-list of an input?"
43707,TPU V3-1024 training crashes randomly,"**System information**
- Platform: Linux-5.4.0-1024-gcp-x86_64-with-debian-buster-sid
- Python version: 3.7.7
- Tennsorflow v1.15.2-30-g4386a66 1.15.3

**Describe the current behavior**
While training T5-11B model using mesh tensorflow and tensorflow on TPU V3-1024, the training crash randomly while training. After I remove the TPU node and create it again, then restart the training it resumes normally.  

**Describe the expected behavior**
The crash should not occur as the dataset was tested with T5-3B model without any issue.

**Standalone code to reproduce the issue**
This will be hard, but this is our running command:
```
python -m t5.models.mesh_transformer_main   \
--module_import=""bfd100_task""   \
--tpu=""tpuv3-1024""   \
--gcp_project=""tum-covid19""   \
--tpu_zone=""europe-west4-a""   \
--model_dir=""gs://prot-transformers-eu/t5/models/bfd100_v2/11b_mask_rsqrt_4k/""   \
--gin_file=""objectives/mask_denoise.gin""   \
--gin_file=""models/t5.1.0.11B.gin""   \
--gin_file=""dataset.gin""   \
--gin_file=""learning_rate_schedules/rsqrt_no_ramp_down.gin""   \
--gin_param=""MIXTURE_NAME = 'unsupervised_bfd_protein_denoising_task'""   \
--gin_param=""utils.tpu_mesh_shape.tpu_topology = '16x32'""   \
--gin_param=""utils.tpu_mesh_shape.model_parallelism = 32""   \
--gin_param=""utils.run.save_checkpoints_steps=10000""   \
--gin_param=""utils.run.batch_size=('tokens_per_batch', 2097152)""   \
--gin_param=""utils.run.train_steps=1200000""   \
--gin_param=""utils.run.iterations_per_loop=2000""   \
--gin_param=""learning_rate_schedule_noam.warmup_steps=80000""   \
--gin_param=""SentencePieceVocabulary.extra_ids=100""   \
--gin_param=""run.perplexity_eval_steps=100""
```

**Other info / logs** Include any logs or source code that would be helpful to
```
I1001 16:08:36.562530 139829429921536 tpu_estimator.py:279] Outfeed finished for iteration (37, 555)                                                                                                      
INFO:tensorflow:Outfeed finished for iteration (37, 564)                                                                                                                                                  
I1001 16:09:42.150621 139829429921536 tpu_estimator.py:279] Outfeed finished for iteration (37, 564)                                                                                                      
2020-10-01 16:10:42.574305: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1601568642.5$
4100849"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC          
2020-10-01 16:10:42.574367: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1601568642.5$
4157686"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC          
2020-10-01 16:10:42.574304: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = ""Unavailable: Socket closed"" and grpc_error_string = ""{""created"":""@1601568642.5$
4136203"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Socket closed"",""grpc_status"":14}"", maybe retrying the RPC          
INFO:tensorflow:An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. This error may also 
occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the jo$
. Error: Session bb9ea551c191de28 is not found.                                                                                                                                                           
I1001 16:13:55.492127 139833409849152 monitored_session.py:1269] An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a 
new session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing t$
e number of parameter servers assigned to the job. Error: Session bb9ea551c191de28 is not found.                                                                                                          
WARNING:tensorflow:An error occurred when attempting to close the session. This may be due to a preemption in a connected worker or parameter server. Error: Session bb9ea551c191de28 is not found. Possi$
ly, this master has restarted.                                                                                                                                                                            
W1001 16:13:55.495698 139833409849152 monitored_session.py:1171] An error occurred when attempting to close the session. This may be due to a preemption in a connected worker or parameter server. Error$
 Session bb9ea551c191de28 is not found. Possibly, this master has restarted.                                                                                                                              
INFO:tensorflow:Graph was finalized.                                                                                                                                                                      
I1001 16:13:55.496506 139833409849152 monitored_session.py:240] Graph was finalized.                                                                                                                      
INFO:tensorflow:Restoring parameters from gs://prot-transformers-eu/t5/models/bfd100_v2/11b_mask_rsqrt_4k/model.ckpt-72000                                                                                
I1001 16:13:55.769371 139833409849152 saver.py:1284] Restoring parameters from gs://prot-transformers-eu/t5/models/bfd100_v2/11b_mask_rsqrt_4k/model.ckpt-72000                                           
WARNING:tensorflow:From /home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_man$
gement) is deprecated and will be removed in a future version.                                                                                                                                            
Instructions for updating:                                                                                                                                                                                
Use standard file utilities to get mtimes.                                                                                                                                                                
W1001 16:18:45.793992 139833409849152 deprecation.py:323] From /home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from te$
sorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.                                                                                                     
Instructions for updating:                                                                                                                                                                                
Use standard file utilities to get mtimes.                                                                                                                                                                
INFO:tensorflow:Running local_init_op.                                                                                                                                                                    
I1001 16:18:50.382259 139833409849152 session_manager.py:500] Running local_init_op.                                                                                                                      
INFO:tensorflow:Done running local_init_op.                                                                                                                                                               
I1001 16:18:54.161618 139833409849152 session_manager.py:502] Done running local_init_op.                                                                                                                 
INFO:tensorflow:Initialized dataset iterators in 4 seconds                                                                                                                                                
I1001 16:19:10.171010 139833409849152 util.py:98] Initialized dataset iterators in 4 seconds
INFO:tensorflow:Installing graceful shutdown hook.
I1001 16:19:10.171908 139833409849152 session_support.py:332] Installing graceful shutdown hook.
2020-10-01 16:19:10.172536: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:370] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session
 has not yet been created.
INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:1/device:CPU:0', '/job:worker/replica:0/task:3/device:CPU:0', '/job:worker/replic
a:0/task:6/device:CPU:0', '/job:worker/replica:0/task:7/device:CPU:0', '/job:worker/replica:0/task:8/device:CPU:0', '/job:worker/replica:0/task:9/device:CPU:0', '/job:worker/replica:0/task:14/device:CPU
:0', '/job:worker/replica:0/task:11/device:CPU:0', '/job:worker/replica:0/task:17/device:CPU:0', '/job:worker/replica:0/task:15/device:CPU:0', '/job:worker/replica:0/task:19/device:CPU:0', '/job:worker/
replica:0/task:16/device:CPU:0', '/job:worker/replica:0/task:23/device:CPU:0', '/job:worker/replica:0/task:22/device:CPU:0', '/job:worker/replica:0/task:24/device:CPU:0', '/job:worker/replica:0/task:25/
device:CPU:0', '/job:worker/replica:0/task:27/device:CPU:0', '/job:worker/replica:0/task:30/device:CPU:0', '/job:worker/replica:0/task:33/device:CPU:0', '/job:worker/replica:0/task:31/device:CPU:0', '/j
ob:worker/replica:0/task:32/device:CPU:0', '/job:worker/replica:0/task:35/device:CPU:0', '/job:worker/replica:0/task:39/device:CPU:0', '/job:worker/replica:0/task:4/device:CPU:0', '/job:worker/replica:0
/task:34/device:CPU:0', '/job:worker/replica:0/task:12/device:CPU:0', '/job:worker/replica:0/task:20/device:CPU:0', '/job:worker/replica:0/task:28/device:CPU:0', '/job:worker/replica:0/task:2/device:CPU
:0', '/job:worker/replica:0/task:36/device:CPU:0', '/job:worker/replica:0/task:26/device:CPU:0', '/job:worker/replica:0/task:10/device:CPU:0', '/job:worker/replica:0/task:18/device:CPU:0', '/job:worker/
replica:0/task:5/device:CPU:0', '/job:worker/replica:0/task:40/device:CPU:0', '/job:worker/replica:0/task:21/device:CPU:0', '/job:worker/replica:0/task:13/device:CPU:0', '/job:worker/replica:0/task:38/d
evice:CPU:0', '/job:worker/replica:0/task:37/device:CPU:0', '/job:worker/replica:0/task:29/device:CPU:0', '/job:worker/replica:0/task:41/device:CPU:0', '/job:worker/replica:0/task:43/device:CPU:0', '/jo
b:worker/replica:0/task:45/device:CPU:0', '/job:worker/replica:0/task:42/device:CPU:0', '/job:worker/replica:0/task:44/device:CPU:0', '/job:worker/replica:0/task:46/device:CPU:0', '/job:worker/replica:0
/task:47/device:CPU:0', '/job:worker/replica:0/task:48/device:CPU:0', '/job:worker/replica:0/task:49/device:CPU:0', '/job:worker/replica:0/task:50/device:CPU:0', '/job:worker/replica:0/task:51/device:CP
U:0', '/job:worker/replica:0/task:53/device:CPU:0', '/job:worker/replica:0/task:52/device:CPU:0', '/job:worker/replica:0/task:55/device:CPU:0', '/job:worker/replica:0/task:54/device:CPU:0', '/job:worker
/replica:0/task:56/device:CPU:0', '/job:worker/replica:0/task:59/device:CPU:0', '/job:worker/replica:0/task:57/device:CPU:0', '/job:worker/replica:0/task:58/device:CPU:0', '/job:worker/replica:0/task:60
/device:CPU:0', '/job:worker/replica:0/task:62/device:CPU:0', '/job:worker/replica:0/task:61/device:CPU:0', '/job:worker/replica:0/task:63/device:CPU:0', '/job:worker/replica:0/task:64/device:CPU:0', '/
job:worker/replica:0/task:65/device:CPU:0', '/job:worker/replica:0/task:67/device:CPU:0', '/job:worker/replica:0/task:66/device:CPU:0', '/job:worker/replica:0/task:68/device:CPU:0', '/job:worker/replica
:0/task:70/device:CPU:0', '/job:worker/replica:0/task:69/device:CPU:0', '/job:worker/replica:0/task:71/device:CPU:0', '/job:worker/replica:0/task:72/device:CPU:0', '/job:worker/replica:0/task:73/device:
CPU:0', '/job:worker/replica:0/task:74/device:CPU:0', '/job:worker/replica:0/task:75/device:CPU:0', '/job:worker/replica:0/task:78/device:CPU:0', '/job:worker/replica:0/task:79/device:CPU:0', '/job:work
er/replica:0/task:76/device:CPU:0', '/job:worker/replica:0/task:77/device:CPU:0', '/job:worker/replica:0/task:80/device:CPU:0', '/job:worker/replica:0/task:82/device:CPU:0', '/job:worker/replica:0/task:
81/device:CPU:0', '/job:worker/replica:0/task:85/device:CPU:0', '/job:worker/replica:0/task:84/device:CPU:0', '/job:worker/replica:0/task:83/device:CPU:0', '/job:worker/replica:0/task:86/device:CPU:0',
'/job:worker/replica:0/task:87/device:CPU:0', '/job:worker/replica:0/task:88/device:CPU:0', '/job:worker/replica:0/task:89/device:CPU:0', '/job:worker/replica:0/task:92/device:CPU:0', '/job:worker/repli
ca:0/task:90/device:CPU:0', '/job:worker/replica:0/task:93/device:CPU:0', '/job:worker/replica:0/task:91/device:CPU:0', '/job:worker/replica:0/task:94/device:CPU:0', '/job:worker/replica:0/task:95/devic
e:CPU:0', '/job:worker/replica:0/task:98/device:CPU:0', '/job:worker/replica:0/task:96/device:CPU:0', '/job:worker/replica:0/task:97/device:CPU:0', '/job:worker/replica:0/task:100/device:CPU:0', '/job:w
orker/replica:0/task:99/device:CPU:0', '/job:worker/replica:0/task:101/device:CPU:0', '/job:worker/replica:0/task:103/device:CPU:0', '/job:worker/replica:0/task:102/device:CPU:0', '/job:worker/replica:0
/task:104/device:CPU:0', '/job:worker/replica:0/task:106/device:CPU:0', '/job:worker/replica:0/task:105/device:CPU:0', '/job:worker/replica:0/task:107/device:CPU:0', '/job:worker/replica:0/task:108/devi
ce:CPU:0', '/job:worker/replica:0/task:109/device:CPU:0', '/job:worker/replica:0/task:111/device:CPU:0', '/job:worker/replica:0/task:110/device:CPU:0', '/job:worker/replica:0/task:113/device:CPU:0', '/j
ob:worker/replica:0/task:112/device:CPU:0', '/job:worker/replica:0/task:114/device:CPU:0', '/job:worker/replica:0/task:115/device:CPU:0', '/job:worker/replica:0/task:117/device:CPU:0', '/job:worker/repl
ica:0/task:116/device:CPU:0', '/job:worker/replica:0/task:118/device:CPU:0', '/job:worker/replica:0/task:119/device:CPU:0', '/job:worker/replica:0/task:121/device:CPU:0', '/job:worker/replica:0/task:123
/device:CPU:0', '/job:worker/replica:0/task:122/device:CPU:0', '/job:worker/replica:0/task:120/device:CPU:0', '/job:worker/replica:0/task:124/device:CPU:0', '/job:worker/replica:0/task:126/device:CPU:0'
, '/job:worker/replica:0/task:125/device:CPU:0', '/job:worker/replica:0/task:127/device:CPU:0']
I1001 16:19:10.224856 139833409849152 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:1/device:CPU:0', '/job:wor[334/1854]
a:0/task:3/device:CPU:0', '/job:worker/replica:0/task:6/device:CPU:0', '/job:worker/replica:0/task:7/device:CPU:0', '/job:worker/replica:0/task:8/device:CPU:0', '/job:worker/replica:0/task:9/device:CPU:
0', '/job:worker/replica:0/task:14/device:CPU:0', '/job:worker/replica:0/task:11/device:CPU:0', '/job:worker/replica:0/task:17/device:CPU:0', '/job:worker/replica:0/task:15/device:CPU:0', '/job:worker/r
eplica:0/task:19/device:CPU:0', '/job:worker/replica:0/task:16/device:CPU:0', '/job:worker/replica:0/task:23/device:CPU:0', '/job:worker/replica:0/task:22/device:CPU:0', '/job:worker/replica:0/task:24/d
evice:CPU:0', '/job:worker/replica:0/task:25/device:CPU:0', '/job:worker/replica:0/task:27/device:CPU:0', '/job:worker/replica:0/task:30/device:CPU:0', '/job:worker/replica:0/task:33/device:CPU:0', '/jo
b:worker/replica:0/task:31/device:CPU:0', '/job:worker/replica:0/task:32/device:CPU:0', '/job:worker/replica:0/task:35/device:CPU:0', '/job:worker/replica:0/task:39/device:CPU:0', '/job:worker/replica:0
/task:4/device:CPU:0', '/job:worker/replica:0/task:34/device:CPU:0', '/job:worker/replica:0/task:12/device:CPU:0', '/job:worker/replica:0/task:20/device:CPU:0', '/job:worker/replica:0/task:28/device:CPU
:0', '/job:worker/replica:0/task:2/device:CPU:0', '/job:worker/replica:0/task:36/device:CPU:0', '/job:worker/replica:0/task:26/device:CPU:0', '/job:worker/replica:0/task:10/device:CPU:0', '/job:worker/r
eplica:0/task:18/device:CPU:0', '/job:worker/replica:0/task:5/device:CPU:0', '/job:worker/replica:0/task:40/device:CPU:0', '/job:worker/replica:0/task:21/device:CPU:0', '/job:worker/replica:0/task:13/de
vice:CPU:0', '/job:worker/replica:0/task:38/device:CPU:0', '/job:worker/replica:0/task:37/device:CPU:0', '/job:worker/replica:0/task:29/device:CPU:0', '/job:worker/replica:0/task:41/device:CPU:0', '/job
:worker/replica:0/task:43/device:CPU:0', '/job:worker/replica:0/task:45/device:CPU:0', '/job:worker/replica:0/task:42/device:CPU:0', '/job:worker/replica:0/task:44/device:CPU:0', '/job:worker/replica:0/
task:46/device:CPU:0', '/job:worker/replica:0/task:47/device:CPU:0', '/job:worker/replica:0/task:48/device:CPU:0', '/job:worker/replica:0/task:49/device:CPU:0', '/job:worker/replica:0/task:50/device:CPU
:0', '/job:worker/replica:0/task:51/device:CPU:0', '/job:worker/replica:0/task:53/device:CPU:0', '/job:worker/replica:0/task:52/device:CPU:0', '/job:worker/replica:0/task:55/device:CPU:0', '/job:worker/
replica:0/task:54/device:CPU:0', '/job:worker/replica:0/task:56/device:CPU:0', '/job:worker/replica:0/task:59/device:CPU:0', '/job:worker/replica:0/task:57/device:CPU:0', '/job:worker/replica:0/task:58/
device:CPU:0', '/job:worker/replica:0/task:60/device:CPU:0', '/job:worker/replica:0/task:62/device:CPU:0', '/job:worker/replica:0/task:61/device:CPU:0', '/job:worker/replica:0/task:63/device:CPU:0', '/j
ob:worker/replica:0/task:64/device:CPU:0', '/job:worker/replica:0/task:65/device:CPU:0', '/job:worker/replica:0/task:67/device:CPU:0', '/job:worker/replica:0/task:66/device:CPU:0', '/job:worker/replica:
0/task:68/device:CPU:0', '/job:worker/replica:0/task:70/device:CPU:0', '/job:worker/replica:0/task:69/device:CPU:0', '/job:worker/replica:0/task:71/device:CPU:0', '/job:worker/replica:0/task:72/device:C
PU:0', '/job:worker/replica:0/task:73/device:CPU:0', '/job:worker/replica:0/task:74/device:CPU:0', '/job:worker/replica:0/task:75/device:CPU:0', '/job:worker/replica:0/task:78/device:CPU:0', '/job:worke
r/replica:0/task:79/device:CPU:0', '/job:worker/replica:0/task:76/device:CPU:0', '/job:worker/replica:0/task:77/device:CPU:0', '/job:worker/replica:0/task:80/device:CPU:0', '/job:worker/replica:0/task:8
2/device:CPU:0', '/job:worker/replica:0/task:81/device:CPU:0', '/job:worker/replica:0/task:85/device:CPU:0', '/job:worker/replica:0/task:84/device:CPU:0', '/job:worker/replica:0/task:83/device:CPU:0', '
/job:worker/replica:0/task:86/device:CPU:0', '/job:worker/replica:0/task:87/device:CPU:0', '/job:worker/replica:0/task:88/device:CPU:0', '/job:worker/replica:0/task:89/device:CPU:0', '/job:worker/replic
a:0/task:92/device:CPU:0', '/job:worker/replica:0/task:90/device:CPU:0', '/job:worker/replica:0/task:93/device:CPU:0', '/job:worker/replica:0/task:91/device:CPU:0', '/job:worker/replica:0/task:94/device
:CPU:0', '/job:worker/replica:0/task:95/device:CPU:0', '/job:worker/replica:0/task:98/device:CPU:0', '/job:worker/replica:0/task:96/device:CPU:0', '/job:worker/replica:0/task:97/device:CPU:0', '/job:wor
ker/replica:0/task:100/device:CPU:0', '/job:worker/replica:0/task:99/device:CPU:0', '/job:worker/replica:0/task:101/device:CPU:0', '/job:worker/replica:0/task:103/device:CPU:0', '/job:worker/replica:0/t
ask:102/device:CPU:0', '/job:worker/replica:0/task:104/device:CPU:0', '/job:worker/replica:0/task:106/device:CPU:0', '/job:worker/replica:0/task:105/device:CPU:0', '/job:worker/replica:0/task:107/device
:CPU:0', '/job:worker/replica:0/task:108/device:CPU:0', '/job:worker/replica:0/task:109/device:CPU:0', '/job:worker/replica:0/task:111/device:CPU:0', '/job:worker/replica:0/task:110/device:CPU:0', '/job
:worker/replica:0/task:113/device:CPU:0', '/job:worker/replica:0/task:112/device:CPU:0', '/job:worker/replica:0/task:114/device:CPU:0', '/job:worker/replica:0/task:115/device:CPU:0', '/job:worker/replic
a:0/task:117/device:CPU:0', '/job:worker/replica:0/task:116/device:CPU:0', '/job:worker/replica:0/task:118/device:CPU:0', '/job:worker/replica:0/task:119/device:CPU:0', '/job:worker/replica:0/task:121/d
evice:CPU:0', '/job:worker/replica:0/task:123/device:CPU:0', '/job:worker/replica:0/task:122/device:CPU:0', '/job:worker/replica:0/task:120/device:CPU:0', '/job:worker/replica:0/task:124/device:CPU:0',
'/job:worker/replica:0/task:126/device:CPU:0', '/job:worker/replica:0/task:125/device:CPU:0', '/job:worker/replica:0/task:127/device:CPU:0']
INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR

I1001 16:19:10.292326 139833409849152 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR

INFO:tensorflow:Starting infeed thread controller.
I1001 16:19:10.425031 139829429921536 tpu_estimator.py:521] Starting infeed thread controller.
INFO:tensorflow:Starting outfeed thread controller.
I1001 16:19:10.425418 139829438314240 tpu_estimator.py:540] Starting outfeed thread controller.
INFO:tensorflow:Before copy master to slices.
I1001 16:19:10.435552 139833409849152 ops.py:5767] Before copy master to slices.
I1001 16:19:10.577743 139829421528832 transport.py:151] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'tpuv3-1024' in state READY, and health HEALTHY.
W1001 16:19:10.706137 139829421528832 preempted_hook.py:91] TPUPollingThread found TPU b'tpuv3-1024' in state READY, and health HEALTHY.
ERROR:tensorflow:Error recorded from training_loop: From /job:worker/replica:0/task:0:
3 root error(s) found.
  (0) Failed precondition: The TPU system has not been initialized.
         [[node shared/embedding_slot_vr_1/Slice_24 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]
  (1) Failed precondition: The TPU system has not been initialized.
         [[node shared/embedding_slot_vr_1/Slice_25 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]
  (2) Failed precondition: The TPU system has not been initialized.
         [[node shared/embedding_slot_vr_1/Slice_26 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]
0 successful operations.
6 derived errors ignored.
Original stack trace for 'shared/embedding_slot_vr_1/Slice_24':                                                                                                                                 [280/1854]
  File ""/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/site-packages/t5/models/mesh_transformer_main.py"", line 244, in <module>
    console_entry_point()
  File ""/site-packages/t5/models/mesh_transformer_main.py"", line 241, in console_entry_point
    app.run(main)
  File ""/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/site-packages/t5/models/mesh_transformer_main.py"", line 235, in main
    model_dir=FLAGS.model_dir)
  File ""/site-packages/gin/config.py"", line 1055, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/site-packages/mesh_tensorflow/transformer/utils.py"", line 2115, in run
    train_dataset_fn, train_steps, ensemble_inputs)
  File ""/site-packages/mesh_tensorflow/transformer/utils.py"", line 1498, in train_model
    estimator.train(input_fn=input_fn, max_steps=train_steps)
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3030, in train
    saving_listeners=saving_listeners)
  File ""/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 370, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1161, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1191, in _train_model_default
    features, labels, ModeKeys.TRAIN, self.config)
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2857, in _call_model_fn
    config)
  File ""/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1149, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3159, in _model_fn
    _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3604, in _train_on_tpu_system
    device_assignment=ctx.device_assignment)
  File ""/site-packages/tensorflow_core/python/tpu/tpu.py"", line 1277, in split_compile_and_shard
    name=name)
  File ""/site-packages/tensorflow_core/python/tpu/tpu.py"", line 992, in split_compile_and_replicate
    outputs = computation(*computation_inputs)
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3589, in multi_tpu_train_steps_on_single_shard
    inputs=[0, _INITIAL_LOSS])
  File ""/site-packages/tensorflow_core/python/tpu/training_loop.py"", line 178, in while_loop
    condition_wrapper, body_wrapper, inputs, name="""", parallel_iterations=1)
  File ""/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 2753, in while_loop
    return_same_structure)
  File ""/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 2245, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 2170, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/site-packages/tensorflow_core/python/tpu/training_loop.py"", line 121, in body_wrapper
    outputs = body(*(inputs + dequeue_ops))
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3588, in <lambda>
    lambda i, loss: [i + 1, single_tpu_train_step(i)],
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1715, in train_step
    self._call_model_fn(features, labels))
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1994, in _call_model_fn
    estimator_spec = self._model_fn(features=features, **kwargs)
  File ""/site-packages/mesh_tensorflow/transformer/utils.py"", line 672, in my_model_fn
     log_file=model_info_file)                                                                                                                                                                             
  File ""/site-packages/mesh_tensorflow/ops.py"", line 726, in __init__                                                                                                                                     
    op.lower(self)
  File ""/site-packages/mesh_tensorflow/ops.py"", line 4038, in lower
    sv = mesh_impl.LaidOutVariable(self, mesh_impl)
  File ""/site-packages/mesh_tensorflow/simd_mesh_impl.py"", line 212, in __init__
    variable.get_master(), shape, slices, slice_shape)
  File ""/site-packages/mesh_tensorflow/simd_mesh_impl.py"", line 250, in _gen_copy_master_to_slices_op
    slice_begin, slice_shape)
  File ""/site-packages/tensorflow_core/python/ops/array_ops.py"", line 855, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""/site-packages/tensorflow_core/python/ops/gen_array_ops.py"", line 9222, in _slice
    ""Slice"", input=input, begin=begin, size=size, name=name)
  File ""/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/site-packages/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""/site-packages/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""/site-packages/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()

E1001 16:19:34.235139 139833409849152 error_handling.py:75] Error recorded from training_loop: From /job:worker/replica:0/task:0:
3 root error(s) found.
  (0) Failed precondition: The TPU system has not been initialized.
         [[node shared/embedding_slot_vr_1/Slice_24 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]
  (1) Failed precondition: The TPU system has not been initialized.
         [[node shared/embedding_slot_vr_1/Slice_25 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]
  (2) Failed precondition: The TPU system has not been initialized.
         [[node shared/embedding_slot_vr_1/Slice_26 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]
0 successful operations.
6 derived errors ignored.

Original stack trace for 'shared/embedding_slot_vr_1/Slice_24':
  File ""/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/site-packages/t5/models/mesh_transformer_main.py"", line 244, in <module>
    console_entry_point()
  File ""/site-packages/t5/models/mesh_transformer_main.py"", line 241, in console_entry_point
    app.run(main)
  File ""/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/site-packages/t5/models/mesh_transformer_main.py"", line 235, in main
    model_dir=FLAGS.model_dir)
  File ""/site-packages/gin/config.py"", line 1055, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/site-packages/mesh_tensorflow/transformer/utils.py"", line 2115, in run
    train_dataset_fn, train_steps, ensemble_inputs)
  File ""/site-packages/mesh_tensorflow/transformer/utils.py"", line 1498, in train_model
    estimator.train(input_fn=input_fn, max_steps=train_steps)
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3030, in train
  File ""/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 370, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1161, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1191, in _train_model_default
    features, labels, ModeKeys.TRAIN, self.config)
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2857, in _call_model_fn
    config)
  File ""/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1149, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3159, in _model_fn
    _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3604, in _train_on_tpu_system
    device_assignment=ctx.device_assignment)
  File ""/site-packages/tensorflow_core/python/tpu/tpu.py"", line 1277, in split_compile_and_shard
    name=name)
  File ""/site-packages/tensorflow_core/python/tpu/tpu.py"", line 992, in split_compile_and_replicate
    outputs = computation(*computation_inputs)
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3589, in multi_tpu_train_steps_on_single_shard
    inputs=[0, _INITIAL_LOSS])
  File ""/site-packages/tensorflow_core/python/tpu/training_loop.py"", line 178, in while_loop
    condition_wrapper, body_wrapper, inputs, name="""", parallel_iterations=1)
  File ""/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 2753, in while_loop
    return_same_structure)
  File ""/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 2245, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 2170, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/site-packages/tensorflow_core/python/tpu/training_loop.py"", line 121, in body_wrapper
    outputs = body(*(inputs + dequeue_ops))
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3588, in <lambda>
    lambda i, loss: [i + 1, single_tpu_train_step(i)],
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1715, in train_step
    self._call_model_fn(features, labels))
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1994, in _call_model_fn
    estimator_spec = self._model_fn(features=features, **kwargs)
  File ""/site-packages/mesh_tensorflow/transformer/utils.py"", line 672, in my_model_fn
    log_file=model_info_file)
  File ""/site-packages/mesh_tensorflow/ops.py"", line 726, in __init__
    op.lower(self)
  File ""/site-packages/mesh_tensorflow/ops.py"", line 4038, in lower
    sv = mesh_impl.LaidOutVariable(self, mesh_impl)
  File ""/site-packages/mesh_tensorflow/simd_mesh_impl.py"", line 212, in __init__
    variable.get_master(), shape, slices, slice_shape)
  File ""/site-packages/mesh_tensorflow/simd_mesh_impl.py"", line 250, in _gen_copy_master_to_slices_op
    slice_begin, slice_shape)
  File ""/site-packages/tensorflow_core/python/ops/array_ops.py"", line 855, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""/site-packages/tensorflow_core/python/ops/gen_array_ops.py"", line 9222, in _slice
    ""Slice"", input=input, begin=begin, size=size, name=name)
  File ""/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/site-packages/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""/site-packages/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""/site-packages/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()

INFO:tensorflow:training_loop marked as finished
I1001 16:19:34.237687 139833409849152 error_handling.py:101] training_loop marked as finished
WARNING:tensorflow:Reraising captured error
W1001 16:19:34.237852 139833409849152 error_handling.py:135] Reraising captured error
Traceback (most recent call last):
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/t5/models/mesh_transformer_main.py"", line 244, in <module>
    console_entry_point()
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/t5/models/mesh_transformer_main.py"", line 241, in console_entry_point
    app.run(main)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/t5/models/mesh_transformer_main.py"", line 235, in main
    model_dir=FLAGS.model_dir)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/gin/config.py"", line 1078, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/gin/utils.py"", line 49, in augment_exception_message_and_reraise
    six.raise_from(proxy.with_traceback(exception.__traceback__), None)
  File ""<string>"", line 3, in raise_from
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/gin/config.py"", line 1055, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/mesh_tensorflow/transformer/utils.py"", line 2115, in run
    train_dataset_fn, train_steps, ensemble_inputs)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/mesh_tensorflow/transformer/utils.py"", line 1498, in train_model
    estimator.train(input_fn=input_fn, max_steps=train_steps)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3035, in train
    rendezvous.raise_errors()
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 136, in raise_errors
    six.reraise(typ, value, traceback)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/six.py"", line 703, in reraise
    raise value
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3030, in train
    saving_listeners=saving_listeners)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 370, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1161, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1195, in _train_model_default
    saving_listeners)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1494, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py"", line 754, in run
    run_metadata=run_metadata)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py"", line 1254, in run
    self._sess = self._create_session()
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py"", line 1212, in _create_session
    return self._sess_creator.create_session()
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py"", line 885, in create_session
    hook.after_create_session(self.tf_sess, self.coord)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/mesh_tensorflow/ops.py"", line 5768, in after_create_session
    session.run(self._op)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 956, in run                                                                          
    run_metadata_ptr)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:worker/replica:0/task:0:
3 root error(s) found.
  (0) Failed precondition: The TPU system has not been initialized.
         [[node shared/embedding_slot_vr_1/Slice_24 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]
  (1) Failed precondition: The TPU system has not been initialized.
         [[node shared/embedding_slot_vr_1/Slice_25 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]
  (2) Failed precondition: The TPU system has not been initialized.
         [[node shared/embedding_slot_vr_1/Slice_26 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]
0 successful operations.
6 derived errors ignored.

Original stack trace for 'shared/embedding_slot_vr_1/Slice_24':
  File ""/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/site-packages/t5/models/mesh_transformer_main.py"", line 244, in <module>
    console_entry_point()
  File ""/site-packages/t5/models/mesh_transformer_main.py"", line 241, in console_entry_point
    app.run(main)
  File ""/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/site-packages/t5/models/mesh_transformer_main.py"", line 235, in main
    model_dir=FLAGS.model_dir)
  File ""/site-packages/gin/config.py"", line 1055, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/site-packages/mesh_tensorflow/transformer/utils.py"", line 2115, in run
    train_dataset_fn, train_steps, ensemble_inputs)
  File ""/site-packages/mesh_tensorflow/transformer/utils.py"", line 1498, in train_model
    estimator.train(input_fn=input_fn, max_steps=train_steps)
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3030, in train
    saving_listeners=saving_listeners)
  File ""/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 370, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1161, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1191, in _train_model_default
    features, labels, ModeKeys.TRAIN, self.config)
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2857, in _call_model_fn
    config)
  File ""/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1149, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3159, in _model_fn
      _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3604, in _train_on_tpu_system
    device_assignment=ctx.device_assignment)
  File ""/site-packages/tensorflow_core/python/tpu/tpu.py"", line 1277, in split_compile_and_shard
    name=name)
  File ""/site-packages/tensorflow_core/python/tpu/tpu.py"", line 992, in split_compile_and_replicate
    outputs = computation(*computation_inputs)
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3589, in multi_tpu_train_steps_on_single_shard
    inputs=[0, _INITIAL_LOSS])
  File ""/site-packages/tensorflow_core/python/tpu/training_loop.py"", line 178, in while_loop
    condition_wrapper, body_wrapper, inputs, name="""", parallel_iterations=1)
  File ""/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 2753, in while_loop
    return_same_structure)
  File ""/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 2245, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 2170, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/site-packages/tensorflow_core/python/tpu/training_loop.py"", line 121, in body_wrapper
    outputs = body(*(inputs + dequeue_ops))
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3588, in <lambda>
    lambda i, loss: [i + 1, single_tpu_train_step(i)],
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1715, in train_step
    self._call_model_fn(features, labels))
  File ""/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1994, in _call_model_fn
    estimator_spec = self._model_fn(features=features, **kwargs)
  File ""/site-packages/mesh_tensorflow/transformer/utils.py"", line 672, in my_model_fn
    log_file=model_info_file)
  File ""/site-packages/mesh_tensorflow/ops.py"", line 726, in __init__
    op.lower(self)
  File ""/site-packages/mesh_tensorflow/ops.py"", line 4038, in lower
    sv = mesh_impl.LaidOutVariable(self, mesh_impl)
  File ""/site-packages/mesh_tensorflow/simd_mesh_impl.py"", line 212, in __init__
    variable.get_master(), shape, slices, slice_shape)
  File ""/site-packages/mesh_tensorflow/simd_mesh_impl.py"", line 250, in _gen_copy_master_to_slices_op
    slice_begin, slice_shape)
  File ""/site-packages/tensorflow_core/python/ops/array_ops.py"", line 855, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""/site-packages/tensorflow_core/python/ops/gen_array_ops.py"", line 9222, in _slice
    ""Slice"", input=input, begin=begin, size=size, name=name)
  File ""/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/site-packages/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""/site-packages/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""/site-packages/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()

  In call to configurable 'run' (<function run at 0x7f2d3bc6df80>)
```

Any idea how I could fix this issue ?
"
43706,ParseTensor (tf.io.parse_tensor) is not vectorized - Vectorizing via tf.vectorized_map uses while_loop,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from: Binary
- TensorFlow version: v2.3.0-rc2-23-gb36436b087 **2.3.0**
- Python version: 3.8
- CUDA/cuDNN version: 10.1
- GPU model and memory: TITAN V, 12G VRAM

**Describe the current behavior**
See the code below. I'm serializing a list of tensors and then attempt to parse them using (1) naive, single-record parsing and (2) batch-parsing using vectorized_map, which I expect to yield a significant performance increase. 

But: `tf.io.parse_tensor` appear to be not implemented for vectorized parsing, as I'm getting a `WARNING:tensorflow:Using a while_loop for converting ParseTensor` message and see little to no performance increase!

I find it very surprising that such an essential operation is not vectorized.. how else would I parse no-scalar features from e.g. a TFRecord file? Meanwhile, `tf.io.parse_example` is vectorized. 

**Describe the expected behavior**
I would expect that using a vectorized version of `tf.io.parse_tensor` to yield significant performance increase. 

**Standalone code to reproduce the issue**

```
import numpy as np
import tensorflow as tf

import time

# This would normaly come from some data stream, e.g. stream of TFRecords
some_tensor_list = [np.zeros(shape=(5,5), dtype=np.int32)]*100000
some_tensor_list_serialized = [tf.io.serialize_tensor(x) for x in some_tensor_list]

# Feed to tf.data
dataset = tf.data.Dataset.from_tensor_slices(some_tensor_list_serialized)

# Parsing whole batch back to tensors
def parse_batch(b):
    return tf.vectorized_map(lambda x: tf.io.parse_tensor(x, out_type = tf.int32), b)

# Parsing single record back to tensors
def parse_single(x):
    return tf.io.parse_tensor(x, out_type = tf.int32)

# Compare speed
def exaust_iterable(it):
    t = time.time()
    for _ in it:
        pass
    print(f'{time.time() - t}s')

# naive
dataset_naive = dataset.map(parse_single)

exaust_iterable(dataset_naive)

# vectorized over batch
dataset_vec = dataset.batch(32)
dataset_vec = dataset_vec.map(parse_batch)

exaust_iterable(dataset_vec)
```"
43705,Project Gutenberg: make printing on TPUs easy!,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.2
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
I would like to easily print the value of a tensor within a function decorated by `@tf.function`, on TPUs. It is currently only possible through various unpleasant, time-consuming, and error-prone hacks. 
**Will this change the current api? How?**
I don't think so, `tf.print` should cover it.
**Who will benefit with this feature?**
Anyone who uses TensorFlow on TPUs.
**Any Other info.** 
"
43703,"For a dataset of nested elements `window` creates a dataset of nested datasets of flat elements, not a dataset of datasets of nested elements.","For a dataset of nested elements `window` creates a dataset of nested datasets of flat elements, not a dataset of datasets of nested elements.

In other words, the `x` in your `lambda` will be a dictionary mapping keys to datasets and if you wish to flatten it to a single dataset you will need to do:

```
def map_fn(x):
  result = {}
  for key in x.keys():
    result[key] = tf.data.experimental.get_single_element(x[key].batch(3))
  return result

dataset = tf.data.Dataset.from_tensor_slices({""a"": [1,2,3,4]}).window(3).map(map_fn)
```

_Originally posted by @jsimsa in https://github.com/tensorflow/tensorflow/issues/23581#issuecomment-529702702_

When running the above, I am able to get a Dataset of dictionnaries of tensors of shape (window_size,).
However, it seems that batching futher such object then triggers an 'out or range' error in model training, and even when going through the elements. Such error seem to occur only when number of windows are not amultiple of the final batch size..

```

import numpy as np
import pandas as pd
import tensorflow as tf

window_size = 30

def map_fn(x):
  result = {}
  for key in x.keys():
    result[key] = tf.data.experimental.get_single_element(x[key].batch(window_size))
  return result

data = pd.DataFrame(
    np.random.random_sample((200,5)),
    columns=['feature1', 'feature2', 'feature3', 'feature4', 'feature5'],
)

dataset = tf.data.Dataset.from_tensor_slices(dict(data))
dataset = dataset.window(window_size, drop_remainder=True)
dataset = dataset.map(map_fn)

for elem in dataset:
    print(elem)

dataset = dataset.batch(32)

for elem in dataset:
    print(elem)
```
"
43702,Memory leaks in repeated model training despite garbage collection and session clearing,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, see below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows and Linux
- TensorFlow installed from (source or binary): `pip`
- TensorFlow version (use command below): `v2.3.0-54-gfcc4b966f1 2.3.1`
- Python version: `3.8.5`
- CUDA/cuDNN version: none
- GPU model and memory: none

**Describe the current behavior**
The code below leaks memory.

From my local, CPU-only system:
<details>

```
iteration 0: rss 368 MB
iteration 1: rss 400 MB  # new maximum
iteration 2: rss 402 MB  # new maximum
iteration 3: rss 439 MB  # new maximum
iteration 4: rss 431 MB
iteration 5: rss 444 MB  # new maximum
iteration 6: rss 442 MB
iteration 7: rss 447 MB  # new maximum
iteration 8: rss 445 MB
iteration 9: rss 461 MB
iteration 10: rss 449 MB  # new maximum
iteration 11: rss 462 MB  # new maximum
iteration 12: rss 460 MB
iteration 13: rss 466 MB  # new maximum
iteration 14: rss 463 MB
iteration 15: rss 483 MB  # new maximum
iteration 16: rss 468 MB
iteration 17: rss 490 MB  # new maximum
iteration 18: rss 476 MB
iteration 19: rss 489 MB
iteration 20: rss 478 MB
iteration 21: rss 491 MB  # new maximum
iteration 22: rss 480 MB
iteration 23: rss 492 MB  # new maximum
iteration 24: rss 480 MB
iteration 25: rss 497 MB  # new maximum
iteration 26: rss 481 MB
iteration 27: rss 493 MB
iteration 28: rss 483 MB
iteration 29: rss 493 MB
iteration 30: rss 486 MB
iteration 31: rss 488 MB
iteration 32: rss 487 MB
iteration 33: rss 489 MB
iteration 34: rss 498 MB  # new maximum
iteration 35: rss 487 MB
iteration 36: rss 494 MB
iteration 37: rss 493 MB
iteration 38: rss 489 MB
iteration 39: rss 500 MB  # new maximum
iteration 40: rss 487 MB
iteration 41: rss 501 MB  # new maximum
iteration 42: rss 494 MB
iteration 43: rss 493 MB
iteration 44: rss 502 MB  # new maximum
iteration 45: rss 510 MB  # new maximum
iteration 46: rss 503 MB
iteration 47: rss 497 MB
iteration 48: rss 494 MB
iteration 49: rss 507 MB
iteration 50: rss 497 MB
iteration 51: rss 507 MB
iteration 52: rss 499 MB
iteration 53: rss 499 MB
iteration 54: rss 507 MB
iteration 55: rss 497 MB
iteration 56: rss 507 MB
iteration 57: rss 500 MB
iteration 58: rss 501 MB
iteration 59: rss 508 MB
iteration 60: rss 498 MB
iteration 61: rss 507 MB
iteration 62: rss 501 MB
iteration 63: rss 502 MB
iteration 64: rss 510 MB
iteration 65: rss 501 MB
iteration 66: rss 511 MB  # new maximum
iteration 67: rss 503 MB
iteration 68: rss 502 MB
iteration 69: rss 512 MB
iteration 70: rss 500 MB
iteration 71: rss 512 MB  # new maximum
iteration 72: rss 505 MB
iteration 73: rss 506 MB
iteration 74: rss 512 MB
iteration 75: rss 522 MB  # new maximum
iteration 76: rss 509 MB
iteration 77: rss 507 MB
iteration 78: rss 507 MB
iteration 79: rss 514 MB
iteration 80: rss 525 MB  # new maximum
iteration 81: rss 514 MB
iteration 82: rss 508 MB
iteration 83: rss 507 MB
iteration 84: rss 516 MB
iteration 85: rss 527 MB  # new maximum
iteration 86: rss 513 MB
iteration 87: rss 511 MB
iteration 88: rss 509 MB
iteration 89: rss 516 MB
iteration 90: rss 508 MB
iteration 91: rss 511 MB
iteration 92: rss 514 MB
iteration 93: rss 509 MB
iteration 94: rss 518 MB
iteration 95: rss 508 MB
iteration 96: rss 513 MB
iteration 97: rss 514 MB
iteration 98: rss 509 MB
iteration 99: rss 519 MB
```

</details>


From Colab:
<details>

```
iteration 0: rss 547 MB
iteration 1: rss 652 MB  # new maximum
iteration 2: rss 671 MB  # new maximum
iteration 3: rss 674 MB  # new maximum
iteration 4: rss 674 MB
iteration 5: rss 674 MB
iteration 6: rss 674 MB
iteration 7: rss 675 MB  # new maximum
iteration 8: rss 680 MB  # new maximum
iteration 9: rss 680 MB
iteration 10: rss 689 MB  # new maximum
iteration 11: rss 689 MB
iteration 12: rss 689 MB
iteration 13: rss 689 MB
iteration 13: rss 689 MB
iteration 14: rss 689 MB
iteration 15: rss 689 MB
iteration 16: rss 689 MB
iteration 17: rss 689 MB
iteration 18: rss 689 MB
iteration 19: rss 689 MB
iteration 20: rss 689 MB
iteration 21: rss 689 MB
iteration 22: rss 689 MB
iteration 23: rss 689 MB
iteration 24: rss 689 MB
iteration 25: rss 689 MB
iteration 26: rss 689 MB
iteration 27: rss 689 MB
iteration 28: rss 689 MB
iteration 29: rss 689 MB
iteration 30: rss 689 MB
iteration 31: rss 689 MB
iteration 32: rss 689 MB
iteration 33: rss 689 MB
iteration 34: rss 689 MB
iteration 35: rss 691 MB  # new maximum
iteration 36: rss 691 MB
iteration 37: rss 697 MB  # new maximum
iteration 38: rss 702 MB  # new maximum
iteration 39: rss 704 MB  # new maximum
iteration 40: rss 704 MB
iteration 41: rss 704 MB
iteration 42: rss 704 MB
iteration 43: rss 704 MB
iteration 44: rss 704 MB
iteration 45: rss 704 MB
iteration 46: rss 704 MB
iteration 47: rss 704 MB
iteration 48: rss 704 MB
iteration 49: rss 704 MB
iteration 50: rss 704 MB
iteration 51: rss 704 MB
iteration 52: rss 704 MB
iteration 53: rss 704 MB
iteration 54: rss 704 MB
iteration 55: rss 704 MB
iteration 56: rss 704 MB
iteration 57: rss 704 MB
iteration 58: rss 704 MB
iteration 59: rss 704 MB
iteration 60: rss 704 MB
iteration 61: rss 704 MB
iteration 62: rss 704 MB
iteration 63: rss 704 MB
iteration 64: rss 704 MB
iteration 65: rss 704 MB
iteration 66: rss 704 MB
iteration 67: rss 704 MB
iteration 68: rss 704 MB
iteration 69: rss 704 MB
iteration 70: rss 704 MB
iteration 71: rss 704 MB
iteration 72: rss 704 MB
iteration 73: rss 704 MB
iteration 74: rss 704 MB
iteration 75: rss 704 MB
iteration 76: rss 705 MB  # new maximum
iteration 77: rss 705 MB
iteration 78: rss 705 MB
iteration 79: rss 705 MB
iteration 80: rss 705 MB
iteration 81: rss 705 MB
iteration 82: rss 706 MB  # new maximum
iteration 83: rss 706 MB
iteration 84: rss 713 MB  # new maximum
iteration 85: rss 713 MB
iteration 86: rss 713 MB
iteration 87: rss 713 MB
iteration 88: rss 719 MB  # new maximum
iteration 89: rss 719 MB
iteration 90: rss 719 MB
iteration 91: rss 719 MB
iteration 92: rss 719 MB
iteration 93: rss 719 MB
iteration 94: rss 719 MB
iteration 95: rss 719 MB
iteration 96: rss 719 MB
iteration 97: rss 720 MB  # new maximum
iteration 98: rss 720 MB
iteration 99: rss 720 MB
```

</details>

**Describe the expected behavior**
Memory consumptions stays constant


**Standalone code to reproduce the issue**
```python
import gc
import os

import numpy as np
import psutil
import tensorflow as tf

tf.get_logger().setLevel(""ERROR"")  # Suppress ""tf.function retracing"" warnings
process = psutil.Process(os.getpid())
for i in range(100):
    # do some work
    model = tf.keras.applications.mobilenet.MobileNet()
    model.compile(loss=""mse"")
    x = tf.zeros((1, *model.input.shape[1:]))
    y = tf.zeros((1, *model.output.shape[1:]))
    history = model.fit(x=x, y=y, verbose=0)
    
    # clean up
    _ = gc.collect()
    tf.keras.backend.clear_session()
    
    # show memory usage
    print(f""iteration {i}: rss {process.memory_info().rss >> 20} MB"")

```

**Other info / logs**
- See also https://stackoverflow.com/q/63411142/
"
43701,RTX3080 install RuntimeError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid,"**System information**
ubuntu18.04.5 LTS
GeForce RTX 3080 
NVIDIA driver 455.23.04 + CUDA10.1 + Cudnn7.6.5
Anaconda3
envs: tensorflow-gpu==2.3.1
**Describe the problem**
```
import tensorflow as tf
tf.test.is_gpu_available()
```

>...
 2020-10-01 17:18:08.728031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2020-10-01 17:18:08.728126: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/librazxc/.conda/envs/tf23/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""/home/librazxc/.conda/envs/tf23/lib/python3.7/site-packages/tensorflow/python/framework/test_util.py"", line 1563, in is_gpu_available
    for local_device in device_lib.list_local_devices():
  File ""/home/librazxc/.conda/envs/tf23/lib/python3.7/site-packages/tensorflow/python/client/device_lib.py"", line 43, in list_local_devices
    _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)
RuntimeError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid

**Any other info / logs**
tensorflow-gpu==2.2 or 2.1 is work well
"
43700, The issue about unexpected inference result (output class and probability-value) through quantized tflite model in more than a certain number of classes,"**System information**
- Model training (VGG16) environment
    - OS Platform and Distribution
        - Windows 10
    - GPU
        - NVIDIA GTX1080
    - Cuda Toolkit 10.0.130
    - Python 3.7
    - Keras 2.2.4
    - TensorFlow 1.14.0

- Model training (MobileNetV2) environment    
    - OS Platform and Distribution
        - Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-99-generic x86_64)
    - GPU
        - NVIDIA Quadro P5000
    - Cuda Toolkit 10.1.243
    - Python 3.7
    - Keras 2.2.4
    - TensorFlow 1.14.0

- Model convert environment
    - OS Platform and Distribution
        - Mac OS X 10.14.5 (18F132)
    - Python 3.6.7
    - Keras 2.2.4
    - TensorFlow 1.14.0

- Target mobile device
    - Pixel 3 XL
        - Android 9 (build PQ3A.1980801.002)

**Trained model information**
- VGG16 and MobileNetV2 model (transfer learning)
- The number of classes: **10,996**
    - This model is for classification of  the huge classes of the images of a certain category.

**Abstract of the issue**
- In case did the tflite conversion with enable quantization option for this model, the output class (1st rank) for the input test data(about 1000-2000 jpg images files) to 10,996 class model was completely different all from the class of input test data, and I got the unexpected output (the output class did not match in all test data and the probability-value in 1st rank is about 0.4-0.5) compared with the output for 10,806 class model.
- But in disabled quantization option, the output class matched the class of the input test data, the probability-value in 1st rank is 0.9 and more, and the classification accuracy for the same test data was 90% and more.
- **This issue did not occur in the model of less 10,806 classes**
    - I'm investigating if this issue occurs between 10,806 and 10,996 classes.
- This issue occurred on mobile device (Pixel 3 XL) but similarly reproduced on Mac too.
- The method of the quantization is dynamic range quantization.

**Output example**
- Classes
    - [ a_0, a_1, ... , a_10995 ]
- Input test data class (expected output class)
    - a_1234
- Output class and probability-values **(Quantized)**
    ```
    a_4952 0.49650624
    a_5831 0.044097286
    a_6422 0.032513995
    ...
    ```
-  Output class and probability-values **(Not-quantized)**
    ```
    a_1234 0.92802435
    a_1235 0.004674452
    a_13731 0.0012423205
    ...
    ```

**Question**
- Is there something that is considered to be the cause of this issue?
- In case a model has the huge number of classes, in internal calculation of a model, overflow or zero-division occurred by applying dynamic range quantization to a trained model, and as a result, is there a possibility that the output result is something wrong?

- A similar issue occurred in CoreML:
    - In CoreML, when a model has the model more than 9,000 classes, this resulted in overflow with Neural Engine mode and GPU mode, but I could avoid this issue with enable CPU mode. 
- In Android, I couldn't avoid this issue with the same method.

**Source code for model conversion**
```python
import os
import tensorflow as tf
from keras.models import load_model, model_from_json
from keras.optimizers import SGD

def convert(input_path, 
            weights_file,
            output_dir, 
            output_filename, 
            initial_lr=2e-4, 
            momentum=0.9, 
            is_quantized=False):

    model = model_from_json(open(os.path.join(input_path, 'model', 'model.json')).read())
    model.load_weights(os.path.join(input_path, 'model', weights_file))
    model.summary()

    opt = SGD(initial_lr, momentum=momentum)
    model.compile(
        optimizer=opt,
        loss='categorical_crossentropy'
    )

    output_modeldir = os.path.split(input_path)[-1]
    output_dir = os.path.join(output_dir, 'models', output_modeldir)

    os.makedirs(output_dir, exist_ok=True)

    keras_file = os.path.join(output_dir, 'weights_temp.h5')
    model_ = model.save(keras_file)

    converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_file)

    if is_quantized:
        converter.post_training_quantize=True
        output_filename = f'{output_filename}_quantization.tflite'
    else:
        output_filename = f'{output_filename}.tflite'

    tflite_model = converter.convert()

    tflite_file = os.path.join(output_dir, f'{output_filename}')
    with open(tflite_file, 'wb') as f:
        f.write(tflite_model)

    print(f'Saved [{tflite_file}]')
```

**Output logs**

```
Using TensorFlow backend.
WARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

WARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
2020-10-01 11:34:43.751852: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-01 11:34:43.752602: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 6. Tune using inter_op_parallelism_threads for best performance.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
image_input (InputLayer)     (None, 128, 128, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0
_________________________________________________________________
gap_vgg16gap (GlobalAverageP (None, 512)               0
_________________________________________________________________
fc1_vgg16gap (Dense)         (None, 4096)              2097152
_________________________________________________________________
bn1_vgg16gap (BatchNormaliza (None, 4096)              16384
_________________________________________________________________
relu1_vgg16gap (Activation)  (None, 4096)              0
_________________________________________________________________
do1_vgg16gap (Dropout)       (None, 4096)              0
_________________________________________________________________
fc2_vgg16gap (Dense)         (None, 4096)              16777216
_________________________________________________________________
bn2_vgg16gap (BatchNormaliza (None, 4096)              16384
_________________________________________________________________
relu2_vgg16gap (Activation)  (None, 4096)              0
_________________________________________________________________
do2_vgg16gap (Dropout)       (None, 4096)              0
_________________________________________________________________
predictions_vgg16gap (Dense) (None, 10996)             45050612
=================================================================
Total params: 78,672,436
Trainable params: 71,020,788
Non-trainable params: 7,651,648
_________________________________________________________________
WARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:507: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.
Instructions for updating:
`normal` is a deprecated alias for `truncated_normal`
2020-10-01 11:34:49.269923: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-10-01 11:34:49.270019: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2020-10-01 11:34:49.759721: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2020-10-01 11:34:49.759743: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.007ms.
2020-10-01 11:34:49.759750: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.004ms.
WARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/lite/python/util.py:238: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/lite/python/lite.py:769: UserWarning: Property post_training_quantize is deprecated, please use optimizations=[Optimize.DEFAULT] instead.
  "" instead."" % name)
2020-10-01 11:34:50.957910: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-10-01 11:34:50.958019: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2020-10-01 11:34:51.559693: E tensorflow/core/grappler/grappler_item_builder.cc:637] Init node block1_conv1/kernel/Assign doesn't exist in graph
Saved [./weights_quantization.tflite]
```

"
43699,Multiple instances of custom metrics don't behave as expected with multiple output model when evaluating.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**
- OS Platform and Distribution: **Ubuntu 20.04.1 LTS**
- TensorFlow installed from (source or binary): `pip install`
- TensorFlow version (use command below): `tensorflow==2.3.0`
- Python version: **Python 3.6.9 :: Anaconda, Inc.**

**Current behavior**
Custom metrics behave unexpectedly when applied on a model / dataset with multpiple outputs.
Below is a snippet to reproduce.

**Describe the expected behavior**
Results should be consistent whatever the number of outputs.

**Standalone code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf


def build_dataset(n_outputs):
    """"""
    Build a random dataset.
    Target is a dictionnary of `n_output` elements with keys as [""out0"", ""out1"", ...]
    """"""
    input_data = np.random.normal(size=(1024, 5)).astype(""float32"")
    target_data = {
        f""out{n}"": np.random.normal(size=(1024,)).astype(""float32"")
        for n in range(n_outputs)
    }
    input_dataset = tf.data.Dataset.from_tensor_slices(input_data)
    target_dataset = tf.data.Dataset.from_tensor_slices(target_data)
    dataset = tf.data.Dataset.zip((input_dataset, target_dataset)).batch(16)
    return dataset


class Model(tf.keras.Model):
    """"""Model with `n_outputs` outputs""""""
    def __init__(self, n_outputs):
        tf.keras.Model.__init__(self)
        self.hidden = tf.keras.layers.Dense(16)
        self.out = {f""out{n}"": tf.keras.layers.Dense(1) for n in range(n_outputs)}

    def call(self, inputs):
        hidden = self.hidden(inputs)
        return {out_name: out_layer(hidden) for out_name, out_layer in self.out.items()}


class DumbMetric(tf.keras.metrics.Metric):
    """"""This metrics takes a param at __init__ time and then always return param as result""""""
    def __init__(self, name=""Dumb"", param=0., **kwargs):
        tf.keras.metrics.Metric.__init__(self, name=name, **kwargs)
        self.param = float(param)

    def update_state(self, y_true, y_pred, sample_weight):
        pass

    def result(self):
        return self.param


for n in (1, 2):
    # For 1 output, metrics behave as expected:
    #   - metric1 returns 1
    #   - metric2 returns 2
    #
    # For 2 outputs, metrics don't behave as expected:
    #   - metric1 returns 0 (default `param` value)
    #   - metric2 returns 0 (default `param` value)

    model = Model(n)
    model.compile(
        metrics=[
            DumbMetric(name=""metric1"", param=1.),
            DumbMetric(name=""metric2"", param=2.)
        ]
    )
    metrics = model.evaluate(build_dataset(n), return_dict=True, verbose=0)
    print(f""For {n} output(s): "", metrics)
```

**Actual output:**
```
For 1 output(s):  {'loss': 0.0, 'metric1': 1.0, 'metric2': 2.0}
For 2 output(s):  {'loss': 0.0, 'out0_metric1': 0.0, 'out0_metric2': 0.0, 'out1_metric1': 0.0, 'out1_metric2': 0.0}
```

**Excpected output:**
```
For 1 output(s):  {'loss': 0.0, 'metric1': 1.0, 'metric2': 2.0}
For 2 output(s):  {'loss': 0.0, 'out0_metric1': 1.0, 'out0_metric2': 2.0, 'out1_metric1': 1.0, 'out1_metric2': 2.0}
```"
43698,TF2 TensorArray with multi-dimensional tensor wrote in it will be stacked into None shape tensor in Autograph mode,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux lz 5.4.0-48-generic #52~18.04.1-Ubuntu`
- TensorFlow installed from (source or binary):  `(in conda env) pip install tensorflow-gpu -i https://pypi.tuna.tsinghua.edu.cn/simple`
- TensorFlow version (use command below): `v2.3.0-rc2-23-gb36436b087 2.3.0`
- Python version: `3.8`
- CUDA/cuDNN version: `conda install cudatoolkit=10.1 cudnn=7.6.5`
- GPU model and memory:  ` 4 x Titan xp 12GB`

**Describe the current behavior**

I use `tf.TensorArray` to dynamically store the specified number of multi-dimensional Tensor, and try to convert the TensorArray into Tensor which will be fed into the next network layer. I use `@tf.function` decorator to convert pythonic code into TF's AutoGraph mode to accelerate the training process. Specifically, take the following code as an example:
``` python
import tensorflow as tf

class TestModel(tf.keras.Model):

    def __init__(self, N):
        super(TestModel, self).__init__()
        self.conv_first = tf.keras.layers.Conv2D(4, (3, 3))
        self.nframe = N

    def __call__(self, x):

        aligned_fea = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)

        def cond(i, N, fea_col):
            return i < N

        def body(i, N, fea_col):
            fea_col = fea_col.write(i, x)
            i = tf.add(i, 1)
            return i, N, fea_col

        _, _, aligned_fea = tf.while_loop(cond, body, [0, self.nframe, aligned_fea])

        tf.print(""aliged_fea shape:"", aligned_fea.size())

        t = aligned_fea.stack()
        tf.print(""t shape:"", t.shape)

        # without these two lines of reshaping the stacked tensor coercively, the t will have no first dimension
        tt = tf.reshape(t,[self.nframe, 8, 4, 6,3])
        tf.print(""tt shape:"", tt.shape)

        return t

@tf.function
def foo(tm):
    x = tf.ones([8,4,6,3], dtype=tf.float32)
    output = tm(x)

nframe = 10
tm = TestModel(nframe)
foo(tm)
```
The output of the above code is:
```
aliged_fea shape: 10
t shape: TensorShape([None, 8, 4, 6, 3])
tt shape: TensorShape([10, 8, 4, 6, 3])
```

**Describe the expected behavior**

I have tested the code in Eager mode, that is to remove the decorator `@tf.function`. Everything goes well. But error will be thrown when the Autograph mode starts, since the output tensor doesn't have got the first dimension of itself shape, and can not be feed into the next layer of the network.

Moreover, if I write scalar into TensorArray, whether I use Eager or Autograph mode, the output of the shape of Tensor will be fine with true numbers.

My problem is the TensorArray, whatever it contains scalar or multi-dimensional tensor, should automatically detect the size of itself and stack into the tensor with full shape in both Eager and Graph mode,  and the output should be as follows so to speak:
```
aliged_fea shape: 10
t shape: TensorShape([10, 8, 4, 6, 3])
tt shape: TensorShape([10, 8, 4, 6, 3])
```

One workaround to deal with this problem is just to reshape the stacked tensor after `TensorArray.stack()` but it is not robust for my codes. Thus, I wonder if this is a bug or just the feature to fit something unknown.

I am struggling with this for a long time. Appreciate your reply in advance."
43697,More details and examples for tf.image.generate_bounding_box_proposals,"
## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/image/generate_bounding_box_proposals

## Description of the issue (what needs changing):
The documentation for this function isn't clear. The meanings of the function's arguments aren't given. There are no usage examples. Seeing the code linked in the documentation doesn't make anything clearer. 

### Clear description
This function can be used to greatly simplify the making of object detection models, as it implements the proposal generation part of the code.
### Correct links

Is the link to the source code correct?
Yes

### Parameters defined

Are all parameters defined and formatted correctly?
No, the parameter definitions aren't available

### Returns defined

Are return values defined?
Returns are defined

### Raises listed and defined

Are the errors defined? For example,
No, the errors raised are not defined


### Usage example

No

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable
There are no visuals available now. However, it will be helpful to add image examples as this is a function from the image module. The bounding box proposals generated by the function can be verified easily with images. 

### Submit a pull request?
I would be happy to fix this issue, but I would need more information about the parameters used. I am unsure what they mean.

Are you planning to also submit a pull request to fix the issue See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
43696,Remove CMSIS Core include from all Makefiles,"Once PR #41860 is merged, the CMSIS include path is added in cmsis.inc. Specifying the include path in all Makefiles is no longer needed. This is a reminder to remove lines like
  INCLUDES +=  isystem$(MAKEFILE_DIR)/downloads/cmsis/CMSIS/Core/Include/ 

See discussion in the original post:

Originally posted by @advaitjain in https://github.com/tensorflow/tensorflow/pull/41860#discussion_r497863693"
43693,Add Ubuntu 20.04 installation instructions,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_101

## Description of issue (what needs changing):

Tensorflow installation instructions for Ubuntu 20.04 with GPU support is needed

### Clear description

With Ubuntu 20.04 around for 6 months, please provide installation instructions for Tensorflow with GPU support. It makes sense to have new development environments set up on Ubuntu 20.04

"
43690,"InvalidArgumentError with `Conv1D` which has the same `dilation_rate` as the batch size, using `tf.keras.Model.fit`, on TPU","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.9
- Accelerator: TPU

**Describe the current behavior**
When I optimized `Conv1D`, which has the same `dilation_rate` as the batch size, using `tf.keras.Model.fit`, I encountered the following error.

```
InvalidArgumentError: 9 root error(s) found.
  (0) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=""BatchToSpaceND"" op_name=""dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND""}. Constraint: multiple_of: 8, stride: 1
	TPU compilation failed
	 [[{{node tpu_compile_succeeded_assert/_4759666286058538557/_4}}]]
	 [[tpu_compile_succeeded_assert/_4759666286058538557/_4/_197]]
  (1) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=""BatchToSpaceND"" op_name=""dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND""}. Constraint: multiple_of: 8, stride: 1
	TPU compilation failed
	 [[{{node tpu_compile_succeeded_assert/_4759666286058538557/_4}}]]
	 [[tpu_compile_succeeded_assert/_4759666286058538557/_4/_157]]
  (2) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=""BatchToSpaceND"" op_name=""dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND""}. Constraint: multiple_of: 8, stride: 1
	TPU compilation failed
	 [[{{node tpu_compile_succeeded_assert/_4759666286058538557/_4}}]]
	 [[tpu_compile_succeeded_assert/_4759666286058538557/_4/_137]]
  (3) ...
```
This error does not occur if I forward using `strategy.run`.
A detailed error log is attached below.

**Describe the expected behavior**
I want to optimize it without errors.

**Standalone code to reproduce the issue**
Colab https://colab.research.google.com/drive/1W-eM2EwzYSMhHyN4GDUd22nKSLrrka3v?usp=sharing
gist https://gist.github.com/Hiroshiba/9c1914dbf57228204a4ed1f661168d36

**Other info / logs** Include any logs or source code that would be helpful to
<details>

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-5-5265a4b37c17> in <module>()
     13     steps_per_epoch=5,
     14     epochs=1,
---> 15     shuffle=False,
     16 )

14 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1101               logs = tmp_logs  # No error, now safe to assign to logs.
   1102               end_step = step + data_handler.step_increment
-> 1103               callbacks.on_train_batch_end(end_step, logs)
   1104         epoch_logs = copy.copy(logs)
   1105 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)
    438     """"""
    439     if self._should_call_train_batch_hooks:
--> 440       self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)
    441 
    442   def on_test_batch_begin(self, batch, logs=None):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)
    287       self._call_batch_begin_hook(mode, batch, logs)
    288     elif hook == 'end':
--> 289       self._call_batch_end_hook(mode, batch, logs)
    290     else:
    291       raise ValueError('Unrecognized hook: {}'.format(hook))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_end_hook(self, mode, batch, logs)
    307       batch_time = time.time() - self._batch_start_time
    308 
--> 309     self._call_batch_hook_helper(hook_name, batch, logs)
    310 
    311     if self._check_timing:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook_helper(self, hook_name, batch, logs)
    340       hook = getattr(callback, hook_name)
    341       if getattr(callback, '_supports_tf_logs', False):
--> 342         hook(batch, logs)
    343       else:
    344         if numpy_logs is None:  # Only convert once.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)
    959 
    960   def on_train_batch_end(self, batch, logs=None):
--> 961     self._batch_update_progbar(batch, logs)
    962 
    963   def on_test_batch_end(self, batch, logs=None):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _batch_update_progbar(self, batch, logs)
   1014     if self.verbose == 1:
   1015       # Only block async when verbose = 1.
-> 1016       logs = tf_utils.to_numpy_or_python_type(logs)
   1017       self.progbar.update(self.seen, list(logs.items()), finalize=False)
   1018 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in to_numpy_or_python_type(tensors)
    535     return t  # Don't turn ragged or sparse tensors to NumPy.
    536 
--> 537   return nest.map_structure(_to_single_numpy_or_python_type, tensors)
    538 
    539 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)
    633 
    634   return pack_sequence_as(
--> 635       structure[0], [func(*x) for x in entries],
    636       expand_composites=expand_composites)
    637 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in <listcomp>(.0)
    633 
    634   return pack_sequence_as(
--> 635       structure[0], [func(*x) for x in entries],
    636       expand_composites=expand_composites)
    637 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in _to_single_numpy_or_python_type(t)
    531   def _to_single_numpy_or_python_type(t):
    532     if isinstance(t, ops.Tensor):
--> 533       x = t.numpy()
    534       return x.item() if np.ndim(x) == 0 else x
    535     return t  # Don't turn ragged or sparse tensors to NumPy.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self)
   1061     """"""
   1062     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.
-> 1063     maybe_arr = self._numpy()  # pylint: disable=protected-access
   1064     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr
   1065 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)
   1029       return self._numpy_internal()
   1030     except core._NotOkStatusException as e:  # pylint: disable=protected-access
-> 1031       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
   1032 
   1033   @property

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: 9 root error(s) found.
  (0) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=""BatchToSpaceND"" op_name=""dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND""}. Constraint: multiple_of: 8, stride: 1
	TPU compilation failed
	 [[{{node tpu_compile_succeeded_assert/_4759666286058538557/_4}}]]
	 [[tpu_compile_succeeded_assert/_4759666286058538557/_4/_197]]
  (1) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=""BatchToSpaceND"" op_name=""dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND""}. Constraint: multiple_of: 8, stride: 1
	TPU compilation failed
	 [[{{node tpu_compile_succeeded_assert/_4759666286058538557/_4}}]]
	 [[tpu_compile_succeeded_assert/_4759666286058538557/_4/_157]]
  (2) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=""BatchToSpaceND"" op_name=""dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND""}. Constraint: multiple_of: 8, stride: 1
	TPU compilation failed
	 [[{{node tpu_compile_succeeded_assert/_4759666286058538557/_4}}]]
	 [[tpu_compile_succeeded_assert/_4759666286058538557/_4/_137]]
  (3) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=""BatchToSpaceND"" op_name=""dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND""}. Constraint: multiple_of: 8, stride: 1
	TPU compilation failed
	 [[{{node tpu_compile_succeeded_assert/_4759666286058538557/_4}}]]
	 [[tpu_compile_succeeded_assert/_4759666286058538557/_4/_187]]
  (4) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=""BatchToSpaceND"" op_name=""dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND""}. Constraint: multiple_of: 8, stride: 1
	TPU compilation failed
	 [[{{ ... [truncated]
```

</details>"
43689,No library found under: /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 
- Python version: 3.8.0
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.1
- GPU model and memory:

```bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package```

```Repository command failed
No library found under: /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1
INFO: Elapsed time: 0.344s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package```"
43688,[TransformGraph & TF2 SavedModel] - Missing API,"### Issue

When using the latest TF2 release and latest available container (`2.3.1` at the time of writing), it appears that the following API is missing:

```python
from tensorflow.tools.graph_transforms import TransformGraph  # ModuleNotFoundError: No module named 'tensorflow.tools.graph_transforms'
from tensorflow.compat.v1.tools.graph_transforms import TransformGraph  # ModuleNotFoundError: No module named 'tensorflow.compat.v1.tools'
```

Which used to be available in TF1 (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms).

Though it doesn't make much of a sense since Tensorflow 2 ship with the C++ library associated:
```bash
ls -al /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_transform_graph.so 
-rwxr-xr-x 1 root staff 1274144 Sep 29 00:25 /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_transform_graph.so
```

### Motivation

This function is very useful to pre-process and ""clean"" the graph before inference when using a SavedModel (namely for TF-TRT, could also maybe concerns TF Serving or TF Lite, I didn't check).

Any chance to have this API (or a replacement) back in the TF API ? I understand that TF2 is eager mode oriented, though the SavedModel format is a graph format ... So, would makes sense to keep this API in the mix since the SavedModel format is still actively used and supported in TF2.

CC: @sanjoy @reedwm "
43687,XLA Compilation does not work with Embeddings Layer,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): Default on colab
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.9
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10.1
- GPU model and memory: Tesla P4 (8gb)


**Describe the current behavior**
I'm using a simple model with Embeddings which works with normal `tf.function` but does not work with `tf.function(experimental_compile=True)`. I suspect it is because Embeddings are kept on the CPU while the other model layers are on the GPU, however, moving the tensors explicitly to the right device did not help (as demonstrated in the script).

**Describe the expected behavior**
The function should run without errors just like with a normal `tf.function`.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1dDjBiks335If3zqBleL_CPrNZ8CIanPP?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43686,kernel changes will break arc,"@tensorflow/micro

Once #43682 is merged, the building with TARGET=`arc` and `TAGS=arc_mli` will fail. This is mostly as a result of namespace changes.

Tagging @dzakhar  and @JaccovG.
"
43684,Kernel changes break xtensa_hifimini,"@tensorflow/micro

Once #43682 is merged, the building with `TARGET=xtensa_hifimini` and TAGS=`xtensa_hifimini` will fail. This is mostly as a result of namespace changes. "
43681,"tf.function expects to return a Tensor error, which is what I'm returning as well","```python
# @tf.function # does not work if we uncomment this line
def only_specific_companies(example: Dict[str, tf.Tensor], valid_prefixes=[""Google"", ""Apple"", ""LinkedIn"", ""Facebook""]):
    curr_company = example['company']
    example['company'] = ""Other""
    
    def company_setter_fn_gen(m):
        def set_company_fn():
            example['company'] = m
        return set_company_fn
    
    set_company_fn = None
    for m in valid_prefixes:
        if tf.strings.regex_full_match(curr_company, f""^{m}.*""):
            set_company_fn = company_setter_fn_gen(m)

    if set_company_fn:
        set_company_fn()

    tf.print(example)
    return example

only_specific_companies({""company"": tf.constant(""Google Inc"")})
```
Reproduced at https://colab.research.google.com/drive/1Sp3PSl16LUJ0NbsniWtVaJitnm7TdaBX?usp=sharing

"
43679,relaxing numpy requirement,"Numpy is fixed to `<1.19` in current main branch. Justification in [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L64) is that it was waiting for https://github.com/numpy/numpy/pull/15355 to be merged. But it seems to be merged and deployed. Is there any reason to maintain the current fixed version of numpy?
"
43677,My code is much slower in TF2 than TF1,"**System information**
- Colab


**Primary issue:** My code is much slower on TF2 compared to TF1

You can verify this by running each notebook and seeing the difference in elapsed times.

I suspect it's a **tf.data** issue, because when you increase the value of ntopics (in TF2), the code slows down even more. The TF1 code is unaffected by changing ntopics. This parameter changes the size of the simulation dataset.


**Secondary issue:** My TF1 code is slower on GPU than CPU.

You can verify this by changing the TF1 code runtime and seeing the elapsed time goes up.

This problem has not (yet) been reproduced on TF2 because of the above primary issue.

I don't know if this is possible to fix because there are a lot of sequential operations in the main calculateloss function. But if you see anything that could be optimized in any way (preferably in TF2) please point it out!


**Standalone code to reproduce the issue**
- [TF1 colab notebook](https://colab.research.google.com/drive/1YVxJ7-HMBctlI8rR6KsaQRDqYjPDSQIe?usp=sharing)

- [TF2 colab notebook](https://colab.research.google.com/drive/1-gF4IZi4wSsnAt8hFJpcVc0ALQcWAJni?usp=sharing)

Thank you"
43676,Converter fails when using a channel first model and batchnorm,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18:04
- TensorFlow installed from: binary (mini conda)
- TensorFlow version: 2.2


**Command used to run the converter or code if you’re using the Python API**
```
#  A VERY SIMPLE TWO LAYER MODEL WHICH FAILS ONLY WHEN CHANNELS FIRST

DATA_FORMAT = ""channels_first""
backend.set_image_data_format(DATA_FORMAT)
img_inputs = keras.Input(shape=(3, 384, 384))
x = layers.Conv2D(8, kernel_size=3, strides=1, padding='same', use_bias=False, data_format=DATA_FORMAT,
                  activation=None, name='conv')(img_inputs)
x = layers.BatchNormalization(axis=1, epsilon=1e-3, momentum=0.999, name='BN')(x)
model = keras.Model(inputs=img_inputs, outputs=x, name=""channel_first_test"")
model.summary()

converter = tf.lite.TFLiteConverter.from_keras_model(model)

# fails with and without these next options, (but without even just Conv2D also fails with channels first)
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
converter.allow_custom_ops=True
converter.experimental_new_converter =True

tflite_model = converter.convert()

```


**The output from the converter invocation**

```

2020-09-30 17:26:45.007381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-09-30 17:26:45.037941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.038216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-09-30 17:26:45.038315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-09-30 17:26:45.039124: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-09-30 17:26:45.039929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-09-30 17:26:45.040053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-09-30 17:26:45.040893: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-09-30 17:26:45.041375: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-09-30 17:26:45.043242: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-09-30 17:26:45.043302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.043595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.043841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-09-30 17:26:45.044038: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-09-30 17:26:45.047755: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3999980000 Hz
2020-09-30 17:26:45.048058: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557076b6a340 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-30 17:26:45.048068: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-30 17:26:45.048152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.048409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-09-30 17:26:45.048426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-09-30 17:26:45.048433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-09-30 17:26:45.048439: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-09-30 17:26:45.048445: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-09-30 17:26:45.048451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-09-30 17:26:45.048456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-09-30 17:26:45.048462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-09-30 17:26:45.048488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.048749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.048988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-09-30 17:26:45.049004: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-09-30 17:26:45.104115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 17:26:45.104128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-09-30 17:26:45.104132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-09-30 17:26:45.104216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.104484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.104733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.104974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9708 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-09-30 17:26:45.106002: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55707a4518d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-30 17:26:45.106009: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
Model: ""channel_first_test""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 3, 384, 384)]     0         
_________________________________________________________________
conv (Conv2D)                (None, 8, 384, 384)       216       
_________________________________________________________________
BN (BatchNormalization)      (None, 8, 384, 384)       32        
=================================================================
Total params: 248
Trainable params: 232
Non-trainable params: 16
_________________________________________________________________
2020-09-30 17:26:45.464048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.464203: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-09-30 17:26:45.464263: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-09-30 17:26:45.464463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.464599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-09-30 17:26:45.464618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-09-30 17:26:45.464625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-09-30 17:26:45.464631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-09-30 17:26:45.464637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-09-30 17:26:45.464643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-09-30 17:26:45.464649: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-09-30 17:26:45.464655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-09-30 17:26:45.464680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.464824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.464947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-09-30 17:26:45.464961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 17:26:45.464964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-09-30 17:26:45.464966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-09-30 17:26:45.465005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.465150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.465279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9708 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-09-30 17:26:45.466142: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize
2020-09-30 17:26:45.466150: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2020-09-30 17:26:45.466152: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-09-30 17:26:45.474700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.474845: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-09-30 17:26:45.474873: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-09-30 17:26:45.475055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.475192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-09-30 17:26:45.475208: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-09-30 17:26:45.475214: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-09-30 17:26:45.475220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-09-30 17:26:45.475225: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-09-30 17:26:45.475231: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-09-30 17:26:45.475237: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-09-30 17:26:45.475243: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-09-30 17:26:45.475267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.475409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.475530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-09-30 17:26:45.475541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 17:26:45.475544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-09-30 17:26:45.475546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-09-30 17:26:45.475583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.475725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:45.475852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9708 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-09-30 17:26:45.477438: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize
2020-09-30 17:26:45.477447: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 9 nodes (-5), 8 edges (-5), time = 0.361ms.
2020-09-30 17:26:45.477450: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 9 nodes (0), 8 edges (0), time = 0.108ms.
Traceback (most recent call last):
  File ""/home/m/PycharmProjects/test/model/tflite_test.py"", line 43, in <module>
    tflite_model = converter.convert()
  File ""/home/m/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 514, in convert
    result = _toco_convert_impl(
  File ""/home/m/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/python/convert.py"", line 491, in toco_convert_impl
    data = toco_convert_protos(
  File ""/home/mn/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/python/convert.py"", line 227, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-09-30 17:26:46.253082: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.
2020-09-30 17:26:46.253096: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.
2020-09-30 17:26:46.257083: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-09-30 17:26:46.279406: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3999980000 Hz
2020-09-30 17:26:46.279736: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560c77ff6240 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-30 17:26:46.279747: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-30 17:26:46.280153: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-09-30 17:26:46.282831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:46.282979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-09-30 17:26:46.283066: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-09-30 17:26:46.283843: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-09-30 17:26:46.284635: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-09-30 17:26:46.284757: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-09-30 17:26:46.285563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-09-30 17:26:46.286029: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-09-30 17:26:46.287805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-09-30 17:26:46.287859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:46.288029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:46.288155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-09-30 17:26:46.288174: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-09-30 17:26:46.323961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-30 17:26:46.323977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-09-30 17:26:46.323981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-09-30 17:26:46.324065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:46.324234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:46.324384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-30 17:26:46.324525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 105 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-09-30 17:26:46.325513: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560c78d5dc30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-30 17:26:46.325522: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
Fatal Python error: Segmentation fault

Current thread 0x00007f70cdd84740 (most recent call first):
  File ""/home/m/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 50 in execute
  File ""/home/m/miniconda3/envs/tf/lib/python3.8/site-packages/absl/app.py"", line 251 in _run_main
  File ""/home/m/miniconda3/envs/tf/lib/python3.8/site-packages/absl/app.py"", line 300 in run
  File ""/home/m/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/home/m/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 93 in main
  File ""/home/m/miniconda3/envs/tf/bin/toco_from_protos"", line 11 in <module>
Segmentation fault (core dumped)




Process finished with exit code 1

```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
Expect the tflite converter to work on channel-first models 


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**
Same two layer test model successfully converts when set up as _channel last_ 
Removing the batchnorm layer succesfully converts with _channel first_.
However, even just a lone conv2d layer fails with _channel first_ if I do not include the extra 'converter.' parameters

"
43674,Wrong maximum value passed in the positional encoding for the Transformer model,"## URL(s) with the issue:
https://www.tensorflow.org/tutorials/text/transformer

## Description of issue (what needs changing):
When creating positional encoding vectors set we specify maximum_position_encoding which should be equal to the maximum possible length of the sequence fed into the model. So it must be equal to MAX_LENGTH (which is 40). In your sample it's equal to the vocabulary length (or 10000).

### Clear description
When max length is so big, the difference between neighboring elements pos encoding is very subtle and it's harder for the model to grasp it. And logically the value is incorrect.

### Correct links

Not applicable.

### Parameters defined

Not applicable.

### Returns defined

Not applicable.

### Raises listed and defined

No error. Just typo in the sample code.

### Usage example

No.

### Request visuals, if applicable

No.

### Submit a pull request?

No."
43672,Issue in installing the package using pip,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10 64 bit 
- TensorFlow installed from (source or binary): pip
- TensorFlow version:
- Python version:3.8.2
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Tried to install tensorflow using pip but showed, **Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
pip install tensorflow


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
43671,Single threaded TensorFlow,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
TensorFlow should generate the specified number of threads, irrespective of the available number of CPUs or CPU cores. 
config.set_inter_op_parallelism_threads(1);
config.set_intra_op_parallelism_threads(1); is not enough to restrict the total number of generated threads.
Some related issues.
https://github.com/tensorflow/tensorflow/issues/33627
https://github.com/tensorflow/tensorflow/issues/42510

**Will this change the current api? How?**

**Who will benefit with this feature?**
People who discuss it at the following issues:
https://github.com/tensorflow/tensorflow/issues/33627
https://github.com/tensorflow/tensorflow/issues/42510

**Any Other info.**
"
43670,(venv) (base) razafimandimby@razafimandimby-Lenovo-G50-30:~$ python3 -c 'import tensorflow as tf; print(tf._version_)' Instruction non permise (core dumped),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43667,tf.keras.layers.experimental.preprocessing.CategoryEncoding: is it possible to apply it to a nD tensor?,"**System information**
- TensorFlow version (you are using): 2.4.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**: currently, tf.keras.layers.experimental.preprocessing.CategoryEncoding layers can only be called with a 2D tensor of shape (samples, ""timesteps""). It would be very useful to allow calling with a nD tensor, for example when working with sequences.. As for ""Normalization"" Layer, an ""axis"" parameter could be added to the layer parameters.
For instance, I would like to use a CategoryEncoding Layer on sequence-input data with the following shape (samples, timesteps, nb_features).
When nb_features= 1 Application of CategoryEncoding Layer to such tensor would return a tensor of shape  (samples, timesteps, nb_classes_in_the_first_feature).

**Will this change the current api? How?**

**Who will benefit with this feature?**
Anyone who wishes to apply Categorical preprocessing such as one-hot encoding to categorical features of sequence / Timeserie data

**Any Other info.**
"
43666,Compiling client code with -mavx2 leads to Eigen errors,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS 10.14**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/a**
- TensorFlow installed from (source or binary): **binary** (compiled in CI, transferred to client system)
- TensorFlow version (use command below): **v2.3.0**
- Python version: **n/a**
- Bazel version (if compiling from source): **3.1.0**
- GCC/Compiler version (if compiling from source): Apple Clang 11.0.0 for the client code (not sure which compiler did the tf build, but likely the same or 10.0.0)
- CUDA/cuDNN version: **n/a**
- GPU model and memory: **n/a**

**Describe the current behavior**

We have compiled the C++ library from 2.3.0 and are now using it from our Code. A line such as 
```
#include <tensorflow/cc/saved_model/loader.h>
```
Leads to a compilation failure when `-mavx2` is used:

```
In file included from /build/<ycode>.hpp:20:
In file included from /build/libs/tensorflow/include/tensorflow/cc/saved_model/loader.h:27:
In file included from /build/libs/tensorflow/include/tensorflow/core/public/session.h:24:
In file included from /build/libs/tensorflow/include/tensorflow/core/framework/tensor.h:23:
In file included from /build/libs/tensorflow/include/tensorflow/core/framework/allocator.h:26:
In file included from /build/libs/tensorflow/include/tensorflow/core/framework/numeric_types.h:24:
In file included from /build/libs/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:41:
/build/libs/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:30:9: error: no template named 'eigen_packet_wrapper'
typedef eigen_packet_wrapper<__m256i, 20> Packet32q8i;
```

Inspecting that file shows that the template is indeed not defined. Other Eigen headers for Neon or SSE (consider `/usr/local/include/eigen3/Eigen/src/Core/arch/SSE/PacketMath.h` in Eigen 3.3.7) do include this template.

**Describe the expected behavior**

Client C++ program compiles against TF 2.3.0 when `-mavx2` or-march=native` are enabled.

**Standalone code to reproduce the issue**

```
# test.cpp
#include <tensorflow/cc/saved_model/loader.h>

int main(int argc, char *argv[])
{
    return 0;
}
```
This works:
```
clang -std=c++14 -L libs/tensorflow/lib/ -isystem /usr/local/include/eigen3/ -isystem libs/tensorflow/include/third_party/eigen3/ -isystem libs/tensorflow/include/ -ltensorflow_cc -lstdc++ test.cpp
```
This does not work:
```
clang -mavx2 -std=c++14 -L libs/tensorflow/lib/ -isystem /usr/local/include/eigen3/ -isystem libs/tensorflow/include/third_party/eigen3/ -isystem libs/tensorflow/include/ -ltensorflow_cc -lstdc++ test.cpp
```

Tensorflow directory:
```
tree -L 2 libs/tensorflow/
libs/tensorflow/
├── include
│   ├── absl
│   ├── external
│   ├── tensorflow
│   └── third_party
└── lib
    ├── libtensorflow_cc.so
    └── libtensorflow_cc.so.2 -> libtensorflow_cc.so
```

This problem for instance prevents me from using some libraries who's CMake configuration brings the `march=native` or `mavx2` flags into the build."
43665,IteratorGetNext: unsupported op: No registered 'IteratorGetNext' OpKernel for XLA_GPU_JIT devices compatible with node {{node IteratorGetNext}} 	.  Registered:  device='XLA_GPU',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
   ```
    #Related info to xs:
    names = [name for name in os.listdir(cdir + xname) if name.endswith('.png')]
    xs = [np.array(Image.open(cdir + sat + name).convert('RGB'),dtype=""float32"") for name in names]
    xs = [x/255.0 for x in xs]
    xs = np.asarray(xs)

    with tf.device(""/device:XLA_GPU:0""):
    history = model.fit(xs,
                    ys,
                    validation_split = 0.1,
                    epochs=EPOCHS,
                    batch_size = BATCH_SIZE,
                       )
    ```
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  ```
  Distributor ID:	Ubuntu
  Description:	Ubuntu 18.04.5 LTS
  Release:	18.04
  Codename:	bionic
  ```
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
   No
- TensorFlow installed from (source or binary):
   ```
    binary 
    pip3.6 install tensorflow==2.1.0 tensorflow-gpu==2.1.0
    pip3.7 install tensorflow==2.1.0 tensorflow-gpu==2.1.0
   ```
- TensorFlow version (use command below):
   ```
   2.1.0
   ```
- Python version:
   ```
   import platform
   import sys
   print(platform.python_version()) #jupyter notebook
   print(sys.version)
   print(sys.version_info)
   3.6.9
   3.6.9 (default, Jul 17 2020, 12:50:27) 
   [GCC 8.4.0]
   sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)
   ```
- Bazel version (if compiling from source):
   No
- GCC/Compiler version (if compiling from source):
    No
- CUDA/cuDNN version:
   ```
   nvcc --version
   nvcc: NVIDIA (R) Cuda compiler driver
   Copyright (c) 2005-2019 NVIDIA Corporation
   Built on Sun_Jul_28_19:07:16_PDT_2019
   Cuda compilation tools, release 10.1, V10.1.243
   ```
- GPU model and memory:
```
nvidia-smi
Wed Sep 30 13:07:10 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce 920M        On   | 00000000:03:00.0 N/A |                  N/A |
| N/A   43C    P8    N/A /  N/A |    118MiB /  2004MiB |     N/A      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

After following all steps from here - https://www.tensorflow.org/install/gpu and installing all these GPU doesn't work
```
# Add NVIDIA package repositories
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub
sudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb
sudo apt-get update
wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt-get update

# Install NVIDIA driver
sudo apt-get install --no-install-recommends nvidia-driver-450
# Reboot. Check that GPUs are visible using the command: nvidia-smi

# Install development and runtime libraries (~4GB)
sudo apt-get install --no-install-recommends \
    cuda-10-1 \
    libcudnn7=7.6.5.32-1+cuda10.1  \
    libcudnn7-dev=7.6.5.32-1+cuda10.1


# Install TensorRT. Requires that libcudnn7 is installed above.
sudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda10.1 \
    libnvinfer-dev=6.0.1-1+cuda10.1 \
    libnvinfer-plugin6=6.0.1-1+cuda10.1


```

**Describe the expected behavior**

Should be able to train on GPU

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.



```
local_device_protos = device_lib.list_local_devices()
for x in local_device_protos:
    print(x)
    
#print(tf.test.is_gpu_available)
name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 5085277874134328407

name: ""/device:XLA_CPU:0""
device_type: ""XLA_CPU""
memory_limit: 17179869184
locality {
}
incarnation: 5803642393619994138
physical_device_desc: ""device: XLA_CPU device""

name: ""/device:XLA_GPU:0""
device_type: ""XLA_GPU""
memory_limit: 17179869184
locality {
}
incarnation: 6332427526805909660
physical_device_desc: ""device: XLA_GPU device""
```
```
with tf.device(""/device:XLA_GPU:0""):
    history = model.fit(xs,
                    ys,
                    validation_split = 0.1,
                    epochs=EPOCHS,
                    batch_size = BATCH_SIZE,
                       )
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
Train on 921 samples, validate on 103 samples
Epoch 1/10
 16/921 [..............................] - ETA: 4:43

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-18-40038c56b2dd> in <module>()
      4                     validation_split = 0.1,
      5                     epochs=EPOCHS,
----> 6                     batch_size = BATCH_SIZE,
      7                        )

/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    817         max_queue_size=max_queue_size,
    818         workers=workers,
--> 819         use_multiprocessing=use_multiprocessing)
    820 
    821   def evaluate(self,

/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    340                 mode=ModeKeys.TRAIN,
    341                 training_context=training_context,
--> 342                 total_epochs=epochs)
    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    344 

/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    126         step=step, mode=mode, size=current_batch_size) as batch_logs:
    127       try:
--> 128         batch_outs = execution_function(iterator)
    129       except (StopIteration, errors.OutOfRangeError):
    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     96     # `numpy` translates Tensors to values in Eager mode.
     97     return nest.map_structure(_non_none_constant_value,
---> 98                               distributed_function(input_fn))
     99 
    100   return execution_function

/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    566         xla_context.Exit()
    567     else:
--> 568       result = self._call(*args, **kwds)
    569 
    570     if tracing_count == self._get_tracing_count():

/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    630         # Lifting succeeded, so variables are initialized and we can run the
    631         # stateless function.
--> 632         return self._stateless_fn(*args, **kwds)
    633     else:
    634       canon_args, canon_kwds = \

/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   2361     with self._lock:
   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2364 
   2365   @property

/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
   1609          if isinstance(t, (ops.Tensor,
   1610                            resource_variable_ops.BaseResourceVariable))),
-> 1611         self.captured_inputs)
   1612 
   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1690       # No tape is watching; skip to running the function.
   1691       return self._build_call_outputs(self._inference_function.call(
-> 1692           ctx, args, cancellation_manager=cancellation_manager))
   1693     forward_backward = self._select_forward_and_backward_functions(
   1694         args,

/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    543               inputs=args,
    544               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 545               ctx=ctx)
    546         else:
    547           outputs = execute.execute_with_cancellation(

/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

/home/maifee/.local/lib/python3.6/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: Function invoked by the following node is not compilable: name: ""__inference_distributed_function_7577"" op: ""__inference_distributed_function_7577"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" input: ""dummy_input"" attr { key: ""_XlaCompile"" value { b: true } } attr { key: ""config_proto"" value { s: ""\n\007\n\003CPU\020\001\n\007\n\003GPU\020\0002\002J\0008\001"" } } attr { key: ""executor_type"" value { s: """" } }.
Uncompilable nodes:
	IteratorGetNext: unsupported op: No registered 'IteratorGetNext' OpKernel for XLA_GPU_JIT devices compatible with node {{node IteratorGetNext}}
	.  Registered:  device='XLA_GPU'
  device='XLA_CPU'
  device='GPU'
  device='CPU'

	Stacktrace:
		Node: __inference_distributed_function_7577, function: 
		Node: IteratorGetNext, function: __inference_distributed_function_7577
 [Op:__inference_distributed_function_7577]


```"
43664,Unable to build tf-master,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- TensorFlow installed from (source or binary): source 
- TensorFlow version: master
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 3.5.0
- GCC/Compiler version (if compiling from source): MSVC 2017
- CUDA/cuDNN version: 11.1 / 7.6.0
- GPU model and memory: RTX 2080 TI

**Describe the problem**

unable to build 

**Any other info / logs**

```
[1,694 / 2,066] Compiling tensorflow/core/framework/graph.pb.cc; 2s local ... (4
 actions, 3 running)
ERROR: E:/dp/tools/tensorflow/tensorflow-master/tensorflow/core/platform/windows
/BUILD:151:11: C++ compilation of rule '//tensorflow/core/platform/windows:platf
orm_port' failed (Exit 2): python.exe failed: error executing command
  cd E:/dp/_internal/_e/up/_bazel_administrator/n2u4av42/execroot/org_tensorflow

  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\T
ools\MSVC\14.16.27023\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Stu
dio\2017\Community\VC\Tools\MSVC\14.16.27023\include;C:\Program Files (x86)\Wind
ows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\includ
e\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\
shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program
 Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;C:\Program Files (x86)\W
indows Kits\10\include\10.0.17763.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Too
ls\MSVC\14.16.27023\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studi
o\2017\Community\VC\Tools\MSVC\14.16.27023\lib\x64;C:\Program Files (x86)\Window
s Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0
.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64
;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\To
ols\MSVC\14.16.27023\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Stu
dio\2017\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft SD
Ks\TypeScript\3.1;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\
Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microso
ft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFound
ation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2017\Communit
y\MSBuild\15.0\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2017\Co
mmunity\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual
 Studio\2017\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Micro
soft Visual Studio\Shared\Common\VSPerfCollectionTools\\x64;C:\Program Files (x8
6)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\;C:\Program Files
 (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;C:\Program Files
 (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Micro
soft\FSharp\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.17763.0\x64;C:\Prog
ram Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual
Studio\2017\Community\\MSBuild\15.0\bin;C:\Windows\Microsoft.NET\Framework64\v4.
0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\ID
E\;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\Tools\;
;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2017\Communi
ty\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)
\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\C
Make\Ninja
    SET PWD=/proc/self/cwd
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=E:\DP\_internal\_e\_t
    SET TMP=E:\DP\_internal\_e\_t
  E:/DP/python/3.6.8/python.exe -B external/local_config_cuda/crosstool/windows/
msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x060
0 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DE
PRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd49
96 /I. /Ibazel-out/x64_windows-opt-exec-50AE0418/bin /Iexternal/eigen_archive /I
bazel-out/x64_windows-opt-exec-50AE0418/bin/external/eigen_archive /Iexternal/co
m_google_absl /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/com_google_
absl /Iexternal/snappy /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/sn
appy /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/exte
rnal/eigen_archive /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE
_TRAITS=0 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DTF_USE_SNAPPY /showIncludes /MD /
O2 /DNDEBUG /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:prep
rocessor /std:c++14 -DGOOGLE_CUDA=1 -DTENSORFLOW_USE_NVCC=1 -DTENSORFLOW_MONOLIT
HIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADP
OOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COM
PILE_LIBRARY /Fobazel-out/x64_windows-opt-exec-50AE0418/bin/tensorflow/core/plat
form/windows/_objs/platform_port/port.obj /c tensorflow/core/platform/windows/po
rt.cc
Execution platform: @local_execution_config_platform//:platform
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\stdio.h(378): w
arning C5105: macro expansion producing 'defined' has undefined behavior
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\stdio.h(2437):
warning C5105: macro expansion producing 'defined' has undefined behavior
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\corecrt_search.
h(188): warning C5105: macro expansion producing 'defined' has undefined behavio
r
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\stdlib.h(79): w
arning C5105: macro expansion producing 'defined' has undefined behavior
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\stdlib.h(1286):
 warning C5105: macro expansion producing 'defined' has undefined behavior
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\corecrt_memory.
h(76): warning C5105: macro expansion producing 'defined' has undefined behavior

C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\corecrt_wstring
.h(573): warning C5105: macro expansion producing 'defined' has undefined behavi
or
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\string.h(531):
warning C5105: macro expansion producing 'defined' has undefined behavior
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\corecrt_math.h(
44): warning C5105: macro expansion producing 'defined' has undefined behavior
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\corecrt_math.h(
963): warning C5105: macro expansion producing 'defined' has undefined behavior
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\malloc.h(173):
warning C5105: macro expansion producing 'defined' has undefined behavior
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\float.h(328): w
arning C5105: macro expansion producing 'defined' has undefined behavior
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\sys/stat.h(87):
 warning C5105: macro expansion producing 'defined' has undefined behavior
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\sys/stat.h(120)
: warning C5105: macro expansion producing 'defined' has undefined behavior
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\sys/stat.h(217)
: warning C5105: macro expansion producing 'defined' has undefined behavior
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt\ctype.h(241): w
arning C5105: macro expansion producing 'defined' has undefined behavior
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um\winbase.h(9254):
warning C5105: macro expansion producing 'defined' has undefined behavior
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um\oaidl.h(487): war
ning C5103: pasting '/' and '/' does not result in a valid preprocessing token
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared\wtypes.h(745)
: note: in expansion of macro '_VARIANT_BOOL'

C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um\oaidl.h(487): err
or C2059: syntax error: '/'
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um\oaidl.h(487): err
or C2238: unexpected token(s) preceding ';'

C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um\oaidl.h(502): war
ning C5103: pasting '/' and '/' does not result in a valid preprocessing token
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared\wtypes.h(745)
: note: in expansion of macro '_VARIANT_BOOL'

C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um\oaidl.h(502): err
or C2059: syntax error: '/'
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um\oaidl.h(502): err
or C2238: unexpected token(s) preceding ';'

C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um\propidlbase.h(319
): warning C5103: pasting '/' and '/' does not result in a valid preprocessing t
oken
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared\wtypes.h(745)
: note: in expansion of macro '_VARIANT_BOOL'

C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um\propidlbase.h(319
): error C2059: syntax error: '/'
C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um\propidlbase.h(319
): error C2238: unexpected token(s) preceding ';'

Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 481.272s, Critical Path: 17.16s
INFO: 458 processes: 458 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
The system cannot find the path specified.
```

seems like _VARIANT_BOOL is replaced thus generates error in windows kits library."
43663,TFLite android image classification demo failed on samsung note10 whose soc is snapdragon 855,"- system platform: macos 10.15

- example: [tflite image classification official demo](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android)

- IDE: android studio 4.0.1

- hardware: samsung note10 equiped with snapdragon 855

errors: when I tried to use the dsp delegate on the phone, I modified the code like this:
```
case DSP:
                dspDelegate = new HexagonDelegate(activity);
                tfliteOptions.addDelegate(dspDelegate);
                break;
```
in the [classifier.java](https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/android/lib_support/src/main/java/org/tensorflow/lite/examples/classification/tflite/Classifier.java#L201)

so I built and run the demo and selected quantized_efficientnet & dsp to run the DNN on the hexagon and it was ok.

However, when I download the android NDK 21.3.6528147, android studio tell me that I need to install NDK 21.0.6113669, I followed the instruction and re-built and re-run the app. When I selected quantized_efficientnet & dsp again, I got the following error:

```
E/AndroidRuntime: FATAL EXCEPTION: inference
E/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:729:Error 45: fopen failed for testsig-0xfe534ed3.so. (No such file or directory)
E/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:729:Error 45: fopen failed for testsig.so. (No such file or directory)
D/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:989: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror segment 0 failed signature verification (0xF4 B) for libhexagon_nn_s
    vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1027: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp
W/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.
I/tflite: Hexagon Delegate is not supported.
E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: org.tensorflow.lite.examples.classification, PID: 9186
    java.lang.UnsupportedOperationException: This Device doesn't support Hexagon DSP execution.
        at org.tensorflow.lite.HexagonDelegate.<init>(HexagonDelegate.java:39)
        at org.tensorflow.lite.examples.classification.tflite.Classifier.<init>(Classifier.java:257)
        at org.tensorflow.lite.examples.classification.tflite.ClassifierQuantizedEfficientNet.<init>(ClassifierQuantizedEfficientNet.java:46)
        at org.tensorflow.lite.examples.classification.tflite.Classifier.create(Classifier.java:159)
        at org.tensorflow.lite.examples.classification.ClassifierActivity.recreateClassifier(ClassifierActivity.java:162)
        at org.tensorflow.lite.examples.classification.ClassifierActivity.lambda$onInferenceConfigurationChanged$0$ClassifierActivity(ClassifierActivity.java:132)
        at org.tensorflow.lite.examples.classification.-$$Lambda$ClassifierActivity$83lGy2TUjuj0M5n4BhMB9qlLgSY.run(Unknown Source:8)
        at android.os.Handler.handleCallback(Handler.java:873)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:216)
        at android.os.HandlerThread.run(HandlerThread.java:65)
```

where Classifier.java:257 is 
dspDelegate = new HexagonDelegate(activity); // (definition is: private HexagonDelegate dspDelegate = null;)


so I uninstalled the NDK and it was ok again.
"
43661,Core dumped when invoking TFLite model converted using latest nightly TFLite converter (2.4.0dev2020929),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 20.04**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (or github SHA if from source): **2.4.0.dev20200929**


**Command used to run the converter or code if you’re using the Python API**
```
import tensorflow as tf

import numpy as np


def wrap_frozen_graph(graph_def, inputs, outputs):
    def _imports_graph_def():
        tf.compat.v1.import_graph_def(graph_def, name="""")
    wrapped_import = tf.compat.v1.wrap_function(_imports_graph_def, [])
    import_graph = wrapped_import.graph
    return wrapped_import.prune(
        tf.nest.map_structure(import_graph.as_graph_element, inputs),
        tf.nest.map_structure(import_graph.as_graph_element, outputs))


graph_def = tf.compat.v1.GraphDef()
_ = graph_def.ParseFromString(open('minimal_093011.pb', 'rb').read())
dnn_function = wrap_frozen_graph(graph_def, inputs='import/first_graph_input:0', outputs='import_1/second_graph_output/Mean:0')
converter = tf.lite.TFLiteConverter.from_concrete_functions([dnn_function])

converter.experimental_enable_mlir_converter = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]


def representative_dataset_gen():
    image = np.random.randint(low=0, high=255, size=(1, 480, 640, 3), dtype='uint8')
    yield [image]


converter.representative_dataset = representative_dataset_gen
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

model = converter.convert()
```

**Link to Google Colab Notebook**

```
https://colab.research.google.com/drive/1U8UVDl6lIs1zKjfpFc7hrr3jAo-0eh_i?usp=sharing
```

**Also, please include a link to the saved model or GraphDef**

```
https://drive.google.com/file/d/1Hvr9hfvaxj3sBi0D0U0iAAe1kEaiJJWB/view?usp=sharing
```

**Failure details**
The conversion is successful in that it generates a tflite graph. However, when I invoke the graph, I get a core dump error:
[1]    511859 abort (core dumped)  python src/reproduce_minimal_tflite_test.py

**Code used to invoke the graph. Also included in Colab notebook linked above.**
```
image = np.random.randint(low=0, high=255, size=(1, 480, 640, 3), dtype='uint8')

tflite_model = tf.lite.Interpreter('models/minimal_093011.tflite')
tflite_model.allocate_tensors()

input_details = tflite_model.get_input_details()
tflite_model.set_tensor(input_details[0]['index'], image)
tflite_model.invoke()
```

**Traceback**

```
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50                                                                                                                                                                     
#1  0x00007ffff7dc0859 in __GI_abort () at abort.c:79                                                                                                                                                                                         
#2  0x00007fffb9386e42 in tflite::QuantizeMultiplierSmallerThanOneExp(double, int*, int*) ()                                                                                                                                                  
   from /home/yousef/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so                                                                                   
#3  0x00007fffb9158090 in void tflite::ops::builtin::comparisons::(anonymous namespace)::ComparisonQuantized<signed char, &(bool tflite::reference_ops::GreaterFn<int>(int, int))>(TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, bo
ol) () from /home/yousef/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so                                                                               
#4  0x00007fffb9158b7e in tflite::ops::builtin::comparisons::(anonymous namespace)::GreaterEval(TfLiteContext*, TfLiteNode*) ()                                                                                                               
   from /home/yousef/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so                                                                                   
#5  0x00007fffb9369713 in tflite::Subgraph::Invoke() () from /home/yousef/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so                              
#6  0x00007fffb936c1f0 in tflite::Interpreter::Invoke() () from /home/yousef/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so                           
#7  0x00007fffb90f7548 in tflite::interpreter_wrapper::InterpreterWrapper::Invoke() ()                                                                                                                                                        
   from /home/yousef/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so                                                                                   
#8  0x00007fffb90eb6ee in pybind11::cpp_function::initialize<pybind11_init__pywrap_tensorflow_interpreter_wrapper(pybind11::module&)::{lambda(tflite::interpreter_wrapper::InterpreterWrapper&)#6}, pybind11::object, tflite::interpreter_wrap
per::InterpreterWrapper&, pybind11::name, pybind11::is_method, pybind11::sibling>(pybind11_init__pywrap_tensorflow_interpreter_wrapper(pybind11::module&)::{lambda(tflite::interpreter_wrapper::InterpreterWrapper&)#6}&&, pybind11::object (*
)(tflite::interpreter_wrapper::InterpreterWrapper&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call) ()                     
   from /home/yousef/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so                                                                                   
#9  0x00007fffb90ecb39 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()                                                                                                                                                 
   from /home/yousef/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so                                                                                   
#10 0x00005555556b9914 in _PyMethodDef_RawFastCallKeywords (method=0x55555694b100, self=0x7fffbb8c9270, args=0x7fffaf04dd98, nargs=<optimised out>, kwnames=<optimised out>)                                                                  
    at /tmp/build/80754af9/python_1598874792229/work/Objects/call.c:693                                                                                                                                                                       
#11 0x00005555556b9a31 in _PyCFunction_FastCallKeywords (func=0x7fffc08de460, args=<optimised out>, nargs=<optimised out>, kwnames=<optimised out>) at /tmp/build/80754af9/python_1598874792229/work/Objects/call.c:732                       
#12 0x000055555572639e in call_function (kwnames=0x0, oparg=<optimised out>, pp_stack=<synthetic pointer>) at /tmp/build/80754af9/python_1598874792229/work/Python/ceval.c:4619                                                               
#13 _PyEval_EvalFrameDefault (f=<optimised out>, throwflag=<optimised out>) at /tmp/build/80754af9/python_1598874792229/work/Python/ceval.c:3093                                                                                              
#14 0x00005555556b8e7b in function_code_fastcall (globals=<optimised out>, nargs=1, args=<optimised out>, co=<optimised out>) at /tmp/build/80754af9/python_1598874792229/work/Objects/call.c:283                                             
#15 _PyFunction_FastCallKeywords (func=<optimised out>, stack=0x7ffff6d615c0, nargs=1, kwnames=<optimised out>) at /tmp/build/80754af9/python_1598874792229/work/Objects/call.c:408                                                           
#16 0x0000555555721740 in call_function (kwnames=0x0, oparg=<optimised out>, pp_stack=<synthetic pointer>) at /tmp/build/80754af9/python_1598874792229/work/Python/ceval.c:4616                                                               
#17 _PyEval_EvalFrameDefault (f=<optimised out>, throwflag=<optimised out>) at /tmp/build/80754af9/python_1598874792229/work/Python/ceval.c:3110                                                                                              
#18 0x0000555555668829 in _PyEval_EvalCodeWithName (_co=0x7ffff6cfa1e0, globals=<optimised out>, locals=<optimised out>, args=<optimised out>, argcount=<optimised out>, kwnames=0x0, kwargs=0x0, kwcount=0, kwstep=2, defs=0x0, defcount=0,  
    kwdefs=0x0, closure=0x0, name=0x0, qualname=0x0) at /tmp/build/80754af9/python_1598874792229/work/Python/ceval.c:3930                                                                                                                     
#19 0x0000555555669714 in PyEval_EvalCodeEx (_co=<optimised out>, globals=<optimised out>, locals=<optimised out>, args=<optimised out>, argcount=<optimised out>, kws=<optimised out>, kwcount=0, defs=0x0, defcount=0, kwdefs=0x0,          
    closure=0x0) at /tmp/build/80754af9/python_1598874792229/work/Python/ceval.c:3959                                                                                                                                                         
#20 0x000055555566973c in PyEval_EvalCode (co=<optimised out>, globals=<optimised out>, locals=<optimised out>) at /tmp/build/80754af9/python_1598874792229/work/Python/ceval.c:524                                                           
#21 0x0000555555780f14 in run_mod (mod=<optimised out>, filename=<optimised out>, globals=0x7ffff6dcac30, locals=0x7ffff6dcac30, flags=<optimised out>, arena=<optimised out>)                                                                
    at /tmp/build/80754af9/python_1598874792229/work/Python/pythonrun.c:1035                                                                                                                                                                  
#22 0x000055555578b331 in PyRun_FileExFlags (fp=0x5555558c3100, filename_str=<optimised out>, start=<optimised out>, globals=0x7ffff6dcac30, locals=0x7ffff6dcac30, closeit=1, flags=0x7fffffffdd80)                                          
    at /tmp/build/80754af9/python_1598874792229/work/Python/pythonrun.c:988                                                                                                                                                                   
#23 0x000055555578b523 in PyRun_SimpleFileExFlags (fp=0x5555558c3100, filename=<optimised out>, closeit=1, flags=0x7fffffffdd80) at /tmp/build/80754af9/python_1598874792229/work/Python/pythonrun.c:429                                      
#24 0x000055555578c655 in pymain_run_file (p_cf=0x7fffffffdd80, filename=0x5555558c2870 L""src/reproduce_minimal_tflite_test.py"", fp=0x5555558c3100) at /tmp/build/80754af9/python_1598874792229/work/Modules/main.c:462                       
#25 pymain_run_filename (cf=0x7fffffffdd80, pymain=0x7fffffffde90) at /tmp/build/80754af9/python_1598874792229/work/Modules/main.c:1652                                                                                                       
#26 pymain_run_python (pymain=0x7fffffffde90) at /tmp/build/80754af9/python_1598874792229/work/Modules/main.c:2913     
#27 pymain_main (pymain=0x7fffffffde90) at /tmp/build/80754af9/python_1598874792229/work/Modules/main.c:3460           
#28 0x000055555578c77c in _Py_UnixMain (argc=<optimised out>, argv=<optimised out>) at /tmp/build/80754af9/python_1598874792229/work/Modules/main.c:3495                                                                                      
#29 0x00007ffff7dc20b3 in __libc_start_main (main=0x555555649c90 <main>, argc=2, argv=0x7fffffffdff8, init=<optimised out>, fini=<optimised out>, rtld_fini=<optimised out>, stack_end=0x7fffffffdfe8) at ../csu/libc-start.c:308             
#30 0x0000555555730ff0 in _start () at ../sysdeps/x86_64/elf/start.S:103
```
"
43660,Error converting from TensorFlow frozen graph to TFLite using TF1.15,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 20.04**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (or github SHA if from source): **1.15.0**

**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.
```
input_name = 'import/first_graph_input'
output_name = 'import_1/second_graph_output/Mean'
converter = tf.lite.TFLiteConverter.from_frozen_graph('minimal_093010.pb', input_arrays=[input_name], output_arrays=[output_name])

converter.optimizations = [tf.lite.Optimize.DEFAULT]

def representative_dataset_gen():
  image = np.random.randint(low=0, high=255, size=(480, 640, 3))
  image = image.astype(np.float32)
  yield [image]

converter.representative_dataset = representative_dataset_gen
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

model = converter.convert()
```

**Link to Colab Notebook**

```
https://colab.research.google.com/drive/1CN8iIlznqCMK7hRBwHw8kP2RLamwGZeQ?usp=sharing
```

**The output from the converter invocation**

```
RuntimeError: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.Failed to create calibrator, context already registered.
```

**Also, please include a link to the saved model or GraphDef**

```
https://drive.google.com/file/d/1dkc3-tYwviEyFJg2Y0IZEyRdTbKj5piF/view?usp=sharing
```

**Failure details**
```
Conversion fails. It is successful for non-quantized tflite graphs, but fails when doing full-integer quantization.
```

**Traceback**
```
Traceback (most recent call last):
  File ""src/convert_pb_to_tflite.py"", line 60, in <module>
    main()
  File ""src/convert_pb_to_tflite.py"", line 48, in main
    convert_pb_to_tflite_with_quantization(args.pb_graph_path, args.tflite_graph_path)
  File ""src/convert_pb_to_tflite.py"", line 41, in convert_pb_to_tflite_with_quantization
    model = converter.convert()
  File ""/home/yousef/miniconda3/envs/tf15/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 993, in convert
    inference_output_type)
  File ""/home/yousef/miniconda3/envs/tf15/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 239, in _calibrate_quantize_model
    inference_output_type, allow_float)
  File ""/home/yousef/miniconda3/envs/tf15/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/calibrator.py"", line 75, in calibrate_and_quantize
    self._calibrator.FeedTensor(calibration_sample)
  File ""/home/yousef/miniconda3/envs/tf15/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py"", line 112, in FeedTensor
    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value)
RuntimeError: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.No calibrator found for context.Node number 0 (CONV_2D) failed to invoke.
```"
43657,null pointer dereference Error in TF2.3.0 with runforMultipleInputOutput,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung A51
TensorFlow installed from (source or binary): Maven
TensorFlow version (use command below): implementation('org.tensorflow:tensorflow-lite:2.3.0'){changing=true}
Python version: n/a
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: n/a
GPU model and memory: n/a


**Describe the current behavior**

.tflite generated using tf.lite.TFLiteConverter.from_saved_model
tf_version = 2.3.0
Python Implementation for Inference works without errors.

using the same model on Android for inference gives the error-
2020-09-24 07:31:59.732 17121-17121/? A/DEBUG: Cause: null pointer dereference

Debugging Log - 


**Model Input Tensor Details**
2020-09-24 07:31:59.284 16761-16761/Processor: hws2:0 2 Input SHAPE- 0
2020-09-24 07:31:59.285 16761-16761/Processor: mask:0 1 1 1 1 Input SHAPE- 1
2020-09-24 07:31:59.285 16761-16761/Processor: image:0 1 1 1 3 Input SHAPE- 2
2020-09-24 07:31:59.285 16761-16761/Processor: hws:0 2 Input SHAPE- 3

2020-09-24 07:31:59.285 16761-16761/Processor: strided_slice_1:0 1 1 3 Output SHAPE- 0

**Model Reallocating the Input Tensor**
after :
tflite.resizeInput(1,dim);
tflite.resizeInput(2,dim);
tflite.allocateTensors();

**Model Input Tensor Details After Reallocation**
2020-09-24 07:31:59.286 16761-16761/Processor: hws2:0 2 Input SHAPE- 0
2020-09-24 07:31:59.286 16761-16761/Processor: mask:0 1 256 256 1 Input SHAPE- 1
2020-09-24 07:31:59.286 16761-16761/Processor: image:0 1 256 256 3 Input SHAPE- 2
2020-09-24 07:31:59.286 16761-16761/Processor: hws:0 2 Input SHAPE- 3

2020-09-24 07:31:59.286 16761-16761/Processor: strided_slice_1:0 1 1 3 Output SHAPE- 0

Notice : Output Shape doesnt changes, which I _assume_ is the correct behavior

On Running :
tflite.runForMultipleInputsOutputs(inputs, outputs);

inputs and outputs are properly initialized and non-null
This error comes -

2020-09-24 07:31:59.442 16761-16761/com.package.deepak A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0 in tid 16761 (service.deepak), pid 16761 (service.deepak)

2020-09-24 07:31:59.731 17121-17121/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
2020-09-24 07:31:59.732 17121-17121/? A/DEBUG: Build fingerprint: Samsung-A50
2020-09-24 07:31:59.732 17121-17121/? A/DEBUG: Revision: '2'
2020-09-24 07:31:59.732 17121-17121/? A/DEBUG: ABI: 'arm64'
2020-09-24 07:31:59.732 17121-17121/? A/DEBUG: pid: 16761, tid: 16761, name: service.deepak >>> com.package.deepak <<<
2020-09-24 07:31:59.732 17121-17121/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0
2020-09-24 07:31:59.732 17121-17121/? A/DEBUG: Cause: null pointer dereference

2020-09-24 07:41:50.415 18015-18015/? A/DEBUG: backtrace:
2020-09-24 07:41:50.415 18015-18015/? A/DEBUG: #00 pc 000000000001dd6c /system/lib64/libc.so (memcpy+124)
2020-09-24 07:41:50.415 18015-18015/? A/DEBUG: #1 pc 0000000000133560 /data/app/com.package.deepak-fuEaz_w7MUZ9fy7vI4iAfA==/lib/arm64/libtensorflowlite_jni.so
2020-09-24 07:41:50.415 18015-18015/? A/DEBUG: #2 pc 00000000001331e8 /data/app/com.package.deepak-fuEaz_w7MUZ9fy7vI4iAfA==/lib/arm64/libtensorflowlite_jni.so
2020-09-24 07:41:50.415 18015-18015/? A/DEBUG: #3 pc 00000000001b269c /data/app/com.package.deepak-fuEaz_w7MUZ9fy7vI4iAfA==/lib/arm64/libtensorflowlite_jni.so
2020-09-24 07:41:50.415 18015-18015/? A/DEBUG: #4 pc 00000000001b546c /data/app/com.package.deepak-fuEaz_w7MUZ9fy7vI4iAfA==/lib/arm64/libtensorflowlite_jni.so
2020-09-24 07:41:50.415 18015-18015/? A/DEBUG: #5 pc 0000000000046738 /data/app/com.package.deepak-fuEaz_w7MUZ9fy7vI4iAfA==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+32)
2020-09-24 07:41:50.415 18015-18015/? A/DEBUG: #6 pc 0000000000563be0 /system/lib64/libart.so (art_quick_generic_jni_trampoline+144)
2020-09-24 07:41:50.416 18015-18015/? A/DEBUG: #7 pc 000000000055ae4c /system/lib64/libart.so (art_quick_invoke_static_stub+604)
2020-09-24 07:41:50.416 18015-18015/? A/DEBUG: #8 pc 00000000000d04e8 /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+232)
2020-09-24 07:41:50.416 18015-18015/? A/DEBUG: #9 pc 00000000002838ac /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)
2020-09-24 07:41:50.416 18015-18015/? A/DEBUG: #10 pc 000000000027d8b4 /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+968)
2020-09-24 07:41:50.416 18015-18015/? A/DEBUG: #11 pc 000000000052b750 /system/lib64/libart.so (MterpInvokeStatic+204)
2020-09-24 07:41:50.416 18015-18015/? A/DEBUG: #12 pc 000000000054d394 /system/lib64/libart.so (ExecuteMterpImpl+14612)
2020-09-24 07:41:50.416 18015-18015/? A/DEBUG: #13 pc 000000000021fcf4 /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.package.deepak-fuEaz_w7MUZ9fy7vI4iAfA==/base.apk_17926_17926 (deleted) (org.tensorflow.lite.NativeInterpreterWrapper.run+156)
2020-09-24 07:41:50.416 18015-18015/? A/DEBUG: #14 pc 00000000002575b8 /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1037722801+488)
2020-09-24 07:41:50.416 18015-18015/? A/DEBUG: #15 pc 000000000025d0ac /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
2020-09-24 07:41:50.416 18015-18015/? A/DEBUG: #16 pc 000000000027d898 /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)
2020-09-24 07:41:50.416 18015-18015/? A/DEBUG: #17 pc 000000000052a24c /system/lib64/libart.so (MterpInvokeVirtual+588)
2020-09-24 07:41:50.416 18015-18015/? A/DEBUG: #18 pc 000000000054d214 /system/lib64/libart.so (ExecuteMterpImpl+14228)
2020-09-24 07:41:50.417 18015-18015/? A/DEBUG: #19 pc 000000000021f2fe /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.package.deepak-fuEaz_w7MUZ9fy7vI4iAfA==/base.apk_17926_17926 (deleted) (org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs+10)
2020-09-24 07:41:50.417 18015-18015/? A/DEBUG: #20 pc 00000000002575b8 /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.1037722801+488)
2020-09-24 07:41:50.417 18015-18015/? A/DEBUG: #21 pc 000000000051aae0 /system/lib64/libart.so (artQuickToInterpreterBridge+1020)
2020-09-24 07:41:50.417 18015-18015/? A/DEBUG: #22 pc 0000000000563cfc /system/lib64/libart.so (art_quick_to_interpreter_bridge+92)
2020-09-24 07:41:50.417 18015-18015/? A/DEBUG: #23 pc 0000000000019ba4 /dev/ashmem/dalvik-jit-code-cache_17926_17926 (deleted) (com.package.deepak.Processor.process+12052)
2020-09-24 07:41:50.417 18015-18015/? A/DEBUG: #24 pc 000000000055aedc /system/lib64/libart.so (art_quick_osr_stub+44)

Describe the expected behavior
the Android code should run.

Ps - Reproducible Code/Model is not available due to confidentiality reasons."
43656,TF hangs on HTC Cluster,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS, Linux 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Pip, under Anaconda
- TensorFlow version (use command below): 2.2.0, 2.3.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1.243 / 7.6.5__cuda-10.1
- GPU model and memory: Various - K40, K80, P100, V100 (12-64GB)

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I am currently experimenting with running my training jobs on an HTC cluster running the [Slurm](https://slurm.schedmd.com/quickstart.html) job queue manager, however my code just hangs, with no error messages printed. The same code works without issue on my own Ubuntu Deep Learning system, equipped with 2 Titan RTX cards, and also on Google Colab.

I have a suspicion that this is related to my dataset code, which involves two `from_generator` transforms. This is shown in the code supplied below, in the `get_dataset` function. The first `from_generator` runs a function `load_audio`, which yields batches of audio files from disk. The second, running `get_subseqs`, slices subsequences from the loaded batches, following the [cross-batch statefulness pattern](https://www.tensorflow.org/guide/keras/rnn#cross-batch_statefulness), described in the TensorFlow RNN documentation. So I am feeding subsequences to my RNNs, since there are too many samples to be fed in one go (note that the subsequences retain the batch size).

If I remove the first `from_generator` the training _does not_ hang, however. The function `get_dataset_NO_HANG` shows this - it builds a dataset using a _single_ `from_generator`, which runs the `get_rand_seqs` function, which simply yields randomly generated subsequences.

**Describe the expected behavior**

It shouldn't just hang forever, with no info...

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import os
import fnmatch
import random
import time
import tensorflow as tf
import numpy as np
#import librosa

NUM_EPOCHS = 2
BATCH_SIZE = 16
FRAME_SIZE = 64
NUM_BATCHES = 4
SEQ_LEN = 1024
OVERLAP = FRAME_SIZE
NUM_SAMPS = 128000 + OVERLAP

##################### DATASET #####################

def find_files(directory, pattern='*.wav'):
    '''Recursively finds all files matching the pattern.'''
    files = []
    for root, dirnames, filenames in os.walk(directory):
        for filename in fnmatch.filter(filenames, pattern):
            files.append(os.path.join(root, filename))
    random.shuffle(files)
    return files

# This can load audio files (preferably of only a few seconds long!), but I have commented out that code
# and replaced with a line that simply generates some arrays of random 'samples'...
def load_audio(files, batch_size):
    #for filename in files:
    for filename in range(0, 350):
        #(audio, _) = librosa.load(filename, sr=None, mono=True)
        audio = np.random.randint(0, 256, size=(batch_size * NUM_SAMPS))
        audio = audio.reshape(-1, 1)
        print(""Loading corpus entry {}"".format(filename))
        yield audio

def pad_batch(batch, batch_size, seq_len, overlap):
    num_samps = ( int(np.floor(len(batch[0]) / float(seq_len))) * seq_len )
    zeros = np.zeros([batch_size, overlap, 1], dtype='float32')
    return tf.concat([zeros, batch[:, :num_samps, :]], axis=1)

def get_subseqs(dataset, batch_size, seq_len, overlap):
    for batch in dataset:
        num_samps = len(batch[0])
        for i in range(overlap, num_samps, seq_len):
            x = batch[:, i-overlap : i+seq_len]
            y = x[:, overlap : overlap+seq_len]
            yield (x, y)

def get_dataset(files=None, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, overlap=OVERLAP,
                seq_len=SEQ_LEN, drop_remainder=False):
    dataset = tf.data.Dataset.from_generator(
        lambda: load_audio(files, batch_size),
        output_types=tf.float32,
        output_shapes=((None, 1))
    )
    dataset = dataset.repeat(num_epochs).batch(batch_size, drop_remainder)
    dataset = dataset.map(lambda batch: tf.py_function(
        func=pad_batch, inp=[batch, batch_size, seq_len, overlap], Tout=tf.float32
    ))
    return tf.data.Dataset.from_generator(
        lambda: get_subseqs(dataset, batch_size, seq_len, overlap),
        output_types=(tf.int32, tf.int32),
        output_shapes=(
            (batch_size, seq_len + overlap, 1),
            (batch_size, seq_len, 1)))

# If we use the following there is NO hang...

def get_rand_seqs(num_samps, num_batches=NUM_BATCHES, batch_size=BATCH_SIZE, overlap=OVERLAP, seq_len=SEQ_LEN):
    time.sleep(0.03)
    for _ in range(0, num_batches):
        batch = np.random.randint(0, 256, size=(batch_size * num_samps))
        batch = batch.reshape((batch_size, num_samps, 1))
        for i in range(overlap, num_samps, seq_len):
            x = batch[:, i-overlap : i+seq_len]
            y = x[:, overlap : overlap+seq_len]
            yield (x, y)

def get_dataset_NO_HANG(num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, overlap=OVERLAP, seq_len=SEQ_LEN):
    dataset = tf.data.Dataset.from_generator(
        lambda: get_subseqs(NUM_SAMPS),
        output_types=(tf.int32, tf.int32),
        output_shapes=(
            (batch_size, seq_len + overlap, 1),
            (batch_size, seq_len, 1)))
    dataset = dataset.repeat(num_epochs)
    return dataset

##################### MODEL #####################

# This is just a dummy model, not my real one, although I do make extensive use of RNNs...
class TestModel(tf.keras.Model):

    def __init__(self, frame_size=FRAME_SIZE, dim=1024):
        super(TestModel, self).__init__()
        self.frame_size = frame_size
        self.q_levels = 256
        self.dim = dim
        self.num_lower_tier_frames = 4
        self.input_expand = tf.keras.layers.Dense(self.dim)
        self.rnn = tf.keras.layers.GRU(self.dim, return_sequences=True, stateful=True)
        self.upsample = tf.Variable(
            tf.initializers.GlorotNormal()(
                shape=[self.num_lower_tier_frames, self.dim, self.dim]),
            name=""upsample"",
        )
        self.out = tf.keras.layers.Dense(self.q_levels, activation='relu')

    def train_step(self, data):
        (x, y) = data
        with tf.GradientTape() as tape:
            raw_output = self(x, training=True)
            prediction = tf.reshape(raw_output, [-1, self.q_levels])
            target = tf.reshape(y, [-1])
            loss = self.compiled_loss(
                target,
                prediction,
                regularization_losses=self.losses)
        grads = tape.gradient(loss, self.trainable_variables)
        grads, _ = tf.clip_by_global_norm(grads, 5.0)
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))
        self.compiled_metrics.update_state(target, prediction)
        return {metric.name: metric.result() for metric in self.metrics}

    def call(self, input):
        batch_size = tf.shape(input)[0]
        input = tf.cast(input[:, : -self.frame_size, :], tf.float32)
        frames = tf.reshape(input, [
            batch_size,
            tf.shape(input)[1] // self.frame_size,
            self.frame_size
        ])
        num_steps = tf.shape(frames)[1]
        frames = self.input_expand(frames)
        hidden = self.rnn(frames)
        output_shape = [
            batch_size,
            num_steps * self.num_lower_tier_frames,
            self.dim
        ]
        outputs = tf.nn.conv1d_transpose(
            hidden,
            self.upsample,
            strides=self.num_lower_tier_frames,
            output_shape=output_shape,
        )
        return self.out(tf.transpose(outputs, perm=(0,2,1)))

##################### TRAINING #####################

#files = find_files('path/to/wavs') # Leave this as is, no audio needs to be loaded
train_dataset = get_dataset() # This hangs on the HTC, but get_dataset_NO_HANG does not...

model = TestModel()

opt = tf.optimizers.Adam(learning_rate=0.001, epsilon=1e-4)
compute_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')
model.compile(optimizer=opt, loss=compute_loss, metrics=[train_accuracy])

model.fit(
    train_dataset,
    epochs=NUM_EPOCHS,
    steps_per_epoch=500,
    shuffle=False
)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

No error messages are produced.

I submit a job using `sbatch <my_run_script.sh>` on the login node, and the system then runs the code on a GPU node. I don't have enough access to the Slurm cluster where I am running my code to investigate what be going on behind the scenes, and I don't have enough knowledge/experience of such systems to be able to hazard a guess (I know that the login node in the cluster runs CentOS... I run my code under Anaconda, with TF 2.2.0, also tried TF 2.3.0 but same issues). So I have no idea what's going on, it's quite baffling. It does seem to be related to the above code, however.
"
43655,A new feature in tf.roll,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
Tensorflow 2.1.0
- Are you willing to contribute it (Yes/No):
Yes


**Describe the feature and the current behavior/state.**
Currently, tensorflow only allows for same shift in the same dimension. I want different shift ability

[[1,2,3,4],
[1,2,3,4],
[1,2,3,4]] 
can be only turned into 
[[2,3,4,1],
[2,3,4,1],
[2,3,4,1]] in the current iteration. 
I need a roll function that can get this without using any for loop, I also need it for any dimension tensor , i.e. 4 dimension that I work on. 
[[2,3,4,1],
[3,4,1,2],
[4,1,2,3]]

**Will this change the current api? How?**

Yes by adding a property. 

**Who will benefit with this feature?**

People who work on system biology using tensorflow for transcription data. A great deal of this will be used in this paper which I authored. I want to expand it to make it more flexible. 

https://academic.oup.com/bioinformatics/article/36/Supplement_1/i499/5870526?itm_content=bioinformatics&itm_source=trendmd-widget&itm_campaign=trendmd-pilot&itm_medium=sidebar

**Any Other info.**
Last time, it was closed using a replace method but it only solve the example problem I give, not the full problem. "
43654,can't compile tensorflow from source,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3.0
- Python version:3.8.2
- Installed using virtualenv? pip? conda?: pip 
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source):9.3.0
- CUDA/cuDNN version: 10.1/7
- GPU model and memory: 1070 ti



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
i can't compile tensorflow from source, i tried to use different versions of cuda and cudn, i tried to compile without using cuda libraries, but nothing works. I do not know what I am doing wrong, I follow the instructions https://www.tensorflow.org/install/source, but I keep getting the error

**how i write the config file**
dmitry@future:~/tensorflow$ ./configure
You have bazel 3.1.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.8/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Found CUDA 10.1 in:
    /usr/local/cuda-10.1/targets/x86_64-linux/lib
    /usr/local/cuda-10.1/targets/x86_64-linux/include
Found cuDNN 7 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: 


Do you want to use clang as CUDA compiler? [y/N]: y
Clang will be used as CUDA compiler.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify which clang should be used as device and host compiler. [Default is /usr/bin/clang]: 


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: -march=native -mno-avx


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished


**here is the output of the compilation process**
dmitry@future:~/tensorflow$ bazel build --config=cuda [--config=option] //tensorflow/tools/pip_package:build_pip_package
WARNING: The following configs were expanded more than once: [cuda_clang, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=236
INFO: Reading rc options for 'build' from /home/dmitry/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/dmitry/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /home/dmitry/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/bin/clang --config=cuda_clang --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /home/dmitry/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/dmitry/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /home/dmitry/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda_clang in file /home/dmitry/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_clang=true --define=using_clang=true --action_env TF_CUDA_CLANG=1
INFO: Found applicable config definition build:using_cuda in file /home/dmitry/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:cuda_clang in file /home/dmitry/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_clang=true --define=using_clang=true --action_env TF_CUDA_CLANG=1
INFO: Found applicable config definition build:using_cuda in file /home/dmitry/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:cuda in file /home/dmitry/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file /home/dmitry/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:linux in file /home/dmitry/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/dmitry/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
ERROR: Skipping '[--config=option]': no such target '//:[--config=option]': target '[--config=option]' not declared in package '' defined by /home/dmitry/tensorflow/BUILD
ERROR: no such target '//:[--config=option]': target '[--config=option]' not declared in package '' defined by /home/dmitry/tensorflow/BUILD
INFO: Elapsed time: 0.267s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package

I really don't understand what the problem is, because I tried to do everything according to the instructions. I would be very grateful if you could help me. My thanks
"
43653,tensorflow.keras.backend.dot does not work as expected when second argument is 3-dimensional or higher,"# System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
    - Yes (but reproducing this bug requires very minimal code)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
    - Manjaro Linux (rolling release)
- TensorFlow installed from (source or binary):
    - binary
- TensorFlow version (use command below):
    - git version: v2.3.0-rc2-23-gb36436b087 (2.3.0)
- Python version:
    - python 3.8

# Describe the current behavior

tensorflow.keras.backend.dot does not compute dot product along expected axes. It computes dot product along the last axis of the first argument and along the next-to-last axis of the second argument.

# Describe the expected behavior

I expect dot product to be computed along the last axis of the first argument and the first axes of the second argument. (Note that this only differs from the current behavior if the second argument is 3-dimensional or higher.)

# Note

This behavior is not incorrect *per se*, rather it is unconventional and unexpected and therefore it can cause extremely mysterious errors. At the very least, this behavior should be included in the documentation (it is not currently documented) with a few more examples to illustrate. Perhaps this is the best solution to avoid breaking the code of people who have figured out this subtlety and used it extensively.

# Standalone code to reproduce the issue

### Example 1:
```
import tensorflow.keras.backend as K

x = K.placeholder(shape=(1, 2))
y = K.placeholder(shape=(2, 3, 4))

print(K.dot(x, y))
```
I expect this to print a placeholder tensor of shape `(1, 3, 4)`, but instead, I get a long traceback, the last line of which is:
```
ValueError: Dimensions must be equal, but are 2 and 3 for '{{node MatMul_3}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Reshape_7, Reshape_8)' with input shapes: [1,2], [3,8].
```
(see below for full traceback)

### Example 2:
```
x = K.placeholder(shape=(1, 2))
y = K.placeholder(shape=(3, 2, 4))

print(K.dot(x, y))
```
I would expect this to raise an error because the last dimension of `x` does not match the first dimension of `y`. Instead, this prints a placeholder tensor of shape `(1,3,4)` indicating that the dot product was computed along the last axis of `x` and the next-to-last axis of `y`.

### Example 3:
```
x = K.placeholder(shape=(1, 2))
y = K.placeholder(shape=(3, 4, 5, 6, 7, 2, 8))

print(K.dot(x, y))
```
Again, I expect this to raise an error, but the dot product is computed without complaint.

# Other info / logs

Traceback from Example 1 above:
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs, op_def)
   1811   try:
-> 1812     c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
   1813   except errors.InvalidArgumentError as e:

InvalidArgumentError: Dimensions must be equal, but are 2 and 3 for '{{node MatMul_5}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Reshape_12, Reshape_13)' with input shapes: [1,2], [3,8].

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-34-711261b03ba4> in <module>
----> 1 K.dot(x, y)

~/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/.local/lib/python3.8/site-packages/tensorflow/python/keras/backend.py in dot(x, y)
   1825         array_ops.transpose(y, perm=y_permute_dim), [y_shape[-2], -1])
   1826     return array_ops.reshape(
-> 1827         math_ops.matmul(xt, yt), x_shape[:-1] + y_shape[:-2] + y_shape[-1:])
   1828   if is_sparse(x):
   1829     out = sparse_ops.sparse_tensor_dense_matmul(x, y)

~/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/.local/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)
   3252       return ret
   3253     else:
-> 3254       return gen_math_ops.mat_mul(
   3255           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
   3256 

~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py in mat_mul(a, b, transpose_a, transpose_b, name)
   5638     transpose_b = False
   5639   transpose_b = _execute.make_bool(transpose_b, ""transpose_b"")
-> 5640   _, _, _op, _outputs = _op_def_library._apply_op_helper(
   5641         ""MatMul"", a=a, b=b, transpose_a=transpose_a, transpose_b=transpose_b,
   5642                   name=name)

~/.local/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)
    740       # Add Op to graph
    741       # pylint: disable=protected-access
--> 742       op = g._create_op_internal(op_type_name, inputs, dtypes=None,
    743                                  name=scope, input_types=input_types,
    744                                  attrs=attr_protos, op_def=op_def)

~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)
    589       inp = self.capture(inp)
    590       inputs[i] = inp
--> 591     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access
    592         op_type, inputs, dtypes, input_types, name, attrs, op_def,
    593         compute_device)

~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)
   3475     # Session.run call cannot occur between creating and mutating the op.
   3476     with self._mutation_lock():
-> 3477       ret = Operation(
   3478           node_def,
   3479           self,

~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)
   1972       if op_def is None:
   1973         op_def = self._graph._get_op_def(node_def.op)
-> 1974       self._c_op = _create_c_op(self._graph, node_def, inputs,
   1975                                 control_input_ops, op_def)
   1976       name = compat.as_str(node_def.name)

~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs, op_def)
   1813   except errors.InvalidArgumentError as e:
   1814     # Convert to ValueError for backwards compatibility.
-> 1815     raise ValueError(str(e))
   1816 
   1817   return c_op

ValueError: Dimensions must be equal, but are 2 and 3 for '{{node MatMul_5}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Reshape_12, Reshape_13)' with input shapes: [1,2], [3,8].
```"
43651,"Error ""Cannot iterate over a tensor with unknown first dimension"" when using TF Keras Attention Layer.","Hello,
I'm trying to implement a Text Summarizing model Seq2Seq model with Attention layer, while building the layer, I'm getting the above error exactly at Attention Layer which is imported from TF Keras Layer.

Code Section:
`# Decoder
decoder_input = Input(shape=(None,))
decoder_embed = Embedding(y_vocab_size, embedding_dim, trainable=True)(decoder_input)

decoder_lstm1 = LSTM(300, return_state=True, return_sequences=True, dropout=0.4)
decoder_lstm_output, decoder_h, decoder_c = decoder_lstm1(decoder_embed, initial_state=[encoder_h, encoder_c])

att_output, att_state = Attention()([encoder_output, decoder_lstm_output])
decoder_concat_output = Concatenate(axis=-1)([decoder_lstm_output, attn_out])

Error:
TypeError                                 Traceback (most recent call last)
<ipython-input-13-a3d0f9f7e616> in <module>()
     25 decoder_lstm_output, decoder_h, decoder_c = decoder_lstm1(decoder_embed, initial_state=[encoder_h, encoder_c])
     26 
---> 27 att_output, att_state = Attention()([encoder_output, decoder_lstm_output])
     28 decoder_concat_output = Concatenate(axis=-1)([decoder_lstm_output, attn_out])
     29 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __iter__(self)
    510     if shape[0] is None:
    511       raise TypeError(
--> 512           ""Cannot iterate over a tensor with unknown first dimension."")
    513     return _TensorIterator(self, shape[0])
    514 

TypeError: Cannot iterate over a tensor with unknown first dimension.`

Can anyone please help me on this? I'm sharing the link to Colab notebook as well. Thanks!

Google Colab Notebook Link:
https://colab.research.google.com/drive/1EvxOwIH8yeqS7pRJh9aNF-XXoj0N0uQw?usp=sharing"
43650,Custom loss function is not working,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 20H2
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.3.0
- Python version: 3.8.3

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1wa93OGIfXVY66GHO6wI1O9PnZyxXB-KJ

**Describe the behavior**
I am implementing the PPO algorithm using Keras but encountered the following issue related to the custom loss function in Keras.

**Error message:**
```bash
Traceback (most recent call last):
  File ""C:\Users\dhyey\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  u/tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: old_prediction_input:0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 426, in <module>
    agent.train()
  File ""train.py"", line 370, in train
    actor_loss = self.actor.fit(
  File ""C:\Users\dhyey\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""C:\Users\dhyey\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1098, in fit
    tmp_logs = train_function(iterator)
  File ""C:\Users\dhyey\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\dhyey\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\def_function.py"", line 840, in _call
    return self._stateless_fn(*args, **kwds)
  File ""C:\Users\dhyey\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\function.py"", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""C:\Users\dhyey\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\function.py"", line 1843, in _filtered_call
    return self._call_flat(
  File ""C:\Users\dhyey\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\function.py"", line 1923, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""C:\Users\dhyey\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\function.py"", line 545, in call
    outputs = execute.execute(
  File ""C:\Users\dhyey\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\execute.py"", line 72, in quick_execute
    raise core._SymbolicException(
tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'old_prediction_input:0' shape=(None, 2) dtype=float32>, <tf.Tensor 'advantage_input:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'reward_input:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'value_input:0' shape=(None, 1) dtype=float32>]
```

**Check the custom loss function [here on Colab](https://colab.research.google.com/drive/1wa93OGIfXVY66GHO6wI1O9PnZyxXB-KJ#scrollTo=DIoqurd3uDb1)**

**Check the actor model [here on Colab](https://colab.research.google.com/drive/1wa93OGIfXVY66GHO6wI1O9PnZyxXB-KJ#scrollTo=rTrPW6F50TbY)**

So after searching I found one work around i.e to add `run_eagerly=True` to the model.compile() method as: `actor_model.compile(... , run_eagerly=True)`

But after applying run_eagerly to true, I am getting 0 loss value from `actor.history['loss']` and to debug this I am not able to print the total_loss value in the ppo_loss(...) function because it gives the `AttributeError: 'Tensor' object has no attribute 'numpy'`.

"
43649,Unable to convert simple LSTM keras model to tflite ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (or github SHA if from source):


**Command used to run the converter or code if you’re using the Python API**
Colab link: https://colab.research.google.com/drive/14HxDxImtW769ldxblyowXulKc7_Poi1G?usp=sharing

# Copy and paste here the exact command
```python
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

def build_model(input_shape):
    X_input = layers.Input(shape = input_shape)

    X = layers.Conv1D(128, 7, strides=4)(X_input)                           
    X = layers.BatchNormalization()(X)                                 
    X = layers.Activation('relu')(X)                                 
    X = layers.Dropout(0.5)(X)                                 

    X = layers.LSTM(64, return_sequences=True)(X)                                
    X = layers.Dropout(0.5)(X)                           
    X = layers.BatchNormalization()(X)                                
    
    X = layers.LSTM(64, return_sequences=True)(X)                              
    X = layers.Dropout(0.5)(X)                                
    X = layers.BatchNormalization()(X)                                
    X = layers.Dropout(0.5)(X)

    X = layers.TimeDistributed(layers.Dense(1, activation = ""sigmoid""))(X) 

    model = keras.models.Model(inputs = X_input, outputs = X)
    return model

model = build_model((1000, 100))
model.summary()

x = np.random.rand(10, 1000, 100).astype(np.float32)
y = np.random.randint(2, size=(10, 249)).astype(np.float32)

model.compile('adam', 'binary_crossentropy')
model.fit(x, y, epochs=1)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
```

**The output from the converter invocation**
```
INFO:tensorflow:Assets written to: /tmp/tmpkt6586gh/assets

INFO:tensorflow:Assets written to: /tmp/tmpkt6586gh/assets

---------------------------------------------------------------------------

Exception                                 Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    198                                                  debug_info_str,
--> 199                                                  enable_mlir_converter)
    200       return model_str

6 frames

Exception: <unknown>:0: error: loc(callsite(callsite(callsite(unknown at ""functional_7/lstm_13/PartitionedCall@__inference__wrapped_model_240475"") at ""StatefulPartitionedCall@__inference_signature_wrapper_247244"") at ""StatefulPartitionedCall"")): We cannot duplicate the value since it's not constant.

<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: note: loc(callsite(callsite(callsite(unknown at ""functional_7/lstm_13/PartitionedCall@__inference__wrapped_model_240475"") at ""StatefulPartitionedCall@__inference_signature_wrapper_247244"") at ""StatefulPartitionedCall"")): see current operation: %10 = ""tfl.unidirectional_sequence_lstm""(%5, %cst_19, %cst_20, %cst_21, %cst_22, %cst_11, %cst_12, %cst_13, %cst_14, %cst_6, %cst_6, %cst_6, %cst_15, %cst_16, %cst_17, %cst_18, %cst_6, %cst_6, %9, %9, %cst_6, %cst_6, %cst_6, %cst_6) {cell_clip = 1.000000e+01 : f32, fused_activation_function = ""TANH"", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<?x249x128xf32>, tensor<64x128xf32>, tensor<64x128xf32>, tensor<64x128xf32>, tensor<64x128xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, none, none, none, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, none, none, tensor<?x64xf32>, tensor<?x64xf32>, none, none, none, none) -> tensor<?x249x64xf32>
<unknown>:0: error: Failed to duplicate values for the stateful op

<unknown>:0: note: see current operation: ""func""() ( {
^bb0(%arg0: tensor<?x1000x100xf32>):  // no predecessors
  %cst = ""std.constant""() {value = dense<9.9943357E-4> : tensor<1xf32>} : () -> tensor<1xf32>
  %cst_0 = ""std.constant""() {value = dense<""0xE30BE3B458690BB4A3B6ACB5D7BB263456F1E83527D417B745E0B1319E918E35717B2034BADFF2B5BC71DA36C9BEA6357FEAB834E78A50363F337B34D9EF64B4243D06B6D7ED4D362185EBB5851C1EB2DF5C6235448401B64B6CBF351CE0B435577819B5A3C8B9B5BAE50FB6EAAE85B4296D0D37742E8636472DA93410A839B6B0F3AA3545ED9DB55EABA73528FE6935C57C20B521FCCAB6146D6F35B1C9E3358C367935A6835C362040E9B5305A5A3675D79B36CCD3E8B5F7D83134F3FCC73558A42E367F36F335F08B0FB62B3B19B74367E4B5A1900EB72DF76FB5304B5C3496D04A357B7F97B6FBEA86348AC3333573A2B9B5B92C32363C003D36A47F273632596D34966B3DB57958B3B70D0C39B6C66FA9363016AB35DAD79835A23DA6B6F3E29E3664AC87B5A2592BB68AC90834D6331835B9B600B54DCBDE341AD586B6189BD2B413F0A6B5B607D3B47F7BACB5ED4D0A361A575436845BD6B6B54B1CB77C09C8B40B3FFAB59DDAEC32E65F85B4081C2DB75E45B836379ED23527C9BF36C94CA336A5C433B614DAC036A6E308B51BD9CE363BA05B3513036035681B1EB3D27FE3B5C7E7A3B5FC8B99B60D8705364DFF6A36E1A98DB5BCA14736CA373AB645C5F3B5F9ED09B6B64C03B7B5838A35DC3846B437DD33B6ECDE2F36C14FBEB50FB1B3B566EE05360741D83504D09E353793CA35265F0A365235DBB59A810935""> : tensor<128xf32>} : () -> tensor<128xf32>
  %cst_1 = ""std.constant""() {value = dense<-3> : tensor<i32>} : () -> tensor<i32>
  %cst_2 = ""std.constant""() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<f32>
  %cst_3 = ""std.constant""() {value = dense<64> : tensor<i32>} : () -> tensor<i32>
  %cst_4 = ""std.constant""() {value = dense<[-1, 64]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_5 = ""std.constant""() {value = dense<[-1, 249, 1]> : tensor<3xi32>} : () -> tensor<3xi32>
  %cst_6 = ""std.constant""() {value} : () -> none
  %cst_7 = ""std.constant""() {value = dense<""0x4868803FB466803F4AA9803F79A8803F6BA8803F6FA9803FB8A9803FEF67803F40A6803FB567803F0F68803FD0A8803FBB67803F14A8803FD067803F28AA803F87A8803F96A8803F9F67803F40A9803FCFA8803F4A67803F3266803F71A9803F03A9803F8567803FF967803FDFA9803F49A8803FA2A8803F2868803FF0A9803FBF67803F0DA9803FFB67803FE867803FD3A8803F6EA9803F5DA9803FBD67803F4967803F1BA9803FD067803F1E67803FBA67803FE266803FCF66803FDE67803FBFA8803F9467803F1969803F7767803F2068803FEAA8803FF7A7803FA1A9803F13A8803F7B68803FECA1803FA0A8803F40A9803FE267803F2768803FD5A9803F3467803FC6A8803FA8A8803F3D67803FADA8803F1D67803F2E67803FC468803F32A8803F6168803FDBA8803F8967803FFB67803F1AAA803F9F66803FEE66803FBFA1803FA2A8803F6FA9803FF767803FE6A7803F5E67803FBF66803FEEA8803FEB67803FC8A7803F05A9803F6067803F47A9803FF267803FE366803F1368803F9C67803F9DA8803F7F68803F1568803F15A9803F3C67803FE1A8803FA367803F3568803FA6A9803F5768803F4767803FD5A8803FF466803FB867803F7667803F3667803F4466803FD767803F99A8803F53A8803F5967803F1BA9803F1BA9803FC367803FC0A9803F00A8803FADA8803F71AA803FB96D803FDC66803F6167803F""> : tensor<128xf32>} : () -> tensor<128xf32>
  %cst_8 = ""std.constant""() {value = dense<""0x160683BB18CC3ABCF0152FBBF47A0BBCCBE290BA473EB03B98DEB43B6E7DE8B9E0962BBB34720DBB8CA08A3B947205BB88E016394D3C5D3A369976BB0448A43A4068CDB9E0EE09BA7EBE473BD78709BC4126053C2EB5053A50CE7D3B4CC968BA2AD9B13A79CB9EBA586D723AAC9489BAC4398CBB14ABD5BAB6B728BB9C6525BB9161F7BA8983CD3BC35ECFBB2D9F9CBBF17F413AC03E83B9E90A02BB5DEC68BB034E65BBA1F7863BFA1E953AB95826BBA83343BB949F05BCC33118BC85B9C43A808605BA0088DDB67116D03B38E3B63B093E0CBBBC50E7BBC46B5C3A4C76DC3A7EE0F0BB66A0243A3E91483A6302453A0496793BBEF99A3A8D1BC4BB945DD739D443583944B28BBBBAF4E53BE6BCC2BB9875453BD01E8D39C44A8ABA5AA8C3B9549853BB217EDA3A346F8AB9A23B1CBB45BF413C62A405BA4C8D97395AEEAA3BFB4FE3BBF6160ABB8568B73AF16F10BB44E2823BDE8B12BB5B0602BC4BACBDBBD95B0D3CE2C34B3A952B8B3AD8C7A6BAE4C12B3B0AB69E3B9853533BB3BD063C623A11BBAD8DF33A4889993BC668D6BA8E00A53B4CDCB0BB89AE693BC4A2B2B90BFDDCBAA60B0CBCE3CA52BAA7CD98BBBAE736BB6E005E3B9E3B4DBC80ABE93BAA4E8C3B6BA7D33AE01EFEBBC409ED391A19A93A2C052FBAA54B14BC8C2D70BC58F684BABC128B39AE80803CEC36D2B97C6839BADB88663B08E706391CC5ABBB""> : tensor<128xf32>} : () -> tensor<128xf32>
  %cst_9 = ""std.constant""() {value = dense<[1.00473976, 1.00521708, 1.00263178, 1.00512218, 1.00516057, 1.00505733, 1.00508463, 1.00512767, 1.00468206, 1.00509381, 1.00515211, 1.00500977, 1.00460923, 1.00526345, 1.00518048, 1.00323498, 1.00313473, 1.00481927, 1.00307453, 1.00518346, 1.00318122, 1.00508034, 1.00511396, 1.00325751, 1.00530183, 1.00303495, 1.00524509, 1.00510859, 1.00507498, 1.00289106, 1.00500321, 1.00317168, 1.00510597, 1.00283813, 1.00500691, 1.00319803, 1.00302553, 1.00532532, 1.00289774, 1.00283813, 1.0031631, 1.00312734, 1.00510907, 1.00331628, 1.00476944, 1.0048629, 1.00319898, 1.00497484, 1.00326228, 1.00507808, 1.005170e+00, 1.00541019, 1.00510037, 1.00526011, 1.002900e+00, 1.00523686, 1.00262475, 1.00306499, 1.00506783, 1.00488198, 1.00327921, 1.00528109, 1.00526297, 1.00270772]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_10 = ""std.constant""() {value = dense<[0.00212347507, 0.00114128285, 0.00424089422, 1.03820697E-4, 0.0025341576, 0.00241684401, -9.20396414E-4, 0.00274362462, -6.54401258E-4, -5.62307425E-4, 1.24871498E-4, -0.00112991163, 0.00194295647, -7.7732536E-4, 0.00172411324, 0.00204211427, -6.12335338E-4, 3.90041969E-4, -5.3005293E-5, 0.00170475454, 4.90864331E-4, 0.00253311382, -3.48295085E-4, 0.00198506215, 9.72406648E-4, 2.20654067E-4, -7.47772981E-4, -8.11745994E-4, 0.00160141359, -0.00112064206, 9.69715358E-4, 0.00184346607, 0.00207264954, 0.00188718142, 0.00195737928, 0.00224771816, 0.00170390774, 5.08747587E-4, -0.00367517141, 0.00248319749, -0.00254705641, 0.00120391347, -2.9670808E-4, 3.36897792E-5, -8.59148916E-4, 0.00119491515, -1.53806293E-4, -1.64092053E-4, -9.07708308E-4, 0.00232663471, -0.00211252738, -0.00117885927, 0.00240447256, 9.73630347E-4, -0.00122551713, -0.00213340251, -2.57467618E-5, 2.70258111E-4, -0.002932237, -0.00138859439, -0.00181194942, -3.7918007E-4, 1.09911198E-4, 0.00112971466]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_11 = ""std.constant""() {value = dense<""0xA182193D6852173C04EF7BBDE9BE8A3D7CEA35BDCFE4B43D92E47D3D203D4CBDF34D47BEF58101BD743B16BD4DC6B0BCEB285FBDDA1B953DBD410BBE8276743DE01D883CA2DD9ABA644B8CBC566B79BCEC738DBD6DFBB23D88AF1E3D16DDF0BCF265EB3D9D6F423D33D63BBDB9250FBDDF2AA33A84F3A13AFABE08BEE40BD73DA8CECEBDCFDAE23C880CE13D2C98B83D1C7C1CBD205A33BDB1CC8F3D5302CABDDF2D263D0FC7F73C924A213B38AEC5BD83D60B3DDC5A903D8C1406BD96AFAC3DAC2C3FBDCAAADD3CBCAD7D3DF47FF0BD3BA8063D3C38D1BC5DD0283D50752E3DC32A87BD19ABBD3CD6CA69BCDC160DBD3CB404BDE0952ABCBD5BD5BC52ECA23DD24424BE36F8E13DDFD9353DCBAC7EBD952CAE3D72B62A3DFAE18EBC64C742BD09C7A6BD782D893DA82564BD76DB853D6F4C3D3D017768BC1F998A3C3D29253DA49FBFBBEACA8EBD78CB4EBDB967943D431578BD7BECF73D87C2083C3DF3CCBD331D8BBC45E580BD6536C03D4252713D076A5ABD1F541B3D6AA0BEBD3E48533D542121BCEFC6A9BC66504E3D31ED84BC06C0823DDFFA8B3BA4E8843DA9B56F3DF12592BD4A8FC83C41E5A33CE4AF413C26658CBDCC65D33C98A9A63D61A3A7BDB0DBA4BDCB8A1DBC5808323DC65F08BD3C8B993D766D323DA37EEF3D067076BC0E4C6CBC1051923DD8E361BC626F77BB0B762A3CE106D3BD39A465BC4A82B23DCBEE70BDCEAC883D097C8A3D6F62DF3C74B00E3D725B843D6EF454BDFCFE3B3C8A2AB63DD790053D75ECF13D21F274BD39D99ABBC924F9BCE27D19BD2BCEC63DF30222BE2107173D3D0BBEBC974E08BEBE8D2E3B393C5CBD896FA1BD45DA24BCE6AB33BBD54CA4BCF304853DEFE0973DE595643D593CA73CAA2DEFBDEF3AA13CF18759BD45EE3E3B107800BC9F800B3E4051833C39EFA23C0904D6BB154CA83D9DA2ACBC847EF93C71CC1ABD31BAB5BD9E910EBEC9966E3C48AC03BC521E3B3C721A4C3D455AE13C1CCDB2BCEABBA4BC6EE801BDDB...
  %cst_12 = ""std.constant""() {value = dense<""0x66A9D5BC396F00BED31B653CBEB2253EB266683DAA630A3DF348CEBCF921B7BD509439BD439542BAD13474BD50CF123EB550903DCC7231BD983BA93CF6F072BB6E887C3D1C737B3D5C8CB9BDEA58B7BCF5C9983D97F4A7BD0192D9BD4E499EBBE0C9653D1DD8D6BBC63A713C85216E3D0030273D52F0853DFC9D473CD1F2993D7BEF073D09E1EABC11318D3DBAEFD3BAF6F5003C8430E53D9FB23CBE15C8913DE8D51D3E97891EBD6F89113D38A9613DA190463CFC4416BDF6021DBD59C065BDFF96FA3C0708A43DEFFD0C3E4C58553D4A549F3BC421A13D5C81D43D121F8F3D6301DDBD5C3DAFBD68E7FBBC8FF3173D860B82BD5CA4813D2459C4BD27B28ABD2360FC3D99B2D93DEB55CF3DF8BD293D7379A63D1CF2803C04D155BDBB080BBDD78D923CDBCA91BC523C43BD2EA68CBCFAB694BD1DBD203E6412133D2CED05BEA8DB97BD997E86BDBC0975BDCCC16BBD4B03E2BD251DCFBC15767DBD60F5A03D3BA1513C4D6232BC4867D9BDBABA2F3D19B2BEBDB4954BBD04CA29BD5D0B78BCD756ABBC32D2C93C1FF7C53D200A42BD560120BBA03F1B3D8117733DFB1C733D33226DBD34D8DA3CD8BE07BE9B29D63B826181BD78DFB8BDAD7EE9BB5423ADBD9CB8C83C081CA23DBE7A573D307787BD8140D93DD924DFBCF5550CBE1D03513DACFEABBB88CFE2BCE2211C3D349F8FBC54DE90BDC9B06D3DCE33ADBD243C023D9738ACBABB02663C5C5EC5BAD8BC023CB6FFB23D5A9C43BCC95EDFBCA81ABC39B596AB3D7E972EBC836699BB42FBF7BC0EE007BCABA3443C2A66A23D3206B83D39AF383DC412C3BC690B83BCD4F0E63D4806D53C734E343C26C90CBD524036BD7EFE7EBDCAB4693C742EEB3DDC93E9BD299513BE850DC23C3EB5A0BD89AD64BEADBA753C9FC18EBDA89B80BCE4728BBDC3E4133DEAC54BBA7C4E47BD22C6C0BAE80E623CF8D8473D16C9B3BDA2950F3D9612A0BB3268D53D28B28FBD3B3BBFBD2A76723DE100AB3C47387E3CBF6718BE722E3E3BB0...
  %cst_13 = ""std.constant""() {value = dense<""0x89C41CBCAF8AC2BD457EC23CD0CC16BCDD6A2E3E4B478DBC4A2143BDE0E4163DC250BE3D7AD2E63C0B11D0BCD79A753D854C7EBD3B4937BD391179BD91CD3ABCA9A6CE3C81A82DBD882EB5BD84F241BD6FE39C3B3454B3BCFC44D63B201256BBCFBA66BC47A1433CBDA498BD270A4DBCC07E54BD9E16AABDBE61383D0DE75A3DEDE8183CAB07A33CCF79503C688DFC3DC0CED7BC1849B13D867A0C3D8D5ACF3B5D2397BCDC9714BDB08E7E3C010337BD73C62A3DAEB2B33DD24E80BA17078FBC6D4424BC209F7E3CC06D473D2241CDBCC26B54BDC6A5A83D9C7856BDE09ABD3C85FDEFBDA4D7833C6915F23D001EEDBDA773F13CDB9F9DBD7380B1BDCFEB073D93BF32BD5F6775BD76C3AA3CA6E285BD7FC16B3C72BCDBBB19CB11BE82DBA73D4B2E753D8FB6413D5B9105BDDA452C3D15C974BDA361F83DB88388BD1448BBBD13E0863D9FA8093E9FFE50BD041B82BD121C153EE47D033CAEBC623D206BCDBCE86DBB3DEC3C0F3DC3B0EBBABB11F8BCDF9FF53DABC904BDB8FCBDBCBEC815BD9C355DBDEE006B3D1F62B9BC2C97CF3BEEEC20BDF51B943B98698E3D29FDD03DB7095B3D004BFDBC496E013E0AB17BBB7DF4DC3C0D276D3D541044BB9E114D3DB58D703B0BC2A03B298C37BDC529E13CF947C03D842C943D51F332BDFFE530BCC7E1FABC9FB590BDF217F5BD5529DB3C3CED813D353E043D6CE673BDD2552DBDD2AD353C518714BCF721C3BCE40CEDBD31E215BDFD2EA33CB41004BD53159BBC96A9E2BD6222D3BDD0096CBD1CF7A93C2D2D30BC1E152BBD6DBE24BD4CE7B1BDE1CFA1BDE07E053EBA5A2F3D6603EBBC1C6BD03B3597A9BD3208B8BD86C3193DBFF3933D0739263D661E893DE212173EAE038CBD400913BD8B40353DAACA84BD0532403E6E5636BCC8ECF6BCB1B4E53D0BB02FBDA03DAB3D36EB00BDFC06E5BDFF78A73CF7FC72BD351988BDEB09BF3C1C5F47BD7861893C3273A6BC8D118ABAC607C63DF3B7FBBDAB3D0A3D2BFCE8BD9E04AEBD0F...
  %cst_14 = ""std.constant""() {value = dense<""0x6D7FE53B90D80ABB900637BD90E9CABC41E959BBC4683A3EF659273CDAB79B3B7277E6BD9680A3BC06664CBD4495EF3C78CB36BCCBEE783DE7233CBDE6E06C3DC4E310BC40B12F3DEEED48BDEF40503D9182B5BCCCE6443CC09188BD2A2999BDCD20EFBCDAFE4EBCA274FC3B446CC03B85408ABDB62E763DB49CE1BCCFCC7A3D2D801E3D624E3D3C00EC92BD55E9B2BC7D302ABD8F380DBD9CD3813D47EBCB3D977EF0BD3ED69D3DA41AA13CC517C63CD524CCBCA81C313D21F81F3DB2BD113D837E02BCDADD843D932C763D4FEF5D3DDEAB18BE12D212BC3B738EBD7B78C4BC6F65353EAD2506BE0FAA3BBCC8604B3DF5740A3D0D51C23D0084E1BD0A42DB3C9FEE8C3B93908BBD930453BDAFD70A3EB82CAC3C2D97083E656B03BD2E28033C25B813BDE6E4DE3D129D213D32C1A2BCD7B222BD972851BD34FD8B3DDA896BBC1EA2803C456AC73C9E63763DCD31DBBC3DFAC4BDB4A13E3DAFB8033D64513A3D8F72343E05EC913D109BA2BCA691A8BD0A49143B3513213DEE201CBC7A9B993B7745F23C686CC4BD9270853C8613F4BDAAA698BC8C5B2DBE3FC999BD8AFA8CBC99B65ABDBA5AAABCA0028A3D9635423D2656C03C37AC0B3D3992013EB451B8BCE33ED03D25F2B0BDE2D507BD65A05F3DD0EDA53D5D1B333BC79CC3BDB4DD04BD7D3116BBE4606D3DD404203DF22C133DB880BCBCDBF5D4BD7D9E8ABC2CEB9E3D04D5CABC95F8343DA6DAC73D7725F1BDB6ADA93B57E9BB3C9F0D9CBC409ACBB7E5088DBB42502EBC4EEAA3BC6EE8283C188C683C916A223DA2FF0ABDC0FD8F3CE56611BCBD8D97BD4628443D928C95BDEA581ABC0D16D63D2E85843C05A28BBC40538437626B323C9A7127BEAE940F3D8556A53C14C3533B1F17993DD2B3E5BD851BC3BD980643BD8980603CD9625A3D0072C1BC279913BD045D55BDE0D406BD55C56BBDA6EBF13C748ECDBC907E77BD0490C23D3C8B923C148F323DEC107A3A1CF4C43D9B2FBE3D556331BD0E6CEA3B5A54833D8B...
  %cst_15 = ""std.constant""() {value = dense<[-9.8901696E-4, 8.5479155E-4, 9.79730626E-4, -9.92890913E-4, -9.96473943E-4, 9.94842383E-4, -9.81436577E-4, -9.86810191E-4, 9.76143696E-4, -9.95050533E-4, 9.96386515E-4, 9.97202587E-4, 9.95036563E-4, 9.89387161E-4, 9.55004477E-4, -9.91261797E-4, -9.74753813E-4, -9.9344307E-4, -9.93789755E-4, -9.86392376E-4, 9.69534274E-4, -9.80411772E-4, 9.94408153E-4, 9.93013498E-4, 9.97127848E-4, -9.42781102E-4, -9.70873865E-4, 9.89013817E-4, 9.76190961E-4, 9.85609483E-4, -9.92171582E-4, -9.76951909E-4, 9.94690228E-4, -9.26485285E-4, -9.87045699E-4, -9.95974405E-4, 9.95763926E-4, -9.95861366E-4, -9.9247531E-4, 9.89652355E-4, -9.91960754E-4, -9.92493587E-4, 9.88994841E-4, -9.96514922E-4, 9.97994909E-4, 7.89895537E-4, -9.94454952E-4, 9.82676516E-4, -9.95628302E-4, -9.84977814E-4, 9.91726177E-4, 9.94249363E-4, -9.90893808E-4, -9.91686829E-4, -9.92248067E-4, 9.85561753E-4, 8.73027078E-4, 9.94159607E-4, 9.96555201E-4, -9.95308626E-4, -9.89207183E-4, 9.23537358E-4, -9.56159958E-4, 9.97224473E-4]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_16 = ""std.constant""() {value = dense<[1.00099027, 1.00098932, 1.00099516, 0.999001443, 0.999003052, 1.00099671, 0.999013066, 0.999002039, 0.999003827, 0.999007761, 0.999003648, 1.00099814, 1.00098109, 1.0009973, 0.999003589, 0.999007225, 1.00099742, 1.00099838, 1.00099516, 1.00099599, 0.999002635, 1.00093877, 1.00099826, 0.999011039, 1.00098026, 0.999085307, 0.999004065, 0.999006569, 1.00098586, 9.990090e-01, 0.999005258, 0.9990049, 1.00098383, 0.999015271, 0.999016046, 9.990040e-01, 1.00099528, 1.00099838, 0.999001204, 1.00099707, 1.00098884, 0.999010801, 1.00098121, 0.999003827, 1.00099885, 0.999003291, 0.9990139, 0.999089419, 0.999002397, 0.999002635, 1.00099599, 1.00099051, 1.00099814, 0.999006092, 1.00099432, 0.999023616, 1.00092232, 1.0009979, 1.00094688, 0.999000966, 0.999003708, 1.0009973, 0.999011337, 1.00099313]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_17 = ""std.constant""() {value = dense<[-9.96331218E-4, 9.9947548E-4, -9.99390729E-4, -9.99690964E-4, 9.99050796E-4, 9.99103183E-4, 9.98863601E-4, 9.99490497E-4, 9.99209471E-4, -9.99391428E-4, 9.99218085E-4, 9.99209703E-4, 9.80420154E-4, -9.99758602E-4, 9.98847535E-4, -9.99662093E-4, -9.99377341E-4, -9.989780e-04, 9.99162555E-4, 9.99477691E-4, -9.99286537E-4, -9.98504692E-4, 9.99714247E-4, 9.97304683E-4, 9.9933322E-4, -9.93805122E-4, -9.5772784E-4, 9.99755342E-4, -9.997870e-04, 9.98014817E-4, -8.64997389E-4, 9.97377792E-4, -9.99638345E-4, 9.9680887E-4, -9.98852308E-4, -9.99114126E-4, -9.99839277E-4, 9.98401781E-4, -9.99705167E-4, -9.99090261E-4, 9.95945185E-4, -9.99628449E-4, -9.98933333E-4, 9.98956267E-4, 9.99524607E-4, 9.99682699E-4, -9.98335075E-4, 9.99589916E-4, 9.97847644E-4, 9.99562675E-4, 9.99166746E-4, 9.97601891E-4, -9.99633572E-4, 9.98750911E-4, 9.99411451E-4, 9.98528324E-4, 9.96360555E-4, 9.99749638E-4, -9.9872041E-4, -9.99665353E-4, -9.9975022E-4, -9.98956849E-4, 9.98632749E-4, 9.98964183E-4]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_18 = ""std.constant""() {value = dense<[-9.969270e-04, -9.82557306E-4, -9.93291963E-4, -9.90343512E-4, -9.97934374E-4, 9.90803935E-4, -9.84228565E-4, -9.96587565E-4, -9.97572089E-4, 9.795690e-04, 9.94196743E-4, 9.95259266E-4, 9.97718772E-4, 9.96876275E-4, 9.75065515E-4, -9.9695567E-4, -9.92954708E-4, 9.90913482E-4, -9.94683359E-4, 9.95306298E-4, 9.89426276E-4, 9.78857278E-4, 9.94338188E-4, 9.96380113E-4, 9.88848274E-4, -9.76715586E-4, -9.89963999E-4, 9.82530764E-4, -9.80048673E-4, -9.93657624E-4, -9.90298925E-4, 9.94730857E-4, -8.47386254E-4, -9.94144822E-4, -9.24524327E-4, -9.96671849E-4, -9.95290349E-4, 9.90399857E-4, 9.92561923E-4, 9.9778769E-4, -9.91268665E-4, 9.9106389E-4, 9.91855398E-4, -9.95182781E-4, 9.90162254E-4, -9.9000032E-4, -9.91538865E-4, 9.92999994E-4, -9.93227935E-4, 9.92302317E-4, 9.95503971E-4, 9.89457941E-4, -9.93986148E-4, -6.98388088E-4, -9.90565284E-4, 9.86032304E-4, 9.90497064E-4, 9.35149088E-4, -9.74465802E-4, 9.96848801E-4, 9.82844852E-4, -9.86666884E-4, -8.83933331E-4, 9.90752945E-4]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_19 = ""std.constant""() {value = dense<""0x5344EF3D6F3B673DAA17D2BCFC80D8BB3A659CBD8BDFABBC20C0E43DC24936BCDCA37A3B048FED3CDC876ABDA432D13DDCA59E3DB15ADABCDD4EA03DD654DC3DC02DC43C2E2A8ABDB2939E3D157A04BD361F1F3CFA9E25BDF88F99BDE1A8D8BD795D84BDC0E7FF3CD640793D7D12E1BD4FADA1BC89F9AABC7645ABBD9E28CDBC7DD1F23CD3C3A5BD0FABD1BD0D9B10BC264CBBBDCDE8EA3C0B15CBBD4A7AF63CAA93AF3D986C5B3D808A6FBD9118FDBDBE8AB0BC2DC1393D1CF2EA3D28A8813B1464803D54D8B7BC1BDEF8BDECAEB13D6D3700BE0ADBDCBD43E14F3D21DBE0BD3F86E9BD0914A73D0E649ABDABE0CF3D915CB03DD4CD003E73D400BC34DFF4BDDDA5A03D5521DC3D21B2D83D7FA049BB99CA493C272F263C87B59B3D5F36D63DD436B43CCF74FE3C937FFC3D8504933D532B813DFF09CDBDE626283B4403893D5F3E71BD3B68CABC8913A53D60C6B1BC7953A5BB25F7FEBC1F508E3DF1B7D43D09231DBD7148EFBDDE25E5BDB49F323CE29DE63D939BB73C853EC4BD37AEAC3C1E5A9F3C38094D3B00D0F83DD5798EBD6D06193D194BEC3CD435E73DEE0315BC622BF53B9A9DDDBD6213CD3D41A660BD7B0BA4BDD4CBC8BD02643E3CC2A0143CD760DABDF6DFC23C9D6D2CBD8FB1B0BD9C19C33D08A4BA3D9350F03D0DFADEBDED66BEBD326683BCF92B8C3DBE63A13D97324DBD36B7CBBDE8340CBD7E6B9D3DD693B1BD7D1EAA3D2CCC7F3C5D209ABDC6B0613DF9EBE9BDD293023B9938A53DC8901EBD766BDA3D8AD7C53D0748EBBD1DDEA03D4011083B17847BBD2D5DBABD0266AF3CA5EA42BD3EB48B3CAE1193BCEDE8333DF9F55CBD9F432BBD85C7573D37DAB3BDC4A1953DD9384ABDB035D0BCAF5A7B3D1CEDFDBD8C4BABBDF96740BCF393D93DA8EBBABC70E1233CC840A33DBEE46DBD312427BD2D9E0ABCBA08C33D7C4C19BD66C8193D742CF0BC0F4459BD1BC9B8BD7920CFBD483BBB3D3FB86CBD7DC88CBB62F5CC3DD8D3BEBDE230003EB4F3263D4C...
  %cst_20 = ""std.constant""() {value = dense<""0xAD2AB8BDDE00973DAD58C4BCAAAA8ABD75A3EEBDAA08B0BD2D53A93D8504B23DDE2FB33BB3BE9FBD989925BC0180AB3DFAD7873C54A8E53C03E8A33D09130CBD58F704399614103D97E8D33DEFB35D3D758777BC79D024BD6DFE94BD24C8F93D476802BDD1F54F3C10553F3D81D7153CF91AC83D9EA319BDAC8620BDB427AABD9A4974BD1D738D3D025E99BC2F7E73BCDE4A403CDBC7BABDE06C773C89D2863DB5C5AA3D583BA8BBEB8C0FBD823FA1BDB832E83D32FCB23DA16C273D96B7B63DF29EF53C0889C73DED0C94BD6E02923D5A82663DE94CB23D2EF47FBD00AC25BD1AF43A3DEA7770BCF737F9BDFBF3373DE486933D909AC7BC5D79FBBB67AF9C3DBD84E7BD233CF23DA612F1BD8D3F1DBD92C2543CD17DA33DD4AF93BDE9BD973C8A8CCDBDE528353D0C4898BD07FCE8BD8D29BCBD5D1F85BDA61EBF3DE184433D9DD575BB95EDB5BD6E2A07BD5EBAFDBD48BDED3D72CF12BCD0A3D13CFCE0DBBD2229363C6CEFEFBDAC20813DBDAA803D3182AFBD7C68D83DFF424DBDD227C4BDFE7BE5BD4FCA4D3DADD4CF3DC6B5B63D3A019FBDC6BC983D3BAAB23D5777833D57B5FA3D37D6E23C3592783B9C3BB5BD6664CA3B78CF8A3D5FDED8BD6B1DCD3C19FA5E3D6A26D93DB1FC913DF855433B59E44F3DDF7182BD8BA8A93C0C70F7BD8C801A3D5206C53D2C47873DB2E6953C11F0AA3DAB8BFCBDBAA3CA3D78D9703DB89D6D3D19439DBD3AEDA23DF0168F3DC76907BD9F8898BDEF5EF6BD9114C53DD43CF23DF13FF23D0893DABCF51D41BD786C123D0B26A5BB19B2263D82838BBDA85EE5BD2690B63DBED5C5BA7C7564BD6E56A6BD19FCC03CF05475BD78E06A3D0C62BC3D4FE8B2BDD99C49BCD47A2E3C688BD53D489791BCF7B716BDA65771BD5EC69EBB2E4B56BCAC51E73D780214BCA4B4343D4C23B1BD05F8A23D4E41B53D1A62FB3C23DFA23D3A5BC0BD63B3DA3C1F69D3BD36DDA03DF7DDEFBC874AE1BDE2A3A33C4DFD1B3D6483773DF80AD43D3106B9BD5D...
  %cst_21 = ""std.constant""() {value = dense<""0x049BE93D7A68483DF5E4723D2580483D76C58E3DA721C63DB6EECABDA84A2BBCF3CA943D1D4856BD4EAB20BD0E74FD3C39EFCFBD859A1F3D8EFF293B871BE43DCC4AB0BDD950D1BD2424A2BCDA6E763DA485C63DAA94F6BD6FFF21BDC5581C3DEA3C0D3D12F634BD665424BC304417BD493C8B3D6746FD3DDBCAA5BDF32FF7BD041A3DBC652130BD3390CCBD082A4FBDC4B9333D68D6A0BD1347783DEB589C3C0E4B87BD3A9EBE3D740E363D79114C3D5C08AE3D4A43B03D6910A5BB2522F8BD2CD2423D79659F3D0008DBBCE94957BDB476DABD66AF84BD0A822DBD5C6C22BDCC74D1BD7CDDC3BD5D3379BD61FCD6BDF149C6BD6CF3C33BD54295BDFC3BE43A68EE5F3CE8EB1BBD0C06A6BDF506E03D95FBE1BD84B8BB3DFA45223DBAE941BDC375333DAA694CBD61B3B2BCDD252BBD665B703DDB02EFBDA3CEA0BDA04E74BD7247763D54AB52BC804867BDCA3EA53D10419DBDEA13593D1EEADA3DDA3D373CB72900BEC178A73C04140CBD56A1B5BCA10AD53D145FB3BD6CE6EE3C2FCD803DE885113DFFD7A4BC0D0229BD65DCDBBD2B7E32BDFADB68BDDBA3C8BD8BFB973DADF3A9BD6BA79CBDDE5289BD691E8BBD3CF32BBD94268E3DE057B8BD34A97BBDD728DBBDDD87CCBC3EC91D3DA4186E3C94F2E0BDD8C727BDE067E63DD96AD4BD02F7AEBD71CDA23B593330BD675072BDC80E00BD7280983D30DFDD3BEE3A873D5D1A6ABDEC6617BDAA5AD5BDFDD60B3DDFA5AA3D14E05FBD2DD79C3D3833BEBCCACCF8BC1701D7BD135BE23B047CD63D5275AB3D904B473BCD9EAB3DF5A0983CF2509C3D473985BB494FE23DB29A9F3DDCA8A0BC8FF44C3DF76AC9BDAD84F3BDCFF94B3DAC8EB23DB00033BD8D7314BD1E60DDBDC38BA4BC3CEB98BDFFBCAA3DD626C9BDF906153CBD87473D76B2CF3C5DE8003EE5CD2C3D9EF7F93CEE2DAC3D109A9DB98D1CB4BDE875F7BD4065D7BD1BF4973DD281083DEDB1D3BDCCECBCBB1A55D5BDDDFB39BD950AC33C31A1853C8628EEBB53...
  %cst_22 = ""std.constant""() {value = dense<""0x5D62F63D997CCD3D4BA877BC4A6FB63DC912423D2C82183D2FA383BDA72B963D6EFED3BD4D49B2BD771090BD3654F3BD4D75333DE2D6C7BBF5B6AABD53769F3C79F556BD9F1BEF3DC5ADD23D4C76EB3D5153AE3D7EBAD03D5BA1D6BC9D1D9FBDFE8C93BA4816E2BDC8DD823D6FCD063C1DBE0D3DE13DEBBD537A9B3DAD92E73DD896F6BDDDDA953D6BFF94BD0360E73DC79267BDEAC94E3D875070BD198A27BDE113E33DFBBD2F3D440148BC47577E3DD1FEA2BD59BA083D3163F03D8E7013BDF103B83DF1901FBD113002BC6713B5BD29AA893DA79F57BC415AEBBDB6D20F3A8472A43D4A5C6ABDF9EEBC3D9A28513D662064BD4A66983AA1D7DEBCB1F2713DD81F89BC22DEFDBC06E136BD32DD2D3D5F5C633D52CB75BD30F0D63D88682ABB6602B0BD018489BD4206E1BD0998A8BD65EFD1BD4D52FCBD471C953D15D42E3DBF46923DED88E53D3BA8C6BC9CA3DEBDD7A1E3BD967CAC3DF35E36BC13528CBDDC24EFBD2599EF3D42D14CBD50D1B0BD01C3D73DA0AEA4BD9517ED3C2ABD0E3D73E5F3BD5B13C8BDDC26ECBCABB5C73D9C64FF3D65C4203DA898E3BB3E24953D9C31B0BDABC9F4BD78ECC43DFCEDBEBD5B1DA4BD8DBE64BD22FFF4BBDBC6F93DB20AB03D3676A0BD6541E33D6D59BB3D43F4AEBDEFDB2CBD0FCC863D4BB24DBC6C4204BC418F6C3DB2F8F5BDD467E4BD581304BDE2BE863D3262053B88C2F9BDF115A0BD06ACB23D3664BD3D223975BBB4F3EABC375DCEBD1FA79DBB83CBF0BDF9D5D6BD4E315ABDB799A9BD46D955BD3144CD3D9BBA753C3B5FA1BD3E158EBDE4E3D9BDC771D3BBFA7AAABD7A36A7BD0AEC6C3C93158CBDABCFC2BD1EEEAD3D643FA73C9DBFDBBD2829D8BCEB47133C22B665BD01F2B4BDF9495FBD1044813C03C9B73CAFE6B8BD32C7CE3DD6C81DBA59A8A13DFA6BC13D5DA5D03C541B69BDF2C0F6BD7F0C6CBD1D150BBD1A04833CAB3B583D57C8F33D6022C53C1411993DE33D67BDD79517BC60ACF73DBE3194BD02C5F13C39...
  %cst_23 = ""std.constant""() {value = dense<[1.00309217, 1.00306129, 1.00305521, 1.00297964, 1.00312328, 1.0031054, 1.00307453, 1.00310564, 1.00313675, 1.00305128, 1.00307429, 1.00305736, 1.003070e+00, 1.0029887, 1.00302231, 1.00311744, 1.00306451, 1.00375688, 1.00299633, 1.0030781, 1.00298929, 1.00297678, 1.00304294, 1.003090e+00, 1.00315344, 1.0030973, 1.00511754, 1.00309551, 1.00313067, 1.00304472, 1.00306332, 1.00306582, 1.00493622, 1.00297606, 1.00302935, 1.00298393, 1.0030632, 1.00310445, 1.00310719, 1.00504351, 1.0031389, 1.00311208, 1.00314331, 1.00296211, 1.00310755, 1.00303924, 1.00302505, 1.00302935, 1.00308597, 1.00316334, 1.00303519, 1.00307107, 1.00308514, 1.00304747, 1.00304878, 1.00306785, 1.0030061, 1.00506783, 1.00308836, 1.00306702, 1.00507414, 1.00302899, 1.0030452, 1.00310063]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_24 = ""std.constant""() {value = dense<[0.00105503935, 9.57053503E-4, 9.19337617E-4, -0.00135998649, 9.4322121E-4, 0.00111438113, -8.29911325E-4, -9.97727853E-4, -8.51589313E-4, 0.00107562402, -8.04005772E-4, -0.00109937368, 0.00111207424, -8.78392602E-4, -9.8252925E-4, -0.00109786494, -9.11238894E-4, 8.16963788E-4, -9.54134739E-4, -0.00101923943, -0.001126813, 0.00118361623, 8.54798709E-4, 9.88998217E-4, 9.06906615E-4, -9.89600433E-4, -0.00116529281, 0.00105114921, 8.55009945E-4, 8.50873534E-4, 9.1296091E-4, -8.51154094E-4, 1.086330e-03, -9.83795617E-4, 8.56689236E-4, -0.00103523524, -9.80255194E-4, -0.00102012744, -0.00115606235, 8.70998425E-4, 9.38069541E-4, 9.53909824E-4, -0.0011939907, 0.00102742529, -8.92813433E-4, -7.36643909E-4, -8.27286276E-4, 8.32178513E-4, -8.53449397E-4, -0.00132732699, -8.82953347E-4, 6.98925112E-4, 0.00104335765, -8.36898805E-4, -9.04436689E-4, 8.80992854E-4, -0.00101167895, 8.2154182E-4, 6.41766353E-4, -8.16440617E-4, -9.5120765E-4, -9.956470e-04, 0.00102537207, -9.21818893E-4]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_25 = ""std.constant""() {value = dense<""0x57DB0CBD98589CBCF5952EBD0F15A3BDF67FB8BDCE40E2BDEF01AEBDE96A9A3BCCA6FA3D00BD52B7717D39BD7535073CA1190EBD86AFFB3C909A783D69947DBCE275DCBC741E1D3C7655B33B0FB10EBE24EC883D7EE41ABD3C77DABADFA302BD45EA86BC642234BCB7F5CB3D1BCBE13C40D2C7BC1404D23B18616BBC207E8FBADCDB2B3D319844BD13F903BE7F4B0A3EC031543D97FA703DD35ABC3CEA50A1BDFD1D9E3CC128573C33151ABD431AB1BDA7544BBC5E766D3D3913F0BD33A8613D0386053DAD2B93BCE34D3BBDDD0DB03C24A202BD1A7DD83D4D7429BD34C57ABDA91C8ABCAB0A9BBC692EF4BD51C18A3D3D669B3C58387C3CC4EB9DBD696F5C3D169C05BD4B6314BD342F143DF44D34BDAEC08B3D99EC033C8C0D0ABAB2E5EABD46A7CF3D36A99BBDCD3003BD0CE8643C6A7FD6BDCD81463CF1BDC43C996EB9BC51C262BB5D381A3DC11208BE4901533DB260B7BD7C11483DCB68BCBB5E504FBD1EEF8EBD2444953D4A354ABD754971BCDF1381BDEB8DAEBBB777513D9B84A8BCF75E80BD44854FBD6441D73C9E98923B9E6730BBF02B3EBD0B7E89BDD8A0C13C3E17A5BDB803A73D72259EBD8FF9C33D0EC2C5BD75751E3DC8CC28BDDE59B23D0A571DBB1729483DFCA694BCB318BFBD71E4ED3DEF78233CB63E7BBD8E5748BDBD76493CA9A88CBDBC63283DD12F7C3D47CF4BBD0DDFBE3DC5AA05BB1484A93D772E123E2D8BA0BD915DA4BD2F18943D316FDEBD4DAA35BD1B72033D4894193C52AB18BE504801BD9C6E133D8E2E0E3DF92B583D7D60E33D05D30C3DDA25A6BCCFDBF9BCFDBD66BC3C8CA73C5EC90FBD38E7243EE6995E3CE04A7E3D0FCAB6BDC18515BD130BC43CA1C52FBD23F1623D9880DEBAA5AF8F3CFAF62CBD61E162BC70BCE7BD05F25A3DEC0BC5BDA1A3E0BBA6DA98BD0AD3BDBB668398BD57A79A3C0F28523D2EDE593DDFA711BD610EAC3D507FF8BBC63F96BDCA47B33DDE4C103DEBE4D7BD2E3A85BDFFE20D3DE346C1BD2AD7BDBC16...
  %cst_26 = ""std.constant""() {value = dense<""0x243AB1BDCD79CEBD1CED5C3D886FB73DEFED813BECC66B3D0B1C16BD2794AD3D33965D3C6882053DF2AFD2BDED128E3AA4A181BD60E8C33D5AF85F3D9E10A23B1471F33C9AF99E3CFDF8B53D614642BCB90694BDD88B5DBDBA7DFD3DBD0BA13CB63220BDEDBB203D26BCF73CFC6CFDBC4264B9BDFD5B95BCE28F65BDC8C2F5BC9A49183EBA09133D9B0C77BBB24684BCAA8C35BCEAA130BD5AD285BD7EC060BDAD7FF93BB0FE163D2C4D20BD58C2F0BC087FFCBD683A603DC8B88CBD6CD782BDEE772BBD9462323DF1B340BC12E8433DDBDD63BD3477AEBD860FA83BFAC9B6BD7FD7753C73C5FD3CC6C709BE60C60E3DB51D013DAC0F2C3D17B884BC5887EEBCEF237DBD9B09E4BC733B2C3D1C80CEBCDEE455BD4F04FDBCAD1E49BC43871E3C1A8ECA3DD20E94BD3B710E3EE2C2FD3BBEC82DBDCAC50DBDFB34C7BC93F06EBC69CA8B3DC6C58F3C7C0BCFBD76B20DBC08282E3D3A339DBD1B8B82BC244C81BD9312BB3DED724EBD0821613D92F4703DF433EDBD68C207BA0A90F1BCE8950B3CC97FD13DCE253CBC6454BFBCE9EA0CBD6AE87ABC031D203DBCCC063DC7F4663DC837163C1902D93C2A18DBBBBE25AB3DF6A756BD34B55DBAC2BC1D3C3C5542BD5EAE673D9CFA2A3C3BECFC3D7E4D54BDD3A00BBD88B5B7BD2FADBE3CD4EF3FBD0A6B7B3DBF47533D3E78133D802185BD03D3B33DA96BAE3DCF705C3DD6D238BD72C20B3D664893BB63218DBB7DC039BCFEF558BB22AAFE3DF312C8BC013DAABC3F77383E099EF13CD4096F3C82F286BDED74873D9E9DFFBB9FCC9ABB697E723D156042BDF276DCBC13F5F03C64E581BD957C0F3DF01A12BB913E85BDEEE2A0BDD01E28BDF00A063D168982BDBAA9F13B0F2FAD3C65E7DABD67D2B3BD8AA73A3B0BF7DCBCB63A843D0F782B3EF9B1EBBCA744093DB28B73BD9A7BDE3D12D26CBD0AF738BC02665F3D664B3BBD7BC5F13DDFC2DFBA33FC24BC57261A3DDFE25D3D14BAEDBCC1F281BD93D46B3DC8CEC5BBEE0DE43C55...
  %cst_27 = ""std.constant""() {value = dense<""0xB7BA29BECA620D3D20AEDBBDCCBDDEBAD30E06BE26719EBC3934383CC1D7C9BD64AB3A3DBF6F17BDEBBEBBBD64B084BDDEE5BE3D1326AA3C0F96943DBA169C3DAE31EC3CAE130B3D6793F3BD3CA1CB3C70FC43BA30AB123C08815E3D23BE82BD79AE013D89C0B83C329594BD3ED8903DD68541BD99E611BD57A6013E71C7283E05E89B3D58F7D9BCF7721CBD4E9D6B3CA373573D1D43CFBD3E417A3DD0FA85BBEC654B3D5E0350BD17BC993C9587B0BDE8E21FBED7152EBDDD2BDDBA0CBA633DC50E0D3E006C96B899F709BE170D8F3D453EFABCDDBF513CDEA0CA3C50C5A43D0D7776BC92EC04BDC0FEE33D901673BCE986D1BD297B223C5F35103CE28748BB8F6AE33D669F8E3DDAC5EB3DB23702BD5EBFEC3BD2D5C5BD75A218BD86DC9B3DCCC3F2BD2C90A23C1685C13C7EFCA0BD3494AE3DF8FACF3D90D6D03C8A2B97BCF78D783A7DC8863A4F5BCFBD2C9A6DBD7D168DBA90CA853C0020C7BC93526F3C14AC773D4F593B3DEA5D733DDBF0093DEDAE0C3D0C9115BC73D80EBC63BE82BA682CD0BDCAC09F3C8276E8BDE8B1B1BD80B19CBCF54CDE3B8FA52C3DE46312BD448A0DBC7220EABD56B6273DDBDECE3DA50DA33C3719843CFF151DBD9E500F3B0713E13B8070E5BBE8CDC5BDD83B4C3D6A906A3D86CAF73C80482CBBD7F815BDDA30893DF15EC93CB056AEBD74EE08BE0B05A73DCC6162BC146C8DBC3FA3C83C6C700B3E407C9BBD6F2593BD56356E3CC687063B0EDC46BDB0BB9B3CA8E916BD5A97EB3CE4DFB93D0E23F53C35078CBD8A2206BDEB0F013CC85FA13CD8D2F43D49FCF23C9E582D3DC3D171BB87D40ABDA395813B36D0173CB93B153D5C1662BBE4383E3DCC5013BD4798D63D0E5CC2BA1F7804BEB417543D20C1803CED3CB8BDC4B407BD6536FF3D0FF19B3DF50E313C5EB773BB498EDEBA77BB9EBDEEC5813B50EE3FBD591A093D5F4AF5BDB198C43A117BB73C23B7903DF63F16BDC373F73C2AF111BD1AAD08BC9A695EBC994078BC5F9770BD82...
  %cst_28 = ""std.constant""() {value = dense<""0x3B2952BD677CF73C5B0D57BD9D4EE8BD51532C3E5255FB3A9457EA3D6E4389BD01FB85BDD2F7753D09B23A3C74B8E4BCEE0DECBDA09AA1BDD545D83C116A7B3D66DBC2BD3318D73BA2EAFA3C3A5CF6BC249C0DBDBC33B4BD4834233ECA2B4DBD99490D3DE709743D006A68BC70A7CCBC2AEA47BCD4C3383E1893433BC7B6E23DE79A90BD211FF5BDF370ACBDE1BE9FBA9B5594BD1E0E0FBE4DC4DFBDBB4364BD0E780ABBA0D5F33B51969D3DA31D0BBCF6BB9A3DF0FD183CC10C1ABD300144BDDA9665BB256B033DADEB813D3818A0BD0EE55A3DFC4FC1BC49130EBEB4FD8E3D65D696BCB4A37EBA6AE42CBDBD6551BD0E80DA3D072448BDAFF731BDE94905BD91F1C4BBF82A943D88BA963DCF358C3A9426A2BC18D114BBAC5A44BC60A2913D6C65C33D1FB8553BA2643A3D386C493BB014373DD428763D34A6B4BB888E773D4FE68FBDE8E8B3BC43CBA2BD9A57C3BC66EB5B3C8A9FD13C7143493D9EA0A63C970CB53B738686BDDEF6353DBE80583BEC4EBC3B70BFBDBD2C25B83DF3A90ABD49B9E93BEAA4663C55915CBDFBD81BBD46E069BD91B5A9BD18DBD9BC0C676CBDC7473CBD8AEF893DBF83213C278C453C8064A839C4CD393E0705933DB8A772BD4B9E8B3DFA9D863D64F30E3D63EF31BD4D00313D1F2B273DC1C1A63D45AF453C776B5F3CE801CBBD114E1C3DE601B4BD27E70BBCED12123DF031C6BCD132943D5793513D0A6FC23CA03545BD23BE083E36C7853D2B270D3BBC43BBBDFE94953DEF66C73D9486073D27149E3D8194AE3CCFF1D6BD648A7C3DFB6D153E29F854BDFEFE66BD1D47693DE950B83D8F62F63C239EAEBC481D4EBDD95AA5BD9AD4783D07FC703D7B8229BD873E47BDE099943D63D4373D8037B93C730D113E60DA0C3D4B121EBDDE0E5B3DE5649EBD29400B3D795B4B3D161FB93C8FF2513DB3CBA63C267D283D746AE03D57555FBD56BAF23C6AC7023D0ACB1CBEC92324BE53B2B5BDDADD4D3DB995DCBC9BDC053D61881BBEB67A02BDCE...
  %cst_29 = ""std.constant""() {value = dense<[9.67585132E-4, -9.76218085E-4, 8.44327267E-4, -9.95431095E-4, 9.78885102E-4, -9.89362248E-4, -9.97170456E-4, 9.94593254E-4, -9.74801718E-4, -9.967850e-04, -9.7882736E-4, -9.50525922E-4, 9.79408272E-4, -9.92655172E-4, 9.97303985E-4, 9.82631579E-4, 9.90360276E-4, -9.92504647E-4, -9.657435E-4, 4.47311701E-4, -9.95136913E-4, 9.76480427E-4, -9.81111777E-4, 9.97065915E-4, -9.86331375E-4, 9.94870206E-4, -9.89393214E-4, 9.676800e-04, -9.70026711E-4, -9.91622568E-4, 9.94842383E-4, -9.94916772E-4, 9.85594349E-4, 9.97806666E-4, -8.78486142E-4, -9.95528069E-4, 9.8985678E-4, -9.93425492E-4, 9.95124923E-4, 9.83530771E-4, 9.93813737E-4, 9.74371622E-4, 9.73470451E-4, -9.74504452E-4, -9.87119157E-4, -9.95987677E-4, -9.89082618E-4, 9.94357862E-4, -9.81126562E-4, -9.95713984E-4, 9.92701738E-4, -9.9737267E-4, -9.672870e-04, -7.41955242E-4, 9.90436179E-4, -8.20829126E-4, -9.95927955E-4, -9.940910e-04, 9.85695864E-4, 9.85741498E-4, 9.95741924E-4, -9.95478476E-4, 9.654010e-04, -9.80447861E-4]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_30 = ""std.constant""() {value = dense<[0.999001801, 0.999005556, 9.990270e-01, 1.00099504, 0.999001979, 0.999002933, 0.999001145, 1.00095022, 0.999001979, 9.990040e-01, 0.999001502, 0.999003231, 1.00099468, 1.00091136, 1.00086093, 1.00098407, 0.999001622, 0.999009847, 0.999010264, 0.999002337, 1.00099421, 1.00099874, 1.00095701, 0.999001383, 0.999006628, 1.00099599, 0.999303638, 1.00099146, 0.999027907, 0.99900186, 1.00099814, 0.99900937, 1.0009948, 1.0009985, 0.999017238, 1.00099349, 1.00099826, 1.00098813, 1.000934, 1.00099516, 1.00097382, 1.00099266, 1.00099349, 1.00099623, 0.999081313, 0.999002218, 0.99900335, 0.999011158, 1.00099492, 1.00099647, 1.00099587, 0.999002039, 1.00099766, 1.00099695, 0.999008536, 0.999002814, 9.990050e-01, 0.99900335, 1.00099838, 1.00099766, 1.00099349, 1.00099289, 1.00097156, 1.00099587]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_31 = ""std.constant""() {value = dense<[9.99597366E-4, -9.9919259E-4, 9.99345094E-4, -9.99698997E-4, 9.98205738E-4, -9.99698182E-4, -9.997270e-04, -9.99498763E-4, -9.98643808E-4, -9.96864866E-4, 9.99229145E-4, -9.99716343E-4, -9.98876639E-4, -9.98718547E-4, 9.98445553E-4, -9.99442767E-4, 9.99625306E-4, -9.8527607E-4, -9.95822483E-4, 9.97575931E-4, 9.998920e-04, 9.99492942E-4, 9.98938339E-4, 9.99762909E-4, 9.9961739E-4, -9.97697352E-4, -9.98864998E-4, -9.97156603E-4, 9.99353942E-4, 9.98489558E-4, 9.99433104E-4, 9.99394804E-4, -9.98000847E-4, -9.99715761E-4, -9.94169735E-4, -9.9937967E-4, -9.99294221E-4, 9.98356263E-4, 9.9906174E-4, -9.97644616E-4, 9.99090727E-4, 9.94966947E-4, 9.99400159E-4, 9.963630e-04, 9.97329829E-4, 9.99510521E-4, 9.99477691E-4, 9.99089912E-4, 9.99575247E-4, 9.99586307E-4, -9.99113777E-4, 9.99390496E-4, -9.97681403E-4, -9.83843114E-4, -9.97356371E-4, -9.98840085E-4, -9.99069889E-4, -9.98023431E-4, -9.98752773E-4, 9.9920528E-4, 9.99455805E-4, -9.99182928E-4, -9.99532174E-4, -9.97857307E-4]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_32 = ""std.constant""() {value = dense<[9.81708988E-4, 9.63601749E-4, 9.90674947E-4, -5.41209592E-4, -9.96806542E-4, -9.83843347E-4, -9.94779984E-4, 9.9403772E-4, 9.31107497E-4, -9.93747846E-4, 9.95395239E-4, -9.97131573E-4, 9.82350902E-4, -9.86792613E-4, 9.92027809E-4, 9.9402538E-4, -9.92390792E-4, -9.95012698E-4, 9.74349154E-4, -9.90020693E-4, -9.8652183E-4, 9.94545291E-4, -9.17962461E-4, 9.61345387E-4, -9.95251466E-4, 9.94973583E-4, -9.72526962E-4, -8.81796877E-4, -9.74486291E-4, -9.71922826E-4, 9.95795126E-4, -9.96316666E-4, 9.90363652E-4, 9.97908413E-4, -9.92957735E-4, 9.958980e-04, -9.9491293E-4, -9.92778339E-4, 9.9361909E-4, 9.95804904E-4, -9.4126031E-4, 9.86351747E-4, -9.81890945E-4, -9.85238584E-4, -9.91161214E-4, 9.91085078E-4, 9.90797532E-4, 9.80959506E-4, -9.92152839E-4, -9.79809905E-4, -9.88770509E-4, -9.91929322E-4, 9.80777665E-4, 9.96440299E-4, 9.86681901E-4, -9.95011068E-4, -9.93575318E-4, -9.95142967E-4, -9.86838829E-4, 9.61801095E-4, 9.941320e-04, -9.97495604E-4, 9.76995914E-4, -9.35959106E-4]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_33 = ""std.constant""() {value = dense<""0x3CBBEA3B451EFC3C60A407BED262B2BD7C0C7A3CFF35FCBD7183ABBC10F0DEBD85B86F3BF859E8BD1E64D33D55CE933DDEAA983D9FEADB3CDA31EC3D3C21CABDCD54ACBDB8C4A53D1C5EE43D1F7C923C3235963B7944063DAB03AD3DEB26ADBD296D1FBD1F3076BD45E99E3CA6A8813D07F06FBD219E023ECF15E3BD65EF063EC05155BDCE53BE3D2376CEBD4699F9BD562A7DBD1212AD3D42EDC0BD5B01773C7A7302BE3B3154BCE519743DDAB986BDCE14F8BDAB8836BD28DB81BD3A817F3DE73DC73D70DBB6BDB0834C3D1304043EADAB303D7655613D43489B3D9D721B3CA09A64BDE792CEBD6595DABD8433703D64C8963D54EE823CA734DC3C2ABD923DFE0A9EBC5A62A33CA5A88C3DA94A37BD71A7D4BD6FDD993BDAB6B1BD0C41A0BD1AD223BD0896563D5B6883BDE674E7BD5F2828BD592626BDB648683D3EFB8D3DC9D5923C7A5A253D020280BD7AD50A3EA061933DEEE6EA3DA68980BDC1E8D73DED3E073DA01DD4BDD8A40EBCC040093DA770DBBD9FDFC6BD82B5B1BDB2CC863C1324FBBC1608A0BC5C5D623DEE427F3D76A0E1BC28ACC1BDEA90FB3C56958A3CCF4B9DBD9C0E75BDCEC400BE4A27B9BCD59D9ABDE0DE8D3DA88210BD4630073E8E5EE53DBA0D78BD9EDC6DBDEEF5393DB1E0013E2978033DAEDF66BDB14407BE71B99A3C54988A3BB329A2BD14870DBD3961FDBCF374FABD966B05BEF002BC3D12D97B3D10827FBCA725B8BD31450F3DF0AA3CBDE61ADE3B88D2083E1DF108BEA99499BDE546FA3D38DD04BE59D6CFBD38D4543D15C7E93B9679813D34A20C3ED939D5BD2CA7E23DF4F10BBE60166EBDDD6CDBBB6A0C33BDBC04F03D9EB6AE3D8B38CBBD8B162B3D45AC2D3DB9151B3DCBF58D3CD2030A3DCACEBE3D041E39BCD2ADD3BC429D92BD5957413DE843D23DF490C4BDB5507D3DCA2E3F3D2B4533BD60AEF03D983C61BD1A148BBDBBF7913D22773EBC47ADD7BCCE0D25BD1BC232BDEB76D2BCA2E7803C6D1319BD5C34B0BDEBDC88BDD3...
  %cst_34 = ""std.constant""() {value = dense<""0x014B1ABDB51FB4BD77A132BD54BFD0BD4F596A3D1439DBBD4B39EB3C3275BB3D4E52EA3B0C8E09BE29C90A3EEE3700BD61CF76BD068808BEA24409BE589A66BD592285BB270BDFBA970207BE86B801BD95EEEBBDA06603BE7BC2BC3D6FE8373C88BF633DD3B03C3DCBECDBBC90F75FBD7C7E1FBD6475ED3D99DD51BD16F0DE3D8BA496BC0371E23D2845C8BD240BDDBD06C70ABECBAE9FBDC59E7EBC127B74BC6C35E2BD0E0204BE62115DBD8162903DFABBC5BDA22CD63C20D287BC54E58E3D915B523D34A8A83D10299DBD4D61D0BD87EAA53D446D613D4CBF5D3D494C573DB2AC4CBCE665B3BDC4D505BD3948DC3CB294F73C1FAF88BD7E9C193DFF06A23D0164063D49C43A3D8A2BF1BD0F5EC53B666907BEDF79373D55F1D3BD551C3A3D9D624CBD78BB9B3D5BEF8A3DCC4AEB3D37CEE73DA0F3D23D5AE4333D398D333C0306E5BD1FD7FD3DFFA9C7BD7039563DFD73EA3D71A1033E011503BEF3378C3DCD73AABDFF37833D52D096BD3D34BB3C54DEDE3D37F3AB3D70D93B3D8A3EFF3DF7C0C23D33575BBD7DEA00BEF8A9933CFE7813BD42A5E23D330B0EBD91A6A7BD3FCD3B3C2336323DCA5FE03D9F115CBB8C62D9BDCD34D73D86A4E1BD4678E03C053EBD3D567A7D3D19B108BDDBF6043E64208E3D4BA3C7BCC430D13D8135DFBDDD03D93B5F07EA3DB103853D8910963CA87AAEBDD58C9BBDF4FC123D6BEFB5BDC1039DBD8BF9EA3D9217E63DD208E7BDB178BB3D150883BBE2B1B2BD231692BD66571FBD09410BBD0CEAA33CA5D80F3CA0EB393CECF9713D7945B9BD2F86E1BDFED7D03DFB04FEBC0A5E623D19BE9C3D3691C8BCA05488BD0BF2003E26D94C3DA780E43D15E7BC3B012DD93D4E7904BE9B62C2BD3F70C5BD5D0883BB1125E63DA76D5F3D1357A13D7CA6DD3C5FA5043DE6124FBD253E18BD96AA56BD2862433D7415093E81CB583C25F1D03DE67913BDA296BCBDD8BDBB3DBCB286BD45C0873CD374D9BD1559DF3DADDD363DC94E09BD045B313D43...
  %cst_35 = ""std.constant""() {value = dense<""0x5D257E3DBA77A83D1581D63D2E973EBDA2A94DBD964DF53DFAD025BC0FF4C2BDF04190BDEE3EF63DCDE432BD168D683D0505E33C1F296EBD1F04DA3D367BF53D7BEE3F3DF8E0BF3BFB4BB6BDE1A901BD3FCDA63DF6C21BBD835D033E90BFE6BC6636C1BDF1DCBD3DBEC6D2BDFDC6D7BD6505F63DC242293DCF3833BD3756FE3C1102E9BD9A23A73CBAFE8B3DAAC1343DE017543CFCE2F53DAB0EB5BC55BE903DCC7B2FBD4047863DF7851ABDD6108DBDD0583C3DE83617BC68409B3DC56C913DCE77F33D119805BE7DB4FDBDD7211D3C14E68ABD8DF580BB31F189BD20D4033D05A4613DEFD400BD18BA263D79EEE13D338CC6BD99A5A13DB00219BC5A25813DDDBA92BDCBC1BE3D163D5D3D9F0E063E21DF79BC2BBCBF3D41A565BD820EC5BC547700BE6C6605BED73D93BD37E4FB3D05D896BD950D6DBD41B33F3D27A1D0BD1EF30F3DD5E09C3D0616D2BB350F903D0362B03D79E6C8BD248981BD707E653DA0AB09BD46F375BDEE20AE3C68F05BBCD2BBA93D9667C9BD79C4C53DC05B043E54A6F1BABACF26BDEDB7773DC6F69EBD472836BC20C101BEFC594B3C4FE1AC3DBDC1833DE29B563D1AA472BDF7CEAEBDB245F63CBC9BF53D74252C3DDFB7CA3D386E97BD3149C43D4481193D520EF93D6017E4BD20CAE3BC8CCD6DBDB81BA43D50C8343C4685BABDDC66003DC0FB15BCF78402BEC98EFEBDA818EB3D3AF8C8BDE9182DBD94E9F63DE5FAC63B4B81B9BD5CA7063E3A8F44BC174B1CBD1438033D2E0D873DED16B6BD637A403DB264DE3D271984BD35E2C63CB16FB53D8CDD2B3D8266E3BDFAAA373C8026B8BD6F229DBDD2C31ABC72CA5E3DEA95363DAB6EDBBC6B5B2F3DCB6F95BDDDD8EA3C8901B73D28680A3E9759F13D3B6FEDBD1908573DCA1696BCA5391CBCD6D098BCB976ADBDB54E06BE8371E23D5151EBBD6732A8BDAE23A83DB32EB8BDAE81083E6E7050BCB540B3BC309A82BC6E119DBCC77870BC10F2E23D1B6E943D5AE878BD8AA005BE709BB7BD22...
  %cst_36 = ""std.constant""() {value = dense<""0x995CD03CBBE0F2BC23A0B83D48070DBDF106813DE32A793D0519BEBDB63500BE784AF03D7D1305BEA39E16BD33ED57BDCA32003E1E046A3DED87D53DCB0B973C34B9E93DD9DEFCBCD0BE98BDC43B023E21A9913C4627BBBD7968C5BC1B3BFE3D0F0D8E3D9634BABD105803BE6B94E63D9E21053EBA02443DF1C36F3CFB839B3DF33E093E6068933C7EDFB0BD083287BDB414D1BD03AA97BDB145483DB00B033DE32F993B77EA6DBD1E9EA03D9C9203BE6C34C53DF5F68C3D322ECEBD15D28E3DB4F9ECBCD16A2A3D4E5010BD9ECD1CBDA52C87BCEDD8F83C012CD8BC0282903D597C61BD031D093E29D503BE3EDC463CC16181BC0DE1C93D945909BE259AA6BDE1E3093D17954C3D88372E3BDE8DD0BDDEE0993DDA9D11BDDB4B153D925C763D4EF890BDB87F5C3D949394BD9CB4C4BD423FACBD59E4C43D0794163C29518BBBAC9DF13D5B3A90BD03E6D9BA55EDCCBD505E51BD39AC9CBCC8FBF43DBD6F043E587DCEBC68F6D33C2F98E33DA63BEB3D5745273DD450AA3D27C306BEDCB2283B2AFDF2BCEDC7E23DC93CD13DC4366FBCC26EE3BBDCEEAA3D5C1BF63DCEA0083E90D650BD5DF6903DA98E813DDA45D7BD905F00BDE0FC01BD8CBCA5BD189D89BD4D14393DF7AE0DBC40F34DBC44231A3D3CE5BEBD251A8ABD842DB5BDB6417DBDB4CE51BC313E08BECE2BE9BCD31644BCC43D6D3DCACACF3D64C1B0BD88C0203D2413C3BD6F8C2F3CC92935BDBB6A89BC233F52BD143A79BCCA0B9A3D9A346DBC1D79AF3D7EB2053E89CD053E23EE00BEA38193BB9BF92D3C994AF4BC5AB848BD95C6E9BD29354DBD664080BD22B2863DE8B2F63D2A9D043EFADDF83DDCB348BD8C43073EF1A77F3D8F37743D7A43813D7D3294BD01EFDC3D66C6973D7DB3F7BD5F09A6BB1960183DE14360BD3C9DDB3CD948DE3B570B79BCC223E83DAE53643CDF1A883DE6B9D9BD7E3C2BBD90DF603DE8BEE6BA3C009C3D3EEC2FBD62B506BE8A230ABEFABE71BB7AC709BE312AE2BBD811A1BC24...
  %cst_37 = ""std.constant""() {value = dense<[[0.141972259, -0.140112832, -6.299400e-02, -0.188756421, -0.296468705, -0.193415761, -0.257958204, 0.138051048, -0.116950154, -0.166800201, 0.279425412, 0.252810538, 0.0246988367, 0.0818047821, 0.16655989, -0.172646925, -0.231393486, -6.86961692E-4, -0.0970985144, 0.223860174, -0.260656685, 0.19828698, -0.0498352349, 0.245914236, 0.0637414455, -0.111480176, -0.0401437283, 0.0500495099, -0.125676543, 0.205395907, 0.16058588, -0.00452343794, 0.0116028748, -0.248046264, 0.07183934, -0.224206865, -2.637570e-01, 0.066637516, -0.215825126, -0.0450223759, 0.197781667, 0.0415594503, 0.103903204, -0.0888473243, -0.0138380127, -0.186089963, 0.185404703, -0.167652592, 0.23974292, -0.218838304, 0.213134632, -0.162772402, 0.181808576, -0.103578761, -0.0596696399, 0.105313681, -0.101291053, 0.0236400794, -0.167137861, -0.129845485, 0.107204571, 0.193863332, 0.0873046442, -0.0449375287]]> : tensor<1x64xf32>} : () -> tensor<1x64xf32>
  %cst_38 = ""std.constant""() {value = dense<0.000000e+00> : tensor<128xf32>} : () -> tensor<128xf32>
  %cst_39 = ""std.constant""() {value = dense<""0x11D42CBD71902DBBAF29743C2A2046BD3D29603DA16E253C59952F3D6711333DE78F003D62E7983C43FC32BD7CEF39BD549D323DAC6DD8BB8D966C3DA49C59BD4A37633DF48E6EBD97D7303DC92BF2BC4B556C3D1D015F3DE9623FBDBB26BE3CEB0D373BBE8CB0BCAF44B7BB10770DBC078021BD6DC820BC7EBF1B3DDAB0003DE611E83CB0176B3A1CF9793D5EFA2DBCDC6C83BC36485EBCA0DC6D3DC12CC53BB0F4203C50F4F8BCC6AA5C3D37C1D0BCFC9D80BB9184B5BC1907B23C816A64BD9C592B3D3F9597BCC49A1EBDC77677BDFDD033BDFEBD78BD1EB8563D10AB72BDC65A883C6F32B93C1FF4483D0B81D3BC21BE743D26379BBBF426C9BCD079413D963334BCC8A072BDB28DA0BC5C6E763C1411B83C646B203DCF8F0ABD8F68EABCE98177BDC03B13BDDED4433DE78B42BD693F3BBC4BB11EBDC14911BDFF5E623D1FA1E83C56BC34BC950A21BDECAD4E3C4B8D423D038441BD2536E0BCC4F1483D1123A03CD98765BD39D2BFBCDB07BA3C77554ABD922F8FBC175AEBBBBE6F613D8BB24A3D3B9B8D3CEC540DBD8A5519BD2A4AA1BCD0835ABDBA1A8D3CE5A677BD5AD34BBA6D350E3D45A85F3D9C85373D70C191BB7AF56E3DD60993BCF984E0BC0D0C96BCB27C2EBD5300A9BBD805433D63F0403D53013E3DC01A133C7FDC733DC71D283C8B9E7BBC9568753D2FA60C3D485BBC3C921A353DD35C0A3BD3C69C3C3857133D52ACCC3CDEEF3DBDB8D1A0BC0A854ABD7967ADBCEBA4303DA95E33BD8F4B853C1EDF0ABD697C3ABD24378A3CF3E05E3D139DA63CBD0B373D48F1E43C5CF140BCEF6804BD147B263D2E1BCFBB35A9023DA01444BDD5E0093D05B8713C58BA5D3D5A41BBBAF4B5E4BCB8ECEB3CBE0B61BDF3AC28BD863C813B958456BDD0ED2ABD6492113DA355663C8322F23CDB5079BD1D40953C210C483DB1B0BFBB3A137FBD3DBB6EBD7D2B8A3CFB42CFBB44F64DBDA1BC473D7099A1B932482BBD08A304BCC7C14D3DD09DD23CF9FC4CBD6440683DF9...
  %cst_40 = ""std.constant""() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_41 = ""std.constant""() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>
  %0 = ""tfl.expand_dims""(%arg0, %cst_1) : (tensor<?x1000x100xf32>, tensor<i32>) -> tensor<?x1x1000x100xf32>
  %1 = ""tfl.conv_2d""(%0, %cst_39, %cst_38) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""VALID"", stride_h = 1 : i32, stride_w = 4 : i32} : (tensor<?x1x1000x100xf32>, tensor<128x1x7x100xf32>, tensor<128xf32>) -> tensor<?x1x249x128xf32>
  %2 = ""tfl.squeeze""(%1) {squeeze_dims = [-3]} : (tensor<?x1x249x128xf32>) -> tensor<?x249x128xf32>
  %3 = ""tfl.add""(%2, %cst_0) {fused_activation_function = ""NONE""} : (tensor<?x249x128xf32>, tensor<128xf32>) -> tensor<?x249x128xf32>
  %4 = ""tfl.mul""(%3, %cst_7) {fused_activation_function = ""NONE""} : (tensor<?x249x128xf32>, tensor<128xf32>) -> tensor<?x249x128xf32>
  %5 = ""tfl.add""(%4, %cst_8) {fused_activation_function = ""RELU""} : (tensor<?x249x128xf32>, tensor<128xf32>) -> tensor<?x249x128xf32>
  %6 = ""tfl.shape""(%5) : (tensor<?x249x128xf32>) -> tensor<3xi32>
  %7 = ""tfl.strided_slice""(%6, %cst_40, %cst_41, %cst_41) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<3xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %8 = ""tfl.pack""(%7, %cst_3) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %9 = ""tfl.fill""(%8, %cst_2) : (tensor<2xi32>, tensor<f32>) -> tensor<?x64xf32>
  %10 = ""tfl.unidirectional_sequence_lstm""(%5, %cst_19, %cst_20, %cst_21, %cst_22, %cst_11, %cst_12, %cst_13, %cst_14, %cst_6, %cst_6, %cst_6, %cst_15, %cst_16, %cst_17, %cst_18, %cst_6, %cst_6, %9, %9, %cst_6, %cst_6, %cst_6, %cst_6) {cell_clip = 1.000000e+01 : f32, fused_activation_function = ""TANH"", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<?x249x128xf32>, tensor<64x128xf32>, tensor<64x128xf32>, tensor<64x128xf32>, tensor<64x128xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, none, none, none, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, none, none, tensor<?x64xf32>, tensor<?x64xf32>, none, none, none, none) -> tensor<?x249x64xf32>
  %11 = ""tfl.mul""(%10, %cst_9) {fused_activation_function = ""NONE""} : (tensor<?x249x64xf32>, tensor<64xf32>) -> tensor<?x249x64xf32>
  %12 = ""tfl.add""(%11, %cst_10) {fused_activation_function = ""NONE""} : (tensor<?x249x64xf32>, tensor<64xf32>) -> tensor<?x249x64xf32>
  %13 = ""tfl.cast""(%12) : (tensor<?x249x64xf32>) -> tensor<?x?x64xf32>
  %14 = ""tfl.shape""(%12) : (tensor<?x249x64xf32>) -> tensor<3xi32>
  %15 = ""tfl.strided_slice""(%14, %cst_40, %cst_41, %cst_41) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<3xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %16 = ""tfl.pack""(%15, %cst_3) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %17 = ""tfl.fill""(%16, %cst_2) : (tensor<2xi32>, tensor<f32>) -> tensor<?x64xf32>
  %18 = ""tfl.unidirectional_sequence_lstm""(%13, %cst_33, %cst_34, %cst_35, %cst_36, %cst_25, %cst_26, %cst_27, %cst_28, %cst_6, %cst_6, %cst_6, %cst_29, %cst_30, %cst_31, %cst_32, %cst_6, %cst_6, %17, %17, %cst_6, %cst_6, %cst_6, %cst_6) {cell_clip = 1.000000e+01 : f32, fused_activation_function = ""TANH"", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<?x?x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, none, none, none, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, none, none, tensor<?x64xf32>, tensor<?x64xf32>, none, none, none, none) -> tensor<?x?x64xf32>
  %19 = ""tfl.mul""(%18, %cst_23) {fused_activation_function = ""NONE""} : (tensor<?x?x64xf32>, tensor<64xf32>) -> tensor<?x?x64xf32>
  %20 = ""tfl.add""(%19, %cst_24) {fused_activation_function = ""NONE""} : (tensor<?x?x64xf32>, tensor<64xf32>) -> tensor<?x?x64xf32>
  %21 = ""tfl.reshape""(%20, %cst_4) : (tensor<?x?x64xf32>, tensor<2xi32>) -> tensor<?x64xf32>
  %22 = ""tfl.fully_connected""(%21, %cst_37, %cst) {fused_activation_function = ""NONE"", keep_num_dims = false, weights_format = ""DEFAULT""} : (tensor<?x64xf32>, tensor<1x64xf32>, tensor<1xf32>) -> tensor<?x1xf32>
  %23 = ""tfl.logistic""(%22) : (tensor<?x1xf32>) -> tensor<?x1xf32>
  %24 = ""tfl.reshape""(%23, %cst_5) : (tensor<?x1xf32>, tensor<3xi32>) -> tensor<?x249x1xf32>
  ""std.return""(%24) : (tensor<?x249x1xf32>) -> ()
}) {arg0 = {tf_saved_model.index_path = [""input_8""]}, result0 = {tf_saved_model.index_path = [""time_distributed_4""]}, sym_name = ""serving_default"", tf.entry_function = {control_outputs = """", inputs = ""serving_default_input_8:0"", outputs = ""StatefulPartitionedCall:0""}, tf_saved_model.exported_names = [""serving_default""], type = (tensor<?x1000x100xf32>) -> tensor<?x249x1xf32>} : () -> ()


During handling of the above exception, another exception occurred:

ConverterError                            Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    200       return model_str
    201     except Exception as e:
--> 202       raise ConverterError(str(e))
    203 
    204   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:

ConverterError: <unknown>:0: error: loc(callsite(callsite(callsite(unknown at ""functional_7/lstm_13/PartitionedCall@__inference__wrapped_model_240475"") at ""StatefulPartitionedCall@__inference_signature_wrapper_247244"") at ""StatefulPartitionedCall"")): We cannot duplicate the value since it's not constant.

<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: note: loc(callsite(callsite(callsite(unknown at ""functional_7/lstm_13/PartitionedCall@__inference__wrapped_model_240475"") at ""StatefulPartitionedCall@__inference_signature_wrapper_247244"") at ""StatefulPartitionedCall"")): see current operation: %10 = ""tfl.unidirectional_sequence_lstm""(%5, %cst_19, %cst_20, %cst_21, %cst_22, %cst_11, %cst_12, %cst_13, %cst_14, %cst_6, %cst_6, %cst_6, %cst_15, %cst_16, %cst_17, %cst_18, %cst_6, %cst_6, %9, %9, %cst_6, %cst_6, %cst_6, %cst_6) {cell_clip = 1.000000e+01 : f32, fused_activation_function = ""TANH"", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<?x249x128xf32>, tensor<64x128xf32>, tensor<64x128xf32>, tensor<64x128xf32>, tensor<64x128xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, none, none, none, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, none, none, tensor<?x64xf32>, tensor<?x64xf32>, none, none, none, none) -> tensor<?x249x64xf32>
<unknown>:0: error: Failed to duplicate values for the stateful op

<unknown>:0: note: see current operation: ""func""() ( {
^bb0(%arg0: tensor<?x1000x100xf32>):  // no predecessors
  %cst = ""std.constant""() {value = dense<9.9943357E-4> : tensor<1xf32>} : () -> tensor<1xf32>
  %cst_0 = ""std.constant""() {value = dense<""0xE30BE3B458690BB4A3B6ACB5D7BB263456F1E83527D417B745E0B1319E918E35717B2034BADFF2B5BC71DA36C9BEA6357FEAB834E78A50363F337B34D9EF64B4243D06B6D7ED4D362185EBB5851C1EB2DF5C6235448401B64B6CBF351CE0B435577819B5A3C8B9B5BAE50FB6EAAE85B4296D0D37742E8636472DA93410A839B6B0F3AA3545ED9DB55EABA73528FE6935C57C20B521FCCAB6146D6F35B1C9E3358C367935A6835C362040E9B5305A5A3675D79B36CCD3E8B5F7D83134F3FCC73558A42E367F36F335F08B0FB62B3B19B74367E4B5A1900EB72DF76FB5304B5C3496D04A357B7F97B6FBEA86348AC3333573A2B9B5B92C32363C003D36A47F273632596D34966B3DB57958B3B70D0C39B6C66FA9363016AB35DAD79835A23DA6B6F3E29E3664AC87B5A2592BB68AC90834D6331835B9B600B54DCBDE341AD586B6189BD2B413F0A6B5B607D3B47F7BACB5ED4D0A361A575436845BD6B6B54B1CB77C09C8B40B3FFAB59DDAEC32E65F85B4081C2DB75E45B836379ED23527C9BF36C94CA336A5C433B614DAC036A6E308B51BD9CE363BA05B3513036035681B1EB3D27FE3B5C7E7A3B5FC8B99B60D8705364DFF6A36E1A98DB5BCA14736CA373AB645C5F3B5F9ED09B6B64C03B7B5838A35DC3846B437DD33B6ECDE2F36C14FBEB50FB1B3B566EE05360741D83504D09E353793CA35265F0A365235DBB59A810935""> : tensor<128xf32>} : () -> tensor<128xf32>
  %cst_1 = ""std.constant""() {value = dense<-3> : tensor<i32>} : () -> tensor<i32>
  %cst_2 = ""std.constant""() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<f32>
  %cst_3 = ""std.constant""() {value = dense<64> : tensor<i32>} : () -> tensor<i32>
  %cst_4 = ""std.constant""() {value = dense<[-1, 64]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_5 = ""std.constant""() {value = dense<[-1, 249, 1]> : tensor<3xi32>} : () -> tensor<3xi32>
  %cst_6 = ""std.constant""() {value} : () -> none
  %cst_7 = ""std.constant""() {value = dense<""0x4868803FB466803F4AA9803F79A8803F6BA8803F6FA9803FB8A9803FEF67803F40A6803FB567803F0F68803FD0A8803FBB67803F14A8803FD067803F28AA803F87A8803F96A8803F9F67803F40A9803FCFA8803F4A67803F3266803F71A9803F03A9803F8567803FF967803FDFA9803F49A8803FA2A8803F2868803FF0A9803FBF67803F0DA9803FFB67803FE867803FD3A8803F6EA9803F5DA9803FBD67803F4967803F1BA9803FD067803F1E67803FBA67803FE266803FCF66803FDE67803FBFA8803F9467803F1969803F7767803F2068803FEAA8803FF7A7803FA1A9803F13A8803F7B68803FECA1803FA0A8803F40A9803FE267803F2768803FD5A9803F3467803FC6A8803FA8A8803F3D67803FADA8803F1D67803F2E67803FC468803F32A8803F6168803FDBA8803F8967803FFB67803F1AAA803F9F66803FEE66803FBFA1803FA2A8803F6FA9803FF767803FE6A7803F5E67803FBF66803FEEA8803FEB67803FC8A7803F05A9803F6067803F47A9803FF267803FE366803F1368803F9C67803F9DA8803F7F68803F1568803F15A9803F3C67803FE1A8803FA367803F3568803FA6A9803F5768803F4767803FD5A8803FF466803FB867803F7667803F3667803F4466803FD767803F99A8803F53A8803F5967803F1BA9803F1BA9803FC367803FC0A9803F00A8803FADA8803F71AA803FB96D803FDC66803F6167803F""> : tensor<128xf32>} : () -> tensor<128xf32>
  %cst_8 = ""std.constant""() {value = dense<""0x160683BB18CC3ABCF0152FBBF47A0BBCCBE290BA473EB03B98DEB43B6E7DE8B9E0962BBB34720DBB8CA08A3B947205BB88E016394D3C5D3A369976BB0448A43A4068CDB9E0EE09BA7EBE473BD78709BC4126053C2EB5053A50CE7D3B4CC968BA2AD9B13A79CB9EBA586D723AAC9489BAC4398CBB14ABD5BAB6B728BB9C6525BB9161F7BA8983CD3BC35ECFBB2D9F9CBBF17F413AC03E83B9E90A02BB5DEC68BB034E65BBA1F7863BFA1E953AB95826BBA83343BB949F05BCC33118BC85B9C43A808605BA0088DDB67116D03B38E3B63B093E0CBBBC50E7BBC46B5C3A4C76DC3A7EE0F0BB66A0243A3E91483A6302453A0496793BBEF99A3A8D1BC4BB945DD739D443583944B28BBBBAF4E53BE6BCC2BB9875453BD01E8D39C44A8ABA5AA8C3B9549853BB217EDA3A346F8AB9A23B1CBB45BF413C62A405BA4C8D97395AEEAA3BFB4FE3BBF6160ABB8568B73AF16F10BB44E2823BDE8B12BB5B0602BC4BACBDBBD95B0D3CE2C34B3A952B8B3AD8C7A6BAE4C12B3B0AB69E3B9853533BB3BD063C623A11BBAD8DF33A4889993BC668D6BA8E00A53B4CDCB0BB89AE693BC4A2B2B90BFDDCBAA60B0CBCE3CA52BAA7CD98BBBAE736BB6E005E3B9E3B4DBC80ABE93BAA4E8C3B6BA7D33AE01EFEBBC409ED391A19A93A2C052FBAA54B14BC8C2D70BC58F684BABC128B39AE80803CEC36D2B97C6839BADB88663B08E706391CC5ABBB""> : tensor<128xf32>} : () -> tensor<128xf32>
  %cst_9 = ""std.constant""() {value = dense<[1.00473976, 1.00521708, 1.00263178, 1.00512218, 1.00516057, 1.00505733, 1.00508463, 1.00512767, 1.00468206, 1.00509381, 1.00515211, 1.00500977, 1.00460923, 1.00526345, 1.00518048, 1.00323498, 1.00313473, 1.00481927, 1.00307453, 1.00518346, 1.00318122, 1.00508034, 1.00511396, 1.00325751, 1.00530183, 1.00303495, 1.00524509, 1.00510859, 1.00507498, 1.00289106, 1.00500321, 1.00317168, 1.00510597, 1.00283813, 1.00500691, 1.00319803, 1.00302553, 1.00532532, 1.00289774, 1.00283813, 1.0031631, 1.00312734, 1.00510907, 1.00331628, 1.00476944, 1.0048629, 1.00319898, 1.00497484, 1.00326228, 1.00507808, 1.005170e+00, 1.00541019, 1.00510037, 1.00526011, 1.002900e+00, 1.00523686, 1.00262475, 1.00306499, 1.00506783, 1.00488198, 1.00327921, 1.00528109, 1.00526297, 1.00270772]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_10 = ""std.constant""() {value = dense<[0.00212347507, 0.00114128285, 0.00424089422, 1.03820697E-4, 0.0025341576, 0.00241684401, -9.20396414E-4, 0.00274362462, -6.54401258E-4, -5.62307425E-4, 1.24871498E-4, -0.00112991163, 0.00194295647, -7.7732536E-4, 0.00172411324, 0.00204211427, -6.12335338E-4, 3.90041969E-4, -5.3005293E-5, 0.00170475454, 4.90864331E-4, 0.00253311382, -3.48295085E-4, 0.00198506215, 9.72406648E-4, 2.20654067E-4, -7.47772981E-4, -8.11745994E-4, 0.00160141359, -0.00112064206, 9.69715358E-4, 0.00184346607, 0.00207264954, 0.00188718142, 0.00195737928, 0.00224771816, 0.00170390774, 5.08747587E-4, -0.00367517141, 0.00248319749, -0.00254705641, 0.00120391347, -2.9670808E-4, 3.36897792E-5, -8.59148916E-4, 0.00119491515, -1.53806293E-4, -1.64092053E-4, -9.07708308E-4, 0.00232663471, -0.00211252738, -0.00117885927, 0.00240447256, 9.73630347E-4, -0.00122551713, -0.00213340251, -2.57467618E-5, 2.70258111E-4, -0.002932237, -0.00138859439, -0.00181194942, -3.7918007E-4, 1.09911198E-4, 0.00112971466]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_11 = ""std.constant""() {value = dense<""0xA182193D6852173C04EF7BBDE9BE8A3D7CEA35BDCFE4B43D92E47D3D203D4CBDF34D47BEF58101BD743B16BD4DC6B0BCEB285FBDDA1B953DBD410BBE8276743DE01D883CA2DD9ABA644B8CBC566B79BCEC738DBD6DFBB23D88AF1E3D16DDF0BCF265EB3D9D6F423D33D63BBDB9250FBDDF2AA33A84F3A13AFABE08BEE40BD73DA8CECEBDCFDAE23C880CE13D2C98B83D1C7C1CBD205A33BDB1CC8F3D5302CABDDF2D263D0FC7F73C924A213B38AEC5BD83D60B3DDC5A903D8C1406BD96AFAC3DAC2C3FBDCAAADD3CBCAD7D3DF47FF0BD3BA8063D3C38D1BC5DD0283D50752E3DC32A87BD19ABBD3CD6CA69BCDC160DBD3CB404BDE0952ABCBD5BD5BC52ECA23DD24424BE36F8E13DDFD9353DCBAC7EBD952CAE3D72B62A3DFAE18EBC64C742BD09C7A6BD782D893DA82564BD76DB853D6F4C3D3D017768BC1F998A3C3D29253DA49FBFBBEACA8EBD78CB4EBDB967943D431578BD7BECF73D87C2083C3DF3CCBD331D8BBC45E580BD6536C03D4252713D076A5ABD1F541B3D6AA0BEBD3E48533D542121BCEFC6A9BC66504E3D31ED84BC06C0823DDFFA8B3BA4E8843DA9B56F3DF12592BD4A8FC83C41E5A33CE4AF413C26658CBDCC65D33C98A9A63D61A3A7BDB0DBA4BDCB8A1DBC5808323DC65F08BD3C8B993D766D323DA37EEF3D067076BC0E4C6CBC1051923DD8E361BC626F77BB0B762A3CE106D3BD39A465BC4A82B23DCBEE70BDCEAC883D097C8A3D6F62DF3C74B00E3D725B843D6EF454BDFCFE3B3C8A2AB63DD790053D75ECF13D21F274BD39D99ABBC924F9BCE27D19BD2BCEC63DF30222BE2107173D3D0BBEBC974E08BEBE8D2E3B393C5CBD896FA1BD45DA24BCE6AB33BBD54CA4BCF304853DEFE0973DE595643D593CA73CAA2DEFBDEF3AA13CF18759BD45EE3E3B107800BC9F800B3E4051833C39EFA23C0904D6BB154CA83D9DA2ACBC847EF93C71CC1ABD31BAB5BD9E910EBEC9966E3C48AC03BC521E3B3C721A4C3D455AE13C1CCDB2BCEABBA4BC6EE801BDDB...
  %cst_12 = ""std.constant""() {value = dense<""0x66A9D5BC396F00BED31B653CBEB2253EB266683DAA630A3DF348CEBCF921B7BD509439BD439542BAD13474BD50CF123EB550903DCC7231BD983BA93CF6F072BB6E887C3D1C737B3D5C8CB9BDEA58B7BCF5C9983D97F4A7BD0192D9BD4E499EBBE0C9653D1DD8D6BBC63A713C85216E3D0030273D52F0853DFC9D473CD1F2993D7BEF073D09E1EABC11318D3DBAEFD3BAF6F5003C8430E53D9FB23CBE15C8913DE8D51D3E97891EBD6F89113D38A9613DA190463CFC4416BDF6021DBD59C065BDFF96FA3C0708A43DEFFD0C3E4C58553D4A549F3BC421A13D5C81D43D121F8F3D6301DDBD5C3DAFBD68E7FBBC8FF3173D860B82BD5CA4813D2459C4BD27B28ABD2360FC3D99B2D93DEB55CF3DF8BD293D7379A63D1CF2803C04D155BDBB080BBDD78D923CDBCA91BC523C43BD2EA68CBCFAB694BD1DBD203E6412133D2CED05BEA8DB97BD997E86BDBC0975BDCCC16BBD4B03E2BD251DCFBC15767DBD60F5A03D3BA1513C4D6232BC4867D9BDBABA2F3D19B2BEBDB4954BBD04CA29BD5D0B78BCD756ABBC32D2C93C1FF7C53D200A42BD560120BBA03F1B3D8117733DFB1C733D33226DBD34D8DA3CD8BE07BE9B29D63B826181BD78DFB8BDAD7EE9BB5423ADBD9CB8C83C081CA23DBE7A573D307787BD8140D93DD924DFBCF5550CBE1D03513DACFEABBB88CFE2BCE2211C3D349F8FBC54DE90BDC9B06D3DCE33ADBD243C023D9738ACBABB02663C5C5EC5BAD8BC023CB6FFB23D5A9C43BCC95EDFBCA81ABC39B596AB3D7E972EBC836699BB42FBF7BC0EE007BCABA3443C2A66A23D3206B83D39AF383DC412C3BC690B83BCD4F0E63D4806D53C734E343C26C90CBD524036BD7EFE7EBDCAB4693C742EEB3DDC93E9BD299513BE850DC23C3EB5A0BD89AD64BEADBA753C9FC18EBDA89B80BCE4728BBDC3E4133DEAC54BBA7C4E47BD22C6C0BAE80E623CF8D8473D16C9B3BDA2950F3D9612A0BB3268D53D28B28FBD3B3BBFBD2A76723DE100AB3C47387E3CBF6718BE722E3E3BB0...
  %cst_13 = ""std.constant""() {value = dense<""0x89C41CBCAF8AC2BD457EC23CD0CC16BCDD6A2E3E4B478DBC4A2143BDE0E4163DC250BE3D7AD2E63C0B11D0BCD79A753D854C7EBD3B4937BD391179BD91CD3ABCA9A6CE3C81A82DBD882EB5BD84F241BD6FE39C3B3454B3BCFC44D63B201256BBCFBA66BC47A1433CBDA498BD270A4DBCC07E54BD9E16AABDBE61383D0DE75A3DEDE8183CAB07A33CCF79503C688DFC3DC0CED7BC1849B13D867A0C3D8D5ACF3B5D2397BCDC9714BDB08E7E3C010337BD73C62A3DAEB2B33DD24E80BA17078FBC6D4424BC209F7E3CC06D473D2241CDBCC26B54BDC6A5A83D9C7856BDE09ABD3C85FDEFBDA4D7833C6915F23D001EEDBDA773F13CDB9F9DBD7380B1BDCFEB073D93BF32BD5F6775BD76C3AA3CA6E285BD7FC16B3C72BCDBBB19CB11BE82DBA73D4B2E753D8FB6413D5B9105BDDA452C3D15C974BDA361F83DB88388BD1448BBBD13E0863D9FA8093E9FFE50BD041B82BD121C153EE47D033CAEBC623D206BCDBCE86DBB3DEC3C0F3DC3B0EBBABB11F8BCDF9FF53DABC904BDB8FCBDBCBEC815BD9C355DBDEE006B3D1F62B9BC2C97CF3BEEEC20BDF51B943B98698E3D29FDD03DB7095B3D004BFDBC496E013E0AB17BBB7DF4DC3C0D276D3D541044BB9E114D3DB58D703B0BC2A03B298C37BDC529E13CF947C03D842C943D51F332BDFFE530BCC7E1FABC9FB590BDF217F5BD5529DB3C3CED813D353E043D6CE673BDD2552DBDD2AD353C518714BCF721C3BCE40CEDBD31E215BDFD2EA33CB41004BD53159BBC96A9E2BD6222D3BDD0096CBD1CF7A93C2D2D30BC1E152BBD6DBE24BD4CE7B1BDE1CFA1BDE07E053EBA5A2F3D6603EBBC1C6BD03B3597A9BD3208B8BD86C3193DBFF3933D0739263D661E893DE212173EAE038CBD400913BD8B40353DAACA84BD0532403E6E5636BCC8ECF6BCB1B4E53D0BB02FBDA03DAB3D36EB00BDFC06E5BDFF78A73CF7FC72BD351988BDEB09BF3C1C5F47BD7861893C3273A6BC8D118ABAC607C63DF3B7FBBDAB3D0A3D2BFCE8BD9E04AEBD0F...
  %cst_14 = ""std.constant""() {value = dense<""0x6D7FE53B90D80ABB900637BD90E9CABC41E959BBC4683A3EF659273CDAB79B3B7277E6BD9680A3BC06664CBD4495EF3C78CB36BCCBEE783DE7233CBDE6E06C3DC4E310BC40B12F3DEEED48BDEF40503D9182B5BCCCE6443CC09188BD2A2999BDCD20EFBCDAFE4EBCA274FC3B446CC03B85408ABDB62E763DB49CE1BCCFCC7A3D2D801E3D624E3D3C00EC92BD55E9B2BC7D302ABD8F380DBD9CD3813D47EBCB3D977EF0BD3ED69D3DA41AA13CC517C63CD524CCBCA81C313D21F81F3DB2BD113D837E02BCDADD843D932C763D4FEF5D3DDEAB18BE12D212BC3B738EBD7B78C4BC6F65353EAD2506BE0FAA3BBCC8604B3DF5740A3D0D51C23D0084E1BD0A42DB3C9FEE8C3B93908BBD930453BDAFD70A3EB82CAC3C2D97083E656B03BD2E28033C25B813BDE6E4DE3D129D213D32C1A2BCD7B222BD972851BD34FD8B3DDA896BBC1EA2803C456AC73C9E63763DCD31DBBC3DFAC4BDB4A13E3DAFB8033D64513A3D8F72343E05EC913D109BA2BCA691A8BD0A49143B3513213DEE201CBC7A9B993B7745F23C686CC4BD9270853C8613F4BDAAA698BC8C5B2DBE3FC999BD8AFA8CBC99B65ABDBA5AAABCA0028A3D9635423D2656C03C37AC0B3D3992013EB451B8BCE33ED03D25F2B0BDE2D507BD65A05F3DD0EDA53D5D1B333BC79CC3BDB4DD04BD7D3116BBE4606D3DD404203DF22C133DB880BCBCDBF5D4BD7D9E8ABC2CEB9E3D04D5CABC95F8343DA6DAC73D7725F1BDB6ADA93B57E9BB3C9F0D9CBC409ACBB7E5088DBB42502EBC4EEAA3BC6EE8283C188C683C916A223DA2FF0ABDC0FD8F3CE56611BCBD8D97BD4628443D928C95BDEA581ABC0D16D63D2E85843C05A28BBC40538437626B323C9A7127BEAE940F3D8556A53C14C3533B1F17993DD2B3E5BD851BC3BD980643BD8980603CD9625A3D0072C1BC279913BD045D55BDE0D406BD55C56BBDA6EBF13C748ECDBC907E77BD0490C23D3C8B923C148F323DEC107A3A1CF4C43D9B2FBE3D556331BD0E6CEA3B5A54833D8B...
  %cst_15 = ""std.constant""() {value = dense<[-9.8901696E-4, 8.5479155E-4, 9.79730626E-4, -9.92890913E-4, -9.96473943E-4, 9.94842383E-4, -9.81436577E-4, -9.86810191E-4, 9.76143696E-4, -9.95050533E-4, 9.96386515E-4, 9.97202587E-4, 9.95036563E-4, 9.89387161E-4, 9.55004477E-4, -9.91261797E-4, -9.74753813E-4, -9.9344307E-4, -9.93789755E-4, -9.86392376E-4, 9.69534274E-4, -9.80411772E-4, 9.94408153E-4, 9.93013498E-4, 9.97127848E-4, -9.42781102E-4, -9.70873865E-4, 9.89013817E-4, 9.76190961E-4, 9.85609483E-4, -9.92171582E-4, -9.76951909E-4, 9.94690228E-4, -9.26485285E-4, -9.87045699E-4, -9.95974405E-4, 9.95763926E-4, -9.95861366E-4, -9.9247531E-4, 9.89652355E-4, -9.91960754E-4, -9.92493587E-4, 9.88994841E-4, -9.96514922E-4, 9.97994909E-4, 7.89895537E-4, -9.94454952E-4, 9.82676516E-4, -9.95628302E-4, -9.84977814E-4, 9.91726177E-4, 9.94249363E-4, -9.90893808E-4, -9.91686829E-4, -9.92248067E-4, 9.85561753E-4, 8.73027078E-4, 9.94159607E-4, 9.96555201E-4, -9.95308626E-4, -9.89207183E-4, 9.23537358E-4, -9.56159958E-4, 9.97224473E-4]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_16 = ""std.constant""() {value = dense<[1.00099027, 1.00098932, 1.00099516, 0.999001443, 0.999003052, 1.00099671, 0.999013066, 0.999002039, 0.999003827, 0.999007761, 0.999003648, 1.00099814, 1.00098109, 1.0009973, 0.999003589, 0.999007225, 1.00099742, 1.00099838, 1.00099516, 1.00099599, 0.999002635, 1.00093877, 1.00099826, 0.999011039, 1.00098026, 0.999085307, 0.999004065, 0.999006569, 1.00098586, 9.990090e-01, 0.999005258, 0.9990049, 1.00098383, 0.999015271, 0.999016046, 9.990040e-01, 1.00099528, 1.00099838, 0.999001204, 1.00099707, 1.00098884, 0.999010801, 1.00098121, 0.999003827, 1.00099885, 0.999003291, 0.9990139, 0.999089419, 0.999002397, 0.999002635, 1.00099599, 1.00099051, 1.00099814, 0.999006092, 1.00099432, 0.999023616, 1.00092232, 1.0009979, 1.00094688, 0.999000966, 0.999003708, 1.0009973, 0.999011337, 1.00099313]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_17 = ""std.constant""() {value = dense<[-9.96331218E-4, 9.9947548E-4, -9.99390729E-4, -9.99690964E-4, 9.99050796E-4, 9.99103183E-4, 9.98863601E-4, 9.99490497E-4, 9.99209471E-4, -9.99391428E-4, 9.99218085E-4, 9.99209703E-4, 9.80420154E-4, -9.99758602E-4, 9.98847535E-4, -9.99662093E-4, -9.99377341E-4, -9.989780e-04, 9.99162555E-4, 9.99477691E-4, -9.99286537E-4, -9.98504692E-4, 9.99714247E-4, 9.97304683E-4, 9.9933322E-4, -9.93805122E-4, -9.5772784E-4, 9.99755342E-4, -9.997870e-04, 9.98014817E-4, -8.64997389E-4, 9.97377792E-4, -9.99638345E-4, 9.9680887E-4, -9.98852308E-4, -9.99114126E-4, -9.99839277E-4, 9.98401781E-4, -9.99705167E-4, -9.99090261E-4, 9.95945185E-4, -9.99628449E-4, -9.98933333E-4, 9.98956267E-4, 9.99524607E-4, 9.99682699E-4, -9.98335075E-4, 9.99589916E-4, 9.97847644E-4, 9.99562675E-4, 9.99166746E-4, 9.97601891E-4, -9.99633572E-4, 9.98750911E-4, 9.99411451E-4, 9.98528324E-4, 9.96360555E-4, 9.99749638E-4, -9.9872041E-4, -9.99665353E-4, -9.9975022E-4, -9.98956849E-4, 9.98632749E-4, 9.98964183E-4]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_18 = ""std.constant""() {value = dense<[-9.969270e-04, -9.82557306E-4, -9.93291963E-4, -9.90343512E-4, -9.97934374E-4, 9.90803935E-4, -9.84228565E-4, -9.96587565E-4, -9.97572089E-4, 9.795690e-04, 9.94196743E-4, 9.95259266E-4, 9.97718772E-4, 9.96876275E-4, 9.75065515E-4, -9.9695567E-4, -9.92954708E-4, 9.90913482E-4, -9.94683359E-4, 9.95306298E-4, 9.89426276E-4, 9.78857278E-4, 9.94338188E-4, 9.96380113E-4, 9.88848274E-4, -9.76715586E-4, -9.89963999E-4, 9.82530764E-4, -9.80048673E-4, -9.93657624E-4, -9.90298925E-4, 9.94730857E-4, -8.47386254E-4, -9.94144822E-4, -9.24524327E-4, -9.96671849E-4, -9.95290349E-4, 9.90399857E-4, 9.92561923E-4, 9.9778769E-4, -9.91268665E-4, 9.9106389E-4, 9.91855398E-4, -9.95182781E-4, 9.90162254E-4, -9.9000032E-4, -9.91538865E-4, 9.92999994E-4, -9.93227935E-4, 9.92302317E-4, 9.95503971E-4, 9.89457941E-4, -9.93986148E-4, -6.98388088E-4, -9.90565284E-4, 9.86032304E-4, 9.90497064E-4, 9.35149088E-4, -9.74465802E-4, 9.96848801E-4, 9.82844852E-4, -9.86666884E-4, -8.83933331E-4, 9.90752945E-4]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_19 = ""std.constant""() {value = dense<""0x5344EF3D6F3B673DAA17D2BCFC80D8BB3A659CBD8BDFABBC20C0E43DC24936BCDCA37A3B048FED3CDC876ABDA432D13DDCA59E3DB15ADABCDD4EA03DD654DC3DC02DC43C2E2A8ABDB2939E3D157A04BD361F1F3CFA9E25BDF88F99BDE1A8D8BD795D84BDC0E7FF3CD640793D7D12E1BD4FADA1BC89F9AABC7645ABBD9E28CDBC7DD1F23CD3C3A5BD0FABD1BD0D9B10BC264CBBBDCDE8EA3C0B15CBBD4A7AF63CAA93AF3D986C5B3D808A6FBD9118FDBDBE8AB0BC2DC1393D1CF2EA3D28A8813B1464803D54D8B7BC1BDEF8BDECAEB13D6D3700BE0ADBDCBD43E14F3D21DBE0BD3F86E9BD0914A73D0E649ABDABE0CF3D915CB03DD4CD003E73D400BC34DFF4BDDDA5A03D5521DC3D21B2D83D7FA049BB99CA493C272F263C87B59B3D5F36D63DD436B43CCF74FE3C937FFC3D8504933D532B813DFF09CDBDE626283B4403893D5F3E71BD3B68CABC8913A53D60C6B1BC7953A5BB25F7FEBC1F508E3DF1B7D43D09231DBD7148EFBDDE25E5BDB49F323CE29DE63D939BB73C853EC4BD37AEAC3C1E5A9F3C38094D3B00D0F83DD5798EBD6D06193D194BEC3CD435E73DEE0315BC622BF53B9A9DDDBD6213CD3D41A660BD7B0BA4BDD4CBC8BD02643E3CC2A0143CD760DABDF6DFC23C9D6D2CBD8FB1B0BD9C19C33D08A4BA3D9350F03D0DFADEBDED66BEBD326683BCF92B8C3DBE63A13D97324DBD36B7CBBDE8340CBD7E6B9D3DD693B1BD7D1EAA3D2CCC7F3C5D209ABDC6B0613DF9EBE9BDD293023B9938A53DC8901EBD766BDA3D8AD7C53D0748EBBD1DDEA03D4011083B17847BBD2D5DBABD0266AF3CA5EA42BD3EB48B3CAE1193BCEDE8333DF9F55CBD9F432BBD85C7573D37DAB3BDC4A1953DD9384ABDB035D0BCAF5A7B3D1CEDFDBD8C4BABBDF96740BCF393D93DA8EBBABC70E1233CC840A33DBEE46DBD312427BD2D9E0ABCBA08C33D7C4C19BD66C8193D742CF0BC0F4459BD1BC9B8BD7920CFBD483BBB3D3FB86CBD7DC88CBB62F5CC3DD8D3BEBDE230003EB4F3263D4C...
  %cst_20 = ""std.constant""() {value = dense<""0xAD2AB8BDDE00973DAD58C4BCAAAA8ABD75A3EEBDAA08B0BD2D53A93D8504B23DDE2FB33BB3BE9FBD989925BC0180AB3DFAD7873C54A8E53C03E8A33D09130CBD58F704399614103D97E8D33DEFB35D3D758777BC79D024BD6DFE94BD24C8F93D476802BDD1F54F3C10553F3D81D7153CF91AC83D9EA319BDAC8620BDB427AABD9A4974BD1D738D3D025E99BC2F7E73BCDE4A403CDBC7BABDE06C773C89D2863DB5C5AA3D583BA8BBEB8C0FBD823FA1BDB832E83D32FCB23DA16C273D96B7B63DF29EF53C0889C73DED0C94BD6E02923D5A82663DE94CB23D2EF47FBD00AC25BD1AF43A3DEA7770BCF737F9BDFBF3373DE486933D909AC7BC5D79FBBB67AF9C3DBD84E7BD233CF23DA612F1BD8D3F1DBD92C2543CD17DA33DD4AF93BDE9BD973C8A8CCDBDE528353D0C4898BD07FCE8BD8D29BCBD5D1F85BDA61EBF3DE184433D9DD575BB95EDB5BD6E2A07BD5EBAFDBD48BDED3D72CF12BCD0A3D13CFCE0DBBD2229363C6CEFEFBDAC20813DBDAA803D3182AFBD7C68D83DFF424DBDD227C4BDFE7BE5BD4FCA4D3DADD4CF3DC6B5B63D3A019FBDC6BC983D3BAAB23D5777833D57B5FA3D37D6E23C3592783B9C3BB5BD6664CA3B78CF8A3D5FDED8BD6B1DCD3C19FA5E3D6A26D93DB1FC913DF855433B59E44F3DDF7182BD8BA8A93C0C70F7BD8C801A3D5206C53D2C47873DB2E6953C11F0AA3DAB8BFCBDBAA3CA3D78D9703DB89D6D3D19439DBD3AEDA23DF0168F3DC76907BD9F8898BDEF5EF6BD9114C53DD43CF23DF13FF23D0893DABCF51D41BD786C123D0B26A5BB19B2263D82838BBDA85EE5BD2690B63DBED5C5BA7C7564BD6E56A6BD19FCC03CF05475BD78E06A3D0C62BC3D4FE8B2BDD99C49BCD47A2E3C688BD53D489791BCF7B716BDA65771BD5EC69EBB2E4B56BCAC51E73D780214BCA4B4343D4C23B1BD05F8A23D4E41B53D1A62FB3C23DFA23D3A5BC0BD63B3DA3C1F69D3BD36DDA03DF7DDEFBC874AE1BDE2A3A33C4DFD1B3D6483773DF80AD43D3106B9BD5D...
  %cst_21 = ""std.constant""() {value = dense<""0x049BE93D7A68483DF5E4723D2580483D76C58E3DA721C63DB6EECABDA84A2BBCF3CA943D1D4856BD4EAB20BD0E74FD3C39EFCFBD859A1F3D8EFF293B871BE43DCC4AB0BDD950D1BD2424A2BCDA6E763DA485C63DAA94F6BD6FFF21BDC5581C3DEA3C0D3D12F634BD665424BC304417BD493C8B3D6746FD3DDBCAA5BDF32FF7BD041A3DBC652130BD3390CCBD082A4FBDC4B9333D68D6A0BD1347783DEB589C3C0E4B87BD3A9EBE3D740E363D79114C3D5C08AE3D4A43B03D6910A5BB2522F8BD2CD2423D79659F3D0008DBBCE94957BDB476DABD66AF84BD0A822DBD5C6C22BDCC74D1BD7CDDC3BD5D3379BD61FCD6BDF149C6BD6CF3C33BD54295BDFC3BE43A68EE5F3CE8EB1BBD0C06A6BDF506E03D95FBE1BD84B8BB3DFA45223DBAE941BDC375333DAA694CBD61B3B2BCDD252BBD665B703DDB02EFBDA3CEA0BDA04E74BD7247763D54AB52BC804867BDCA3EA53D10419DBDEA13593D1EEADA3DDA3D373CB72900BEC178A73C04140CBD56A1B5BCA10AD53D145FB3BD6CE6EE3C2FCD803DE885113DFFD7A4BC0D0229BD65DCDBBD2B7E32BDFADB68BDDBA3C8BD8BFB973DADF3A9BD6BA79CBDDE5289BD691E8BBD3CF32BBD94268E3DE057B8BD34A97BBDD728DBBDDD87CCBC3EC91D3DA4186E3C94F2E0BDD8C727BDE067E63DD96AD4BD02F7AEBD71CDA23B593330BD675072BDC80E00BD7280983D30DFDD3BEE3A873D5D1A6ABDEC6617BDAA5AD5BDFDD60B3DDFA5AA3D14E05FBD2DD79C3D3833BEBCCACCF8BC1701D7BD135BE23B047CD63D5275AB3D904B473BCD9EAB3DF5A0983CF2509C3D473985BB494FE23DB29A9F3DDCA8A0BC8FF44C3DF76AC9BDAD84F3BDCFF94B3DAC8EB23DB00033BD8D7314BD1E60DDBDC38BA4BC3CEB98BDFFBCAA3DD626C9BDF906153CBD87473D76B2CF3C5DE8003EE5CD2C3D9EF7F93CEE2DAC3D109A9DB98D1CB4BDE875F7BD4065D7BD1BF4973DD281083DEDB1D3BDCCECBCBB1A55D5BDDDFB39BD950AC33C31A1853C8628EEBB53...
  %cst_22 = ""std.constant""() {value = dense<""0x5D62F63D997CCD3D4BA877BC4A6FB63DC912423D2C82183D2FA383BDA72B963D6EFED3BD4D49B2BD771090BD3654F3BD4D75333DE2D6C7BBF5B6AABD53769F3C79F556BD9F1BEF3DC5ADD23D4C76EB3D5153AE3D7EBAD03D5BA1D6BC9D1D9FBDFE8C93BA4816E2BDC8DD823D6FCD063C1DBE0D3DE13DEBBD537A9B3DAD92E73DD896F6BDDDDA953D6BFF94BD0360E73DC79267BDEAC94E3D875070BD198A27BDE113E33DFBBD2F3D440148BC47577E3DD1FEA2BD59BA083D3163F03D8E7013BDF103B83DF1901FBD113002BC6713B5BD29AA893DA79F57BC415AEBBDB6D20F3A8472A43D4A5C6ABDF9EEBC3D9A28513D662064BD4A66983AA1D7DEBCB1F2713DD81F89BC22DEFDBC06E136BD32DD2D3D5F5C633D52CB75BD30F0D63D88682ABB6602B0BD018489BD4206E1BD0998A8BD65EFD1BD4D52FCBD471C953D15D42E3DBF46923DED88E53D3BA8C6BC9CA3DEBDD7A1E3BD967CAC3DF35E36BC13528CBDDC24EFBD2599EF3D42D14CBD50D1B0BD01C3D73DA0AEA4BD9517ED3C2ABD0E3D73E5F3BD5B13C8BDDC26ECBCABB5C73D9C64FF3D65C4203DA898E3BB3E24953D9C31B0BDABC9F4BD78ECC43DFCEDBEBD5B1DA4BD8DBE64BD22FFF4BBDBC6F93DB20AB03D3676A0BD6541E33D6D59BB3D43F4AEBDEFDB2CBD0FCC863D4BB24DBC6C4204BC418F6C3DB2F8F5BDD467E4BD581304BDE2BE863D3262053B88C2F9BDF115A0BD06ACB23D3664BD3D223975BBB4F3EABC375DCEBD1FA79DBB83CBF0BDF9D5D6BD4E315ABDB799A9BD46D955BD3144CD3D9BBA753C3B5FA1BD3E158EBDE4E3D9BDC771D3BBFA7AAABD7A36A7BD0AEC6C3C93158CBDABCFC2BD1EEEAD3D643FA73C9DBFDBBD2829D8BCEB47133C22B665BD01F2B4BDF9495FBD1044813C03C9B73CAFE6B8BD32C7CE3DD6C81DBA59A8A13DFA6BC13D5DA5D03C541B69BDF2C0F6BD7F0C6CBD1D150BBD1A04833CAB3B583D57C8F33D6022C53C1411993DE33D67BDD79517BC60ACF73DBE3194BD02C5F13C39...
  %cst_23 = ""std.constant""() {value = dense<[1.00309217, 1.00306129, 1.00305521, 1.00297964, 1.00312328, 1.0031054, 1.00307453, 1.00310564, 1.00313675, 1.00305128, 1.00307429, 1.00305736, 1.003070e+00, 1.0029887, 1.00302231, 1.00311744, 1.00306451, 1.00375688, 1.00299633, 1.0030781, 1.00298929, 1.00297678, 1.00304294, 1.003090e+00, 1.00315344, 1.0030973, 1.00511754, 1.00309551, 1.00313067, 1.00304472, 1.00306332, 1.00306582, 1.00493622, 1.00297606, 1.00302935, 1.00298393, 1.0030632, 1.00310445, 1.00310719, 1.00504351, 1.0031389, 1.00311208, 1.00314331, 1.00296211, 1.00310755, 1.00303924, 1.00302505, 1.00302935, 1.00308597, 1.00316334, 1.00303519, 1.00307107, 1.00308514, 1.00304747, 1.00304878, 1.00306785, 1.0030061, 1.00506783, 1.00308836, 1.00306702, 1.00507414, 1.00302899, 1.0030452, 1.00310063]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_24 = ""std.constant""() {value = dense<[0.00105503935, 9.57053503E-4, 9.19337617E-4, -0.00135998649, 9.4322121E-4, 0.00111438113, -8.29911325E-4, -9.97727853E-4, -8.51589313E-4, 0.00107562402, -8.04005772E-4, -0.00109937368, 0.00111207424, -8.78392602E-4, -9.8252925E-4, -0.00109786494, -9.11238894E-4, 8.16963788E-4, -9.54134739E-4, -0.00101923943, -0.001126813, 0.00118361623, 8.54798709E-4, 9.88998217E-4, 9.06906615E-4, -9.89600433E-4, -0.00116529281, 0.00105114921, 8.55009945E-4, 8.50873534E-4, 9.1296091E-4, -8.51154094E-4, 1.086330e-03, -9.83795617E-4, 8.56689236E-4, -0.00103523524, -9.80255194E-4, -0.00102012744, -0.00115606235, 8.70998425E-4, 9.38069541E-4, 9.53909824E-4, -0.0011939907, 0.00102742529, -8.92813433E-4, -7.36643909E-4, -8.27286276E-4, 8.32178513E-4, -8.53449397E-4, -0.00132732699, -8.82953347E-4, 6.98925112E-4, 0.00104335765, -8.36898805E-4, -9.04436689E-4, 8.80992854E-4, -0.00101167895, 8.2154182E-4, 6.41766353E-4, -8.16440617E-4, -9.5120765E-4, -9.956470e-04, 0.00102537207, -9.21818893E-4]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_25 = ""std.constant""() {value = dense<""0x57DB0CBD98589CBCF5952EBD0F15A3BDF67FB8BDCE40E2BDEF01AEBDE96A9A3BCCA6FA3D00BD52B7717D39BD7535073CA1190EBD86AFFB3C909A783D69947DBCE275DCBC741E1D3C7655B33B0FB10EBE24EC883D7EE41ABD3C77DABADFA302BD45EA86BC642234BCB7F5CB3D1BCBE13C40D2C7BC1404D23B18616BBC207E8FBADCDB2B3D319844BD13F903BE7F4B0A3EC031543D97FA703DD35ABC3CEA50A1BDFD1D9E3CC128573C33151ABD431AB1BDA7544BBC5E766D3D3913F0BD33A8613D0386053DAD2B93BCE34D3BBDDD0DB03C24A202BD1A7DD83D4D7429BD34C57ABDA91C8ABCAB0A9BBC692EF4BD51C18A3D3D669B3C58387C3CC4EB9DBD696F5C3D169C05BD4B6314BD342F143DF44D34BDAEC08B3D99EC033C8C0D0ABAB2E5EABD46A7CF3D36A99BBDCD3003BD0CE8643C6A7FD6BDCD81463CF1BDC43C996EB9BC51C262BB5D381A3DC11208BE4901533DB260B7BD7C11483DCB68BCBB5E504FBD1EEF8EBD2444953D4A354ABD754971BCDF1381BDEB8DAEBBB777513D9B84A8BCF75E80BD44854FBD6441D73C9E98923B9E6730BBF02B3EBD0B7E89BDD8A0C13C3E17A5BDB803A73D72259EBD8FF9C33D0EC2C5BD75751E3DC8CC28BDDE59B23D0A571DBB1729483DFCA694BCB318BFBD71E4ED3DEF78233CB63E7BBD8E5748BDBD76493CA9A88CBDBC63283DD12F7C3D47CF4BBD0DDFBE3DC5AA05BB1484A93D772E123E2D8BA0BD915DA4BD2F18943D316FDEBD4DAA35BD1B72033D4894193C52AB18BE504801BD9C6E133D8E2E0E3DF92B583D7D60E33D05D30C3DDA25A6BCCFDBF9BCFDBD66BC3C8CA73C5EC90FBD38E7243EE6995E3CE04A7E3D0FCAB6BDC18515BD130BC43CA1C52FBD23F1623D9880DEBAA5AF8F3CFAF62CBD61E162BC70BCE7BD05F25A3DEC0BC5BDA1A3E0BBA6DA98BD0AD3BDBB668398BD57A79A3C0F28523D2EDE593DDFA711BD610EAC3D507FF8BBC63F96BDCA47B33DDE4C103DEBE4D7BD2E3A85BDFFE20D3DE346C1BD2AD7BDBC16...
  %cst_26 = ""std.constant""() {value = dense<""0x243AB1BDCD79CEBD1CED5C3D886FB73DEFED813BECC66B3D0B1C16BD2794AD3D33965D3C6882053DF2AFD2BDED128E3AA4A181BD60E8C33D5AF85F3D9E10A23B1471F33C9AF99E3CFDF8B53D614642BCB90694BDD88B5DBDBA7DFD3DBD0BA13CB63220BDEDBB203D26BCF73CFC6CFDBC4264B9BDFD5B95BCE28F65BDC8C2F5BC9A49183EBA09133D9B0C77BBB24684BCAA8C35BCEAA130BD5AD285BD7EC060BDAD7FF93BB0FE163D2C4D20BD58C2F0BC087FFCBD683A603DC8B88CBD6CD782BDEE772BBD9462323DF1B340BC12E8433DDBDD63BD3477AEBD860FA83BFAC9B6BD7FD7753C73C5FD3CC6C709BE60C60E3DB51D013DAC0F2C3D17B884BC5887EEBCEF237DBD9B09E4BC733B2C3D1C80CEBCDEE455BD4F04FDBCAD1E49BC43871E3C1A8ECA3DD20E94BD3B710E3EE2C2FD3BBEC82DBDCAC50DBDFB34C7BC93F06EBC69CA8B3DC6C58F3C7C0BCFBD76B20DBC08282E3D3A339DBD1B8B82BC244C81BD9312BB3DED724EBD0821613D92F4703DF433EDBD68C207BA0A90F1BCE8950B3CC97FD13DCE253CBC6454BFBCE9EA0CBD6AE87ABC031D203DBCCC063DC7F4663DC837163C1902D93C2A18DBBBBE25AB3DF6A756BD34B55DBAC2BC1D3C3C5542BD5EAE673D9CFA2A3C3BECFC3D7E4D54BDD3A00BBD88B5B7BD2FADBE3CD4EF3FBD0A6B7B3DBF47533D3E78133D802185BD03D3B33DA96BAE3DCF705C3DD6D238BD72C20B3D664893BB63218DBB7DC039BCFEF558BB22AAFE3DF312C8BC013DAABC3F77383E099EF13CD4096F3C82F286BDED74873D9E9DFFBB9FCC9ABB697E723D156042BDF276DCBC13F5F03C64E581BD957C0F3DF01A12BB913E85BDEEE2A0BDD01E28BDF00A063D168982BDBAA9F13B0F2FAD3C65E7DABD67D2B3BD8AA73A3B0BF7DCBCB63A843D0F782B3EF9B1EBBCA744093DB28B73BD9A7BDE3D12D26CBD0AF738BC02665F3D664B3BBD7BC5F13DDFC2DFBA33FC24BC57261A3DDFE25D3D14BAEDBCC1F281BD93D46B3DC8CEC5BBEE0DE43C55...
  %cst_27 = ""std.constant""() {value = dense<""0xB7BA29BECA620D3D20AEDBBDCCBDDEBAD30E06BE26719EBC3934383CC1D7C9BD64AB3A3DBF6F17BDEBBEBBBD64B084BDDEE5BE3D1326AA3C0F96943DBA169C3DAE31EC3CAE130B3D6793F3BD3CA1CB3C70FC43BA30AB123C08815E3D23BE82BD79AE013D89C0B83C329594BD3ED8903DD68541BD99E611BD57A6013E71C7283E05E89B3D58F7D9BCF7721CBD4E9D6B3CA373573D1D43CFBD3E417A3DD0FA85BBEC654B3D5E0350BD17BC993C9587B0BDE8E21FBED7152EBDDD2BDDBA0CBA633DC50E0D3E006C96B899F709BE170D8F3D453EFABCDDBF513CDEA0CA3C50C5A43D0D7776BC92EC04BDC0FEE33D901673BCE986D1BD297B223C5F35103CE28748BB8F6AE33D669F8E3DDAC5EB3DB23702BD5EBFEC3BD2D5C5BD75A218BD86DC9B3DCCC3F2BD2C90A23C1685C13C7EFCA0BD3494AE3DF8FACF3D90D6D03C8A2B97BCF78D783A7DC8863A4F5BCFBD2C9A6DBD7D168DBA90CA853C0020C7BC93526F3C14AC773D4F593B3DEA5D733DDBF0093DEDAE0C3D0C9115BC73D80EBC63BE82BA682CD0BDCAC09F3C8276E8BDE8B1B1BD80B19CBCF54CDE3B8FA52C3DE46312BD448A0DBC7220EABD56B6273DDBDECE3DA50DA33C3719843CFF151DBD9E500F3B0713E13B8070E5BBE8CDC5BDD83B4C3D6A906A3D86CAF73C80482CBBD7F815BDDA30893DF15EC93CB056AEBD74EE08BE0B05A73DCC6162BC146C8DBC3FA3C83C6C700B3E407C9BBD6F2593BD56356E3CC687063B0EDC46BDB0BB9B3CA8E916BD5A97EB3CE4DFB93D0E23F53C35078CBD8A2206BDEB0F013CC85FA13CD8D2F43D49FCF23C9E582D3DC3D171BB87D40ABDA395813B36D0173CB93B153D5C1662BBE4383E3DCC5013BD4798D63D0E5CC2BA1F7804BEB417543D20C1803CED3CB8BDC4B407BD6536FF3D0FF19B3DF50E313C5EB773BB498EDEBA77BB9EBDEEC5813B50EE3FBD591A093D5F4AF5BDB198C43A117BB73C23B7903DF63F16BDC373F73C2AF111BD1AAD08BC9A695EBC994078BC5F9770BD82...
  %cst_28 = ""std.constant""() {value = dense<""0x3B2952BD677CF73C5B0D57BD9D4EE8BD51532C3E5255FB3A9457EA3D6E4389BD01FB85BDD2F7753D09B23A3C74B8E4BCEE0DECBDA09AA1BDD545D83C116A7B3D66DBC2BD3318D73BA2EAFA3C3A5CF6BC249C0DBDBC33B4BD4834233ECA2B4DBD99490D3DE709743D006A68BC70A7CCBC2AEA47BCD4C3383E1893433BC7B6E23DE79A90BD211FF5BDF370ACBDE1BE9FBA9B5594BD1E0E0FBE4DC4DFBDBB4364BD0E780ABBA0D5F33B51969D3DA31D0BBCF6BB9A3DF0FD183CC10C1ABD300144BDDA9665BB256B033DADEB813D3818A0BD0EE55A3DFC4FC1BC49130EBEB4FD8E3D65D696BCB4A37EBA6AE42CBDBD6551BD0E80DA3D072448BDAFF731BDE94905BD91F1C4BBF82A943D88BA963DCF358C3A9426A2BC18D114BBAC5A44BC60A2913D6C65C33D1FB8553BA2643A3D386C493BB014373DD428763D34A6B4BB888E773D4FE68FBDE8E8B3BC43CBA2BD9A57C3BC66EB5B3C8A9FD13C7143493D9EA0A63C970CB53B738686BDDEF6353DBE80583BEC4EBC3B70BFBDBD2C25B83DF3A90ABD49B9E93BEAA4663C55915CBDFBD81BBD46E069BD91B5A9BD18DBD9BC0C676CBDC7473CBD8AEF893DBF83213C278C453C8064A839C4CD393E0705933DB8A772BD4B9E8B3DFA9D863D64F30E3D63EF31BD4D00313D1F2B273DC1C1A63D45AF453C776B5F3CE801CBBD114E1C3DE601B4BD27E70BBCED12123DF031C6BCD132943D5793513D0A6FC23CA03545BD23BE083E36C7853D2B270D3BBC43BBBDFE94953DEF66C73D9486073D27149E3D8194AE3CCFF1D6BD648A7C3DFB6D153E29F854BDFEFE66BD1D47693DE950B83D8F62F63C239EAEBC481D4EBDD95AA5BD9AD4783D07FC703D7B8229BD873E47BDE099943D63D4373D8037B93C730D113E60DA0C3D4B121EBDDE0E5B3DE5649EBD29400B3D795B4B3D161FB93C8FF2513DB3CBA63C267D283D746AE03D57555FBD56BAF23C6AC7023D0ACB1CBEC92324BE53B2B5BDDADD4D3DB995DCBC9BDC053D61881BBEB67A02BDCE...
  %cst_29 = ""std.constant""() {value = dense<[9.67585132E-4, -9.76218085E-4, 8.44327267E-4, -9.95431095E-4, 9.78885102E-4, -9.89362248E-4, -9.97170456E-4, 9.94593254E-4, -9.74801718E-4, -9.967850e-04, -9.7882736E-4, -9.50525922E-4, 9.79408272E-4, -9.92655172E-4, 9.97303985E-4, 9.82631579E-4, 9.90360276E-4, -9.92504647E-4, -9.657435E-4, 4.47311701E-4, -9.95136913E-4, 9.76480427E-4, -9.81111777E-4, 9.97065915E-4, -9.86331375E-4, 9.94870206E-4, -9.89393214E-4, 9.676800e-04, -9.70026711E-4, -9.91622568E-4, 9.94842383E-4, -9.94916772E-4, 9.85594349E-4, 9.97806666E-4, -8.78486142E-4, -9.95528069E-4, 9.8985678E-4, -9.93425492E-4, 9.95124923E-4, 9.83530771E-4, 9.93813737E-4, 9.74371622E-4, 9.73470451E-4, -9.74504452E-4, -9.87119157E-4, -9.95987677E-4, -9.89082618E-4, 9.94357862E-4, -9.81126562E-4, -9.95713984E-4, 9.92701738E-4, -9.9737267E-4, -9.672870e-04, -7.41955242E-4, 9.90436179E-4, -8.20829126E-4, -9.95927955E-4, -9.940910e-04, 9.85695864E-4, 9.85741498E-4, 9.95741924E-4, -9.95478476E-4, 9.654010e-04, -9.80447861E-4]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_30 = ""std.constant""() {value = dense<[0.999001801, 0.999005556, 9.990270e-01, 1.00099504, 0.999001979, 0.999002933, 0.999001145, 1.00095022, 0.999001979, 9.990040e-01, 0.999001502, 0.999003231, 1.00099468, 1.00091136, 1.00086093, 1.00098407, 0.999001622, 0.999009847, 0.999010264, 0.999002337, 1.00099421, 1.00099874, 1.00095701, 0.999001383, 0.999006628, 1.00099599, 0.999303638, 1.00099146, 0.999027907, 0.99900186, 1.00099814, 0.99900937, 1.0009948, 1.0009985, 0.999017238, 1.00099349, 1.00099826, 1.00098813, 1.000934, 1.00099516, 1.00097382, 1.00099266, 1.00099349, 1.00099623, 0.999081313, 0.999002218, 0.99900335, 0.999011158, 1.00099492, 1.00099647, 1.00099587, 0.999002039, 1.00099766, 1.00099695, 0.999008536, 0.999002814, 9.990050e-01, 0.99900335, 1.00099838, 1.00099766, 1.00099349, 1.00099289, 1.00097156, 1.00099587]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_31 = ""std.constant""() {value = dense<[9.99597366E-4, -9.9919259E-4, 9.99345094E-4, -9.99698997E-4, 9.98205738E-4, -9.99698182E-4, -9.997270e-04, -9.99498763E-4, -9.98643808E-4, -9.96864866E-4, 9.99229145E-4, -9.99716343E-4, -9.98876639E-4, -9.98718547E-4, 9.98445553E-4, -9.99442767E-4, 9.99625306E-4, -9.8527607E-4, -9.95822483E-4, 9.97575931E-4, 9.998920e-04, 9.99492942E-4, 9.98938339E-4, 9.99762909E-4, 9.9961739E-4, -9.97697352E-4, -9.98864998E-4, -9.97156603E-4, 9.99353942E-4, 9.98489558E-4, 9.99433104E-4, 9.99394804E-4, -9.98000847E-4, -9.99715761E-4, -9.94169735E-4, -9.9937967E-4, -9.99294221E-4, 9.98356263E-4, 9.9906174E-4, -9.97644616E-4, 9.99090727E-4, 9.94966947E-4, 9.99400159E-4, 9.963630e-04, 9.97329829E-4, 9.99510521E-4, 9.99477691E-4, 9.99089912E-4, 9.99575247E-4, 9.99586307E-4, -9.99113777E-4, 9.99390496E-4, -9.97681403E-4, -9.83843114E-4, -9.97356371E-4, -9.98840085E-4, -9.99069889E-4, -9.98023431E-4, -9.98752773E-4, 9.9920528E-4, 9.99455805E-4, -9.99182928E-4, -9.99532174E-4, -9.97857307E-4]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_32 = ""std.constant""() {value = dense<[9.81708988E-4, 9.63601749E-4, 9.90674947E-4, -5.41209592E-4, -9.96806542E-4, -9.83843347E-4, -9.94779984E-4, 9.9403772E-4, 9.31107497E-4, -9.93747846E-4, 9.95395239E-4, -9.97131573E-4, 9.82350902E-4, -9.86792613E-4, 9.92027809E-4, 9.9402538E-4, -9.92390792E-4, -9.95012698E-4, 9.74349154E-4, -9.90020693E-4, -9.8652183E-4, 9.94545291E-4, -9.17962461E-4, 9.61345387E-4, -9.95251466E-4, 9.94973583E-4, -9.72526962E-4, -8.81796877E-4, -9.74486291E-4, -9.71922826E-4, 9.95795126E-4, -9.96316666E-4, 9.90363652E-4, 9.97908413E-4, -9.92957735E-4, 9.958980e-04, -9.9491293E-4, -9.92778339E-4, 9.9361909E-4, 9.95804904E-4, -9.4126031E-4, 9.86351747E-4, -9.81890945E-4, -9.85238584E-4, -9.91161214E-4, 9.91085078E-4, 9.90797532E-4, 9.80959506E-4, -9.92152839E-4, -9.79809905E-4, -9.88770509E-4, -9.91929322E-4, 9.80777665E-4, 9.96440299E-4, 9.86681901E-4, -9.95011068E-4, -9.93575318E-4, -9.95142967E-4, -9.86838829E-4, 9.61801095E-4, 9.941320e-04, -9.97495604E-4, 9.76995914E-4, -9.35959106E-4]> : tensor<64xf32>} : () -> tensor<64xf32>
  %cst_33 = ""std.constant""() {value = dense<""0x3CBBEA3B451EFC3C60A407BED262B2BD7C0C7A3CFF35FCBD7183ABBC10F0DEBD85B86F3BF859E8BD1E64D33D55CE933DDEAA983D9FEADB3CDA31EC3D3C21CABDCD54ACBDB8C4A53D1C5EE43D1F7C923C3235963B7944063DAB03AD3DEB26ADBD296D1FBD1F3076BD45E99E3CA6A8813D07F06FBD219E023ECF15E3BD65EF063EC05155BDCE53BE3D2376CEBD4699F9BD562A7DBD1212AD3D42EDC0BD5B01773C7A7302BE3B3154BCE519743DDAB986BDCE14F8BDAB8836BD28DB81BD3A817F3DE73DC73D70DBB6BDB0834C3D1304043EADAB303D7655613D43489B3D9D721B3CA09A64BDE792CEBD6595DABD8433703D64C8963D54EE823CA734DC3C2ABD923DFE0A9EBC5A62A33CA5A88C3DA94A37BD71A7D4BD6FDD993BDAB6B1BD0C41A0BD1AD223BD0896563D5B6883BDE674E7BD5F2828BD592626BDB648683D3EFB8D3DC9D5923C7A5A253D020280BD7AD50A3EA061933DEEE6EA3DA68980BDC1E8D73DED3E073DA01DD4BDD8A40EBCC040093DA770DBBD9FDFC6BD82B5B1BDB2CC863C1324FBBC1608A0BC5C5D623DEE427F3D76A0E1BC28ACC1BDEA90FB3C56958A3CCF4B9DBD9C0E75BDCEC400BE4A27B9BCD59D9ABDE0DE8D3DA88210BD4630073E8E5EE53DBA0D78BD9EDC6DBDEEF5393DB1E0013E2978033DAEDF66BDB14407BE71B99A3C54988A3BB329A2BD14870DBD3961FDBCF374FABD966B05BEF002BC3D12D97B3D10827FBCA725B8BD31450F3DF0AA3CBDE61ADE3B88D2083E1DF108BEA99499BDE546FA3D38DD04BE59D6CFBD38D4543D15C7E93B9679813D34A20C3ED939D5BD2CA7E23DF4F10BBE60166EBDDD6CDBBB6A0C33BDBC04F03D9EB6AE3D8B38CBBD8B162B3D45AC2D3DB9151B3DCBF58D3CD2030A3DCACEBE3D041E39BCD2ADD3BC429D92BD5957413DE843D23DF490C4BDB5507D3DCA2E3F3D2B4533BD60AEF03D983C61BD1A148BBDBBF7913D22773EBC47ADD7BCCE0D25BD1BC232BDEB76D2BCA2E7803C6D1319BD5C34B0BDEBDC88BDD3...
  %cst_34 = ""std.constant""() {value = dense<""0x014B1ABDB51FB4BD77A132BD54BFD0BD4F596A3D1439DBBD4B39EB3C3275BB3D4E52EA3B0C8E09BE29C90A3EEE3700BD61CF76BD068808BEA24409BE589A66BD592285BB270BDFBA970207BE86B801BD95EEEBBDA06603BE7BC2BC3D6FE8373C88BF633DD3B03C3DCBECDBBC90F75FBD7C7E1FBD6475ED3D99DD51BD16F0DE3D8BA496BC0371E23D2845C8BD240BDDBD06C70ABECBAE9FBDC59E7EBC127B74BC6C35E2BD0E0204BE62115DBD8162903DFABBC5BDA22CD63C20D287BC54E58E3D915B523D34A8A83D10299DBD4D61D0BD87EAA53D446D613D4CBF5D3D494C573DB2AC4CBCE665B3BDC4D505BD3948DC3CB294F73C1FAF88BD7E9C193DFF06A23D0164063D49C43A3D8A2BF1BD0F5EC53B666907BEDF79373D55F1D3BD551C3A3D9D624CBD78BB9B3D5BEF8A3DCC4AEB3D37CEE73DA0F3D23D5AE4333D398D333C0306E5BD1FD7FD3DFFA9C7BD7039563DFD73EA3D71A1033E011503BEF3378C3DCD73AABDFF37833D52D096BD3D34BB3C54DEDE3D37F3AB3D70D93B3D8A3EFF3DF7C0C23D33575BBD7DEA00BEF8A9933CFE7813BD42A5E23D330B0EBD91A6A7BD3FCD3B3C2336323DCA5FE03D9F115CBB8C62D9BDCD34D73D86A4E1BD4678E03C053EBD3D567A7D3D19B108BDDBF6043E64208E3D4BA3C7BCC430D13D8135DFBDDD03D93B5F07EA3DB103853D8910963CA87AAEBDD58C9BBDF4FC123D6BEFB5BDC1039DBD8BF9EA3D9217E63DD208E7BDB178BB3D150883BBE2B1B2BD231692BD66571FBD09410BBD0CEAA33CA5D80F3CA0EB393CECF9713D7945B9BD2F86E1BDFED7D03DFB04FEBC0A5E623D19BE9C3D3691C8BCA05488BD0BF2003E26D94C3DA780E43D15E7BC3B012DD93D4E7904BE9B62C2BD3F70C5BD5D0883BB1125E63DA76D5F3D1357A13D7CA6DD3C5FA5043DE6124FBD253E18BD96AA56BD2862433D7415093E81CB583C25F1D03DE67913BDA296BCBDD8BDBB3DBCB286BD45C0873CD374D9BD1559DF3DADDD363DC94E09BD045B313D43...
  %cst_35 = ""std.constant""() {value = dense<""0x5D257E3DBA77A83D1581D63D2E973EBDA2A94DBD964DF53DFAD025BC0FF4C2BDF04190BDEE3EF63DCDE432BD168D683D0505E33C1F296EBD1F04DA3D367BF53D7BEE3F3DF8E0BF3BFB4BB6BDE1A901BD3FCDA63DF6C21BBD835D033E90BFE6BC6636C1BDF1DCBD3DBEC6D2BDFDC6D7BD6505F63DC242293DCF3833BD3756FE3C1102E9BD9A23A73CBAFE8B3DAAC1343DE017543CFCE2F53DAB0EB5BC55BE903DCC7B2FBD4047863DF7851ABDD6108DBDD0583C3DE83617BC68409B3DC56C913DCE77F33D119805BE7DB4FDBDD7211D3C14E68ABD8DF580BB31F189BD20D4033D05A4613DEFD400BD18BA263D79EEE13D338CC6BD99A5A13DB00219BC5A25813DDDBA92BDCBC1BE3D163D5D3D9F0E063E21DF79BC2BBCBF3D41A565BD820EC5BC547700BE6C6605BED73D93BD37E4FB3D05D896BD950D6DBD41B33F3D27A1D0BD1EF30F3DD5E09C3D0616D2BB350F903D0362B03D79E6C8BD248981BD707E653DA0AB09BD46F375BDEE20AE3C68F05BBCD2BBA93D9667C9BD79C4C53DC05B043E54A6F1BABACF26BDEDB7773DC6F69EBD472836BC20C101BEFC594B3C4FE1AC3DBDC1833DE29B563D1AA472BDF7CEAEBDB245F63CBC9BF53D74252C3DDFB7CA3D386E97BD3149C43D4481193D520EF93D6017E4BD20CAE3BC8CCD6DBDB81BA43D50C8343C4685BABDDC66003DC0FB15BCF78402BEC98EFEBDA818EB3D3AF8C8BDE9182DBD94E9F63DE5FAC63B4B81B9BD5CA7063E3A8F44BC174B1CBD1438033D2E0D873DED16B6BD637A403DB264DE3D271984BD35E2C63CB16FB53D8CDD2B3D8266E3BDFAAA373C8026B8BD6F229DBDD2C31ABC72CA5E3DEA95363DAB6EDBBC6B5B2F3DCB6F95BDDDD8EA3C8901B73D28680A3E9759F13D3B6FEDBD1908573DCA1696BCA5391CBCD6D098BCB976ADBDB54E06BE8371E23D5151EBBD6732A8BDAE23A83DB32EB8BDAE81083E6E7050BCB540B3BC309A82BC6E119DBCC77870BC10F2E23D1B6E943D5AE878BD8AA005BE709BB7BD22...
  %cst_36 = ""std.constant""() {value = dense<""0x995CD03CBBE0F2BC23A0B83D48070DBDF106813DE32A793D0519BEBDB63500BE784AF03D7D1305BEA39E16BD33ED57BDCA32003E1E046A3DED87D53DCB0B973C34B9E93DD9DEFCBCD0BE98BDC43B023E21A9913C4627BBBD7968C5BC1B3BFE3D0F0D8E3D9634BABD105803BE6B94E63D9E21053EBA02443DF1C36F3CFB839B3DF33E093E6068933C7EDFB0BD083287BDB414D1BD03AA97BDB145483DB00B033DE32F993B77EA6DBD1E9EA03D9C9203BE6C34C53DF5F68C3D322ECEBD15D28E3DB4F9ECBCD16A2A3D4E5010BD9ECD1CBDA52C87BCEDD8F83C012CD8BC0282903D597C61BD031D093E29D503BE3EDC463CC16181BC0DE1C93D945909BE259AA6BDE1E3093D17954C3D88372E3BDE8DD0BDDEE0993DDA9D11BDDB4B153D925C763D4EF890BDB87F5C3D949394BD9CB4C4BD423FACBD59E4C43D0794163C29518BBBAC9DF13D5B3A90BD03E6D9BA55EDCCBD505E51BD39AC9CBCC8FBF43DBD6F043E587DCEBC68F6D33C2F98E33DA63BEB3D5745273DD450AA3D27C306BEDCB2283B2AFDF2BCEDC7E23DC93CD13DC4366FBCC26EE3BBDCEEAA3D5C1BF63DCEA0083E90D650BD5DF6903DA98E813DDA45D7BD905F00BDE0FC01BD8CBCA5BD189D89BD4D14393DF7AE0DBC40F34DBC44231A3D3CE5BEBD251A8ABD842DB5BDB6417DBDB4CE51BC313E08BECE2BE9BCD31644BCC43D6D3DCACACF3D64C1B0BD88C0203D2413C3BD6F8C2F3CC92935BDBB6A89BC233F52BD143A79BCCA0B9A3D9A346DBC1D79AF3D7EB2053E89CD053E23EE00BEA38193BB9BF92D3C994AF4BC5AB848BD95C6E9BD29354DBD664080BD22B2863DE8B2F63D2A9D043EFADDF83DDCB348BD8C43073EF1A77F3D8F37743D7A43813D7D3294BD01EFDC3D66C6973D7DB3F7BD5F09A6BB1960183DE14360BD3C9DDB3CD948DE3B570B79BCC223E83DAE53643CDF1A883DE6B9D9BD7E3C2BBD90DF603DE8BEE6BA3C009C3D3EEC2FBD62B506BE8A230ABEFABE71BB7AC709BE312AE2BBD811A1BC24...
  %cst_37 = ""std.constant""() {value = dense<[[0.141972259, -0.140112832, -6.299400e-02, -0.188756421, -0.296468705, -0.193415761, -0.257958204, 0.138051048, -0.116950154, -0.166800201, 0.279425412, 0.252810538, 0.0246988367, 0.0818047821, 0.16655989, -0.172646925, -0.231393486, -6.86961692E-4, -0.0970985144, 0.223860174, -0.260656685, 0.19828698, -0.0498352349, 0.245914236, 0.0637414455, -0.111480176, -0.0401437283, 0.0500495099, -0.125676543, 0.205395907, 0.16058588, -0.00452343794, 0.0116028748, -0.248046264, 0.07183934, -0.224206865, -2.637570e-01, 0.066637516, -0.215825126, -0.0450223759, 0.197781667, 0.0415594503, 0.103903204, -0.0888473243, -0.0138380127, -0.186089963, 0.185404703, -0.167652592, 0.23974292, -0.218838304, 0.213134632, -0.162772402, 0.181808576, -0.103578761, -0.0596696399, 0.105313681, -0.101291053, 0.0236400794, -0.167137861, -0.129845485, 0.107204571, 0.193863332, 0.0873046442, -0.0449375287]]> : tensor<1x64xf32>} : () -> tensor<1x64xf32>
  %cst_38 = ""std.constant""() {value = dense<0.000000e+00> : tensor<128xf32>} : () -> tensor<128xf32>
  %cst_39 = ""std.constant""() {value = dense<""0x11D42CBD71902DBBAF29743C2A2046BD3D29603DA16E253C59952F3D6711333DE78F003D62E7983C43FC32BD7CEF39BD549D323DAC6DD8BB8D966C3DA49C59BD4A37633DF48E6EBD97D7303DC92BF2BC4B556C3D1D015F3DE9623FBDBB26BE3CEB0D373BBE8CB0BCAF44B7BB10770DBC078021BD6DC820BC7EBF1B3DDAB0003DE611E83CB0176B3A1CF9793D5EFA2DBCDC6C83BC36485EBCA0DC6D3DC12CC53BB0F4203C50F4F8BCC6AA5C3D37C1D0BCFC9D80BB9184B5BC1907B23C816A64BD9C592B3D3F9597BCC49A1EBDC77677BDFDD033BDFEBD78BD1EB8563D10AB72BDC65A883C6F32B93C1FF4483D0B81D3BC21BE743D26379BBBF426C9BCD079413D963334BCC8A072BDB28DA0BC5C6E763C1411B83C646B203DCF8F0ABD8F68EABCE98177BDC03B13BDDED4433DE78B42BD693F3BBC4BB11EBDC14911BDFF5E623D1FA1E83C56BC34BC950A21BDECAD4E3C4B8D423D038441BD2536E0BCC4F1483D1123A03CD98765BD39D2BFBCDB07BA3C77554ABD922F8FBC175AEBBBBE6F613D8BB24A3D3B9B8D3CEC540DBD8A5519BD2A4AA1BCD0835ABDBA1A8D3CE5A677BD5AD34BBA6D350E3D45A85F3D9C85373D70C191BB7AF56E3DD60993BCF984E0BC0D0C96BCB27C2EBD5300A9BBD805433D63F0403D53013E3DC01A133C7FDC733DC71D283C8B9E7BBC9568753D2FA60C3D485BBC3C921A353DD35C0A3BD3C69C3C3857133D52ACCC3CDEEF3DBDB8D1A0BC0A854ABD7967ADBCEBA4303DA95E33BD8F4B853C1EDF0ABD697C3ABD24378A3CF3E05E3D139DA63CBD0B373D48F1E43C5CF140BCEF6804BD147B263D2E1BCFBB35A9023DA01444BDD5E0093D05B8713C58BA5D3D5A41BBBAF4B5E4BCB8ECEB3CBE0B61BDF3AC28BD863C813B958456BDD0ED2ABD6492113DA355663C8322F23CDB5079BD1D40953C210C483DB1B0BFBB3A137FBD3DBB6EBD7D2B8A3CFB42CFBB44F64DBDA1BC473D7099A1B932482BBD08A304BCC7C14D3DD09DD23CF9FC4CBD6440683DF9...
  %cst_40 = ""std.constant""() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_41 = ""std.constant""() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>
  %0 = ""tfl.expand_dims""(%arg0, %cst_1) : (tensor<?x1000x100xf32>, tensor<i32>) -> tensor<?x1x1000x100xf32>
  %1 = ""tfl.conv_2d""(%0, %cst_39, %cst_38) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""VALID"", stride_h = 1 : i32, stride_w = 4 : i32} : (tensor<?x1x1000x100xf32>, tensor<128x1x7x100xf32>, tensor<128xf32>) -> tensor<?x1x249x128xf32>
  %2 = ""tfl.squeeze""(%1) {squeeze_dims = [-3]} : (tensor<?x1x249x128xf32>) -> tensor<?x249x128xf32>
  %3 = ""tfl.add""(%2, %cst_0) {fused_activation_function = ""NONE""} : (tensor<?x249x128xf32>, tensor<128xf32>) -> tensor<?x249x128xf32>
  %4 = ""tfl.mul""(%3, %cst_7) {fused_activation_function = ""NONE""} : (tensor<?x249x128xf32>, tensor<128xf32>) -> tensor<?x249x128xf32>
  %5 = ""tfl.add""(%4, %cst_8) {fused_activation_function = ""RELU""} : (tensor<?x249x128xf32>, tensor<128xf32>) -> tensor<?x249x128xf32>
  %6 = ""tfl.shape""(%5) : (tensor<?x249x128xf32>) -> tensor<3xi32>
  %7 = ""tfl.strided_slice""(%6, %cst_40, %cst_41, %cst_41) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<3xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %8 = ""tfl.pack""(%7, %cst_3) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %9 = ""tfl.fill""(%8, %cst_2) : (tensor<2xi32>, tensor<f32>) -> tensor<?x64xf32>
  %10 = ""tfl.unidirectional_sequence_lstm""(%5, %cst_19, %cst_20, %cst_21, %cst_22, %cst_11, %cst_12, %cst_13, %cst_14, %cst_6, %cst_6, %cst_6, %cst_15, %cst_16, %cst_17, %cst_18, %cst_6, %cst_6, %9, %9, %cst_6, %cst_6, %cst_6, %cst_6) {cell_clip = 1.000000e+01 : f32, fused_activation_function = ""TANH"", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<?x249x128xf32>, tensor<64x128xf32>, tensor<64x128xf32>, tensor<64x128xf32>, tensor<64x128xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, none, none, none, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, none, none, tensor<?x64xf32>, tensor<?x64xf32>, none, none, none, none) -> tensor<?x249x64xf32>
  %11 = ""tfl.mul""(%10, %cst_9) {fused_activation_function = ""NONE""} : (tensor<?x249x64xf32>, tensor<64xf32>) -> tensor<?x249x64xf32>
  %12 = ""tfl.add""(%11, %cst_10) {fused_activation_function = ""NONE""} : (tensor<?x249x64xf32>, tensor<64xf32>) -> tensor<?x249x64xf32>
  %13 = ""tfl.cast""(%12) : (tensor<?x249x64xf32>) -> tensor<?x?x64xf32>
  %14 = ""tfl.shape""(%12) : (tensor<?x249x64xf32>) -> tensor<3xi32>
  %15 = ""tfl.strided_slice""(%14, %cst_40, %cst_41, %cst_41) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<3xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %16 = ""tfl.pack""(%15, %cst_3) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %17 = ""tfl.fill""(%16, %cst_2) : (tensor<2xi32>, tensor<f32>) -> tensor<?x64xf32>
  %18 = ""tfl.unidirectional_sequence_lstm""(%13, %cst_33, %cst_34, %cst_35, %cst_36, %cst_25, %cst_26, %cst_27, %cst_28, %cst_6, %cst_6, %cst_6, %cst_29, %cst_30, %cst_31, %cst_32, %cst_6, %cst_6, %17, %17, %cst_6, %cst_6, %cst_6, %cst_6) {cell_clip = 1.000000e+01 : f32, fused_activation_function = ""TANH"", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<?x?x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, tensor<64x64xf32>, none, none, none, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, none, none, tensor<?x64xf32>, tensor<?x64xf32>, none, none, none, none) -> tensor<?x?x64xf32>
  %19 = ""tfl.mul""(%18, %cst_23) {fused_activation_function = ""NONE""} : (tensor<?x?x64xf32>, tensor<64xf32>) -> tensor<?x?x64xf32>
  %20 = ""tfl.add""(%19, %cst_24) {fused_activation_function = ""NONE""} : (tensor<?x?x64xf32>, tensor<64xf32>) -> tensor<?x?x64xf32>
  %21 = ""tfl.reshape""(%20, %cst_4) : (tensor<?x?x64xf32>, tensor<2xi32>) -> tensor<?x64xf32>
  %22 = ""tfl.fully_connected""(%21, %cst_37, %cst) {fused_activation_function = ""NONE"", keep_num_dims = false, weights_format = ""DEFAULT""} : (tensor<?x64xf32>, tensor<1x64xf32>, tensor<1xf32>) -> tensor<?x1xf32>
  %23 = ""tfl.logistic""(%22) : (tensor<?x1xf32>) -> tensor<?x1xf32>
  %24 = ""tfl.reshape""(%23, %cst_5) : (tensor<?x1xf32>, tensor<3xi32>) -> tensor<?x249x1xf32>
  ""std.return""(%24) : (tensor<?x249x1xf32>) -> ()
}) {arg0 = {tf_saved_model.index_path = [""input_8""]}, result0 = {tf_saved_model.index_path = [""time_distributed_4""]}, sym_name = ""serving_default"", tf.entry_function = {control_outputs = """", inputs = ""serving_default_input_8:0"", outputs = ""StatefulPartitionedCall:0""}, tf_saved_model.exported_names = [""serving_default""], type = (tensor<?x1000x100xf32>) -> tensor<?x249x1xf32>} : () -> ()



```

"
43648,LookupError when computing nested gradient with UpSampling2D,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7.4
- CUDA/cuDNN version: 10.1
- GPU model and memory: NVIDIA GeForce GTX 1070 8GB VRAM

**Describe the current behavior**
The provided code fails with the following error (see Colab link for full stacktrace):
`LookupError: gradient registry has no entry for: ResizeNearestNeighborGrad`

**Describe the expected behavior**
The GradientTape.gradient method should be able to compute the gradient.

**Standalone code to reproduce the issue**
[Google Colab standalone code](https://colab.research.google.com/drive/1eGMvXJxVvY7zb8OISkS1H3D90N0J380d?usp=sharing)

**Other info / logs**
I encountered the error while developing a GAN. I used Conv2DTranspose for upsampling at first but encountered artifacts. After changing Conv2DTranspose to a combination of UpSampling2D and Conv2D (as is common in GAN's) the inner gradient stopped working."
43646,CMake issues when building TFLite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS
- TensorFlow installed from (source or binary): source
- TensorFlow version: 8a643858ce174b8b (master)
- Bazel version (if compiling from source): CMake 3.18.2
- GCC/Compiler version (if compiling from source): GCC 7.5.0 / LLVM 6.0




**Describe the problem**

I'm seeing various issues whilst trying to build `libtensorflow-lite.a`. I have a fix for the first but not sure what the remedy is for the others.

1] Missing dependency on `sparsity` required by `lite/kernels/density.cc` and `lite/kernels/fully_connected.cc`

This fixes

```
diff --git a/tensorflow/lite/CMakeLists.txt b/tensorflow/lite/CMakeLists.txt
index ac7e373f18..b731401fc1 100644
--- a/tensorflow/lite/CMakeLists.txt
+++ b/tensorflow/lite/CMakeLists.txt
@@ -264,6 +264,10 @@ populate_tflite_source_vars(""kernels/internal/reference/integer_ops""
 populate_tflite_source_vars(""kernels/internal/reference/sparse_ops""
   TFLITE_KERNEL_INTERNAL_REF_SPARSE_OPS_SRCS
 )
+populate_tflite_source_vars(""tools/optimize/sparsity""
+  TFLITE_TOOLS_OPTIMIZE_SPARSITY_SRCS
+)
+
 
 # Common include directories
 include_directories(
@@ -294,6 +298,7 @@ add_library(tensorflow-lite
   ${TFLITE_KERNEL_SRCS}
   ${TFLITE_NNAPI_SRCS}
   ${TFLITE_SRCS}
+  ${TFLITE_TOOLS_OPTIMIZE_SPARSITY_SRCS}
 )
 target_link_libraries(tensorflow-lite
   PUBLIC

```

2] The built library `libtensorflow-lite.a` doesn't itself include all the dependencies it needs as build in `[build dir]/_deps`


3] Enabling `TFLITE_ENABLE_XNNPACK` gives the following error when built with GCC (build's OK with LLVM)

```
/home/gavin/CK-TOOLS/lib-tflite-src-static-gcc-7.5.0-v2.3.91-with.xnnpack-linux-64/build/xnnpack/src/f16-clamp/gen/neonfp16arith-x16.c: In function ‘xnn_f16_clamp_ukernel__neonfp16arith_x16’:
/home/gavin/CK-TOOLS/lib-tflite-src-static-gcc-7.5.0-v2.3.91-with.xnnpack-linux-64/build/xnnpack/src/f16-clamp/gen/neonfp16arith-x16.c:32:44: warning: passing argument 1 of ‘vld1q_dup_f16’ from incompatible pointer type [-Wincompatible-pointer-types]
   const float16x8_t vy_min = vld1q_dup_f16(&params->min);
                                            ^
In file included from /home/gavin/CK-TOOLS/lib-tflite-src-static-gcc-7.5.0-v2.3.91-with.xnnpack-linux-64/build/xnnpack/src/f16-clamp/gen/neonfp16arith-x16.c:12:0:
/usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:17359:1: note: expected ‘const float16_t * {aka const __fp16 *}’ but argument is of type ‘const uint16_t * {aka const short unsigned int *}’
 vld1q_dup_f16 (const float16_t* __a)
 ^~~~~~~~~~~~~
```


4] Building `TFLITE_ENABLE_XNNPACK` with LLVM looks like it builds all the right things but doesn't affect performance.

"
43645,Keras EarlyStopping callback on val_auc running mysteriously,"**System information**
- OS : Windows 10
- Python version : 3.6.5
- Tensorflow : 2.3.0
### Simple DL GridsearchCV modelling using Keras

```
epochs = [1, 6, 11, 16, 21, 26, 31, 36, 41, 46]
batch_size = [2,4]

 stopper = EarlyStopping(monitor='val_auc', patience= 10, mode = 'max', min_delta = 1, restore_best_weights=True)
 save_mod = ModelCheckpoint(r'C:\......................\best_model.h5', monitor='val_acc', mode='max', save_best_only=True, save_freq 
 = 'epoch')

 param_grid = dict(epochs = epochs, batch_size = batch_size)

 def create_classify_model(learn_rate = 0.01, momentum = 0):
            # create model
            network = Sequential()
            #input layer and 1st hidden layer
            network.add(Dense(units = data_seg.x_tr.shape[1], activation=""relu"", input_shape = 
            (data_seg.x_tr.shape[1],)))

            #dropout layer for input layer
            network.add(Dropout(0.2, input_shape = (data_seg.x_tr.shape[1],)))

            #second hidden layer
            network.add(Dense(units = math.trunc(data_seg.x_tr.shape[1]*0.8), activation = ""relu""))

            #dropout layer for hidden layer
            network.add(Dropout(0.5))

            #output layer
            network.add(Dense(units = len(np.unique(data_seg.y_tr)), activation = ""softmax""))

            optimizer = SGD(lr=learn_rate, momentum=momentum)

            #compiling neural network
            network.compile(loss = ""categorical_crossentropy"", optimizer = optimizer, metrics = 
['acc',auc_metric()])

            return network

neural_network = KerasClassifier(build_fn=create_classify_model, workers = -1, use_multiprocessing = 
True, verbose=2)
grid = GridSearchCV(estimator=neural_network, param_grid=param_grid)

fit_params = dict(callbacks=[stopper, save_mod], validation_split = 0.05)

grid_model = grid.fit(data_seg.x_tr, data_seg.y_tr, **fit_params)
```

### Output
`471/471 - 1s - loss: 0.7568 - acc: 0.6019 - auc: 0.6357 - val_loss: 0.4909 - val_acc: 0.7600 - 
val_auc: 0.8676
125/125 - 0s - loss: 0.5264 - acc: 0.7751 - auc: 0.8459
WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available 
metrics are: loss,acc,auc_1,val_loss,val_acc,val_auc_1
472/472 - 1s - loss: 0.7242 - acc: 0.6288 - auc_1: 0.6494 - val_loss: 0.5680 - val_acc: 0.7000 - 
val_auc_1: 0.7896
124/124 - 0s - loss: 0.6316 - acc: 0.6250 - auc_1: 0.6942
WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available 
metrics are: loss,acc,auc_2,val_loss,val_acc,val_auc_2
472/472 - 1s - loss: 0.7193 - acc: 0.6182 - auc_2: 0.6563 - val_loss: 0.5279 - val_acc: 0.7400 - 
val_auc_2: 0.8544
124/124 - 0s - loss: 0.5929 - acc: 0.7863 - auc_2: 0.7995
WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available 
metrics are: loss,acc,auc_3,val_loss,val_acc,val_auc_3
472/472 - 1s - loss: 0.7060 - acc: 0.6299 - auc_3: 0.6649 - val_loss: 0.5369 - val_acc: 0.8000 - 
val_auc_3: 0.8400
124/124 - 0s - loss: 0.5564 - acc: 0.7661 - auc_3: 0.8305
WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available 
metrics are: loss,acc,auc_4,val_loss,val_acc,val_auc_4
472/472 - 1s - loss: 0.7258 - acc: 0.6161 - auc_4: 0.6398 - val_loss: 0.5676 - val_acc: 0.7400 - 
val_auc_4: 0.7936`

The ouput here, showcases that the val_auc metric which has been used to monitor EarlyStopping has been given an integer value, which increases with every epoch end I suppose. Hence, can someone help me understand about how can I monitor the val_auc in this case ?
"
43644,TRTEngineOp is not used,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pip3 install tensorflow-gpu =1.13.1
- TensorFlow version (use command below):b'v1.13.1-0-g6612da8951' 1.13.1
- Python version:Python 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Cuda10.0.130-1 cudnn:7.6.0.64-1+cuda10.0
- GPU model and memory: GeForce GTX 1080 Ti 11G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**  
[This is the place to do it.](https://github.com/tensorflow/models/tree/master/research/object_detection)

TRTEngineOp is not used.
I performed the TF-TRT conversion of [ssd_inception_v2_coco](http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz) with the following code.(convert_trt_minimum.py)
```
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.25.9) or chardet (3.0.4) doesn't match a supported version!
  RequestsDependencyWarning)
WARNING:tensorflow:From convert_trt_minimum.py:7: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.gfile.GFile.
WARNING:tensorflow:TensorRT mismatch. Compiled against version 5.0.2, but loaded 5.1.5. Things may not work
2020-09-29 17:19:48.474780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-29 17:19:48.525360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-29 17:19:48.525702: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 2
2020-09-29 17:19:48.525790: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2020-09-29 17:19:48.526009: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-09-29 17:19:48.530274: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1fdeb10 executing computations on platform CUDA. Devices:
2020-09-29 17:19:48.530286: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-09-29 17:19:48.530290: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-09-29 17:19:48.550032: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz
2020-09-29 17:19:48.551389: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1fddee0 executing computations on platform Host. Devices:
2020-09-29 17:19:48.551438: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2020-09-29 17:19:48.551788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 9.67GiB
2020-09-29 17:19:48.551877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:02:00.0
totalMemory: 10.92GiB freeMemory: 10.77GiB
2020-09-29 17:19:48.554355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1
2020-09-29 17:19:49.861234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-29 17:19:49.861263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1 
2020-09-29 17:19:49.861268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N Y 
2020-09-29 17:19:49.861271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   Y N 
2020-09-29 17:19:49.861383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9370 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-09-29 17:19:49.861825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10441 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
2020-09-29 17:19:51.064635: I tensorflow/contrib/tensorrt/segment/segment.cc:443] There are 3957 ops of 51 different types in the graph that are not converted to TensorRT: TopKV2, NonMaxSuppressionV2, TensorArrayWriteV3, Const, Squeeze, ResizeBilinear, Maximum, Where, Switch, TensorArrayGatherV3, TensorArrayV3, LoopCond, NoOp, TensorArrayScatterV3, Placeholder, Add, ExpandDims, Exit, Cast, Identity, Shape, StridedSlice, Less, TensorArraySizeV3, RealDiv, TensorArrayReadV3, Reshape, Merge, Enter, Range, Conv2D, NextIteration, Greater, Split, ZerosLike, Pack, Mul, Equal, Sub, Minimum, Tile, ConcatV2, Size, Unpack, Assert, DataFormatVecPermute, Transpose, Gather, Exp, Slice, Fill, (For more information see https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html#support-ops).
2020-09-29 17:19:51.088668: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:913] Number of TensorRT candidate segments: 4
2020-09-29 17:19:51.117406: W tensorflow/contrib/tensorrt/convert/convert_nodes.cc:3710] Validation failed for TensorRTInputPH_0 and input slot 0: Input tensor with shape [?,?,?,3] has an unknown non-batch dimension at dim 1
2020-09-29 17:19:51.117436: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:1021] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 476 nodes failed: Invalid argument: Validation failed for TensorRTInputPH_0 and input slot 0: Input tensor with shape [?,?,?,3] has an unknown non-batch dimension at dim 1. Fallback to TF...
2020-09-29 17:19:51.117587: W tensorflow/contrib/tensorrt/convert/convert_nodes.cc:3710] Validation failed for TensorRTInputPH_0 and input slot 0: Input tensor with shape [?,546,?,?] has an unknown non-batch dimension at dim 2
2020-09-29 17:19:51.117598: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:1021] TensorRT node TRTEngineOp_1 added for segment 1 consisting of 3 nodes failed: Invalid argument: Validation failed for TensorRTInputPH_0 and input slot 0: Input tensor with shape [?,546,?,?] has an unknown non-batch dimension at dim 2. Fallback to TF...
2020-09-29 17:19:51.135200: W tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Half2 support requested on hardware without native FP16 support, performance will be negatively affected.
2020-09-29 17:19:52.748882: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:1015] TensorRT node TRTEngineOp_2 added for segment 2 consisting of 3 nodes succeeded.
2020-09-29 17:19:52.749750: W tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Half2 support requested on hardware without native FP16 support, performance will be negatively affected.
2020-09-29 17:19:52.864456: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:1015] TensorRT node TRTEngineOp_3 added for segment 3 consisting of 4 nodes succeeded.
2020-09-29 17:19:52.940261: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] Optimization results for grappler item: tf_graph
2020-09-29 17:19:52.940287: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   constant folding: Graph size after: 6180 nodes (1), 10272 edges (2), time = 392.139ms.
2020-09-29 17:19:52.940292: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   layout: Graph size after: 6211 nodes (31), 10304 edges (32), time = 116.429ms.
2020-09-29 17:19:52.940295: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   constant folding: Graph size after: 6201 nodes (-10), 10304 edges (0), time = 302.23ms.
2020-09-29 17:19:52.940298: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   TensorRTOptimizer: Graph size after: 6196 nodes (-5), 10299 edges (-5), time = 2023.22498ms.

```
I then ran the inference through sample_detection.py.
![trtbug](https://user-images.githubusercontent.com/19382501/94534788-c7211880-027b-11eb-8d9c-5001e9f70a20.png)
TRTEngineOp is there, but it is not used for inference.
The defined TRTEngineOp should be used for inference.


**Standalone code to reproduce the issue**
[codes.zip](https://github.com/tensorflow/tensorflow/files/5297462/codes.zip)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43643,how to save a model with weights and paramters ,"when i create a model and compiled it and call the model.fit() and then i saved the model called model.save(path)
then i tried to load my saved module and i get the model correctly but  it cannot load the ckpt files so the evaluate result showed accuracy is around 10%,  i mean, how can i load module without training it again?"
43642,doc issue,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
43641,scatter_nd documentation is wrong,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/scatter_nd

## Description of issue (what needs changing):
The output for the second example is wrong. The documentation has this:
[[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],

 [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],

 [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],

 [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]

When I ran this test code in Colab, I get this output:
[[[5 5 5 5]
  [6 6 6 6]
  [7 7 7 7]
  [8 8 8 8]]

 [[0 0 0 0]
  [0 0 0 0]
  [0 0 0 0]
  [0 0 0 0]]

 [[5 5 5 5]
  [6 6 6 6]
  [7 7 7 7]
  [8 8 8 8]]

 [[0 0 0 0]
  [0 0 0 0]
  [0 0 0 0]
  [0 0 0 0]]]

### Clear description

The documentation has the wrong output. 

### Correct links

Here is a Colab notebook that demonstrates the problem:

https://colab.research.google.com/drive/1XYjNsbz1Atfa5bbD7W4FxvIr0zap-HOJ?usp=sharing

### Parameters defined

Are all parameters defined and formatted correctly?

Exactly as in the documentation page.

### Returns defined

Are return values defined?

They are not what are in the documentation page :)

### Raises listed and defined

Are the errors defined? 

Yes, error returns are defined. I have not tested them.

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

No.

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? 

No. Frankly, I have trouble understanding what the correct output should be.
"
43640,bug in tf,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43639,this is a test issue,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
43638,Keras batch norm performance issue with parameter server strategy,"**System information**

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device:
TensorFlow installed from (source or
binary): source
TensorFlow version (use command below): 1.14
Python version: - Bazel
version (if compiling from source):
GCC/Compiler version (if compiling from
source):
CUDA/cuDNN version: Cuda 10.1 cuDNN 7.6.5
GPU model and memory: V100

**Describe the current behavior**

Currently I'm observing a performance issue regarding Keras batch norm layer on tensorflow 1.14 with the following setup: Parameter server for multi-workers, mirrored strategy within each individual worker (8 GPUs per worker). All variables are cached on the worker cpus via `caching_device` to reduce parameter server <-> gpu communication.

Specifically, I'm wondering why https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/normalization.py#L492 exists (first added in https://github.com/tensorflow/tensorflow/commit/88ad9949ef4ea6e07105a326a1d21c108cb2883a). This places all moving mean / variance update tensors, for each GPU, on the parameter server, causing communication between worker GPUs and the parameter server that does not scale (passing the tensor 64 times in my setup rather than 8 times in a 1 GPU per worker, 8 workers setup). Upon removing the colocation, the update tensors are placed on individual GPUs, and the communication disappears and distributed training with the parameter server scales properly. Does the colocation exist to properly synchronize the moving mean / variance updates?

**Describe the expected behavior**

I'd expect there to be no communication between individual GPUs and the parameter server, and the moving mean and variance update tensors placed on devices in a way that is efficient."
43637,libhexagon_interface.so for non Android - eLinux platform,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Automotive Grade Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: SA8195P
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.3
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): aarch64-linux-gnu 8.3.0



**Describe the problem**
Hello.
I'm trying to get hexagon delgate to work on Qualcomm's SA8195P platform. This platform is based on Snapdragon 8xx series. But the problem is that this platform works on Automotive Grade Linux. The currently available libhexagon_interface.so doesn't work as it has Android dependencies. I cannot find the source code of libhexagon_interface.so in tensorflow git. Is it possible to get source code of libhexagon_interface.so or a complied binary of libhexagon_interface.so for aarch64 Linux without Android dependencies?

I noticed that the binary of libhexagon_interface.so for openwrt is provided to hardiksd.
https://github.com/tensorflow/tensorflow/issues/39736

Thank you.



"
43634,k hot encoding for tf.keras.preprocessing.image.DirectoryIterator,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

tf.keras.preprocessing.image.DirectoryIterator

**System information**
- TensorFlow version (you are using):latest
- Are you willing to contribute it (Yes/No):Yes



**Describe the feature and the current behavior/state.**
Currently the function `tf.keras.preprocessing.image.DirectoryIterator` doesn't have any option for multiple classes for a single image. Currently only a single class can be assigned to an image.

**Will this change the current api? How?**

**Who will benefit with this feature?**
Anyone

**Any Other info.**
"
43631,tf.cond throws AssertionError when computing its gradient twice,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Tested on Arch Linux and the default Google Colab env
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
v2.3.0-0-gb36436b087 2.3.0 (and I've also tried 2.4.0, same problem)
- Python version:
3.6.9
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
Calling `g.gradient` twice on a tensor obtained from `tf.cond` call in `@tf.function` causes AssertionError:
`assert len(func_graph.outputs) == len(grads)`. The problem appears only if variable in the `true_fn` or `false_fn` was transformed (multiplied, exponentiated, etc.)

**Describe the expected behavior**
TF should either:
* Successfully calculate the gradient, or 
* Consistently fail after the first time and even if the `true_fn` doesn't contain any transformation of the variable.

**Standalone code to reproduce the issue**
```
import tensorflow as tf

a = tf.Variable(tf.ones(10))

@tf.function
def test_cond(cond_var):
    with tf.GradientTape(persistent=True) as g:
         loss = tf.cond(
             cond_var > 0,
             lambda: tf.reduce_sum(1 * a),
             lambda: tf.reduce_sum(1 * a))

    gradient1 = g.gradient(loss, [a])
    gradient2 = g.gradient(loss, [a])
    return gradient1, gradient2

g1, g2 = test_cond(tf.convert_to_tensor(1))
```

<details>
<summary>This throws the following AssertionError:</summary>


```
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-27-7a87d1b88655> in <module>()
     15     return gradient1, gradient2
     16 
---> 17 g1, g2 = test_cond(tf.convert_to_tensor(1))

8 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = ""nonXla""
--> 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    821       # This is the first call of __call__, so we have to initialize.
    822       initializers = []
--> 823       self._initialize(args, kwds, add_initializers_to=initializers)
    824     finally:
    825       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    695     self._concrete_stateful_fn = (
    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 697             *args, **kwds))
    698 
    699     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-> 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3073             arg_names=arg_names,
   3074             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3075             capture_by_value=self._capture_by_value),
   3076         self._function_attributes,
   3077         function_spec=self.function_spec,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    599         # the function a weak reference to itself to avoid a reference cycle.
--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    601     weak_wrapped_fn = weakref.ref(wrapped_fn)
    602 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    971           except Exception as e:  # pylint:disable=broad-except
    972             if hasattr(e, ""ag_error_metadata""):
--> 973               raise e.ag_error_metadata.to_exception(e)
    974             else:
    975               raise

AssertionError: in user code:

    <ipython-input-27-7a87d1b88655>:14 test_cond  *
        gradient2 = g.gradient(loss, [a])
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py:1073 gradient  **
        unconnected_gradients=unconnected_gradients)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py:77 imperative_grad
        compat.as_str(unconnected_gradients.value))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py:162 _gradient_function
        return grad_fn(mock_op, *out_grads)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py:121 _IfGrad
        true_graph, grads, util.unique_grad_fn_name(true_graph.name))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py:384 _create_grad_func
        func_graph=_CondGradFuncGraph(name, func_graph))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:986 func_graph_from_py_func
        func_outputs = python_func(*func_args, **func_kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py:383 <lambda>
        lambda: _grad_fn(func_graph, grads), [], {},
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py:359 _grad_fn
        assert len(func_graph.outputs) == len(grads)

    AssertionError: 

```
</details>

What's interesting, if you replace `1 * a` with `a` inside the `tf.cond` call, then everything works fine. If you remove one of the `g.gradient` calls, it also works fine.

I've uploaded the code [to colab](https://colab.research.google.com/drive/1XximLOFyx828syl5QteZ8oOvXE7CqUGT?usp=sharing) as well."
43629,Fail to build TF 1.15 on Cuda 11.1,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- TensorFlow installed from (source or binary): source 
- TensorFlow version: 1.15
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.25.3
- GCC/Compiler version (if compiling from source): MSVC 2017
- CUDA/cuDNN version: 11.1 / 7.6.0
- GPU model and memory: RTX 2080 TI

**Describe the problem**

unable to build TF 1.15 on Cuda 11.1

**Any other info / logs**

```
Execution platform: @bazel_tools//platforms:host_platform
tensorflow/core/kernels/cuda_sparse.cc(212): error C2065: 'cusparseSgtsv': undec
lared identifier
tensorflow/core/kernels/cuda_sparse.cc(212): error C2065: 'cusparseDgtsv': undec
lared identifier
tensorflow/core/kernels/cuda_sparse.cc(212): error C2065: 'cusparseCgtsv': undec
lared identifier
tensorflow/core/kernels/cuda_sparse.cc(212): error C2065: 'cusparseZgtsv': undec
lared identifier
tensorflow/core/kernels/cuda_sparse.cc(224): error C2065: 'cusparseSgtsv_nopivot
': undeclared identifier
tensorflow/core/kernels/cuda_sparse.cc(224): error C2065: 'cusparseDgtsv_nopivot
': undeclared identifier
tensorflow/core/kernels/cuda_sparse.cc(224): error C2065: 'cusparseCgtsv_nopivot
': undeclared identifier
tensorflow/core/kernels/cuda_sparse.cc(224): error C2065: 'cusparseZgtsv_nopivot
': undeclared identifier
tensorflow/core/kernels/cuda_sparse.cc(250): error C2065: 'cusparseSgtsvStridedB
atch': undeclared identifier
tensorflow/core/kernels/cuda_sparse.cc(250): error C2065: 'cusparseDgtsvStridedB
atch': undeclared identifier
tensorflow/core/kernels/cuda_sparse.cc(250): error C2065: 'cusparseCgtsvStridedB
atch': undeclared identifier
tensorflow/core/kernels/cuda_sparse.cc(250): error C2065: 'cusparseZgtsvStridedB
atch': undeclared identifier
```"
43628,tf.feature_column.embedding_column: wrong dtype in lookup variable,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:

Yes

-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Ubuntu 18.10

-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:

No

-   **TensorFlow installed from (source or binary)**:

pip install -U --force-reinstall tf-nightly

-   **TensorFlow version (use command below)**:

'2.4.0-dev20200712'

-   **Python version**:

python --version
Python 3.7.7


-   **Bazel version (if compiling from source)**:

NO

-   **GCC/Compiler version (if compiling from source)**:

NO

-   **CUDA/cuDNN version**:

Reproducable on CPU

-   **GPU model and memory**:

Reproducable on CPU

-   **Exact command to reproduce**:

https://gist.github.com/dennisjay/9482041ba5e037b0cc8fd12dcd2292c8

### Describe the problem
The tf.feature_column.embedding_column of my tensorflow Version is internally working with int32 instead of int64 and thus there are errors when exporting the model to saved model and compiling it with i.e. triton server.

![image](https://user-images.githubusercontent.com/1181660/94471941-481fd780-01ca-11eb-9693-8b7b8537bf0d.png)

The strided slice operation outputs a int64 but the following operation is accepting int32 only. Reproducable is the error with the use of the jupyter notebook above.

### Source code / logs

InferenceServerException: [_Derived_]{{function_node __inference_signature_wrapper_3345}} {{function_node __inference_signature_wrapper_3345}} input segment_ids[0] expected type int32 != int64, the type of sequential_1/dense_features_1/test_col_embedding/test_col_embedding_weights/embedding_lookup_sparse/strided_slice:output:0[0]
	In {{node sequential_1/dense_features_1/test_col_embedding/test_col_embedding_weights/embedding_lookup_sparse}}
	 [[StatefulPartitionedCall]]
	 [[StatefulPartitionedCall_1]]
"
43627,TF2 Memory Leak when fitting multiple models in Docker,"When fitting multiple models within same tuning process, the consumption of RAM grows until all of the memory is consumed, which ultimately leads to failure of the complete training process.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): docker pull tensorflow/tensorflow:2.0.0-py3-jupyter
- TensorFlow version (use command below): tensorflow:2.0.0  and every one up!
- Python version: 3.6 

**Describe the current behavior**
When fitting multiple keras models RAM is consumed until there is no more free RAM and training process fails.

**Describe the expected behavior**
tf.keras.backend.clear_session() should reset the memory consumption after each tuning iteration.

**Standalone code to reproduce the issue**
from tensorflow import keras
import psutil

for i in range(0, 200):
    keras.backend.clear_session()
    a = keras.layers.Input(shape=(32,))
    b = keras.layers.Dense(32)(a)
    model = keras.Model(inputs=a, outputs=b)
    print('memory usesd: ' + str(psutil.virtual_memory().used // 1e6))"
43626,Feature request: MCUXpresso IDE integration,"The [NXP MCUXpresso IDE](https://www.nxp.com/design/software/development-software/mcuxpresso-software-and-tools-/mcuxpresso-integrated-development-environment-ide:MCUXpresso-IDE) is probably the most widely used development environment for NXP micro-controllers. It is free to use and available for Linux, macOS and Windows. It seems like there is no support for it in TensorFlow Lite for Microcontrollers yet. Would be great to have support for it."
43625,Feature request: STM32CubeIDE integration,"The [STM32CubeIDE](https://www.st.com/en/development-tools/stm32cubeide.html) is probably the most widely used development environment for STM32 micro-controllers. It is free to use and available for Linux, macOS and Windows. It seems like there is no support for it in *TensorFlow Lite for Microcontrollers* yet. Would be great to have support for it."
43624,Converted model gives shape error during inference in C++ API.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.4.0-dev20200630


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# -*- coding: utf-8 -*-
""""""TensorflowTTS-TFLite.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HudLLpT9CQdh2k04c06bHUwLubhGTWxA

Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Author : [jaeyoo@](https://github.com/jaeyoo),
[khanhlvg@](https://github.com/khanhlvg),
[abattery@](https://github.com/abattery), [thaink@](https://github.com/thaink)
(Google Research) and [dathudeptrai@](https://github.com/dathudeptrai) (owner
TensorflowTTS)

# TensorflowTTS real TFLite Conversion demonstration

This notebook provides a demo to convert models from TensorflowTTS to TFlite

- Github: https://github.com/TensorSpeech/TensorflowTTS
- Audio samples: https://tensorspeech.github.io/TensorflowTTS/
""""""

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
import sys
import time

import yaml
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow_tts.configs import FastSpeech2Config, FastSpeechConfig, Tacotron2Config
from tensorflow_tts.models import TFFastSpeech, TFFastSpeech2, TFTacotron2
from tensorflow_tts.processor import LJSpeechProcessor
from tensorflow_tts.processor.ljspeech import LJSPEECH_SYMBOLS
from tensorflow_tts.configs import MultiBandMelGANGeneratorConfig

from tensorflow_tts.models import TFMelGANGenerator
from tensorflow_tts.models import TFPQMF

os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""
sys.path.append(""tensorflowtts/"")
tf.__version__


def load_tacotron2():
    # Tacotron2
    with open(""tacotron2_config.yml"") as f:
        config = yaml.load(f, Loader=yaml.Loader)

    config = Tacotron2Config(**config[""tacotron2_params""])
    tacotron2 = TFTacotron2(
        config=config,
        training=False,
        name=""tacotron2v2"",
        enable_tflite_convertible=True,
    )

    # Newly added :
    tacotron2.setup_window(win_front=6, win_back=6)
    tacotron2.setup_maximum_iterations(1000)  # be careful

    tacotron2._build()
    tacotron2.load_weights(""tacotron2-120k.h5"")
    return tacotron2


def load_fastspeech():
    # FastSpeech

    with open(""fastspeech_config.yml"") as f:
        config = yaml.load(f, Loader=yaml.Loader)

    config = FastSpeechConfig(**config[""fastspeech_params""])
    fastspeech = TFFastSpeech(
        config=config, enable_tflite_convertible=True, name=""fastspeech""
    )
    fastspeech._build()
    fastspeech.load_weights(""fastspeech-150k.h5"")
    return fastspeech


def load_fastspeech2():
    # FastSpeech2
    with open(""fastspeech2_config.yml"") as f:
        config = yaml.load(f, Loader=yaml.Loader)

    config = FastSpeech2Config(**config[""fastspeech_params""])
    fastspeech2 = TFFastSpeech2(
        config=config, enable_tflite_convertible=True, name=""fastspeech2""
    )
    fastspeech2._build()
    fastspeech2.load_weights(""fastspeech2-150k.h5"")
    return fastspeech2


def convert_tacotron2(tacotron2):
    """"""## Convert to TF Lite

    ### Tacotron-2
    """"""

    # Concrete Function
    tacotron2_concrete_function = tacotron2.inference_tflite.get_concrete_function()

    converter = tf.lite.TFLiteConverter.from_concrete_functions(
        [tacotron2_concrete_function]
    )
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS,
    ]
    tflite_model = converter.convert()

    # Save the TF Lite model.
    with open(""tacotron2.tflite"", ""wb"") as f:
        f.write(tflite_model)

    print(""Model size is %f MBs."" % (len(tflite_model) / 1024 / 1024.0))


def convert_fastspeech(fastspeech):
    """"""### FastSpeech""""""

    # Concrete Function
    fastspeech_concrete_function = fastspeech.inference_tflite.get_concrete_function()

    converter = tf.lite.TFLiteConverter.from_concrete_functions(
        [fastspeech_concrete_function]
    )
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS,
    ]
    tflite_model = converter.convert()

    # Save the TF Lite model.
    with open(""fastspeech_quant.tflite"", ""wb"") as f:
        f.write(tflite_model)

    print(""Model size is %f MBs."" % (len(tflite_model) / 1024 / 1024.0))


def convert_fastspeech2(fastspeech2):
    """"""### FastSpeech2""""""

    # Concrete Function
    fastspeech2_concrete_function = fastspeech2.inference_tflite.get_concrete_function()

    converter = tf.lite.TFLiteConverter.from_concrete_functions(
        [fastspeech2_concrete_function]
    )
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS,
    ]
    tflite_model = converter.convert()

    # Save the TF Lite model.
    with open(""fastspeech2_quant.tflite"", ""wb"") as f:
        f.write(tflite_model)

    print(""Model size is %f MBs."" % (len(tflite_model) / 1024 / 1024.0))


""""""# Inference""""""


def visualize_mel_spectrogram(mels):
    mels = tf.reshape(mels, [-1, 80]).numpy()
    fig = plt.figure(figsize=(10, 8))
    ax1 = fig.add_subplot(311)
    ax1.set_title(""Predicted Mel-after-Spectrogram"")
    im = ax1.imshow(np.rot90(mels), aspect=""auto"", interpolation=""none"")
    fig.colorbar(mappable=im, shrink=0.65, orientation=""horizontal"", ax=ax1)
    plt.show()
    plt.close()


def infer_tacotron2():
    """"""## Tacotron-2""""""
    # Load the TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path=""tacotron2.tflite"")
    interpreter.allocate_tensors()

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Prepare input data.
    def prepare_input(input_ids):
        return (
            tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0),
            tf.convert_to_tensor([len(input_ids)], tf.int32),
            tf.convert_to_tensor([0], dtype=tf.int32),
        )

    # Test the model on random input data.
    def infer(input_text):
        processor = LJSpeechProcessor(None, symbols=LJSPEECH_SYMBOLS)
        input_ids = processor.text_to_sequence(input_text.lower())
        print(""Input ids shape:"", input_ids.shape)
        interpreter.resize_tensor_input(input_details[0][""index""], [1, len(input_ids)])
        interpreter.allocate_tensors()
        input_data = prepare_input(input_ids)
        for i, detail in enumerate(input_details):
            print(""Input tensor %d detail:"" % i, detail)
            input_shape = detail[""shape""]
            print(""Shape of tensor %d:"" % i, input_data[i].shape)
            interpreter.set_tensor(detail[""index""], input_data[i])

        interpreter.invoke()

        # The function `get_tensor()` returns a copy of the tensor data.
        # Use `tensor()` in order to get a pointer to the tensor.
        return (
            interpreter.get_tensor(output_details[0][""index""]),
            interpreter.get_tensor(output_details[1][""index""]),
        )

    input_text = ""Recent research at Harvard has shown meditating\
    for as little as 8 weeks, can actually increase the grey matter in the \
    parts of the brain responsible for emotional regulation, and learning.""

    decoder_output_tflite, mel_output_tflite = infer(input_text)

    return decoder_output_tflite, mel_output_tflite


def infer_fastspeech():
    """"""## FastSpeech""""""
    # Load the TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path=""fastspeech_quant.tflite"")

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Prepare input data.
    def prepare_input(input_ids):
        input_ids = tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0)
        return (
            input_ids,
            tf.convert_to_tensor([0], tf.int32),
            tf.convert_to_tensor([1.0], dtype=tf.float32),
        )

    # Test the model on random input data.
    def infer(input_text):
        for x in input_details:
            print(x)
        for x in output_details:
            print(x)
        processor = LJSpeechProcessor(None, symbols=LJSPEECH_SYMBOLS)
        input_ids = processor.text_to_sequence(input_text.lower())
        interpreter.resize_tensor_input(input_details[0][""index""], [1, len(input_ids)])
        interpreter.resize_tensor_input(input_details[1][""index""], [1])
        interpreter.resize_tensor_input(input_details[2][""index""], [1])
        interpreter.allocate_tensors()
        input_data = prepare_input(input_ids)
        for i, detail in enumerate(input_details):
            input_shape = detail[""shape""]
            interpreter.set_tensor(detail[""index""], input_data[i])

        interpreter.invoke()

        # The function `get_tensor()` returns a copy of the tensor data.
        # Use `tensor()` in order to get a pointer to the tensor.
        return (
            interpreter.get_tensor(output_details[0][""index""]),
            interpreter.get_tensor(output_details[1][""index""]),
        )

    input_text = ""Recent research at Harvard has shown meditating\
    for as little as 8 weeks, can actually increase the grey matter in the \
    parts of the brain responsible for emotional regulation, and learning.""

    decoder_output_tflite, mel_output_tflite = infer(input_text)


def infer_fastspeech2():
    """"""## FastSpeech2""""""
    # Load the TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path=""fastspeech2_quant.tflite"")

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Prepare input data.
    def prepare_input(input_ids):
        input_ids = tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0)
        return (
            input_ids,
            tf.convert_to_tensor([0], tf.int32),
            tf.convert_to_tensor([1.0], dtype=tf.float32),
            tf.convert_to_tensor([1.0], dtype=tf.float32),
            tf.convert_to_tensor([1.0], dtype=tf.float32),
        )

    # Test the model on random input data.
    def infer(input_text):
        for x in input_details:
            print(x)
        for x in output_details:
            print(x)
        processor = LJSpeechProcessor(None, symbols=LJSPEECH_SYMBOLS)
        input_ids = processor.text_to_sequence(input_text.lower())
        interpreter.resize_tensor_input(input_details[0][""index""], [1, len(input_ids)])
        interpreter.resize_tensor_input(input_details[1][""index""], [1])
        interpreter.resize_tensor_input(input_details[2][""index""], [1])
        interpreter.resize_tensor_input(input_details[3][""index""], [1])
        interpreter.resize_tensor_input(input_details[4][""index""], [1])
        interpreter.allocate_tensors()
        input_data = prepare_input(input_ids)
        for i, detail in enumerate(input_details):
            input_shape = detail[""shape""]
            interpreter.set_tensor(detail[""index""], input_data[i])

        interpreter.invoke()

        # The function `get_tensor()` returns a copy of the tensor data.
        # Use `tensor()` in order to get a pointer to the tensor.
        return (
            interpreter.get_tensor(output_details[0][""index""]),
            interpreter.get_tensor(output_details[1][""index""]),
        )

    input_text = ""Recent research at Harvard has shown meditating\
    for as little as 8 weeks, can actually increase the grey matter in the \
    parts of the brain responsible for emotional regulation, and learning.""

    decoder_output_tflite, mel_output_tflite = infer(input_text)


def convert_vocoder():
    # Convert the vocoder as a TFLite model.
    with open(
        ""tensorflowtts/examples/multiband_melgan/conf/multiband_melgan.v1.yaml""
    ) as f:
        config = yaml.load(f, Loader=yaml.Loader)

    config = MultiBandMelGANGeneratorConfig(
        **config[""multiband_melgan_generator_params""]
    )

    class TFMBMelGANGenerator(TFMelGANGenerator):
        def __init__(self, config, **kwargs):
            super().__init__(config, **kwargs)
            self.pqmf = TFPQMF(config=config, name=""pqmf"")

        def build(self, input_shape):
            self.pqmf.build(input_shape)
            self.built = True

        @tf.function(
            experimental_relax_shapes=True,
            input_signature=[tf.TensorSpec(shape=[None, None, 80], dtype=tf.float32)],
        )
        def call(self, mels):
            mb_audios = super().call(mels)
            audios = self.pqmf.synthesis(mb_audios)
            return audios

    mb_melgan = TFMBMelGANGenerator(config, name=""mb_melgan"")
    fake_mels = tf.random.uniform(shape=[1, 256, 80], dtype=tf.float32)
    audios = mb_melgan(fake_mels)[0, :, 0]
    mb_melgan.summary()
    mb_melgan.load_weights(""generator-940000.h5"")
    concrete_func = mb_melgan.call.get_concrete_function()
    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS,
    ]
    mbmlite = converter.convert()
    # Save the TF Lite model.
    with tf.io.gfile.GFile(""mbm_quant.tflite"", ""wb"") as f:
        f.write(mbmlite)
    print(""Model saved to mbm_quant.tflite file"")
    print(""Model size is %f MBs."" % (len(mbmlite) / 1024 / 1024.0))


def infer_mbmelgan(mel_output_tflite):
    class MBMelganLite:
        def __init__(self, path):
            # Load TFLite model and allocate tensors.
            self.interpreter = tf.lite.Interpreter(model_path=path)
            # Get input and output tensors.
            self.inputs = self.interpreter.get_input_details()
            self.outputs = self.interpreter.get_output_details()
            self.current_shape = self.inputs[0][""shape""]

        def printDetails(self):
            for x in self.inputs:
                print(x)
            for x in self.outputs:
                print(x)

        def infer(self, mel):
            # Resize if input length different, assuming batch size is always 1.
            if self.current_shape[1] is not mel.shape[1]:
                print(
                    f""Input shape: {mel.shape} , interpreter shape: {self.current_shape}""
                )
                print(""Warning: Latency might be affected due to change in input shape"")
                self.interpreter.resize_tensor_input(
                    self.inputs[0][""index""], list(mel.shape)
                )
                self.interpreter.allocate_tensors()
                self.current_shape = mel.shape
            self.interpreter.set_tensor(self.inputs[0][""index""], mel)
            self.interpreter.invoke()
            tflite_results = self.interpreter.get_tensor(self.outputs[0][""index""])
            return tflite_results

    mbmelganlite = MBMelganLite(path=""mbm_quant.tflite"")

    start = time.time()
    audio_mblite = mbmelganlite.infer(mel_output_tflite)[0, :, 0]
    print(f""Time taken: {time.time() - start}s"")


if __name__ == ""__main__"":
    # Load the synthesizer and vocoder models, convert them to TFLite, and save to disk.
    tacotron2 = load_tacotron2()
    convert_tacotron2(tacotron2)
    convert_vocoder()

    # TFLite inference.
    decoder_output_tflite, mel_output_tflite = infer_tacotron2()
    print(""Type of ``mel_output_tflite``:"", type(mel_output_tflite))
    print(""Value of ``mel_output_tflite``:"", mel_output_tflite)
    infer_mbmelgan(mel_output_tflite)

```

**The output from the converter invocation**

```
2020-09-28 11:49:35.986313: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-09-28 11:49:35.986361: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2020-09-28 11:49:39.385944: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-09-28 11:49:39.385986: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-09-28 11:49:39.386014: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host: /proc/driver/nvidia/version does not exist
2020-09-28 11:49:39.386244: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-28 11:49:39.424376: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2199955000 Hz
2020-09-28 11:49:39.428501: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5610b8b69120 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-28 11:49:39.428546: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-28 11:49:47.039592: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-09-28 11:49:47.039824: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-09-28 11:49:47.201050: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-09-28 11:49:47.201094: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 882 nodes (146), 970 edges (166), time = 16.951ms.
2020-09-28 11:49:47.201102: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 882 nodes (0), 970 edges (0), time = 15.414ms.
2020-09-28 11:49:47.201108: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: decoder_while_cond_6632
2020-09-28 11:49:47.201115: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-09-28 11:49:47.201120: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-09-28 11:49:47.201126: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: decoder_while_body_6633
2020-09-28 11:49:47.201132: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2020-09-28 11:49:47.201138: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-09-28 11:49:47.201144: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: while_body_4839
2020-09-28 11:49:47.201150: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-09-28 11:49:47.201155: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-09-28 11:49:47.201161: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: while_cond_3160
2020-09-28 11:49:47.201167: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-09-28 11:49:47.201186: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-09-28 11:49:47.201193: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: while_cond_4838
2020-09-28 11:49:47.201198: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-09-28 11:49:47.201204: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-09-28 11:49:47.201210: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: while_body_3161
2020-09-28 11:49:47.201215: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-09-28 11:49:47.201221: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-09-28 11:49:50.042865: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-09-28 11:49:50.042923: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
2020-09-28 11:49:51.573781: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor LocationSensitiveAttention/memory_layer/Tensordot/Shape that is not type float.
2020-09-28 11:49:51.573831: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor LocationSensitiveAttention/memory_layer/Tensordot/Shape that is not type float.
2020-09-28 11:49:51.573842: I tensorflow/lite/tools/optimize/quantize_weights.cc:219] Skipping quantization of tensor boolean_mask/Reshape because it has fewer than 1024 elements (1).
2020-09-28 11:49:51.573852: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor residual_projection/Tensordot/Shape that is not type float.
2020-09-28 11:49:51.573857: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor residual_projection/Tensordot/Shape that is not type float.
2020-09-28 11:49:51.573863: I tensorflow/lite/tools/optimize/quantize_weights.cc:219] Skipping quantization of tensor boolean_mask_1/Reshape because it has fewer than 1024 elements (1).
2020-09-28 11:49:51.683054: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor arg6 that is not type float.
2020-09-28 11:49:51.683096: I tensorflow/lite/tools/optimize/quantize_weights.cc:219] Skipping quantization of tensor arg7 because it has fewer than 1024 elements (512).
2020-09-28 11:49:51.686444: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor arg6 that is not type float.
2020-09-28 11:49:51.686455: I tensorflow/lite/tools/optimize/quantize_weights.cc:219] Skipping quantization of tensor arg7 because it has fewer than 1024 elements (512).
2020-09-28 11:49:51.689772: I tensorflow/lite/tools/optimize/quantize_weights.cc:219] Skipping quantization of tensor decoder/while/decoder_cell/location_conv/conv1d1 because it has fewer than 1024 elements (992).
2020-09-28 11:49:51.689783: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor decoder/while/decoder_cell/location_layer/Tensordot/Shape that is not type float.
2020-09-28 11:49:51.689788: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor decoder/while/decoder_cell/location_layer/Tensordot/Shape that is not type float.
2020-09-28 11:49:51.689798: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor decoder/while/decoder_cell/scores_attention/Tensordot/Shape that is not type float.
2020-09-28 11:49:51.689803: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor decoder/while/decoder_cell/scores_attention/Tensordot/Shape that is not type float.
2020-09-28 11:49:51.689819: I tensorflow/lite/tools/optimize/quantize_weights.cc:219] Skipping quantization of tensor decoder/while/decoder_cell/scores_attention/Tensordot/MatMul because it has fewer than 1024 elements (128).
2020-09-28 11:49:54.531714: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-09-28 11:49:54.531884: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-09-28 11:49:54.613588: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-09-28 11:49:54.613620: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 1200 nodes (1114), 1264 edges (1178), time = 43.114ms.
2020-09-28 11:49:54.613628: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.771ms.
2020-09-28 11:49:56.158239: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-09-28 11:49:56.158292: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
2020-09-28 11:49:56.439344: I tensorflow/lite/tools/optimize/quantize_weights.cc:219] Skipping quantization of tensor conv1d;PartitionedCall/conv1d1 because it has fewer than 1024 elements (252).
INFO: Created TensorFlow Lite delegate for select TF ops.
INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 218 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 31 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 31 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 1 nodes delegated out of 4 nodes with 1 partitions.

INFO: TfLiteFlexDelegate delegate: 3 nodes delegated out of 139 nodes with 3 partitions.

/home/brendan/conda/envs/py37/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:44: UserWarning: You are currently using a nightly version of TensorFlow (2.4.0-dev20200630). 
TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. 
If you encounter a bug, do not file an issue on GitHub.
  UserWarning,
Model size is 32.616089 MBs.
Model: ""mb_melgan""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
sequential (Sequential)      (None, None, 4)           2534356   
_________________________________________________________________
pqmf (TFPQMF)                multiple                  0         
=================================================================
Total params: 2,534,356
Trainable params: 2,534,356
Non-trainable params: 0
_________________________________________________________________
Model saved to mbm_quant.tflite file
Model size is 6.481171 MBs.
Type of seq_symbols: <class 'str'>
Value of seq_symbols: recent research at harvard has shown meditating for as little as eight weeks, can actually increase the grey matter in the parts of the brain responsible for emotional regulation, and learning.
Input tensor 0 detail: {'name': 'input_ids', 'index': 0, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}
Shape of tensor 0: (1, 194)
Input tensor 1 detail: {'name': 'input_lengths', 'index': 1, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}
Shape of tensor 1: (1,)
Input tensor 2 detail: {'name': 'speaker_ids', 'index': 2, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}
Shape of tensor 2: (1,)
Type of ``mel_output_tflite``: <class 'numpy.ndarray'>
Value of ``mel_output_tflite``: [[[-1.1824543  -0.73486066 -0.6587808  ... -1.4803959  -1.6538712
   -1.7523229 ]
  [-0.61233926 -0.1465351   0.26401407 ... -1.4384888  -1.5623676
   -1.6496788 ]
  [ 0.00431968  0.206805    0.68081516 ... -1.3629606  -1.5245864
   -1.6202413 ]
  ...
  [ 0.21240304 -0.1798481   1.0284772  ... -0.42555726 -0.47812086
   -0.3530782 ]
  [ 0.179356   -0.16748641  1.031336   ... -0.7199661  -0.76034176
   -0.6377524 ]
  [ 0.21463726 -0.1416688   0.9862344  ... -0.8474455  -0.8514927
   -0.7260057 ]]]
Input shape: (1, 1000, 80) , interpreter shape: [ 1  1 80]
Warning: Latency might be affected due to change in input shape
Time taken: 46.32718467712402s

```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
[tacotron2.tflite.zip](https://github.com/tensorflow/tensorflow/files/5293557/tacotron2.tflite.zip)
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)

The conversion is successful, but running inference in C++ is failing. I suspect it is because I am loading the input tensors in incorrectly. The code and output is below:
```
/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#include <cstdio>
#include <bits/stdc++.h>
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/optional_debug_tools.h""
//#include ""ljspeech_functional.cc""

// BEGIN LJSPEECH_FUNCTIONAL CODE



std::map<std::string, int> symbol_to_id = {
    {""pad"", 0},
	{""-"", 1},
	{""!"", 2},
	{""'"", 3},
	{""("", 4},
	{"")"", 5},
	{"""", 6},
	{""."", 7},
	{"":"", 8},
	{"";"", 9},
	{""?"", 10},
	{"" "", 11},
	{""A"", 12},
	{""B"", 13},
	{""C"", 14},
	{""D"", 15},
	{""E"", 16},
	{""F"", 17},
	{""G"", 18},
	{""H"", 19},
	{""I"", 20},
	{""J"", 21},
	{""K"", 22},
	{""L"", 23},
	{""M"", 24},
	{""N"", 25},
	{""O"", 26},
	{""P"", 27},
	{""Q"", 28},
	{""R"", 29},
	{""S"", 30},
	{""T"", 31},
	{""U"", 32},
	{""V"", 33},
	{""W"", 34},
	{""X"", 35},
	{""Y"", 36},
	{""Z"", 37},
	{""a"", 38},
	{""b"", 39},
	{""c"", 40},
	{""d"", 41},
	{""e"", 42},
	{""f"", 43},
	{""g"", 44},
	{""h"", 45},
	{""i"", 46},
	{""j"", 47},
	{""k"", 48},
	{""l"", 49},
	{""m"", 50},
	{""n"", 51},
	{""o"", 52},
	{""p"", 53},
	{""q"", 54},
	{""r"", 55},
	{""s"", 56},
	{""t"", 57},
	{""u"", 58},
	{""v"", 59},
	{""w"", 60},
	{""x"", 61},
	{""y"", 62},
	{""z"", 63},
	{""@AA"", 64},
	{""@AA0"", 65},
	{""@AA1"", 66},
	{""@AA2"", 67},
	{""@AE"", 68},
	{""@AE0"", 69},
	{""@AE1"", 70},
	{""@AE2"", 71},
	{""@AH"", 72},
	{""@AH0"", 73},
	{""@AH1"", 74},
	{""@AH2"", 75},
	{""@AO"", 76},
	{""@AO0"", 77},
	{""@AO1"", 78},
	{""@AO2"", 79},
	{""@AW"", 80},
	{""@AW0"", 81},
	{""@AW1"", 82},
	{""@AW2"", 83},
	{""@AY"", 84},
	{""@AY0"", 85},
	{""@AY1"", 86},
	{""@AY2"", 87},
	{""@B"", 88},
	{""@CH"", 89},
	{""@D"", 90},
	{""@DH"", 91},
	{""@EH"", 92},
	{""@EH0"", 93},
	{""@EH1"", 94},
	{""@EH2"", 95},
	{""@ER"", 96},
	{""@ER0"", 97},
	{""@ER1"", 98},
	{""@ER2"", 99},
	{""@EY"", 100},
	{""@EY0"", 101},
	{""@EY1"", 102},
	{""@EY2"", 103},
	{""@F"", 104},
	{""@G"", 105},
	{""@HH"", 106},
	{""@IH"", 107},
	{""@IH0"", 108},
	{""@IH1"", 109},
	{""@IH2"", 110},
	{""@IY"", 111},
	{""@IY0"", 112},
	{""@IY1"", 113},
	{""@IY2"", 114},
	{""@JH"", 115},
	{""@K"", 116},
	{""@L"", 117},
	{""@M"", 118},
	{""@N"", 119},
	{""@NG"", 120},
	{""@OW"", 121},
	{""@OW0"", 122},
	{""@OW1"", 123},
	{""@OW2"", 124},
	{""@OY"", 125},
	{""@OY0"", 126},
	{""@OY1"", 127},
	{""@OY2"", 128},
	{""@P"", 129},
	{""@R"", 130},
	{""@S"", 131},
	{""@SH"", 132},
	{""@T"", 133},
	{""@TH"", 134},
	{""@UH"", 135},
	{""@UH0"", 136},
	{""@UH1"", 137},
	{""@UH2"", 138},
	{""@UW"", 139},
	{""@UW0"", 140},
	{""@UW1"", 141},
	{""@UW2"", 142},
	{""@V"", 143},
	{""@W"", 144},
	{""@Y"", 145},
	{""@Z"", 146},
	{""@ZH"", 147},
	{""eos"", 148},
};

std::string _eos = ""eos"";
std::string cleaner_names = ""english_cleaners"";
int eos_id = 148;

/* Returns lowercase version of input string.*/
std::string lowercase(std::string text) {
    std::transform(text.begin(), text.end(), text.begin(),
                   [](unsigned char c){return std::tolower(c);});
    return text;
}

/* Removes leading, trailing and extra consecutive spaces from text.*/
void collapse_whitespace(std::string &text) {
    int n = text.length();
    bool space = false;
    int k = 0;
    for (int i = 0; i < n; ++i) {
        // Handle leading spaces
        while (k == 0 && i < n && text[i] == ' ') {
            ++i;
        }
        // Handle all other spaces (remove > 1 consecutive)
        if (text[i] == ' ') {
            if (!space) {
                text[k++] = text[i];
                space = true;
            }
        }
        else if (ispunct(text[i])) {
            if (k > 0 && text[k - 1] == ' ') {
                text[k - 1] = text[i];
            }
            else {
                text[k++] = text[i];
            }
            space = false;
        }
        else {
            text[k++] = text[i];
            space = false;
        }
    }
    // Handle trailing spaces
    text.erase(text.begin() + k - 1, text.end());
}

/* Returns input string lowercase with extra spaces collapsed.*/
std::string english_cleaners(std::string &text) {
    text = lowercase(text);
    collapse_whitespace(text);
    return text;
}

/* Returns true if symbol is valid and should be kept in text.*/
bool _should_keep_symbol(std::string s) {
    bool in_dict = symbol_to_id.find(s) != symbol_to_id.end();
    return in_dict && s != ""_"" && s != ""~"";
}

/* Caller must free returned vector.*/
std::vector<int>* _symbols_to_sequence(std::string symbols) {
    std::vector<int>* sequence = new std::vector<int>;
    int n = symbols.length();
    for (int i = 0; i < n; ++i) {

        // Cast to std::string.
        std::string symbol(1, symbols[i]);

        if (_should_keep_symbol(symbol)) {
            sequence->push_back(symbol_to_id[symbol]);
        }
    }
    return sequence;
}

/* Caller must free returned vector.*/

/*
std::vector<int>* _arpabet_to_sequence(std::string text) {
    // Build symbols vector from text
    std::vector<char> v(text.begin(), text.end());
    std::vector<std::string> symbols;
    int n = v.size();
    for (int i = 0; i < n; ++i) {
        symbols.push_back(""@"" + v[i]);
    }

    // Get sequence vector from symbols vector
    return _symbols_to_sequence(symbols);
}
*/

/* Translates raw text into token IDs.*/
std::vector<int>* text_to_sequence(std::string text) {
    std::string clean_text = english_cleaners(text);
    std::vector<int>* sequence = _symbols_to_sequence(clean_text);
    sequence->push_back(eos_id);
    return sequence;
}

// END LJSPEECH_FUNCTIONAL.CC CODE

// This is an example that is minimal to read a model
// from disk and perform inference. There is no data being loaded
// that is up to you to add as a user.
//
// NOTE: Do not add any dependencies to this that cannot be built with
// the minimal makefile. This example must remain trivial to build with
// the minimal build tool.
//
// Usage: minimal <tflite model>

#define TFLITE_MINIMAL_CHECK(x)                              \
  if (!(x)) {                                                \
    fprintf(stderr, ""Error at %s:%d\n"", __FILE__, __LINE__); \
    exit(1);                                                 \
  }


int main(int argc, char* argv[]) {
  if (argc != 2) {
    fprintf(stderr, ""minimal <tflite model>\n"");
    return 1;
  }
  const char* filename = argv[1];

  // Load model
  std::unique_ptr<tflite::FlatBufferModel> model =
      tflite::FlatBufferModel::BuildFromFile(filename);
  TFLITE_MINIMAL_CHECK(model != nullptr);

  // Build the interpreter with the InterpreterBuilder.
  // Note: all Interpreters should be built with the InterpreterBuilder,
  // which allocates memory for the Intrepter and does various set up
  // tasks so that the Interpreter can read the provided model.
  tflite::ops::builtin::BuiltinOpResolver resolver;
  tflite::InterpreterBuilder builder(*model, resolver);
  std::unique_ptr<tflite::Interpreter> interpreter;
  builder(&interpreter);
  TFLITE_MINIMAL_CHECK(interpreter != nullptr);

  // Allocate tensor buffers.
  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);
  printf(""=== Pre-invoke Interpreter State ===\n"");
  tflite::PrintInterpreterState(interpreter.get());

  // Define test input string.
  std::string in = ""hello."";

  // Convert to input ids.
  std::vector<int>* input_ids = text_to_sequence(in);
  int n = input_ids->size();

  // Fill input buffers
  // TODO(user): Insert code to fill input tensors.
  // Note: The buffer of the input tensor with index `i` of type T can
  // be accessed with `T* input = interpreter->typed_input_tensor<T>(i);`
  int* input = interpreter->typed_input_tensor<int>(0);

  // Resize input tensors.
  int t0 = interpreter->inputs()[0];
  int t1 = interpreter->inputs()[1];
  int t2 = interpreter->inputs()[2];
  interpreter->ResizeInputTensor(t0, {1, n});
  interpreter->ResizeInputTensor(t1, {1});
  interpreter->ResizeInputTensor(t2, {1});

  // Allocate tensors again?
  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);

  for (int i = 0; i < n; i++) {
    input[i] = input_ids->data()[i];
  }

  // Run inference - uncomment invoke when input filled
  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);
  printf(""\n\n=== Post-invoke Interpreter State ===\n"");
  tflite::PrintInterpreterState(interpreter.get());


  // Read output buffers
  // TODO(user): Insert getting data out code.
  // Note: The buffer of the output tensor with index `i` of type T can
  // be accessed with `T* output = interpreter->typed_output_tensor<T>(i);`
  float* output = interpreter->typed_output_tensor<float>(0);
  return 0;
}
```

```
=== Pre-invoke Interpreter State ===
Interpreter has 445 tensors and 218 nodes
Inputs: 0 1 2
Outputs: 297 352 353

Tensor   0 input_ids            kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor   1 input_lengths        kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor   2 speaker_ids          kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor   3 Const_2              kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) 
Tensor   4 LocationSensitiveAttention/memory_layer/Tensordot/Const_2 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1
Tensor   5 boolean_mask_1/Reshape_1/shape kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1
Tensor   6 decoder/Tile/input   kTfLiteBool   kTfLiteMmapRo          1 bytes ( 0.0 MB)  1
Tensor   7 decoder/Tile_1/input kTfLiteFloat32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1 1
Tensor   8 decoder/Tile_1/multiples/1 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) 
Tensor   9 decoder/maximum_iterations kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) 
Tensor  10 decoder/LessEqual    kTfLiteBool   kTfLiteMmapRo          1 bytes ( 0.0 MB) 
Tensor  11 decoder/while/input_18 kTfLiteFloat32   kTfLiteMmapRo       1024 bytes ( 0.0 MB)  256
Tensor  12 decoder/while/input_20 kTfLiteFloat32   kTfLiteMmapRo       1024 bytes ( 0.0 MB)  256
Tensor  13 decoder/while/input_23 kTfLiteFloat32   kTfLiteMmapRo      16384 bytes ( 0.0 MB)  4096
Tensor  14 decoder/while/input_29 kTfLiteFloat32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1
Tensor  15 decoder/while/input_37 kTfLiteFloat32   kTfLiteMmapRo      16384 bytes ( 0.0 MB)  4096
Tensor  16 decoder/while/input_39 kTfLiteFloat32   kTfLiteMmapRo        320 bytes ( 0.0 MB)  80
Tensor  17 decoder/while/input_41 kTfLiteFloat32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1
Tensor  18 encoder/bilstm/backward_lstm/Read_2/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       4096 bytes ( 0.0 MB)  1024
Tensor  19 encoder/bilstm/concat/axis kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) 
Tensor  20 transpose_2/perm;encoder/bilstm/forward_lstm/PartitionedCall/transpose_2/perm kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3
Tensor  21 encoder/bilstm/forward_lstm/Read_2/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       4096 bytes ( 0.0 MB)  1024
Tensor  22 encoder/bilstm/forward_lstm/zeros_1/packed/1 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) 
Tensor  23 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/conv_._0/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  24 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/conv_._1/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  25 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/conv_._2/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  26 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/conv_._3/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  27 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/conv_._4/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  28 encoder/embeddings/Gather/resource kTfLiteInt8   kTfLiteMmapRo      76288 bytes ( 0.1 MB)  149 512
Tensor  29 encoder/embeddings/LayerNorm/batchnorm/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  30 encoder/embeddings/LayerNorm/batchnorm/add/y kTfLiteFloat32   kTfLiteMmapRo          4 bytes ( 0.0 MB) 
Tensor  31 encoder/embeddings/LayerNorm/batchnorm/mul/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  32 input_sequence_masks/ExpandDims/dim kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) 
Tensor  33 post_net/tf_tacotron_conv_batch_norm_5/conv_._0/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  34 post_net/tf_tacotron_conv_batch_norm_6/conv_._1/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  35 post_net/tf_tacotron_conv_batch_norm_7/conv_._2/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  36 post_net/tf_tacotron_conv_batch_norm_8/conv_._3/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  37 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  38 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/conv1d/ExpandDims/dim kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) 
Tensor  39 range/delta          kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) 
Tensor  40 residual_projection/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo        320 bytes ( 0.0 MB)  80
Tensor  41 residual_projection/Tensordot/Const_2 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1
Tensor  42 residual_projection/Tensordot/free kTfLiteInt32   kTfLiteMmapRo          8 bytes ( 0.0 MB)  2
Tensor  43 zeros_3/packed/1     kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) 
Tensor  44 zeros_4/packed/1     kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) 
Tensor  45 decoder/TensorArrayV2 kTfLiteFloat32   kTfLiteMmapRo     320000 bytes ( 0.3 MB)  1000 80
Tensor  46 decoder/TensorArrayV2_1 kTfLiteFloat32   kTfLiteMmapRo       4000 bytes ( 0.0 MB)  1000 1
Tensor  47 decoder/TensorArrayV2_2 kTfLiteInt32   kTfLiteMmapRo       4000 bytes ( 0.0 MB)  1000 1
Tensor  48 TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteInt32   kTfLiteMmapRo          8 bytes ( 0.0 MB)  2
Tensor  49 TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_11 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) 
Tensor  50 TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_12 kTfLiteFloat32   kTfLiteMmapRo          4 bytes ( 0.0 MB) 
Tensor  51 LocationSensitiveAttention/memory_layer/Tensordot/MatMul kTfLiteInt8   kTfLiteMmapRo      65536 bytes ( 0.1 MB)  128 512
Tensor  52 MatMul;encoder/bilstm/backward_lstm/PartitionedCall/MatMul kTfLiteInt8   kTfLiteMmapRo     524288 bytes ( 0.5 MB)  1024 512
Tensor  53 MatMul_1;encoder/bilstm/backward_lstm/PartitionedCall/MatMul_1 kTfLiteInt8   kTfLiteMmapRo     262144 bytes ( 0.2 MB)  1024 256
Tensor  54 MatMul;encoder/bilstm/forward_lstm/PartitionedCall/MatMul kTfLiteInt8   kTfLiteMmapRo     524288 bytes ( 0.5 MB)  1024 512
Tensor  55 MatMul_1;encoder/bilstm/forward_lstm/PartitionedCall/MatMul_1 kTfLiteInt8   kTfLiteMmapRo     262144 bytes ( 0.2 MB)  1024 256
Tensor  56 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/batch_norm_._0/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  57 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/batch_norm_._0/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  58 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/batch_norm_._1/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  59 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/batch_norm_._1/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  60 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/batch_norm_._2/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  61 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/batch_norm_._2/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  62 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/batch_norm_._3/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  63 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/batch_norm_._3/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  64 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/batch_norm_._4/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  65 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/batch_norm_._4/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  66 post_net/tf_tacotron_conv_batch_norm_5/batch_norm_._0/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  67 post_net/tf_tacotron_conv_batch_norm_5/batch_norm_._0/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  68 post_net/tf_tacotron_conv_batch_norm_6/batch_norm_._1/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  69 post_net/tf_tacotron_conv_batch_norm_6/batch_norm_._1/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  70 post_net/tf_tacotron_conv_batch_norm_7/batch_norm_._2/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  71 post_net/tf_tacotron_conv_batch_norm_7/batch_norm_._2/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  72 post_net/tf_tacotron_conv_batch_norm_8/batch_norm_._3/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  73 post_net/tf_tacotron_conv_batch_norm_8/batch_norm_._3/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  74 post_net/tf_tacotron_conv_batch_norm_9/batch_norm_._4/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  75 post_net/tf_tacotron_conv_batch_norm_9/batch_norm_._4/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  76 residual_projection/Tensordot/MatMul kTfLiteInt8   kTfLiteMmapRo      40960 bytes ( 0.0 MB)  80 512
Tensor  77 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/conv_._0/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512
Tensor  78 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/conv_._1/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512
Tensor  79 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/conv_._2/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512
Tensor  80 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/conv_._3/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512
Tensor  81 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/conv_._4/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512
Tensor  82 post_net/tf_tacotron_conv_batch_norm_5/conv_._0/conv1d kTfLiteInt8   kTfLiteMmapRo     204800 bytes ( 0.2 MB)  512 1 5 80
Tensor  83 post_net/tf_tacotron_conv_batch_norm_6/conv_._1/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512
Tensor  84 post_net/tf_tacotron_conv_batch_norm_7/conv_._2/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512
Tensor  85 post_net/tf_tacotron_conv_batch_norm_8/conv_._3/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512
Tensor  86 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/conv1d kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512
Tensor  87 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/conv1d1 kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512
Tensor  88 strided_slice_1;encoder/bilstm/forward_lstm/PartitionedCall/strided_slice_1 kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3
Tensor  89 strided_slice_1;encoder/bilstm/forward_lstm/PartitionedCall/strided_slice_11 kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3
Tensor  90 strided_slice_1;encoder/bilstm/forward_lstm/PartitionedCall/strided_slice_12 kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3
Tensor  91 LocationSensitiveAttention/SequenceMask/ExpandDims kTfLiteInt32   kTfLiteMmapRo          8 bytes ( 0.0 MB)  2
Tensor  92 boolean_mask_1/strided_slice_2 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1
Tensor  93 boolean_mask_1/strided_slice_21 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1
Tensor  94 boolean_mask_1/strided_slice_22 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1
Tensor  95 encoder/embeddings/Gather;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteInt8  kTfLiteArenaRw        512 bytes ( 0.0 MB)  1 1 512
Tensor  96 encoder/embeddings/LayerNorm/moments/mean kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor  97 encoder/embeddings/LayerNorm/moments/SquaredDifference kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor  98 encoder/embeddings/LayerNorm/moments/variance kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor  99 encoder/embeddings/LayerNorm/batchnorm/add kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 100 encoder/embeddings/LayerNorm/batchnorm/Rsqrt kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 101 encoder/embeddings/LayerNorm/batchnorm/mul kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 102 encoder/embeddings/LayerNorm/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 103 encoder/embeddings/LayerNorm/batchnorm/mul_2 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 104 encoder/embeddings/LayerNorm/batchnorm/sub kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 105 encoder/embeddings/LayerNorm/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 106 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/conv_._0/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 107 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/conv_._0/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 108 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/conv_._0/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 109 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/conv_._0/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 110 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/batch_norm_._0/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 111 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/Relu;encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/batch_norm_._0/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 112 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/conv_._1/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 113 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/conv_._1/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 114 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/conv_._1/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 115 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/conv_._1/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 116 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/batch_norm_._1/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 117 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/Relu;encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/batch_norm_._1/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 118 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/conv_._2/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 119 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/conv_._2/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 120 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/conv_._2/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 121 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/conv_._2/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 122 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/batch_norm_._2/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 123 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/Relu;encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/batch_norm_._2/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 124 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/conv_._3/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 125 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/conv_._3/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 126 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/conv_._3/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 127 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/conv_._3/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 128 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/batch_norm_._3/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 129 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/Relu;encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/batch_norm_._3/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 130 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/conv_._4/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 131 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/conv_._4/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 132 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/conv_._4/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 133 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/conv_._4/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 134 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/batch_norm_._4/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 135 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/Relu;encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/batch_norm_._4/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 136 transpose;encoder/bilstm/backward_lstm/PartitionedCall/transpose kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 137 ReverseV2;encoder/bilstm/backward_lstm/PartitionedCall/ReverseV2 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 138 Shape;encoder/bilstm/backward_lstm/PartitionedCall/Shape kTfLiteInt32 kTfLitePersistentRo         12 bytes ( 0.0 MB)  3
Tensor 139 strided_slice;encoder/bilstm/backward_lstm/PartitionedCall/strided_slice kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 140 TensorArrayV2_1;encoder/bilstm/backward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 141 TensorArrayV2_1;encoder/bilstm/backward_lstm/PartitionedCall/TensorArrayV2_11 kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 142 TensorArrayV2_1;encoder/bilstm/backward_lstm/PartitionedCall/TensorArrayV2_12 kTfLiteFloat32  kTfLiteDynamic          4 bytes ( 0.0 MB)  1 1 1
Tensor 143 strided_slice_1;encoder/bilstm/backward_lstm/PartitionedCall/strided_slice_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 512
Tensor 144 MatMul;encoder/bilstm/backward_lstm/PartitionedCall/MatMul1 kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024
Tensor 145 MatMul;encoder/bilstm/forward_lstm/PartitionedCall/MatMul1 kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024
Tensor 146 encoder/bilstm/backward_lstm/Shape kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 147 encoder/bilstm/backward_lstm/strided_slice kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 148 encoder/bilstm/backward_lstm/zeros/packed kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 149 encoder/bilstm/backward_lstm/zeros kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 150 MatMul_1;encoder/bilstm/backward_lstm/PartitionedCall/MatMul_11 kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024
Tensor 151 add;encoder/bilstm/backward_lstm/PartitionedCall/add kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024
Tensor 152 BiasAdd;encoder/bilstm/backward_lstm/PartitionedCall/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024
Tensor 153 split;encoder/bilstm/backward_lstm/PartitionedCall/split kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 154 split;encoder/bilstm/backward_lstm/PartitionedCall/split1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 155 split;encoder/bilstm/backward_lstm/PartitionedCall/split2 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 156 split;encoder/bilstm/backward_lstm/PartitionedCall/split3 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 157 Sigmoid;encoder/bilstm/backward_lstm/PartitionedCall/Sigmoid kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 158 Sigmoid_1;encoder/bilstm/backward_lstm/PartitionedCall/Sigmoid_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 159 Sigmoid_2;encoder/bilstm/backward_lstm/PartitionedCall/Sigmoid_2 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 160 Tanh;encoder/bilstm/backward_lstm/PartitionedCall/Tanh kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 161 mul_1;encoder/bilstm/backward_lstm/PartitionedCall/mul_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 162 mul;encoder/bilstm/backward_lstm/PartitionedCall/mul kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 163 add_1;encoder/bilstm/backward_lstm/PartitionedCall/add_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 164 Tanh_1;encoder/bilstm/backward_lstm/PartitionedCall/Tanh_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 165 mul_2;encoder/bilstm/backward_lstm/PartitionedCall/mul_2 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 166 zeros_like;encoder/bilstm/backward_lstm/PartitionedCall/zeros_like kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 167 MatMul_1;encoder/bilstm/forward_lstm/PartitionedCall/MatMul_11 kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024
Tensor 168 add;encoder/bilstm/forward_lstm/PartitionedCall/add kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024
Tensor 169 BiasAdd;encoder/bilstm/forward_lstm/PartitionedCall/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024
Tensor 170 split;encoder/bilstm/forward_lstm/PartitionedCall/split kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 171 split;encoder/bilstm/forward_lstm/PartitionedCall/split1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 172 split;encoder/bilstm/forward_lstm/PartitionedCall/split2 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 173 split;encoder/bilstm/forward_lstm/PartitionedCall/split3 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 174 Sigmoid;encoder/bilstm/forward_lstm/PartitionedCall/Sigmoid kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 175 Sigmoid_1;encoder/bilstm/forward_lstm/PartitionedCall/Sigmoid_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 176 Sigmoid_2;encoder/bilstm/forward_lstm/PartitionedCall/Sigmoid_2 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 177 Tanh;encoder/bilstm/forward_lstm/PartitionedCall/Tanh kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 178 mul_1;encoder/bilstm/forward_lstm/PartitionedCall/mul_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 179 mul;encoder/bilstm/forward_lstm/PartitionedCall/mul kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 180 add_1;encoder/bilstm/forward_lstm/PartitionedCall/add_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 181 Tanh_1;encoder/bilstm/forward_lstm/PartitionedCall/Tanh_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 182 mul_2;encoder/bilstm/forward_lstm/PartitionedCall/mul_2 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 183 zeros_like;encoder/bilstm/forward_lstm/PartitionedCall/zeros_like kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 184 LocationSensitiveAttention/SequenceMask/ExpandDims1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 185 Max                  kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 186 input_sequence_masks/Range kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 187 input_sequence_masks/Less kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1
Tensor 188 ExpandDims;encoder/bilstm/backward_lstm/PartitionedCall/ExpandDims kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1 1
Tensor 189 transpose_1;encoder/bilstm/backward_lstm/PartitionedCall/transpose_1 kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1 1
Tensor 190 ReverseV2_1;encoder/bilstm/backward_lstm/PartitionedCall/ReverseV2_1 kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1 1
Tensor 191 while;encoder/bilstm/backward_lstm/PartitionedCall/while kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 192 while;encoder/bilstm/backward_lstm/PartitionedCall/while1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 193 while;encoder/bilstm/backward_lstm/PartitionedCall/while2 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 194 while;encoder/bilstm/backward_lstm/PartitionedCall/while3 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 195 while;encoder/bilstm/backward_lstm/PartitionedCall/while4 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 196 while;encoder/bilstm/backward_lstm/PartitionedCall/while5 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 197 while;encoder/bilstm/backward_lstm/PartitionedCall/while6 kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1 1
Tensor 198 while;encoder/bilstm/backward_lstm/PartitionedCall/while7 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 199 while;encoder/bilstm/backward_lstm/PartitionedCall/while8 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 200 TensorArrayV2Stack/TensorListStack;encoder/bilstm/backward_lstm/PartitionedCall/TensorArrayV2Stack/TensorListStack kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 201 TensorArrayV2Stack/TensorListStack;encoder/bilstm/backward_lstm/PartitionedCall/TensorArrayV2Stack/TensorListStack1 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 202 transpose_2;encoder/bilstm/backward_lstm/PartitionedCall/transpose_2 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 203 encoder/bilstm/ReverseV2 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 204 while;encoder/bilstm/forward_lstm/PartitionedCall/while kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 205 while;encoder/bilstm/forward_lstm/PartitionedCall/while1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 206 while;encoder/bilstm/forward_lstm/PartitionedCall/while2 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 207 while;encoder/bilstm/forward_lstm/PartitionedCall/while3 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 208 while;encoder/bilstm/forward_lstm/PartitionedCall/while4 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 209 while;encoder/bilstm/forward_lstm/PartitionedCall/while5 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 210 while;encoder/bilstm/forward_lstm/PartitionedCall/while6 kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1 1
Tensor 211 while;encoder/bilstm/forward_lstm/PartitionedCall/while7 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 212 while;encoder/bilstm/forward_lstm/PartitionedCall/while8 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256
Tensor 213 TensorArrayV2Stack/TensorListStack;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2Stack/TensorListStack kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 214 TensorArrayV2Stack/TensorListStack;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2Stack/TensorListStack1 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 215 transpose_2;encoder/bilstm/forward_lstm/PartitionedCall/transpose_2 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 216 encoder/bilstm/concat kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 217 LocationSensitiveAttention/Shape kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 218 LocationSensitiveAttention/strided_slice kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 219 LocationSensitiveAttention/SequenceMask/Range kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 220 LocationSensitiveAttention/SequenceMask/Less kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1
Tensor 221 LocationSensitiveAttention/SequenceMask/Cast_1 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 222 LocationSensitiveAttention/Shape_1 kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 223 LocationSensitiveAttention/concat kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 224 LocationSensitiveAttention/Reshape kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 225 LocationSensitiveAttention/mul kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 226 LocationSensitiveAttention/memory_layer/Tensordot/Shape kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 227 LocationSensitiveAttention/memory_layer/Tensordot/GatherV2;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 228 LocationSensitiveAttention/memory_layer/Tensordot/Prod kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 229 LocationSensitiveAttention/memory_layer/Tensordot/concat_1 kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 230 LocationSensitiveAttention/memory_layer/Tensordot/GatherV2_1;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 231 LocationSensitiveAttention/memory_layer/Tensordot/Prod_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 232 LocationSensitiveAttention/memory_layer/Tensordot/stack kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 233 LocationSensitiveAttention/memory_layer/Tensordot/Reshape kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 234 LocationSensitiveAttention/memory_layer/Tensordot/MatMul1 kTfLiteFloat32  kTfLiteArenaRw        512 bytes ( 0.0 MB)  1 128
Tensor 235 LocationSensitiveAttention/memory_layer/Tensordot kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 236 Shape_2              kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 237 strided_slice        kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 238 Reshape/shape        kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 239 Reshape_1/shape      kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 240 decoder/Tile/multiples kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 241 decoder/Tile         kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1
Tensor 242 decoder/LogicalOr    kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1
Tensor 243 decoder/zeros_like/Shape kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 244 decoder/zeros_like   kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 245 decoder/Tile_1/multiples kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 246 decoder/Tile_1       kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 80
Tensor 247 zeros_5/packed       kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 248 strided_slice_2      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 249 range                kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 250 ExpandDims           kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 251 strided_slice_3      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 252 Tile/multiples       kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 253 Tile                 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 254 zeros/packed         kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 255 zeros                kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024
Tensor 256 zeros_4/packed       kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 257 zeros_4              kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 512
Tensor 258 zeros_5              kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 259 zeros_7              kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 260 decoder/while        kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 261 decoder/while1       kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 262 decoder/while2       kTfLiteFloat32  kTfLiteArenaRw     320000 bytes ( 0.3 MB)  1000 80
Tensor 263 decoder/while3       kTfLiteFloat32  kTfLiteArenaRw       4000 bytes ( 0.0 MB)  1000 1
Tensor 264 decoder/while4       kTfLiteInt32  kTfLiteArenaRw       4000 bytes ( 0.0 MB)  1000 1
Tensor 265 decoder/while5       kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024
Tensor 266 decoder/while6       kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024
Tensor 267 decoder/while7       kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024
Tensor 268 decoder/while8       kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024
Tensor 269 decoder/while9       kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 512
Tensor 270 decoder/while10      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 271 decoder/while11      kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 272 decoder/while12      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 273 decoder/while13      kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 80
Tensor 274 decoder/while14      kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1
Tensor 275 decoder/while15      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 276 decoder/while16      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 277 decoder/while17      kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 278 decoder/while18      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 279 decoder/while19      kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 280 decoder/while20      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 281 Reshape              kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 1 80
Tensor 282 Abs                  kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 1 80
Tensor 283 Sum                  kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 284 Cast                 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 285 NotEqual             kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1
Tensor 286 boolean_mask/Reshape_1 kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1
Tensor 287 boolean_mask/Where   kTfLiteInt64  kTfLiteArenaRw          8 bytes ( 0.0 MB)  1 1
Tensor 288 boolean_mask/Squeeze kTfLiteInt64  kTfLiteArenaRw          8 bytes ( 0.0 MB)  1
Tensor 289 boolean_mask/Shape   kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 290 boolean_mask/strided_slice kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 291 boolean_mask/Prod    kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 292 boolean_mask/concat/values_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 293 boolean_mask/strided_slice_2 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 294 boolean_mask/concat  kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 295 boolean_mask/Reshape kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 296 boolean_mask/GatherV2;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 297 Identity             kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 298 post_net/tf_tacotron_conv_batch_norm_5/conv_._0/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 1 1 80
Tensor 299 post_net/tf_tacotron_conv_batch_norm_5/conv_._0/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 300 post_net/tf_tacotron_conv_batch_norm_5/conv_._0/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 301 post_net/tf_tacotron_conv_batch_norm_5/conv_._0/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 302 post_net/tf_tacotron_conv_batch_norm_5/batch_norm_._0/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 303 post_net/tf_tacotron_conv_batch_norm_5/batch_norm_._0/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 304 post_net/tf_tacotron_conv_batch_norm_5/activation_7/Tanh kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 305 post_net/tf_tacotron_conv_batch_norm_6/conv_._1/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 306 post_net/tf_tacotron_conv_batch_norm_6/conv_._1/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 307 post_net/tf_tacotron_conv_batch_norm_6/conv_._1/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 308 post_net/tf_tacotron_conv_batch_norm_6/conv_._1/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 309 post_net/tf_tacotron_conv_batch_norm_6/batch_norm_._1/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 310 post_net/tf_tacotron_conv_batch_norm_6/batch_norm_._1/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 311 post_net/tf_tacotron_conv_batch_norm_6/activation_7/Tanh kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 312 post_net/tf_tacotron_conv_batch_norm_7/conv_._2/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 313 post_net/tf_tacotron_conv_batch_norm_7/conv_._2/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 314 post_net/tf_tacotron_conv_batch_norm_7/conv_._2/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 315 post_net/tf_tacotron_conv_batch_norm_7/conv_._2/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 316 post_net/tf_tacotron_conv_batch_norm_7/batch_norm_._2/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 317 post_net/tf_tacotron_conv_batch_norm_7/batch_norm_._2/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 318 post_net/tf_tacotron_conv_batch_norm_7/activation_7/Tanh kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 319 post_net/tf_tacotron_conv_batch_norm_8/conv_._3/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 320 post_net/tf_tacotron_conv_batch_norm_8/conv_._3/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 321 post_net/tf_tacotron_conv_batch_norm_8/conv_._3/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 322 post_net/tf_tacotron_conv_batch_norm_8/conv_._3/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 323 post_net/tf_tacotron_conv_batch_norm_8/batch_norm_._3/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 324 post_net/tf_tacotron_conv_batch_norm_8/batch_norm_._3/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 325 post_net/tf_tacotron_conv_batch_norm_8/activation_7/Tanh kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 326 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 327 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/conv1d2 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512
Tensor 328 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 329 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 330 post_net/tf_tacotron_conv_batch_norm_9/batch_norm_._4/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 331 post_net/tf_tacotron_conv_batch_norm_9/batch_norm_._4/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 332 residual_projection/Tensordot/Shape kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 333 residual_projection/Tensordot/GatherV2;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 334 residual_projection/Tensordot/Prod kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 335 residual_projection/Tensordot/concat_1 kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 336 residual_projection/Tensordot/GatherV2_1;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 337 residual_projection/Tensordot/Prod_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 338 residual_projection/Tensordot/stack kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 339 residual_projection/Tensordot/Reshape kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 340 residual_projection/Tensordot/MatMul2 kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 80
Tensor 341 residual_projection/Tensordot kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 342 residual_projection/BiasAdd kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 1 80
Tensor 343 add                  kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 1 80
Tensor 344 boolean_mask_1/Shape kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 345 boolean_mask_1/strided_slice kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 346 boolean_mask_1/Prod  kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) 
Tensor 347 boolean_mask_1/concat/values_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 348 boolean_mask_1/strided_slice_23 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 349 boolean_mask_1/concat kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2
Tensor 350 boolean_mask_1/Reshape kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 351 boolean_mask_1/GatherV2;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 352 Identity_1           kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1
Tensor 353 Identity_2           kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1
Tensor 354 encoder/embeddings/Gather;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1_dequantize kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512
Tensor 355 (null)               kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 356 (null)               kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 357 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 358 (null)               kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3
Tensor 359 (null)               kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 360 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 361 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 362 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 363 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 364 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 365 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 366 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 367 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 368 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 369 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 370 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 371 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 372 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 373 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 374 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 375 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 376 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 377 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 378 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 379 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 380 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 381 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 382 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 383 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 384 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 385 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 386 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 387 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 388 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 389 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 390 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 391 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 392 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 393 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 394 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 395 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 396 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 397 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 398 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 399 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 400 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 401 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 402 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 403 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 404 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 405 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 406 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 407 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 408 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 409 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 410 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 411 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 412 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 413 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 414 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)
Tensor 415 (null)               kTfLiteInt8  kTfLiteArenaRw       2560 bytes ( 0.0 MB)  1 1 1 2560
Tensor 416 (null)               kTfLiteInt8  kTfLiteArenaRw        512 bytes ( 0.0 MB)  1 1 1 512
Tensor 417 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 418 (null)               kTfLiteInt32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  512 1
Tensor 419 (null)               kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 420 (null)               kTfLiteInt32 kTfLiteArenaRwPersistent       2048 bytes ( 0.0 MB)  512
Tensor 421 (null)               kTfLiteInt8  kTfLiteArenaRw       2560 bytes ( 0.0 MB)  1 1 1 2560
Tensor 422 (null)               kTfLiteInt8  kTfLiteArenaRw        512 bytes ( 0.0 MB)  1 1 1 512
Tensor 423 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 424 (null)               kTfLiteInt32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  512 1
Tensor 425 (null)               kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 426 (null)               kTfLiteInt32 kTfLiteArenaRwPersistent       2048 bytes ( 0.0 MB)  512
Tensor 427 (null)               kTfLiteInt8  kTfLiteArenaRw       2560 bytes ( 0.0 MB)  1 1 1 2560
Tensor 428 (null)               kTfLiteInt8  kTfLiteArenaRw        512 bytes ( 0.0 MB)  1 1 1 512
Tensor 429 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 430 (null)               kTfLiteInt32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  512 1
Tensor 431 (null)               kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 432 (null)               kTfLiteInt32 kTfLiteArenaRwPersistent       2048 bytes ( 0.0 MB)  512
Tensor 433 (null)               kTfLiteInt8  kTfLiteArenaRw       2560 bytes ( 0.0 MB)  1 1 1 2560
Tensor 434 (null)               kTfLiteInt8  kTfLiteArenaRw        512 bytes ( 0.0 MB)  1 1 1 512
Tensor 435 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 436 (null)               kTfLiteInt32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  512 1
Tensor 437 (null)               kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 438 (null)               kTfLiteInt32 kTfLiteArenaRwPersistent       2048 bytes ( 0.0 MB)  512
Tensor 439 (null)               kTfLiteInt8  kTfLiteArenaRw       2560 bytes ( 0.0 MB)  1 1 1 2560
Tensor 440 (null)               kTfLiteInt8  kTfLiteArenaRw        512 bytes ( 0.0 MB)  1 1 1 512
Tensor 441 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 442 (null)               kTfLiteInt32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  512 1
Tensor 443 (null)               kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1
Tensor 444 (null)               kTfLiteInt32 kTfLiteArenaRwPersistent       2048 bytes ( 0.0 MB)  512

Node   0 Operator Builtin Code  36 GATHER
  Inputs: 28 0
  Outputs: 95
Node   1 Operator Builtin Code   6 DEQUANTIZE
  Inputs: 95
  Outputs: 354
Node   2 Operator Builtin Code  40 MEAN
  Inputs: 354 92
  Outputs: 96
  Temporaries: 355 356 357
Node   3 Operator Builtin Code  99 SQUARED_DIFFERENCE
  Inputs: 354 96
  Outputs: 97
Node   4 Operator Builtin Code  40 MEAN
  Inputs: 97 92
  Outputs: 98
  Temporaries: 358 359 360
Node   5 Operator Builtin Code   0 ADD
  Inputs: 98 30
  Outputs: 99
Node   6 Operator Builtin Code  76 RSQRT
  Inputs: 99
  Outputs: 100
Node   7 Operator Builtin Code  18 MUL
  Inputs: 100 31
  Outputs: 101
Node   8 Operator Builtin Code  18 MUL
  Inputs: 354 101
  Outputs: 102
Node   9 Operator Builtin Code  18 MUL
  Inputs: 96 101
  Outputs: 103
Node  10 Operator Builtin Code  41 SUB
  Inputs: 29 103
  Outputs: 104
Node  11 Operator Builtin Code   0 ADD
  Inputs: 102 104
  Outputs: 105
Node  12 Operator Builtin Code  70 EXPAND_DIMS
  Inputs: 105 38
  Outputs: 106
Node  13 Operator Builtin Code   3 CONV_2D
  Inputs: 106 77 86
  Outputs: 107
  Temporaries: 415 416 417 418 419 420
Node  14 Operator Builtin Code  43 SQUEEZE
  Inputs: 107
  Outputs: 108
Node  15 Operator Builtin Code   0 ADD
  Inputs: 108 23
  Outputs: 109
Node  16 Operator Builtin Code  18 MUL
  Inputs: 109 56
  Outputs: 110
Node  17 Operator Builtin Code   0 ADD
  Inputs: 110 57
  Outputs: 111
Node  18 Operator Builtin Code  70 EXPAND_DIMS
  Inputs: 111 38
  Outputs: 112
Node  19 Operator Builtin Code   3 CONV_2D
  Inputs: 112 78 86
  Outputs: 113
  Temporaries: 421 422 423 424 425 426
Node  20 Operator Builtin Code  43 SQUEEZE
  Inputs: 113
  Outputs: 114
Node  21 Operator Builtin Code   0 ADD
  Inputs: 114 24
  Outputs: 115
Node  22 Operator Builtin Code  18 MUL
  Inputs: 115 58
  Outputs: 116
Node  23 Operator Builtin Code   0 ADD
  Inputs: 116 59
  Outputs: 117
Node  24 Operator Builtin Code  70 EXPAND_DIMS
  Inputs: 117 38
  Outputs: 118
Node  25 Operator Builtin Code   3 CONV_2D
  Inputs: 118 79 86
  Outputs: 119
  Temporaries: 427 428 429 430 431 432
Node  26 Operator Builtin Code  43 SQUEEZE
  Inputs: 119
  Outputs: 120
Node  27 Operator Builtin Code   0 ADD
  Inputs: 120 25
  Outputs: 121
Node  28 Operator Builtin Code  18 MUL
  Inputs: 121 60
  Outputs: 122
Node  29 Operator Builtin Code   0 ADD
  Inputs: 122 61
  Outputs: 123
Node  30 Operator Builtin Code  70 EXPAND_DIMS
  Inputs: 123 38
  Outputs: 124
Node  31 Operator Builtin Code   3 CONV_2D
  Inputs: 124 80 86
  Outputs: 125
  Temporaries: 433 434 435 436 437 438
Node  32 Operator Builtin Code  43 SQUEEZE
  Inputs: 125
  Outputs: 126
Node  33 Operator Builtin Code   0 ADD
  Inputs: 126 26
  Outputs: 127
Node  34 Operator Builtin Code  18 MUL
  Inputs: 127 62
  Outputs: 128
Node  35 Operator Builtin Code   0 ADD
  Inputs: 128 63
  Outputs: 129
Node  36 Operator Builtin Code  70 EXPAND_DIMS
  Inputs: 129 38
  Outputs: 130
Node  37 Operator Builtin Code   3 CONV_2D
  Inputs: 130 81 86
  Outputs: 131
  Temporaries: 439 440 441 442 443 444
Node  38 Operator Builtin Code  43 SQUEEZE
  Inputs: 131
  Outputs: 132
Node  39 Operator Builtin Code   0 ADD
  Inputs: 132 27
  Outputs: 133
Node  40 Operator Builtin Code  18 MUL
  Inputs: 133 64
  Outputs: 134
Node  41 Operator Builtin Code   0 ADD
  Inputs: 134 65
  Outputs: 135
Node  42 Operator Builtin Code  39 TRANSPOSE
  Inputs: 135 20
  Outputs: 136
Node  43 Operator Builtin Code 105 REVERSE_V2
  Inputs: 136 93
  Outputs: 137
Node  44 Operator Builtin Code  77 SHAPE
  Inputs: 136
  Outputs: 138
Node  45 Operator Builtin Code  45 STRIDED_SLICE
  Inputs: 138 93 94 94
  Outputs: 139
Node  46 Operator Builtin Code  22 RESHAPE
  Inputs: 139 94
  Outputs: 140
Node  47 Operator Builtin Code   2 CONCATENATION
  Inputs: 140 48
  Outputs: 141
Node  48 Operator Builtin Code  94 FILL
  Inputs: 141 50
  Outputs: 142
Node  49 Operator Builtin Code  45 STRIDED_SLICE
  Inputs: 136 88 89 90
  Outputs: 143
Node  50 Operator Builtin Code   9 FULLY_CONNECTED
  Inputs: 143 52 -1
  Outputs: 144
Node  51 Operator Builtin Code   9 FULLY_CONNECTED
  Inputs: 143 54 -1
  Outputs: 145
Node  52 Operator Builtin Code  77 SHAPE
  Inputs: 135
  Outputs: 146
Node  53 Operator Builtin Code  45 STRIDED_SLICE
  Inputs: 146 93 94 94
  Outputs: 147
Node  54 Operator Builtin Code  83 PACK
  Inputs: 147 22
  Outputs: 148
Node  55 Operator Builtin Code  94 FILL
  Inputs: 148 50
  Outputs: 149
Node  56 Operator Builtin Code   9 FULLY_CONNECTED
  Inputs: 149 53 -1
  Outputs: 150
Node  57 Operator Builtin Code   0 ADD
  Inputs: 144 150
  Outputs: 151
Node  58 Operator Builtin Code   0 ADD
  Inputs: 151 18
  Outputs: 152
Node  59 Operator Builtin Code  49 SPLIT
  Inputs: 39 152
  Outputs: 153 154 155 156
Node  60 Operator Builtin Code  14 LOGISTIC
  Inputs: 153
  Outputs: 157
Node  61 Operator Builtin Code  14 LOGISTIC
  Inputs: 154
  Outputs: 158
Node  62 Operator Builtin Code  14 LOGISTIC
  Inputs: 156
  Outputs: 159
Node  63 Operator Builtin Code  28 TANH
  Inputs: 155
  Outputs: 160
Node  64 Operator Builtin Code  18 MUL
  Inputs: 157 160
  Outputs: 161
Node  65 Operator Builtin Code  18 MUL
  Inputs: 158 149
  Outputs: 162
Node  66 Operator Builtin Code   0 ADD
  Inputs: 162 161
  Outputs: 163
Node  67 Operator Builtin Code  28 TANH
  Inputs: 163
  Outputs: 164
Node  68 Operator Builtin Code  18 MUL
  Inputs: 159 164
  Outputs: 165
Node  69 Operator Builtin Code  93 ZEROS_LIKE
  Inputs: 165
  Outputs: 166
Node  70 Operator Builtin Code   9 FULLY_CONNECTED
  Inputs: 149 55 -1
  Outputs: 167
Node  71 Operator Builtin Code   0 ADD
  Inputs: 145 167
  Outputs: 168
Node  72 Operator Builtin Code   0 ADD
  Inputs: 168 21
  Outputs: 169
Node  73 Operator Builtin Code  49 SPLIT
  Inputs: 39 169
  Outputs: 170 171 172 173
Node  74 Operator Builtin Code  14 LOGISTIC
  Inputs: 170
  Outputs: 174
Node  75 Operator Builtin Code  14 LOGISTIC
  Inputs: 171
  Outputs: 175
Node  76 Operator Builtin Code  14 LOGISTIC
  Inputs: 173
  Outputs: 176
Node  77 Operator Builtin Code  28 TANH
  Inputs: 172
  Outputs: 177
Node  78 Operator Builtin Code  18 MUL
  Inputs: 174 177
  Outputs: 178
Node  79 Operator Builtin Code  18 MUL
  Inputs: 175 149
  Outputs: 179
Node  80 Operator Builtin Code   0 ADD
  Inputs: 179 178
  Outputs: 180
Node  81 Operator Builtin Code  28 TANH
  Inputs: 180
  Outputs: 181
Node  82 Operator Builtin Code  18 MUL
  Inputs: 176 181
  Outputs: 182
Node  83 Operator Builtin Code  93 ZEROS_LIKE
  Inputs: 182
  Outputs: 183
Node  84 Operator Builtin Code  22 RESHAPE
  Inputs: 1 91
  Outputs: 184
Node  85 Operator Builtin Code  82 REDUCE_MAX
  Inputs: 1 93
  Outputs: 185
Node  86 Operator Builtin Code  96 RANGE
  Inputs: 49 185 39
  Outputs: 186
Node  87 Operator Builtin Code  58 LESS
  Inputs: 186 184
  Outputs: 187
Node  88 Operator Builtin Code  70 EXPAND_DIMS
  Inputs: 187 32
  Outputs: 188
Node  89 Operator Builtin Code  39 TRANSPOSE
  Inputs: 188 20
  Outputs: 189
Node  90 Operator Builtin Code 105 REVERSE_V2
  Inputs: 189 93
  Outputs: 190
Node  91 Operator Builtin Code 119 WHILE
  Inputs: 49 49 142 149 149 139 190 137 166
  Outputs: 191 192 193 194 195 196 197 198 199
Node  92 Operator Builtin Code  77 SHAPE
  Inputs: 193
  Outputs: 200
Node  93 Operator Builtin Code  22 RESHAPE
  Inputs: 193 200
  Outputs: 201
Node  94 Operator Builtin Code  39 TRANSPOSE
  Inputs: 201 20
  Outputs: 202
Node  95 Operator Builtin Code 105 REVERSE_V2
  Inputs: 202 94
  Outputs: 203
Node  96 Operator Builtin Code 119 WHILE
  Inputs: 49 49 142 149 149 139 189 136 183
  Outputs: 204 205 206 207 208 209 210 211 212
Node  97 Operator Builtin Code  77 SHAPE
  Inputs: 206
  Outputs: 213
Node  98 Operator Builtin Code  22 RESHAPE
  Inputs: 206 213
  Outputs: 214
Node  99 Operator Builtin Code  39 TRANSPOSE
  Inputs: 214 20
  Outputs: 215
Node 100 Operator Builtin Code   2 CONCATENATION
  Inputs: 215 203
  Outputs: 216
Node 101 Operator Builtin Code  77 SHAPE
  Inputs: 216
  Outputs: 217
Node 102 Operator Builtin Code  45 STRIDED_SLICE
  Inputs: 217 94 92 94
  Outputs: 218
Node 103 Operator Builtin Code  96 RANGE
  Inputs: 49 218 39
  Outputs: 219
Node 104 Operator Builtin Code  58 LESS
  Inputs: 219 184
  Outputs: 220
Node 105 Operator Builtin Code  53 CAST
  Inputs: 220
  Outputs: 221
Node 106 Operator Builtin Code  77 SHAPE
  Inputs: 221
  Outputs: 222
Node 107 Operator Builtin Code   2 CONCATENATION
  Inputs: 222 94
  Outputs: 223
Node 108 Operator Builtin Code  22 RESHAPE
  Inputs: 221 223
  Outputs: 224
Node 109 Operator Builtin Code  18 MUL
  Inputs: 216 224
  Outputs: 225
Node 110 Operator Builtin Code  77 SHAPE
  Inputs: 225
  Outputs: 226
Node 111 Operator Builtin Code  36 GATHER
  Inputs: 226 42
  Outputs: 227
Node 112 Operator Builtin Code  81 REDUCE_PROD
  Inputs: 227 93
  Outputs: 228
Node 113 Operator Builtin Code   2 CONCATENATION
  Inputs: 227 4
  Outputs: 229
Node 114 Operator Builtin Code  36 GATHER
  Inputs: 226 92
  Outputs: 230
Node 115 Operator Builtin Code  81 REDUCE_PROD
  Inputs: 230 93
  Outputs: 231
Node 116 Operator Builtin Code  83 PACK
  Inputs: 228 231
  Outputs: 232
Node 117 Operator Builtin Code  22 RESHAPE
  Inputs: 225 232
  Outputs: 233
Node 118 Operator Builtin Code   9 FULLY_CONNECTED
  Inputs: 233 51 -1
  Outputs: 234
Node 119 Operator Builtin Code  22 RESHAPE
  Inputs: 234 229
  Outputs: 235
Node 120 Operator Builtin Code  77 SHAPE
  Inputs: 235
  Outputs: 236
Node 121 Operator Builtin Code  45 STRIDED_SLICE
  Inputs: 217 93 94 94
  Outputs: 237
Node 122 Operator Builtin Code  83 PACK
  Inputs: 237 32 8
  Outputs: 238
Node 123 Operator Builtin Code  83 PACK
  Inputs: 237 32
  Outputs: 239
Node 124 Operator Builtin Code  83 PACK
  Inputs: 237
  Outputs: 240
Node 125 Operator Builtin Code  69 TILE
  Inputs: 6 240
  Outputs: 241
Node 126 Operator Builtin Code  84 LOGICAL_OR
  Inputs: 241 10
  Outputs: 242
Node 127 Operator Builtin Code  77 SHAPE
  Inputs: 242
  Outputs: 243
Node 128 Operator Builtin Code  94 FILL
  Inputs: 243 49
  Outputs: 244
Node 129 Operator Builtin Code  83 PACK
  Inputs: 237 8
  Outputs: 245
Node 130 Operator Builtin Code  69 TILE
  Inputs: 7 245
  Outputs: 246
Node 131 Operator Builtin Code  83 PACK
  Inputs: 237 218
  Outputs: 247
Node 132 Operator Builtin Code  45 STRIDED_SLICE
  Inputs: 236 94 92 94
  Outputs: 248
Node 133 Operator Builtin Code  96 RANGE
  Inputs: 49 248 39
  Outputs: 249
Node 134 Operator Builtin Code  70 EXPAND_DIMS
  Inputs: 249 49
  Outputs: 250
Node 135 Operator Builtin Code  45 STRIDED_SLICE
  Inputs: 236 93 94 94
  Outputs: 251
Node 136 Operator Builtin Code  83 PACK
  Inputs: 251 39
  Outputs: 252
Node 137 Operator Builtin Code  69 TILE
  Inputs: 250 252
  Outputs: 253
Node 138 Operator Builtin Code  83 PACK
  Inputs: 237 43
  Outputs: 254
Node 139 Operator Builtin Code  94 FILL
  Inputs: 254 50
  Outputs: 255
Node 140 Operator Builtin Code  83 PACK
  Inputs: 237 44
  Outputs: 256
Node 141 Operator Builtin Code  94 FILL
  Inputs: 256 50
  Outputs: 257
Node 142 Operator Builtin Code  94 FILL
  Inputs: 247 50
  Outputs: 258
Node 143 Operator Builtin Code  94 FILL
  Inputs: 240 49
  Outputs: 259
Node 144 Operator Builtin Code 119 WHILE
  Inputs: 49 49 45 46 47 255 255 255 255 257 49 258 259 246 242 244 253 235 184 225 240
  Outputs: 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280
Node 145 Operator Builtin Code  22 RESHAPE
  Inputs: 262 238
  Outputs: 281
Node 146 Operator Builtin Code 101 ABS
  Inputs: 281
  Outputs: 282
Node 147 Operator Builtin Code  74 SUM
  Inputs: 282 32
  Outputs: 283
Node 148 Operator Builtin Code  53 CAST
  Inputs: 283
  Outputs: 284
Node 149 Operator Builtin Code  72 NOT_EQUAL
  Inputs: 284 49
  Outputs: 285
Node 150 Operator Builtin Code  22 RESHAPE
  Inputs: 285 5
  Outputs: 286
Node 151 Operator Builtin Code 109 WHERE
  Inputs: 286
  Outputs: 287
Node 152 Operator Builtin Code  43 SQUEEZE
  Inputs: 287
  Outputs: 288
Node 153 Operator Builtin Code  77 SHAPE
  Inputs: 281
  Outputs: 289
Node 154 Operator Builtin Code  45 STRIDED_SLICE
  Inputs: 289 93 92 94
  Outputs: 290
Node 155 Operator Builtin Code  81 REDUCE_PROD
  Inputs: 290 93
  Outputs: 291
Node 156 Operator Builtin Code  83 PACK
  Inputs: 291
  Outputs: 292
Node 157 Operator Builtin Code  45 STRIDED_SLICE
  Inputs: 289 92 93 94
  Outputs: 293
Node 158 Operator Builtin Code   2 CONCATENATION
  Inputs: 292 293
  Outputs: 294
Node 159 Operator Builtin Code  22 RESHAPE
  Inputs: 262 294
  Outputs: 295
Node 160 Operator Builtin Code  36 GATHER
  Inputs: 295 288
  Outputs: 296
Node 161 Operator Builtin Code  70 EXPAND_DIMS
  Inputs: 296 49
  Outputs: 297
Node 162 Operator Builtin Code  70 EXPAND_DIMS
  Inputs: 281 38
  Outputs: 298
Node 163 Operator Builtin Code   3 CONV_2D
  Inputs: 298 82 86
  Outputs: 299
Node 164 Operator Builtin Code  43 SQUEEZE
  Inputs: 299
  Outputs: 300
Node 165 Operator Builtin Code   0 ADD
  Inputs: 300 33
  Outputs: 301
Node 166 Operator Builtin Code  18 MUL
  Inputs: 301 66
  Outputs: 302
Node 167 Operator Builtin Code   0 ADD
  Inputs: 302 67
  Outputs: 303
Node 168 Operator Builtin Code  28 TANH
  Inputs: 303
  Outputs: 304
Node 169 Operator Builtin Code  70 EXPAND_DIMS
  Inputs: 304 38
  Outputs: 305
Node 170 Operator Builtin Code   3 CONV_2D
  Inputs: 305 83 86
  Outputs: 306
Node 171 Operator Builtin Code  43 SQUEEZE
  Inputs: 306
  Outputs: 307
Node 172 Operator Builtin Code   0 ADD
  Inputs: 307 34
  Outputs: 308
Node 173 Operator Builtin Code  18 MUL
  Inputs: 308 68
  Outputs: 309
Node 174 Operator Builtin Code   0 ADD
  Inputs: 309 69
  Outputs: 310
Node 175 Operator Builtin Code  28 TANH
  Inputs: 310
  Outputs: 311
Node 176 Operator Builtin Code  70 EXPAND_DIMS
  Inputs: 311 38
  Outputs: 312
Node 177 Operator Builtin Code   3 CONV_2D
  Inputs: 312 84 86
  Outputs: 313
Node 178 Operator Builtin Code  43 SQUEEZE
  Inputs: 313
  Outputs: 314
Node 179 Operator Builtin Code   0 ADD
  Inputs: 314 35
  Outputs: 315
Node 180 Operator Builtin Code  18 MUL
  Inputs: 315 70
  Outputs: 316
Node 181 Operator Builtin Code   0 ADD
  Inputs: 316 71
  Outputs: 317
Node 182 Operator Builtin Code  28 TANH
  Inputs: 317
  Outputs: 318
Node 183 Operator Builtin Code  70 EXPAND_DIMS
  Inputs: 318 38
  Outputs: 319
Node 184 Operator Builtin Code   3 CONV_2D
  Inputs: 319 85 86
  Outputs: 320
Node 185 Operator Builtin Code  43 SQUEEZE
  Inputs: 320
  Outputs: 321
Node 186 Operator Builtin Code   0 ADD
  Inputs: 321 36
  Outputs: 322
Node 187 Operator Builtin Code  18 MUL
  Inputs: 322 72
  Outputs: 323
Node 188 Operator Builtin Code   0 ADD
  Inputs: 323 73
  Outputs: 324
Node 189 Operator Builtin Code  28 TANH
  Inputs: 324
  Outputs: 325
Node 190 Operator Builtin Code  70 EXPAND_DIMS
  Inputs: 325 38
  Outputs: 326
Node 191 Operator Builtin Code   3 CONV_2D
  Inputs: 326 87 86
  Outputs: 327
Node 192 Operator Builtin Code  43 SQUEEZE
  Inputs: 327
  Outputs: 328
Node 193 Operator Builtin Code   0 ADD
  Inputs: 328 37
  Outputs: 329
Node 194 Operator Builtin Code  18 MUL
  Inputs: 329 74
  Outputs: 330
Node 195 Operator Builtin Code   0 ADD
  Inputs: 330 75
  Outputs: 331
Node 196 Operator Builtin Code  77 SHAPE
  Inputs: 331
  Outputs: 332
Node 197 Operator Builtin Code  36 GATHER
  Inputs: 332 42
  Outputs: 333
Node 198 Operator Builtin Code  81 REDUCE_PROD
  Inputs: 333 93
  Outputs: 334
Node 199 Operator Builtin Code   2 CONCATENATION
  Inputs: 333 41
  Outputs: 335
Node 200 Operator Builtin Code  36 GATHER
  Inputs: 332 92
  Outputs: 336
Node 201 Operator Builtin Code  81 REDUCE_PROD
  Inputs: 336 93
 ERROR: tensorflow/lite/kernels/transpose.cc:55 op_context->perm->dims->data[0] != dims (3 != 2)
ERROR: Node number 89 (TRANSPOSE) failed to prepare.

Error at tensorflow/lite/examples/minimal/minimal.cc:505
 Outputs: 337
Node 202 Operator Builtin Code  83 PACK
  Inputs: 334 337
  Outputs: 338
Node 203 Operator Builtin Code  22 RESHAPE
  Inputs: 331 338
  Outputs: 339
Node 204 Operator Builtin Code   9 FULLY_CONNECTED
  Inputs: 339 76 -1
  Outputs: 340
Node 205 Operator Builtin Code  22 RESHAPE
  Inputs: 340 335
  Outputs: 341
Node 206 Operator Builtin Code   0 ADD
  Inputs: 341 40
  Outputs: 342
Node 207 Operator Builtin Code   0 ADD
  Inputs: 281 342
  Outputs: 343
Node 208 Operator Builtin Code  77 SHAPE
  Inputs: 343
  Outputs: 344
Node 209 Operator Builtin Code  45 STRIDED_SLICE
  Inputs: 344 93 92 94
  Outputs: 345
Node 210 Operator Builtin Code  81 REDUCE_PROD
  Inputs: 345 93
  Outputs: 346
Node 211 Operator Builtin Code  83 PACK
  Inputs: 346
  Outputs: 347
Node 212 Operator Builtin Code  45 STRIDED_SLICE
  Inputs: 344 92 93 94
  Outputs: 348
Node 213 Operator Builtin Code   2 CONCATENATION
  Inputs: 347 348
  Outputs: 349
Node 214 Operator Builtin Code  22 RESHAPE
  Inputs: 343 349
  Outputs: 350
Node 215 Operator Builtin Code  36 GATHER
  Inputs: 350 288
  Outputs: 351
Node 216 Operator Builtin Code  70 EXPAND_DIMS
  Inputs: 351 49
  Outputs: 352
Node 217 Operator Builtin Code  22 RESHAPE
  Inputs: 263 239
  Outputs: 353
```

**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43623,Tensorflow 2.3 CUDNN - Detected cudnn out-of-bounds write in convolution buffer!  + GPU Sync Failed,"### System information

-   I was trying to train standard CNN network
-   OS Platform: Windows 10
-   TF installed via Anaconda - standard release
-   TF2.3.1 latest release
-   Python 3.6.12
-   Cuda  10.1.168 (via cudatoolkit) Cnn 7.6.5
-   NVidia GTX 1080.

### Describe the problem
Simple CNN networks fails to train. At the beginning of each epoch the following message is displayed:

This message will be only logged once.
2020-09-28 16:32:44.253794: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.

After few epochs the script simply crash with

Traceback (most recent call last):
  File ""cnn.py"", line 44, in <module>
    history = model.fit(train_images, train_labels, epochs=100, validation_data=(test_images, test_labels))
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1103, in fit
    callbacks.on_train_batch_end(end_step, logs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 440, in on_train_batch_end
    self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 289, in _call_batch_hook
    self._call_batch_end_hook(mode, batch, logs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 309, in _call_batch_end_hook
    self._call_batch_hook_helper(hook_name, batch, logs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 342, in _call_batch_hook_helper
    hook(batch, logs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 961, in on_train_batch_end
    self._batch_update_progbar(batch, logs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 1016, in _batch_update_progbar
    logs = tf_utils.to_numpy_or_python_type(logs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\utils\tf_utils.py"", line 537, in to_numpy_or_python_type
    return nest.map_structure(_to_single_numpy_or_python_type, tensors)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\util\nest.py"", line 635, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\util\nest.py"", line 635, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\utils\tf_utils.py"", line 533, in _to_single_numpy_or_python_type
    x = t.numpy()
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\framework\ops.py"", line 1063, in numpy
    maybe_arr = self._numpy()  # pylint: disable=protected-access
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\framework\ops.py"", line 1031, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: GPU sync failed


### Source code / logs
Source code attached to the issue. 
[cnn.zip](https://github.com/tensorflow/tensorflow/files/5293538/cnn.zip)


Example logs are below:

2020-09-28 16:32:34.965794: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-28 16:32:40.621408: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-09-28 16:32:40.639751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s
2020-09-28 16:32:40.639856: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-28 16:32:40.687455: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-28 16:32:40.723571: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-28 16:32:40.740330: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-28 16:32:40.779951: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-28 16:32:40.809284: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-28 16:32:40.882255: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-28 16:32:40.882629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-28 16:32:40.888697: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-28 16:32:40.914399: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f8da8ad150 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-28 16:32:40.914510: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-28 16:32:40.915321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s
2020-09-28 16:32:40.915404: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-28 16:32:40.915461: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-28 16:32:40.915517: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-28 16:32:40.915592: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-28 16:32:40.915655: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-28 16:32:40.915711: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-28 16:32:40.915766: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-28 16:32:40.915855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-28 16:32:41.807875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-28 16:32:41.807946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0
2020-09-28 16:32:41.808090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N
2020-09-28 16:32:41.809160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6692 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-09-28 16:32:41.813395: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f954359b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-28 16:32:41.813452: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1
1 Physical GPUs, 1 Logical GPUs
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d (Conv2D)              (None, 30, 30, 32)        896
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928
_________________________________________________________________
flatten (Flatten)            (None, 1024)              0
_________________________________________________________________
dense (Dense)                (None, 64)                65600
_________________________________________________________________
dense_1 (Dense)              (None, 10)                650
=================================================================
Total params: 122,570
Trainable params: 122,570
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
2020-09-28 16:32:42.623247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-28 16:32:43.041335: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-28 16:32:44.216588: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation.
Modify $PATH to customize ptxas location.
This message will be only logged once.
2020-09-28 16:32:44.253794: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2020-09-28 16:32:44.253938: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 7993648; expected ffffffffffffffff but was ffffffff.
2020-09-28 16:32:44.283822: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2020-09-28 16:32:44.283952: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 1530416; expected ffffffffffffffff but was 3c6ddc20ffffffff.
2020-09-28 16:32:44.341115: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2020-09-28 16:32:44.341354: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 749232; expected ffffffffffffffff but was 3b28f5e2ffffffff.
2020-09-28 16:32:44.362704: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2020-09-28 16:32:44.362874: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 6120752; expected ffffffffffffffff but was bb0a6dcdffffffff.
2020-09-28 16:32:44.404275: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2020-09-28 16:32:44.404426: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 738712; expected ffffffffffffffff but was 3b0dd497ffffffff.
2020-09-28 16:32:44.424932: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2020-09-28 16:32:44.425087: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x70295c600 at offset 482608; expected ffffffffffffffff but was b96f566dffffffff.
1543/1563 [============================>.] - ETA: 0s - loss: 1.5241 - accuracy: 0.44602020-09-28 16:32:48.198062: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2020-09-28 16:32:48.198215: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in LHS redzone of buffer 0x702400000 at offset 522160; expected ffffffffffffffff but was ffffffff.
2020-09-28 16:32:48.228288: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2020-09-28 16:32:48.228471: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 350000; expected ffffffffffffffff but was 3e26dfe4ffffffff.
2020-09-28 16:32:48.247663: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2020-09-28 16:32:48.247826: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 1634120; expected ffffffffffffffff but was b9b83185ffffffff.
2020-09-28 16:32:48.263505: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2020-09-28 16:32:48.263808: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in LHS redzone of buffer 0x702400000 at offset 808496; expected ffffffffffffffff but was ffffffff.
2020-09-28 16:32:48.298224: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2020-09-28 16:32:48.298378: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 354992; expected ffffffffffffffff but was b9847af9ffffffff.
2020-09-28 16:32:48.323588: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2020-09-28 16:32:48.323747: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 5820336; expected ffffffffffffffff but was 3b0bd7deffffffff.
2020-09-28 16:32:48.340817: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.
2020-09-28 16:32:48.340980: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 1602328; expected ffffffffffffffff but was bb654981ffffffff.
1563/1563 [==============================] - 5s 3ms/step - loss: 1.5212 - accuracy: 0.4470 - val_loss: 1.2762 - val_accuracy: 0.5441
Epoch 2/100
 187/1563 [==>...........................] - ETA: 3s - loss: 1.2447 - accuracy: 0.55582020-09-28 16:32:49.840793: E tensorflow/stream_executor/cuda/cuda_driver.cc:951] could not synchronize on CUDA context: CUDA_ERROR_MISALIGNED_ADDRESS: misaligned address :: 0x00007FFE82613305       tensorflow::CurrentStackTrace
0x00007FFE82361A5E      tensorflow::CostGraphDef_Node::set_is_final
0x00007FFE82509F4E      stream_executor::StreamExecutor::SetDeviceSharedMemoryConfig
0x00007FFE8009ACC6      tensorflow::StepStats::internal_default_instance
0x00007FFE800AC4F4      google::protobuf::RepeatedPtrField<tensorflow::InterconnectLink>::Add
0x00007FFE67F60137      std::vector<tensorflow::DtypeAndPartialTensorShape,std::allocator<tensorflow::DtypeAndPartialTensorShape> >::operator=
0x00007FFE67F3B07B      absl::lts_2020_02_25::Span<tensorflow::Tensor const >::end
0x00007FFE67EB3A8F      TFE_TensorHandleResolve
0x00007FFE67E50E63      TFE_Py_TensorShapeSlice
0x00007FFE67E4E6DA      std::_Tree<std::_Tmap_traits<std::array<std::basic_string<char,std::char_traits<char>,std::allocator<char> >,0>,tensorflow::monitoring::CounterCell,std::less<std::array<std::basic_string<char,std::char_traits<char>,std::allocator<char>
0x0000000070A8C25D      PyCFunction_FastCallDict
0x0000000070A8C7B9      PyObject_GetAttr
0x0000000070A8D1EE      PyEval_EvalFrameDefault
0x0000000070A8C980      PyObject_GetAttr
0x0000000070A8D1EE      PyEval_EvalFrameDefault
0x0000000070A8C980      PyObject_GetAttr
0x0000000070A8D1EE      PyEval_EvalFrameDefault
0x0000000070A8A488      PyObject_IsInstance
0x0000000070A9B2A5      PyBytes_FromStringAndSize
0x0000000070A84BA4      Py_BuildValue
0x0000000070A8DFF7      PyEval_EvalFrameDefault
0x0000000070A8A488      PyObject_IsInstance
0x0000000070A8CC27      PyObject_GetAttr
0x0000000070A8D1EE      PyEval_EvalFrameDefault
0x0000000070A8A488      PyObject_IsInstance
0x0000000070A8CC27      PyObject_GetAttr
0x0000000070A8D1EE      PyEval_EvalFrameDefault
0x0000000070A8C980      PyObject_GetAttr
0x0000000070A8D1EE      PyEval_EvalFrameDefault
0x0000000070A8A488      PyObject_IsInstance
0x0000000070A8CC27      PyObject_GetAttr
0x0000000070A8D1EE      PyEval_EvalFrameDefault
0x0000000070A8A488      PyObject_IsInstance
0x0000000070A8CC27      PyObject_GetAttr
0x0000000070A8D1EE      PyEval_EvalFrameDefault
0x0000000070A8C980      PyObject_GetAttr
0x0000000070A8D1EE      PyEval_EvalFrameDefault
0x0000000070A8C980      PyObject_GetAttr
0x0000000070A8D1EE      PyEval_EvalFrameDefault
0x0000000070A8A488      PyObject_IsInstance
0x0000000070A8CC27      PyObject_GetAttr
0x0000000070A8DE48      PyEval_EvalFrameDefault
0x0000000070A8A488      PyObject_IsInstance
0x0000000070A8CC27      PyObject_GetAttr
0x0000000070A8D1EE      PyEval_EvalFrameDefault
0x0000000070A8A488      PyObject_IsInstance
0x0000000070A9B2A5      PyBytes_FromStringAndSize
0x0000000070A84BA4      Py_BuildValue
0x0000000070A8DFF7      PyEval_EvalFrameDefault
0x0000000070A8A488      PyObject_IsInstance
0x0000000070A8CC27      PyObject_GetAttr
0x0000000070A8DE48      PyEval_EvalFrameDefault
0x0000000070A8A488      PyObject_IsInstance
0x0000000070AA69C7      PyEval_EvalCodeEx
0x0000000070AA6925      PyEval_EvalCode
0x0000000070AA68CF      PyArena_Free
0x0000000070BFE011      PyRun_FileExFlags
0x0000000070BFE83C      PyRun_SimpleFileExFlags
0x0000000070BFDEDF      PyRun_AnyFileExFlags
0x0000000070B4CCEB      Py_hashtable_size
0x0000000070AD9DDD      PyThreadState_UncheckedGet
0x000000001C7B1258      (unknown)
0x00007FFF127F6FD4      BaseThreadInitThunk
0x00007FFF13C1CEC1      RtlUserThreadStart

Traceback (most recent call last):
  File ""cnn.py"", line 44, in <module>
    history = model.fit(train_images, train_labels, epochs=100, validation_data=(test_images, test_labels))
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1103, in fit
    callbacks.on_train_batch_end(end_step, logs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 440, in on_train_batch_end
    self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 289, in _call_batch_hook
    self._call_batch_end_hook(mode, batch, logs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 309, in _call_batch_end_hook
    self._call_batch_hook_helper(hook_name, batch, logs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 342, in _call_batch_hook_helper
    hook(batch, logs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 961, in on_train_batch_end
    self._batch_update_progbar(batch, logs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 1016, in _batch_update_progbar
    logs = tf_utils.to_numpy_or_python_type(logs)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\utils\tf_utils.py"", line 537, in to_numpy_or_python_type
    return nest.map_structure(_to_single_numpy_or_python_type, tensors)
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\util\nest.py"", line 635, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\util\nest.py"", line 635, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\keras\utils\tf_utils.py"", line 533, in _to_single_numpy_or_python_type
    x = t.numpy()
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\framework\ops.py"", line 1063, in numpy
    maybe_arr = self._numpy()  # pylint: disable=protected-access
  File ""C:\Users\Palkos\.conda\envs\TF2.3GPU\lib\site-packages\tensorflow\python\framework\ops.py"", line 1031, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: GPU sync failed"
43622,divide_no_nan logic,"The [divide_no_nan()](https://www.tensorflow.org/api_docs/python/tf/math/divide_no_nan) function by definition ""Computes a safe divide which returns 0 if the y is zero"".

My question is, wouldn't it make more sense to return the maximum value of dtype of the passed tensors? When dividing by very small positive numbers we get very large positive numbers, after all.
It is, of course, possible to apply `clip_by_value()` to the denominator first, and then simply use `divide()`, but that's probably less efficient and straightforward?"
43621,Tensorflow 2.3 doesn't log batch metrics and learning rate in Tensorboard,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14 or Ubuntu 18.04.3 (bug happens on both platforms)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nope
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.3 (bug) / 2.2 (good behavior)
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: None
- GPU model and memory: None

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I used to make custom training loops to fit my models, with a tensorboard callback to log metrics, losses and learning rate using the following command:

```python
    callbacks.append(tf.keras.callbacks.TensorBoard(
        tensorboard_dir,
        profile_batch=0,
        update_freq=2, # write in tensorboard metrics and losses every 2 batches
        write_graph=False)
    )
```

Now, after upgrading tensorflow from 2.2 to 2.3, the same code doesn't log in tensorboard the metrics and losses one every 2 batches, and also the learning rate for each epoch (that I manage either by using `tf.keras.callbacks.LearningRateScheduler`
 or `tf.keras.callbacks.ReduceLROnPlateau`).

I shared a link below to a Colab with a minimal example, and the tensorboard for a model trained in 2.2 and in 2.3.

**Describe the expected behavior**

Same behavior in tf 2.3 as in tf 2.2.

**Standalone code to reproduce the issue**

https://colab.research.google.com/drive/16G5b0wfcG0SuNWrbQbv1pQfPbk6Ix9qU#scrollTo=7tuJE8iW_-NY

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached."
43620,Tensorflow session.run command 20% slower in tf2.3 compared to tf1.15 ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** Yea but an extremely simple one.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Ubuntu 18.04
- **TensorFlow installed from (source or binary):**  Binary
- **TensorFlow version (use command below):** tf1.15.0 and tf2.3.0
- **Python version:** python v3.6.12 and python 3.8
- **CUDA/cuDNN version:** CUDA  v11.0/ cuDNN V9.1.85
- **GPU model and memory:** GPU: NVIDIA Quadro P1000 / Intel UHD Graphics 630 - Memory: 2x Samsung M471A4G43MB1-CTD

**tf_env_collect tool output:**

[tf_env.txt](https://github.com/tensorflow/tensorflow/files/5292801/tf_env.txt)

### Describe the current behaviour

I'm currently converting my tf1.15 scripts to tf2.3 eager execution scripts. I did this according to the [original migration documentation](https://www.tensorflow.org/guide/migrate). After running the automatic conversion script, however, I noticed that the converted `tf2.3` version is considerably slower than the `tf1.15` version. I used to profiler to find the cause of this increased execution time and found out that although the script is exactly the same the `sess.run` method takes longer to execute in `tf2.3`. As can be seen from the results below when `tf2.3` is used the script takes 20% longer to execute. For this simple script, the difference is not much but for my original script which runs for about 1 hour, this difference really starts to add up.

#### Tf1.15 result

**Script execution time:** 125.98906636238098

**Cprofiler file:**

[tf_115_cprof.zip](https://github.com/tensorflow/tensorflow/files/5293014/tf_115.zip)

**Spyder profiler file:**

[tf115.zip](https://github.com/tensorflow/tensorflow/files/5293107/tf115.zip)

#### tf2.3 result

**Script execution time:** 157.48224687576294

**Cprofiler file:**

[tf_23_cprof.zip](https://github.com/tensorflow/tensorflow/files/5293016/tf_23.zip)

**Spyder profiler file**

[tf23.zip](https://github.com/tensorflow/tensorflow/files/5293104/tf23.zip)

#### Spyder profiler difference screenshot

As can be seen from the screenshot below. It looks like the biggest difference can be found in the [flatten_dict_items](https://github.com/tensorflow/tensorflow/blob/02159bbe1f87638bb6cde6a6f4aaa2fc0362e53b/tensorflow/python/util/nest.py#L414) and [FetchHandler.__init__](https://github.com/tensorflow/tensorflow/blob/02159bbe1f87638bb6cde6a6f4aaa2fc0362e53b/tensorflow/python/client/session.py#L474) methods (Left is `tf2.3` and right `tf1.15`).

![image](https://user-images.githubusercontent.com/17570430/94447852-8e653e80-01aa-11eb-86f9-bc4d79d1aadf.png)

When we dive deeper we can see these methods take longer since the [__hash__](https://github.com/tensorflow/tensorflow/blob/02159bbe1f87638bb6cde6a6f4aaa2fc0362e53b/tensorflow/python/framework/ops.py#L826) method takes longer. In the [__hash__](https://github.com/tensorflow/tensorflow/blob/02159bbe1f87638bb6cde6a6f4aaa2fc0362e53b/tensorflow/python/framework/ops.py#L826) method of `tf2.3` python spends a long time executing the [executing_eagerly_outside_functions](https://github.com/tensorflow/tensorflow/blob/02159bbe1f87638bb6cde6a6f4aaa2fc0362e53b/tensorflow/python/framework/ops.py#L5741) method.

![image](https://user-images.githubusercontent.com/17570430/94450330-6e834a00-01ad-11eb-9b54-3c53b7f69db4.png)

These methods take longer since they are executed using the `executing_eagerly_outside_functions`.

### Describe the expected behaviour

I expected the execution time to be equal in both `tf1.15` and `tf2.3`.

### Standalone code to reproduce the issue

The performance difference can be seen when running the following script in both tf1.15 and tf2.3:

```python
""""""Script used to compare the execution time difference when running similar code in
tensorflow 1.15 and tf2.3.
""""""

import time
import os

from packaging import version
import tensorflow as tf

# Script settings
USE_GPU = False
N_STEPS = 1e5

# Disable GPU if requested
if not USE_GPU:  # NOTE: This works in both TF115 and tf2
    # tf.config.set_visible_devices([], ""GPU"")
    if version.parse(tf.__version__) > version.parse(""1.15.4""):
        tf.config.set_visible_devices([], ""GPU"")
    else:
        os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""
    print(""Tensorflow is using CPU"")
else:
    print(""Tensorflow is using GPU"")

# Disable eager execution
tf.compat.v1.disable_eager_execution()

# Create session
sess = tf.compat.v1.Session()

# Setup placeholders
t = 0  # Time
x = tf.compat.v1.placeholder(tf.float32, [None, 1], ""x"")
y = tf.math.sin(x, name=None)

# Run session in loop
print(""===TF115 speed test script==="")
print(f""Running a tf session {int(N_STEPS)} times to test the execution speed."")
print(""Starting sess run loop."")
t1 = time.time()
for i in range(0, int(N_STEPS)):

    # Run result in session
    y_val = sess.run(y, feed_dict={x: [[t]]})

    # Increment time
    t += 1

    # print current step
    if i % 10000 == 0:
        print(f""Performed {i} steps."")

# Print end result
print(""sess run loop stopped."")
print(""Running time: "", time.time() - t1)
```

I also created a simple [test repository](https://github.com/rickstaa/tf23-sess-run-performance-slowdown) that includes this script and a guide on how to run it.


## Other info and logs

**Is this fixed in tf_nightly:** No the script also takes 160 seconds in tf_nightly."
43619,Error converting Tensorflow saved model to TFLite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.6
- TensorFlow installed from (source or binary): pip install tensorflow (command line)
- TensorFlow version (or github SHA if from source): 2.3.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# Copy and paste here the exact command
converter = tf.lite.TFLiteConverter.from_saved_model(""tf_proximity_saved_model"")  # ""tf_proximity_saved_model"" is the name of my model

**The output from the converter invocation**

```
# Copy and paste the output here.
error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
tf.Cast {Truncate = false, device = """"}
Traceback (most recent call last):
File ""/usr/local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 199, in toco_convert_protos
enable_mlir_converter)
File ""/usr/local/lib/python3.6/site-packages/tensorflow/lite/python/wrap_toco.py"", line 38, in wrapped_toco_convert
enable_mlir_converter)
Exception: <unknown>:0: error: loc(fused[""sequential/Cast@__inference__wrapped_model_40145"", ""StatefulPartitionedCall/sequential/Cast""]): 'tf.Cast' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""sequential/Cast@__inference__wrapped_model_40145"", ""StatefulPartitionedCall/sequential/Cast""]): see current operation: %0 = ""tf.Cast""(%arg0) {Truncate = false, device = """"} : (tensor<?x1xf64>) -> tensor<?x1xf32>
<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
tf.Cast {Truncate = false, device = """"}
<unknown>:0: note: see current operation: ""func""() ( {
^bb0(%arg0: tensor<?x1xf64>): // no predecessors
%cst = ""std.constant""() {value = dense<[0.136683047, 0.000000e+00, -1.8840152, 0.000000e+00, 0.000000e+00, 0.000000e+00, 3.257740e-01, 0.000000e+00, 0.000000e+00, 0.000000e+00]> : tensor<10xf32>} : () -> tensor<10xf32>
%cst_0 = ""std.constant""() {value = dense<[0.000000e+00, 1.69775057, 0.000000e+00, 0.000000e+00, -0.638654768, -1.93405676, -1.95157754, -0.0406359173, 1.72332501, -1.7537955]> : tensor<10xf32>} : () -> tensor<10xf32>
%cst_1 = ""std.constant""() {value = dense<1.7260325> : tensor<1xf32>} : () -> tensor<1xf32>
%cst_2 = ""std.constant""() {value = dense<[[0.00675311219], [0.341173232], [-0.481861204], [0.701621353], [0.0275682211], [0.0613476038], [-0.44652465], [0.166085422], [0.189539731], [0.020105958]]> : tensor<10x1xf32>} : () -> tensor<10x1xf32>
%cst_3 = ""std.constant""() {value = dense<[[0.29277724, 0.295953631, 0.205961823, 0.338899374, 0.49851656, -0.437764347, -0.505791903, 0.403120697, 0.0529763699, -0.417837113], [-0.445695817, -0.361287892, 0.101978905, -0.418562263, 0.0107798576, 0.0221389532, 0.220730424, 4.990560e-02, -0.208217949, -0.172240525], [0.0161076784, -0.396935463, -0.530536652, -0.379159153, -0.44315511, 0.213063776, -0.0682218968, -0.221574157, -0.0774391591, -0.472134769], [0.288429618, -0.0110605955, -0.490244329, 0.494588017, 0.330006599, 0.54180479, -0.499053359, 0.0791082978, -0.524112284, -0.0557569563], [-0.546745062, -4.394100e-01, 0.459474474, -0.248545527, 0.0341958404, -0.320446074, -0.368350893, 0.00832664966, -0.0379120708, -0.451237142], [-0.266778737, 0.49375844, -1.604300e-01, -0.315872908, 0.163485587, 0.116762996, 0.266193658, -0.30224967, 0.105117381, -0.534985602], [-0.263971537, -0.0919606983, -0.281746626, -0.310481608, -0.222500026, -0.0813589692, 0.382073581, 0.154834032, -0.285841495, -0.547358394], [0.459437639, 0.0883420109, -0.0378421694, 0.256017208, 0.206906617, -0.461118042, -0.0286277644, -0.377283901, 0.478136897, -0.519345164], [0.513453424, -0.12144509, -0.134186059, -0.39923197, -0.395939708, -0.34776777, 0.461619705, -0.185738802, -0.495007396, -0.359370708], [0.33326897, -0.100557774, 0.382778198, 0.0325809717, 0.231879592, -0.251019359, 0.165992916, -0.542559922, 0.0710098147, -0.00225681067]]> : tensor<10x10xf32>} : () -> tensor<10x10xf32>
%cst_4 = ""std.constant""() {value = dense<[[-6.715610e-01, 0.230374008, -0.0477389693, 0.28339678, -0.580778599, -1.10261536, -0.777723848, -0.451123297, 0.602046907, -0.688027441]]> : tensor<1x10xf32>} : () -> tensor<1x10xf32>
%0 = ""tf.Cast""(%arg0) {Truncate = false, device = """"} : (tensor<?x1xf64>) -> tensor<?x1xf32>
%1 = ""tfl.fully_connected""(%0, %cst_2, %cst) {fused_activation_function = ""RELU"", keep_num_dims = false, weights_format = ""DEFAULT""} : (tensor<?x1xf32>, tensor<10x1xf32>, tensor<10xf32>) -> tensor<?x10xf32>
%2 = ""tfl.fully_connected""(%1, %cst_3, %cst_0) {fused_activation_function = ""RELU"", keep_num_dims = false, weights_format = ""DEFAULT""} : (tensor<?x10xf32>, tensor<10x10xf32>, tensor<10xf32>) -> tensor<?x10xf32>
%3 = ""tfl.fully_connected""(%2, %cst_4, %cst_1) {fused_activation_function = ""NONE"", keep_num_dims = false, weights_format = ""DEFAULT""} : (tensor<?x10xf32>, tensor<1x10xf32>, tensor<1xf32>) -> tensor<?x1xf32>
""std.return""(%3) : (tensor<?x1xf32>) -> ()
}) {sym_name = ""main"", tf.entry_function = {control_outputs = """", inputs = ""dense_input"", outputs = ""Identity""}, type = (tensor<?x1xf64>) -> tensor<?x1xf32>} : () -> ()


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""prox_data_learn.py"", line 156, in <module>
tflite_model = converter.convert()
File ""/usr/local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1076, in convert
return super(TFLiteConverterV2, self).convert()
File ""/usr/local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 900, in convert
self).convert(graph_def, input_tensors, output_tensors)
File ""/usr/local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 633, in convert
**converter_kwargs)
File ""/usr/local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 574, in toco_convert_impl
enable_mlir_converter=enable_mlir_converter)
File ""/usr/local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 202, in toco_convert_protos
raise ConverterError(str(e))
tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(fused[""sequential/Cast@__inference__wrapped_model_40145"", ""StatefulPartitionedCall/sequential/Cast""]): 'tf.Cast' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""sequential/Cast@__inference__wrapped_model_40145"", ""StatefulPartitionedCall/sequential/Cast""]): see current operation: %0 = ""tf.Cast""(%arg0) {Truncate = false, device = """"} : (tensor<?x1xf64>) -> tensor<?x1xf32>
<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
tf.Cast {Truncate = false, device = """"}
<unknown>:0: note: see current operation: ""func""() ( {
^bb0(%arg0: tensor<?x1xf64>): // no predecessors
%cst = ""std.constant""() {value = dense<[0.136683047, 0.000000e+00, -1.8840152, 0.000000e+00, 0.000000e+00, 0.000000e+00, 3.257740e-01, 0.000000e+00, 0.000000e+00, 0.000000e+00]> : tensor<10xf32>} : () -> tensor<10xf32>
%cst_0 = ""std.constant""() {value = dense<[0.000000e+00, 1.69775057, 0.000000e+00, 0.000000e+00, -0.638654768, -1.93405676, -1.95157754, -0.0406359173, 1.72332501, -1.7537955]> : tensor<10xf32>} : () -> tensor<10xf32>
%cst_1 = ""std.constant""() {value = dense<1.7260325> : tensor<1xf32>} : () -> tensor<1xf32>
%cst_2 = ""std.constant""() {value = dense<[[0.00675311219], [0.341173232], [-0.481861204], [0.701621353], [0.0275682211], [0.0613476038], [-0.44652465], [0.166085422], [0.189539731], [0.020105958]]> : tensor<10x1xf32>} : () -> tensor<10x1xf32>
%cst_3 = ""std.constant""() {value = dense<[[0.29277724, 0.295953631, 0.205961823, 0.338899374, 0.49851656, -0.437764347, -0.505791903, 0.403120697, 0.0529763699, -0.417837113], [-0.445695817, -0.361287892, 0.101978905, -0.418562263, 0.0107798576, 0.0221389532, 0.220730424, 4.990560e-02, -0.208217949, -0.172240525], [0.0161076784, -0.396935463, -0.530536652, -0.379159153, -0.44315511, 0.213063776, -0.0682218968, -0.221574157, -0.0774391591, -0.472134769], [0.288429618, -0.0110605955, -0.490244329, 0.494588017, 0.330006599, 0.54180479, -0.499053359, 0.0791082978, -0.524112284, -0.0557569563], [-0.546745062, -4.394100e-01, 0.459474474, -0.248545527, 0.0341958404, -0.320446074, -0.368350893, 0.00832664966, -0.0379120708, -0.451237142], [-0.266778737, 0.49375844, -1.604300e-01, -0.315872908, 0.163485587, 0.116762996, 0.266193658, -0.30224967, 0.105117381, -0.534985602], [-0.263971537, -0.0919606983, -0.281746626, -0.310481608, -0.222500026, -0.0813589692, 0.382073581, 0.154834032, -0.285841495, -0.547358394], [0.459437639, 0.0883420109, -0.0378421694, 0.256017208, 0.206906617, -0.461118042, -0.0286277644, -0.377283901, 0.478136897, -0.519345164], [0.513453424, -0.12144509, -0.134186059, -0.39923197, -0.395939708, -0.34776777, 0.461619705, -0.185738802, -0.495007396, -0.359370708], [0.33326897, -0.100557774, 0.382778198, 0.0325809717, 0.231879592, -0.251019359, 0.165992916, -0.542559922, 0.0710098147, -0.00225681067]]> : tensor<10x10xf32>} : () -> tensor<10x10xf32>
%cst_4 = ""std.constant""() {value = dense<[[-6.715610e-01, 0.230374008, -0.0477389693, 0.28339678, -0.580778599, -1.10261536, -0.777723848, -0.451123297, 0.602046907, -0.688027441]]> : tensor<1x10xf32>} : () -> tensor<1x10xf32>
%0 = ""tf.Cast""(%arg0) {Truncate = false, device = """"} : (tensor<?x1xf64>) -> tensor<?x1xf32>
%1 = ""tfl.fully_connected""(%0, %cst_2, %cst) {fused_activation_function = ""RELU"", keep_num_dims = false, weights_format = ""DEFAULT""} : (tensor<?x1xf32>, tensor<10x1xf32>, tensor<10xf32>) -> tensor<?x10xf32>
%2 = ""tfl.fully_connected""(%1, %cst_3, %cst_0) {fused_activation_function = ""RELU"", keep_num_dims = false, weights_format = ""DEFAULT""} : (tensor<?x10xf32>, tensor<10x10xf32>, tensor<10xf32>) -> tensor<?x10xf32>
%3 = ""tfl.fully_connected""(%2, %cst_4, %cst_1) {fused_activation_function = ""NONE"", keep_num_dims = false, weights_format = ""DEFAULT""} : (tensor<?x10xf32>, tensor<1x10xf32>, tensor<1xf32>) -> tensor<?x1xf32>
""std.return""(%3) : (tensor<?x1xf32>) -> ()
}) {sym_name = ""main"", tf.entry_function = {control_outputs = """", inputs = ""dense_input"", outputs = ""Identity""}, type = (tensor<?x1xf64>) -> tensor<?x1xf32>} : () -> ()

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
https://drive.google.com/file/d/1U59Z_Z4ZH4tGn2dTpvB5tm4t0NujUfTz/view?usp=sharing 

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43616,Compiling Person Detection Model for zephyr_riscv,"Hi,
I was able to compile both the example applications(Hello World and Magic wand) for zephyr_riscv. 

How to compile the other TFLite Models(Person detection and Image Recognition) for zephyr_riscv? What are the changes that need to be made in order to compile it for zephyr_riscv?

Note: **make -f tensorflow/lite/micro/tools/make/Makefile TARGET=zephyr_vexriscv person_detection_bin** I used this command for Person Detection and I am getting the output as: **make: Nothing to be done for 'person_detection_bin'**.

Best Regards,
Darshan"
43615,Metal delegate Crash with C++ interface,"Hello, I am trying to compare performance of TFLite delegates on iOS devices.
This issue is related to [comments](https://github.com/tensorflow/tensorflow/commit/60c4c3e2105afe28fb7a11cb8d440e0caabbf735#commitcomment-42709367) on 60c4c3e.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not really
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy): iPhone 6, iPhone SE
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): r2.3
- Python version: 3.8.3
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): Apple Clang version 12.0.0

**Describe the current behavior**

I build an iOS static framework which includes below code and link it with camera demo application (iOS) to run image classification task.
```cpp
/* Same with tensorflowlite internal usage */
using TfLiteDelegatePtr = std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)>;

/* for Metal delegate */
TFLGpuDelegateOptions gpu_opts = {};
gpu_opts.allow_precision_loss = true;
gpu_opts.enable_quantization = true;
gpu_delegate = TfLiteDelegatePtr(TFLGpuDelegateCreate(&gpu_opts), TFLGpuDelegateDelete);

/* Error occur in `ModifyGraphWithDelegate()` */
if (interpreter_->ModifyGraphWithDelegate(std::move(gpu_delegate)) != kTfLiteOk) {
    LOGE(TAG, ""%s. failed to delegate to GPU, fallback to CPU"", __func__);
    return;
}
```

Then run tflite interpreter with full-integer quantized TFLite model.
- On Xcode, `interpreter_->ModifyGraphWithDelegate()` issues `EXC_BAD_ACCESS (code=1, address=0x11261ca70)` (address varies after runs)
- XNNPack delegate **does not break** on same quantized model.
- GPU (Metal) delegate **does not break** on float16 model.

**Describe the expected behavior**

`interpreter_->ModifyGraphWithDelegate()` function (with Metal delegate on full-integer quantized model) does not crash and at least return an error code. (`!= kTfLiteOK`)

**XCode Logs**
ps. `HyperFast` is the name of the iOS static framework.
To build the framework, I defined custom bazel rule to build static library (`.a`) (includes tflite runtime + xnnpack delegate + metal delegate + coreml delegate) with C++ interfaces.
And then build framework with that static library. So xcode does not know about TFLite source code.

```asm
HyperFast`tflite::delegates::CreateNewTensorWithDifferentType:
    0x1043ff408 <+0>:   stp    x24, x23, [sp, #-0x40]!
    0x1043ff40c <+4>:   stp    x22, x21, [sp, #0x10]
    0x1043ff410 <+8>:   stp    x20, x19, [sp, #0x20]
    0x1043ff414 <+12>:  stp    x29, x30, [sp, #0x30]
    0x1043ff418 <+16>:  add    x29, sp, #0x30            ; =0x30 
    0x1043ff41c <+20>:  mov    x23, x4
    0x1043ff420 <+24>:  mov    x20, x3
    0x1043ff424 <+28>:  mov    x22, x2
    0x1043ff428 <+32>:  mov    x21, x1
    0x1043ff42c <+36>:  mov    x19, x0
    0x1043ff430 <+40>:  ldr    x24, [x0, #0x10]
    0x1043ff434 <+44>:  ldr    x8, [x0, #0x30]
    0x1043ff438 <+48>:  mov    w1, #0x1
    0x1043ff43c <+52>:  mov    x2, x4
    0x1043ff440 <+56>:  blr    x8
    0x1043ff444 <+60>:  cbnz   w0, 0x1043ff4dc           ; <+212>
    0x1043ff448 <+64>:  ldr    x8, [x19, #0x10]
    0x1043ff44c <+68>:  ldrsw  x9, [x23]
    0x1043ff450 <+72>:  mov    w10, #0x70
    0x1043ff454 <+76>:  madd   x8, x9, x10, x8
    0x1043ff458 <+80>:  str    x8, [x20]
    0x1043ff45c <+84>:  str    w22, [x8]
    0x1043ff460 <+88>:  mov    w9, #0x2
    0x1043ff464 <+92>:  str    w9, [x8, #0x20]
    0x1043ff468 <+96>:  smaddl x8, w21, w10, x24
->  0x1043ff46c <+100>: ldr    x21, [x8, #0x10]
    0x1043ff470 <+104>: ldr    w0, [x21]
    0x1043ff474 <+108>: bl     0x1044100fc               ; TfLiteIntArrayCreate
    0x1043ff478 <+112>: mov    x2, x0
    0x1043ff47c <+116>: ldr    w8, [x21]
    0x1043ff480 <+120>: cmp    w8, #0x1                  ; =0x1 
    0x1043ff484 <+124>: b.lt   0x1043ff4b0               ; <+168>
    0x1043ff488 <+128>: mov    x8, #0x0
    0x1043ff48c <+132>: add    x9, x2, #0x4              ; =0x4 
    0x1043ff490 <+136>: add    x10, x21, #0x4            ; =0x4 
    0x1043ff494 <+140>: lsl    x11, x8, #2
    0x1043ff498 <+144>: ldr    w12, [x10, x11]
    0x1043ff49c <+148>: str    w12, [x9, x11]
    0x1043ff4a0 <+152>: add    x8, x8, #0x1              ; =0x1 
    0x1043ff4a4 <+156>: ldrsw  x11, [x21]
    0x1043ff4a8 <+160>: cmp    x8, x11
    0x1043ff4ac <+164>: b.lt   0x1043ff494               ; <+140>
    0x1043ff4b0 <+168>: ldr    x8, [x19, #0x20]
    0x1043ff4b4 <+172>: ldr    x1, [x20]
    0x1043ff4b8 <+176>: mov    x0, x19
    0x1043ff4bc <+180>: blr    x8
    0x1043ff4c0 <+184>: cbz    w0, 0x1043ff4dc           ; <+212>
    0x1043ff4c4 <+188>: ldr    x8, [x19, #0x28]
    0x1043ff4c8 <+192>: adr    x1, #0xa58e9              ; ""Could not resize new delegate tensor""
    0x1043ff4cc <+196>: nop    
    0x1043ff4d0 <+200>: mov    x0, x19
    0x1043ff4d4 <+204>: blr    x8
    0x1043ff4d8 <+208>: mov    w0, #0x1
    0x1043ff4dc <+212>: ldp    x29, x30, [sp, #0x30]
    0x1043ff4e0 <+216>: ldp    x20, x19, [sp, #0x20]
    0x1043ff4e4 <+220>: ldp    x22, x21, [sp, #0x10]
    0x1043ff4e8 <+224>: ldp    x24, x23, [sp], #0x40
    0x1043ff4ec <+228>: ret    
```

```asm
HyperFast`tflite::impl::Interpreter::ModifyGraphWithDelegate:
    0x1043f1a6c <+0>:   stp    x24, x23, [sp, #-0x40]!
    0x1043f1a70 <+4>:   stp    x22, x21, [sp, #0x10]
    0x1043f1a74 <+8>:   stp    x20, x19, [sp, #0x20]
    0x1043f1a78 <+12>:  stp    x29, x30, [sp, #0x30]
    0x1043f1a7c <+16>:  add    x29, sp, #0x30            ; =0x30 
    0x1043f1a80 <+20>:  mov    x20, x1
    0x1043f1a84 <+24>:  mov    x19, x0
    0x1043f1a88 <+28>:  ldp    x8, x9, [x0, #0x18]
    0x1043f1a8c <+32>:  cmp    x8, x9
    0x1043f1a90 <+36>:  b.hs   0x1043f1ab4               ; <+72>
    0x1043f1a94 <+40>:  ldr    x9, [x20]
    0x1043f1a98 <+44>:  str    xzr, [x20]
    0x1043f1a9c <+48>:  str    x9, [x8]
    0x1043f1aa0 <+52>:  ldr    x9, [x20, #0x8]
    0x1043f1aa4 <+56>:  str    x9, [x8, #0x8]
    0x1043f1aa8 <+60>:  add    x8, x8, #0x10             ; =0x10 
    0x1043f1aac <+64>:  str    x8, [x19, #0x18]
    0x1043f1ab0 <+68>:  b      0x1043f1bb4               ; <+328>
    0x1043f1ab4 <+72>:  add    x0, x19, #0x10            ; =0x10 
    0x1043f1ab8 <+76>:  ldr    x10, [x0]
    0x1043f1abc <+80>:  sub    x8, x8, x10
    0x1043f1ac0 <+84>:  asr    x21, x8, #4
    0x1043f1ac4 <+88>:  add    x8, x21, #0x1             ; =0x1 
    0x1043f1ac8 <+92>:  lsr    x11, x8, #60
    0x1043f1acc <+96>:  cbnz   x11, 0x1043f1c34          ; <+456>
    0x1043f1ad0 <+100>: sub    x9, x9, x10
    0x1043f1ad4 <+104>: asr    x10, x9, #3
    0x1043f1ad8 <+108>: cmp    x10, x8
    0x1043f1adc <+112>: csel   x8, x8, x10, lo
    0x1043f1ae0 <+116>: mov    x10, #0x7ffffffffffffff
    0x1043f1ae4 <+120>: cmp    x10, x9, asr #4
    0x1043f1ae8 <+124>: mov    x9, #0xfffffffffffffff
    0x1043f1aec <+128>: csel   x22, x8, x9, hi
    0x1043f1af0 <+132>: cbz    x22, 0x1043f1b08          ; <+156>
    0x1043f1af4 <+136>: lsr    x8, x22, #60
    0x1043f1af8 <+140>: cbnz   x8, 0x1043f1c38           ; <+460>
    0x1043f1afc <+144>: lsl    x0, x22, #4
    0x1043f1b00 <+148>: bl     0x10444f960               ; symbol stub for: operator new(unsigned long)
    0x1043f1b04 <+152>: b      0x1043f1b0c               ; <+160>
    0x1043f1b08 <+156>: mov    x0, #0x0
    0x1043f1b0c <+160>: add    x10, x0, x21, lsl #4
    0x1043f1b10 <+164>: add    x9, x0, x22, lsl #4
    0x1043f1b14 <+168>: ldr    q0, [x20]
    0x1043f1b18 <+172>: str    xzr, [x20]
    0x1043f1b1c <+176>: mov    x11, x10
    0x1043f1b20 <+180>: str    q0, [x11], #0x10
    0x1043f1b24 <+184>: ldp    x8, x12, [x19, #0x10]
    0x1043f1b28 <+188>: cmp    x12, x8
    0x1043f1b2c <+192>: b.eq   0x1043f1b68               ; <+252>
    0x1043f1b30 <+196>: ldr    x13, [x12, #-0x10]!
    0x1043f1b34 <+200>: str    xzr, [x12]
    0x1043f1b38 <+204>: stur   x13, [x10, #-0x10]
    0x1043f1b3c <+208>: ldr    x13, [x12, #0x8]
    0x1043f1b40 <+212>: stur   x13, [x10, #-0x8]
    0x1043f1b44 <+216>: sub    x10, x10, #0x10           ; =0x10 
    0x1043f1b48 <+220>: cmp    x8, x12
    0x1043f1b4c <+224>: b.ne   0x1043f1b30               ; <+196>
    0x1043f1b50 <+228>: ldp    x20, x8, [x19, #0x10]
    0x1043f1b54 <+232>: stp    x10, x11, [x19, #0x10]
    0x1043f1b58 <+236>: str    x9, [x19, #0x20]
    0x1043f1b5c <+240>: cmp    x8, x20
    0x1043f1b60 <+244>: b.ne   0x1043f1b7c               ; <+272>
    0x1043f1b64 <+248>: b      0x1043f1ba8               ; <+316>
    0x1043f1b68 <+252>: mov    x20, x8
    0x1043f1b6c <+256>: stp    x10, x11, [x19, #0x10]
    0x1043f1b70 <+260>: str    x9, [x19, #0x20]
    0x1043f1b74 <+264>: cmp    x8, x20
    0x1043f1b78 <+268>: b.eq   0x1043f1ba8               ; <+316>
    0x1043f1b7c <+272>: mov    x21, x8
    0x1043f1b80 <+276>: b      0x1043f1b90               ; <+292>
    0x1043f1b84 <+280>: mov    x8, x21
    0x1043f1b88 <+284>: cmp    x20, x21
    0x1043f1b8c <+288>: b.eq   0x1043f1ba8               ; <+316>
    0x1043f1b90 <+292>: ldr    x0, [x21, #-0x10]!
    0x1043f1b94 <+296>: str    xzr, [x21]
    0x1043f1b98 <+300>: cbz    x0, 0x1043f1b84           ; <+280>
    0x1043f1b9c <+304>: ldur   x8, [x8, #-0x8]
    0x1043f1ba0 <+308>: blr    x8
    0x1043f1ba4 <+312>: b      0x1043f1b84               ; <+280>
    0x1043f1ba8 <+316>: cbz    x20, 0x1043f1bb4          ; <+328>
    0x1043f1bac <+320>: mov    x0, x20
    0x1043f1bb0 <+324>: bl     0x10444f948               ; symbol stub for: operator delete(void*)
    0x1043f1bb4 <+328>: ldp    x21, x8, [x19, #0x68]
    0x1043f1bb8 <+332>: cmp    x21, x8
    0x1043f1bbc <+336>: b.eq   0x1043f1c1c               ; <+432>
    0x1043f1bc0 <+340>: ldr    x9, [x19, #0x18]
    0x1043f1bc4 <+344>: ldur   x20, [x9, #-0x10]
    0x1043f1bc8 <+348>: sub    x22, x8, #0x8             ; =0x8 
    0x1043f1bcc <+352>: mov    x23, x21
    0x1043f1bd0 <+356>: ldr    x0, [x23], #0x8
    0x1043f1bd4 <+360>: mov    x1, x20
    0x1043f1bd8 <+364>: bl     0x1043ef2d0               ; tflite::impl::Subgraph::ModifyGraphWithDelegate(TfLiteDelegate*)
->  0x1043f1bdc <+368>: cmp    x22, x21
    0x1043f1be0 <+372>: b.eq   0x1043f1bec               ; <+384>
    0x1043f1be4 <+376>: mov    x21, x23
    0x1043f1be8 <+380>: cbz    w0, 0x1043f1bd0           ; <+356>
    0x1043f1bec <+384>: cmp    w0, #0x2                  ; =0x2 
    0x1043f1bf0 <+388>: b.ne   0x1043f1c20               ; <+436>
    0x1043f1bf4 <+392>: ldp    x20, x19, [x19, #0x68]
    0x1043f1bf8 <+396>: cmp    x20, x19
    0x1043f1bfc <+400>: b.eq   0x1043f1c14               ; <+424>
    0x1043f1c00 <+404>: ldr    x0, [x20], #0x8
    0x1043f1c04 <+408>: bl     0x1043f01c8               ; tflite::impl::Subgraph::RemoveAllDelegates()
    0x1043f1c08 <+412>: cbnz   w0, 0x1043f1c20           ; <+436>
    0x1043f1c0c <+416>: cmp    x19, x20
    0x1043f1c10 <+420>: b.ne   0x1043f1c00               ; <+404>
    0x1043f1c14 <+424>: mov    w0, #0x2
    0x1043f1c18 <+428>: b      0x1043f1c20               ; <+436>
    0x1043f1c1c <+432>: mov    w0, #0x0
    0x1043f1c20 <+436>: ldp    x29, x30, [sp, #0x30]
    0x1043f1c24 <+440>: ldp    x20, x19, [sp, #0x20]
    0x1043f1c28 <+444>: ldp    x22, x21, [sp, #0x10]
    0x1043f1c2c <+448>: ldp    x24, x23, [sp], #0x40
    0x1043f1c30 <+452>: ret    
    0x1043f1c34 <+456>: bl     0x10444f498               ; symbol stub for: std::__1::__vector_base_common<true>::__throw_length_error() const
    0x1043f1c38 <+460>: bl     0x1043f2a0c               ; std::__1::__throw_length_error(char const*)
```
"
43614,Failed to load the native TensorFlow runtime.,"System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): window 10
![Capture1](https://user-images.githubusercontent.com/63404689/94395515-8642da00-017d-11eb-984a-0a23d1c0b19c.JPG)
![Capture2](https://user-images.githubusercontent.com/63404689/94395523-880c9d80-017d-11eb-82e2-9dfbad52761c.JPG)

- TensorFlow installed from (source or binary): jupyter notebook
- TensorFlow version: 2.3.1
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



Unable to import keras and tensorflow in jupyter notebook

Installing tensorflow through pip install tensorflow but unable to install.


"
43612,from tensorflow.contrib.seq2seq import Helper,"HI.

I am developing a system but in Tensorflow 2 don't exist this:

from tensorflow.contrib.seq2seq import Helper

How can migrate to a similar solution in Tensorflow 2? Thank's a lot"
43609,The format of the table is wrong.,"## URL(s) with the issue:

https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/PPOAgent#

## Description of issue (what needs changing):
Table format is wrong. HTML code like table, tr, td etc is visible.

### Clear description
Table format is wrong. HTML code like table, tr, td etc is visible in the 2nd half of the page This makes it hard to read.

### Parameters defined

Are all parameters defined and formatted correctly?
No"
43608,Debugger V2 not working.  Invalid argument:  DebugNumericSummaryV2Op requires tensor_id to be less than or equal to (2^53). Given tensor_id:26,"### System information

-  I have used the test example from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/examples/v2/debug_mnist_v2.py)
-   OS: Windows 10
-   Tensorflow 2.3.1 (installed with pip):
-   Python 3.6
-   CUDA 10.1
-   nVidia GeForce GTX 1050

I cannot make the example work with Debugger V2.

By executing the example from the link above I get the following output:

```
D:\src\ai\visualthing\venv\Scripts\python.exe ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\pydevd.py"" --multiproc --qt-support=auto --client 127.0.0.1 --port 50790 --file D:/src/ai/visualthing/debug_mnist_v2.py --dump_dir /tmp/tfdbg2_logdir --dump_tensor_debug_mode FULL_HEALTH

pydev debugger: process 8484 is connecting

Connected to pydev debugger (build 192.5728.105)
2020-09-27 20:31:08.451881: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
INFO:tensorflow:Enabled dumping callback in thread MainThread (dump root: /tmp/tfdbg2_logdir, tensor debug mode: FULL_HEALTH)
I0927 20:31:11.284601  1260 dumping_callback.py:871] Enabled dumping callback in thread MainThread (dump root: /tmp/tfdbg2_logdir, tensor debug mode: FULL_HEALTH)
2020-09-27 20:31:11.557685: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-09-27 20:31:11.584474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1
coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2020-09-27 20:31:11.584652: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-27 20:31:11.588047: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-27 20:31:11.591169: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-27 20:31:11.592204: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-27 20:31:11.595773: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-27 20:31:11.597733: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-27 20:31:11.605092: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-27 20:31:11.605244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-27 20:31:11.605644: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-27 20:31:11.614513: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f4c545b410 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-27 20:31:11.614778: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-27 20:31:11.615119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1
coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2020-09-27 20:31:11.615425: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-27 20:31:11.615585: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-27 20:31:11.615691: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-27 20:31:11.615830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-27 20:31:11.615921: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-27 20:31:11.616011: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-27 20:31:11.616099: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-27 20:31:11.616214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-27 20:31:12.188255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-27 20:31:12.188425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-27 20:31:12.188484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-27 20:31:12.188686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2987 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-09-27 20:31:12.191306: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f4e366a9f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-27 20:31:12.191431: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050, Compute Capability 6.1
2020-09-27 20:31:13.537229: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
Traceback (most recent call last):
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\pydevd.py"", line 2060, in <module>
    main()
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\pydevd.py"", line 2054, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\pydevd.py"", line 1405, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\pydevd.py"", line 1412, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""D:/src/ai/visualthing/debug_mnist_v2.py"", line 238, in <module>
    absl.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""D:\src\ai\visualthing\venv\lib\site-packages\absl\app.py"", line 299, in run
    _run_main(main, args)
  File ""D:\src\ai\visualthing\venv\lib\site-packages\absl\app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""D:/src/ai/visualthing/debug_mnist_v2.py"", line 223, in main
    y = model(x_train)
  File ""D:\src\ai\visualthing\venv\lib\site-packages\tensorflow\python\eager\def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""D:\src\ai\visualthing\venv\lib\site-packages\tensorflow\python\eager\def_function.py"", line 846, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""D:\src\ai\visualthing\venv\lib\site-packages\tensorflow\python\eager\function.py"", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File ""D:\src\ai\visualthing\venv\lib\site-packages\tensorflow\python\eager\function.py"", line 1933, in _call_flat
    cancellation_manager=cancellation_manager)
  File ""D:\src\ai\visualthing\venv\lib\site-packages\tensorflow\python\eager\function.py"", line 550, in call
    ctx=ctx)
  File ""D:\src\ai\visualthing\venv\lib\site-packages\tensorflow\python\eager\execute.py"", line 138, in execute_with_callbacks
    tensors = quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
  File ""D:\src\ai\visualthing\venv\lib\site-packages\tensorflow\python\eager\execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  DebugNumericSummaryV2Op requires tensor_id to be less than or equal to (2^53). Given tensor_id:26
	 [[{{node StatefulPartitionedCall/MatMul/ReadVariableOp/DebugNumericSummaryV2}}]]
	 [[x/_1]]
  (1) Invalid argument:  DebugNumericSummaryV2Op requires tensor_id to be less than or equal to (2^53). Given tensor_id:26
	 [[{{node StatefulPartitionedCall/MatMul/ReadVariableOp/DebugNumericSummaryV2}}]]
0 successful operations.
0 derived errors ignored. [Op:__forward_model_324]

Function call stack:
model -> model

INFO:tensorflow:Disabled dumping callback in thread MainThread (dump root: /tmp/tfdbg2_logdir)
I0927 20:31:55.200698  1260 dumping_callback.py:895] Disabled dumping callback in thread MainThread (dump root: /tmp/tfdbg2_logdir)

Process finished with exit code 1
```

I have also tried to build my own example with no success, same error: 
`DebugNumericSummaryV2Op requires tensor_id to be less than or equal to (2^53)`



"
43607,Example of inferencing a Tensorflow lite model with parsing_serving_input_receiver_fn using C++ API,"I have followed the Tensorflow2 documentation to convert my trained  tf.estimator model to tflite model; in order to convert my model, first I had to save my model in saved_model format with a input_receiver_fn and then convert it with SELECT_OPS flag:

```
classifier = tf.estimator.LinearClassifier(n_classes=2, model_dir = classifier_dir, feature_columns=features)
classifier.train(input_fn = lambda: trian_fn(features = train_datas, labels = trian_labels))

serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(tf.feature_column.make_parse_example_spec(features))

classifier.export_saved_model(classifier_dir+""\saved_model"", serving_input_fn)

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir = saved_model_dir , signature_keys=['serving_default']) 
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```
I wanted to run my tflite model on an ARM device without python support so I built the C++ interpreter shared libs with Bazel as it is explained in the documentation :
 [Cross-compile for armhf with Bazel](https://www.tensorflow.org/lite/guide/build_rpi#cross-compile_for_armhf_with_bazel)
[Select TensorFlow operators C++
](https://www.tensorflow.org/lite/guide/ops_select#c)

My model has 3 input features but when I try to use the following [guide](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_c) for inferencing I get a segmentation fault.

I used the following code to extract my model details:
```
interpreter = tf.lite.Interpreter(model_path=""./model.tflite"")
interpreter.allocate_tensors()
print(""all ok"")
# Print input shape and type
inputs = interpreter.get_input_details()
print('{} input(s):'.format(len(inputs)))
for i in range(0, len(inputs)):
    print('{} {}'.format(inputs[i]['shape'], inputs[i]['dtype']))

# Print output shape and type
outputs = interpreter.get_output_details()
print('\n{} output(s):'.format(len(outputs)))
for i in range(0, len(outputs)):
    print('{} {}'.format(outputs[i]['shape'], outputs[i]['dtype']))
```
I got the following output:

```
all ok
1 input(s):
[1] <class 'numpy.bytes_'>

2 output(s):
[1 2] <class 'numpy.bytes_'>
[1 2] <class 'numpy.float32'>
```
first few lines of the output of   `tflite::PrintInterpreterState(interpreter.get())`  are:
```
INFO: Created TensorFlow Lite delegate for select TF ops.
INFO: TfLiteFlexDelegate delegate: 1 nodes delegated out of 25 nodes with 1 partitions.

Interpreter has 54 tensors and 26 nodes
Inputs: 0
Outputs: 38 34

Tensor   0 input_example_tensor kTfLiteString  kTfLiteDynamic          0 bytes ( 0.0 MB)  1
```

The output illustrates that the input shape is not the same as the original model, also the input type is <class 'numpy.bytes_'> but the Tensorflow 2 model inputs are [numpy.float32, numpy.float32, numpy.float32].
my input dictionary for prediction in TF2 model is something like : {'feature0' : data0, 'feature1' : data1, 'feature2' : data2}
[here is the Google Colab link to the project](https://colab.research.google.com/drive/1fkj8zM2FM-xd6cajWkStcasmzliZWF9s?usp=sharing)

I tried to fill the input buffer with a vector of zeros but it was without success. Here is my C++ code to load a tflite model and feed it inputs for prediction. can someone please point me to the right direction since I could not find any examples or related documentation for feeding inputs to converted tf.estimator with a serving_input_fn.


```
#include <cstdio>
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/optional_debug_tools.h""

int main()
{
  // Load model
      std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""model.tflite"");
      
  // Build the interpreter with the InterpreterBuilder.
  tflite::ops::builtin::BuiltinOpResolver resolver;
  tflite::InterpreterBuilder builder(*model, resolver);
  std::unique_ptr<tflite::Interpreter> interpreter;
  builder(&interpreter);
  tflite::PrintInterpreterState(interpreter.get());
  
  // Allocate tensor buffers.
  interpreter->AllocateTensors();
  printf(""=== Pre-invoke Interpreter State ===\n"");
  tflite::PrintInterpreterState(interpreter.get());

  // Fill input buffers
  std::vector<float> tensor(3, 0);	//Vector of zeros
  int input = interpreter->inputs()[0];
  float* input_data_ptr = interpreter->typed_input_tensor<float>(input);
  for(int i = 0; i < 3; ++i)
  {
  	*(input_data_ptr) = (float)tensor[i];
  	input_data_ptr++;
  }
  // Run inference
  interpreter->Invoke();
  printf(""\n\n=== Post-invoke Interpreter State ===\n"");
  
  return 0;
}
```
"
43606,Beginner tutorial fails with internal error,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

My pc runs windows 10 pro
Installed anaconda 3 individual edition, Python 3.8
Installed tensorflow 2.3.1 using pip install tensorflow

Bug appears running the very first beginner tutorial  beginner.ipynb.
An exception is raised while trying to create the model object in the third tutorial's command. Below a copy of the console putput:

Python 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 7.16.1 -- An enhanced Interactive Python.

import tensorflow as tf

2020-09-27 11:38:37.267183: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])

2020-09-27 11:38:37.267183: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-27 11:39:35.773790: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
Traceback (most recent call last):

  File ""<ipython-input-3-9eaab81d20e2>"", line 1, in <module>
    model = tf.keras.models.Sequential([

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\training\tracking\base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\keras\engine\sequential.py"", line 116, in __init__
    super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\training\tracking\base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 308, in __init__
    self._init_batch_counters()

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\training\tracking\base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 317, in _init_batch_counters
    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\ops\variables.py"", line 262, in __call__
    return cls._variable_v2_call(*args, **kwargs)

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\ops\variables.py"", line 244, in _variable_v2_call
    return previous_getter(

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\ops\variables.py"", line 237, in <lambda>
    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 2633, in default_variable_creator_v2
    return resource_variable_ops.ResourceVariable(

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\ops\variables.py"", line 264, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 1507, in __init__
    self._init_from_args(

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 1661, in _init_from_args
    handle = eager_safe_variable_handle(

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 242, in eager_safe_variable_handle
    return _variable_handle_from_shape_and_dtype(

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 174, in _variable_handle_from_shape_and_dtype
    gen_logging_ops._assert(  # pylint: disable=protected-access

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\ops\gen_logging_ops.py"", line 49, in _assert
    _ops.raise_from_not_ok_status(e, name)

  File ""C:\Users\nmb31\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 6843, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)

  File ""<string>"", line 3, in raise_from

InvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse


2020-09-27 11:38:37.267183: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-27 11:39:35.773790: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-09-27 11:39:36.282700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0
coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 37.33GiB/s
2020-09-27 11:39:36.282745: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-27 11:39:36.288963: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-27 11:39:36.292338: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-27 11:39:36.293694: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-27 11:39:36.298451: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-27 11:39:36.301791: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-27 11:39:36.311463: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-27 11:39:36.312165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-27 11:39:36.312794: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-27 11:39:36.320212: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1ed9b2f1ee0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-27 11:39:36.320264: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-27 11:39:36.321043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0
coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 37.33GiB/s
2020-09-27 11:39:36.321082: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-27 11:39:36.321094: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-27 11:39:36.321102: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-27 11:39:36.321110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-27 11:39:36.321117: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-27 11:39:36.321125: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-27 11:39:36.321132: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-27 11:39:36.321849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-27 11:39:36.379494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-27 11:39:36.379521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-27 11:39:36.379530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-27 11:39:36.380229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3128 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)
2020-09-27 11:39:36.383452: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1ed9e39a4d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-27 11:39:36.383481: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce 940MX, Compute Capability 5.0

"
43605,TF2.3 load subclassing model within tf.feature_column layer get ValueError: Could not find matching function to call loaded from the SavedModel,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: Python 3.7.4
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory: None

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I am trying to create a custom classification model using Tensorflow2.3 through tf.keras.Model subclassing method, in the subclass model init function, i use tf.feature_column layer to precess features. going through all of above part, i can train, save and reload the Saved_model, but when i use the reload model to do inference, i get the following error:
```
ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (3 total):
    * {'age': [35], 'education': ['Bachelors']}
    * False
    * None
  Keyword arguments: {}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (3 total):
    * {'education': TensorSpec(shape=(None, 1), dtype=tf.string, name='education'), 'age': TensorSpec(shape=(None, 1), dtype=tf.int64, name='age')}
    * True
    * None
  Keyword arguments: {}

Option 2:
  Positional arguments (3 total):
    * {'age': TensorSpec(shape=(None, 1), dtype=tf.int64, name='age'), 'education': TensorSpec(shape=(None, 1), dtype=tf.string, name='education')}
    * False
    * None
  Keyword arguments: {}

Option 3:
  Positional arguments (3 total):
    * {'age': TensorSpec(shape=(None, 1), dtype=tf.int64, name='inputs/age'), 'education': TensorSpec(shape=(None, 1), dtype=tf.string, name='inputs/education')}
    * False
    * None
  Keyword arguments: {}

Option 4:
  Positional arguments (3 total):
    * {'education': TensorSpec(shape=(None, 1), dtype=tf.string, name='inputs/education'), 'age': TensorSpec(shape=(None, 1), dtype=tf.int64, name='inputs/age')}
    * True
    * None
  Keyword arguments: {}
```
When I try to create model with tf.Keras.sequential class or without tf.feature_column layer, every thing works fine, so how can I use the reloaded tf.Keras.subclassing model within tf.feature_column layer to do inference？
This puzzled me for days.

**Describe the expected behavior**

 tf.Keras.subclassing model within tf.feature_column layer can do inference like sequential model.


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Jupyer minimum demo:
**https://www.kaggle.com/hailinfufu/notebookadf7121b80**

Here is a minimal demo to reproduce my problem:

```py
import pathlib
import time

import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split

__SELECT_COLUMN_NAMES = ['age', 'education', 'income_bracket']


def get_train_test_pandas_data():
    # data can be download from: https://www.kaggle.com/uciml/adult-census-income?select=adult.csv
    census = pd.read_csv(""adult_data.csv"")

    census['income_bracket'] = census['income_bracket'].apply(lambda label: 0 if label == ' <=50K' else 1)
    census = census[__SELECT_COLUMN_NAMES]

    y_labels = census.pop('income_bracket')
    x_data = census

    x_train, x_test, y_train, y_test = train_test_split(x_data, y_labels, test_size=0.3)

    return x_train, x_test, y_train, y_test


def get_feature_columns():
    age = tf.feature_column.numeric_column(""age"", dtype=tf.int64)
    education = tf.feature_column.embedding_column(
        tf.feature_column.categorical_column_with_hash_bucket(""education"", hash_bucket_size=1000),
        dimension=100)

    feat_cols = [age, education]

    return feat_cols


if (tf.__version__ < '2.0'):
    tf.enable_eager_execution()

x_train, _, y_train, _ = get_train_test_pandas_data()

dataset = tf.data.Dataset.from_tensor_slices((dict(x_train), y_train))

dataset = dataset.shuffle(len(x_train)).batch(4)

feat_cols = get_feature_columns()


class mymodel(tf.keras.Model):
    def __init__(self):
        super(mymodel, self).__init__()
        self.layer1 = tf.keras.layers.DenseFeatures(feature_columns=feat_cols)
        self.layer2 = tf.keras.layers.Dense(10, activation='relu')
        self.layer3 = tf.keras.layers.Dense(10, activation='relu')
        self.layer4 = tf.keras.layers.Dense(1, activation='sigmoid')

    @tf.function
    def call(self, inputs, training=None, mask=None):
        x = self.layer1(inputs)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        return x


model = mymodel()

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(dataset, epochs=1)


__SAVED_MODEL_DIR = './saved_models/census_keras/{}'.format(int(time.time()))
pathlib.Path(__SAVED_MODEL_DIR).mkdir(parents=True, exist_ok=True)

tf.saved_model.save(model, export_dir=__SAVED_MODEL_DIR)
```

you can replace model = mymodel() with

```
model = tf.keras.Sequential([
    tf.keras.layers.DenseFeatures(feature_columns=feat_cols),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```
that will work fine.

After trained and saved the model, i try to load the SavedModel to do predict:


```py
import tensorflow as tf

# loaded_model = tf.keras.models.load_model(""./saved_models/census_keras/1601196783"")  # tf.saved_model.load(""saved/1"")
loaded_model = tf.keras.models.load_model(""./saved_models/census_keras/1601196783"")

y_pred = loaded_model.call({""age"": [35],
                            ""education"": [""Bachelors""]})
print(y_pred)


y_pred = loaded_model.call({""age"": [40],
                            ""education"": [""Assoc-voc""]})
print(y_pred)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43604,tfio.audio.AudioIOTensor doesn't prepare data in time in loop.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No mobile device used.
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CPU
- GPU model and memory: CPU


**Describe the current behavior**
Training loop crashes. It seems ```tfio.audio.AudioIOTensor``` doesn't prepare data in time when processing. It lazy loads and supposed to prepare data when called.

**Describe the expected behavior**
```tfio.audio.AudioIOTensor``` prepares audio data and process in time.

**Standalone code to reproduce the issue**


dataloader.py
```py
import os

import tensorflow as tf
import tensorflow_io as tfio


duration = 5
rate = 20000
samples = duration * rate


def process_paths(file_paths):
  """"""receive a batch of filepaths and return a batch of db mel-spectrogram and batch of labels
  params
  filepaths: string[]
  
  return
  (db_mel_spectrograms: Tensor<tf.float32>, labels: Tensor<tf.string>)
  """"""
  db_mel_spectrograms, labels = tf.map_fn(
    fn=process_path,
    elems=file_paths,
    fn_output_signature=(tf.float32, tf.string)
  )
  return db_mel_spectrograms, labels

def process_path(file_path):
  label = tf.strings.split(file_path, os.sep)[-2]
  audio = tfio.audio.AudioIOTensor(file_path, dtype=tf.float32)
  re_audio = tfio.audio.resample(
    audio.to_tensor(),
    rate_in=tf.cast(audio.rate, tf.int64),
    rate_out=rate
  )
  re_audio_1c = tf.reduce_mean(re_audio, axis=-1)  # [samples, channels] => [samples]
  zeros = tf.math.maximum(samples - tf.shape(re_audio_1c)[0], 0)
  paddings = [[zeros // 2, zeros // 2 + zeros % 2]]
  pad_audio = tf.pad(re_audio_1c, paddings=paddings, mode='CONSTANT')  # pad if audio is too short
  cropped_audio = tf.image.random_crop(pad_audio, [samples])

  return cropped_audio, label

def get_data(glob_path, batch_size):
  """"""
  get optimized tf.data.Dataset. Use like
  data = get_data(path, 128)
  epochs = 10
  for epoch in range(epochs):
    for spectrograms, labels in data:
      train_this_batch(spectrograms, labels)
      
  Args:
    glob_path: string like /path/to/data/*.mp3
    batch_size: int
  Returns:
    tf.data.Dataset with optimization
  """"""
# The code in this comment is not optimized (no parallel, no prefetch), but even when I use this code, training loop fails miserably.
#  data = (
#    tf.data.Dataset.list_files(glob_path)
#    .batch(
#      batch_size,
#      drop_remainder=True
#    )
#    .map(
#      process_paths,
#      num_parallel_calls=1
#    )
#  )
#  

# This is optimized data preparation
  data = (
    tf.data.Dataset.range(2)
    .interleave(
      lambda *args: tf.data.Dataset.list_files(glob_path),
      num_parallel_calls=tf.data.experimental.AUTOTUNE
    )
    .batch(
      batch_size,
      drop_remainder=True
    )
    .map(
      process_paths,
      num_parallel_calls=tf.data.experimental.AUTOTUNE
    )
    .cache()
    .prefetch(tf.data.experimental.AUTOTUNE)
  )

  return data
```


main.py
```py
import time

import dataloader

data = dataloader.get_data('path/to/audio/*/*.mp3', 64)  # Sorry I updated this line to call function.
# path/to/audio/メジロ/1.mp3
# path/to/audio/メジロ/2.mp3
# ...
# path/to/audio/ウグイス/1.mp3
# path/to/audio/ウグイス/2.mp3
# ...

for f, l in data:
  # Training code is left out as it is irrelevant. This time.sleep is enough to reproduce.
  time.sleep(0.01)  
```


**Other info / logs** 

When I replace ```tfio.audio.AudioIOTensor(path)``` with ```tf.zeros((6000,), dtype=tf.float32)``` , whole epoch run finely. I think tfio.audio.AudioIOTensor has no data when read, so below error happens as it describes. I have no idea how to data prepared when reading.

```
> python main.py2020-09-27 21:32:37.473956: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA
2020-09-27 21:32:37.582070: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-27 21:32:37.593090: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc74c15d830 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-27 21:32:37.593109: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-27 21:32:47.870608: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at audio_kernels.cc:212 : Invalid argument: read 1100254 from 0 failed: 0
2020-09-27 21:32:47.895352: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at audio_kernels.cc:212 : Invalid argument: read 1100254 from 0 failed: 0
2020-09-27 21:32:48.574386: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at audio_kernels.cc:212 : Invalid argument: read 9796964 from 0 failed: 0
2020-09-27 21:32:50.874190: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at audio_kernels.cc:212 : Invalid argument: read 307678 from 0 failed: 0
2020-09-27 21:32:56.575449: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at audio_kernels.cc:212 : Invalid argument: read 3505630 from 0 failed: 0
Traceback (most recent call last):
  File ""/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/context.py"", line 2102, in execution_mode
    yield
  File ""/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 758, in _next_internal
    output_shapes=self._flat_output_shapes)
  File ""/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2610, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File ""/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 6843, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: read 1100254 from 0 failed: 0
	 [[{{node map/while/body/_1/map/while/IO>AudioReadableRead}}]] [Op:IteratorGetNext]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""a.py"", line 21, in <module>
    for f, l in data:
  File ""/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 736, in __next__
    return self.next()
  File ""/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 772, in next
    return self._next_internal()
  File ""/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 764, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/contextlib.py"", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/context.py"", line 2105, in execution_mode
    executor_new.wait()
  File ""/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/executor.py"", line 67, in wait
    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.InvalidArgumentError: read 1100254 from 0 failed: 0
	 [[{{node map/while/body/_1/map/while/IO>AudioReadableRead}}]]
2020-09-27 21:33:04.393965: W tensorflow/core/kernels/data/cache_dataset_ops.cc:798] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.```"
43603,Batch  normalisation with shared vision model sometimes causes NaN.,"Hello.

**Intorduction:**

I am writing a neural network that analyses video. For analyzing different frames I use shared vision model (the same set of layers and weights is applied over various images) and then I concatenate these columns into one model.

Why do I use Shared Vision Model?
As 3D convolutions and TimeDistributed models are not supported in conversion to TFLite that goes in pair with Tensorflow 1...
I need to resort to using a **Shared Vision Model**.

**When Problem does NOT occur:**
Training goes without any problems when I do not use Batch Normalization.
Training goes without problems if I use Batch Normalization in layers of neural network that are after the consatenation of frames.
Training also goes well if I train columns of neural network that analyze single images, with batch normalization, using TimeDistributed Embedding.
Training goes well if I use for the same purpose, 3D convolutions with depth (along time axis) set to 1.

**When Problem occurs:**
If I train neural network with usage of Batch Normalization on layers that belong to Shared Vision Model... training sometimes ends with NaN loss.

When in the same situation I switch batch normalization off, the problem never occurs.

Here is an example of training with shared vision model and BatchNormalization layers activated:
![obraz](https://user-images.githubusercontent.com/4332432/94397173-8fbf4880-0163-11eb-8bb5-8c24ef77b004.png)


My questions are:

- What could have gone wrong?

- Is it possible that batch normalization with usage of Shared Vision model has problem with accumulation of values from various instances?
- Is it possible that Batch Normalization returns NaN when all samples in a batch returned the same value (so variance is 0 and difference between mean and max is 0)?

Could NaN have came from a bug in implementation of Keras BatchNormalization() layer?
(I have seen a suspicious line of code:) 
`# sample variance - unbiased estimator of population variance`
`variance *= sample_size / (sample_size - (1.0 + self.epsilon))`

For me it seems that internal brackets cause that epsilon is subtracted instead of added, which for sample_size 1 will cause variance to be devided by a negative value.
Inside of K.normalize_batch_in_training() -> _broadcast_normalize_batch_in_training() -> tf.nn.batch_normalization(), there is line of code:
inv = math_ops.rsqrt(variance + variance_epsilon)

I guess that in some scenarios, negative value of variance could lead to rsqrt() from a negative value, which results in NaN...


NaN appearance during training is so unpredictable, dependent on outputs of previous layers and on combination of random batch, that I am not able to exactly provide inputs that cause mentioned problem.
However analysis what could have gone wrong (division by zero, root from minus value, or something else) could lead to finding a volatile spot.

I do not knwo if the one I have fund is the one...
I do nto know why this occurs for me only randomly and only in case of usage of Shard Vision Model.

Could it be wrong aggregation of data, that occurs with Shared Vision Model, but not with TimeDistributed wrapper and not with 3DConvolutions which have more stable data aggregation?

Could this error occur when all activations of layer predecessing to BatchNormalization() are the same across the batch?

That issue is important for me."
43602,i can't  find tutorials. Examples. ,"
"
43601,Dynamic Library loading failed 'cudart64_101.dll' & AttributeError: module 'tensorflow' has no attribute 'Session',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, 64 config
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip3 install upgrade --tensorflow
- TensorFlow version: 2.3.1
- Python version: 3.8.3
- Installed using virtualenv? pip? conda?: Created a virtual environment. Used this command: conda create -n tfp3.8 python= 3.8
- Bazel version (if compiling from source): No, not compliling from source.
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0
- GPU model and memory: NVIDIA GeForce MX250 

**Problem Description:**

Firstly it shows this after the installation of tensorflow:

2020-09-27 13:11:39.558775:
W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-09-27 13:11:39.563129:
I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

And secondly, 

AttributeError                            Traceback (most recent call last)
<ipython-input-3-f75057d1d95f> in <module>
----> 1 sess = tf.Session()

AttributeError: module 'tensorflow' has no attribute 'Session'

**Steps Performed:**

For the first problem, I executed the following steps:
Step 1: conda activate tfp3.8
This logs into the environment for the created tensorflow.

Step 2: python
Logs into the python terminal

>>import tensorflow as tf
This resulted in the above error enlisted as the first.

For the second problem, I executed the following steps in the jupyter notebook:
Step 1:  import tensorflow as tf //Success
Step 2:  hello = tf.constant(""Hello, Tensorflow!"") //Success
Step 3: sess = tf.Session() //Oops! Attribute Error 

"
43600,"Some times  ,interpreter_->Invoke() doesn't return","**System information**
RK3399 
Linux localhost 4.4.126 #1070 SMP PREEMPT Fri Sep 18 16:41:27 HKT 2020 aarch64

In mediapipe project, this function interpreter_->Invoke()  was called but don't return,so the program got stucked."
43599,tensorflow returns error 132 on macbook pro 2009,"Hello, i am trying to run tensorflow, with the following setup:
macbook mid 2009
Ubuntu 20.04
Pycharm latest version
python 3.7.9, installed with pycharm

now if i use tensorflow 2.3.1 i get an error 132
as suggested in others posts i down grade to tensorflow 1.5 but i get a different error; ModuleNotFoundError: No module named 'tensorflow.python.platform' but it is install on the virtual environment.

is there a fix or a work around?
"
43597,How add two pre defined keras models in keras tensorflow,"I've wanted to define network that has two input and use EfficientNetB1 to extract features and fine-tune on new layers so I've tried below code:

    def createNet(self,shape):
        FE1 = K.applications.EfficientNetB1(include_top=False, input_shape=shape)
        FE2 = K.applications.EfficientNetB1(include_top=False, input_shape=shape)
        inp1 = FE1.input
        out1 = FE1.layers[-1].output
        inp2 = FE2.input
        out2 = FE2.layers[-1].output

        merged_out = K.layers.concatenate((out1, out2))
        # .... other layers
        self.model = K.models.Model(inputs=[inp1, inp2], outputs=[merged_out])
        
        self.model.summary()

But I got this error :

    ValueError: The name ""stem_conv_pad"" is used 2 times in the model. All layer names should be unique.

So how can I define such a this network ?"
43596,"RuntimeError: Fill only currently supports int32, int64, float32, bool, string for input 1, got 9.Node number 412 (FILL) failed to invoke.","Hi,
I tried to run tflite model with INT8 post training. There is  no error converting .pb to .tflite. But error occurs when run inference on INT8 tflite model. BTW, there is no error when run inference on FP32 tflite model.


**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from : bianry
- TensorFlow version : tf-nightly 2.4.0-dev20200917

**Provide the text output from tflite_convert**

[{'name': 'input_x', 'index': 0, 'shape': array([ 1, 50], dtype=int32), 'shape_signature': array([-1, 50], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
[{'name': 'output/y_hat', 'index': 3339, 'shape': array([   1, 2363], dtype=int32), 'shape_signature': array([  -1, 2363], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
Traceback (most recent call last):
  File ""run_tflite.py"", line 67, in <module>
    interpreter.invoke()
  File ""/home/ai/anaconda5/envs/tf-nightly2.4/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py"", line 539, in invoke
    self._interpreter.Invoke()
RuntimeError: Fill only currently supports int32, int64, float32, bool, string for input 1, got 9.Node number 412 (FILL) failed to invoke.

**Standalone code to reproduce the issue** 
Convert:

##INT8
converter.optimizations = [tf.lite.Optimize.DEFAULT]
def representive_dataset_gen():
    num_calibration_steps = 10
    for i in range(num_calibration_steps):
        random_num = np.random.random_integers(0,10000, size=[50]).astype('int32')
        random_input = np.expand_dims(random_num, 0)
        yield [random_input]

converter.representative_dataset = representive_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS, tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
    
converter.experimental_new_converter = True
tflite_model = converter.convert()


Inference:
import tensorflow as tf

model_path = './model_int8.tflite'

interpreter = tf.lite.Interpreter(model_path=model_path)

interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
print(str(input_details))

output_details = interpreter.get_output_details()
print(str(output_details))

input_arr = np.random.random_integers(0,10000, size=[50])
input_arr_expanded = np.expand_dims(input_arr, 0)

input_arr_expanded = input_arr_expanded.astype('int32')

interpreter.set_tensor(input_details[0]['index'], input_arr_expanded)
interpreter.invoke()

"
43595,post quant keras model with  UpSample2D ，after post-quantized  get a false output shape,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):pip
- TensorFlow version (or github SHA if from source):2.3.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
convert=tf.lite.TFLiteConvert.from_keras_model(model)
convert.target_ops=[tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
convert.optimizations=[tf.lite.Optimize.DEFAULT]
convert.representative
```

**The output from the converter invocation**

```
got a false tflite model,ps: the up  layer is 1x10x10x64 with an UpSample2D in the tflite i got a 1x1x1x64 output
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43593,Possible GPU memory leak at tf.vectorized_map,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 18.04
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below): 2.4.0-dev20200821
- Python version:3.7
- GPU model and memory:v100 / titan rtx

When using tf.vectorized_map, GPU memory is continually increased, and finaly caused OOM after many training iterations. tf.map_fn works fine.
"
43591,RaggedTensor raises error with Keras model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pip installed
- TensorFlow version (use command below):2.3.0
- Python version:3.7.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:RTX 2060 6G/16G RAM

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I builded a lstm model for one to one word classification:
```
model = Sequential()
model.add(layers.Input(shape=(None, 512), ragged=True))
model.add(layers.LSTM(32, return_sequences=True, dropout=0.4))
model.add(layers.TimeDistributed(layers.Dense(13, activation='softmax')))
model.compile(loss='categorical_crossentropy', optimizer='adam')
model.summary()
```
I then generated my x_train as a tf.RaggedTensor, with shape [10000, None, 512], dtype='float32'.
y_train as a tf.RaggedTensor, with shape [10000, None, 13], dtype='float32'.

Now if run it with:
`history = model.fit(x, y, epochs=10, verbose=1)`

I'd get:
> TypeError: Failed to convert object of type <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. Contents: tf.RaggedTensor(values=Tensor(""sequential_42/time_distributed_46/dense_54/Softmax:0"", shape=(None, 13), dtype=float32), row_splits=Tensor(""sequential_42/time_distributed_46/RaggedFromRowLengths/control_dependency:0"", shape=(None,), dtype=int64)). Consider casting elements to a supported type.
**Describe the expected behavior**
TimeDistributed layer should support RaggedTensor if the underlying model can process slice of RaggedTensor and output dimension is matched.
**Standalone code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras import layers, Model, Sequential
import numpy as np

x = tf.RaggedTensor.from_row_splits(np.ones((100, 512)), [0, 4, 20, 100])
y = tf.RaggedTensor.from_row_splits(np.ones((100, 13)), [0, 4, 20, 100])
print(x.shape)
print(y.shape)
model = Sequential()
model.add(layers.Input(shape=(None, 512), ragged=True))
model.add(layers.LSTM(32, return_sequences=True, dropout=0.4))
model.add(layers.TimeDistributed(layers.Dense(13, activation='softmax')))
model.compile(loss='categorical_crossentropy', optimizer='adam')
model.summary()
history = model.fit(x=x, y=y, epochs=10, verbose=1)
```"
43589,some questions for coreml_executor.mm ,"I have a few questions for tensorflow/lite/experimental/delegates/coreml/coreml_executor.mm:
1.  line 143: id<MLFeatureProvider> outputFeature = [_model predictionFromFeatures:inputFeature options:options error:&error];
How does Coreml recognize custom constructs TensorData?

2.line 80: if ([featureName cStringUsingEncoding:NSUTF8StringEncoding] == input.name) {}
featureName  is output name of network and input.name is input name of network, the network output node name must not be equal to the input node name, this code doesn't make sense?
"
43588,Failed to build on Cuda-11.1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: related to servers
- TensorFlow installed from (source or binary): source
- TensorFlow version: commit: a0b68d1ecc46f9bbc8fad4f18c68f25c6bd5ae48
- Python version: 3.6
- Installed using virtualenv? pip? conda?: none
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 7.5.0-3ubuntu1
- CUDA/cuDNN version: CUDA-11.1, CuDNN 8.0.3 for CUDA-11.0
- GPU model and memory: RTX 3090, 24GB, RTX Titan, 24GB, RTX 2080Ti, 11GB (total three GPUs)



**Describe the problem**
Hello,
failed to compile Tensorflow.
Please check the log messages then give me an advice.
Thanks!


**Provide the exact sequence of commands / steps that you executed before running into the problem**
(Including logging messages)
bazel build --config=mkl //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
tensorflow$ deactivate
sephiroce@bike:~/open_source/tensorflow$ bazel build --config=mkl //tensorflow/tools/pip_package:build_pip_package
WARNING: Ignoring JAVA_HOME, because it must point to a JDK, not a JRE.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=228
INFO: Reading rc options for 'build' from /home/sephiroce/open_source/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/sephiroce/open_source/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /home/sephiroce/open_source/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/sephiroce/virtualenv/py3-tf2-gpu/bin/python3 --action_env PYTHON_LIB_PATH=/home/sephiroce/virtualenv/py3-tf2-gpu/lib/python3.6/site-packages --python_path=/home/sephiroce/virtualenv/py3-tf2-gpu/bin/python3 --config=xla --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.6 --action_env LD_LIBRARY_PATH=/home/sephiroce/open_source/rdkit/build/lib:/usr/local/cuda/lib64:/usr/local/lib:/usr/local/lib/openmpi:/home/sephiroce/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib: --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 --config=cuda --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /home/sephiroce/open_source/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/sephiroce/open_source/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /home/sephiroce/open_source/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file /home/sephiroce/open_source/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file /home/sephiroce/open_source/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:mkl in file /home/sephiroce/open_source/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 -c opt
INFO: Found applicable config definition build:linux in file /home/sephiroce/open_source/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/sephiroce/open_source/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Repository local_config_cuda instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule cuda_configure defined at:
  /home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl:1407:18: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1377
                _create_local_cuda_repository(<1 more arguments>)
        File ""/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1054, in _create_local_cuda_repository
                _find_libs(repository_ctx, <2 more arguments>)
        File ""/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 599, in _find_libs
                _check_cuda_libs(repository_ctx, <2 more arguments>)
        File ""/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 501, in _check_cuda_libs
                execute(repository_ctx, <1 more arguments>)
        File ""/home/sephiroce/open_source/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
No library found under: /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1377
                _create_local_cuda_repository(<1 more arguments>)
        File ""/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1054, in _create_local_cuda_repository
                _find_libs(repository_ctx, <2 more arguments>)
        File ""/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 599, in _find_libs
                _check_cuda_libs(repository_ctx, <2 more arguments>)
        File ""/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 501, in _check_cuda_libs
                execute(repository_ctx, <1 more arguments>)
        File ""/home/sephiroce/open_source/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
No library found under: /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1
WARNING: Target pattern parsing failed.
ERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1377
                _create_local_cuda_repository(<1 more arguments>)
        File ""/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1054, in _create_local_cuda_repository
                _find_libs(repository_ctx, <2 more arguments>)
        File ""/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 599, in _find_libs
                _check_cuda_libs(repository_ctx, <2 more arguments>)
        File ""/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 501, in _check_cuda_libs
                execute(repository_ctx, <1 more arguments>)
        File ""/home/sephiroce/open_source/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
No library found under: /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1
INFO: Elapsed time: 1.183s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package
```"
43585,Slow training with tfRecords and tf.functions,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 2.2
- Python version: 3.7
- Bazel version (if compiling from source): unknown
- GCC/Compiler version (if compiling from source): unknown
- CUDA/cuDNN version: 11.0
- GPU model and memory: 2 Nvidia RTX 2080 Ti graphics cards

**Describe the current behavior**
I have been trying to train a 3D CNN with tfRecords and also do augmentation on the fly when reading the records using @tf.function decorator and tf.numpy_function but the training is horribly slow.

**Standalone code to reproduce the issue**

This is the function for reading the tfRecords and make the augmentation on the fly
```
def input_fn(filenames, subset, batch_size, buffer_size=512, data_augmentation=True):
    # Args:
    # filenames:   Filenames for the TFRecords files.
    # subset:      Subset to make either train, valid, test.
    # batch_size:  Return batches of this size.
    # buffer_size: Read buffers of this size. The random shuffling
    #              is done on the buffer, so it must be big enough.

    # Create a TensorFlow Dataset-object which has functionality
    # for reading and shuffling data from TFRecords files.
    dataset = tf.data.TFRecordDataset(filenames=filenames)

    # Parse the serialized data in the TFRecords files.
    # This returns TensorFlow tensors for the image and labels.
    dataset = dataset.map(parse_example, num_parallel_calls=8)

    if subset == 'train' or subset =='valid':
        # Allow infinite reading of the data.
        dataset = dataset.repeat()
    else :
        dataset = dataset.repeat(1)

    if subset == 'train':
        dataset = dataset.shuffle(buffer_size=buffer_size)

    ''' DATA AUGMENTATION '''
    if (subset != 'test' and data_augmentation == True):
        dataset = dataset.map(elastic3D)
        dataset = dataset.map(flip3D)
        dataset = dataset.map(rotation3D)
        dataset = dataset.map(blur3D)


    # Get a batch of data with the given size.
    dataset = dataset.batch(batch_size)

    if subset == 'train':
        dataset = dataset.prefetch(16)

    return dataset

```


This is one function that I use in the map for augmentation
```
@tf.function
def flip3D(volume, label):

    def flip(volume):
        choice = np.random.randint(3)
        if choice == 0: # flip on x
            volume_flip = volume[::-1, :, :, :]
        if choice == 1: # flip on y
            volume_flip = volume[:, ::-1, :, :]
        if choice == 2: # flip on z
            volume_flip = volume[:, :, ::-1, :]

        return volume_flip

    augmented_volume = tf.numpy_function(flip, [volume], tf.float32)

    return augmented_volume, label
```

**Other info / logs** Include any logs or source code that would be helpful to
"
43584,tf.summary.create_file_writer does not write any event file and tensorboard show nothing,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution : Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.8.3
- CUDA/cuDNN version: 10.1/7.6
- GPU model and memory: RTX 2070super 8GB

**Describe the current behavior**
I have tried the example of https://www.tensorflow.org/api_docs/python/tf/summary
```python
import tensorflow as tf

writer = tf.summary.create_file_writer(""/tmp/mylogs"")
with writer.as_default():
  for step in range(100):
    # other model code would go here
    tf.summary.scalar(""my_metric"", 0.5, step=step)
    writer.flush()
```
and it works fine. However, when I migrate some codes from tf 1.X to tf 2.2 using tf_upgrade_v2 script, and add some codes to store some data in files, the summary_writer doesn't create any folder or event files.
``` python
#!/usr/bin/env python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)

def start_experiment():
    file_writer = tf.summary.create_file_writer(""/tmp/{}_{}"".format('none', 0))

    trainer = Trainer(file_writer=file_writer)
    trainer.train()
    file_writer.close()


class Trainer(object):
    def __init__(self, file_writer):
        self.file_writer = file_writer

    def train(self):
        info = {'tcount': 0, 'int_rew': 1, 'eprew':1, 'opt_tot':1}
        while True:
            # record data to tensorboard
            with self.file_writer.as_default():
                x = int(info['tcount'])
                tf.summary.scalar(""int_reward"", info['int_rew'], step=x)
                tf.summary.scalar(""ext_reward"", info['eprew'], step=x)
                tf.summary.scalar(""total_loss"", info['opt_tot'], step=x)
                self.file_writer.flush()
            info['tcount'] += 1
            if info['tcount'] > 100:
                break


if __name__ == '__main__':
    start_experiment()
```
I removed some detail codes to make it more clear. The training part of codes works fine, and it just doesn't record anything. 
**Describe the expected behavior**
It should create a folder in /tmp/none_0 and write some event file, but it did nothing."
43583,Tensorflow lite(ssd_mobilenet_v2_fpnlite_640*640)  Android APP detect nothing,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**Linux Ubuntu 16.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  **Motorola**
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version (use command below): **Tf-nightly 2.4.0**
- Python version: **3.6.10**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **CUDA 10.2 cuDNN**
- GPU model and memory: **GeForce GTX 1080 8G**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
1. Use my data to retrain the model **ssd_mobilenet_v2_fpnlite_640*640**, [link](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)
2. train command: **model_main_tf2.tf** [,link](https://github.com/tensorflow/models/blob/master/research/object_detection/model_main_tf2.py)
    export command: **export_tflite_graph_tf2.py**, [link](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_tf2.py)

**model convert :**
`model = tf.saved_model.load(""./saved_model"")
model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs[0].set_shape([1, 640, 640, 3])
tf.saved_model.save(model, ""saved_model_updated"", signatures=model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY])
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir='./saved_model', signature_keys=['serving_default'])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open(""./ssd_resnet50.tflite"", ""wb"").write(tflite_model)`

3. Android app, [link](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android)
4. main lines in **DetectActivity.java** 

` private static final int TF_OD_API_INPUT_SIZE = 640;

  private static final boolean TF_OD_API_IS_QUANTIZED = true;

  private static final String TF_OD_API_MODEL_FILE = ""detect.tflite"";

  private static final String TF_OD_API_LABELS_FILE = ""labelmap.txt"";

  private static final DetectorMode MODE = DetectorMode.TF_OD_API;

  // Minimum detection confidence to track a detection.
  private static final float MINIMUM_CONFIDENCE_TF_OD_API = 0.5f;

  private static final boolean MAINTAIN_ASPECT = false;

  private static final Size DESIRED_PREVIEW_SIZE = new Size(640, 480);`

**Describe the expected behavior**
**Android app should  detect objects  with their names location** 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
**When run app, errs are like this:**

`2020-09-26 10:13:37.689 21845-21845/org.tensorflow.lite.examples.detection E/tensorflow: CameraActivity: Exception!
    java.lang.IllegalStateException: This model does not contain associated files, and is not a Zip file.
        at org.tensorflow.lite.support.metadata.MetadataExtractor.assertZipFile(MetadataExtractor.java:325)
        at org.tensorflow.lite.support.metadata.MetadataExtractor.getAssociatedFile(MetadataExtractor.java:165)
        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:116)
        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:106)
        at org.tensorflow.lite.examples.detection.CameraActivity.onPreviewFrame(CameraActivity.java:200)
        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1175)
        at android.os.Handler.dispatchMessage(Handler.java:105)
        at android.os.Looper.loop(Looper.java:164)
        at android.app.ActivityThread.main(ActivityThread.java:6695)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:772)`

**By the way i tested the converted tflite and it's ok.**

"
43582,getting an error as 'tuple' object has no attribute 'apply_gradients',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): kaggle environment
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6+ 

I'm a beginner in the field of deep learning and getting hands on working with bert implementation for the classification problem using tensorflow. However I have encountered this error as mentioned in the title.  This is [link ](https://www.kaggle.com/dv1453/notebook3d0337bd04?scriptVersionId=43629422) to my kaggle notebook.  I referred some other notebooks to create train function as below:
`
def train_step(model, token_ids, masks, labels):
    
    labels = tf.dtypes.cast(labels, tf.float32)
    
    with tf.GradientTape() as tape:
        
        predictions = model(token_ids, attention_mask=masks)
        
        loss = loss_object(labels, predictions)
    
    
    gradients = tape.gradient(loss, model.trainable_variables)
    
    optimizer.apply_gradients(zip(gradients, model.trainable_variables), name = 'gradients')
    
    train_loss(loss)

    for i, auc in enumerate(train_auc_metrics):
        
        auc.update_state(labels[:,i], predictions[:,i])
        
def validation_step(model, token_ids, masks, labels):
    
    labels = tf.dtypes.cast(labels, tf.float32)

    predictions = model(token_ids, attention_mask=masks, training=False)
    
    v_loss = loss_object(labels, predictions)

    validation_loss(v_loss)
    
    for i, auc in enumerate(validation_auc_metrics):
        
        auc.update_state(labels[:,i], predictions[:,i])

seeds = [0]

for seed in range(len(seeds)):
    
    print('=' * 50, f""CV {seed+1}"", '=' * 50)
    
    model = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))
    
    labels =  train_df[label_cols].values
    
    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=seed, test_size = 0.2)

    train_masks, validation_masks = train_test_split(attention_masks, random_state=seed, test_size=0.2)

    train_size = len(train_inputs)

    validation_size = len(validation_inputs)


    train_dataset = create_dataset((train_inputs, train_masks, train_labels), batch_size=BATCH_SIZE,train=True)

    validation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), batch_size=BATCH_SIZE,train=False)
    
    
    steps_per_epoch = train_size // (BATCH_SIZE)

    #  Loss Function
    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)

    train_loss = tf.keras.metrics.Mean(name='train_loss')

    validation_loss = tf.keras.metrics.Mean(name='val_loss')

    #  Optimizer (with 1-cycle-policy)
    warmup_steps = steps_per_epoch // 3

    total_steps = steps_per_epoch * NR_EPOCHS - warmup_steps

    optimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)

    # Gradients
    
    gradients = 0
    
    #  Metrics
    train_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]

    validation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]


    for epoch in range(NR_EPOCHS):

        print('=' * 50, f""EPOCH {epoch+1}"", '=' * 50)

        start = time.time()


        for batch_no, (token_ids, masks, labels) in enumerate(tqdm(train_dataset)):

            train_step(model, token_ids, masks, labels)

            if batch_no % 100 == 0:

                    print(f'\nTrain Step: {batch_no}, Loss: {train_loss.result()}')

                    for i, label_name in enumerate(label_cols):

                        print(f""{label_name} roc_auc {train_auc_metrics[i].result()}"")

                        train_auc_metrics[i].reset_states()

        for batch_no, (token_ids, masks, labels) in enumerate(tqdm(validation_dataset)):

            validation_step(model, token_ids, masks, labels)

        print(f'\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\n')

        for i, label_name in enumerate(label_cols):

            print(f""{label_name} roc_auc {validation_auc_metrics[i].result()}"")

            validation_auc_metrics[i].reset_states()
`
however I got the error as fallowing:

`---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-16-11f3f3661ec3> in <module>
     60         for batch_no, (token_ids, masks, labels) in enumerate(tqdm(train_dataset)):
     61 
---> 62             train_step(model, token_ids, masks, labels)
     63 
     64             if batch_no % 100 == 0:

<ipython-input-14-d39f9869214e> in train_step(model, token_ids, masks, labels)
     12     gradients = tape.gradient(loss, model.trainable_variables)
     13 
---> 14     optimizer.apply_gradients(zip(gradients, model.trainable_variables), name = 'gradients')
     15 
     16     train_loss(loss)

AttributeError: 'tuple' object has no attribute 'apply_gradients'`


"
43581,tf-nightly fails to initialize TPUs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  2.4.0-dev20200925
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A


**Describe the current behavior**

Everything was fine 1.5 days ago,
 until I created a TPU node half a day ago with TPU software version ""nightly"":
```
gcloud compute tpus create node-123 \
	--zone=us-central1-f \
	--network=default \
    --accelerator-type=v2-8 \
    --range=10.123.0.0 \
	--preemptible \
    --version=nightly
```
and tried to initialize it:
```
import tensorflow as tf
tf.compat.v1.disable_eager_execution()

TPU_NAME='node-123'
ZONE='us-central1-f'

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_NAME, zone=ZONE)
tf.tpu.experimental.initialize_tpu_system(resolver)
```
The output was
```
INFO:tensorflow:Initializing the TPU system: node-123
```
and I was stuck with the initialization.


I believe this is a new bug in `2.4.0-dev20200925`.
I also suggest that there should be more tf-nightly versions to choose from when creating TPUs."
43580,Training loop from scratch for multi-out model - how to calculate loss and update it.,"
I have a ML models which have 3 output. Each of them have passed into a Soft-Max player. I'm trying to train the model from scratch with training loop in tf.keras. My generator output are something like this:

```
x,y = (batch_size, h,w,c), [(batch_size, 2), (batch_size, 8), (batch_size, 10)]
```

As you notice, each of the 3 output has different numbers of labels. First output layer has 2 labels output, second one has 8 and last one has 10. The problem is not about the data generator; It's fine. **However, I don't understand how will I calculate loss function and update it in scratch training.**

---

I've started with [document guidelines](https://keras.io/guides/writing_a_training_loop_from_scratch/). Here is the code snippet,

```python

epochs = 1
optimizer   = tf.keras.optimizers.Adam(learning_rate=0.05)
loss_fn     = tf.keras.losses.CategoricalCrossentropy()

train_acc_metric = tf.keras.metrics.Accuracy()
val_acc_metric   = tf.keras.metrics.Accuracy()

@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)  # Logits for this minibatch

        # Compute the loss value for this minibatch.
        train_loss_value = loss_fn(y, logits)

    grads = tape.gradient(train_loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    train_acc_metric.update_state(y, logits)
    return train_loss_value
```

And, 

```python

for epoch in range(epochs):
    # Iterate over the batches of the dataset.
    for step, (x_batch_train, y_batch_train) in enumerate(train_generator):
        train_loss_value = train_step(x_batch_train, y_batch_train)

    # Reset metrics at the end of each epoch
    train_acc_metric.reset_states()
```


I suspect, I am not handling properly the computation of loss in here: `loss_fn(y, logits)`. `y` has 3 output, `logits` has also 3 output. 

Using `model.compile` is simply straight-forward, just passing the loss function in the loss argument. But what to do in scratch training. The 3 output (though are co-related) needs to trained separately. 

Asked [SO](https://stackoverflow.com/q/64065473/9215780), nobody knows. 
 "
43579,ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.,"Getting this Error, please help me to resolve this issue.
(tensor) C:\Users\Bkmaurya>python
Python 3.6.12 |Anaconda, Inc.| (default, Sep  9 2020, 00:29:25) [MSC v.1916 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Bkmaurya\anaconda3\envs\tensor\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
**ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.**

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Bkmaurya\anaconda3\envs\tensor\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Bkmaurya\anaconda3\envs\tensor\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\Bkmaurya\anaconda3\envs\tensor\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\Users\Bkmaurya\anaconda3\envs\tensor\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Bkmaurya\anaconda3\envs\tensor\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Bkmaurya\anaconda3\envs\tensor\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Thanks!"
43578,Compiled TF2.3 with MKL on OSX does not progress beyond first epoch,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.15.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8.5
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): clang 10.0.1
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**
When executing code:
- vae.fit(minst_digits, epochs = 30, batch_size = 128)
it does not progress beyond the epoch 1 and seems to hang even after waiting several hours to see if the training progresses beyond epoch 1. I notice that the CPU load is showing 2 threads executing at 100%

**Describe the expected behavior**
Needs to be able to progress beyond 1st epoch and start training.

**Standalone code to reproduce the issue**
Please find the code to run the test here [vae 0.1.py](https://drive.google.com/file/d/14ntp7WeSGVHOy_Fx_5PS1nIneRS1DjOB/view?usp=sharing)

**Other info / logs** 

Basel build info:
bazel build --action_env CC=/usr/local/opt/llvm/bin/clang  --config=noaws --config=nogcp --config=mkl --config=opt -c opt //tensorflow/tools/pip_package:build_pip_package

Console output after export MKLDNN_VERBOSE=1
```

2020-09-26 08:52:10.680539: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ffc4db8dd80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-26 08:52:10.680577: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-26 08:52:10.680685: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Model: ""encoder""

Layer (type)                    Output Shape         Param #    Connected to                     
input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            
conv2d (Conv2D)                 (None, 14, 14, 32)   320         input_1[0][0]                    
conv2d_1 (Conv2D)               (None, 7, 7, 64)     18496       conv2d[0][0]                     
flatten (Flatten)               (None, 3136)         0           conv2d_1[0][0]                   
dense (Dense)                   (None, 16)           50192       flatten[0][0]                    
z_mean (Dense)                  (None, 2)            34          dense[0][0]                      
z_log_var (Dense)               (None, 2)            34          dense[0][0]                      
sampling (Sampling)             (None, 2)            0           z_mean[0][0]                     
                                                                 z_log_var[0][0]                  

Total params: 69,076
Trainable params: 69,076
Non-trainable params: 0

Model: ""decoder""
Layer (type)                 Output Shape              Param #   
input_2 (InputLayer)         [(None, 2)]               0         
dense_1 (Dense)              (None, 3136)              9408      
reshape (Reshape)            (None, 7, 7, 64)          0         
conv2d_transpose (Conv2DTran (None, 14, 14, 64)        36928     
conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32)        18464     
conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         289       

Total params: 65,089
Trainable params: 65,089
Non-trainable params: 0

Epoch 1/30
dnnl_verbose,info,oneDNN v1.4.0 (commit N/A)
dnnl_verbose,info,cpu,runtime:OpenMP
dnnl_verbose,info,cpu,isa:Intel AVX2
dnnl_verbose,info,gpu,runtime:none
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:Acdb8a:f0,,,32x1x3x3,0.00488281
dnnl_verbose,exec,cpu,convolution,jit:avx2,forward_training,src_f32::blocked:abcd:f0 wei_f32::blocked:Acdb8a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:aBcd8b:f0,post_ops:'eltwise_relu;';,alg:convolution_direct,mb128_ic1oc32_ih28oh14kh3sh2dh0ph0_iw28ow14kw3sw2dw0pw0,1.90991
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:ABcd8b8a:f0,,,64x32x3x3,0.0571289
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,128x32x14x14,1.68311
dnnl_verbose,exec,cpu,convolution,jit:avx2,forward_training,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8b8a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:aBcd8b:f0,post_ops:'eltwise_relu;';,alg:convolution_direct,mb128_ic32oc64_ih14oh7kh3sh2dh0ph0_iw14ow7kw3sw2dw0pw0,3.87109
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,128x64x7x7,0.25
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,128x64x7x7,0.225098
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ba:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,post_ops:'eltwise_relu;';,,mb128ic3136oc16,15.105
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ba:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb128ic16oc2,0.0161133
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ba:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb128ic16oc2,0.00512695
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ba:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,post_ops:'eltwise_relu;';,,mb128ic2oc3136,2.96802
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:ABcd8a8b:f0,,,64x64x3x3,0.0251465
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x64x7x7,0.787109
dnnl_verbose,exec,cpu,convolution,jit:avx2,backward_data,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8a8b:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic64oc64_ih14oh7kh3sh2dh0ph0_iw14ow7kw3sw2dw0pw0,9.69995
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,128x64x14x14,2.89893
dnnl_verbose,exec,cpu,eltwise,jit:avx2,forward_training,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,128x14x14x64,0.614014
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:ABcd8a8b:f0,,,64x32x3x3,0.0151367
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x64x14x14,3.52783
dnnl_verbose,exec,cpu,convolution,jit:avx2,backward_data,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8a8b:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic32oc64_ih28oh14kh3sh2dh0ph0_iw28ow14kw3sw2dw0pw0,12.1011
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,128x32x28x28,2.14697
dnnl_verbose,exec,cpu,eltwise,jit:avx2,forward_training,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,128x28x28x32,1.00903
dnnl_verbose,exec,cpu,reorder,simple:any,undef,src_f32::blocked:cdba:f0 dst_f32:p:blocked:ABcd8a8b:f0,,,32x1x3x3,0.00488281
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x32x28x28,6.56982
dnnl_verbose,exec,cpu,convolution,jit:avx2,backward_data,src_f32:p:blocked:aBcd8b:f0 wei_f32:p:blocked:ABcd8a8b:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic1oc32_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,6.87988
dnnl_verbose,exec,cpu,reorder,simple:any,undef,src_f32:p:blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,128x1x28x28,0.666992
dnnl_verbose,exec,cpu,sum,simple:any,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,,128x28x28x1,0.0817871
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:Acdb8a:f0,,,32x1x3x3,0.00292969
dnnl_verbose,exec,cpu,convolution,jit:avx2,forward_training,src_f32::blocked:abcd:f0 wei_f32::blocked:Acdb8a:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic1oc32_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,1.34204
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x32x28x28,3.01611
dnnl_verbose,exec,cpu,reorder,simple:any,undef,src_f32::blocked:acdb:f0 dst_f32:p:blocked:aBcd8b:f0,,,128x1x28x28,0.464111
dnnl_verbose,exec,cpu,eltwise,jit:avx2,backward_data,data_f32::blocked:aBcd8b:f0 diff_f32::blocked:aBcd8b:f0,,alg:eltwise_relu alpha:0 beta:0,128x32x28x28,2.26904
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x64x14x14,1.04614
dnnl_verbose,exec,cpu,convolution,jit:avx2,backward_weights,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8b8a:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic32oc64_ih28oh14kh3sh2dh0ph0_iw28ow14kw3sw2dw0pw0,2.33911
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:ABcd8b8a:f0 dst_f32::blocked:cdba:f0,,,64x32x3x3,0.0158691
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:ABcd8b8a:f0,,,64x32x3x3,0.013916
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x32x28x28,7.34912
dnnl_verbose,exec,cpu,convolution,jit:avx2,forward_training,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8b8a:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic32oc64_ih28oh14kh3sh2dh0ph0_iw28ow14kw3sw2dw0pw0,11.5352
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x64x14x14,0.933105
dnnl_verbose,exec,cpu,eltwise,jit:avx2,backward_data,data_f32::blocked:aBcd8b:f0 diff_f32::blocked:aBcd8b:f0,,alg:eltwise_relu alpha:0 beta:0,128x64x14x14,0.945068
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x64x7x7,0.780029
dnnl_verbose,exec,cpu,convolution,jit:avx2,backward_weights,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8b8a:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic64oc64_ih14oh7kh3sh2dh0ph0_iw14ow7kw3sw2dw0pw0,1.15698
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:ABcd8b8a:f0 dst_f32::blocked:cdba:f0,,,64x64x3x3,0.0258789
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:ABcd8b8a:f0,,,64x64x3x3,0.0200195
dnnl_verbose,exec,cpu,convolution,jit:avx2,forward_training,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8b8a:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic64oc64_ih14oh7kh3sh2dh0ph0_iw14ow7kw3sw2dw0pw0,5.6731
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,128x64x7x7,0.27002
dnnl_verbose,exec,cpu,eltwise,jit:avx2,backward_data,data_f32::blocked:ab:f0 diff_f32::blocked:ab:f0,,alg:eltwise_relu alpha:0 beta:0,128x3136,0.307129
dnnl_verbose,exec,cpu,sum,simple:any,undef,src_f32::blocked:ab:f0 src_f32::blocked:ab:f0 src_f32::blocked:ab:f0 dst_f32::blocked:ab:f0,,,128x2,0.00195312
dnnl_verbose,exec,cpu,sum,simple:any,undef,src_f32::blocked:ab:f0 src_f32::blocked:ab:f0 dst_f32::blocked:ab:f0,,,128x2,0.000976562
dnnl_verbose,exec,cpu,sum,simple:any,undef,src_f32::blocked:ab:f0 src_f32::blocked:ab:f0 dst_f32::blocked:ab:f0,,,128x16,0.000976562
dnnl_verbose,exec,cpu,eltwise,jit:avx2,backward_data,data_f32::blocked:ab:f0 diff_f32::blocked:ab:f0,,alg:eltwise_relu alpha:0 beta:0,128x16,0.00390625
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x64x7x7,0.231201
dnnl_verbose,exec,cpu,eltwise,jit:avx2,backward_data,data_f32::blocked:aBcd8b:f0 diff_f32::blocked:aBcd8b:f0,,alg:eltwise_relu alpha:0 beta:0,128x64x7x7,0.28418
dnnl_verbose,exec,cpu,convolution,jit:avx2,backward_weights,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8b8a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic32oc64_ih14oh7kh3sh2dh0ph0_iw14ow7kw3sw2dw0pw0,0.559082
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:ABcd8b8a:f0 dst_f32::blocked:cdba:f0,,,64x32x3x3,0.0100098
dnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:ABcd8b8a:f0 dst_f32::blocked:ABcd8a8b:f0,,,64x32x3x3,0.012207
dnnl_verbose,exec,cpu,convolution,jit:avx2,backward_data,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8a8b:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic32oc64_ih14oh7kh3sh2dh0ph0_iw14ow7kw3sw2dw0pw0,3.57495
dnnl_verbose,exec,cpu,eltwise,jit:avx2,backward_data,data_f32::blocked:aBcd8b:f0 diff_f32::blocked:aBcd8b:f0,,alg:eltwise_relu alpha:0 beta:0,128x32x14x14,0.571045
dnnl_verbose,exec,cpu,reorder,simple:any,undef,src_f32::blocked:acdb:f0 dst_f32:p:blocked:aBcd8b:f0,,,128x1x28x28,0.368164

```"
43576,Problem loading model trained with EfficientNetB0 ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.9
- CUDA/cuDNN version: 10.2
- GPU model and memory: GeForce GTX 1660 Ti with Max-Q Design 6 GB

**Describe the current behavior**
Error using tensorflow.keras.applications EfficientNetB0.

When I load my trained model, I have the following warnings (I only copied part of them here, you can check the rest in the colab link that is below:

```
WARNING:tensorflow:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_344566) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_344566) with ops with custom gradients. Will likely fail if a gradient is requested.
```
I basically save the model and load it again in the colab link below and I get this message.

Because of this I cannot continue training my model if I wish to do so. Am I missing something here?


**Standalone code to reproduce the issue**
Link to COLAB:
https://colab.research.google.com/drive/1Vjb65Y7E_FDbVZm_ZDs6ZP3yjjAN-rr5?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

I am basically using the same code from here: https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/
for the colab example. Am I doing something wrong?"
43575,tensorflow.keras.wrappers.scikit_learn KerasClassifier using 1% of GPU,"Hi,
Having tf 2.3.0 and keras 2.4.3
GridSearchCV not using full GPU
```
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
BatchSize = [100,200,300,400,500,600,700,800,900,1000]
Epochs = [100,200,300,400,500,600,700,800,900,1000]

    def create_model():
    	model = Sequential()
    	model.add(Dense(int(num_cols/2), input_dim=num_cols, activation='relu'))
    	model.add(Dense(1, activation='sigmoid'))
    	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    	return model
    model = KerasClassifier(build_fn=create_model, verbose=0)
    param_grid = dict(batch_size=BatchSize , epochs=Epochs )
    grid = GridSearchCV(model, cv = 3, param_grid=param_grid,n_jobs=-1)
    grid_result = grid.fit(X_train, y_train)         
    best_hyperparameters = grid_result.best_params_
```
i have set
```
from tensorflow.python.client import device_lib
assert 'GPU' in str(device_lib.list_local_devices())
```
![image](https://user-images.githubusercontent.com/30790120/94322155-0712a200-ffd5-11ea-83bf-c057628c71cf.png)

Is this correct?"
43569,RuntimeError: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.Failed to apply the default TensorFlow Lite delegate indexed at 0.,"Hi,

I tried to use build XNNPack with respect of accelerating tflite inference on CPU. I built tensorflow2.4 from source code with flag ""--define tflite_with_xnnpack=true"".When I run inference with my .tflite model with BiLSTM layer, ther is an error:
RuntimeError: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.Failed to apply the default TensorFlow Lite delegate indexed at 0.

But when I run a ResNet50 tflite model downloaded on TensorFlow Hub, no error occurs. What's more, I tried to run my tflite model with tf-nightly 2.4.0-dev20200917 and no error occurs.

What's probably going wrong? Thank you.
"
43568,TensorBoard callback doesn't update step properly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: 18.04.1-Ubuntu
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.6.9
- CUDA/cuDNN version: libcudart.so.10.1
- GPU model and memory: GeForce RTX 2080 Ti

**Describe the current behavior**
If using `tf.summary` operations while an `tf.keras.callbacks.TensorBoard` callback is active, the default `step` is always `0`.
This does not affect the step value of logged metrics, because `tf.keras.Model` does not rely on the default step, but directly uses `_train_counter`.

The reason for this is in https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/callbacks.py#L2075 which evaluates the value of the `step` variable, i.e `self._train_step` aka `self.model._train_counter` once when `on_train_begin()` or a similar method is called, but not when a new batch starts.

**Describe the expected behavior**
The default step value, i.e the value returned by `tf.summary.experimental.get_step()`, should reflect the current step. As `self._train_step` is a variable it should be sufficient to just pass it directly to `tf.summary.experimental.set_step()`.

**Standalone code to reproduce the issue**
log an arbitrary scalar with `tf.summary.scalar` during training but don't provide a step argument. Make sure that you created a `tf.keras.callbacks.TensorBoard` and set the `update_freq` parameter."
43567,Test code for tensorflow source code,"Sometimes, it is hard to debug the code without testing, or it is needed to write test for code.
In this case, if we create tests manually, then it will be needed to delete the test code directory or create a new branch without tests to send pull request.
Is it possible to take into account?"
43565,Detection postprocess operator missing,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source): 9aff666a8db689168e2d3aaef17b8a252791ada7
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**
Detection postprocess operator is missing from TFLu. It is needed for e.g. SSD Mobilenet V2.

**Please provide the exact sequence of commands/steps when you ran into the problem**

"
43564,Missing 1.15.4 release on Docker Hub,"I'm sorry if this is not the proper place to file that kind of issue, but I could not find a better place to report that issue. If such a place exist, please advise me of the details, I'll happily forward my request.

So, 1.15.4 has been released yesterday, containing a fix for CuDNN usage. While working on upgrading our codebase to refer that new version, I stumbled upon the fact that the TensorFlow Docker Hub does not offer any image after 1.15.2: as you can see on that link https://hub.docker.com/r/tensorflow/tensorflow/tags?page=1&name=1.15 there is no 1.15.3 nor 1.15.4.

I obviously don't know the details of your release process, but I don't think this is expected, and the lack of 1.15.3 makes me thinking this is not just a delay in building / uploading.

So, it would really be nice if the 1.15.4 images could be built and pushed to Docker Hub :)."
43562,issue numpy at Bazel build,"In continue #29845:
>@navi63 this issue has no relationship with your question. Please open new issues if they >are issues with TensorFlow library code or please ask other questions on StackOverflow.
>
>Locking conversation here as it has been solved already
>
>_Originally posted by @mihaimaruseac in >https://github.com/tensorflow/tensorflow/issues/29845#issuecomment-613544203_

@mihaimaruseac New contributors have a problem with build with Bazel. What is a solution you refer once you locked not resolved issue?!
Is any valid build instruction?"
43561,TF1 Keras Model Errors on Loading using TF2 - IndexError: list index out of range,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu **18.04**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **2.3.1**
- Python version: **3.7.3**

**Describe the current behavior**

When When trying to load the sequential model [here](https://github.com/vogon101/skincancer/blob/master/models/model_6_combined_2_DA.h5) using `tf.keras.models.load_model` in **TF 2.3.1**, an error is thrown at the following location:

```bash
~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in _should_skip_first_node(layer)
   1031   return (isinstance(layer, Functional) and
   1032           # Filter out Sequential models without an input shape.
-> 1033           isinstance(layer._layers[0], input_layer_module.InputLayer))
   1034 
   1035 
IndexError: list index out of range
```

The model is believed to be trained using keras and under TF1.9, and the structure of the model can be found [here](https://github.com/vogon101/skincancer/blob/master/src/sandbox/ml_lib/combined_model_v2.py), and here's [the code for training](https://github.com/vogon101/skincancer/blob/master/src/sandbox/v6_combined_model_v2.py).

Here you can find the full stack trace and running code under TF 2.3.1: https://colab.research.google.com/drive/1Lfo0O7D0cM8EtR0h6noqCoWqoqf8bzAD?usp=sharing

Then I downgraded to TF 2.2 and 2.1 with the same code above, it threw the error just as #35934 .

Then I downgraded to TF 2.0, the code was executing indefinitely. Finally I had to manually stop it:

```bash
/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py in IsMapping(o)
   2569 
   2570     """"""
-> 2571     return _pywrap_tensorflow_internal.IsMapping(o)
   2572 
   2573 def IsMappingView(o):
KeyboardInterrupt: 
```

Here you can find the full stack trace when I stopped the code under TF 2.0: https://colab.research.google.com/drive/1fCR-ci05NuYhQ8M9O2lRVG0F0YzI9Ggo?usp=sharing

Then I had no choice but to use TF 1.15.4 and Keras 2.3.1, and finally it worked out fine, inputs, outputs, summary etc. are all parsed correctly, as well as being able to run data through the model: https://colab.research.google.com/drive/1XaRMeiT1SefS6Q10wsa0y9rEercyFlCR?usp=sharing

**Describe the expected behavior**

I hope the Bug can be resolved so that TF2 can enhance the support for old models.
"
43560,Predict results by java and python api are not exactly same?,"Hi, I'm trying to run tensorflow model on spark, and testing the java api. I built a simple model by tensorflow and tried to run it with both java and python API. I found many results are not exactly same, but with different bit precision, and java results' precision bits are usually less then python's, for example:

```
java results:
       [9.9787010e-01, 1.7358017e-05, 2.1125420e-03],
       [9.9581650e-01, 8.3105200e-04, 3.3524006e-03],
       [9.9857880e-01, 3.4846460e-05, 1.3863753e-03],
       [9.9834824e-01, 3.1828644e-05, 1.6199955e-03],
       [9.9810120e-01, 2.9924562e-05, 1.8689097e-03],
       [9.8520744e-01, 7.6777805e-03, 7.1147494e-03],

python results:
       [9.9787009e-01, 1.7358017e-05, 2.1125420e-03],
       [9.9581653e-01, 8.3105202e-04, 3.3524006e-03],
       [9.9857879e-01, 3.4846460e-05, 1.3863753e-03],
       [9.9834824e-01, 3.1828644e-05, 1.6199955e-03],
       [9.9810117e-01, 2.9924562e-05, 1.8689097e-03],
       [9.8520744e-01, 7.6777805e-03, 7.1147499e-03],
```

The example I used is iris classification, results are iris's three types, so I used one-hot code.
The most wired thing is that java results' precision bits are less then python's obviously. So, is there any config or any way to make sure java result get same precision with python?

The codes I used to generate model:
```
def create_model():
    inputs = tf.keras.Input(shape=(4,), name='k_input')
    x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(inputs)
    outputs = tf.keras.layers.Dense(3, activation=tf.nn.softmax, name='out')(x)
    model = tf.keras.Model(inputs=inputs, outputs=outputs)

    model.compile(optimizer='adam',
                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=[tf.metrics.SparseCategoricalAccuracy()])

    return model


model = create_model()
model.fit(vec_train, res_train, epochs=10)
model.save(""saved_model/iris"")
```
The codes I used to run model in python:
```
loaded = keras.models.load_model(""saved_model/iris"")
res = loaded.predict(vec_sample)
```
The codes I used in java:
```
    val model = SavedModelBundle.load(s""$resourceDir/iris"", ""serve"")
    val session = model.session()

    val ret = session
      .runner()
      .feed(""serving_default_k_input"", Tensor.create(inputData))
      .fetch(""StatefulPartitionedCall"")
      .run()
    val size = inputData.length * 3
    val r = FloatBuffer.allocate(size.toInt)
    val result = ret.get(0).writeTo(r)
```

tensorflow version: 2.1
python version: 3.7
java version: 1.8
java api version: 1.15.0"
43559,Decoding error during vocabulary loading from TextVectorization layer,"**Intro**
Error occurs within get_vocabulary() method from TextVectorization when one of tokens can't be decoded. The exact place in a code is string_lookup's get_vocabulary() method:

`return [x.decode(self.encoding) for _, x in sorted(zip(values, keys))]`

**How to reproduce?**
```
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

text = 'Był to świetny pomysł, bo punktował Prawo i Sprawiedliwość tam, gdzie jest ono najsłabsze, mimo że udaje najsilniejsze. Uderzał w wizerunek państwa dobrobytu, które nikogo nie zostawia z tyłu i wyrównuje szanse. Tutaj mamy pewnego rodzaju déjà vu.'

vectorize_layer = TextVectorization()
vectorize_layer.adapt([text])
print(vectorize_layer.get_vocabulary())
```

Simpler code:

```
values = [1]
keys = [b'warszawie\xc2']
[x.decode('utf-8') for _, x in sorted(zip(values, keys))]
```

> Error: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 9: unexpected end of data

**Fix**

I was able to fix it simply by writing my own _get_vocabulary() method, just by ignoring decoding errors which are rare but frustrating:

```
def _get_vocabulary():
    keys, values = vectorize_layer._index_lookup_layer._table_handler.data()
    return [x.decode('utf-8', errors='ignore') for _, x in sorted(zip(values, keys))]
```

***Can option to ignore decoding errors be passed to string_lookup?***

- Windows 10 
- TensorFlow 2.2
- Python 3.8
"
43556,[MLIR / MHLO ]: tf_to_gpu_binary  Internal: Lowering to GPU kernels failed.,"I am using **tf_to_gpu_binary** to convert the following mlir file to a gpu kernel 

```
func @abs(%arg0: tensor<*xf32>) -> tensor<*xf32> {
 %0 = ""tf.Abs""(%arg0) { }
 : (tensor<*xf32>) -> tensor<*xf32>
 return %0 : tensor<*xf32>
}
```


On TF top of master (SHA 7f5b8ad3708d0e2cd98e63714352df60b772db36)  it fails with:
(tf_env) foo@bar:$ MLIR_CRASH_REPRODUCER_DIRECTORY=. ./build/install/bin/tf_to_gpu_binary --input=tf_abs.mlir --output=abs_kernel.o
loc(""-"":2:7): error: failed to legalize operation 'mhlo.abs' that was explicitly marked illegal
loc(""-"":0:0): error: A failure has been detected while processing the MLIR module, a reproducer has been generated in './mlir_reproducer_ironman-725e3dc1-43003-5b01dc8776cbc.mlir'
2020-09-24 23:54:07.367852: E tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.cc:95] Internal: Lowering to GPU kernels failed.


**MLIR CRASH Repro:**

```

// configuration: -pass-pipeline='func(xla-legalize-tf{allow-partial-conversion=false device-type=INVALID_DEVICE_TYPE legalize-chlo=true use-tf2xla-fallback=false}, materialize-broadcast, unfuse-batch-norm), unknown<mlir::mhlo::{anonymous}::HloLegalizeToLhlo>{results-escape-function=true}, func(buffer-placement, unknown<{anonymous}::CopyRemovalPass>), shape-to-descriptors, canonicalize, func(unknown<mlir::{anonymous}::LhloLegalizeToLinalgPass>, unknown<mlir::lmhlo::{anonymous}::LhloFuseLinalgPass>{tile-sizes= use-parallel-loops=true}, convert-linalg-to-parallel-loops, canonicalize, cse, unknown<xla::mlir_gpu::{anonymous}::FuseInnerParallelLoopsPass>, cse, unknown<xla::mlir_gpu::{anonymous}::StoreForwardingPass>, unknown<xla::mlir_gpu::{anonymous}::DeadTempBufferRemovalPass>, canonicalize, cse, unknown<xla::mlir_gpu::{anonymous}::MapParallelLoopsPass>), convert-parallel-loops-to-gpu, func(canonicalize, cse, unknown<mlir::mhlo::{anonymous}::LegalizeTrigonometricToApproximationPass>, unknown<xla::mlir_gpu::{anonymous}::MoveScalarComputationsIntoGpuLaunchPass>), gpu-kernel-outlining, func(unknown<xla::mlir_gpu::{anonymous}::RewriteKernelSignaturePass>), lower-affine, convert-scf-to-std'
// note: verifyPasses=true


module {
  func @abs(%arg0: tensor<*xf32>) -> tensor<*xf32> {
    %0 = ""tf.Abs""(%arg0) : (tensor<*xf32>) -> tensor<*xf32>
    return %0 : tensor<*xf32>
  }
}

```
"
43555,"Spurious tf.function retracing warnings, when developing Keras layer in colab","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Developing in colab with TF/Keras I always get these random seemingly unrelated warnings. 

**Describe the expected behavior**
No warnings.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/18XwRl5d8qY1L1qKPMm-A9NT_m4qB0UxZ?usp=sharing

Code that generate issue:

```
import tensorflow as tf
for ii in range(10):
  # Notice class definition *inside the loop*.
  class MyLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
      super(MyLayer, self).__init__(**kwargs)

    def call(self, inputs, training=None):
      return 2*inputs

  i1 = tf.keras.Input((1,))
  logits = MyLayer()(i1)
  model = tf.keras.Model(inputs=i1, outputs=logits)
  model.predict(x=[[0.0], [1.0], [3.0]], batch_size=1)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43554,WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues.,"
```
> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.gamma
> W0925 09:38:56.472692  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.gamma
> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.beta
> W0925 09:38:56.473077  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.beta
> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.0.0.kernel
> W0925 09:38:56.473470  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.0.0.kernel
> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.0.0.bias
> W0925 09:38:56.473858  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.0.0.bias
> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.1.0.kernel
> W0925 09:38:56.474293  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.1.0.kernel
> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.1.0.bias
> W0925 09:38:56.474685  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.residual_blocks.1.0.bias
> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.0.0.kernel
> W0925 09:38:56.475035  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.0.0.kernel
> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.0.1.gamma
> W0925 09:38:56.475502  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.0.1.gamma
> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.0.1.beta
> W0925 09:38:56.475951  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.0.1.beta
> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.1.0.kernel
> W0925 09:38:56.476325  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.1.0.kernel
> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.1.1.gamma
> W0925 09:38:56.476714  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.1.1.gamma
> WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.1.1.beta
> W0925 09:38:56.477157  6844 util.py:150] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).model._feature_extractor._fpn_features_generator.conv_layers.1.1.beta
> WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
> W0925 09:38:56.477508  6844 util.py:158] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
> 
```

i am getting this error while training steps are 25000. tensorflow v2.3 python 3.7.3 

"
43552,'Operation' object has no attribute '_unconditional_update' when using pre-trained model that used batchnorm,"When trying to use tf.keras.applications that use bacthnorm(like mobilenet or inceptionv3) i'm getting:
`'Operation' object has no attribute '_unconditional_update'`
Other keras.applications that do not use batch-norm(like vgg) work fine with the same code.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): debian  8
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 2.3
- Python version: 3.8.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: Nvidia titan xp

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When trying to use tf.keras.applications that use bacthnorm(like mobilenet or inceptionv3) i'm getting:
`'Operation' object has no attribute '_unconditional_update'` when running fit
Other keras.applications that do not use batch-norm(like vgg) work fine with the same code.
**Describe the expected behavior**
to be able to run fit on keras.applications models


**Standalone code to reproduce the issue**
use applications model: model_base = keras.applications.InceptionV3(input_shape=INPUT_SIZE,weights = ""imagenet"",include_top=False)
add fully connected layers to it and then call fit() 

"
43551,[comp:lite] App crashes while using GPU Delegate with Float_MobileNet or Float_EffieintNet,"Hi,
I am trying out the following example: [TFLite on Android (with Android (Studio)](https://www.tensorflow.org/lite/performance/gpu)

I could see it working for CPU, but while using GPU Delegate with Float_MobileNet or Float_EffieintNet, the app is crashing. Added
```
dependencies {
    ...
    implementation 'org.tensorflow:tensorflow-lite:2.3.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'
}
```

in order to use GPU delegate. And yes, the GPU delegate does not support Quantized models yet, so I can not even test that.

As per @jdduke's suggestion, I tried
```
implementation('org.tensorflow:tensorflow-lite:0.0.0-nightly') { changing = true }
implementation('org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly') { changing = true }
```
but could not succeed.

**Hardware Details**
Pixel 3a XL 6.0 1080x2160 xhdpi
Q Android 10.0  x86

If someone can help me out here, it would be a great help. 

Reference: [TFLite Google Group](https://groups.google.com/a/tensorflow.org/g/tflite/c/h2wSUviSywo/m/BSWIETLZBgAJ)

Thanks"
43550,'tf.Conv2D' op,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# Copy and paste here the exact command
```

**The output from the converter invocation**

```
# Copy and paste the output here.
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43547,Cannot use Hexagon delegate in Samsung S20 ultra. Failed to fetch Hexagon NN version. ,"
2020-09-25 00:49:30.103 3562-3562/org.tensorflow.lite.examples.detection E/Zygote: isWhitelistProcess - Process is Whitelisted
2020-09-25 00:49:30.103 3562-3562/org.tensorflow.lite.examples.detection E/Zygote: accessInfo : 1
2020-09-25 00:49:30.105 3562-3562/org.tensorflow.lite.examples.detection I/mples.detectio: Late-enabling -Xcheck:jni
2020-09-25 00:49:30.115 3562-3562/org.tensorflow.lite.examples.detection E/mples.detectio: Unknown bits set in runtime_flags: 0x8000
2020-09-25 00:49:30.120 3562-3562/org.tensorflow.lite.examples.detection D/ActivityThread: setConscryptValidator
2020-09-25 00:49:30.120 3562-3562/org.tensorflow.lite.examples.detection D/ActivityThread: setConscryptValidator - put
2020-09-25 00:49:30.161 3562-3562/org.tensorflow.lite.examples.detection W/ActivityThread: Application org.tensorflow.lite.examples.detection is waiting for the debugger on port 8100...
2020-09-25 00:49:30.164 3562-3562/org.tensorflow.lite.examples.detection I/System.out: Sending WAIT chunk
2020-09-25 00:49:31.166 3562-3562/org.tensorflow.lite.examples.detection I/System.out: Debugger has connected
2020-09-25 00:49:31.166 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...
2020-09-25 00:49:31.367 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...
2020-09-25 00:49:31.568 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...
2020-09-25 00:49:31.769 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...
2020-09-25 00:49:31.969 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...
2020-09-25 00:49:32.174 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...
2020-09-25 00:49:32.375 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...
2020-09-25 00:49:32.576 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...
2020-09-25 00:49:32.777 3562-3562/org.tensorflow.lite.examples.detection I/System.out: debugger has settled (1432)
2020-09-25 00:49:32.782 3562-3562/org.tensorflow.lite.examples.detection D/Proxy: setHttpProxySystemPropertyInternal for uid 10283 The host value is null the port value is null
2020-09-25 00:49:32.796 3562-3562/org.tensorflow.lite.examples.detection I/mples.detectio: The ClassLoaderContext is a special shared library.
2020-09-25 00:49:33.272 3562-3562/org.tensorflow.lite.examples.detection W/SemFloatingFeature: You called API `String getString(String tag, String defaultValue)` with feature [SEC_FLOATING_FEATURE_MESSAGE_CONFIG_PACKAGE_NAME].It has been deprecated after android Q. Instead, please Use `String getString(String tag)`
2020-09-25 00:49:33.347 3562-3562/org.tensorflow.lite.examples.detection D/tensorflow: CameraActivity: onCreate org.tensorflow.lite.examples.detection.DetectorActivity@5dbc894
2020-09-25 00:49:33.509 3562-3562/org.tensorflow.lite.examples.detection I/MultiWindowDecorSupport: [INFO] isPopOver = false
2020-09-25 00:49:33.510 3562-3562/org.tensorflow.lite.examples.detection I/MultiWindowDecorSupport: updateCaptionType >> DecorView@4870239[], isFloating: false, isApplication: true, hasWindowDecorCaption: false, hasWindowControllerCallback: true
2020-09-25 00:49:33.512 3562-3562/org.tensorflow.lite.examples.detection D/MultiWindowDecorSupport: setCaptionType = 0, DecorView = DecorView@4870239[]
2020-09-25 00:49:33.632 3562-3562/org.tensorflow.lite.examples.detection W/mples.detectio: Accessing hidden method Landroid/view/View;->computeFitSystemWindows(Landroid/graphics/Rect;Landroid/graphics/Rect;)Z (greylist, reflection, allowed)
2020-09-25 00:49:33.634 3562-3562/org.tensorflow.lite.examples.detection W/mples.detectio: Accessing hidden method Landroid/view/ViewGroup;->makeOptionalFitsSystemWindows()V (greylist, reflection, allowed)
2020-09-25 00:49:33.722 3562-3562/org.tensorflow.lite.examples.detection D/skia: ImageDecoder : EncodedFormat=4, Size=[800x128], ColorType=4, SampleSize=1
2020-09-25 00:49:33.777 3562-3562/org.tensorflow.lite.examples.detection D/skia: ImageDecoder : EncodedFormat=4, Size=[60x18], ColorType=4, SampleSize=1
2020-09-25 00:49:34.189 3562-3562/org.tensorflow.lite.examples.detection D/skia: ImageDecoder : EncodedFormat=4, Size=[81x81], ColorType=4, SampleSize=1
2020-09-25 00:49:34.195 3562-3562/org.tensorflow.lite.examples.detection D/skia: ImageDecoder : EncodedFormat=4, Size=[81x81], ColorType=4, SampleSize=1
2020-09-25 00:49:34.223 3562-3562/org.tensorflow.lite.examples.detection D/skia: ImageDecoder : EncodedFormat=4, Size=[71x48], ColorType=4, SampleSize=1
2020-09-25 00:49:34.319 3562-3562/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Connecting to camera service
2020-09-25 00:49:34.325 3562-3562/org.tensorflow.lite.examples.detection D/VendorTagDescriptor: addVendorDescriptor: vendor tag id 3854507339 added
2020-09-25 00:49:34.345 3562-15156/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 0 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client org.tensorflow.lite.examples.detection API Level 2
2020-09-25 00:49:34.350 3562-15178/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 1 facing CAMERA_FACING_FRONT state now CAMERA_STATE_CLOSED for client android.system API Level 2
2020-09-25 00:49:34.356 3562-15178/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 2 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2
2020-09-25 00:49:34.362 3562-15963/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 20 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2
2020-09-25 00:49:34.367 3562-15156/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 21 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2
2020-09-25 00:49:34.373 3562-15156/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 23 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2
2020-09-25 00:49:34.380 3562-15156/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 3 facing CAMERA_FACING_FRONT state now CAMERA_STATE_CLOSED for client android.system API Level 2
2020-09-25 00:49:34.385 3562-15156/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 4 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2
2020-09-25 00:49:34.390 3562-15178/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 40 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2
2020-09-25 00:49:34.396 3562-15178/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 41 facing CAMERA_FACING_FRONT state now CAMERA_STATE_CLOSED for client android.system API Level 2
2020-09-25 00:49:34.401 3562-15178/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 5 facing CAMERA_FACING_FRONT state now CAMERA_STATE_CLOSED for client android.system API Level 2
2020-09-25 00:49:34.407 3562-15178/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 52 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2
2020-09-25 00:49:34.413 3562-15156/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 80 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2
2020-09-25 00:49:34.560 3562-3562/org.tensorflow.lite.examples.detection I/tensorflow: CameraActivity: Camera API lv2?: true
2020-09-25 00:49:34.639 3562-3562/org.tensorflow.lite.examples.detection D/tensorflow: CameraActivity: onStart org.tensorflow.lite.examples.detection.DetectorActivity@5dbc894
2020-09-25 00:49:34.650 3562-3562/org.tensorflow.lite.examples.detection D/tensorflow: CameraActivity: onResume org.tensorflow.lite.examples.detection.DetectorActivity@5dbc894
2020-09-25 00:49:34.689 3562-15846/org.tensorflow.lite.examples.detection D/NativeCustomFrequencyManager: [NativeCFMS] BpCustomFrequencyManager::BpCustomFrequencyManager()
2020-09-25 00:49:34.695 3562-3562/org.tensorflow.lite.examples.detection D/ViewRootImpl@e74cbb7[DetectorActivity]: ThreadedRenderer.create() translucent=false
2020-09-25 00:49:34.696 3562-3562/org.tensorflow.lite.examples.detection E/ViewRootImpl@e74cbb7[DetectorActivity]: shouldSkipPokeDrawLockIfNeeded, Surface is not valid.
2020-09-25 00:49:34.715 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: setView = com.android.internal.policy.DecorView@4870239 TM=true MM=false
2020-09-25 00:49:34.729 3562-3562/org.tensorflow.lite.examples.detection D/tensorflow: CameraActivity: onPause org.tensorflow.lite.examples.detection.DetectorActivity@5dbc894
2020-09-25 00:49:34.749 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: stopped(true) old=false
2020-09-25 00:49:34.753 3562-3562/org.tensorflow.lite.examples.detection D/tensorflow: CameraActivity: onStop org.tensorflow.lite.examples.detection.DetectorActivity@5dbc894
2020-09-25 00:49:34.931 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: Relayout returned: old=(0,0,1080,2400) new=(0,0,1080,2009) req=(0,0)8 dur=21 res=0x100001 s={false 0} ch=false
2020-09-25 00:49:34.953 3562-3562/org.tensorflow.lite.examples.detection E/ViewRootImpl@e74cbb7[DetectorActivity]: shouldSkipPokeDrawLockIfNeeded, Surface is not valid.
2020-09-25 00:49:38.128 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: Relayout returned: old=(0,0,1080,2009) new=(0,0,1080,2009) req=(0,0)4 dur=8 res=0x100001 s={false 0} ch=false
2020-09-25 00:49:38.130 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: stopped(false) old=true
2020-09-25 00:49:38.131 3562-3562/org.tensorflow.lite.examples.detection D/tensorflow: CameraActivity: onStart org.tensorflow.lite.examples.detection.DetectorActivity@5dbc894
2020-09-25 00:49:38.133 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: stopped(false) old=false
2020-09-25 00:49:38.133 3562-3562/org.tensorflow.lite.examples.detection D/tensorflow: CameraActivity: onResume org.tensorflow.lite.examples.detection.DetectorActivity@5dbc894
2020-09-25 00:49:38.171 3562-3562/org.tensorflow.lite.examples.detection D/Surface: Create surface, surface=0x7d32b36000
2020-09-25 00:49:38.172 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: Relayout returned: old=(0,0,1080,2009) new=(0,0,1080,2009) req=(1080,2009)0 dur=7 res=0x100007 s={true 537721528320} ch=true
2020-09-25 00:49:38.173 3562-15846/org.tensorflow.lite.examples.detection D/OpenGLRenderer: createReliableSurface : 0x7c9517b1c0(0x7d32b36000)
2020-09-25 00:49:38.173 3562-15846/org.tensorflow.lite.examples.detection I/AdrenoGLES-0: QUALCOMM build                   : 48175c6, I33ebe07a9a
    Build Date                       : 07/27/20
    OpenGL ES Shader Compiler Version: EV031.29.00.09
    Local Branch                     : 
    Remote Branch                    : refs/tags/AU_LINUX_ANDROID_LA.UM.8.12.C1.10.00.00.649.113
    Remote Branch                    : NONE
    Reconstruct Branch               : NOTHING
2020-09-25 00:49:38.173 3562-15846/org.tensorflow.lite.examples.detection I/AdrenoGLES-0: Build Config                     : S P 8.0.12 AArch64
2020-09-25 00:49:38.173 3562-15846/org.tensorflow.lite.examples.detection I/AdrenoGLES-0: Driver Path                      : /vendor/lib64/egl/libGLESv2_adreno.so
2020-09-25 00:49:38.173 3562-3562/org.tensorflow.lite.examples.detection D/ViewRootImpl@e74cbb7[DetectorActivity]: mThreadedRenderer.initialize() mSurface={isValid=true 537721528320} hwInitialized=true
2020-09-25 00:49:38.178 3562-15846/org.tensorflow.lite.examples.detection I/AdrenoGLES-0: PFP: 0x016dd087, ME: 0x00000000
2020-09-25 00:49:38.187 3562-15846/org.tensorflow.lite.examples.detection D/OpenGLRenderer: makeCurrent EglSurface : 0x0 -> 0x0
2020-09-25 00:49:38.190 3562-15846/org.tensorflow.lite.examples.detection D/OpenGLRenderer: eglCreateWindowSurface : 0x7c95175d00
2020-09-25 00:49:38.201 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: [DrawPending] drawPending(1) 1 android.view.ViewRootImpl.reportNextDraw:9955 android.view.ViewRootImpl.performTraversals:3332 android.view.ViewRootImpl.doTraversal:2225 
2020-09-25 00:49:38.201 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: [DrawPending] performDraw() Waiting asnyc report
2020-09-25 00:49:38.206 3562-3562/org.tensorflow.lite.examples.detection D/BufferQueueProducer: Create producer, producer=0x7c9af71000
2020-09-25 00:49:38.206 3562-3562/org.tensorflow.lite.examples.detection D/Surface: Create surface, surface=0x7ca3b80000
2020-09-25 00:49:38.246 3562-3562/org.tensorflow.lite.examples.detection I/tensorflow: CameraConnectionFragment: Desired size: 640x480, min size: 480x480
2020-09-25 00:49:38.248 3562-3562/org.tensorflow.lite.examples.detection I/tensorflow: CameraConnectionFragment: Valid preview sizes: [4000x3000, 4000x2252, 4000x1800, 2992x2992, 2400x1080, 1920x864, 1920x824, 3840x2160, 1920x1080, 1440x1080, 1088x1088, 1280x720, 960x720, 720x480, 640x480]
2020-09-25 00:49:38.248 3562-3562/org.tensorflow.lite.examples.detection I/tensorflow: CameraConnectionFragment: Rejected preview sizes: [640x360, 352x288, 320x240, 256x144, 176x144]
2020-09-25 00:49:38.248 3562-3562/org.tensorflow.lite.examples.detection I/tensorflow: CameraConnectionFragment: Exact size match found.
2020-09-25 00:49:44.005 3562-3562/org.tensorflow.lite.examples.detection I/org.tensorflow.lite.examples.detection: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:2022: fastrpc_apps_user_init done
2020-09-25 00:49:44.006 3562-3562/org.tensorflow.lite.examples.detection W/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.
2020-09-25 00:49:44.006 3562-3562/org.tensorflow.lite.examples.detection I/tflite: Hexagon Delegate is not supported.
"
43546,"Problem In tensorflow-gpu with error ""Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0.""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS: Windows 10 Home with 8GB ram, NVIDIA MX250 2GB graphic card.- TensorFlow installed from (source or binary):
- TensorFlow version :2.2.0
- Python version: 3.6
- CUDA/cuDNN version: CUDA 10.2 and CUDNN: 8.0.3.33 i.e. for cuda 10.1
- GPU model and memory: NVIDIA MX250 with 2GB Graphic

**Describe the current behavior**
My tensorflow-gpu was working fine. Once I have reinstall my anaconda and install tensorflow-gpu again after that
when every I try to train any model on tensorflow-gpu It always gives me this error:
_Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2020-09-24 21:35:23.361799: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-09-24 21:35:23.363348: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-09-24 21:35:23.627463: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available._
**Describe the expected behavior**
I was Expecting that my train will work fine.
**Standalone code to reproduce the issue**

from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, Flatten,Conv2D,MaxPool2D,AvgPool2D,Dropout
from tensorflow.keras.models import Model,Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from glob import glob
import matplotlib.pyplot as plt

train_path=""E:/AI-Application-Implementation/trained_model/Classification/Cifar-10/data/train""
test_path=""E:/AI-Application-Implementation/trained_model/Classification/Cifar-10/data/test""

folders=glob(""E:/All Data Set/CIFAR10/train/*"")

datagen=ImageDataGenerator(rotation_range=0.5,
                                 brightness_range=[0.2,0.5],
                                 zoom_range=[0.1,0.8],
                                 horizontal_flip=True,
                                 validation_split=0.2,
                                 rescale=1./255)

train=datagen.flow_from_directory(directory=train_path,
                                        target_size=(256,256),
                                        # color_mode=""grayscale"",
                                        shuffle=True,
                                        class_mode='categorical',
                                        subset='training')

test=datagen.flow_from_directory(directory=train_path,
                                        target_size=(256,256),
                                        # color_mode=""grayscale"",
                                        shuffle=True,
                                        class_mode='categorical',
                                        subset='validation')

model=Sequential()

model.add(Conv2D(filters=32,kernel_size=(3,3),input_shape=(256,256,3),activation='relu'))
model.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu'))
model.add(MaxPool2D(pool_size=(3,3)))
model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))
model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))
model.add(MaxPool2D(pool_size=(3,3)))
model.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))
model.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))
model.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))
model.add(MaxPool2D(pool_size=(3,3)))
model.add(AvgPool2D(pool_size=(6,6)))
model.add(Flatten())
model.add(Dense(units=64,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=10,activation='softmax'))
model.summary()
model.compile(loss='sparse_categorical_crossentropy',optimizer=""adam"",metrics=['accuracy'])

history=model.fit(train,validation_data=test,epochs=5,steps_per_epoch=len(train),validation_steps=len(test))


**Other info / logs** Include any logs or source code that would be helpful to
My full console wile running my code:
2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
Found 40000 images belonging to 10 classes.

2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll

2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
Found 10000 images belonging to 10 classes.
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param  
=================================================================
conv2d (Conv2D)              (None, 254, 254, 32)      896       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 252, 252, 32)      9248      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 84, 84, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 82, 82, 64)        18496     
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 80, 80, 64)        36928     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 26, 26, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 24, 24, 128)       73856     
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 22, 22, 128)       147584    
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 20, 20, 128)       147584    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         
_________________________________________________________________
average_pooling2d (AveragePo (None, 1, 1, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 128)               0         
_________________________________________________________________
dense (Dense)                (None, 64)                8256      
_________________________________________________________________
dropout (Dropout)            (None, 64)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 10)                650       

Total params: 443,498
Trainable params: 443,498
Non-trainable params: 0
_________________________________________________________________

2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)
2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX250, Compute Capability 6.1
Epoch 1/5

2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)
2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX250, Compute Capability 6.1
2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll

2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)
2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX250, Compute Capability 6.1
2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll

2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)
2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX250, Compute Capability 6.1
2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:22.184494: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.

2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)
2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX250, Compute Capability 6.1
2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:22.184494: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2020-09-24 21:35:23.361799: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-09-24 21:35:23.363348: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-09-24 21:35:23.627463: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.

2020-09-24 21:35:09.493907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:14.913210: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-09-24 21:35:15.668676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2020-09-24 21:35:15.673588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:15.729057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:15.750802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-24 21:35:15.756718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-24 21:35:15.777083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-24 21:35:15.781830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-24 21:35:15.795484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:15.795889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-24 21:35:15.797938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-24 21:35:15.809116: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29073047360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-24 21:35:15.810096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-24 21:35:15.811012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:06:00.0 name: GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2020-09-24 21:35:15.812247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-09-24 21:35:15.812911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:15.813512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-09-24 21:35:15.814104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-09-24 21:35:15.814716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-09-24 21:35:15.815307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-09-24 21:35:15.815903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:15.816520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-24 21:35:17.064070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-24 21:35:17.064704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-24 21:35:17.064993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-24 21:35:17.065525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: GeForce MX250, pci bus id: 0000:06:00.0, compute capability: 6.1)
2020-09-24 21:35:17.071980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2901975f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-24 21:35:17.072790: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX250, Compute Capability 6.1
2020-09-24 21:35:19.562964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-09-24 21:35:20.877584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-09-24 21:35:22.184494: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2020-09-24 21:35:23.361799: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.20GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-09-24 21:35:23.363348: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-09-24 21:35:23.627463: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-09-24 21:35:24.619275: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 796.38MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
"
43543,Add int16 support to the CMSIS-NN version of softmax,"@tensorflow/micro

The softmax reference kernel have been updated with int16 support, and the CMSIS-NN version should to updated with int16 support as well.

"
43542,tf.numpy_function() does not support symbolic Tensors,"**System information**
- I have written custom code (see below)
- Red Hat Enterprise Linux Server release 7.7 (Maipo)
- TensorFlow installed from: binary (pip)
- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.6.9

**Describe the current behavior**
Calling `tf.numpy_function()` with symbolic Tensor crashes (see below). This prevents `tf.numpy_function()` from being directly used when building a Keras Model with the Functional API.

**Describe the expected behavior**
Calling `tf.numpy_function()` with symbolic Tensor should return the symbolic Tensors which are the result of the operation.

**Standalone code to reproduce the issue**
```python
>>> import tensorflow as tf, numpy as np
>>> (tf.__version__, np.__version__)
('2.3.0', '1.18.5')

# Create a symbolic Tensor.
>>> x = tf.keras.layers.Input(shape=(2,))
>>> x
<tf.Tensor 'input_2:0' shape=(None, 2) dtype=float32>

# Calling it on tf.sin() gives a symbolic Tensor as output.
>>> tf.sin(x)
<tf.Tensor 'Sin:0' shape=(None, 2) dtype=float32>

# Calling it on tf.numpy_function() with np.sin() fails:
>>> tf.numpy_function(np.sin, [x], [tf.float32])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 632, in numpy_function
    return py_func_common(func, inp, Tout, stateful=True, name=name)
  File ""/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 519, in py_func_common
    result = func(*[np.array(x) for x in inp])
  File ""/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 519, in <listcomp>
    result = func(*[np.array(x) for x in inp])
  File ""/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 848, in __array__
    "" a NumPy call, which is not supported"".format(self.name))
NotImplementedError: Cannot convert a symbolic Tensor (input_2:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
```

**Other info / logs**
`tf.numpy_function()` works as expected under other circumstances.
```python
# With an eager Tensor.
>>> x = tf.constant([0., 1.])
>>> x
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 1.], dtype=float32)>
>>> tf.sin(x)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.        , 0.84147096], dtype=float32)>
>>> tf.numpy_function(np.sin, [x], [tf.float32])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.        , 0.84147096], dtype=float32)>
```
```python
# When eager execution is disabled.
>>> import tensorflow as tf, numpy as np
>>> tf.python.framework.ops.disable_eager_execution()
>>> x = tf.keras.layers.Input(shape=(2,))
>>> tf.numpy_function(np.sin, [x], [tf.float32])
[<tf.Tensor 'PyFunc:0' shape=<unknown> dtype=float32>]
```
```python
# When wrapping tf.numpy_function() inside a Keras Lambda Layer.
>>> import tensorflow as tf, numpy as np
>>> x = tf.keras.layers.Input(shape=(2,))
>>> tf.keras.layers.Lambda(lambda t: tf.numpy_function(np.sin, [t], [tf.float32]))(x)
[<tf.Tensor 'lambda/PyFunc:0' shape=<unknown> dtype=float32>]
```"
43541,NNAPI Delegate BUG,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Xiaomi 9Pro (SnapDragon 855+), Android Q
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15.0
- Python version: 3.7.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

I have several Questions.

**1.** When I run the model  **mobilenet_v1_1.0_224_quant.tflite** in tflite examples, and I select NNAPI device, there are log output as followings in LOGCAT. All layers of the model are supported by NNAPI, so what does these outputs mean? 
```
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.028 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {AVERAGE_POOLING_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.029 12125-12127/? E/hta-unnhal: {SOFTMAX, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.031 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {AVERAGE_POOLING_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.032 12125-12127/? E/hta-unnhal: {SOFTMAX, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {DEPTHWISE_CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {AVERAGE_POOLING_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {CONVOLUTION_2D, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.
2020-09-24 20:54:57.034 12125-12127/? E/hta-unnhal: {SOFTMAX, TENSOR_QUANT8} is not supported.
```

**2.**  When I run the model **efficientnet-lite0-int8.tflite** in the examples, I find that the performance using NNAPI is much slower than using CPU. However, all the layers are supported by NNAPI. So Why ?

**3.** About NNAPI.   The following two APIs can not be set in the meantime, otherwise the error log occurs. So which API is valid，or both are valid but can not be set in the meantime?
```
 tfliteOptions.addDelegate(nnApiDelegate);
 tfliteOptions.setUseNNAPI(true);

Internal error: Failed to apply delegate: ModifyGraphWithDelegate is disallowed when graph is immutable.
```

**4.** How can I set GPU inference using fp16 or fp32? I find the following api has been deprecated.
```
setAllowFp16PrecisionForFp32(boolean allow);
```

**5.** I have tried to quantize my model to 8bit using the following code. But I find that some conv op's weights are still fp32, some conv op's weights are int8. How can I solve it ?

```
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
tflite_quant_model = converter.convert()
```

**Describe the expected behavior**

**1.** solve the not supported log output in logcat.
**2.** NNAPI 's performance is not slower than cpu.
**3.** use proper API to enable NNAPI delegate.
**4.** I can set to use fp16 or fp32 in GPU delegate.
**5.** quantize model to 8bit successfully.

Thanks very much, looking forward to your reply.
"
43540,TF 2.1: ModelCheckpoint save best model fails,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**

Mac OS 10.15.16

```
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
v2.1.0-rc2-17-ge5bf8de410 2.1.0
```


**Describe the current behavior**

When trying to use `tf.keras.callbacks.ModelCheckpoint` a crash occurs when setting `save_best_only` to `True`

**Describe the expected behavior**

Training should not crash between epochs

**NOTE**: Upgrading to TF 2.3 does not cause this to happen. However the 2.1 docs still say this option should work.

**Standalone code to reproduce the issue**

https://colab.research.google.com/drive/18jS5tqPo-PRaoxR1qSMMlgUjyM0sIll1?usp=sharing
"
43539,TensorFlow in Practice Specialization has been renamed on Coursera,"[Here](https://github.com/tensorflow/tensorflow#resources), the ""TensorFlow in Practice from Coursera"" should be renamed as ""DeepLearning.AI TensorFlow Developer Professional Certificate"" to reflect the current name of it."
43537,Ethos-U scratch tensors not allocated,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source): b023b033299a2e88a8f25980e234144657df5fc2
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Ethos-U

**Describe the problem**
Ethos-u relies on differentiating between having operator input tensors as subgraph inputs or not. It is depending on the offline planner and is using one or two additional operator input tensors (similar to scratch tensors, but part of the tflite file). We want these to remain as input tensors to the operator, but not as inputs to the subgraph. There has been a workaround for this, see PR: https://github.com/tensorflow/tensorflow/pull/42697
However it does not work if there is any CPU operator before the Ethos-U operator. Also a more general solution is wanted.

**Please provide the exact sequence of commands/steps when you ran into the problem**

"
43536,help!!!!! tensorflow error,"![image](https://user-images.githubusercontent.com/27395455/94133874-72505d00-fe8b-11ea-9cb8-dae6a43a7926.png)
"
43535,Problem with custom grad with multiple external variables,"**Describe the current behavior**

The ""variables"" argument in custom grad seems to be buggy. For example this code, which is a complexified version of the ""custom_grad"" example in the doc:

```
import tensorflow as tf
import numpy as np
import time

m=1000

varr=tf.Variable(200.0)
varr3=tf.Variable(200.0)
varr2=tf.constant(1.0)

@tf.function
@tf.custom_gradient
def log1pexp(x1, x2):
  #some basic function fluff 
  e=tf.exp(x1)
  e=tf.math.log(1 + e)
  for i in range(m):
  	e += 0.0*tf.exp(x1) + 1.0

  #here comes the problem
  print(""tracing"")
  def grad(dy, dz, variables=[varr, varr3]):
    return [dy * (1 - 1 / (1 + e)) + 0 * varr2, tf.constant(0.0)], [0.0 * varr, 0.0*varr3]

  return [e+varr, varr3], grad#

x = tf.constant(1.0)
with tf.GradientTape() as g:
  g.watch(x)
  y=log1pexp(x,0.0)

dy_dx = g.gradient(y, x)
print (dy_dx)
```

Leads to the error:
``ValueError: Must return gradient for each variable from @custom_gradient grad_fn.``

Which i seem to be doing (log1pexp has 2 inputs, outputs and two external variables, so the return size should be  list of size 2 each ?)

However if I change line 23 to 
```    return [dy * (1 - 1 / (1 + e)) + 0 * varr2, tf.constant(0.0)], [0.0 * varr] ```

Then it doesn't give an error, which doesn't make sense to me since in that case, grad_vars only contain gradient info for ONE of the external variables I have registered in the variables parameters.

Also it doesn't care about varr2 at all ...

**Describe the expected behavior**

The code above should not give error?



**Standalone code to reproduce the issue**

See above

**Other info / logs** 

Basic ubuntu 20. error with tf 2.2 and 2.3"
43534,Duplicate node name in graph: 'ones',"Hi,

I have the following error testing the yolov3 example: https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3

2020-09-24 11:28:23.492471: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
Traceback (most recent call last):
  File ""C:\Users\c81441\AppData\Local\Continuum\Anaconda3_64_2020_02\envs\tensor_flow_env_test\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1610, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Duplicate node name in graph: 'ones'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2017.3\helpers\pydev\pydevd.py"", line 1683, in <module>
    main()
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2017.3\helpers\pydev\pydevd.py"", line 1677, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2017.3\helpers\pydev\pydevd.py"", line 1087, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2017.3\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""C:/Users/c81441/Documents/75-GIT-PYTHON-PHOTOGRAMMETRY/deep_learning/20-TensorFlow-2.x-YOLOv3-master/detection_demo.py"", line 22, in <module>
    yolo = Load_Yolo_model()
  File ""C:/Users/c81441/Documents/75-GIT-PYTHON-PHOTOGRAMMETRY/deep_learning/20-TensorFlow-2.x-YOLOv3-master\yolov3\utils.py"", line 84, in Load_Yolo_model
    yolo = Create_Yolo(input_size=YOLO_INPUT_SIZE, CLASSES=YOLO_COCO_CLASSES)
  File ""C:/Users/c81441/Documents/75-GIT-PYTHON-PHOTOGRAMMETRY/deep_learning/20-TensorFlow-2.x-YOLOv3-master\yolov3\yolov4.py"", line 398, in Create_Yolo
    pred_tensor = decode(conv_tensor, NUM_CLASS, i)
  File ""C:/Users/c81441/Documents/75-GIT-PYTHON-PHOTOGRAMMETRY/deep_learning/20-TensorFlow-2.x-YOLOv3-master\yolov3\yolov4.py"", line 427, in decode
    xy_grid = tf.meshgrid(tf.range(output_size), tf.range(output_size))
  File ""C:\Users\c81441\AppData\Local\Continuum\Anaconda3_64_2020_02\envs\tensor_flow_env_test\lib\site-packages\tensorflow_core\python\ops\array_ops.py"", line 2954, in meshgrid
    mult_fact = ones(shapes, output_dtype)
  File ""C:\Users\c81441\AppData\Local\Continuum\Anaconda3_64_2020_02\envs\tensor_flow_env_test\lib\site-packages\tensorflow_core\python\ops\array_ops.py"", line 2583, in ones
    output = fill(shape, constant(one, dtype=dtype), name=name)
  File ""C:\Users\c81441\AppData\Local\Continuum\Anaconda3_64_2020_02\envs\tensor_flow_env_test\lib\site-packages\tensorflow_core\python\ops\array_ops.py"", line 171, in fill
    result = gen_array_ops.fill(dims, value, name=name)
  File ""C:\Users\c81441\AppData\Local\Continuum\Anaconda3_64_2020_02\envs\tensor_flow_env_test\lib\site-packages\tensorflow_core\python\ops\gen_array_ops.py"", line 3601, in fill
    ""Fill"", dims=dims, value=value, name=name)
  File ""C:\Users\c81441\AppData\Local\Continuum\Anaconda3_64_2020_02\envs\tensor_flow_env_test\lib\site-packages\tensorflow_core\python\framework\op_def_library.py"", line 793, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\c81441\AppData\Local\Continuum\Anaconda3_64_2020_02\envs\tensor_flow_env_test\lib\site-packages\tensorflow_core\python\framework\func_graph.py"", line 548, in create_op
    compute_device)
  File ""C:\Users\c81441\AppData\Local\Continuum\Anaconda3_64_2020_02\envs\tensor_flow_env_test\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 3429, in _create_op_internal
    op_def=op_def)
  File ""C:\Users\c81441\AppData\Local\Continuum\Anaconda3_64_2020_02\envs\tensor_flow_env_test\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1773, in __init__
    control_input_ops)
  File ""C:\Users\c81441\AppData\Local\Continuum\Anaconda3_64_2020_02\envs\tensor_flow_env_test\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1613, in _create_c_op
    raise ValueError(str(e))
ValueError: Duplicate node name in graph: 'ones'

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7, 64bits
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0. I have tested with different tf/pythons versions and I get the same error
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA
- GPU model and memory: NA
"
43531,Installation via Miniconda fails,"**System information**
- OS Platform and Distribution: Windows 10 Version 2004 (Build 19041.508)
- TensorFlow installed from: with the code `conda install -c conda-forge tensorflow` 
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: yes, with conda
- GPU model and memory: NVIDIA GeForce GTX 860M 

**Describe the problem**
When trying to install the software, the following message appears:

UnsatisfiableError: The following specifications were found
to be incompatible with the existing python installation in your environment:

Specifications:

  - tensorflow -> python[version='3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.6,<3.7.0a0|>=3.7,<3.8.0a0|3.7.*']

Your python: python=3.8

If python is on the left-most side of the chain, that's the version you've asked for.
When python appears to the right, that indicates that the thing on the left is somehow
not available for the python version you are constrained to. Note that conda will not
change your python version to a different minor version unless you explicitly specify
that.

The following specifications were found to be incompatible with your system:

  - feature:/win-64::__cuda==8.0=0
  - feature:|@/win-64::__cuda==8.0=0

Your installed version is: 8.0

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Install Miniconda3 Windows 64-bit (SHA 1f4ff67f051c815b6008f144fdc4c3092af2805301d248b56281c36c1f4333e5)
2. Install Spyder: `conda install -c anaconda spyder`
3. Install matplotlib: `conda install -c conda-forge matplotlib`
4. Install scipy: `conda install -c anaconda scipy`
5. Install sympy: `conda install -c anaconda sympy`
6. Install tensorflow: `conda install -c conda-forge tensorflow`
"
43530,Import error | from pip | At ubuntu 18.04,"System information
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):pypi
- TensorFlow version:2.0.0a0
- Python version:3.6.9
- Installed using virtualenv? pip? conda?:pip
- CUDA/cuDNN version:I don't have a cuda
- GPU model and memory: Atom/Celeron/Pentium Processor x5-E8000/J3xxx/N3xxx Integrated Graphics Controller [1.9G]

### **Describe the problem**
I tried the code below
and I got the output something like this:

>Illegal instruction

 in my bash terminal


###**Provide the exact sequence of commands / steps that you executed before running into the problem**
**I created a new python3 virtual environment and ran the command:**

> $ pip install tensorflow==2.0.0a0

> $ pip install protobuf==3.10 #installed protobuf 

**Then I tested the code using:**

> $ python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
"
43529, 'tf.TensorScatterUpdate' Conversion to tflite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
windows 10
- TensorFlow installed from (source or binary):
python binary 
- TensorFlow version (or github SHA if from source):
2.3


**Provide the text output from tflite_convert**
error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
(im already using TF ops)
```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
43528,Tensorflow 2.3.0 can't detect CUDA on Python 3.7.9,"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version: 2.3.0
- Python version: 3.7.9
- CUDA/cuDNN version: 10.1/7.6
- GPU model and memory: Nvidia GTX 960m

I just discovered compatibility problem between Tensorflow 2.3.0 and Python 3.7.9. So, as you might have noticed, there're A LOT of ""cudart64_X.dll not found"" problems floating around, with standard fixes here and there.

But I just couldn't find any single solution that can fix it. I even did everything cleanly, the CUDA/cuDNN install process. I also make sure my environment vars & PATH are as complete as possible.

Until I downgraded to Python 3.7.1, and the problem magically gone, that's all. This is my steps:

1. Have CUDA/cuDNN installed complete with path & environment.
2. Conda create environment python 3.7 (so I get Python 3.7.9).
3. Activate that new env.
4. pip install tensorflow (so I get v2.3.0).
5. try to import tensorflow, get ""cudart64_101.dll not found"".
6. Conda install python=3.7.1 (so, downgrade).
7. try to import tensorflow, get ""Successfully opened dynamic library cudart64_101.dll"""
43527,"""IndexError: list index out of range"" when load EfficientDet SavedModel with tf.keras","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: No
- GPU model and memory: No

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

 I try to load SavedModel efficientDetD0 with load_model of keras:

`model = tf.keras.models.load_model(""saved_model"")'
'model.summary()`



And following error occur: 

Traceback (most recent call last):
  File ""E:/Detection/test_infer_model.py"", line 129, in <module>
    model = tf.keras.models.load_model(""saved_model"")
  File ""C:\Users\Luxury\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\saving\save.py"", line 187, in load_model
    return saved_model_load.load(filepath, compile, options)
  File ""C:\Users\Luxury\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\saving\saved_model\load.py"", line 121, in load
    path, options=options, loader_cls=KerasObjectLoader)
  File ""C:\Users\Luxury\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\saved_model\load.py"", line 633, in load_internal
    ckpt_options)
  File ""C:\Users\Luxury\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\saving\saved_model\load.py"", line 194, in __init__
    super(KerasObjectLoader, self).__init__(*args, **kwargs)
  File ""C:\Users\Luxury\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\saved_model\load.py"", line 130, in __init__
    self._load_all()
  File ""C:\Users\Luxury\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\saving\saved_model\load.py"", line 221, in _load_all
    self._finalize_objects()
  File ""C:\Users\Luxury\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\saving\saved_model\load.py"", line 526, in _finalize_objects
    _finalize_saved_model_layers(layers_revived_from_saved_model)
  File ""C:\Users\Luxury\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\saving\saved_model\load.py"", line 706, in _finalize_saved_model_layers
    inputs = infer_inputs_from_restored_call_function(call_fn)
  File ""C:\Users\Luxury\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\saving\saved_model\load.py"", line 982, in infer_inputs_from_restored_call_function
    spec = fn.concrete_functions[0].structured_input_signature[0][0]
IndexError: list index out of range
**Standalone code to reproduce the issue **
link colab: https://colab.research.google.com/drive/1fR8nu3INEWVUJJhex3Dline10296vxri?usp=sharing
and model: https://drive.google.com/file/d/18D8NQp9zP4UW06jeG_75YQWyB531CWUj/view?usp=sharing
I lost 2 days for searching on internet, stackoverflow even this repo's issue but no result, please help !
"
43526,Cannot install tensorflow-gpu==2.0.0 by pip,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac
- TensorFlow installed from (source or binary):pip
- TensorFlow version: 2.0.0
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 10.0
- GPU model and memory: p40

**Describe the problem**

I got this error.
```bash
root@dc45fa59c9f0:/workspace# pip install tensorflow-gpu==2.0.0
ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.0.0 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0)
ERROR: No matching distribution found for tensorflow-gpu==2.0.0
```"
43521,TF 2.0 create model-- 'Tensor' object has no attribute 'numpy',"tf version is 2.0,
below is my import packages:
```
import numpy as np
import keras.backend as K
import keras
from keras.engine.topology import Layer
from keras.layers import Input, Reshape,Dense, Activation, Flatten,Dropout,TimeDistributed,dot,concatenate,multiply,Permute,Add
from keras.layers.core import Lambda, Activation
from keras.models import Model
```
below is my code:
```
	inp = Input(shape=(TIME_STEPS, INPUT_DIM,))
	inp_time = Input(shape=(1,))
	print(inp_time.numpy())
```

below is my log
```
Using TensorFlow backend.
Traceback (most recent call last):
  File ""/Users/coolinear/Desktop/tjk_full_atte/demo_tjk_full_atte/model_full_atte.py"", line 71, in <module>
    transformer_model(1251)
  File ""/Users/coolinear/Desktop/tjk_full_atte/demo_tjk_full_atte/model_full_atte.py"", line 32, in transformer_model
    print(inp_time.numpy())
AttributeError: 'Tensor' object has no attribute 'numpy'
```

what can i do, to transform a tensor(there is variable ""inp_time"") to a int?"
43520,Difference between keras and tf.keras,"I found tf.keras is quite different with keras, in keras, bug occur when you use normal function instead Lambda wrapper function, bug is ""AttributeError: 'NoneType' object has no attribute '_inbound_nodes'"", while in tf.keras, it is allowed use normal function and tf.keras.layers. funtion simultaneously.
keras seems use different data structure, create innode between layers.

"
43519,Need use cases to guide tensorflow installation,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/install

## Description of issue (what needs changing):

People want tensorflow for different reasons - to play, to learn from, to deploy in this/that/other environment, to integrate into legacy systems, etc. Not all installations are appropriate to all use cases. So far I can find zero guidance on integrating tensorflow inference module that works in python laboratory environment into Windows .exe desktop application built from C++ source. That is: ""Install on Windows"" is simply too coarse a description that applies to some use cases, but not to others. I've definitely installed something on Windows, and I can definitely use it for python-based explorations... but it is completely unclear to me if I can deploy it in production in the environment I need to, or what exactly the installation contains. I suggest that there needs to be a comprehensive (and growing) list of use cases that lay out how tensorflow can be used in conjunction with various requirements.

Sadly, the modern trend is to offer 2 sentences of introduction, and then say ""sign up and get started"". That's not helpful if it takes 6 months to realise there is either a dead end to the preferred development path, or a prohibitively expensive redeployment  to a new platform required for a legacy product :-( Right now, I simply cannot tell if either of those is a possibility... or something else.

### Correct links

Is the link to the source code correct? n/a

### Parameters defined

Are all parameters defined and formatted correctly? n/a

### Returns defined

Are return values defined? n/a

### Raises listed and defined

Are the errors defined?  n/a

### Usage example

Is there a usage example? n/a

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content? n/a

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? I have no idea what a pull request is, so that would be a no.
"
43518,Keras saved model gives different accuracy compared to the original model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8.2
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: NVidia driver v450.66 CUDA 11.0
- GPU model and memory: 2080ti 11GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
**Describe the expected behavior**

The following code is supposed to output the same accuracy from model and loaded_model, but somehow they're different.
If I run model.predict instead, they're consistent though.

```
import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split

x = np.random.rand(10000, 10)
y = np.random.choice([0, 1], (10000, ))

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)


model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

model.fit(x_train, y_train, epochs=10)

model.save('./test_model/')

loaded_model = tf.keras.models.load_model('./test_model/')

print(model.evaluate(x_test, y_test))
print(loaded_model.evaluate(x_test, y_test))
```


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43517,Getting a “RuntimeError: Expected bias tensor to be a vector” while trying to do full integer quantization on a keras model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Installed via pip
- TensorFlow version (or github SHA if from source): [2.4.0.dev20200923]


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# Copy and paste here the exact command
model = load_model('path_to_model/model_v1.h5', custom_objects = {'ScaleLayer': ScaleLayer, 'ReshapeLayer': ReshapeLayer})

for i,layer in enumerate(model.layers):
    print (layer.trainable, layer.name)

def representative_dataset_gen():
    i = 0
    for i in range(1000, 3190):
        img_crop = cv2.imread('path_to_img/Frame_'
                                + str(i) + '.jpg')
        img_crop = img_crop.astype(np.float32)
        img_cropped = cv2.resize(img_crop, (150, 150))
        print (img_cropped.shape)
        oImage = np.reshape(img_cropped, (1, 150, 150, 3))
        yield [oImage]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

tflite_quant_model = converter.convert()
open('./converted/full_int_quant.tflite', ""wb"").write(tflite_quant_model)
```

**The output from the converter invocation**

```
# Copy and paste the output here.
File ""main.py"", line 77, in <module>
    main(parse_arg(sys.argv[1:]))
  File ""main.py"", line 46, in main
    tflite_quant_model = converter.convert()
  File ""/home/anand/virtual_environs/env_16.04_py3.6/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 892, in convert
    self).convert(graph_def, input_tensors, output_tensors)
  File ""/home/anand/virtual_environs/env_16.04_py3.6/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 650, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File ""/home/anand/virtual_environs/env_16.04_py3.6/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 478, in _calibrate_quantize_model
    inference_output_type, allow_float, activations_type)
  File ""/home/anand/virtual_environs/env_16.04_py3.6/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 98, in calibrate_and_quantize
    np.dtype(activations_type.as_numpy_dtype()).num)
RuntimeError: Expected bias tensor to be a vector.

```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.

[model_v1.txt](https://github.com/tensorflow/tensorflow/files/5273123/model_v1.txt)

```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43516,TF 2.0 create model-- 'Tensor' object has no attribute 'numpy',"```
1 source code
-------------------
inp = Input(shape=(TIME_STEPS, INPUT_DIM,))
inp_time = Input(shape=(1,))
print(tf.executing_eagerly())  # return True

print(inp_time.numpy()) # error
# inp_time=inp_time.eval()/ K.get_value()/ x.numpy()/ K.eval()
x = ConvAtteShare(inp_time=inp_time.numpy(),kernel=(32,32),stride=(16,16),filter_num=3)(inp)
x = Dense(kind_num,activation='softmax')(x)
m = Model(inputs=[inp,inp_time], outputs=[x], name='convatte-test')
print(m.summary())

2 error
----------------------------
print(inp_time.numpy())
AttributeError: 'Tensor' object has no attribute 'numpy'
```"
43515,Pybadge error compiling example sketch micro_speech_arcarda,"@tensorflow/micro

**System information**
Windows 10 laptop
- TensorFlow installed from (source or binary): Both Arduino_TensorflowLite 2.1.0-ALPHA and Adafruit Tensorflow Lite 1.2.1 fronm the Arduino IDE libraries
- Tensorflow version (commit SHA if source): dont know 
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Pybadge M4 with external microphone

**Describe the problem**
got the following compile error:
Arduino: 1.8.13 (Windows 10), Board: ""Adafruit pyBadge M4 Express (SAMD51), Enabled, 180 MHz (overclock), Fastest (-Ofast), 50 MHz (standard), TinyUSB, Off""
Multiple libraries were found for ""Adafruit_ZeroDMA.h""
Used: C:\Users\Joanna\AppData\Local\Arduino15\packages\adafruit\hardware\samd\1.6.2\libraries\Adafruit_ZeroDMA
In file included from C:\Users\Joanna\Documents\Arduino\libraries\Adafruit_TensorFlow_Lite\examples\micro_speech_arcada\micro_speech_arcada.ino:22:
Not used: C:\Users\Joanna\Documents\Arduino\libraries\Adafruit_Zero_DMA_Library
audio_provider.h:19:10: fatal error: tensorflow/lite/c/c_api_internal.h: No such file or directory
19 | #include ""tensorflow/lite/c/c_api_internal.h""
| ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
exit status 1
tensorflow/lite/c/c_api_internal.h: No such file or directory


**Please provide the exact steps when you ran into the problem**
Setup new Arduino IDE
Added Adafruit https://adafruit.github.io/arduino-board-index/package_adafruit_index.json
to boards manager url in preferences
Added Adafruit SAMD Boards 1.6.2
Added all Adafruit arcada libraries 2.5.0
Loaded Examples > Adafruit Tensorflow Lite > micro_speech_arcarda.

Got the attached error

"
43514,Is there a tf-serving document updated for tf2 ?,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
[https://www.tensorflow.org/tfx/serving/serving_advanced](https://www.tensorflow.org/tfx/serving/serving_advanced)

## Description of issue (what needs changing):

This documment use tf1 function , Is there a new version for tf2 ?"
43512,einsum for SparseTensor,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.2.0
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
Multiply two tensors that are not necessarily sparse matrices (2D tensors). For example, a 'tf.einsum' like function available for  sparse tensors. Converting sparse tensors to dense tensors must be avoided.

An example: Multiply A and B where A is a 2D dense tensor and B is a 3D sparse tensor as follows: C[i,j] = \sum_k A[i,k] B[i,k,j]
```
# Example
# inputs
A = tf.constant([[1, 2, 3],
                 [4, 5, 6],
                 [7, 8, 9]], tf.float32)
B = tf.sparse.SparseTensor(values=tf.constant([1,1,1,1,2,1,1,1,1,1,-1], tf.float32),indices=[[0,0,1],[0,1,2],[0,2,0],[1,0,0],[1,1,1],[1,1,2],[1,2,2],[2,0,2],[2,1,1],[2,2,1],[2,2,2]], dense_shape=[3,3,3])


# output
C = tf.constant([[3, 1, 2],
                 [4, 10, 11],
                 [9, 8, -1]], tf.float32)
```



**Will this change the current api? How?**
A 'tf.sparse.einsum(A,B,'...', a_sparse = False, b_sparse = True)' kind of function would be very useful for dealing with this kind of operations. 

**Who will benefit with this feature?**
Developers that need to deal with sparse tensor multiplications in high dimensions (grater than 2D sparse tensors)

**Any Other info.**
"
43510,keyword benchmark broken for bluepill with TAGS=cmsis-nn,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): c8109b89a2bf7fce60680682794f6ff4ca25de1b
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): bluepill

Command:
```
make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=bluepill TAGS=cmsis-nn keyword_benchmark
```

Error:
```
tensorflow/lite/micro/tools/make/downloads/cmsis//CMSIS/NN/Source/BasicMathFunctions/arm_elementwise_mul_s8.c: In function 'arm_elementwise_mul_s8':
tensorflow/lite/micro/tools/make/downloads/cmsis//CMSIS/NN/Source/BasicMathFunctions/arm_elementwise_mul_s8.c:178:21: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
   while (loop_count > 0U)
                     ^
cc1: all warnings being treated as errors
```

This was not caught by our CI system because we do not build the benchmarks. PR #43509 should allow us to catch these issues before they are merged.

"
43508,tensorflow build and install,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
43503,"Training AlexNet structure on entire ImageNet in Tensorflow 2.0, But why the loss and accuracy are all flat?","I am trying to replicate AlexNet model on ImageNet (for learning purposes). The dataset is 1.2 million ImageNet dataset with 50K validation. 
Model is a sequential keras CNN model, and I am training this model on 8 GPUs. 

The code can be seen here:

[Code on GitHub](https://github.com/anejad/Convolutional-Neural-Network-Champions/blob/master/AlexNet/AlexNet_Tensorflow_Full.py).

However, when I start to train this model (8 GPUs, Batch size=32*nGPU) I get flat loss and accuracy:
Epoch 14/90
5004/5004 [==============================] - 748s 150ms/step - loss: 6.9080 - accuracy: 8.8055e-04 - val_loss: 6.9078 - val_accuracy: 9.6154e-04
Epoch 15/90
5004/5004 [==============================] - 747s 149ms/step - loss: 6.9080 - accuracy: 9.6798e-04 - val_loss: 6.9078 - val_accuracy: 3.2051e-04
Epoch 16/90
5004/5004 [==============================] - 745s 149ms/step - loss: 6.9080 - accuracy: 0.0010 - val_loss: 6.9077 - val_accuracy: 0.0013
Epoch 17/90
5004/5004 [==============================] - 748s 149ms/step - loss: 6.9080 - accuracy: 9.5559e-04 - val_loss: 6.9078 - val_accuracy: 8.0128e-04
Epoch 18/90
5004/5004 [==============================] - 749s 150ms/step - loss: 6.9080 - accuracy: 8.8055e-04 - val_loss: 6.9078 - val_accuracy: 0.0014
Epoch 19/90
5004/5004 [==============================] - 750s 150ms/step - loss: 6.9080 - accuracy: 9.4924e-04 - val_loss: 6.9078 - val_accuracy: 0.0013
Epoch 20/90
5004/5004 [==============================] - 750s 150ms/step - loss: 6.9080 - accuracy: 9.3051e-04 - val_loss: 6.9078 - val_accuracy: 0.0016
Epoch 21/90
5004/5004 [==============================] - 748s 149ms/step - loss: 6.9080 - accuracy: 0.0010 - val_loss: 6.9078 - val_accuracy: 0.0011
Epoch 22/90
5004/5004 [==============================] - 749s 150ms/step - loss: 6.9080 - accuracy: 9.8047e-04 - val_loss: 6.9078 - val_accuracy: 0.0014
Epoch 23/90
5004/5004 [==============================] - 748s 149ms/step - loss: 6.9080 - accuracy: 7.3067e-04 - val_loss: 6.9078 - val_accuracy: 9.6154e-04
Epoch 24/90
5004/5004 [==============================] - 750s 150ms/step - loss: 6.9080 - accuracy: 0.0011 - val_loss: 6.9078 - val_accuracy: 0.0014
Epoch 25/90
5004/5004 [==============================] - 750s 150ms/step - loss: 6.9080 - accuracy: 0.0010 - val_loss: 6.9078 - val_accuracy: 1.6026e-04

Any ideas how to fix this or what causes the learning to be flat???



"
43502,keras.layers.experimental.preprocessing.Discretization fails for mixed precision training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Databricks Runtime 7.3
- TensorFlow installed from (source or binary):  binary
- TensorFlow version (use command below): 2.3
- Python version: 3
- CUDA/cuDNN version: 10.1
- GPU model and memory: AWS p3.xlarge

**Describe the current behavior**
keras.layers.experimental.preprocessing.Discretization fails when mixed precision is enabled

Errors:
```
TypeError: Value passed to parameter 'input' has DataType float16 not in list of allowed values: int32, int64, float32, float64
tensorflow/python/keras/layers/preprocessing/discretization.py in call(self, inputs)
     99           dense_shape=array_ops.identity(inputs.dense_shape))
    100     else:
--> 101       return math_ops._bucketize(inputs, boundaries=self.bins)  # pylint: disable=protected-access
````

```python
raw_input = keras.Input(shape=1, name=""raw_input"")
boundaries = [0, 1, 2, 3]
discretization = keras.layers.experimental.preprocessing.Discretization(bins=boundaries)(raw_input)
```

This appears to come from tf.math.bucketize()

**Describe the expected behavior**
The replacement for TF Feature Columns should support mixed-precision training out of the box.


**Standalone code to reproduce the issue**
See above.

**Other info / logs** Include any logs or source code that would be helpful to
Work around:
```
raw_input = keras.Input(shape=1, name=""raw_input"")
boundaries = [0, 1, 2, 3]
discretization = keras.layers.experimental.preprocessing.Discretization(bins=boundaries, autocast=False)(raw_input)
```

However, I am having issues with serialization."
43501,call back error ,"WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9fb782eee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
this the model
Model: ""sequential""


this the error 

my code is 
prediction(consine):
new_model = tf.keras.models.load_model(""model.h5"")
print(new_model.summary())
test_output = new_model.predict(cosine_s, verbose=0)
 return test_output


I make call to this function every time i received new data.  "
43500,Official Tensorflow build for s390x CPU,"There is currently a Tensorflow on Linux s390x CPU Stable Release as well as a Tensorflow on Linux s390x Nightly build. Both under the category of community builds. 

User story:
As a developer Tensorflow user, I want to be able to work with an official Tensorflow build for s390x so I can have certainty on the lifecycle and I can communicate to my end users (enterprises) that I work with an official Tensorflow version.

There is a growing demand for ML applications in the s390x arch platform, recent feedback from large enterprise customers like banks are working on ML models in the same platforms where their mission-critical apps are, s390x.

"
43498,ValueError: Unknown layer when i am loading of the model using tf.keras.models.load_model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): True
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow version (use command below): Latest

Hi,

I am facing a model loading issue using  `tf.keras.models.load_model`.

I saved custom keras model name- CustomModel

`model.save('model.h5')`

and then I try to load
1. 
`new_model = keras.models.load_model('model.h5', custom_objects={'CustomModel': CustomModel})`

**Error**

> usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)
> 294 cls = get_registered_object(class_name, custom_objects, module_objects)
> 295 if cls is None:
> --> 296 raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)
> 297
> 298 cls_config = config['config']
> 
> ValueError: Unknown layer: Mean

2. 
`new_model = keras.models.load_model('model.h5', custom_objects={'CustomModel': CustomModel, 'Mean':keras.metrics.Mean})`

> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in load_weights_from_hdf5_group(f, layers)
> 684 'containing ' + str(len(layer_names)) +
> 685 ' layers into a model with ' + str(len(filtered_layers)) +
> --> 686 ' layers.')
> 687
> 688 # We batch weight value assignments in a single backend call
> 
> ValueError: You are trying to load a weight file containing 2 layers into a model with 3 layers.


I made a dummy notebook. https://colab.research.google.com/drive/1GH-WxghmkhgLSO2vVcSKgQ4UI3WX6GKC?usp=sharing

It looks like a bug in tf.keras.models.load_model."
43497,einsum for SparseTensors,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac
- TensorFlow installed from (source or binary): Source
- TensorFlow version (or github SHA if from source): 2.2.0

**Provide the text output from tflite_convert**
I do not find an 'optimized' way to compute a multiplication between an sparse and a dense tensor that are not matrices.
For example, I would like to perform the following multiplication between a 3D sparse tensor A and a 2D dense tensor B:
C[i,j] = \sum_k B[i,k] A[i,k,j]

IMPORTANT! A needs to maintain its sparsity. Functions like tf.sparse.to_dense() must be avoided!
```
# Example
# inputs
A = tf.sparse.SparseTensor(values=tf.constant([1,1,1,1,2,1,1,1,1,1,-1], tf.float32),indices=[[0,0,1],[0,1,2],[0,2,0],[1,0,0],[1,1,1],[1,1,2],[1,2,2],[2,0,2],[2,1,1],[2,2,1],[2,2,2]], dense_shape=[3,3,3])
B = tf.constant([[1, 2, 3],
                 [4, 5, 6],
                 [7, 8, 9]], tf.float32)

# output
C = tf.constant([[3, 1, 2],
                 [4, 10, 11],
                 [9, 8, -1]], tf.float32)
```
A 'tf.sparse.einsum(B,A, 'ik,ikj->ij')'-like function would be nice to reproduce the behaviour of einsum for dense tensors in sparse tensors.
"
43496,[Keras] Loading a model with custom layers in savedmodel format is very slow,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary pip
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I'm saving a keras model in savedmodel and in h5. While in H5 the loading takes 3 seconds it can take up to 15 seconds in savedmodel format. I also noticed that reloading the keras model as a savedmodel instead of keras is faster (in our private use case 6seconds against 15 seconds with keras).

We also noticed that the loading time depends on the platform and can vary given the number of cores available. 

**Describe the expected behavior**
I would expect a faster loading for both tf.keras.models.load_model('tf') and tf.saved_model.load. 

**Standalone code to reproduce the issue**
https://colab.research.google.com/gist/tanguycdls/4ef12115456f788c61771ab4fef5fdbb/untitled.ipynb
The model is:

````
inps = []
for name in range(30):
  inps.append(Input(ragged=True, shape=(None,), name=str(name)))
class SumRagged(tf.keras.layers.Layer):
    def call(self, input):
        return tf.reduce_sum(input, axis=1)
out_summed = [SumRagged()(Embedding(10, 2)(inp)) for inp in inps]
concat = Dense(1)(Concatenate()(out_summed))
model = tf.keras.models.Model(inps, concat)
````
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
In the current benchmarks:

### saving:

- savedmodel : 9.98 s per loop
- H5: 55.7 ms per loop

### Loading:
- savedmodel (keras api): 5 seconds
- savedmodel: 3 seconds
- H5: 56ms.
"
43495,Cannot convert CenterNet+KeyPoints Model Zoo tf2 models to tflite,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 LTS
- TensorFlow installed from (source or binary): binary (tf-nightly)
- TensorFlow version (use command below): 2.4.0-dev20200907
- Python version: 3.6.9
- CUDA/cuDNN version: 11.0/8.0

**Describe the current behavior**

When I try to convert a model from TensorFlow 2 Detection Model Zoo into a tflite version, I get some error.

[EDIT] This is only the case for models using CenterNet AND KeyPoints. All other models are OK

**Describe the expected behavior**

Convertion is successfull

**Standalone code to reproduce the issue**

Download ""CenterNet Resnet50 V1 FPN Keypoints 512x512"" from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md

Uncompress the archive

Run ""tflite_convert --saved_model_dir=centernet_resnet50_v1_fpn_512x512_kpts_coco17_tpu-8/saved_model --output_file=model.tflite""

I got the following error:

loc(callsite(callsite(""map/TensorArrayV2_2@__inference___call___13919"" at ""StatefulPartitionedCall@__inference_signature_wrapper_15800"") at ""StatefulPartitionedCall"")): error: requires element_shape to be 1D tensor during TF Lite transformation pass
loc(callsite(callsite(""map/TensorArrayV2_2@__inference___call___13919"" at ""StatefulPartitionedCall@__inference_signature_wrapper_15800"") at ""StatefulPartitionedCall"")): error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
Traceback (most recent call last):
  File ""/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 199, in toco_convert_protos
    enable_mlir_converter)
  File ""/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/wrap_toco.py"", line 38, in wrapped_toco_convert
    enable_mlir_converter)
Exception: <unknown>:0: error: loc(callsite(callsite(""map/TensorArrayV2_2@__inference___call___13919"" at ""StatefulPartitionedCall@__inference_signature_wrapper_15800"") at ""StatefulPartitionedCall"")): requires element_shape to be 1D tensor during TF Lite transformation pass
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: note: loc(callsite(callsite(""map/TensorArrayV2_2@__inference___call___13919"" at ""StatefulPartitionedCall@__inference_signature_wrapper_15800"") at ""StatefulPartitionedCall"")): see current operation: %456 = ""tf.TensorListReserve""(%67, %77) {device = """"} : (tensor<i32>, tensor<i32>) -> tensor<!tf.variant<tensor<*xf32>>>
<unknown>:0: error: loc(callsite(callsite(""map/TensorArrayV2_2@__inference___call___13919"" at ""StatefulPartitionedCall@__inference_signature_wrapper_15800"") at ""StatefulPartitionedCall"")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: note: loc(callsite(callsite(""map/TensorArrayV2_2@__inference___call___13919"" at ""StatefulPartitionedCall@__inference_signature_wrapper_15800"") at ""StatefulPartitionedCall"")): see current operation: %456 = ""tf.TensorListReserve""(%67, %77) {device = """"} : (tensor<i32>, tensor<i32>) -> tensor<!tf.variant<tensor<*xf32>>>


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/biroute/.local/bin/tflite_convert"", line 8, in <module>
    sys.exit(main())
  File ""/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py"", line 640, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/biroute/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/biroute/.local/lib/python3.6/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/biroute/.local/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py"", line 623, in run_main
    _convert_tf2_model(tflite_flags)
  File ""/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py"", line 239, in _convert_tf2_model
    tflite_model = converter.convert()
  File ""/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 726, in convert
    output_tensors)
  File ""/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 643, in convert
    **converter_kwargs)
  File ""/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 573, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py"", line 202, in toco_convert_protos
    raise ConverterError(str(e))
tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(callsite(""map/TensorArrayV2_2@__inference___call___13919"" at ""StatefulPartitionedCall@__inference_signature_wrapper_15800"") at ""StatefulPartitionedCall"")): requires element_shape to be 1D tensor during TF Lite transformation pass
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: note: loc(callsite(callsite(""map/TensorArrayV2_2@__inference___call___13919"" at ""StatefulPartitionedCall@__inference_signature_wrapper_15800"") at ""StatefulPartitionedCall"")): see current operation: %456 = ""tf.TensorListReserve""(%67, %77) {device = """"} : (tensor<i32>, tensor<i32>) -> tensor<!tf.variant<tensor<*xf32>>>
<unknown>:0: error: loc(callsite(callsite(""map/TensorArrayV2_2@__inference___call___13919"" at ""StatefulPartitionedCall@__inference_signature_wrapper_15800"") at ""StatefulPartitionedCall"")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: note: loc(callsite(callsite(""map/TensorArrayV2_2@__inference___call___13919"" at ""StatefulPartitionedCall@__inference_signature_wrapper_15800"") at ""StatefulPartitionedCall"")): see current operation: %456 = ""tf.TensorListReserve""(%67, %77) {device = """"} : (tensor<i32>, tensor<i32>) -> tensor<!tf.variant<tensor<*xf32>>>
"
43494,Bug in tf.math.rsqrt,"Hello!

On TF 2.3, about the function `tf.math.rsqrt(x, name=None)` the documentation says:

>x | A tf.Tensor. Must be one of the following types: bfloat16, half, float32, float64. int32

When run with `int32` type:

```
import tensorflow as tf

x = tf.constant([2, 0, -2])
tf.math.rsqrt(x)
```

I get the following error:
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of int32 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128
        ; NodeDef: {{node Rsqrt}}; Op<name=Rsqrt; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]> [Op:Rsqrt]
```

The list raised by the error and the list given in the documentation are not the same. Then either it is an error in the documentation or a bug in the implementation of the function.

Thanks.

"
43493,TensorFlow build is failing on Bazel CI (Release Bazel) for MacOS and Windows,"https://buildkite.com/bazel/tensorflow/builds/5772#d0cb252a-2fab-4c9c-aea3-a1b9e914ea15

Windows seems to have been failing since Sep 18 2020
MacOS seems to have been failing since Sep 19 2020

"
43492,failed to build pip windows 10,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source(master)
- TensorFlow version: source(master)
- Python version: 3.8.4
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 3.1
- CUDA/cuDNN version: 11
- GPU model and memory: 1060super

```
 ERROR: G:/tensorflow/tensorflow/core/grappler/BUILD:168:1: C++ compilation of rule '//tensorflow/core/grappler:grappler_item_builder' failed (Exit 2): python.exe failed: error executing command
  cd F:/users/sonfire/_bazel_sonfire/ic7qrvhc/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.27.29110\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.27.29110\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt
    SET LD_LIBRARY_PATH=C:Program FilesNVIDIA GPU Computing ToolkitCUDAv11.0lib
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.27.29110\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.27.29110\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\FSharp\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=F:/Users/sonfire/AppData/Local/Programs/Python/Python38-32/python.exe
    SET PYTHON_LIB_PATH=F:/Users/sonfire/AppData/Local/Programs/Python/Python38-32/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=F:\Users\sonfire\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_NEED_CUDA=1
    SET TMP=F:\Users\sonfire\AppData\Local\Temp
```


`Target //tensorflow/tools/pip_package:build_pip_package failed to build`
"
43491,tf.dynamic_stitch gives wrong shape and outputs raw memory,"**System information**
- Linux Ubuntu 20.04
- TensorFlow 2.3.0 installed from binary
- Python version 3.8.2 (default, Jul 16 2020, 14:00:26) [GCC 9.3.0]
- no GPU available

**Describe the current behavior**
When stitching empty partitions, the output shape is wrong.
```python
>>> tf.dynamic_stitch([1], [1.2])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2.4e-44, 1.2e+00], dtype=float32)>
```
The `2.4e-44` is from uninitialized memory and will typically vary per execution. There is nothing special about 1.2

**Describe the expected behavior**
From documentation: `merged[indices[m], ...] = data[m][...]` so we would expect:
```python
>>> tf.dynamic_stitch([1], [1.2])
<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.2], dtype=float32)>
```

**Extra**
I don't know if it's a security concern, but the amount of data read can be arbitrarily large:
```python
size = 8
>>> tf.dynamic_stitch([1], [tf.zeros([1, size])])[0]
<tf.Tensor: shape=(8,), dtype=float32, numpy=
array([-2.8330522e-19,  4.5886920e-41, -2.8330522e-19,  4.5886920e-41,
        6.6383240e-07,  1.6802996e-04,  1.7299214e-04,  4.3915941e-05],
      dtype=float32)>
```
Edit: simplified the code"
43490,Feature Request: Support sparse to dense with max_length input,"Using tensorflow 2.3, some times I found I need tfrecords with tf.io.VarlenFeature, but when using tpu or for some other reasons you might need to pad it to fixed max_length like 50, 200(fixed) not just using padded_batch or tf.sparse.to_dense to padd it to the max length in the batch(dynamic).

It is ok if you pre padding to max_length when generating tfrecords but that is wasting space and also reduce flexbilty.

I'd like padded_batch(x, max_length), tf.sparse.to_dense(x, max_length) or just set tf.io.VarlenFeature(max_length).

Will this feature be possible ? Thanks!

"
43489,map_and_batch() is way faster than map().batch() ???,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tensorflow-gpu 2.2.0
- Python version: Python 3.5.2
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
- CUDA/cuDNN version:  CUDA Version: 10.1 , libcudnn7-dev 7.6.5.32-1
- GPU model and memory: NVIDIA Tesla V100, Memory 32480MiB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When training ResNet-50 with ImageNet (in TFRecord), the time per epoch with ""map_and_batch()"" is 3 times faster than using "".map().batch()"".
I know ""map_and_batch()"" will be depracated (https://www.tensorflow.org/api_docs/python/tf/data/experimental/map_and_batch?hl=en). 
But if this is faster and working just fine, then why ??

**Describe the expected behavior**
Accoridng to some previous posts, these two API should have similar performance.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
On the same machine, when i use 
dataset = dataset.apply(tf.data.experimental.map_and_batch(
    lambda value: parse_record_fn(value, is_training, dtype),
    batch_size=batch_size))
the results are:
Epoch 1/30
2859/2859 [==============================] - 2638s 923ms/step - accuracy: 0.5294 - loss: 2.9772 - val_accuracy: 0.6064 - lr: 0.1000 - val_loss: 2.6385
Epoch 2/30
1700/2859 [================>.............] - ETA: 17:26 - accuracy: 0.6093 - loss: 2.6254

However, when we use
dataset = dataset.map(lambda value: parse_record_fn(value, is_training, dtype))
dataset = dataset.batch(batch_size=batch_size)
the results are:
Epoch 1/30
2859/2859 [==============================] - 6732s 2s/step - loss: 2.9723 - accuracy: 0.5304 - lr: 0.1000 - val_loss: 2.6656 - val_accuracy: 0.5948
Epoch 2/30
2110/2859 [=====================>........] - ETA: 27:56 - loss: 2.6110 - accuracy: 0.6128"
43487,Add support for dataset to pandas dataframe,"**System information**
- TensorFlow version (you are using): 2.3
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**

There is a feature for numpy conversion, but none for a conversion to a pandas dataframe. 

**Will this change the current api? How?**

It will add a function to do this, could be called as_dataframe()

**Who will benefit with this feature?**

Anyone who uses pandas over numpy or prefers pandas dataframes over numpy arrays. 

**Any Other info.**

Inspired by the feature in the datasets package:
https://www.tensorflow.org/datasets/api_docs/python/tfds/as_dataframe
"
43483,Reusing Keras.Model errors,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: None
- GPU model and memory: None


**Describe the current behavior**
The code below throws an exception on training.

**Describe the expected behavior**
No exception expected.


**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1KYTl8T_8_dZgqFcdSd6QIn9lEB9d7CPU?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to

See in the colab notebook. The exception thrown is:
InvalidArgumentError: You must feed a value for placeholder tensor 'ftrs' with dtype float and shape [?,10]
	 [[{{node ftrs}}]]

"
43481,Uplampling2D args:interpolation='nearest' isn't supported in TFLite or TFLite converter,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04
- TensorFlow installed from (source or binary):binary
- TensorFlow version (or github SHA if from source):2.2.0

Tensorflow installed using pip3 insall tensorflow==2.2.0
$pip3 show tensorflow
```
Name: tensorflow
Version: 2.2.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/nymble/.local/lib/python3.6/site-packages
Requires: wrapt, google-pasta, gast, protobuf, six, opt-einsum, absl-py, astunparse, tensorboard, scipy, keras-preprocessing, grpcio, wheel, tensorflow-estimator, numpy, termcolor, h5py

```
Model code :
```
import tensorflow as tf
from  tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras.optimizers import SGD, Adam
#-----------------------------------------------------##
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), padding='same', input_shape=(224, 224, 3)))

model.add(layers.Conv2D(32, (3, 3), padding='same'))
#model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.UpSampling2D((2, 2), interpolation='nearest'))
model.add(layers.Flatten())
model.add(layers.Dense(1, activation='sigmoid'))

model.summary()

```
Model's summary looks like this: 
```
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 224, 224, 32)      896       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 224, 224, 32)      9248      
_________________________________________________________________
activation (Activation)      (None, 224, 224, 32)      0         
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 112, 112, 32)      0         
_________________________________________________________________
up_sampling2d (UpSampling2D) (None, 224, 224, 32)      0         
_________________________________________________________________
flatten (Flatten)            (None, 1605632)           0         
_________________________________________________________________
dense (Dense)                (None, 1)                 1605633   
=================================================================
Total params: 1,615,777
Trainable params: 1,615,777
Non-trainable params: 0
__________________________
```

Trained this model and saved it as sample_model.h5. 
Loaded this model again in a separate code using:
```

model_dir = '/home/nymble'
nymble_model = models.load_model(os.path.join(model_dir,'sample_model.h5'))
print(""Model loaded"")

converter = tf.lite.TFLiteConverter.from_keras_model(nymble_model)
#converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
tflmodel = converter.convert()

```
Running this code gives error at 6th line : 
```

Exception: /home/nymble/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:865:9: error: 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op
        self._initialize(args, kwargs, add_initializers_to=initializers)
        ^
/home/nymble/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:959:5: note: called from
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
    ^
/home/nymble/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py:435:5: note: called from
    concrete_func = func.get_concrete_function()
    ^
/media/nymble/Nymble_Storage/ML_Data/my_passport/keras_data/convert_keras_to_tflite.py:20:1: note: called from
converter = tf.lite.TFLiteConverter.from_keras_model(nymble_model)
^
<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag): ResizeNearestNeighbor.
```

Also if i use 'bilinear' interpolation parameter in Upsampling2D, Tflconverter doesn't raises this error.

Attached is the sample_mdoel.h5 graph to look into.

[sample_model.zip](https://github.com/tensorflow/tensorflow/files/5267661/sample_model.zip)
"
43480,[RNN] Invoke the tflite model for inference with dynamic batchsize,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): tf-nightly(2.4.0.dev20200826)
- Python version:  3.7.7
- Bazel version (if compiling from source): no
- GCC/Compiler version (if compiling from source): no
- CUDA/cuDNN version: 7.6.5
- GPU model and memory: GTX1050/2G


**Describe the current behavior**


When I want to invoke the tflite_model for inference(with dynamic batchsize)，
An error occurred：


```
RuntimeError: tensorflow/lite/kernels/concatenation.cc:76 t->dims->data[d] != t0->dims->data[d] (13 != 1)Node number 23 (CONCATENATION) failed to prepare.
Node number 10 (WHILE) failed to invoke.
```

**Describe the expected behavior**

Inference with dynamic batchsize

**Standalone code to reproduce the issue**

**_Here is the link to my Colab to reproduce the issue:_**

[https://colab.research.google.com/drive/13fr-C53JjRxIKFC9d96H9iwwexkGeH2O?usp=sharing](url)

Also here is the code segment where issure occured:
```
# Run the model with TensorFlow Lite
interpreter = tf.lite.Interpreter(model_content=tflite_model)

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

PREDICT_BATCH_SIZE = 13

# resize the input tensor
interpreter.resize_tensor_input(input_details[0]['index'], (PREDICT_BATCH_SIZE,28,28))
interpreter.allocate_tensors()

interpreter.set_tensor(input_details[0][""index""], x_test[0:PREDICT_BATCH_SIZE, :, :])
interpreter.invoke()
result = interpreter.get_tensor(output_details[0][""index""])

print(result)

interpreter.reset_all_variables()
```

Here is the error occured:

```
RuntimeError: tensorflow/lite/kernels/concatenation.cc:76 t->dims->data[d] != t0->dims->data[d] (13 != 1)Node number 23 (CONCATENATION) failed to prepare.
Node number 10 (WHILE) failed to invoke.
```

**Other info / logs**  the whole error:

```
RuntimeError                              Traceback (most recent call last)
<ipython-input-28-6198c0bfcdb3> in <module>()
     13 
     14 interpreter.set_tensor(input_details[0][""index""], x_test[0:PREDICT_BATCH_SIZE, :, :])
---> 15 interpreter.invoke()
     16 result = interpreter.get_tensor(output_details[0][""index""])
     17 

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in invoke(self)
    537     """"""
    538     self._ensure_safe()
--> 539     self._interpreter.Invoke()
    540 
    541   def reset_all_variables(self):

RuntimeError: tensorflow/lite/kernels/concatenation.cc:76 t->dims->data[d] != t0->dims->data[d] (13 != 1)Node number 23 (CONCATENATION) failed to prepare.
Node number 10 (WHILE) failed to invoke.
```
"
43479,Installing Tensorflow,"(tensoflow) C:\Users\Compaq>python
Python 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Compaq\anaconda3\envs\tensoflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Compaq\anaconda3\envs\tensoflow\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Compaq\anaconda3\envs\tensoflow\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\Compaq\anaconda3\envs\tensoflow\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\Users\Compaq\anaconda3\envs\tensoflow\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Compaq\anaconda3\envs\tensoflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Compaq\anaconda3\envs\tensoflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
43478,tf2.3 keras.models.load_model setting compile=False fails to load saved_model but tf2.0 works.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Use tensorflow Addons
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac (Can be reproduced on colab)
- TensorFlow installed from (source or binary):
pip
- TensorFlow version (use command below):
2.3 fails 2.0works
- Python version:
python3

**Describe the current behavior**

I use F1score from addons as the metric. After training, I  use `keras.models.load_model`  to  load the saved_model  and also set `compile=False`.  I got an error. 
```
ValueError: Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements `get_config`and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.
```
This **happens with tf2.3**, but **works with tf2.0**.

**Describe the expected behavior**

If `compile=False` is set, it shouldn't check the metrics or losses.


**Standalone code to reproduce the issue**

- CODE

```
import numpy as np
import tensorflow as tf
import tensorflow_addons as tfa

print(tf.__version__)

_input = tf.keras.layers.Input(shape=(500), name=""fbank"") # B*T*F*c
out = tf.keras.layers.Dense(50, activation=""tanh"")(_input)
probabilities = tf.keras.layers.Dense(2, activation=""softmax"")(out)
model = tf.keras.Model(inputs=_input, outputs=probabilities)

model.compile(optimizer=""sgd"", loss=tf.keras.losses.CategoricalCrossentropy(), 
              metrics= [""accuracy"", tfa.metrics.F1Score(num_classes=2, average=""micro"")])

model.summary()

x=np.random.rand(300,500)
y=np.random.rand(300,2)
model.fit(x,y,batch_size=100, epochs=2)

path = 'saved_model/'
model.save(path, save_format='tf')

del model
model = tf.keras.models.load_model('saved_model', compile=False)

```

- OUTPUT

```
2.3.0
Model: ""functional_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
fbank (InputLayer)           [(None, 500)]             0         
_________________________________________________________________
dense (Dense)                (None, 50)                25050     
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 102       
=================================================================
Total params: 25,152
Trainable params: 25,152
Non-trainable params: 0
_________________________________________________________________
Epoch 1/2
3/3 [==============================] - 0s 4ms/step - loss: 0.7292 - accuracy: 0.5033 - f1_score: 0.0000e+00
Epoch 2/2
3/3 [==============================] - 0s 4ms/step - loss: 0.7192 - accuracy: 0.5200 - f1_score: 0.0000e+00
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
INFO:tensorflow:Assets written to: saved_model/assets
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-9ff0edc2f186> in <module>()
     23 
     24 del model
---> 25 model = tf.keras.models.load_model('saved_model', compile=False)

8 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/load.py in revive_custom_object(identifier, metadata)
    844                      'and `from_config` when saving. In addition, please use '
    845                      'the `custom_objects` arg when calling `load_model()`.'
--> 846                      .format(identifier))
    847 
    848 

ValueError: Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements `get_config`and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.
```

**colab**

https://colab.research.google.com/drive/17DI2N1L9EKSJ8-Ua88mcSnkmRT5adna3?usp=sharing


"
43476,Significant accuracy drop during conversion of DistillBert.,"**System information**
- OS: Ubuntu 18.04.5 LTS (x86_64)
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Command used to run the converter or code if you’re using the Python API**

```
converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)
# For normal conversion:
#converter.experimental_new_converter = True
#converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
```

**The output from the converter invocation**

```
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f6cc9b669e8>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f6c426cba58>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f6c3c4fd080>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f6c3c50d668>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f6c3c519c50>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f6c3c4b5278>, because it is not built.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
INFO:tensorflow:Assets written to: /tmp/tmpsca4zbmo/assets
INFO:absl:Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False
```

**Also, please include a link to the saved model or GraphDef**

```
https://huggingface.co/distilbert-base-multilingual-cased#.
```

**Failure details**
- Producing wrong results and/or decrease in accuracy
The original model produces output:
```
TFBaseModelOutput([('last_hidden_state',
                    <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=
                    array([[[ 0.07324436, -0.13601898,  0.24253392, ...,  0.2757071 ,
                              0.05075948, -0.067513  ],
                            [-0.00969718, -0.6601796 ,  0.07741702, ...,  0.20618156,
                             -0.3217669 ,  0.25172737],
                            [ 0.28169543, -0.36096776, -0.07832485, ...,  0.00399976,
                              0.07738406, -0.06696145],
                            ...,
                            [ 0.04910686, -0.37189117,  0.33814144, ...,  0.03981559,
                              0.14327443, -0.20109934],
                            [ 0.07308616, -0.35554424,  0.33334315, ...,  0.08324002,
                              0.1068868 , -0.22119546],
                            [ 0.06794482, -0.37025544,  0.3678054 , ...,  0.0837122 ,
                              0.11539332, -0.2085252 ]]], dtype=float32)>)])
```
The converted model gives:
```
array([[[ 0.19460055, -0.07871183,  0.348869  , ...,  0.56901807,
          0.40855035, -0.36336952],
        [ 0.111944  , -0.10565656,  0.47841206, ...,  0.7020322 ,
          0.1973463 , -0.56403255],
        [ 0.2462692 , -0.04639575,  0.47352695, ...,  0.6418205 ,
          0.26107264, -0.72952   ],
        ...,
        [ 0.2746247 , -0.06043699,  0.4579026 , ...,  0.7356582 ,
          0.4970783 , -0.6069904 ],
        [ 0.2640199 , -0.05569951,  0.4638841 , ...,  0.734777  ,
          0.48725146, -0.60665435],
        [ 0.25305653, -0.04610543,  0.44723004, ...,  0.7529757 ,
          0.47617406, -0.6056732 ]]], dtype=float32)
```


**Any other info / logs**

Source code to reproduce:

```
from transformers import TFDistilBertModel, DistilBertTokenizer, DistilBertConfig
import tensorflow as tf
from typing import Dict
import numpy as np

class myTFDistilBertModel(TFDistilBertModel):
    # DUMMY_INPUTS = [[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]]
    # tf.constant(np.zeros(shape=(1, 100), dtype=int).tolist())
    # For details refer https://github.com/huggingface/transformers/blob/7cbf0f722d23440f3342aafc27697b50ead5996b/src/transformers/modeling_tf_utils.py#L218
    def __init__(self, config, *inputs, **kwargs):
        super().__init__(config, *inputs, **kwargs)
    
    @property
    def dummy_inputs(self) -> Dict[str, tf.Tensor]:
        """"""
        Dummy inputs to build the network.
        Returns:
            :obj:`Dict[str, tf.Tensor]`: The dummy inputs.
        """"""
        return {""input_ids"": tf.constant(np.zeros(shape=(1, 128), dtype=int).tolist())}

tf_model = myTFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', return_dict=True)
tf_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased', return_tensors='tf')

inputs = tf_tokenizer(""le droit d'accès"", padding='max_length', max_length=128, return_tensors='tf')
outputs  = tf_model(inputs)

input_spec = tf.TensorSpec([1, 128], tf.int32)
tf_model._set_inputs(input_spec, training=False)

converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)
# For normal conversion:
#converter.experimental_new_converter = True
#converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()

# Run the model with TensorFlow Lite
interpreter = tf.lite.Interpreter(model_content=tflite_model)
# Allocate tensors.
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input = tf_tokenizer(""le droit d'accès"", padding='max_length', max_length=128, return_tensors='tf')
interpreter.set_tensor(0, input['input_ids'])
interpreter.invoke()
interpreter_output = interpreter.get_tensor(output_details[0][""index""])
```
"
43475,CMSIS-NN: conv unit test cases fail for non-unity dilation cases,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): Source
- Tensorflow version (commit SHA if source): 712fd5cfe4a5de420a4226c874664423eba4cb1a
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): stm32f4

**Describe the problem**
The unit test cases which have dilation parameters not equal to 1 fail for cmsis-nn.

**Please provide the exact sequence of commands/steps when you ran into the problem**
1. Remove conv_test in exclusion filter list in target Makefile for stm32f4.

2. make -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn TARGET=stm32f4 test_kernel_fully_connected_test

"
43473,TFLiteCCoreML Delegate Runtime Crash (Espresso::ANERuntimeEngine overflow error),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

iOS 13.5.1

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:

iPhone 11 Max Pro

- TensorFlow installed from (source or binary):

Binary

- TensorFlow version (use command below):

TensorFlowLiteCCoreML v0.0.1 (via CocoaPods)

**Describe the current behavior**

When creating a CoreML delegate using the TensorFlowLiteC API, creating an interpreter using the delegate, then invoking the interpreter, iOS crashes because of a memory error upon invocation of the interpreter.

**Describe the expected behavior**

For the app not to crash when invoking the interpreter.

**Standalone code to reproduce the issue**

The code used to create the CoreML delegate is as follows (this all runs fine without error)

```
TfLiteCoreMlDelegateOptions coremldelegate_opts;                            
coremldelegate_opts.enabled_devices =  TfLiteCoreMlDelegateDevicesWithNeuralEngine;                            
coremldelegate_opts.coreml_version = 3; // have tried switching this to 2 but doesn't fix the crash                                 
coremldelegate_opts.max_delegated_partitions = 200;                           
                                                                              
TfLiteDelegate *delegate = TfLiteCoreMlDelegateCreate(&coremldelegate_opts);

TfLiteInterpreterOptions *options = TfLiteInterpreterOptionsCreate();       
                                                                             
if (delegate)                                                               
    TfLiteInterpreterOptionsAddDelegate(options, delegate);                 
                                                                               
TfLiteInterpreterOptionsSetNumThreads(options, 2);                          
interpreter = TfLiteInterpreterCreate(model, options); 

```

**Other info / logs**

Without the CoreML delegate the app doesn't crash when invoking the interpreter. The app also doesn't crash when creating and using an XNNPackDelegate with the interpreter.

The model being used is a non-quantized yolov4 tflite model. The interpreter runs the model fine (although with large latency) without the CoreML delegate. Here is a link to the model: https://www.dropbox.com/s/zqq1mooq3icepvk/yolov4-coco.tflite?dl=0

XCode console log upon crash is below

2020-09-23 01:51:38.095470-0400 testapp[5212:1077669] [espresso] [Espresso::ANERuntimeEngine::__forward_segment 0] evaluate[RealTime]WithModel returned 0; code=5 err=Error Domain=com.apple.appleneuralengine Code=5 ""processRequest:qos:qIndex:error:: 0x1: Program Inference overflow"" UserInfo={NSLocalizedDescription=processRequest:qos:qIndex:error:: 0x1: Program Inference overflow}

2020-09-23 01:51:38.095507-0400 testapp[5212:1077669] [espresso] [Espresso::overflow_error] /private/var/mobile/Containers/Data/Application/93F932CB-0799-4691-874F-B8DF78E67F5B/tmp/(A Document Being Saved By testapp 180)/A8A4CB7A-68E3-4352-B9C8-840119ACDA73-5212-0000030A34678314.mlmodelc/model.espresso.net:0

2020-09-23 01:51:38.126818-0400 testapp[5212:1077669] [espresso] [Espresso::ANERuntimeEngine::__forward_segment 0] evaluate[RealTime]WithModel returned 0; code=5 err=Error Domain=com.apple.appleneuralengine Code=5 ""processRequest:qos:qIndex:error:: 0x1: Program Inference overflow"" UserInfo={NSLocalizedDescription=processRequest:qos:qIndex:error:: 0x1: Program Inference overflow}

2020-09-23 01:51:38.126847-0400 testapp[5212:1077669] [espresso] [Espresso::overflow_error] /private/var/mobile/Containers/Data/Application/93F932CB-0799-4691-874F-B8DF78E67F5B/tmp/(A Document Being Saved By testapp 181)/2B38A561-4343-4868-8BF1-7AE106179D5D-5212-0000030A347917A7.mlmodelc/model.espresso.net:0
Message from debugger: Terminated due to memory issue


"
43472,Missing TRANSPOSE Op Kernel,"**System information**
- Linux Ubuntu 20.04:
- TensorFlow installed from (source or binary): binary 
- TensorFlow version (or github SHA if from source):  tf-nightly==2.4.0.dev20200917

I'm attempting to use a TFLite converted model, which was created and trained using TF2 + Keras.  The converter successfully created the TFLite file, and I've loaded it into a micro-controller app, as a flatbuffer cpp + h file.

I'm unable to share the model at this time due to confidentiality, however the model contains Conv2D, BatchNormalization, ReLu, MaxPooling2D, Permute, Dropout, Flatten, Dense and Softmax.

After conversion, the model is loaded into an Arduino sketch, but upon loading the model, an error is reported.

```txt
8 bytes lost due to alignment. To avoid this loss, please make sure the tensor_arena is 16 bytes aligned.
Didn't find op for builtin opcode 'TRANSPOSE' version '2'

Failed to get registration from op code TRANSPOSE
 
Failed starting model allocation.
```

Given that this operation was chosen from the Builtin operation set, I believe this is a fault/bug.  Can you please advise?

type registration
``` 
const tflite::Model* model = nullptr;
tflite::MicroInterpreter* interpreter = nullptr;
TfLiteTensor* input = nullptr;
TfLiteTensor* output = nullptr;
alignas(16) uint8_t tensor_arena[kTensorArenaSize]
```

AllOps missing TRANSPOSE
```cpp
static tflite::AllOpsResolver resolver;  // NO TRANSPOSE kernel registration
```

MicroMutableOpResolver missing TRANSPOSE registration
```cpp
tflite::MicroMutableOpResolver<6> resolver;
resolver.AddConv2D();    
resolver.AddDepthwiseConv2D();
resolver.AddFullyConnected();
resolver.AddReshape();
resolver.AddSoftmax();
resolver.AddBuiltin(tflite::BuiltinOperator_MAX_POOL_2D    
    ,tflite::ops::micro::Register_MAX_POOL_2D()); 
// resolver.AddBuiltin(tflite::BuiltinOperator_TRANSPOSE,
//     ,tflite::ops::micro::Register_TRANSPOSE); //BuiltinOperator_TRANSPOSE exists, but no Register_TRANSPOSE exists
```

**Standalone code to reproduce the issue** 
```python
 def model_to_tflite(self, features_path = None, tflite_path = None):
        '''Converts a Keras model into a TFLite model'''
        assert self.model is not None, 'TFLite conversion requires the model be loaded'
        assert self.x_data is not None and self.y_data is not None, 'Sample data must be loaded'

        if os.path.exists(tflite_path):
            logging.warning(f'TFLite file already exists: {tflite_path}')

        logging.info(f'Found {len(self.x_data)} features')

        # Construction of a representative dataset
        def representative_dataset():
            for i in range(len(self.x_data)):
                yield([self.x_data[i:i+1,:,:,:]])

        # Construction of a TFLite converter
        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)
        converter.representative_dataset = representative_dataset
        
        converter.optimizations = [ tf.lite.Optimize.OPTIMIZE_FOR_LATENCY ]
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        
        converter.inference_input_type = tf.int8
        converter.inference_output_type = tf.int8
        
        tflite_model = converter.convert()
        bytes_written = open(tflite_path, 'wb').write(tflite_model)

        return bytes_written
```

**Any other info / logs**

```
/home/ian/Documents/source/acdnet_pipeline/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2289: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
2020-09-23 14:34:59.654363: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
/home/ian/Documents/source/acdnet_pipeline/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1376: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`layer.updates` will be removed in a future version. '
2020-09-23 14:35:02.599832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-23 14:35:02.600152: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-09-23 14:35:02.600313: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-09-23 14:35:02.600650: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-09-23 14:35:02.600775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-23 14:35:02.601273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 computeCapability: 6.1
coreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s
2020-09-23 14:35:02.601398: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2020-09-23 14:35:02.601464: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2020-09-23 14:35:02.601493: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-23 14:35:02.601521: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-23 14:35:02.601531: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-23 14:35:02.601622: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2020-09-23 14:35:02.601715: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2020-09-23 14:35:02.601745: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-23 14:35:02.894277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-23 14:35:02.894345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-23 14:35:02.894368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-23 14:35:02.919186: I tensorflow/core/platform/profile_utils/cpu_utils.cc:108] CPU Frequency: 2599990000 Hz
2020-09-23 14:35:02.982735: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:872] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 5.462ms.
  function_optimizer: function_optimizer did nothing. time = 0.003ms.

2020-09-23 14:35:03.203503: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:315] Ignored output_format.
2020-09-23 14:35:03.203549: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:318] Ignored drop_control_dependency.
2020-09-23 14:35:03.469971: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-09-23 14:35:03.470367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-23 14:35:03.471405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 computeCapability: 6.1
coreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s
2020-09-23 14:35:03.471730: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2020-09-23 14:35:03.471928: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2020-09-23 14:35:03.471981: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-23 14:35:03.472028: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-23 14:35:03.472067: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-23 14:35:03.472238: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2020-09-23 14:35:03.472414: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2020-09-23 14:35:03.472450: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-23 14:35:03.472494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-23 14:35:03.472519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-23 14:35:03.472540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 

```"
43471,Additive Attention Layer is not really Additive Attention,"In Bahdanau's paper, after applying the non-linearity (tanh), there is a dot product between a vector va and the result of the non-linearity (page 12 of the paper). Indeed the result of the tanh(...) operation is a n x 1 matrix. va is a n x 1 matrix, which is transposed into a 1 x n. So the final result is a single score for each key. After this, the softmax function is applied to the set of scores.
Instead, the current AdditiveAttention layer does a simple (optional) scaling along the feature dimension.

Moreover, the score is computed incorrectly, as reported by #39332


"
43470,"Bad Loss History, Callback vs. Progressbar Loss Mismatch","**System information**
- Have I written custom code - Yes
- OS Platform and Distribution - RHEL7
- Mobile device - None
- TensorFlow installed from - pip
- TensorFlow version v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.7
- CUDA/cuDNN version: 
cudatoolkit               10.1.243             h6bb024c_0  
cudnn                     7.6.5                cuda10.1_0  
- GPU model and memory: 4x Nvidia TITAN X (Maxwell)



**Describe the current behavior**

I am working with a GAN, using code by subclassing tf.keras.Model in the style of: https://keras.io/examples/generative/dcgan_overriding_train_step/

I attempted to make a plot of the generator loss and the discriminator loss using the history object from the result of model.fit(), and noticed the values in the history object were all constants, despite obvious variability as displayed by the progress bar.  I dug deeper, and added a callback to print the `d_loss` and `g_loss` after `on_epoch_end` and `on_train_batch_end`, switched the model.compile() to include `run_eagerly=True`, added a print statement at the end of `train_step()` and I noticed that they do not match the values being printed by the progress bar.

Example terminal output:
```
Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.
Epoch 1/4

Train Step:  tf.Tensor(0.72807, shape=(), dtype=float32) tf.Tensor(0.46352494, shape=(), dtype=float32)
Batch End:  {'d_loss': 0.7280700206756592, 'g_loss': 0.4635249376296997}
  1/100 [..............................] - ETA: 0s - d_loss: 0.7281 - g_loss: 0.4635

Train Step:  tf.Tensor(0.7460346, shape=(), dtype=float32) tf.Tensor(0.36776978, shape=(), dtype=float32)
Batch End:  {'d_loss': 0.7460346221923828, 'g_loss': 0.3677697777748108}
  2/100 [..............................] - ETA: 12s - d_loss: 0.7371 - g_loss: 0.4156

Train Step:  tf.Tensor(0.67379105, shape=(), dtype=float32) tf.Tensor(0.6921332, shape=(), dtype=float32)
Batch End:  {'d_loss': 0.6737910509109497, 'g_loss': 0.6921331882476807}
  3/100 [..............................] - ETA: 15s - d_loss: 0.7160 - g_loss: 0.5078
```

As you can see, on the first batch, all the values match precisely - the print statement at the end of `train_step`, the callback for `on_train_batch_end` and the progressbar output all match.  By the 2nd batch, `train_step` and `on_train_batch_end` are reporting a value of 0.746 for d_loss by the progressbar is printing a value of .7371, and g_loss is .3677 vs .4156.  

Originally, I suspected the distribute_strategy might be involved, so I commented that out and I re-ran with `CUDA_VISIBLE_DEVICES=0`, and got the same value mismatching.  I have this issue regardless of whether I run with `run_eagerly=True` or False.

My actual `train_step` for reference in case I'm just doing something stupid:
```
import tensorflow as tf
from tensorflow.nn import sigmoid_cross_entropy_with_logits

def train_step(self, train_step_data):
        
        high_res_imgs = train_step_data[1]
        
        low_res_imgs = train_step_data[0] 
        
        with tf.GradientTape(persistent=True) as tape:
           
            generated_images = self.generator(low_res_imgs)
    
            logits_on_generated = self.discriminator(generated_images)
            
            logits_on_real = self.discriminator(high_res_imgs)
            
            d_real_loss = sigmoid_cross_entropy_with_logits(labels=tf.ones_like(logits_on_real), logits=logits_on_real)
            
            d_gen_loss = sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(logits_on_generated), logits=logits_on_generated)
            
            d_loss = 0.5*(d_real_loss + d_gen_loss)
            
            
            g_adv_loss = sigmoid_cross_entropy_with_logits(labels=tf.ones_like(logits_on_generated), logits=logits_on_generated)
            
            
            g_loss = g_adv_loss


        ################################################################################
        # Now backpropagate discriminator based on the losses
        ################################################################################
        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)
        
        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))
        
        ################################################################################
        # Now backpropagate generator based on the losses
        ################################################################################
        grads = tape.gradient(g_loss, self.generator.trainable_weights)
        
        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))


        #print(d_loss.shape)
        #print(d_loss.numpy())
        print(""\nTrain Step: "", d_loss[0,0], g_loss[0,0])
        return {""d_loss"": d_loss[0,0], ""g_loss"": g_loss[0,0]}
```

I still have this issue regardless of whether I just return d_loss and g_loss or 'd_loss[0,0]' and g_loss[0,0].

And my callback:
```
class ModelPrinter(tf.keras.callbacks.Callback):
        
        def __init__(self, model):
            
            self.model = model
            
        def on_epoch_end(self, epoch, logs=None):
            
            print(self.model)
            print(""\nEpoch End: "", logs)
            
        def on_train_batch_end(self, step_num, logs=None):
            
            print(""\nBatch End: "", logs)
```"
43469,reduce_sum and reduce_prod Permuting order of columns on Power9 CPUs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04.3 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary - community build
- TensorFlow version (use command below): v2.2.0-0-g2b96f3662b 2.2.0
- Python version: 3.7.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory: V100  32gb


**Describe the current behavior**
tf.reduce_sum is permuting the columns when producing the result.  Stand-alone code below produces a tensor [3.,4.,1.,2.]
This only happens when computation is on CPU and we've only been able to reproduce it on our Power9 machines.

**Describe the expected behavior**
tf.reduce_sum should preserve the order of columns.  It should produce [1.,2.,3.,4.]

**Standalone code to reproduce the issue**
```
import os
os.environ['CUDA_VISIBLE_DEVICES'] = ''

import tensorflow as tf
import numpy as np

x = tf.constant(np.array([[1.0,2.0,3.0,4.0],
       [0.0,0.0,0.0,0.0]]),
      dtype=tf.float32)

out = tf.reduce_sum(x, axis=0)
print(out)
```

Prints:

> tf.Tensor([3. 4. 1. 2.], shape=(4,), dtype=float32)
<img width=""469"" alt=""Screen Shot 2020-09-22 at 6 57 33 PM"" src=""https://user-images.githubusercontent.com/11985639/93945751-8407f800-fd05-11ea-8008-793b0c72084e.png"">

tf.reduce_prod is producing similar results:
<img width=""470"" alt=""Screen Shot 2020-09-22 at 6 59 50 PM"" src=""https://user-images.githubusercontent.com/11985639/93945902-d9dca000-fd05-11ea-8496-e878adc64b19.png"">


We notice the same thing in more complicated examples breaking the UnitNorm constraint, as it's using reduce_sum when computing the norm and then dividing vectors by a permutation of the relevant lengths."
43468,[ROCm] Incorrect path to clang. ,"Linux 5.7.6

Python 3.8

ROCm 3.8

gcc 10.2.0-9 (Debian testing).

rocminfo, hipconfig works.

pip3 install tensorflow-rocm  works.

Verified tensorflow-rocm mostly works by checking few things manually.


```
import tensorflow as tf
>>> tf.version.GIT_VERSION
'v2.3.0-rc1-2358-gc0826c7973'
>>> tf.version.VERSION
'2.3.0'
>>> tf.test.is_built_with_rocm()
True
>>> tf.config.list_physical_devices('GPU')
2020-09-22 21:25:14.201524: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libamdhip64.so
2020-09-22 21:25:14.255178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: 
pciBusID: 0000:43:00.0 name: Fiji [Radeon R9 FURY / NANO Series]     ROCm AMD GPU ISA: gfx803
coreClock: 1.05GHz coreCount: 64 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 59.60GiB/s
2020-09-22 21:25:14.258181: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocblas.so
2020-09-22 21:25:14.258947: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libMIOpen.so
2020-09-22 21:25:14.264645: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocfft.so
2020-09-22 21:25:14.264860: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocrand.so
2020-09-22 21:25:14.264958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
>>> 
```

Doing more complex things, like mnist training sanity check fails:

```
root@debian:~# LD_LIBRARY_PATH=/opt/rocm-3.8.0/lib ROCM_PATH=/opt/rocm-3.8.0 python3 /home/user/tf_train_test.py 
x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
2020-09-22 21:20:23.251851: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libamdhip64.so
2020-09-22 21:20:23.304418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: 
pciBusID: 0000:43:00.0 name: Fiji [Radeon R9 FURY / NANO Series]     ROCm AMD GPU ISA: gfx803
coreClock: 1.05GHz coreCount: 64 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 59.60GiB/s
2020-09-22 21:20:23.307404: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocblas.so
2020-09-22 21:20:23.308140: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libMIOpen.so
2020-09-22 21:20:23.313617: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocfft.so
2020-09-22 21:20:23.313820: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocrand.so
2020-09-22 21:20:23.313904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-22 21:20:23.314125: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-22 21:20:23.320609: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3499500000 Hz
2020-09-22 21:20:23.321952: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28c5f10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-22 21:20:23.322004: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-22 21:20:23.323548: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2815ff0 initialized for platform ROCM (this does not guarantee that XLA will be used). Devices:
2020-09-22 21:20:23.323562: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Fiji [Radeon R9 FURY / NANO Series], AMDGPU ISA version: gfx803
2020-09-22 21:20:23.615134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: 
pciBusID: 0000:43:00.0 name: Fiji [Radeon R9 FURY / NANO Series]     ROCm AMD GPU ISA: gfx803
coreClock: 1.05GHz coreCount: 64 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 59.60GiB/s
2020-09-22 21:20:23.615223: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocblas.so
2020-09-22 21:20:23.615241: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libMIOpen.so
2020-09-22 21:20:23.615256: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocfft.so
2020-09-22 21:20:23.615269: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocrand.so
2020-09-22 21:20:23.615352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-22 21:20:23.615382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-22 21:20:23.615393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-22 21:20:23.615400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-22 21:20:23.615531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3796 MB memory) -> physical GPU (device: 0, name: Fiji [Radeon R9 FURY / NANO Series], pci bus id: 0000:43:00.0)
2020-09-22 21:20:23.840997: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:505] ROCm Fusion is enabled.
2020-09-22 21:20:23.844554: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:505] ROCm Fusion is enabled.
2020-09-22 21:20:23.846662: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:505] ROCm Fusion is enabled.
Epoch 1/12
2020-09-22 21:20:24.065540: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:505] ROCm Fusion is enabled.
2020-09-22 21:20:24.071930: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocblas.so
2020-09-22 21:20:24.116035: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libMIOpen.so
MIOpen(HIP): Error [ValidateGcnAssemblerImpl] Wrong path to assembler: '/opt/rocm/llvm/bin/clang'. Expect performance degradation.
clang-11: error: cannot find ROCm installation.  Provide its path via --rocm-path, or pass -nogpulib.
MIOpen Error: /root/driver/MLOpen/src/tmp_dir.cpp:47: Can't execute cd /tmp/miopen-gridwise_convolution_backward_data_implicit_gemm_v1r1_nchw_kcyx_nkhw.cpp-c34b-acdd-79a3-774e;  /opt/rocm-3.8.0/llvm/bin/clang++  -std=c++14  -DCK_PARAM_PROBLEM_N=128 -DCK_PARAM_PROBLEM_K=64 -DCK_PARAM_PROBLEM_C=32 -DCK_PARAM_PROBLEM_HI=26 -DCK_PARAM_PROBLEM_WI=26 -DCK_PARAM_PROBLEM_HO=24 -DCK_PARAM_PROBLEM_WO=24 -DCK_PARAM_PROBLEM_Y=3 -DCK_PARAM_PROBLEM_X=3 -DCK_PARAM_PROBLEM_CONV_STRIDE_H=1 -DCK_PARAM_PROBLEM_CONV_STRIDE_W=1 -DCK_PARAM_PROBLEM_CONV_DILATION_H=1 -DCK_PARAM_PROBLEM_CONV_DILATION_W=1 -DCK_PARAM_PROBLEM_IN_LEFT_PAD_H=0 -DCK_PARAM_PROBLEM_IN_LEFT_PAD_W=0 -DCK_PARAM_PROBLEM_IN_RIGHT_PAD_H=0 -DCK_PARAM_PROBLEM_IN_RIGHT_PAD_W=0 -DCK_PARAM_TUNABLE_BLOCK_SIZE=64 -DCK_PARAM_TUNABLE_GEMM_M_PER_BLOCK=32 -DCK_PARAM_TUNABLE_GEMM_N_PER_BLOCK=64 -DCK_PARAM_TUNABLE_GEMM_K_PER_BLOCK=16 -DCK_PARAM_TUNABLE_GEMM_M_PER_THREAD=2 -DCK_PARAM_TUNABLE_GEMM_N_PER_THREAD=4 -DCK_PARAM_TUNABLE_GEMM_M_LEVEL0_CLUSTER=4 -DCK_PARAM_TUNABLE_GEMM_N_LEVEL0_CLUSTER=4 -DCK_PARAM_TUNABLE_GEMM_M_LEVEL1_CLUSTER=2 -DCK_PARAM_TUNABLE_GEMM_N_LEVEL1_CLUSTER=2 -DCK_PARAM_TUNABLE_GEMM_A_BLOCK_COPY_CLUSTER_LENGTHS_GEMM_K=8 -DCK_PARAM_TUNABLE_GEMM_A_BLOCK_COPY_CLUSTER_LENGTHS_GEMM_M=8 -DCK_PARAM_TUNABLE_GEMM_A_BLOCK_COPY_SRC_DATA_PER_READ_GEMM_M=4 -DCK_PARAM_TUNABLE_GEMM_B_BLOCK_COPY_CLUSTER_LENGTHS_GEMM_K=4 -DCK_PARAM_TUNABLE_GEMM_B_BLOCK_COPY_CLUSTER_LENGTHS_GEMM_N=16 -DCK_PARAM_TUNABLE_GEMM_B_BLOCK_COPY_SRC_DATA_PER_READ_GEMM_N=4 -DCK_PARAM_TUNABLE_GEMM_C_THREAD_COPY_DST_DATA_PER_WRITE_GEMM_N1=1 -DCK_PARAM_DEPENDENT_GRID_SIZE=10368 -DCK_THREADWISE_GEMM_USE_AMD_INLINE_ASM=0 -DCK_USE_AMD_INLINE_ASM=0 -DCK_USE_AMD_BUFFER_ATOMIC_ADD=0 -DMIOPEN_USE_FP16=0 -DMIOPEN_USE_FP32=1 -DMIOPEN_USE_INT8=0 -DMIOPEN_USE_INT8x4=0 -DMIOPEN_USE_BFP16=0 -DMIOPEN_USE_INT32=0 -DMIOPEN_USE_RNE_BFLOAT16=1 -DCK_PARAM_TUNABLE_GEMM_A_BLOCK_COPY_DST_DATA_PER_WRITE_GEMM_M=4 -DCK_PARAM_TUNABLE_GEMM_B_BLOCK_COPY_DST_DATA_PER_WRITE_GEMM_N=4 -mcpu=gfx803 -Wno-everything --cuda-gpu-arch=gfx803 --cuda-device-only -c -O3  -Wno-unused-command-line-argument -I. -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -xhip --hip-device-lib-path=/opt/rocm/lib -L/opt/rocm-3.8.0/llvm/lib/clang/11.0.0/include/../lib/linux -lclang_rt.builtins-x86_64 -D__HIP_ROCclr__=1 -isystem /opt/rocm-3.8.0/hip/../include -isystem /opt/rocm-3.8.0/llvm/lib/clang/11.0.0/include/.. -L/opt/rocm-3.8.0/llvm/lib/clang/11.0.0/include/../lib/linux -lclang_rt.builtins-x86_64 -D__HIP_PLATFORM_HCC__=1 -D__HIP_ROCclr__=1 -isystem /opt/rocm-3.8.0/hip/include -isystem /opt/rocm/include -L/opt/rocm-3.8.0/llvm/lib/clang/11.0.0/include/../lib/linux -lclang_rt.builtins-x86_64 --hip-link -L/opt/rocm-3.8.0/llvm/lib/clang/11.0.0/include/../lib/linux -lclang_rt.builtins-x86_64 -mllvm -amdgpu-enable-global-sgpr-addr -mllvm --amdgpu-spill-vgpr-to-agpr=0 gridwise_convolution_backward_data_implicit_gemm_v1r1_nchw_kcyx_nkhw.cpp -o /tmp/miopen-gridwise_convolution_backward_data_implicit_gemm_v1r1_nchw_kcyx_nkhw.cpp-c34b-acdd-79a3-774e/gridwise_convolution_backward_data_implicit_gemm_v1r1_nchw_kcyx_nkhw.cpp.o
2020-09-22 21:20:28.516500: F tensorflow/stream_executor/rocm/rocm_dnn.cc:3572] call to miopenFindConvolutionBackwardDataAlgorithm failed: miopenStatusUnknownError
Aborted
```




hipconfig:

```
root@debian:~# ROCM_PATH=/opt/rocm-3.8.0 /opt/rocm-3.8.0/bin/hipconfig 
HIP version  : 3.8.20371-d1886b0b

== hipconfig
HIP_PATH     : /opt/rocm-3.8.0/hip
ROCM_PATH    : /opt/rocm-3.8.0
HIP_COMPILER : clang
HIP_PLATFORM : hcc
HIP_RUNTIME  : ROCclr
CPP_CONFIG   :  -D__HIP_PLATFORM_HCC__=  -I/opt/rocm-3.8.0/hip/include -I/opt/rocm-3.8.0/llvm/bin/../lib/clang/11.0.0 -I/opt/rocm-3.8.0/hsa/include -D__HIP_ROCclr__

== hip-clang
HSA_PATH         : /opt/rocm-3.8.0/hsa
HIP_CLANG_PATH   : /opt/rocm-3.8.0/llvm/bin
clang version 11.0.0 (/src/external/llvm-project/clang b98349b12ffa706d0e863a3f1176b20d2a6c438b)
Target: x86_64-unknown-linux-gnu
Thread model: posix
InstalledDir: /opt/rocm-3.8.0/llvm/bin
LLVM (http://llvm.org/):
  LLVM version 11.0.0git
  Optimized build.
  Default target: x86_64-unknown-linux-gnu
  Host CPU: znver1

  Registered Targets:
    amdgcn - AMD GCN GPUs
    r600   - AMD GPUs HD2XXX-HD6XXX
    x86    - 32-bit X86: Pentium-Pro and above
    x86-64 - 64-bit X86: EM64T and AMD64
hip-clang-cxxflags : -D__HIP_ROCclr__ -std=c++11 -isystem /opt/rocm-3.8.0/llvm/lib/clang/11.0.0/include/.. -isystem /opt/rocm-3.8.0/hsa/include -D__HIP_ROCclr__ -isystem /opt/rocm-3.8.0/hip/include -D__HIP_ARCH_GFX803__=1  -O3
hip-clang-ldflags  :  -L/opt/rocm-3.8.0/hip/lib -O3 -lgcc_s -lgcc -lpthread -lm

=== Environment Variables
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

== Linux Kernel
Hostname     : debian
Linux debian 5.7.0-1-amd64 #1 SMP Debian 5.7.6-1 (2020-06-24) x86_64 GNU/Linux
No LSB modules are available.
Distributor ID:	Debian
Description:	Debian GNU/Linux bullseye/sid
Release:	unstable
Codename:	sid
```

rocminfo:
```
# /opt/rocm-3.8.0/bin/rocminfo
ROCk module is loaded
Able to open /dev/kfd read-write
=====================    
HSA System Attributes    
=====================    
Runtime Version:         1.1
System Timestamp Freq.:  1000.000000MHz
Sig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count)
Machine Model:           LARGE                              
System Endianness:       LITTLE                             

==========               
HSA Agents               
==========               
*******                  
Agent 1                  
*******                  
  Name:                    AMD Ryzen Threadripper 2950X 16-Core Processor
  Uuid:                    CPU-XX                             
  Marketing Name:          AMD Ryzen Threadripper 2950X 16-Core Processor
  Vendor Name:             CPU                                
  Feature:                 None specified                     
  Profile:                 FULL_PROFILE                       
  Float Round Mode:        NEAR                               
  Max Queue Number:        0(0x0)                             
  Queue Min Size:          0(0x0)                             
  Queue Max Size:          0(0x0)                             
  Queue Type:              MULTI                              
  Node:                    0                                  
  Device Type:             CPU                                
  Cache Info:              
    L1:                      32768(0x8000) KB                   
  Chip ID:                 0(0x0)                             
  Cacheline Size:          64(0x40)                           
  Max Clock Freq. (MHz):   3500                               
  BDFID:                   0                                  
  Internal Node ID:        0                                  
  Compute Unit:            16                                 
  SIMDs per CU:            0                                  
  Shader Engines:          0                                  
  Shader Arrs. per Eng.:   0                                  
  WatchPts on Addr. Ranges:1                                  
  Features:                None
  Pool Info:               
    Pool 1                   
      Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED
      Size:                    65850456(0x3eccc58) KB             
      Allocatable:             TRUE                               
      Alloc Granule:           4KB                                
      Alloc Alignment:         4KB                                
      Accessible by all:       TRUE                               
    Pool 2                   
      Segment:                 GLOBAL; FLAGS: COARSE GRAINED      
      Size:                    65850456(0x3eccc58) KB             
      Allocatable:             TRUE                               
      Alloc Granule:           4KB                                
      Alloc Alignment:         4KB                                
      Accessible by all:       TRUE                               
  ISA Info:                
    N/A                      
*******                  
Agent 2                  
*******                  
  Name:                    AMD Ryzen Threadripper 2950X 16-Core Processor
  Uuid:                    CPU-XX                             
  Marketing Name:          AMD Ryzen Threadripper 2950X 16-Core Processor
  Vendor Name:             CPU                                
  Feature:                 None specified                     
  Profile:                 FULL_PROFILE                       
  Float Round Mode:        NEAR                               
  Max Queue Number:        0(0x0)                             
  Queue Min Size:          0(0x0)                             
  Queue Max Size:          0(0x0)                             
  Queue Type:              MULTI                              
  Node:                    1                                  
  Device Type:             CPU                                
  Cache Info:              
    L1:                      32768(0x8000) KB                   
  Chip ID:                 0(0x0)                             
  Cacheline Size:          64(0x40)                           
  Max Clock Freq. (MHz):   3500                               
  BDFID:                   0                                  
  Internal Node ID:        1                                  
  Compute Unit:            16                                 
  SIMDs per CU:            0                                  
  Shader Engines:          0                                  
  Shader Arrs. per Eng.:   0                                  
  WatchPts on Addr. Ranges:1                                  
  Features:                None
  Pool Info:               
    Pool 1                   
      Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED
      Size:                    66044092(0x3efc0bc) KB             
      Allocatable:             TRUE                               
      Alloc Granule:           4KB                                
      Alloc Alignment:         4KB                                
      Accessible by all:       TRUE                               
    Pool 2                   
      Segment:                 GLOBAL; FLAGS: COARSE GRAINED      
      Size:                    66044092(0x3efc0bc) KB             
      Allocatable:             TRUE                               
      Alloc Granule:           4KB                                
      Alloc Alignment:         4KB                                
      Accessible by all:       TRUE                               
  ISA Info:                
    N/A                      
*******                  
Agent 3                  
*******                  
  Name:                    gfx803                             
  Uuid:                    GPU-XX                             
  Marketing Name:          Fiji [Radeon R9 FURY / NANO Series]
  Vendor Name:             AMD                                
  Feature:                 KERNEL_DISPATCH                    
  Profile:                 BASE_PROFILE                       
  Float Round Mode:        NEAR                               
  Max Queue Number:        128(0x80)                          
  Queue Min Size:          4096(0x1000)                       
  Queue Max Size:          131072(0x20000)                    
  Queue Type:              MULTI                              
  Node:                    2                                  
  Device Type:             GPU                                
  Cache Info:              
    L1:                      16(0x10) KB                        
  Chip ID:                 29440(0x7300)                      
  Cacheline Size:          64(0x40)                           
  Max Clock Freq. (MHz):   1050                               
  BDFID:                   17152                              
  Internal Node ID:        2                                  
  Compute Unit:            64                                 
  SIMDs per CU:            4                                  
  Shader Engines:          4                                  
  Shader Arrs. per Eng.:   1                                  
  WatchPts on Addr. Ranges:4                                  
  Features:                KERNEL_DISPATCH 
  Fast F16 Operation:      FALSE                              
  Wavefront Size:          64(0x40)                           
  Workgroup Max Size:      1024(0x400)                        
  Workgroup Max Size per Dimension:
    x                        1024(0x400)                        
    y                        1024(0x400)                        
    z                        1024(0x400)                        
  Max Waves Per CU:        40(0x28)                           
  Max Work-item Per CU:    2560(0xa00)                        
  Grid Max Size:           4294967295(0xffffffff)             
  Grid Max Size per Dimension:
    x                        4294967295(0xffffffff)             
    y                        4294967295(0xffffffff)             
    z                        4294967295(0xffffffff)             
  Max fbarriers/Workgrp:   32                                 
  Pool Info:               
    Pool 1                   
      Segment:                 GLOBAL; FLAGS: COARSE GRAINED      
      Size:                    4194304(0x400000) KB               
      Allocatable:             TRUE                               
      Alloc Granule:           4KB                                
      Alloc Alignment:         4KB                                
      Accessible by all:       FALSE                              
    Pool 2                   
      Segment:                 GROUP                              
      Size:                    64(0x40) KB                        
      Allocatable:             FALSE                              
      Alloc Granule:           0KB                                
      Alloc Alignment:         0KB                                
      Accessible by all:       FALSE                              
  ISA Info:                
    ISA 1                    
      Name:                    amdgcn-amd-amdhsa--gfx803          
      Machine Models:          HSA_MACHINE_MODEL_LARGE            
      Profiles:                HSA_PROFILE_BASE                   
      Default Rounding Mode:   NEAR                               
      Default Rounding Mode:   NEAR                               
      Fast f16:                TRUE                               
      Workgroup Max Size:      1024(0x400)                        
      Workgroup Max Size per Dimension:
        x                        1024(0x400)                        
        y                        1024(0x400)                        
        z                        1024(0x400)                        
      Grid Max Size:           4294967295(0xffffffff)             
      Grid Max Size per Dimension:
        x                        4294967295(0xffffffff)             
        y                        4294967295(0xffffffff)             
        z                        4294967295(0xffffffff)             
      FBarrier Max Size:       32                                 
*** Done ***             
```



Training code:
```python3
#!/usr/bin/env python3

import tensorflow.keras as keras
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras import backend as K

batch_size = 128
num_classes = 10
epochs = 12

# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

if K.image_data_format() == 'channels_first':
  x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
  x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
  input_shape = (1, img_rows, img_cols)
else:
  x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
  x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
  input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])

model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
```"
43467,"""Cadence processor cores only"" license restriction","There are 29 files in the repo which look like they have an MIT license header, but upon closer inspection it actually contains a restricted license. For example, from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/testing/test_xtensa_hifi_binary.sh:

> Permission is hereby granted, free of charge, to any person obtaining
> a copy of this software and associated documentation files (the
> ""Software""), **to use this Software with Cadence processor cores only and
> not with any other processors and platforms**, subject to
> the following conditions:

The files with this license notice can be seen at https://github.com/tensorflow/tensorflow/search?q=cadence+processor+cores+only

The project may want to consider removing these files, or perhaps moving them to a different repository, as content with a use restriction like this might not be seen by the community as compatible with Apache-2.0. (I gather this code will be received by anyone who checks out the source code repo, whether or not they're using this xtensa section of it.)"
43465,Is there any user data that is either being collected or Tracked by the tensorflow,"As part of the iOS 14 /AppTracking Transparency framework, would like to know if any kind of the user data is collected or Tracked by the tensorflow.

As mentioned in the Apple (Ref: https://developer.apple.com/support/app-privacy-on-the-app-store/),
below were some of the examples for Tracking:

- Sharing device location data or email lists with a data broker.
- Placing a third-party SDK in our app that combines user data from the app with user data from other developers’ apps to target advertising or measure advertising efficiency, even if we don’t use the SDK for these purposes. For example, using a login SDK that repurposes the data it collects from the app to enable targeted advertising in other developers’ apps.

The following situations are not considered tracking:

- When the data is linked solely on the end-user’s device and is not sent off the device in a way that can identify the end-user or device.
- When the data broker uses the data shared with them solely for fraud detection or prevention or security purposes, and solely on our behalf."
43464,'Tensor' object has no attribute 'numpy',"I'll just post this as bug issue because if I post other issues you guys would just tell me to post at stackoverflow every time while stackoverflow never helps and I actually tried many methods they had posted before I open this issue. 

So I try to print the output of each layer and got this error. `tf.executing_eagerly` and `model.run_eagerly` are all TRUE. I've checked most issues on the github and this situation is rare so those just don't help. Here is the code(very simple). 
[model_print_data.txt](https://github.com/tensorflow/tensorflow/files/5263474/model_print_data.txt)
This may provide some details. Much appreciated.
"
43463,Flower_Classification_with_TFLite_Model_Maker: missing labels.txt,"The Tutorial and the Notebook state that I need a labels.txt file that should be generated along with the model.tflite but is not:

> After this simple 4 steps, we can now download the model and label files

https://github.com/tensorflow/examples/blob/master/lite/codelabs/flower_classification/ml/Flower_Classification_with_TFLite_Model_Maker.ipynb

>Copy the TensorFlow Lite model model.tflite and label.txt that you trained earlier to assets

https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android

I tried

`model.export(export_dir='/content/export/tflite_quant', label_filename='labels.txt', tflite_filename='model_quant.tflite', quantization_config=config)`

and it says `INFO:tensorflow:Label file is inside the TFLite model with metadata.` in the log.

So is this a new feature? Is the labels.txt no longer required? What changes need to be made?

I manually created the labels.txt for the app and it worked, but I am not sure if it works without.


---

Also

lite/codelabs/flower_classification/start/app/src/main/assets/

changed to

lite/codelabs/flower_classification/android/start/app/src/main/assets

in https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/#4
"
43462,Tensorflow developer certificate,Can we refer some resources like stackover flow(for any errors) or our noted material while taking the exam?
43461,g++ compilation results in undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal - collect2: error: ld returned 1 exit status,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 on docker (image: `nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04`)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.3
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: 10.1/7.6
- GPU model and memory: RTX 2080 Super - 8GB



**Describe the problem**
I'm compiling  a very simple C++ program and linking it to the tensorflow_cc library. However i get a compilation error`

Compiling the source file using the following:

```
# g++ -I /opt/tensorflow/lib/include/ -L /opt/tensorflow/lib/ -ltensorflow_cc -ltensorflow_framework -lstdc++ -L/usr/local/lib/ -L /usr/lib/x86_64-linux-gnu/ -o /object_detection/load_saved_model /object_detection/load_saved_model.cpp
```

results in:

```
/tmp/ccz16e32.o: In function `tensorflow::core::RefCounted::~RefCounted()':
load_saved_model.cpp:(.text._ZN10tensorflow4core10RefCountedD2Ev[_ZN10tensorflow4core10RefCountedD5Ev]+0xf4): undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'
load_saved_model.cpp:(.text._ZN10tensorflow4core10RefCountedD2Ev[_ZN10tensorflow4core10RefCountedD5Ev]+0x11c): undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'
/tmp/ccz16e32.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long, int>(long const&, int const&, char const*)':
load_saved_model.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x33): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)'
load_saved_model.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x5d): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::ForVar2()'
load_saved_model.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x7b): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'
load_saved_model.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x8a): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'
load_saved_model.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0xad): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'
collect2: error: ld returned 1 exit status
```


**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. `git clone https://github.com/tensorflow/tensorflow.git `
2. `git checkout r2.3`
3. `git checkout r2.3`
4. `bazel clean`
5. 
```
root@167c90dbc0fe:/tensorflow# ./configure
You have bazel 3.1.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 


Found possible Python library paths:
  /usr/local/lib/python3.6/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.6/dist-packages]
/usr/local/lib/python3.6/dist-packages
Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: N
No TensorRT support will be enabled for TensorFlow.

Found CUDA 10.1 in:
    /usr/local/cuda-10.1/targets/x86_64-linux/lib
    /usr/local/cuda-10.1/targets/x86_64-linux/include
Found cuDNN 7 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 3.5, 7.0, 7.5

Do you want to use clang as CUDA compiler? [y/N]: N
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```

6. 
```
bazel build --jobs=8 --config=v2 --copt=-O3 --copt=-m64 --copt=-march=native --verbose_failures //tensorflow:install_headers //tensorflow:tensorflow //tensorflow:tensorflow_cc //tensorflow:tensorflow_framework //tensorflow/tools/lib_package:libtensorflow
```

`Target //tensorflow:libtensorflow_cc.so up-to-date:
  bazel-bin/tensorflow/libtensorflow_cc.so
INFO: Elapsed time: 3582.131s, Critical Path: 175.91s
INFO: 11806 processes: 11806 local.
INFO: Build completed successfully, 16884 total actions
`

7. 
```
 g++ -I /opt/tensorflow/lib/include/ -L /opt/tensorflow/lib/ -ltensorflow_cc -ltensorflow_framework -lstdc++ -L/usr/local/lib/ -L /usr/lib/x86_64-linux-gnu/ -o /object_detection/load_saved_model /object_detection/load_saved_model.cpp
```

**Any other info / logs**
load_saved_model.cpp

```
#include <stdlib.h>
#include <stdio.h>
#include <string>
#include ""tensorflow/cc/saved_model/loader.h""

int  main(int argc, char* argv[]){
    printf(""Will load and serve a saved model..."");
}
```"
43460,Feature Request: Support for ragged argmin/argmax,"**System information**
- TensorFlow version: 2.3.0
- Are you willing to contribute it: Possibly



**Describe the feature and the current behavior/state.**
tf.math.argmax/argmin does not support ragged tensors.
**Will this change the current api? How?**
No breaking changes, simply additive in nature.
**Who will benefit with this feature?**
In my opinion this is particularly important, as it can condense a ragged dimension down to a 1-dimension. In my use case, I have predictions that can be of variable length. I want to take the minimum, which would effectively eliminate the ragged dimension.
**Any Other info.**
For anyone else looking for a workaround, currently my approach is to convert the ragged tensor to a regular tensor. The default value should be max positive number if you are performing min or max negative number if you are performing max."
43459,ModuleNotFoundError: No module named 'tensorflow',"-using anaconda python v3.8
-working with jupyter notebook 
-try to add on cmd system --- conda install tensorflow but loading old version tensorflow and than give to me this error code:

```
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
```
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-1-62c45ef75a16> in <module>
----> 1 from tensorflow.keras.models import Sequential
      2 #modelleri oluşturmak için
      3 from tensorflow.keras.layers import Dense
      4 #katmanları da böyle oluştururuz

ModuleNotFoundError: No module named 'tensorflow'


"
43457,Unexpected accuracy output from tf.keras.Model.evaluate called on a saved and loaded model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1
- GPU model and memory: N/A (run with x86_64 CPU)


**Describe the current behavior**
A model is trained for a multiclass classification task. Labels are a class vector. Method [evaluate](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate) called on the trained model outputs an accuracy of 0.9579. After saving the model and loading the saved model, calling method `evaluate` on the loaded model outputs an accuracy of 0.0997.

**Describe the expected behavior**
I would have expected the same accuracy output (0.9579) on the loaded model.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1L4vgn3H5PquUJEgBY3PCrlGEHSXd6Rtc

**Other info / logs**
"
43455,"A commit message should be ""unit64""","In the commit message: https://github.com/tensorflow/tensorflow/commit/4be466a87efc152a8581febe7c1deaae562465af#diff-f0675f2568ff9470bcfc1f2bc79c5386, 

I find that it said ""int64"". But when I check the API documentation, it seems that it should be ""unit64"". Please look at parameter description: TF2.3:https://www.tensorflow.org/api_docs/python/tf/Variable#__mod__
TF2.2:  https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/Variable#__mod__"
43453,InaccessibleTensorError: The tensor cannot be accessed here: it is defined in another function or code block.,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 20.04 LTS (GNU/Linux 4.4.0-18362-Microsoft x86_64)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I was trying to create a custom model using the Subclassing API by subclassing the tf.keras.models.Model class. After creating the model and compiling, while training the model the following issue was raised
`InaccessibleTensorError: The tensor 'Tensor(""mul:0"", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=139645200248064); accessed from: FuncGraph(name=train_function, id=139645180620608).`

**Describe the expected behavior**
I have tried using the Subclassing API before but i haven't seen such an error raised. Maybe some new updates in TF.

**Standalone code to reproduce the issue**
Link to the gist
[https://gist.github.com/Gokul-S-Kumar/4834f19e78a3415caf8612327812b599](url)

**Other info/logs** 
Link to the error description, in case useful:
[https://gist.github.com/Gokul-S-Kumar/8d36b7fd40aded0fbc7a7a3c3a3a1baa](url)

PS:- I may be committing a simple mistake, if yes please help in pointing it out so that I can rectify it."
43452,failed precondition: error while reading resource variable block8_sepconv3_bn/gamma,"Hello,

I have found very few documentation concerning the error : failed precondition: error while reading resource variable block8_sepconv3_bn/gamma from Container: localhost. This could mean that the variable was uninitialized. Not found : Resource localhost/block8_speconv3_bn/gamma/class tensorflow::Var does not exist

I guess the problem must be in the train function before the model.predict and has something to do with the sessions and threads but I would be happy to get any help I can get.

I use : 
Python 3.7.8
Cuda 10.1
CUDNN v7.6.5
Tensorflow 2.3.0
keras 2.4.3


You can find my code in attachment or here : https://github.com/8rax/Carla/blob/master/JDO_Tutorial_5.py
[Carla_Env_DQN.zip](https://github.com/tensorflow/tensorflow/files/5259658/Carla_Env_DQN.zip)

"
43451,how to save weights without optimizer weights by using checkpoint file format in tensorflow keras model?,
43450,Feature request: Test in cl_build for the esp32,"Please make a test for the esp32 in the folder 

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/tools/ci_build



![image](https://user-images.githubusercontent.com/5605614/93850427-ec51cd80-fc62-11ea-93cf-850023ba26ad.png)


Many other builds have tests and the esp32 has a makefile here https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/tools/make/targets
"
43449,tf.autodiff.ForwardAccumulator fails for Embedding layer,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.4.0-dev20200813
- Python version: 3.8.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1.168/7.6.5
- GPU model and memory:  GeForce GTX 1050/4GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Calculating the Jacobian-vector product of an embedding layer produces
`AttributeError: 'IndexedSlices' object has no attribute '_as_tf_output'`

**Describe the expected behavior**
No error, just like Dense, LSTM, convolutional layers. See a notebook link below that shows this error is ONLY related to Embedding layer.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf

class RNN_Model(tf.keras.Model):
    def __init__(self):
        super(RNN_Model, self).__init__()
        self.embed=tf.keras.layers.Embedding(5,1)
        self.d2 = tf.keras.layers.Dense(2)
        self(tf.constant([4,3,2])) # initialize
    
    @tf.function
    def call(self, x):
        x = self.embed(x)
        return self.d2(x)
     
model=RNN_Model()

v=[tf.ones(w.shape) for w in model.trainable_variables]
with tf.autodiff.ForwardAccumulator(primals = model.trainable_variables, tangents = v) as acc:
    loss = tf.reduce_sum(tf.constant([1,0])-model(tf.constant([[2,2,2], [1,1,1]]), training=True))
acc.jvp(loss)
```

See a complete example in [this notebook](https://drive.google.com/file/d/10Cb1nxcovmBSNE5zJOvRhyu-lHYhux15/view?usp=sharing)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43448,Complete functionality of SparseTensor.__mul__,"**System information**
- TensorFlow version (you are using): 2,3
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Currently SparseTensor.__mul__ is limited to multiplication between a sparse tensor and a dense tensor. Broadcasting is only supported from the dense side to the sparse side, as per the documentation: https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor#__mul__

My request is to expand this behaviour to also include multiplications between two sparse tensors, and to allow broadcasting in both directions. This will complete the functionality and make the use of sparse tensors similar to that of dense tensors.

**Will this change the current api? How?**
The current options should of course still be supported, but more tensor combinations should be supported. This is purely the removal of a limitation that is already stated in the API.

**Who will benefit with this feature?**
Anyone who uses SparseTensors will want to manipulate them using the basic functions like multiplication and division (which can probably be added at the same time). This is certainly true for my own use in Graph Neural networks, but sparse tensor calculations are needed in many other applications as well.

**Any Other info.**
I have written a pure python implementation of broadcast multiply between two sparse tensors, which is shared in this stackoverflow question: https://stackoverflow.com/questions/63023958/how-to-efficiently-broadcast-multiply-two-sparse-tensors-in-tensorflow. However, as noted there, it is not memory efficient enough to be really useful. I expect that a C++ implementation is needed, and unfortunately I do not know how to write that. If I can contribute any more code, I would be happy to, but my knowledge of C++ and CUDA is limited."
43447,top_k crashes for certain large structures,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version:
3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
10.1 update 2
- GPU model and memory:
GTX 960m

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
The following operation crashes (possible because of gpu l2 usage (?)
```python
import tensorflow as tf
x = tf.random.stateless_normal(shape=(1_000_000, 2), dtype=tf.float32, seed=(4,2))
print(tf.math.top_k(x, k=1)[0][:2])
```

Crashes on my machine. Interestingly using a shape of (2, 1_000_000) works fine

**Describe the expected behavior**
it shouldn't crash, and if there is a memory limitation it should be specified in the crash. this problem user that indirectly use top_k and not throwing the correct exception makes it very hard to debug (see https://github.com/tensorflow/probability/issues/1086)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


```
2020-09-22 06:51:17.737213: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-09-22 06:51:19.945793: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-09-22 06:51:20.385293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0
coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s
2020-09-22 06:51:20.385745: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-09-22 06:51:20.390534: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-09-22 06:51:20.395306: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-09-22 06:51:20.396930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-09-22 06:51:20.402414: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-09-22 06:51:20.405454: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-09-22 06:51:20.415167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-09-22 06:51:20.416576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-09-22 06:51:20.417067: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-09-22 06:51:20.427041: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1a8f253a180 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-22 06:51:20.427523: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-22 06:51:20.429145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0
coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s
2020-09-22 06:51:20.429756: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-09-22 06:51:20.429991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-09-22 06:51:20.430219: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-09-22 06:51:20.430449: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-09-22 06:51:20.430672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-09-22 06:51:20.430902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-09-22 06:51:20.431136: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-09-22 06:51:20.432293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-09-22 06:51:21.959567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-22 06:51:21.959917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-09-22 06:51:21.960127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-09-22 06:51:21.961302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3031 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2020-09-22 06:51:21.965674: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1a89849b300 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-22 06:51:21.966109: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0
2020-09-22 06:51:25.072632: E tensorflow/stream_executor/cuda/cuda_driver.cc:939] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure :: 0x00007FFBB77F8E85	tensorflow::CurrentStackTrace
0x00007FFBB752A4BE	tensorflow::DeviceProperties::l2_cache_size
0x00007FFBB7530BEE	stream_executor::StreamExecutor::EnablePeerAccessTo
0x00007FFBA431BF38	tensorflow::StepStats::internal_default_instance
0x00007FFBA432D064	google::protobuf::RepeatedPtrField<tensorflow::InterconnectLink>::Add
0x00007FFBA4067712	std::vector<tensorflow::DtypeAndPartialTensorShape,std::allocator<tensorflow::DtypeAndPartialTensorShape> >::operator=
0x00007FFB9EE4A4D1	tensorflow::MemoryLogRawDeallocation::deferred
0x00007FFB9EE54A01	TFE_TensorHandleResolve
0x00007FFB9EBBD6A3	TFE_Py_TensorShapeSlice
0x00007FFB9EBBADDA	std::vector<tensorflow::monitoring::Point::Label,std::allocator<tensorflow::monitoring::Point::Label> >::reserve
0x00007FFC7010BDEF	PyMethodDef_RawFastCallKeywords
0x00007FFC7010B6E7	PyArg_UnpackStack
0x00007FFC70118484	PyEval_EvalFrameDefault
0x00007FFC7010B841	PyArg_UnpackStack
0x00007FFC70118484	PyEval_EvalFrameDefault
0x00007FFC7010C0FC	PyEval_EvalCodeWithName
0x00007FFC7010B95C	PyArg_UnpackStack
0x00007FFC70118A6E	PyEval_EvalFrameDefault
0x00007FFC7010B294	PyFunction_FastCallDict
0x00007FFC700F22D4	PyObject_FastCall_Prepend
0x00007FFC700F2225	PySequence_GetItem
0x00007FFC700FE78D	PyObject_Str
0x00007FFC7013807E	PyFile_WriteObject
0x00007FFC70137F5D	PySys_EndInit
0x00007FFC7010BE06	PyMethodDef_RawFastCallKeywords
0x00007FFC7010B8A3	PyArg_UnpackStack
0x00007FFC70118A6E	PyEval_EvalFrameDefault
0x00007FFC7010C0FC	PyEval_EvalCodeWithName
0x00007FFC70120197	PyEval_EvalCodeEx
0x00007FFC701200F5	PyEval_EvalCode
0x00007FFC7012009F	PyArena_Free
0x00007FFC70293E5D	PyRun_FileExFlags
0x00007FFC702945F9	PyRun_SimpleFileExFlags
0x00007FFC70293D3B	PyRun_AnyFileExFlags
0x00007FFC701E1054	Py_UnixMain
0x00007FFC701E10FB	Py_UnixMain
0x00007FFC7017936E	PyErr_NoMemory
0x00007FFC701309DB	Py_Main
0x00007FFC701309B6	Py_Main
0x00007FF7E889126D	(unknown)
0x00007FFCBAAA6FD4	BaseThreadInitThunk
0x00007FFCBB0DCEC1	RtlUserThreadStart

Traceback (most recent call last):
  File ""<project>/bug4.py"", line 6, in <module>
    print(tf.math.top_k(x, k=1)[0][:2])
  File ""<env>\lib\site-packages\tensorflow\python\framework\ops.py"", line 904, in __str__
    return ""tf.Tensor(%s, shape=%s, dtype=%s)"" % (numpy_text(self), self.shape,
  File ""<env>\lib\site-packages\tensorflow\python\framework\ops.py"", line 264, in numpy_text
    text = repr(tensor._numpy()) if is_repr else str(tensor._numpy())
  File ""<env>\lib\site-packages\tensorflow\python\framework\ops.py"", line 929, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: GPU sync failed

Process finished with exit code 1
```"
43436,TF 2.3 concrete function spec issue: list index out of range,"In TF 2.3, if we define a model in `class`, it errors out with following error when run predict:

```
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _structured_signature_check_arg_types(self, args, kwargs)
   1778     arg_specs, kwarg_specs = self.structured_input_signature
   1779     for i, (arg, spec) in enumerate(zip(args, arg_specs)):
-> 1780       name = self._function_spec.arg_names[i]
   1781       self._structured_signature_check_arg_type(arg, spec, name)
   1782     for (name, arg) in kwargs.items():

IndexError: list index out of range
```

Here's a small code sample to reproduce this issue:

https://colab.research.google.com/drive/1TukOd1vtYYtO5UrCYXl3VebAhcYZNYcc#scrollTo=uerkKqrPnTBg&uniqifier=1

Note: this issue is **only in TF 2.3** if you change the TF version (first line) to `!pip install tensorflow==2.2`. This code snippet runs without any issue. 
"
43433,nan is clipped to the upper,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.3
- Python version: 3.8
- CUDA/cuDNN version: 10.1/7.6
- GPU model and memory: TITAN RTX

**Describe the current behavior**
tf.clip_by_value(tf.constant(np.nan), -1, 1) == 1

**Describe the expected behavior**
tf.clip_by_value(tf.constant(np.nan), -1, 1) == NaN"
43431,import error -> tensorflow,"<em></em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 home edition 
- TensorFlow installed from (source or binary): Installed from conda using (pip install tensorflow) 
- TensorFlow version: 2.3.0
- Python version: 3.8.3
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA 
- CUDA/cuDNN version: NA 
- GPU model and memory: 940mx 



**Describe the problem**
Tensorflow installed successfully but unable to import it using ""import tensorflow as tf"" 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Installation Commands: pip install tensorflow

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
43430,ValueError: Failed to convert value into readable tensor.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source): 1.15

# Describe the problem
I am trying to convert the model to TFLite format using representative dataset. But I got the error : ValueError: Failed to convert value into readable tensor.

**Command used to run the converter or code if you’re using the Python API**

def rep_data_gen():
    f = '/home/pychen/traffic_3class/creDa_test_public/ImageSets/test_less.txt'
    f_name = np.loadtxt(f, dtype = np.str).reshape(-1)
    # print('Here', type(f_name))
    # print(f_name)
    NORM_H = 300
    NORM_W = 600
    for i in range(5):
        image = next(iter(f_name))
        image = tf.io.read_file(img_dir + image + '.jpg')
        image = tf.io.decode_jpeg(image, channels=3)
        image = tf.image.resize(image, [NORM_H, NORM_W])
        image = tf.cast(image / 255., tf.float32)
        image = tf.expand_dims(image, 0)
        yield [image]

frozen_graph='./tflite_graph.pb'
input_arrays=[""normalized_input_image_tensor""]
output_arrays=['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']
input_shapes={""normalized_input_image_tensor"":[1,300,600,3]}
converter = tf.lite.TFLiteConverter.from_frozen_graph(frozen_graph,input_arrays,output_arrays,input_shapes=input_shapes)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset=rep_data_gen
converter.allow_custom_ops=True

tflite_quant_model = converter.convert()

with open('quantized_model.tflite', 'wb') as f:
    f.write(tflite_quant_model)


# Copy and paste the output here.

Traceback (most recent call last):
  File ""convert_tflite.py"", line 79, in <module>
    tflite_quant_model = converter.convert()
  File ""/home/pychen/anaconda3/envs/tf_v115/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 993, in convert
    inference_output_type)
  File ""/home/pychen/anaconda3/envs/tf_v115/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 239, in _calibrate_quantize_model
    inference_output_type, allow_float)
  File ""/home/pychen/anaconda3/envs/tf_v115/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/calibrator.py"", line 75, in calibrate_and_quantize
    self._calibrator.FeedTensor(calibration_sample)
  File ""/home/pychen/anaconda3/envs/tf_v115/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py"", line 112, in FeedTensor
    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value)
ValueError: Failed to convert value into readable tensor.

"
43429,Warning when trying to Load CIFAR10 in Tensorflow 1.x,"**System information**
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Google Colab
- **TensorFlow version (use command below):** v1.15.2-0-g5d80e1e8e6 1.15.2
- **Python version:** 3.6.9

**Describe the current behavior**
WARNING:tensorflow:Entity <function _get_dataset_from_filename at 0x7f1866eac6a8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <function _get_dataset_from_filename at 0x7f1866eac6a8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'
WARNING: Entity <function _get_dataset_from_filename at 0x7f1866eac6a8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of FeaturesDict({
    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of FeaturesDict({
    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4
WARNING: Entity <bound method TopLevelFeature.decode_example of FeaturesDict({
    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4
**Describe the expected behavior**
"
43425,Negate Kernel for int8 is yet to be implemented,"There's a TODO to implement the negate kernel for the int8 data type of TFLite Micro: https://github.com/tensorflow/tensorflow/blob/b253e82d5360c83db0d2c4e958f7822b38a06660/tensorflow/lite/micro/kernels/neg.cc#L36
I already have the code written in a PR earlier, however, I have to restart due to the present guidelines, should I proceed or leave this as it seems it's not a high-priority concern at the moment."
43423,Cannot fit concatenated model to two datasets as input,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: MX 150

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior** Can't input two datasets when a model merges two models. The error I get is : 

`Failed to find data adapter that can handle input: (<class 'list'> containing values of types {""<class 'tensorflow.python.data.ops.dataset_ops.TensorDataset'>""}), <class 'NoneType'>`

**Describe the expected behavior** I should be able to input the two data sets or I should be able to somehow merge them together into a single dataset

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Code : 
```

import tensorflow as tf

# Create first model
model1 = tf.keras.Sequential()
model1.add(tf.keras.layers.Dense(1))
model1.compile()
model1.build([None,3])

# Create second model
model2 = tf.keras.Sequential()
model2.add(tf.keras.layers.Dense(1))
model2.compile()
model2.build([None,3])


# Concatenate
fusion_model = tf.keras.layers.Concatenate()([model1.output, model2.output])
t = tf.keras.layers.Dense(1, activation='tanh')(fusion_model)
model = tf.keras.models.Model(inputs=[model1.input, model2.input], outputs=t)
model.compile()

#Datasets
ds1 = tf.data.Dataset.from_tensors(([1,2,3],1))
ds2 = tf.data.Dataset.from_tensors(([1,2,3], 2))

print(ds1)
print(ds2)
# Fit
model.fit([ds1,ds2])
```


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43422,ValueError: Can only save/restore ResourceVariables when executing eagerly,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:v2.3.0-rc2-23-gb36436b087 
-   **Python version**:3.7.9
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:10.1/7.6.5
-   **GPU model and memory**:2080 Ti 
-   **Exact command to reproduce**:

### Describe the problem
When I run, prog run till base.save
While run tf.compat.v1.train.Saver(), it hint
`

    Traceback (most recent call last):    
      File ""main.py"", line 131, in <module>
           main()
      File ""main.py"", line 113, in main
           T.train()
      File ""/home/yang/Documents/paper/refer/MGNN-SPred-master/Train.py"", line 40, in train
           self.model.save()
      File ""/home/yang/Documents/paper/refer/MGNN-SPred-master/models/base.py"", line 116, in save
           self.saver.save(self.sess, name)
      File ""/home/yang/anaconda3/envs/tsorflw/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 1180, in save
           checkpoint_file, build_save=True, build_restore=False)
      File ""/home/yang/anaconda3/envs/tsorflw/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 852, in _build_eager
           checkpoint_path, build_save=build_save, build_restore=build_restore)
      File ""/home/yang/anaconda3/envs/tsorflw/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 886, in _build
            build_restore=build_restore)
      File ""/home/yang/anaconda3/envs/tsorflw/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 490, in _build_internal
            names_to_saveables)
      File ""/home/yang/anaconda3/envs/tsorflw/lib/python3.7/site-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 360, in validate_and_slice_inputs
            for converted_saveable_object in saveable_objects_for_op(op, name):
       File ""/home/yang/anaconda3/envs/tsorflw/lib/python3.7/site-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 209, in saveable_objects_for_op
           ""executing eagerly, got type: %s."" % type(op))
       ValueError: Can only save/restore ResourceVariables when executing eagerly, got type: <class'tensorflow.python.framework.ops.Tensor'>.
`
### Source code / logs
I run this model 
https://github.com/Autumn945/MGNN-SPred
and  change some tf1 code to tf2

`    
    
    def make_model(self):
        with tf.compat.v1.variable_scope('Graph', reuse=tf.compat.v1.AUTO_REUSE, regularizer=self.l2_loss('all')) as self.graph_scope:
            n = args.nb_nodes
            k = args.dim_k
            self.embedding_matrix = tf.compat.v1.get_variable(name='emb_w', shape=[n, k])
            with tf.compat.v1.variable_scope('graph_agg', reuse=tf.compat.v1.AUTO_REUSE) as self.graph_agg_scope:
                pass

        with tf.compat.v1.variable_scope('Network', reuse=tf.compat.v1.AUTO_REUSE, regularizer=self.l2_loss('all')):
            score, label = self.forward(*self.inputs)
            seq_loss = tf.compat.v1.losses.softmax_cross_entropy(label, score)
            tf.summary.scalar('seq_loss', seq_loss)

        self.loss = seq_loss
        self.loss += tf.compat.v1.losses.get_regularization_loss()

        opt = tf.compat.v1.train.AdamOptimizer(learning_rate=args.lr)
        self.minimizer = opt.minimize(self.loss)
        tf.summary.scalar('loss', self.loss)


        graph_var_list = tf.compat.v1.trainable_variables(scope='^Graph/')
        network_var_list = tf.compat.v1.trainable_variables(scope='^Network/')
        for v in graph_var_list:
            print('graph', v)
        for v in network_var_list:
            print('network', v)

        self.saver = tf.compat.v1.train.Saver()
        self.sess = self.get_session()
        self.sess.run(tf.compat.v1.global_variables_initializer())
    def save(self):
        name = f'{self.save_dir}/model.ckpt'
        self.saver.save(self.sess, name)
`

and 

` 

     for ep in range(args.epochs):
            pbar = tqdm(total=args.nb_vali_step, desc='training', leave=False)
            loss = []
            t0 = time.time()
            for _ in range(args.nb_vali_step):
                data = next(data_generator)
                _loss = self.model.fit(data)

                loss.append(_loss)
                pbar.update(1)
            pbar.close()
            train_time = time.time() - t0

            vali_v, vali_str = self.metric('vali')
            if vali_v > best_vali:
                brk = 0
                best_vali = vali_v
                self.model.save()
            else:
                brk += 1
            red = (brk == 0)`


Is it wrong ? Where? I need help"
43421,Unable to build Tensorflow 2.3 on OSX with LLVM,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: OSX 10.15.6
- TensorFlow installed from: source
- TensorFlow version: 2.3
- Python version: 3.7
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): clang version 10.0.1 
- CUDA/cuDNN version: No
- GPU model and memory: No GPU



**Describe the problem**

I am unable to build from source and I keep getting errors around:
> bazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/saved_object_graph.pb.h:131:3: error: expected identifier TRUE = 1,
> In file included from bazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/meta_graph.pb.h:43:
bazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/saved_object_graph.pb.h:132:3: error: expected identifier
  FALSE = 2,
  ^
/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include/mach/boolean.h:85:17: note: expanded from macro 'FALSE'
#define FALSE   0

I tried with different compilers from brew but this error keeps appearing.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

% export BAZEL_USE_CPP_ONLY_TOOLCHAIN=1
% export CC=/usr/local/opt/llvm/bin/clang
% bazel build --action_env CC=/usr/local/opt/llvm/bin/clang  --config=noaws --config=nogcp --config=mkl -c opt --copt=-O2 --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-mfma --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**

Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=174
INFO: Reading rc options for 'build' from /Users/stb/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/stb/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /Users/stb/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Users/stb/opt/anaconda3/envs/tfmkl_py37/bin/python3 --action_env PYTHON_LIB_PATH=/Users/stb/opt/anaconda3/envs/tfmkl_py37/lib/python3.7/site-packages --python_path=/Users/stb/opt/anaconda3/envs/tfmkl_py37/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /Users/stb/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/stb/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /Users/stb/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:noaws in file /Users/stb/tensorflow/.bazelrc: --define=no_aws_support=true
INFO: Found applicable config definition build:nogcp in file /Users/stb/tensorflow/.bazelrc: --define=no_gcp_support=true
INFO: Found applicable config definition build:mkl in file /Users/stb/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 -c opt
INFO: Found applicable config definition build:macos in file /Users/stb/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule git_repository defined at:
  /private/var/tmp/_bazel_stb/c17790719609824de969482a41b5bf78/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (390 packages loaded, 30618 targets configured).
INFO: Found 1 target...
ERROR: /Users/stb/tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:1620:1: C++ compilation of rule '//tensorflow/compiler/mlir/tensorflow:compile_mlir_util_no_tf_dialect_passes' failed (Exit 1)
In file included from tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.cc:16:
In file included from ./tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.h:29:
In file included from ./tensorflow/core/common_runtime/device.h:42:
In file included from ./tensorflow/core/framework/resource_mgr.h:32:
./tensorflow/core/framework/type_index.h:61:17: warning: unused variable 'hash_bit' [-Wunused-variable]
    static bool hash_bit[1];
                ^
In file included from tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.cc:45:
In file included from ./tensorflow/compiler/mlir/tensorflow/translate/import_model.h:25:
In file included from ./tensorflow/cc/saved_model/bundle_v2.h:28:
In file included from bazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/meta_graph.pb.h:43:
bazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/saved_object_graph.pb.h:131:3: error: expected identifier
  TRUE = 1,
  ^
/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include/mach/boolean.h:81:17: note: expanded from macro 'TRUE'
#define TRUE    1
                ^
In file included from tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.cc:45:
In file included from ./tensorflow/compiler/mlir/tensorflow/translate/import_model.h:25:
In file included from ./tensorflow/cc/saved_model/bundle_v2.h:28:
In file included from bazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/meta_graph.pb.h:43:
bazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/saved_object_graph.pb.h:132:3: error: expected identifier
  FALSE = 2,
  ^
/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include/mach/boolean.h:85:17: note: expanded from macro 'FALSE'
#define FALSE   0
                ^
In file included from tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.cc:45:
In file included from ./tensorflow/compiler/mlir/tensorflow/translate/import_model.h:25:
In file included from ./tensorflow/cc/saved_model/bundle_v2.h:28:
In file included from bazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/meta_graph.pb.h:43:
bazel-out/darwin-opt-exec-50AE0418/bin/tensorflow/core/protobuf/saved_object_graph.pb.h:138:31: error: cannot initialize a variable of type 'const tensorflow::ExperimentalCompile' with an rvalue of type 'int'
constexpr ExperimentalCompile ExperimentalCompile_MAX = FALSE;
                              ^                         ~~~~~
1 warning and 3 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /Users/stb/tensorflow/tensorflow/python/tools/BUILD:82:1 C++ compilation of rule '//tensorflow/compiler/mlir/tensorflow:compile_mlir_util_no_tf_dialect_passes' failed (Exit 1)
INFO: Elapsed time: 4667.715s, Critical Path: 318.82s
INFO: 5927 processes: 5927 local.
FAILED: Build did NOT complete successfully
"
43420,I encountered the following problems when installing the Go version of TensorFlow in the windows environment,"I encountered the following problems when installing the Go version of TensorFlow in the windows environment：

go run tensorflow.go
result：
..\..\..\github.com\tensorflow\tensorflow\tensorflow\go\saved_model.go:25:2: cannot find package ""github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto"" in any
of:
        C:\Go\src\github.com\tensorflow\tensorflow\tensorflow\go\core\protobuf\for_core_protos_go_proto (from $GOROOT)
        H:\project\src\github.com\tensorflow\tensorflow\tensorflow\go\core\protobuf\for_core_protos_go_proto (from $GOPATH)

------------------------
I installed tensorflow(CPU) of C , the installation location is: C:\Program Files\tensorflow\lib
"
43419,invalid syntax error while importing tensorflow and keras,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10
- TensorFlow installed from (source or binary):source
- TensorFlow version:1.8.0
- Python version:3.8.5
- Installed using virtualenv? pip? conda?:virtualenv


**Provide the exact sequence of commands / steps that you executed before running into the problem**
Traceback (most recent call last):
  File ""c:/Users/win10/Desktop/python projects/try.py"", line 5, in <module>
    import keras
  File ""C:\Users\win10\tensor\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from tensorflow.keras.layers.experimental.preprocessing import RandomRotation
  File ""C:\Users\win10\tensor\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\win10\tensor\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\win10\tensor\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\win10\tensor\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 124
    def TFE_ContextOptionsSetAsync(arg1, async):
                                         ^
SyntaxError: invalid syntax
![Capture](https://user-images.githubusercontent.com/47914144/93744112-eaf6b580-fc0e-11ea-9add-ba65b39da2e3.JPG)
![Capture](https://user-images.githubusercontent.com/47914144/93744138-f8ac3b00-fc0e-11ea-8af4-3fc8dae1565a.JPG)
![Capture](https://user-images.githubusercontent.com/47914144/93744171-082b8400-fc0f-11ea-8ab4-f743631a702a.JPG)




**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43416,Got Segmentation fault when calling tflite::Model::UnPack() ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):master
- Python version:3.7
- Bazel version (if compiling from source):2.0.0
- GCC/Compiler version (if compiling from source):7.5.0
- CUDA/cuDNN version:NA
- GPU model and memory:NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Pic.1:
![image](https://user-images.githubusercontent.com/36526001/93737881-e3d3a580-fc16-11ea-8c06-4fb29ea06305.png)
Pic.2:
![image](https://user-images.githubusercontent.com/36526001/93738189-bdfad080-fc17-11ea-8a54-bef065ec5ad7.png)
Pic.3:
![image](https://user-images.githubusercontent.com/36526001/93738347-25b11b80-fc18-11ea-9b6b-235368499445.png)

I'm doing benchmark on cartoongan model from tensorflow hub(https://tfhub.dev/sayakpaul/lite-model/cartoongan/int8/1). When I adding a line ""ModelT *modelT = model_->GetModel()->UnPack();"" to  tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc:712(Pic.1) for printing some debug info, I got a ""Segmentation fault"". I have found out that it happens when flatbuffers UnPacking zero_point in QuantizationParameters(Pic.2&3). 

**Describe the expected behavior**
Calling tflite::Model::UnPack() should return tflite::ModelT

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43409,In load_model method: Input 'values' passed int64 expected int32,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution:  Linux Ubuntu 20.04 and kaggle

- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.3.0
- Python version:3.7

**Describe the current behavior**
I have trained a model with custom loss and after save, I tried to load it but got this error

**2 errors while building NodeDef 'tf_op_layer_stack_2/stack_2' using Op<name=Pack; signature=values:N*T -> output:T; attr=N:int,min=1; attr=T:type; attr=axis:int,default=0>:
Input 'values' passed int64 expected int32
Inconsistent values for attr 'T' DT_INT32 vs. DT_INT64**


**Describe the expected behavior**
It should be loaded without any error

**Standalone code to reproduce the issue**
A minimalistic gist is [attached](https://gist.github.com/partha117/bbcd22f096c71d6f30858c4e560e9684).

"
43397,Hexagon delegate performance,"Looking at http://ai-benchmark.com/ranking_detailed.html I note that hexagon delegate performance is practically identical on snapdragon 855 und snapdragon 865, even though the DSP on SD 865 is much more powerful according to specs. Why is that ?"
43393,GPUDelegate Produces Incorrect Result for reduce_sum,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux (Kernel version 5.8.10)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung s10e (Android 10)
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.1-41975-g46b6537110 2.4.0
- Python version: 3.8.5
- Bazel version (if compiling from source): 3.5.0
- GCC/Compiler version (if compiling from source): 10.2.0
- CUDA/cuDNN version: CUDA 11.0.3/cuDNN 8.0.2.39
- GPU model and memory: 2080 Ti/11G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When using GPU delegate on mobile with reduce_sum, the result is incorrect (the absolute difference is larger than 1e-1). By contrast, If GPU is not used the result is correct.

**Describe the expected behavior**
The result difference should be relatively very small.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

The codes to generate the graph:
```python
import tensorflow as tf


def generate_buggy_graph(batch_size):
    with tf.Graph().as_default() as graph, tf.compat.v1.Session(graph=graph) as session:
        source = tf.compat.v1.placeholder(tf.float32, shape=[batch_size, 100])
        target = tf.transpose(tf.reduce_sum(tf.transpose(source), axis=1))
        # target = tf.reduce_sum(source, axis=0)

    converter = tf.compat.v1.lite.TFLiteConverter.from_session(session, [source], [target])
    with open(""buggy_graph.tflite"", ""wb"") as writer:
        writer.write(converter.convert())


generate_buggy_graph(256)
```

To push the generated model to the mobile I used the following:
```shell
adb push buggy_graph.tflite /storage/emulated/0/Android/data/com.example.mobilenn/files/models/BuggyGraph/buggy_graph.tflite
```

The codes to run the graph on mobile:

BuggyModel.java
```java
package com.example.mobilenn.lite.models;

import android.util.Log;

import org.tensorflow.lite.Interpreter;
import org.tensorflow.lite.gpu.GpuDelegate;
import org.tensorflow.lite.nnapi.NnApiDelegate;

import java.io.File;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.util.Locale;

public class BuggyModel {
    public static void run(File basePath) {
        Interpreter.Options interpreterOptions = new Interpreter.Options();
//        interpreterOptions.addDelegate(new NnApiDelegate());
        interpreterOptions.addDelegate(new GpuDelegate());
        Interpreter interpreter = new Interpreter(
                new File(basePath, ""buggy_graph.tflite""),
                interpreterOptions
        );

        ByteBuffer inputs = ByteBuffer
                .allocateDirect(interpreter.getInputTensor(0).numBytes())
                .order(ByteOrder.nativeOrder());
        ByteBuffer outputs = ByteBuffer
                .allocateDirect(interpreter.getOutputTensor(0).numBytes())
                .order(ByteOrder.nativeOrder());

        float[] stdAnswer = new float[100];
        for (int batchId = 0; batchId < 256; batchId++) {
            for (int channelId = 0; channelId < 100; channelId++) {
                float value = (float) Math.random();
                stdAnswer[channelId] += value;
                inputs.putFloat(value);
            }
        }
        interpreter.run(inputs, outputs);

        for (int channelId = 0; channelId < 100; channelId++) {
            Log.d(""BuggyModel"", String.format(
                    Locale.getDefault(),
                    ""Channel %d: real: %.3f correct: %.3f diff: %.3f"",
                    channelId,
                    outputs.getFloat(channelId * Float.BYTES),
                    stdAnswer[channelId],
                    outputs.getFloat(channelId * Float.BYTES) - stdAnswer[channelId]
            ));
        }
    }
}
```

MainActivity.java
```java
package com.example.mobilenn;

import android.os.Bundle;

import androidx.appcompat.app.AppCompatActivity;

import com.example.mobilenn.lite.models.BuggyModel;

import java.io.File;

public class MainActivity extends AppCompatActivity {

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        BuggyModel.run(new File(getExternalFilesDir(""models""), ""BuggyGraph""));
    }

}
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

When used with GPU delegate:
```plain
2020-09-20 18:32:39.361 24389-24389/com.example.mobilenn D/BuggyModel: Channel 0: real: 135.750 correct: 135.681 diff: 0.069
2020-09-20 18:32:39.361 24389-24389/com.example.mobilenn D/BuggyModel: Channel 1: real: 131.625 correct: 131.705 diff: -0.080
2020-09-20 18:32:39.361 24389-24389/com.example.mobilenn D/BuggyModel: Channel 2: real: 128.500 correct: 128.509 diff: -0.009
2020-09-20 18:32:39.361 24389-24389/com.example.mobilenn D/BuggyModel: Channel 3: real: 127.500 correct: 127.556 diff: -0.056
2020-09-20 18:32:39.362 24389-24389/com.example.mobilenn D/BuggyModel: Channel 4: real: 126.438 correct: 126.508 diff: -0.071
2020-09-20 18:32:39.362 24389-24389/com.example.mobilenn D/BuggyModel: Channel 5: real: 130.125 correct: 130.037 diff: 0.088
2020-09-20 18:32:39.362 24389-24389/com.example.mobilenn D/BuggyModel: Channel 6: real: 123.938 correct: 123.990 diff: -0.052
2020-09-20 18:32:39.362 24389-24389/com.example.mobilenn D/BuggyModel: Channel 7: real: 128.625 correct: 128.717 diff: -0.092
2020-09-20 18:32:39.363 24389-24389/com.example.mobilenn D/BuggyModel: Channel 8: real: 127.063 correct: 127.131 diff: -0.069
2020-09-20 18:32:39.363 24389-24389/com.example.mobilenn D/BuggyModel: Channel 9: real: 122.500 correct: 122.633 diff: -0.133
...
```

When used without any delegate or with NNAPI delegate:
```plain
2020-09-20 18:39:55.368 24973-24973/com.example.mobilenn D/BuggyModel: Channel 0: real: 135.449 correct: 135.449 diff: 0.000
2020-09-20 18:39:55.368 24973-24973/com.example.mobilenn D/BuggyModel: Channel 1: real: 130.922 correct: 130.922 diff: 0.000
2020-09-20 18:39:55.369 24973-24973/com.example.mobilenn D/BuggyModel: Channel 2: real: 128.056 correct: 128.056 diff: 0.000
2020-09-20 18:39:55.369 24973-24973/com.example.mobilenn D/BuggyModel: Channel 3: real: 133.586 correct: 133.586 diff: 0.000
2020-09-20 18:39:55.369 24973-24973/com.example.mobilenn D/BuggyModel: Channel 4: real: 139.664 correct: 139.664 diff: 0.000
2020-09-20 18:39:55.370 24973-24973/com.example.mobilenn D/BuggyModel: Channel 5: real: 130.863 correct: 130.863 diff: 0.000
2020-09-20 18:39:55.370 24973-24973/com.example.mobilenn D/BuggyModel: Channel 6: real: 127.332 correct: 127.332 diff: 0.000
2020-09-20 18:39:55.370 24973-24973/com.example.mobilenn D/BuggyModel: Channel 7: real: 130.422 correct: 130.422 diff: 0.000
2020-09-20 18:39:55.371 24973-24973/com.example.mobilenn D/BuggyModel: Channel 8: real: 130.867 correct: 130.867 diff: 0.000
2020-09-20 18:39:55.371 24973-24973/com.example.mobilenn D/BuggyModel: Channel 9: real: 122.770 correct: 122.770 diff: 0.000
...
```"
43387,So frustrating with this error: Failed to load the native TensorFlow runtime,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OS X
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version: 2.0.1
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
It is so frustrating that when i google this error, so many webpages are found but no (working) solutions are provided. If this is such a common error, can you guys provide a fix or an official solution for fixing the problem? 
Basically, i am trying to install tensorflow on my Mac. So what i did is:
pip install tensorflow
It was finished without any error.
But when I try:
 import tensorflow as tf

The common import error came up: Failed to load the native TensorFlow runtime.

I couldn't figure out what is the exact issue, is it because Python 3.7 does not work with tensorflow 2.0.1? I tried:
pip install tensorflow==1.15.0. 
But it says no 1.15.0 found in 2.0.1, not sure what does that mean.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
pip install tensorflow
import tensorflow as tf

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

"
43386,tf.lite.Optimize.DEFAULT - Hybrid models are not supported on TFLite Micro.,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.6
- TensorFlow installed from (source or binary): 2.3.0
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32

**Describe the problem**

I'm trying to convert a model for tflite, but keep hitting:

""Hybrid models are not supported on TFLite Micro.""

**Please provide the exact sequence of commands/steps when you ran into the problem**

Here is my model:

```
model = Sequential([
    Conv2D(4, 3, 
           padding='same',
           activation='relu',
           input_shape=(IMG_WIDTH, IMG_HEIGHT, 1),
           name='conv_layer1'),
    MaxPooling2D(name='max_pooling1'),
    Conv2D(4, 3, 
           padding='same',
           activation='relu',
           name='conv_layer2'),
    MaxPooling2D(name='max_pooling2', pool_size=(2,2)),
    Flatten(),
    Dense(
        20,
        activation='relu',
        name='hidden_layer'
    ),
    Dense(1, activation='sigmoid', name='output')
])
```

And here is the code I am using to convert the mode:

```
converter = tf.lite.TFLiteConverter.from_saved_model(""checkpoint.model"")
converter.optimizations = [tf.lite.Optimize.DEFAULT]
model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(model)
```

I have also tried `tf.lite.Optimize.OPTIMIZE_FOR_SIZE` which has the same issue. Removing all optimisations lets me 

Is there any way to avoid triggering this error with my model? Ideally, I would like to optimize my model to make it smaller.
"
43385,"TF 2.3: Learning rate is NOT printed in the training output when using ""ReduceLROnPlateau"" callback","**TensorFlow version:** 2.3.0
**Python version:** 3.8.2
**Issue:**
Given the TensorFlow version is upgraded to 2.3.0,
Given using ""ReduceLROnPlateau"" callback,
When perform training on the model with learning rate decay,
Then learning rate ""lr - xxx"" is NOT printed in the training output

```
reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=5)
history = model.fit(x_train, y_train, epochs=training_epochs, batch_size=batch_size, validation_data=(x_test, y_test), callbacks=[reduce_learning_rate])
```

**TensorFlow 2.2.0:** Showing learning rate in the output of each epoch
`133/133 [==============================] - 2s 15ms/step - loss: 6.5944 - accuracy: 0.5781 - val_loss: 0.1361 - val_accuracy: 0.9640 - lr: 0.0200`

**TensorFlow 2.3.0:** Not showing learning rate in the output of each epoch
`133/133 [==============================] - 2s 15ms/step - loss: 6.5944 - accuracy: 0.5781 - val_loss: 0.1361 - val_accuracy: 0.9640`"
43380,Using keras GRUCell layer in TFLM,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- TensorFlow installed from (source or binary): TFLM source, tf-nightly from installer
- Tensorflow version (commit SHA if source):  tf-nightly 2.4.0.dev20200918
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**
My goal is to use a network using a keras.GRUCell layer in TFLM.

This issue is somwhat connected to:
https://github.com/tensorflow/tensorflow/issues/42582
and
https://github.com/tensorflow/tensorflow/issues/41690
As they both arose from the goal of using a GRU layer. 



This is a sample model:
state0 = tf.keras.Input(shape=( 1,128), dtype='float32', name='state0')
main_input = tf.keras.Input(shape=(1, 200), dtype='float32', name='main_input')
gru0, state0_out = tf.keras.layers.GRUCell(128,reset_after=True,recurrent_activation='sigmoid',use_bias = True,name=""grucell0"")(main_input,state0)

model3 = tf.keras.models.Model(inputs=[main_input,state0],outputs = [gru0,state0_out])


after adding in SPLIT_V, I ran into two problems:
A. Missing SHAPE operator - I will open a PR shortly.

B. A problem with tensor allocation which I will describe and suggest a workaround for TFLM and open a PR if you think the solution is acceptable.



**Please provide the exact sequence of commands/steps when you ran into the problem**

I converted the above model to TFLite and then imported into TFLM.
The error I got was from AllocationInfoBuilder::AddTensors() in micro_allocator.cc

""Logic error in memory planner, tensor 14 has an invalid lifetime: ""
          ""first_created: 1, last_used: -1"",

I examined the ops the converter generated:
{'index': 0, 'op_name': 'SHAPE', 'inputs': array([0], dtype=int32), 'outputs': array([12], dtype=int32)
 **{'index': 1, 'op_name': 'UNPACK', 'inputs': array([12], dtype=int32), 'outputs': array([13, 14, 15], dtype=int32)**
 {'index': 2, 'op_name': 'RESHAPE', 'inputs': array([0, 3], dtype=int32), 'outputs': array([16], dtype=int32)
 {'index': 3, 'op_name': 'PACK', 'inputs': array([13,  5,  6], dtype=int32), 'outputs': array([17], dtype=int32)
 {'index': 4, 'op_name': 'FULLY_CONNECTED', 'inputs': array([16, 10, -1], dtype=int32), 'outputs': array([18], dtype=int32)
 {'index': 5, 'op_name': 'RESHAPE', 'inputs': array([18, 17], dtype=int32), 'outputs': array([19], dtype=int32)
 {'index': 6, 'op_name': 'ADD', 'inputs': array([19,  9], dtype=int32), 'outputs': array([20], dtype=int32)
 {'index': 7, 'op_name': 'SPLIT', 'inputs': array([ 7, 20], dtype=int32), 'outputs': array([21, 22, 23], dtype=int32)
 {'index': 8, 'op_name': 'RESHAPE', 'inputs': array([1, 4], dtype=int32), 'outputs': array([24], dtype=int32)
 {'index': 9, 'op_name': 'FULLY_CONNECTED', 'inputs': array([24, 11, -1], dtype=int32), 'outputs': array([25], dtype=int32)
 {'index': 10, 'op_name': 'SHAPE', 'inputs': array([1], dtype=int32), 'outputs': array([26], dtype=int32)
 {'index': 11, 'op_name': 'UNPACK', 'inputs': array([26], dtype=int32), 'outputs': array([27, 28, 29], dtype=int32)
 {'index': 12, 'op_name': 'PACK', 'inputs': array([27,  5,  6], dtype=int32), 'outputs': array([30], dtype=int32)
 {'index': 13, 'op_name': 'RESHAPE', 'inputs': array([25, 30], dtype=int32), 'outputs': array([31], dtype=int32)
 {'index': 14, 'op_name': 'ADD', 'inputs': array([31,  9], dtype=int32), 'outputs': array([32], dtype=int32)
 {'index': 15, 'op_name': 'SPLIT_V', 'inputs': array([32,  2,  7], dtype=int32), 'outputs': array([33, 34, 35], dtype=int32)
 {'index': 16, 'op_name': 'ADD', 'inputs': array([21, 33], dtype=int32), 'outputs': array([36], dtype=int32)
 {'index': 17, 'op_name': 'LOGISTIC', 'inputs': array([36], dtype=int32), 'outputs': array([37], dtype=int32)
 {'index': 18, 'op_name': 'SUB', 'inputs': array([ 8, 37], dtype=int32), 'outputs': array([38], dtype=int32)
 {'index': 19, 'op_name': 'ADD', 'inputs': array([22, 34], dtype=int32), 'outputs': array([39], dtype=int32)
 {'index': 20, 'op_name': 'LOGISTIC', 'inputs': array([39], dtype=int32), 'outputs': array([40], dtype=int32)
 {'index': 21, 'op_name': 'MUL', 'inputs': array([40, 35], dtype=int32), 'outputs': array([41], dtype=int32)
 {'index': 22, 'op_name': 'ADD', 'inputs': array([23, 41], dtype=int32), 'outputs': array([42], dtype=int32)
 {'index': 23, 'op_name': 'TANH', 'inputs': array([42], dtype=int32), 'outputs': array([43], dtype=int32)
 {'index': 24, 'op_name': 'MUL', 'inputs': array([38, 43], dtype=int32), 'outputs': array([44], dtype=int32)
 {'index': 25, 'op_name': 'MUL', 'inputs': array([37,  1], dtype=int32), 'outputs': array([45], dtype=int32)
 {'index': 26, 'op_name': 'ADD', 'inputs': array([45, 44], dtype=int32), 'outputs': array([46], dtype=int32)}

Notice that UNPACK uses tensors 14 and 15 as outputs but they aren't used later on in the model. This is also true for the next SHAPE/UNPACK pattern and tensors 28 and 29.

I assume this is caused for reasons unknown by the TFLite conveter? The model works fine in python so I assume the tensor allocator in TFLite doesn't check for this.

What happens in TFLM is that those tensors aren't ""caught"" in the flow marking last usage.

The solution/workaround I implemented and works for me was to initialize all tensors as persistent  (in the same way the current code does for the graph's output tensors):

for (int i = (subgraph->operators()->size() - 1); i >= 0; --i) {
    const auto* op = subgraph->operators()->Get(i);
    for (size_t n = 0; n < op->outputs()->size(); ++n) {
      const int tensor_index = op->outputs()->Get(n);
      AllocationInfo* current = &info_[tensor_index];
      current->last_used = subgraph->operators()->size() - 1;
    }
  }

Afterwards, the current implementation will handle all the tensors that serve as input to ops correctly and leave these problematic ones as persistent to the end of the invocation.

Another option would be to search specifically for this case and handle it separately and
YET another option would be to see what in the TFLite converter is doing this...

@advaitjain Please let me know what you think, I'd really like to get this sorted out :)







"
43379,"tf.keras.Model with multiple outputs, using fit() with a generator dataset passes y to loss with wrong shape","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (Docker on WSL)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.9:
- CUDA/cuDNN version: 10.1 / 7
- GPU model and memory: RTX 2070 Super 8GB

**Describe the current behavior**
When using a tf.keras.Model with multiple outputs, then using fit() with a generator dataset (created with tf.data.Dataset.from_generator), the loss function is passed a wrong shape (looks to be the shape of a flattened array of the y's for all toutputs).


**Describe the expected behavior**
The loss function should be passed the correct shape from the generator.


**Standalone code to reproduce the issue**

Here I am creating a model with one input of shape (1), and 2 outputs of shape (32) and a SparseCategoricalCrossentropy for the loss. Then I am creating a generator dataset with batch size 2 which yields 2 inputs, and 2 arrays with value for each output.
```python
import tensorflow as tf
from tensorflow.keras import datasets, layers, models

inp = layers.Input((1))
out1 = layers.Dense(32)(inp)
out2 = layers.Dense(32)(inp)

model = models.Model(inputs=[inp], outputs=[out1, out2])
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy())

def gen():
    for i in itertools.count(1):
        yield [1, 2], [[11, 12], [21, 22]]

generator_dataset = tf.data.Dataset.from_generator(
    gen,
    (tf.uint8, tf.uint8),
    output_shapes=(
        tf.TensorShape((2, 1)),
        tf.TensorShape((2, 2))
    )
)

model.fit(generator_dataset)
```
Collab notebook: https://colab.research.google.com/drive/1vEoWRzSKyeJCNkucqmkGQrVQp0mrJE4u

**Other info / logs**
Below I post the error I get from the example above. The loss is being passed a tensor of shape (4,), which is the flattened shape of the y's instead of the correct shape, (2,1).
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-4b723daf89ac> in <module>()
     22 )
     23 
---> 24 model.fit(generator_dataset)

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1096                 batch_size=batch_size):
   1097               callbacks.on_train_batch_begin(step)
-> 1098               tmp_logs = train_function(iterator)
   1099               if data_handler.should_sync:
   1100                 context.async_wait()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = ""nonXla""
--> 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    821       # This is the first call of __call__, so we have to initialize.
    822       initializers = []
--> 823       self._initialize(args, kwds, add_initializers_to=initializers)
    824     finally:
    825       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    695     self._concrete_stateful_fn = (
    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 697             *args, **kwds))
    698 
    699     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-> 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3073             arg_names=arg_names,
   3074             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3075             capture_by_value=self._capture_by_value),
   3076         self._function_attributes,
   3077         function_spec=self.function_spec,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    599         # the function a weak reference to itself to avoid a reference cycle.
--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    601     weak_wrapped_fn = weakref.ref(wrapped_fn)
    602 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    971           except Exception as e:  # pylint:disable=broad-except
    972             if hasattr(e, ""ag_error_metadata""):
--> 973               raise e.ag_error_metadata.to_exception(e)
    974             else:
    975               raise

ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:749 train_step
        y, y_pred, sample_weight, regularization_losses=self.losses)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:149 __call__
        losses = ag_call(y_true, y_pred)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:253 call  **
        return ag_fn(y_true, y_pred, **self._fn_kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1567 sparse_categorical_crossentropy
        y_true, y_pred, from_logits=from_logits, axis=axis)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4783 sparse_categorical_crossentropy
        labels=target, logits=output)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py:4176 sparse_softmax_cross_entropy_with_logits_v2
        labels=labels, logits=logits, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py:4091 sparse_softmax_cross_entropy_with_logits
        logits.get_shape()))

    ValueError: Shape mismatch: The shape of labels (received (4,)) should equal the shape of logits except for the last dimension (received (2, 32)).
```
"
43378,google colab crashed after run tensofrflow code which is upgraded from 1.8 to 2.2,"I subscribed in colab pro and I have enough RAM but the code crashes the colab after upgrade it from TF1.8 to TF 2.2, how to solve this problem
![colab](https://user-images.githubusercontent.com/26867350/93712611-952bfa00-fb89-11ea-9e6f-eb4df272bd9a.png)
 "
43377,Extract Graph in TensorFlow 2,"I'm wondering how one might extract a graph in Tensorflow2.
Something like [this](https://github.com/tensorflow/tensorflow/blob/ae3c8479f88da1cd5636b974f653f27755cb0034/tensorflow/tensorboard/components/tf-tensorboard/test/data/graph_run_run2.pbtxt) in the pbtxt format: 
```
node {
  name: ""a""
  op: ""matmul""
}
node {
  name: ""b""
  op: ""matmul""
  input: ""a:0""
}
node {
  name: ""c""
  op: ""matmul""
  input: ""a:0""
  input: ""b:0""
}
```

It was pretty straightforward in TF1."
43376,"I have found this error. tensorflow.python.framework.errors_impl.InvalidArgumentError:  indices[5] = 1 is not in [0, 1)","```
# set the matplotlib backend so figures can be saved in the background
import matplotlib
matplotlib.use(""Agg"")
# import the necessary packages
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.optimizers import Adagrad
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from keras.layers import Dense, Embedding
from pyimagesearch.cancernet import CancerNet
from pyimagesearch import config
from imutils import paths
import matplotlib.pyplot as plt
import numpy as np
import argparse
import os

# determine the total number of image paths in training, validation,
# and testing directories
trainPaths = list(paths.list_images(config.TRAIN_PATH))
totalTrain = len(trainPaths)
totalVal = len(list(paths.list_images(config.VAL_PATH)))
totalTest = len(list(paths.list_images(config.TEST_PATH)))
# calculate the total number of training images in each class and
# initialize a dictionary to store the class weights
trainLabels = [int(p.split(os.path.sep)[-2]) for p in trainPaths]
trainLabels = to_categorical(trainLabels)
classTotals = trainLabels.sum(axis=0)
classWeight = dict()
# loop over all classes and calculate the class weight
for i in range(0, len(classTotals)):
	classWeight[i] = classTotals.max() / classTotals[i]
# construct the argument parser and parse the arguments
	ap = argparse.ArgumentParser()
	ap.add_argument(""-p"", ""--plot"", type=str, default=""plot.png"",
					help=""path to output loss/accuracy plot"")
	args = vars(ap.parse_args())
	# initialize our number of epochs, initial learning rate, and batch
	# size
	NUM_EPOCHS = 2

	INIT_LR = 1e-2
	BS = 32
	# initialize the training data augmentation object
	trainAug = ImageDataGenerator(
		rescale=1 / 255.0,
		rotation_range=20,
		zoom_range=0.05,
		width_shift_range=0.1,
		height_shift_range=0.1,
		shear_range=0.05,
		horizontal_flip=True,
		vertical_flip=True,
		fill_mode=""nearest"")
	# initialize the validation (and testing) data augmentation object
	valAug = ImageDataGenerator(rescale=1 / 255.0)
	# initialize the training generator
	trainGen = trainAug.flow_from_directory(
		config.TRAIN_PATH,
		class_mode=""categorical"",
		target_size=(48, 48),
		color_mode=""rgb"",
		shuffle=True,
		batch_size=BS)
	# initialize the validation generator
	valGen = valAug.flow_from_directory(
		config.VAL_PATH,
		class_mode=""categorical"",
		target_size=(48, 48),
		color_mode=""rgb"",
		shuffle=False,
		batch_size=BS)
	# initialize the testing generator
	testGen = valAug.flow_from_directory(
		config.TEST_PATH,
		class_mode=""categorical"",
		target_size=(48, 48),
		color_mode=""rgb"",
		shuffle=False,
		batch_size=BS)
	# initialize our CancerNet model and compile it
	model = CancerNet.build(width=48, height=48, depth=3,
							classes=2)
	model.add(Embedding(batch_size=32, input_shape=(classWeight,), input_dim=1024*1000, output_dim=256))

	opt = Adagrad(lr=INIT_LR, decay=INIT_LR / NUM_EPOCHS)
	model.compile(loss=""binary_crossentropy"", optimizer=opt,
				  metrics=[""accuracy""])
	# fit the model
	H = model.fit(
		x=trainGen,
		steps_per_epoch=totalTrain // BS,
		validation_data=valGen,
		validation_steps=totalVal // BS,
		class_weight=classWeight,
		epochs=NUM_EPOCHS)
# reset the testing generator and then use our trained model to
# make predictions on the data
print(""[INFO] evaluating network..."")
testGen.reset()
predIdxs = model.predict(x=testGen, steps=(totalTest // BS) + 1)
# for each image in the testing set we need to find the index of the
# label with corresponding largest predicted probability
predIdxs = np.argmax(predIdxs, axis=1)
# show a nicely formatted classification report
print(classification_report(testGen.classes, predIdxs,
	target_names=testGen.class_indices.keys()))
# compute the confusion matrix and and use it to derive the raw
# accuracy, sensitivity, and specificity
cm = confusion_matrix(testGen.classes, predIdxs)
total = sum(sum(cm))
acc = (cm[0, 0] + cm[1, 1]) / total
sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])
specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])
# show the confusion matrix, accuracy, sensitivity, and specificity
print(cm)
print(""accuracy: {:.4f}"".format(acc))
print(""sensitivity: {:.4f}"".format(sensitivity))
print(""specificity: {:.4f}"".format(specificity))
# plot the training loss and accuracy
N = NUM_EPOCHS
plt.style.use(""ggplot"")
plt.figure()
plt.plot(np.arange(0, N), H.history[""loss""], label=""train_loss"")
plt.plot(np.arange(0, N), H.history[""val_loss""], label=""val_loss"")
plt.plot(np.arange(0, N), H.history[""accuracy""], label=""train_acc"")
plt.plot(np.arange(0, N), H.history[""val_accuracy""], label=""val_acc"")
plt.title(""Training Loss and Accuracy on Dataset"")
plt.xlabel(""Epoch #"")
plt.ylabel(""Loss/Accuracy"")
plt.legend(loc=""lower left"")
plt.savefig(args[""plot""])
```"
43375,[RNN] Error while converting the Decoder in image-Captioning model to TFLite file.,"**System information**
-Google Colab
-TensorFlow version: 2.3.0


**Command used to run the converter or code if you’re using the Python API**
```
converter = tf.lite.TFLiteConverter.from_keras_model(encoder)
tflite_model = converter.convert()
open(""decoder.tflite"", ""wb"").write(tflite_model)
```

**The output from the converter invocation**

```
TypeError                                 Traceback (most recent call last)
<ipython-input-35-c6f70bd3ca2b> in <module>()
----> 1 tflite_model2 = converter2.convert()
      2 open(""decoder.tflite"", ""wb"").write(tflite_model2)

11 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)
    802 
    803     func = _saving_utils.trace_model_call(self._keras_model, input_signature)
--> 804     concrete_func = func.get_concrete_function()
    805     self._funcs = [concrete_func]
    806 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)
   1165       ValueError: if this object has not yet been called on concrete values.
   1166     """"""
-> 1167     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
   1168     concrete._garbage_collector.release()  # pylint: disable=protected-access
   1169     return concrete

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
   1071       if self._stateful_fn is None:
   1072         initializers = []
-> 1073         self._initialize(args, kwargs, add_initializers_to=initializers)
   1074         self._initialize_uninitialized_variables(initializers)
   1075 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    695     self._concrete_stateful_fn = (
    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 697             *args, **kwds))
    698 
    699     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-> 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3073             arg_names=arg_names,
   3074             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3075             capture_by_value=self._capture_by_value),
   3076         self._function_attributes,
   3077         function_spec=self.function_spec,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    599         # the function a weak reference to itself to avoid a reference cycle.
--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    601     weak_wrapped_fn = weakref.ref(wrapped_fn)
    602 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py in _wrapped_model(*args)
    132     with base_layer_utils.call_context().enter(
    133         model, inputs=inputs, build_graph=False, training=False, saving=True):
--> 134       outputs = model(inputs, training=False)
    135 
    136     # Outputs always has to be a flat dict.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    983 
    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):
--> 985           outputs = call_fn(inputs, *args, **kwargs)
    986 
    987         if self._activity_regularizer:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    300   def wrapper(*args, **kwargs):
    301     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
--> 302       return func(*args, **kwargs)
    303 
    304   if inspect.isfunction(func) or inspect.ismethod(func):

TypeError: call() missing 2 required positional arguments: 'features' and 'hidden'
```

**Also, please include a link to the saved model or GraphDef**

```
https://colab.research.google.com/drive/106LqEnqXNpTp110ZW1JBguPE4RdiapIj?usp=sharing
```

**Failure details**
TypeError: call() missing 2 required positional arguments: 'features' and 'hidden'
I converted the encoder in the same fashion and I could obtain the corresponding TFLite file without errors.
I believe this error is in some way related to how we are defining the decoder using RNN. (Please correct me if I'm wrong).

"
43374,XLA future,"As declared the XLA shows promising results for certain use cases, its specialization for executable size reduction, and TensorFlow could use `tfcompile` tool that leverages XLA for ahead-of-time compilation (AOT). I would wonder if there is the implementation supporting other alternative backends and devices (e.g. ARM64)?
What is the interconnection between XLA and  TensorFlow Lite?

"
43373,"TF2.0, how to include feature processing code in saved model.","`<em>Please` make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution : macOS Catalina 10.15.3
- TensorFlow installed from : binary
- TensorFlow version : 2.2.0
- Python version: 3.7.3

Hi, I have a working code (shown below): it processed the raw input (in tensor functions); then the processed input is used as the input of the keras model. I'm able to save the model, and also can make inference after loading model. But my problem is: this feature processing is done before inputing to the keras model, so it is not part of the saved model. When I run inference, I need to process the raw input first, then run model.predict on the processed data.

Because we would like to put the model on production, we need to include the feature processing code into the saved model, so the raw data can be fed into the saved model directly. I've tried different ways to make it work, but all failed. Please help, thanks!

Note that in the following code, 'FeatureProcess' is the function processing the raw input, I don't show the details of this 'FeatureProcess' function due to privacy reason. Inside this 'FeatureProcess' function is all tensor functions and manipulations.

def line_to_multiple_features(inputString, mode):
  inputString = [inputString]
  batch_ids, input_ids_long, labels, input_ids = FeatureProcess(inputString)
  d = {'ids':batch_ids[0], 'input_ids_long':input_ids_long[0], 'input_ids':input_ids[0]}
  if mode == 'train':
    return d, labels[0]
  else:
    return d

def get_unbatched_feature_label(filename, mode, epochs, feature_delimiter):
  dataset = tf.data.TextLineDataset(filename)
  if mode == 'train':
    dataset = dataset.repeat(epochs).shuffle(10000)
    # convert every line to features
  unbatch_parse_data = dataset.map(lambda x: line_to_multiple_features(x))
  return unbatch_parse_data

# Dataset for input data
def get_feature_label(_input):
  input_data = ctx.absolute_path(_input)
  input_files = tf.data.Dataset.list_files(input_data)
  datasets_unbatched = get_unbatched_feature_label(input_files, args.mode, args.epochs, '\t')
  data_feature_label = datasets_unbatched.batch(GLOBAL_BATCH_SIZE)
  return data_feature_label

def build_keras_model():
  vocab_size = 100000 
  embedding_dim = args.hidden_size
  num_filters = 512
  filter_sizes = [2,3,4]
  drop = 0.5

  inputs = {'ids':Input(shape=(4, ), dtype='string'),
              'input_ids_long':Input(shape=(args.seq_length_long, ), dtype='int32'),
              'input_ids':Input(shape=(args.seq_length, ), dtype='int32'))
             }

  embedding_short = Embedding(input_dim=vocab_size, output_dim=embedding_dim, 
  input_length=args.seq_length)(inputs['input_ids'])
  embedding_long = Embedding(input_dim=vocab_size, output_dim=embedding_dim, 
  input_length=args.seq_length_long)(inputs['input_ids_long'])
  reshape_short = Reshape((args.seq_length, embedding_dim, 1))(embedding_short)
  reshape_long = Reshape((args.seq_length_long, embedding_dim, 1))(embedding_long)
  conv_0_short = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_short)
  conv_1_short = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_short)
  conv_2_short = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_short)

  conv_0_long = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_long)
  conv_1_long = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_long)
  conv_2_long = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_long)

  maxpool_0_short = MaxPool2D(pool_size=(args.seq_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0_short)
  maxpool_1_short = MaxPool2D(pool_size=(args.seq_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1_short)
  maxpool_2_short = MaxPool2D(pool_size=(args.seq_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2_short)

  maxpool_0_long = MaxPool2D(pool_size=(args.seq_length_long - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0_long)
  maxpool_1_long = MaxPool2D(pool_size=(args.seq_length_long - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1_long)
  maxpool_2_long = MaxPool2D(pool_size=(args.seq_length_long - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2_long)

  concatenated_tensor = Concatenate(axis=1)([maxpool_0_short, maxpool_1_short, maxpool_2_short, maxpool_0_long, maxpool_1_long, maxpool_2_long])
  flatten = Flatten()(concatenated_tensor)
  dropout = Dropout(drop)(flatten)
  logits = Dense(units=args.num_classes)(dropout)
  pred_probs = tf.keras.activations.sigmoid(logits)
  model = Model(inputs=inputs, outputs=pred_probs)

  def get_loss_fn():
    def get_loss(labels, outputs):
      logits = tf.math.log((outputs+0.0000001) / ( 1 - outputs+0.0000001))
      loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)
      cost = tf.reduce_mean(
                tf.reduce_sum(loss, axis=1)
            )
      return cost
    return get_loss

  adam = Adam(lr=0.001)
  model.compile(optimizer=adam,
                  loss=get_loss_fn(),
                  metrics=['accuracy'])
  return model

tf.io.gfile.makedirs(args.model_dir)
filepath = args.model_dir # + ""/weights-{epoch:04d}""
callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=filepath, monitor=""val_accuracy"", verbose=1, save_best_only=True, mode='auto', save_freq=10)]
steps_per_epoch = 60000 / GLOBAL_BATCH_SIZE * 0.9

if args.mode == 'train':
  with strategy.scope():
    multi_worker_model = build_keras_model()
  train_feature_label = get_feature_label(args.input)
  validation_feature_label = get_feature_label(args.val_path)
  multi_worker_model.fit(x=train_feature_label, validation_data=validation_feature_label, 
                                        epochs=args.epochs, steps_per_epoch=steps_per_epoch, callbacks=callbacks)
  results = multi_worker_model.evaluate(validation_feature_label, verbose=2)
  multi_worker_model.save(args.model_dir)
else: 
  print('now is inference mode')
  test_feature = get_feature_label(args.test_path)
  reconstructed_model = tf.keras.models.load_model(args.model_dir, compile=False)
  for elem in test_feature:
    y_pred_elem = reconstructed_model.predict(elem)"
43372,Build from source for the Raspberry Pi: Bazel binary wasn't found,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Buster 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4 8Gb 
- TensorFlow installed from (source or binary): source 
- TensorFlow version: 2.3
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.5.0
- GCC/Compiler version (if compiling from source): 8.3
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I had a similar issue as #43232, when I am trying to deploy a TF2 Mask RCNN model to the Pi. Hence, I am trying to follow https://www.tensorflow.org/install/source_rpi#python-3.7 to get the TF nightly to the Pi. 

When I am trying to run: 
`tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \
    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh`

The following error appears:
`/usr/local/bin/bazel: line 163: /usr/local/lib/bazel/bin/bazel-real: cannot execute binary file: Exec format error
TF_BUILD_INFO = {container_type: ""pi-python37"", command: ""tensorflow/tools/ci_build/pi/build_raspberry_pi.sh"", source_HEAD: ""da8558533d925694483d2c136a9220d6d49d843c"", source_remote_origin: ""https://github.com/tensorflow/tensorflow.git"", OS: ""Linux"", kernel: ""5.4.51-v7l+"", architecture: ""armv7l"", processor: ""ARMv7 Processor rev 3 (v7l)"", processor_count: ""4"", memory_total: ""7882804 kB"", swap_total: ""102396 kB"", Bazel_version: ""ERROR: The project you're trying to build requires Bazel 3.5.0 (specified in /workspace/.bazelversion), but it wasn't found in /usr/local/lib/bazel/bin."", Java_version: ""1.8.0_265"", Python_version: ""2.7.12"", gpp_version: ""g++ (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609"", swig_version: """", NVIDIA_driver_version: """", CUDA_device_count: ""0"", CUDA_device_names: """", CUDA_toolkit_version: """"}
WARNING: current bazel installation is not a release version.

...

Building for the Pi Two/Three, with NEON acceleration
/usr/local/bin/bazel: line 163: /usr/local/lib/bazel/bin/bazel-real: cannot execute binary file: Exec format error
ERROR: The project you're trying to build requires Bazel 3.5.0 (specified in /workspace/.bazelversion), but it wasn't found in /usr/local/lib/bazel/bin.

Bazel binaries for all official releases can be downloaded from here:
  https://github.com/bazelbuild/bazel/releases

Please put the downloaded Bazel binary into this location:
  /usr/local/lib/bazel/bin/bazel-3.5.0-linux-armv7l`

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I have built the Bazel binary and copy it to both: /usr/local/lib/bazel/bin/bazel-3.5.0-linux-armv7l and  /usr/local/bin

/tf_workspace ./configure gives:

You have bazel 3.5.0- (@non-git) installed.
Please specify the location of python. [Default is /usr/bin/python3]: 


However, when I tried to run 
'tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \
    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh'

The above error happens. 

Any help would be highly appreciated. 

KInd regards,
Brian




"
43371,whats the equivalent of torch.nn.Parameter() in TF ?,"from my understanding `torch.tensor(5.5, requires_grad=True)` is equivalent to `tf.Variable(5.5, trainable=True)`

how about `torch.nn.Parameter(torch.zeros([1,1,1]))` ?"
43370,Can lhlo_fuse_linalg support fusion like which in XLA ?,"Hi , I found that lhlo_fuse_linalg can do some fusion at Linalg level, but it seems can not support fusion like which in XLA now.
About fusion, have a design or completely plan at Linalg level ? 
Thanks."
43369,Cant save keras RNN model with custom cell whose call function accepts constants,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.3.0
- Python version:3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

When creating an RNN model with custom cells that accept constants. saving the model with the default SavedModel format will raise a ValueError ""RNN cell does not support constants"". Using the h5 format does work

**Describe the expected behavior**

I dont expect the ValueError, since the call function does support the constants argument

**Standalone code to reproduce the issue**

I've re-used the custom cell sample from https://keras.io/guides/working_with_rnns/#rnns-with-listdict-inputs-or-nested-inputs
but just added the constants argument:

```
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


class NestedCell(keras.layers.Layer):
    def __init__(self, unit_1, unit_2, unit_3, **kwargs):
        self.unit_1 = unit_1
        self.unit_2 = unit_2
        self.unit_3 = unit_3
        self.state_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]
        self.output_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]
        super(NestedCell, self).__init__(**kwargs)

    def build(self, input_shapes):
        # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]
        i1 = input_shapes[0][1]
        i2 = input_shapes[1][1]
        i3 = input_shapes[1][2]

        self.kernel_1 = self.add_weight(
            shape=(i1, self.unit_1), initializer=""uniform"", name=""kernel_1""
        )
        self.kernel_2_3 = self.add_weight(
            shape=(i2, i3, self.unit_2, self.unit_3),
            initializer=""uniform"",
            name=""kernel_2_3"",
        )

    def call(self, inputs, states, constants):
        # inputs should be in [(batch, input_1), (batch, input_2, input_3)]
        # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]
        input_1, input_2 = tf.nest.flatten(inputs)
        s1, s2 = states

        output_1 = tf.matmul(input_1, self.kernel_1)
        output_2_3 = tf.einsum(""bij,ijkl->bkl"", input_2, self.kernel_2_3)
        state_1 = s1 + output_1
        state_2_3 = s2 + output_2_3

        output = (output_1, output_2_3)
        new_states = (state_1, state_2_3)

        return output, new_states

    def get_config(self):
        return {""unit_1"": self.unit_1, ""unit_2"": unit_2, ""unit_3"": self.unit_3}

unit_1 = 10
unit_2 = 20
unit_3 = 30

i1 = 32
i2 = 64
i3 = 32
batch_size = 64
num_batches = 10
timestep = 50


input_1 = keras.Input((None, i1))
input_2 = keras.Input((None, i2, i3))
input_const = keras.Input((None, i1))
cell = NestedCell(unit_1, unit_2, unit_3)
outputs = keras.layers.RNN(cell = cell)(inputs = (input_1, input_2), constants=input_const)

model = keras.models.Model([input_1, input_2, input_const], outputs)

model.compile(optimizer=""adam"", loss=""mse"", metrics=[""accuracy""])
model.save(""test"")
```"
43368,Tim,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
43367,Unable to build tensorflowlite.dll in Windows with FLEX delegate support - library limit of 65535 objects exceeded,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 build 19041.450
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.3
- Python version: 3.8.5
- Bazel version (if compiling from source): 3.5.0
- GCC/Compiler version (if compiling from source): Visual C++ 2019
- CUDA/cuDNN version: No GPU support

**Describe the problem**

I'm trying to build tensorflowlite.dll in Windows for integration with my C++ Visual Studio project. I can build it just fine without flex delegate support, but the exported model I'm trying to load uses several flex nodes, so the call to interpreter->AllocateTensors() fails. 

So I added support for flex delegates and now I can't build the library. It keeps telling me I've exceeded the DLL limit of 65535 objects.

I've made some research and apparently is a limitation of the PE header, but the only suggestion they have is to split the library into multiple, smaller libs. I've also tried to remove some dependencies and symbols but it hasn't worked so far. I'd really be thankful if someone has a workaround for this problem or can give me any guidance.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I'm compiling from ""rc2.3"" branch, but I've tried ""master"" before with the same result.

I added --config=monolithic to my bazel build command and ""//tensorflow/lite/delegates/flex:delegate"" line to the ""deps"" section in the ""tensorflowlite"" target inside ""tensorflow/lite/BUILD"" file. 

This is my command line:

`bazel build --config=opt --config=monolithic --config=windows --config=v2 //tensorflow/lite:tensorflowlite`

I've tried removing symbols modifying the EXCLUDE_RE reg expression in ""tools\def_file_filter\def_file_filter.py.tpl"" but apparently didn't work or wasn't enough.

**Any other info / logs**

This is the full error. I get another error line complaining about multiple definition but that doesn't worry me as much since I don't remember getting it when I was trying to build from master.

```
ERROR: D:/users/[...]/documents/projects/tensorflow/tensorflow/lite/BUILD:644:24: Linking of rule '//tensorflow/lite:tensorflowlite.dll' failed (Exit 1189): link.exe failed: error executing command
  cd C:/users/[...]/_bazel_[...]/rwi2ipkq/execroot/org_tensorflow
  SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/[...]/AppData/Local/Programs/Python/Python38-32/python.exe
    SET PYTHON_LIB_PATH=C:/Users/[...]/AppData/Local/Programs/Python/Python38-32/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\[...]\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TF_CONFIGURE_IOS=0
    SET TF_ENABLE_XLA=1
    SET TMP=C:\Users\[...]\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.27.29110/bin/HostX64/x64/link.exe @bazel-out/x64_windows-opt/bin/tensorflow/lite/tensorflowlite.dll-2.params
Execution platform: @local_execution_config_platform//:platform
LINK : warning LNK4044: unrecognized option '/s'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
delegate_only_runtime.lo.lib(delegate.obj) : error LNK2005: ""class std::unique_ptr<struct TfLiteDelegate,void (__cdecl*)(struct TfLiteDelegate *)> __cdecl tflite::AcquireFlexDelegate(void)"" (?AcquireFlexDelegate@tflite@@YA?AV?$unique_ptr@UTfLiteDelegate@@P6AXPEAU1@@Z@std@@XZ) already defined in framework_lib.lo.lib(interpreter_builder.obj)
LINK : fatal error LNK1189: library limit of 65535 objects exceeded
Target //tensorflow/lite:tensorflowlite failed to build
INFO: Elapsed time: 81.219s, Critical Path: 49.74s
INFO: 1 process: 1 local.
FAILED: Build did NOT complete successfully
```
"
43364,C++ compilation of rule '@nccl_archive//:device_lib' failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution Arch Linux
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3.0
- Python version: 3.8.5
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): clang 10.0.1
- CUDA/cuDNN version: 11.0/8.0.2
- GPU model and memory: GTX1080 32Gib RAM



**Describe the problem**

Build failed with the following error present:

ERROR: /d/tensorflow/tensorflow/python/tools/BUILD:226:1 C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1)

**Provide the exact sequence of commands/steps that you executed before running into the problem**

/home/erina/Desktop/bazel-3.1.0-linux-x86_64 build --config=opt -c opt //tensorflow/tools/pip_package:build_pip_package


**Any other info/logs**

The full error is as follows:

```log
ERROR: /home/erina/.cache/bazel/_bazel_erina/e4985779bd7c6b512f3eaa8e37e6fd2a/external/nccl_archive/BUILD.bazel:53:1: C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1)
clang: warning: Unknown CUDA version 11.0. Assuming the latest supported version 10.1 [-Wunknown-cuda-version]
clang: warning: argument unused during compilation: '-Xcuda-fatbinary=--compress-all' [-Wunused-command-line-argument]
In file included from <built-in>:1:
In file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:
In file included from /usr/include/string.h:446:
In file included from /usr/include/strings.h:144:
/usr/include/bits/strings_fortified.h:23:8: error: exception specification in declaration does not match previous declaration
__NTH (bcopy (const void *__src, void *__dest, size_t __len))
       ^
/usr/include/strings.h:38:13: note: previous declaration is here
extern void bcopy (const void *__src, void *__dest, size_t __n)
            ^
In file included from <built-in>:1:
In file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:
In file included from /usr/include/string.h:446:
In file included from /usr/include/strings.h:144:
/usr/include/bits/strings_fortified.h:29:8: error: exception specification in declaration does not match previous declaration
__NTH (bzero (void *__dest, size_t __len))
       ^
/usr/include/strings.h:42:13: note: previous declaration is here
extern void bzero (void *__s, size_t __n) __THROW __nonnull ((1));
            ^
In file included from <built-in>:1:
In file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:
In file included from /usr/include/string.h:519:
/usr/include/bits/string_fortified.h:31:8: error: exception specification in declaration does not match previous declaration
__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,
       ^
/usr/include/string.h:43:14: note: previous declaration is here
extern void *memcpy (void *__restrict __dest, const void *__restrict __src,
             ^
In file included from <built-in>:1:
In file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:
In file included from /usr/include/string.h:519:
/usr/include/bits/string_fortified.h:38:8: error: exception specification in declaration does not match previous declaration
__NTH (memmove (void *__dest, const void *__src, size_t __len))
       ^
/usr/include/string.h:47:14: note: previous declaration is here
extern void *memmove (void *__dest, const void *__src, size_t __n)
             ^
In file included from <built-in>:1:
In file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:
In file included from /usr/include/string.h:519:
/usr/include/bits/string_fortified.h:45:8: error: exception specification in declaration does not match previous declaration
__NTH (mempcpy (void *__restrict __dest, const void *__restrict __src,
       ^
/usr/include/string.h:384:14: note: previous declaration is here
extern void *mempcpy (void *__restrict __dest,
             ^
In file included from <built-in>:1:
In file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:
In file included from /usr/include/string.h:519:
/usr/include/bits/string_fortified.h:59:8: error: exception specification in declaration does not match previous declaration
__NTH (memset (void *__dest, int __ch, size_t __len))
       ^
/usr/include/string.h:61:14: note: previous declaration is here
extern void *memset (void *__s, int __c, size_t __n) __THROW __nonnull ((1));
             ^
In file included from <built-in>:1:
In file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:
In file included from /usr/include/string.h:519:
/usr/include/bits/string_fortified.h:81:8: error: exception specification in declaration does not match previous declaration
__NTH (explicit_bzero (void *__dest, size_t __len))
       ^
/usr/include/string.h:450:13: note: previous declaration is here
extern void explicit_bzero (void *__s, size_t __n) __THROW __nonnull ((1))
            ^
In file included from <built-in>:1:
In file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:
In file included from /usr/include/string.h:519:
/usr/include/bits/string_fortified.h:88:8: error: exception specification in declaration does not match previous declaration
__NTH (strcpy (char *__restrict __dest, const char *__restrict __src))
       ^
/usr/include/string.h:125:14: note: previous declaration is here
extern char *strcpy (char *__restrict __dest, const char *__restrict __src)
             ^
In file included from <built-in>:1:
In file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:
In file included from /usr/include/string.h:519:
/usr/include/bits/string_fortified.h:95:8: error: exception specification in declaration does not match previous declaration
__NTH (stpcpy (char *__restrict __dest, const char *__restrict __src))
       ^
/usr/include/string.h:475:14: note: previous declaration is here
extern char *stpcpy (char *__restrict __dest, const char *__restrict __src)
             ^
In file included from <built-in>:1:
In file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:
In file included from /usr/include/string.h:519:
/usr/include/bits/string_fortified.h:103:8: error: exception specification in declaration does not match previous declaration
__NTH (strncpy (char *__restrict __dest, const char *__restrict __src,
       ^
/usr/include/string.h:128:14: note: previous declaration is here
extern char *strncpy (char *__restrict __dest,
             ^
In file included from <built-in>:1:
In file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:
In file included from /usr/include/string.h:519:
/usr/include/bits/string_fortified.h:117:8: error: exception specification in declaration does not match previous declaration
__NTH (stpncpy (char *__dest, const char *__src, size_t __n))
       ^
/usr/include/string.h:483:14: note: previous declaration is here
extern char *stpncpy (char *__restrict __dest,
             ^
In file included from <built-in>:1:
In file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:
In file included from /usr/include/string.h:519:
/usr/include/bits/string_fortified.h:127:8: error: exception specification in declaration does not match previous declaration
__NTH (strcat (char *__restrict __dest, const char *__restrict __src))
       ^
/usr/include/string.h:133:14: note: previous declaration is here
extern char *strcat (char *__restrict __dest, const char *__restrict __src)
             ^
In file included from <built-in>:1:
In file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:
In file included from /usr/include/string.h:519:
/usr/include/bits/string_fortified.h:134:8: error: exception specification in declaration does not match previous declaration
__NTH (strncat (char *__restrict __dest, const char *__restrict __src,
       ^
/usr/include/string.h:136:14: note: previous declaration is here
extern char *strncat (char *__restrict __dest, const char *__restrict __src,
             ^
13 errors generated when compiling for sm_61.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /d/tensorflow/tensorflow/python/tools/BUILD:226:1 C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1)
INFO: Elapsed time: 1867.652s, Critical Path: 127.88s
INFO: 9857 processes: 9857 local.
FAILED: Build did NOT complete successfully
```
"
43363,Module 'tensorflow' has no attribute 'get_default_session' ,"Hi there,
i got the error: if tf.get_default_session() is not None:
AttributeError: module 'tensorflow' has no attribute 'get_default_session' 

searched similar issues online, it was all related to the versions of the packages or python i have installed, but i have tried all the suggestions with no luck, just posted here, wondering if i can please get some help.. 

python version  3.7.8, 
tensorflow             2.2.0
tensorflow-estimator   2.2.0
Keras: 2.1.4 

Thank you so much"
43362,distributed_dataset tensorflow.python.framework.errors_impl.UnavailableError: Socket closed,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Buster
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): bin
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7.7
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: TPU v3-8

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
urllib.error.URLError: <urlopen error [Errno 110] Connection timed out> in Colab,
tensorflow.python.framework.errors_impl.UnavailableError: Socket closed in GCE VM

**Describe the expected behavior**
Not having these errors(code ran just fine a few hours ago)

**Standalone code to reproduce the issue**
[COLAB](https://colab.research.google.com/drive/173UJ_icUBvl-a6-ujSZHoHHLL2qj9B-3?usp=sharing)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
in Colab:
```
Traceback (most recent call last):
  File ""/usr/lib/python3.6/urllib/request.py"", line 1325, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File ""/usr/lib/python3.6/http/client.py"", line 1264, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/usr/lib/python3.6/http/client.py"", line 1310, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""/usr/lib/python3.6/http/client.py"", line 1259, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""/usr/lib/python3.6/http/client.py"", line 1038, in _send_output
    self.send(msg)
  File ""/usr/lib/python3.6/http/client.py"", line 976, in send
    self.connect()
  File ""/usr/lib/python3.6/http/client.py"", line 948, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File ""/usr/lib/python3.6/socket.py"", line 724, in create_connection
    raise err
  File ""/usr/lib/python3.6/socket.py"", line 713, in create_connection
    sock.connect(sa)
TimeoutError: [Errno 110] Connection timed out
```

in VM:
```
Traceback (most recent call last):
  File ""train_gpt2.py"", line 77, in <module>
    train()
  File ""/usr/local/lib/python3.7/dist-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/usr/local/lib/python3.7/dist-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/usr/local/lib/python3.7/dist-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""train_gpt2.py"", line 64, in train
    model.fit(iter(dist_dataset), graph_mode)
  File ""/home/ksjcom0705_gmail_com/gpt-2-tensorflow2.0/gpt2_model.py"", line 343, in fit
    (inputs, targets) = next(train_dataset)
  File ""/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py"", line 649, in __next__
    return self.get_next()
  File ""/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py"", line 706, in get_next
    global_has_value, replicas = _get_next_as_optional(self, self._strategy)
  File ""/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py"", line 548, in _get_next_as_optional
    iterator._iterators[i].get_next_as_list(new_name))  # pylint: disable=protected-access
  File ""/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py"", line 1512, in get_next_as_list
    strict=True,
  File ""/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
2020-09-19 08:05:53.047063: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running
 a tf.function, it usually indicates some op in the graph gets an error: Socket closed
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1600502753.043380028"",""description"":""Error received from peer ipv4:10.82.138.50:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.
cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}
    return func(*args, **kwargs)
  File ""/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1207, in cond
    if pred:
  File ""/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 984, in __bool__
    return bool(self._numpy())
  File ""/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1031, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access   
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnavailableError: Socket closed
Additional GRPC error information from remote target /job:worker/replica:0/task:0:```..."
43361,tensorflow import error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): anaconda
- TensorFlow version: 2.2.0
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: conda
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: No
- GPU model and memory: No


**Describe the problem**

I am getting this error when importing tensorflow. I don't have gpu. And my system is not gpu enabled. But still I am getting this error. It says "" Ignore above cudart dlerror if you do not have a GPU set up on your machine."" Please help to get rid of this and tell me how ignore this.

2020-09-19 12:27:43.405252: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-09-19 12:27:43.413500: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

Any help is appreciated :)
"
43360,TFLITE not enable RUY AVX runtime path ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.3.0
- Tensorflow Lite version:  https://github.com/google/ruy/archive/34ea9f4993955fa1ff4eb58e504421806b7f2e8f.zip

**Describe the current behavior**
Tensorflow Lite build with RUY, even so RUY will detect AVX feature: 
```
constexpr Path kDefaultPaths = Path::kStandardCpp | kDefaultArchPaths;
```
But TFLITE not set the runtime to use with this API: https://github.com/google/ruy/blob/be065e42dd898f565c3e439b70957debb28dfa34/ruy/ctx.cc#L59-L61

And thus RUY can only use the kStandardCpp PATH.

**Describe the expected behavior**
TFLITE should set the AVX runtime PATH and RUY can use it when detected.

Command to build tflite:
```
make SHELL=/bin/bash BUILD_WITH_NNAPI=false BUILD_WITH_RUY=true -C /home/lesliefang/tflite/tensorflow_test/tensorflow-2.3.0 -f tensorflow/lite/tools/make/Makefile -j 4
```
"
43359,micro_speech.bin file creation fails even if compiling is successful,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.15.2
- TensorFlow installed from: source 
- Tensorflow version (commit SHA if source): 19 September 2020
- Target platform: Sparkfun Edge

**Describe the problem**

I build micro_speech project:
gmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge  micro_speech_bin

last line of the build:
tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/arm-none-eabi-objcopy tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech.bin -O binary

The result bin file in tensorflow-master/tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech.bin file is only 64 bytes which cannot correct. It should be about 250KB.

I have GNU Make 4.2.1"
43352,cuda and tensorflow,"hello i install cuda_10.1.243_426.00_win10 and cudnn-10.1-windows10-x64-v7.5.0.56 and tensorflow2.3 and tensorflow-gpu2.3 and when i run matrixMul_vs2019.vcxproj 
show
````

 [Matrix Multiply Using CUDA] - Starting...
GPU Device 0: ""GeForce GTX 1650"" with compute capability 7.5

MatrixA(320,320), MatrixB(640,320)
Computing result using CUDA Kernel...
done
Performance= 31.97 GFlop/s, Time= 4.099 msec, Size= 131072000 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

NOTE: The CUDA Samples are not meant for performancemeasurements. Results may vary when GPU Boost is enabled.

C:\ProgramData\NVIDIA Corporation\CUDA Samples\v10.1\0_Simple\matrixMul\../../bin/win64/Debug/matrixMul.exe (process 15308) exited with code 0.
Press any key to close this window . . .
````

but when run a tensorflow app this commend show
```
 python3 .\detection_custom.py     2020-09-19 01:41:17.705558: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-09-19 01:41:17.706529: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2020-09-19 01:41:38.314508: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-09-19 01:41:38.350208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5
coreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s
2020-09-19 01:41:38.351252: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-09-19 01:41:38.352227: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found
2020-09-19 01:41:38.353091: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found
2020-09-19 01:41:38.353915: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found
2020-09-19 01:41:38.354613: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found
2020-09-19 01:41:38.355197: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found
2020-09-19 01:41:38.355578: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found
2020-09-19 01:41:38.355633: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-09-19 01:41:38.356356: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-19 01:41:38.368268: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x19c0bdffd00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-19 01:41:38.368484: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-19 01:41:38.369451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-19 01:41:38.369768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]
```
and this is my C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\bin
![image](https://user-images.githubusercontent.com/53191454/93645602-99390a00-fa19-11ea-8ac1-ccc3348323e9.png)

can you help me
os: windows 10
i7 9750h
nvidia 1650
"
43351,tf.contrib.lookup.string_to_index_table_from_file optimization  ,"**System information** 

- OS Platform and Distribution : macOS Catalina 10.15.3

- TensorFlow installed from : binary

- TensorFlow version : 1.15.x
- Python version: 3.7.3


**Describe the current behavior**

Hi ,

If there any way we can make reading of vocab faster esp for api ""tf.contrib.lookup.string_to_index_table_from_file"" ?
For token size more than few thousand it take close to 500ms , how can we optimize it ?"
43350,TF2.0 SavedModel with feature processing inference code ,"


**System information** 

- OS Platform and Distribution : macOS Catalina 10.15.3

- TensorFlow installed from : binary

- TensorFlow version : 2.2.0
- Python version: 3.7.3


**Describe the current behavior**

Hi ,

I am looking for sample code to do inference in TF2.0 . We have savedModel in TF2.0 with feature processing as part of saved model.

In TF1.x we used to do inference as follows 

```
import tensorflow as tf

saved_model_path = ""<path>""""


with tf.Session(graph=tf.Graph()) as sess:
  metagraph = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)
  prediction = sess.run(['loss/scores:0'], feed_dict={'<some place holder>':<input_list>})
  print(prediction)
```
what will be corresponding code in TF2.0  since it doesnot use session concept ?  
Note: I dont need to have  working code , please point me to the sample code somethere

"
43349,Different gradients in tf2 when eager mode is enabled compared to graph mode,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Ubuntu 18.04
- **TensorFlow installed from (source or binary):** Binary
- **TensorFlow version (use command below):**  v2.3.0
- **Python version:**  v3.8.5
- **CUDA/cuDNN version:** CUDA  v11.0/ cuDNN V9.1.85
- **GPU model and memory:** GPU: NVIDIA Quadro P1000 / Intel UHD Graphics 630 - Memory: 2x Samsung M471A4G43MB1-CTD

You can collect some of this information using our environment capture:

- [tf_env.txt](https://github.com/tensorflow/tensorflow/files/5247881/tf_env.txt)

### Describe the current behaviour

I'm currently porting several TensorFlow v1.x legacy repositories over to tf2.3 with eager execution enabled. I used the steps in the [documentation](https://www.tensorflow.org/guide/migrate) to do this. Unfortunately, one of the RL Agents which is based on the [Lyapunov Actor-Critic ](http://arxiv.org/abs/2004.14288) architecture of [Han et al. 2019](http://arxiv.org/abs/2004.14288) is not training when eager execution is enabled. I did some debugging, and it looks like there is a problem with computing the gradients of the Squashed Gaussian Actor-network:

```python
class SquashedGaussianActor(tf.keras.Model):
    def __init__(
        self, obs_dim, act_dim, hidden_sizes, name, seeds=None, **kwargs,
    ):
        """"""Squashed Gaussian actor network.

        Args:
            obs_dim (int): The dimension of the observation space.

            act_dim (int): The dimension of the action space.

            hidden_sizes (list): Array containing the sizes of the hidden layers.

            name (str): The keras module name.

            seeds (list, optional): The random seeds used for the weight initialization
                and the sampling ([weights_seed, sampling_seed]). Defaults to
                [None, None]
        """"""
        super().__init__(name=name, **kwargs)

        # Get class parameters
        self.s_dim = obs_dim
        self.a_dim = act_dim
        self._seed = seeds[0]
        self._initializer = tf.keras.initializers.GlorotUniform(
            seed=self._seed
        )  # Seed weights initializer
        self._tfp_seed = seeds[1]

        # Create fully connected layers
        self.net = tf.keras.Sequential(
            [
                tf.keras.layers.InputLayer(
                    dtype=tf.float32, input_shape=(self.s_dim), name=name + ""/input""
                )
            ]
        )
        for i, hidden_size_i in enumerate(hidden_sizes):
            self.net.add(
                tf.keras.layers.Dense(
                    hidden_size_i,
                    activation=""relu"",
                    name=name + ""/l{}"".format(i + 1),
                    kernel_initializer=self._initializer,
                )
            )

        # Create Mu and log sigma output layers
        self.mu = tf.keras.Sequential(
            [
                tf.keras.layers.InputLayer(
                    dtype=tf.float32, input_shape=hidden_sizes[-1]
                ),
                tf.keras.layers.Dense(
                    act_dim,
                    activation=None,
                    name=name + ""/mu"",
                    kernel_initializer=self._initializer,
                ),
            ]
        )
        self.log_sigma = tf.keras.Sequential(
            [
                tf.keras.layers.InputLayer(
                    dtype=tf.float32, input_shape=hidden_sizes[-1]
                ),
                tf.keras.layers.Dense(
                    act_dim,
                    activation=None,
                    name=name + ""/log_sigma"",
                    kernel_initializer=self._initializer,
                ),
            ]
        )

    @tf.function
    def call(self, inputs):
        """"""Perform forward pass.""""""

        # Retrieve inputs
        obs = inputs

        # Perform forward pass through fully connected layers
        net_out = self.net(obs)

        # Calculate mu and log_sigma
        mu = self.mu(net_out)
        log_sigma = self.log_sigma(net_out)
        log_sigma = tf.clip_by_value(
            log_sigma, LOG_SIGMA_MIN_MAX[0], LOG_SIGMA_MIN_MAX[1]
        )

        # Perform re-parameterization trick
        sigma = tf.exp(log_sigma)

        # Create bijectors (Used in the re-parameterization trick)
        squash_bijector = SquashBijector()
        affine_bijector = tfp.bijectors.Shift(mu)(tfp.bijectors.Scale(sigma))

        # Sample from the normal distribution and calculate the action
        batch_size = tf.shape(input=obs)[0]
        base_distribution = tfp.distributions.MultivariateNormalDiag(
            loc=tf.zeros(self.a_dim), scale_diag=tf.ones(self.a_dim)
        )
        epsilon = base_distribution.sample(batch_size, seed=self._tfp_seed)
        raw_action = affine_bijector.forward(epsilon)
        clipped_a = squash_bijector.forward(raw_action)

        # Transform distribution back to the original policy distribution
        reparm_trick_bijector = tfp.bijectors.Chain((squash_bijector, affine_bijector))
        distribution = tfp.distributions.TransformedDistribution(
            distribution=base_distribution, bijector=reparm_trick_bijector
        )
        clipped_mu = squash_bijector.forward(mu)

        # Return network outputs and noise sample
        return clipped_a, clipped_mu, distribution.log_prob(clipped_a), epsilon
```

Although gradients are computed for this network, these are very different in eager mode as compared to when the `tf.compat.v1.disable_eager_execution()` flag is used. This is strange since the loss functions, random seeds, weights/biases and inputs are equal. Furthermore, also the outputs of a forward pass through the network are identical in both Eager and legacy Graph mode. This problem does not seem to exist for accompanying Lyapunov Critic (a modified version of a deep Q network). I first wanted to post it here before posting on StackOverflow as I am unsure whether this a translation issue or a bug. I tried searching for possible causes, but I did not find a possible solution to my problem. 

### Describe the expected behaviour

I expected the gradients to be equal in both the script in which Eager mode is enabled and the one in which Eager mode disabled. Instead, although the Actor loss is equal (`-1084.2743`) the gradients are different:

**Results eager mode:**

```python
grad/l1/weights:
[[ 13.408794     2.178104   -11.570398   108.1129       0.      46.277092  ]
 [ 30.369755    -0.23067771 -16.505516    87.53128      0.      22.498222]]

grad/l1/bias:
[ 47.802944    0.4118477 -26.826544  185.92018     0.         60.14121]

grad/l2/weights:
[[-12.058103     0.          -0.33448434  14.862488     0.      -1.8673693 ]
 [ -0.30301327   0.           0.          18.134262     0.      0.        ]
 [-58.740616     0.          -1.113349   119.78081      0.      -2.120421  ]
 [-24.779123     0.          -0.4045168   66.25852      0.      -0.24545981]
 [  0.           0.           0.           0.           0.      0.        ]
 [ -3.3655543    0.           0.          41.697414     0.      0.]]

grad/l2/bias:
[-98.14556     0.          1.5422362 206.91672     0.         -4.5077815]

grad/mu/weights:
[[ 4.81568050e+00 -2.37027216e+00]
 [ 0.00000000e+00  0.00000000e+00]
 [-1.84061453e-01  1.09151885e-01]
 [ 3.29189644e+01 -2.93549042e+01]
 [ 0.00000000e+00  0.00000000e+00]
 [-4.64032125e-03  6.64837938e-03]]

grad/mu/bias:
[110.85972 -87.11501]

grad/log_sigma/weights:
[[5.8719816e+00 5.4135699e+00]
 [0.0000000e+00 0.0000000e+00]
 [2.2099029e-01 3.2953596e-01]
 [7.5656143e+01 6.9250507e+00]
 [0.0000000e+00 0.0000000e+00]
 [6.7091000e-04 9.1812750e-03]]

grad/log_sigma/bias:
[238.21657   47.906483]
```

**Results graph mode:**

```python
grad/l1/weights:
[[  1.1570635   2.2042375 -11.598429   88.867676    0.         48.9553   ]
 [ 11.854488   -0.2453581 -16.115313   49.29876     0.         24.385675 ]]

grad/l1/bias:
[ 29.577682     0.16548407 -29.669703   144.31357      0.      64.56108   ]

grad/l2/weights:
[[ -9.905338     0.           4.7838783    3.8737621    0.      -3.4614413 ]
 [ -0.30002874   0.           0.          18.411116     0.      0.]
 [-53.221275     0.           5.242466    79.01278      0.      -3.936808]
 [-23.196867     0.           0.53559065  51.048943     0.      -0.45809162]
 [  0.           0.           0.           0.           0.      0. ]
 [ -4.0506964    0.           0.          42.10633      0.      0.]]

grad/l2/bias:
[-98.10234    0.        16.95784  155.67645    0.        -8.705088]

grad/mu/weights:
[[ 1.62482891e+01  1.15455017e+01]
 [ 0.00000000e+00  0.00000000e+00]
 [ 2.01459303e-01  6.31435871e-01]
 [ 1.06661575e+02  4.60808792e+01]
 [ 0.00000000e+00  0.00000000e+00]
 [-3.92461056e-03  9.14150383e-03]]

grad/mu/bias:
[372.89713 178.99115]

grad/log_sigma/weights:
[[7.4654112e+00 1.1838960e+01]
 [0.0000000e+00 0.0000000e+00]
 [3.2749507e-01 6.5140498e-01]
 [8.7392433e+01 4.8770226e+01]
 [0.0000000e+00 0.0000000e+00]
 [1.4014873e-03 9.9075940e-03]]

grad/log_sigma/bias:
[289.77673 198.32103]
```

### Code to reproduce the problem

I placed two small stand-alone example scripts [tf2_val_grad.py](https://github.com/rickstaa/tf2_eager_vs_graph_grad_problem/blob/master/tf2_val_grad.py) and [tf2_val_grad_eager.py](https://github.com/rickstaa/tf2_eager_vs_graph_grad_problem/blob/master/tf2_val_grad_eager.py) in my repository that can be used to reproduce the problem. [The repository](https://github.com/rickstaa/tf2_eager_vs_graph_grad_problem) also contains a small README.md which explains how to run the code examples.

### Other info / logs

[tf2_val_grad_eager_terminal_output.txt](https://github.com/tensorflow/tensorflow/files/5247877/tf2_val_grad_eager_terminal_output.txt)

[tf2_val_grad_terminal_output.txt](https://github.com/tensorflow/tensorflow/files/5247879/tf2_val_grad_terminal_output.txt)

### Possible related issues
- #27827
- https://github.com/tensorflow/probability/issues/345


### Further debug steps
- Trim the call function down to see where the gradients converge."
43348,r1.15 Eigen download link is dead,"version r1.15

The download link for Eigen is dead  (https://bitbucket.org/eigen/eigen/get/49177915a14a.tar.gz)

This patch switches the link to use the google storage bucket

```
From 765a2b04268d030d247a45d600686f3bac5e5d38 Mon Sep 17 00:00:00 2001
From: Chris Roed <chris.roed@omcare.com>
Date: Tue, 1 Sep 2020 11:36:49 -0500
Subject: [PATCH 3/3] use the google repo for eigen

---
 tensorflow/lite/tools/make/download_dependencies.sh | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/tensorflow/lite/tools/make/download_dependencies.sh b/tensorflow/lite/tools/make/download_dependencies.sh
index ef4a7777e6..678e35d987 100755
--- a/tensorflow/lite/tools/make/download_dependencies.sh
+++ b/tensorflow/lite/tools/make/download_dependencies.sh
@@ -29,7 +29,7 @@ if [ ! -f $BZL_FILE_PATH ]; then
   exit 1;
 fi
 
-EIGEN_URL=""$(grep -o 'http.*bitbucket.org/eigen/eigen/get/.*tar\.gz' ""${BZL_FILE_PATH}"" | grep -v mirror.tensorflow | head -n1)""
+EIGEN_URL=""$(grep -o 'http.*bitbucket.org/eigen/eigen/get/.*tar\.gz' ""${BZL_FILE_PATH}"" | grep mirror.tensorflow | head -n1)""
 GEMMLOWP_URL=""$(grep -o 'https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/gemmlowp/.*zip' ""${BZL_FILE_PATH}"" | head -n1)""
 GOOGLETEST_URL=""https://github.com/google/googletest/archive/release-1.8.0.tar.gz""
 ABSL_URL=""$(grep -o 'https://github.com/abseil/abseil-cpp/.*tar.gz' ""${BZL_FILE_PATH}"" | head -n1)""
-- 
2.20.1

```"
43347,Documentation about tensorflow ,"Hello,
I would like some information about tensorflow:

1- if I build tensorflow from the source how much RAM does it take? ;

2- If I install tensorflow from here https://www.tensorflow.org/install/source and use the instructions to install the tensorflow package with pip is it necessary to use Bazel too or can I do without it? Also will I have instructions for AVX, AVX2 and FMA?

3- what is the difference between tensorflow 1x and 2x V 

Best Regards
"
43345,Faild build windows,"System information

OS Platform and Distribution: Windows 10
TensorFlow installed from: source
TensorFlow version: 2.3
Python version: 3.8.3
Bazel version (if compiling from source): 3.1.0
CUDA/cuDNN version: 11.0.2_451.48/8.0.2.39
GPU model and memory: 1060 super
```
Repository rule cuda_configure defined at:
  G:/tensorflow/third_party/gpus/cuda_configure.bzl:1407:18: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1377
                _create_local_cuda_repository(<1 more arguments>)
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1221, in _create_local_cuda_repository
                to_list_of_strings(<1 more arguments>)
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1222, in to_list_of_strings
                _cuda_include_path(<2 more arguments>)
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 364, in _cuda_include_path
                inc_entries.append(<1 more arguments>)
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 364, in inc_entries.append
                realpath(repository_ctx, <1 more arguments>)
        File ""G:/tensorflow/third_party/remote_config/common.bzl"", line 268, in realpath
                execute(repository_ctx, <1 more arguments>)
        File ""G:/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
[FATAL 22:56:20.372 src/main/cpp/blaze.cc:1290] Unknown startup option: '-c'.
  For more info, run 'bazel help startup_options'.
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1377
                _create_local_cuda_repository(<1 more arguments>)
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1221, in _create_local_cuda_repository
                to_list_of_strings(<1 more arguments>)
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1222, in to_list_of_strings
                _cuda_include_path(<2 more arguments>)
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 364, in _cuda_include_path
                inc_entries.append(<1 more arguments>)
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 364, in inc_entries.append
                realpath(repository_ctx, <1 more arguments>)
        File ""G:/tensorflow/third_party/remote_config/common.bzl"", line 268, in realpath
                execute(repository_ctx, <1 more arguments>)
        File ""G:/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
[FATAL 22:56:20.372 src/main/cpp/blaze.cc:1290] Unknown startup option: '-c'.
  For more info, run 'bazel help startup_options'.
WARNING: Target pattern parsing failed.
ERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1377
                _create_local_cuda_repository(<1 more arguments>)
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1221, in _create_local_cuda_repository
                to_list_of_strings(<1 more arguments>)
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1222, in to_list_of_strings
                _cuda_include_path(<2 more arguments>)
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 364, in _cuda_include_path
                inc_entries.append(<1 more arguments>)
        File ""G:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 364, in inc_entries.append
                realpath(repository_ctx, <1 more arguments>)
        File ""G:/tensorflow/third_party/remote_config/common.bzl"", line 268, in realpath
                execute(repository_ctx, <1 more arguments>)
        File ""G:/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
[FATAL 22:56:20.372 src/main/cpp/blaze.cc:1290] Unknown startup option: '-c'.
  For more info, run 'bazel help startup_options'.
INFO: Elapsed time: 1.209s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package
```

"
43344,Support AWS IAM roles when using TensorFlow file_io,"**System information**
- TensorFlow version (you are using): 2.1.0, 2.3.0
- Are you willing to contribute it (Yes/No): No, sorry, I'm not very comfortable with C++


**Describe the feature and the current behavior/state.**
I really appreciate the functionality provided by TensorFlow file_io to allow the user to treat files stored locally on disk in the same was as files stored in S3. It just works! It's wonderful! However, this functionality doesn't work if your AWS credentials are being provided using an AWS IAM role. Unfortunately the default AWS SDK credentials behavior does not account for this situation, and the maintainers have said that they will not incorporate this feature into their default credentials provided. They did offer a suggested way for people using the AWS SDK to support this feature. Here is the thread where this is discussed: https://github.com/aws/aws-sdk-cpp/issues/150.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
TensorFlow users who are using AWS S3 with credentials provided using AWS IAM roles

**Any Other info.**
Here's an example from TensorFlow 2.3.0 with the error message, run in a Docker container that uses tensorflow/tensorflow:2.3.0-cpu as the base image. The AWS_PROFILE EV is set to use my IAM role. I've confirmed that `aws s3 ls s3://my-bucket/my-file.txt` works.

```
>>> from tensorflow.python.lib.io import file_io as tf_file_io
>>> tf_file_io.file_exists('s3://my-bucket/my-file.txt')

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 249, in file_exists
    return file_exists_v2(filename)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 267, in file_exists_v2
    _pywrap_file_io.FileExists(compat.as_bytes(path))
tensorflow.python.framework.errors_impl.FailedPreconditionError: AWS Credentials have not been set properly. Unable to access the specified S3 location
```"
43341,Deprecated get_losses_for function in migration guide,"
## URL(s) with the issue:
https://www.tensorflow.org/guide/migrate?hl=en

## Description of issue (what needs changing):
In the **Custom model_fn with minimal changes** section of the migration guide,  it is recommended to use `get_losses_for` but this function is deprecated and mapped to the model.losses property.
 https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/engine/base_layer.py#L1895


Furthermore, in the example this function is called two times with differents parameters (`None` and `features`) which would results in adding the regularization loss twice. 



"
43340,Tensorflow lite speech sample is not detecting silence (wrong detections),"Hi, I am using the sample https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/android . I am facing the following issues:
- When there is a silence it detects as a speech?
- It also detects fan noise, clap or any tick noises as a speech which it should not as it is not a speech of a person.
- It does not detect voice correctly, like If I have spoken Yes it does not detect and same for other words as well.

Can you please guide which are the missing steps or which model to use if sample is not update? Speech and silence should be detected correctly. Currently it detects silence as words as well."
43339,Tensorflow Lite: iOS 14 breaks the NPU Delegate,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 iOS 14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
iPhone 11 pro,
- TensorFlow installed from (source or binary):
via pods: pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly'
- TensorFlow version (use command below):
TensorFlowLiteSwift 0.0.1-nightly

**Describe the current behavior**
The update to iOS 14 broke the NPU (coreML) Delegate in tensorflow Lite.
The issue presents itself in the tensorflow lite example on iOS for posenet.
As soon as the coreML delegate is selected the joints of the body arent recognized properly anymore and the output of the net is garbage.

Images showing the correct (GPU) and incorrect (NPU) pose estimation:
[https://imgur.com/a/UzMM6Pt](https://imgur.com/a/UzMM6Pt)


**Describe the expected behavior**
The NPU delegate should produce the same results as CPU/GPU

**Standalone code to reproduce the issue**
Download the tensorflow Lite example for posenet on iOS and select the NPU delegate on a iOS 14 device
"
43337,Using Tensorflow-2.3.0 with GPU,"I downloaded **tensorflow-gpu** using pip. I set `Ld_PATH_LIBRARY` to:
```
Ld_PATH_LIBRARY=""/usr/local/cuda/lib64""
Ld_PATH_LIBRARY=""/usr/local/cuda-10.1/lib64""
Ld_PATH_LIBRARY=""/usr/local/cuda/extras/CUPTI/lib64""
```
for all path tensorflow shows a problem:
```
2020-09-18 17:37:06.179293: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
```
I am using **Ubuntu 20.04** and **python-3.8**"
43336,2020-09-18 19:08:48.694940: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found 2020-09-18 19:08:48.699968: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
![image](https://user-images.githubusercontent.com/60377675/93592515-e77fe600-f9e4-11ea-8482-e8dbc12ef6d0.png)
"
43335,Output of TensorFlow Java API differs from the one in TensorFlow Python,"Hi there, 

I'm processing an image with a TF trained model using the TensorFlow Java API but I get slightly different results from what the model outputs in python. 

The model was trained with TF version 1.15.2 in Python and libtensorflow-1.15.0.jar. 
Is there any well-known difference between the last two?

Thank you!
"
43334,F tensorflow/core/framework/tensor_shape.cc:44] Check failed: NDIMS == dims() (2 vs. 5)Asking for tensor of 2 dimensions from a tensor of 5 dimensions,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
43332,Tf Lite Micro Global Average Pooling not supported,"@tensorflow/micro

**System information**
- Project build on windows, cygwin console:
- TensorFlow installed from source
- Tensorflow version: 2.3.0, commit 4230dd89cff
- Target platform: stm32f429, ARM4

I use quantized TF Lite model trying to run it on target platform. Model loads correctly, tenaor allocates correctly, but I have a problem during invoking. In my original Keras model I use Global Average Pooling, which is being converted to Mean operation in Tflite file. Netron diagram:

![image](https://user-images.githubusercontent.com/58625554/93563695-b40f6e00-f988-11ea-9a10-3e754cf3cdf5.png)


After invoking I get folowing error:
_tensorflow/lite/micro/kernels/reduce.cc Number of Input dimensions != 4 OR the Axis is not either [1, 2] or [2, 1]

Node MEAN (number 19) failed to invoke with status 1_

I belive that there is a problem in reduce.cc file and MEAN operation implementation. I am not sure why should I provide 4D input for MEAN operation.
Do you plan to implement 3D impleentation or may suggest any workaround for this problem?

Fault file: 
tensorflow_src\tensorflow\lite\micro\kernels\reduce.cc
In EvaMean function:
`TF_LITE_ENSURE_MSG(
      context, is_valid_inputs == true,
      ""Number of Input ""
      ""dimensions != 4 OR the Axis is not either [1, 2] or [2, 1]"");`




"
43331,InternalError:cudnn poolforward launch failed,"information:
     centos 7
     tensorflow-gpu: 12.0
     tensorflow-compression: 10.0
     cuda:  9.0

when i doing a forward test code,the wrong information as belows:
![5074244104368395475](https://user-images.githubusercontent.com/37282247/93563200-fef5a980-f9b9-11ea-9913-7a29191d1c3d.jpg)


and if i use tensorflow: 12.0 it's ok ,but using cpu runnig  is too slow.

Does anyone has anygood idea to solve it?


"
43330,How does TF decide to add examples for one API?,"I find that TF add examples for [tf.keras.backend.clear_session()](https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session), concatenate and etc., but not for [tf.keras.backend.batch_normalization](https://www.tensorflow.org/api_docs/python/tf/keras/backend/batch_normalization) or tf.keras.backend.binary_crossentropy. 

Does it freely for API developers to write examples? If that, does it seems chaotic?"
43324,"Standard Deviation calculation Pytorch vs TF, how to set unbiased","std calculation in Pytorch (with `unbiased=True`):
```
import torch
a = torch.tensor([[3.4, 5.6, 7.8], [2, 3, 4], [0.1, 0.2, 0.3]])
a.std(-1, unbiased=True)

>>> tensor([2.2000, 1.0000, 0.1000])
```
which is the **results I want in Tensorflow**. 
However, in TF I get:

```
import tensorflow as tf
a = tf.constant([[3.4, 5.6, 7.8], [2, 3, 4], [0.1, 0.2, 0.3]])
tf.math.reduce_std(a, axis=-1)

>>> <tf.Tensor: shape=(3,), dtype=float32, numpy=array([1.7962925 , 0.8164966 , 0.08164966], dtype=float32)>
```
This is similar to when `unbiased=False` in Pytorch:

```
import torch
a = torch.tensor([[3.4, 5.6, 7.8], [2, 3, 4], [0.1, 0.2, 0.3]])
a.std(-1, unbiased=False)

>>> tensor([1.7963, 0.8165, 0.0816])
```

Question: is there a way to have `unbiased=True` in TF to get this `tensor([2.2000, 1.0000, 0.1000])` std result in TF?
"
43323,AttributeError: '_TfDeviceCaptureOp' object has no attribute 'node_def',"Hi, i'm sorry to bother you, but i was meet a error when I use keras. my envs as follow: tensorflow-gpu==1.12, keras==2.2.4 , tensorpack==0.9.0.1
the error is:
![1](https://user-images.githubusercontent.com/56585970/93546332-26864b00-f995-11ea-9391-93b2373ec165.png)
and i use functions of keras by follow way:
![2](https://user-images.githubusercontent.com/56585970/93546361-30a84980-f995-11ea-908a-aa1242ceb4eb.png)
of course, it has the same error when i use ConvLSTM2D have get while i use the function ""BatchNormalization()"", which belong to keras. but the function 'Reshape()' and 'concatenate()' are run well . 

By the way, I used the keras function directly instead of modeling with keras.

Could you help me slove this error? I'd appreciate it if you could help me with this error.
"
43322,vectorized_map fails if function contains an abs of a complex128,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: 3.7.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
`vectorized_map` throws an `AssertionError` if the body contains a `tf.abs` of a `complex128` tensor.

**Describe the expected behavior**
No exception should be thrown.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
tf.vectorized_map(tf.abs, tf.cast([0, 1], dtype=tf.complex128))
```
(also see https://colab.research.google.com/drive/1ExIw0N92bHpgWxwRONmeP24BWCzWG3QL?usp=sharing)

**Other info / logs**
I think the issue is that the `ComplexAbs` gets converted to a straight call to `math_ops.complex_abs` (https://github.com/tensorflow/tensorflow/blame/master/tensorflow/python/ops/parallel_for/pfor.py#L2731), which by default returns a `float32` (instead of deciding the dtype based on the dtype of the argument). I believe it could be fixed just by changing that call to `math_ops.abs`."
43319,Undocumented NotFoundError occurs when using tf.io.gfile.glob() to retrieve files in a non-existent directory,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Chrome OS Linux 5.4.40-04224-g891a6cce2d44
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a
- TensorFlow installed from (source or binary): Installed from source (pip3 install tensoflow)
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.7
- Bazel version (if compiling from source): N/a
- GCC/Compiler version (if compiling from source): N/a
- CUDA/cuDNN version: N/a
- GPU model and memory: N/a

**Describe the current behavior**
When globbing files in a directory that does not exist, a non-documented tf.errors.NotFoundError is thrown if we try to wildcard glob files in a non-existent directory. This is not currently documented [here](https://www.tensorflow.org/api_docs/python/tf/io/gfile/glob).

**Describe the expected behavior**
We expect that either a ValueError should be thrown, or the an empty list returned (since no files were matched).

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
import tensorflow as tf
files = tf.io.gfile.glob(""non_existent_dir/*"")
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Issue was first discovered through [this PR](https://github.com/tensorflow/tfx/pull/2339#discussion_r490576170) for TFX, where we are expecting a ValueError instead of a NotFoundError.
"
43312,TensorFlow Custom loop gives different value of loss,"I am using following versions:

```
tf version:  2.3.0
keras version:  2.4.0
```
my customized error is

```
def mse_fn(y_true, y_pred):
  error = y_true - y_pred
  mserror = tf.reduce_mean(tf.square(error))
  return mserror
```
data processing for my code is

```
(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()
Y_train = np.array(Y_train).astype(np.float32)
Y_test = np.array(Y_test).astype(np.float32)
X_train = X_train/255.
X_test = X_test/255.
```
my customized DNN layer is

```
class MyDense(keras.layers.Layer):
  def __init__(self, units, activation=None, **kwargs):
    super().__init__(**kwargs)
    self.units = units
    self.activation = keras.activations.get(activation)
  
  def build(self, batch_input_shape):
    self.kernel = self.add_weight(name = ""kernel"", shape=[batch_input_shape[-1], self.units], initializer=""glorot_normal"")
    self.bias = self.add_weight(name = ""bias"", shape=[self.units], initializer = 'zeros')
    super().build(batch_input_shape)

  def call(self, X):
    return self.activation(X @ self.kernel + self.bias)
  
  def compute_output_shape(self, batch_input_shape):
    return tf.TensorShape(batch_input_shape.as_list()[:-1]+[self.units])
  
  def get_config(self):
    base_config = super().get_config()
    return {**base_config, ""units"":self.units, ""activation"": keras.activations.serialize(self.activation)}

```
the model is

```
class DNN_model(keras.Model):
  def __init__(self, output_dim, **kwargs):
    super().__init__(**kwargs)
    self.hidden1 = keras.layers.Flatten(input_shape = [28, 28])
    self.hidden2 = MyDense(32, activation='relu')
    self.hidden3 = MyDense(16, activation='relu')
    self.hidden4 = MyDense(8, activation='relu')
    self.hidden5 = MyDense(output_dim, activation='relu')
  
  def call(self, inputs):
    Z = self.hidden1(inputs)
    Z = self.hidden2(Z)
    Z = self.hidden3(Z)
    Z = self.hidden4(Z)
    Z = self.hidden5(Z)
    return Z

```
when I compile and fit using model.fit I get following loss

```
model_cust_loss = DNN_model(1)
model_cust_loss.compile(optimizer='adam', loss = mse_fn, metrics=['accuracy'])
model_cust_loss.fit(X_train, Y_train, epochs=20 , batch_size=32)
```
the output

```
1875/1875 [==============================] - 3s 1ms/step - loss: 2.5784 - accuracy: 0.1690
Epoch 2/20
1875/1875 [==============================] - 3s 1ms/step - loss: 1.0625 - accuracy: 0.1910
Epoch 3/20
1875/1875 [==============================] - 3s 1ms/step - loss: 0.8146 - accuracy: 0.1947
Epoch 4/20
1875/1875 [==============================] - 3s 1ms/step - loss: 0.7062 - accuracy: 0.1981
Epoch 5/20
1875/1875 [==============================] - 3s 1ms/step - loss: 0.6342 - accuracy: 0.1999
Epoch 6/20
1875/1875 [==============================] - 3s 1ms/step - loss: 0.5785 - accuracy: 0.2014
Epoch 7/20
1875/1875 [==============================] - 3s 1ms/step - loss: 0.5263 - accuracy: 0.2015
Epoch 8/20
1875/1875 [==============================] - 3s 1ms/step - loss: 0.4914 - accuracy: 0.2036
Epoch 9/20
1875/1875 [==============================] - 3s 1ms/step - loss: 0.4480 - accuracy: 0.2066
Epoch 10/20
...
```
and then I try customized loop

```
def random_batch(X, y, batch_size=32):
  idx = np.random.randint(len(X), size=batch_size)
  return X[idx], y[idx]

n_epochs = 5
batch_size = 32
n_steps = len(X_train)//batch_size
optimizer = keras.optimizers.Adam() 
print(""n_steps: "", n_steps)

epoch_losses = []
for epoch in range (1,n_epochs+1):
  print(""Epoch {}/{}"".format(epoch, n_epochs))
  batch_losses = []
  for step in range(1, n_steps+1):
    #print(""step:"",step)
    X_batch, Y_batch = random_batch(X_train, Y_train, batch_size = 32)
    with tf.GradientTape() as tape:
      Y_pred = model_cust_loss(X_batch, training=True)
      current_loss = mse_fn(Y_batch, Y_pred) 
    batch_losses.append(current_loss)
    gradients = tape.gradient(current_loss, model_cust_loss.trainable_variables)
    optimizer.apply_gradients(zip(gradients,model_cust_loss.trainable_variables))
  epoch_losses.append(np.mean(batch_losses))
  print(""loss per epoch:"", np.mean(batch_losses))
```
The output gives a different value of loss than the model.fit method

```
Epoch 1/5
loss per epoch: 8.3357525
Epoch 2/5
loss per epoch: 8.387993
Epoch 3/5
loss per epoch: 8.356688
Epoch 4/5
loss per epoch: 8.362662
Epoch 5/5
loss per epoch: 8.374953
```
could you help?

Thanks, Debottam"
43311,Building Tensorflow using non-default GCC fails with /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found and further errors,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**SLURM cluster node: Linux gnode028 3.10.0-1062.12.1.el7.x86_64 #1 SMP x86_64 x86_64 x86_64 GNU/Linux**

- TensorFlow version:
**TensorFlow v2.3.0**

- Python version:
**Python 3.8.0 complied from source**

- Bazel version (if compiling from source):
**bazel 3.1.0 compiled from source**

- GCC/Compiler version (if compiling from source):
**gcc (GCC) 7.3.0 compiled from source**

- CUDA/cuDNN version:
**CUDA 10.2**

- GPU model and memory:
**Tesla K20m**

**Describe the problem**
I'm trying to compile Tensorflow on a SLURM cluster machine where the default version of GCC is outdated (`4.8.5`) and cannot be upgraded. The only solution is to use newer version of GCC (in my case `7.3.0`)  installed as user. However, when using non-default GCC the compilation procedure ends up with the following error:

`bazel-out/host/bin/external/flatbuffers/flatc: /lib64/libstdc++.so.6: version GLIBCXX_3.4.20 not found (required by bazel-out/host/bin/external/flatbuffers/flatc)`

From what I understand the root of the problem is the fact that `Bazel` links to the default libraries from hardcoded `/lib64/libstdc++` instead of using the libraries from non-default gcc. The problem has been reported frequently by many different users. I tried all the solutions provided in multiple issues (https://github.com/bazelbuild/bazel/issues/4510, https://github.com/tensorflow/tensorflow/issues/38718, https://github.com/tensorflow/tensorflow/issues/26826), implemented many of them but eventually failed to finish compilation successfully. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
My goal is to create a comprehensive instructions for all the people facing this issue. I created a gist file listing all the steps I followed (including compiling `Bazel` from source): https://gist.github.com/jakublipinski/40ba68994fe0092600a05b0060e7d445

After all the steps my setup consists of `Python 3.8`, `GCC 7.3.0`, `Bazel 3.1.0` and `Binutils 2.34`. All these packages are installed locally as user.

**Any other info / logs**
`Bazel 3.1.0` is compiled from source using the following command:
```
export GCC_HOST_COMPILER_PATH=$HOME/gcc/bin/gcc
export GCC_HOST_COMPILER_PREFIX=$HOME/gcc/bin
export CXX=$HOME/gcc/bin/gcc
export CC=$HOME/gcc/bin/gcc
export LD_LIBRARY_PATH=$HOME/gcc/lib64
export LDFLAGS=""-L$HOME/gcc/lib -L$HOME/gcc/lib64""
export CXXFLAGS=""-L$HOME/gcc/lib -L$HOME/gcc/lib64""

env BAZEL_LINKOPTS=-static-libstdc++:-static-libgcc BAZEL_LINKLIBS=-l%:libstdc++.a:-lm bash ./compile.sh
mkdir $HOME/bin && cp output/bazel $HOME/bin
```

The TensorFlow build process initiated by:
```
export PATH=~/bin:$PATH
export GCC_HOST_COMPILER_PATH=$HOME/gcc/bin/gcc
export GCC_HOST_COMPILER_PREFIX=$HOME/gcc/bin
export CXX=$HOME/gcc/bin/gcc
export CC=$HOME/gcc/bin/gcc
export LD_LIBRARY_PATH=$HOME/gcc/lib64
export LDFLAGS=""-L$HOME/gcc/lib -L$HOME/gcc/lib64""
export CXXFLAGS=""-L$HOME/gcc/lib -L$HOME/gcc/lib64""

env BAZEL_LINKOPTS=-static-libstdc++:-static-libgcc BAZEL_LINKLIBS=-l%:libstdc++.a:-lm BAZEL_CXXOPTS=-std=gnu++0x bazel build //tensorflow/tools/pip_package:build_pip_package --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}"" --verbose_failures
```

fails with:

```
ERROR: /home/users/user/tensorflow/tensorflow/lite/toco/BUILD:439:1: Linking of rule '//tensorflow/lite/toco:toco' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/users/user/.cache/bazel/_bazel_user/77a387926d2bc2a010179601b7d63fc2/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/home/users/user/gcc/lib64 \
    PATH=/home/users/user/bin:/home/users/user/tensorflow/venv/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/slurm/bin:/bin:/opt/slurm/bin:/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/host/bin/tensorflow/lite/toco/toco-2.params)
Execution platform: @local_execution_config_platform//:platform
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Slite_Stoco_Ctoco___Utensorflow/libtensorflow_framework.so.2: undefined reference to `std::allocator<long long>::allocator()'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 10.872s, Critical Path: 1.07s
INFO: 5 processes: 5 local.
FAILED: Build did NOT complete successfully
```

I allow myself to cc @krafczyk and @soporteCluster as they were reporting similar issues and may provide some insight.

I'm ready to try different solutions and provide more info. I really want this issue to be solved once and for all. I understand that the root cause may be related to how Bazel operates. Anyway, the issue affects many TensorFlow users. Let's provide some workaround for them. Thanks in advance for all suggestions. I promise to test them all.
"
43309,tf.nn.softmax_cross_entropy_with_logits output on multichannel continuous targets,"Hi,

How does the tf.nn.softmax_cross_entropy_with_logits work for a target that is continuous with C channels,
eg - HxWxC, they are not soft targets (values across channels dont add up to 1) with values ranging between 0 and 1.

Thanks in advance
"
43307,g++/gcc cannot find header files in/or libtensorflow_cc.so,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04 on docker (image: `nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04`)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version: **r2.3**
- Python version: **3.7.6**
- Installed using virtualenv? pip? conda?: **No**
- Bazel version (if compiling from source): **3.1.0**
- GCC/Compiler version (if compiling from source): **7.5.0**
- CUDA/cuDNN version: **10.1/7.6**
- GPU model and memory: **RTX 2080 Super - 8GB**



**Describe the problem**
I'm building tensor flow from source for a C++ project using the guide here: https://www.tensorflow.org/install/source
I can generate the `.so` files using the bazel build however, when i try to link the lib to my program, i get

`fatal error: tensorflow/cc/saved_model/loader.h: No such file or directory`

Location of build files:

```
root@167c90dbc0fe:~# ls -l /tensorflow/bazel-bin/tensorflow/
total 427692
drwxr-xr-x  5 root root      4096 Sep 17 19:17 c
drwxr-xr-x  6 root root      4096 Sep 17 19:16 cc
drwxr-xr-x  6 root root      4096 Sep 17 18:28 compiler
drwxr-xr-x 19 root root     12288 Sep 17 19:19 core
lrwxrwxrwx  1 root root        21 Sep 17 19:19 libtensorflow_cc.so -> libtensorflow_cc.so.2
lrwxrwxrwx  1 root root        25 Sep 17 19:19 libtensorflow_cc.so.2 -> libtensorflow_cc.so.2.3.0
-r-xr-xr-x  1 root root 399797080 Sep 17 19:19 libtensorflow_cc.so.2.3.0
-r-xr-xr-x  1 root root    178414 Sep 17 18:39 libtensorflow_cc.so.2.3.0-2.params
lrwxrwxrwx  1 root root        32 Sep 17 18:30 libtensorflow_framework.so.2 -> libtensorflow_framework.so.2.3.0
-r-xr-xr-x  1 root root  37893936 Sep 17 18:30 libtensorflow_framework.so.2.3.0
-r-xr-xr-x  1 root root     47188 Sep 17 18:20 libtensorflow_framework.so.2.3.0-2.params
drwxr-xr-x 10 root root      4096 Sep 17 19:06 stream_executor
```

Source file: `load_saved_model.cpp`

```
#include <stdlib.h>
#include <stdio.h>
#include <string>
#include ""tensorflow/cc/saved_model/loader.h""

using namespace std;



int  main(int argc, char* argv[]){
    printf(""Will load and serve a saved model..."")
}
```

Compiling the source file using the following:

```g++ -L/tensorflow/bazel-bin/tensorflow/ -ltensorflow_cc -ltensorflow_framework  -o /object_detection/load_saved_model -c /object_detection/load_saved_model.cpp```

results in:

```
root@167c90dbc0fe:/object_detection# g++ -L/tensorflow/bazel-bin/tensorflow/ -ltensorflow_cc -ltensorflow_framework -I/tensorflow/bazel-bin/  -o /object_detection/load_saved_model -c /object_detection/load_saved_model.cpp 
/object_detection/load_saved_model.cpp:4:10: fatal error: tensorflow/cc/saved_model/loader.h: No such file or directory
 #include ""tensorflow/cc/saved_model/loader.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. `git clone https://github.com/tensorflow/tensorflow.git `
2. `git checkout r2.3`
3. `bazel clean`
4. `root@167c90dbc0fe:/tensorflow# ./configure`

```
You have bazel 3.1.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 


Found possible Python library paths:
  /usr/local/lib/python3.6/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.6/dist-packages]
/usr/local/lib/python3.6/dist-packages
Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: N
No TensorRT support will be enabled for TensorFlow.

Found CUDA 10.1 in:
    /usr/local/cuda-10.1/targets/x86_64-linux/lib
    /usr/local/cuda-10.1/targets/x86_64-linux/include
Found cuDNN 7 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 3.5, 7.0, 7.5

Do you want to use clang as CUDA compiler? [y/N]: N
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```

5. `bazel build --jobs=8 --verbose_failures --config=cuda -c opt //tensorflow:libtensorflow_cc.so`

```
Target //tensorflow:libtensorflow_cc.so up-to-date:
  bazel-bin/tensorflow/libtensorflow_cc.so
INFO: Elapsed time: 3582.131s, Critical Path: 175.91s
INFO: 11806 processes: 11806 local.
INFO: Build completed successfully, 16884 total actions
```

7. Run the `load_saved_model.cpp`

```
root@167c90dbc0fe:/object_detection# g++ -L/tensorflow/bazel-bin/tensorflow/ -ltensorflow_cc -ltensorflow_framework -I/tensorflow/bazel-bin/  -o /object_detection/load_saved_model -c /object_detection/load_saved_model.cpp 

/object_detection/load_saved_model.cpp:4:10: fatal error: tensorflow/cc/saved_model/loader.h: No such file or directory
 #include ""tensorflow/cc/saved_model/loader.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
```

**Additional info**

How can i link the generated `.so` files to my cpp program?

I also tried 

1. moving the `.so` files to `/usr/lib/x86_64-linux-gnu/` followed by `ldconfig` 
2. Adding the `.so` location to a `.conf` file and then adding the conf file to `./etc/ld.so.conf.d` which didn't help either.

Thanks"
43304,"Model and layers set to non-trainable, but weights still adjusted","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom Code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow installed from (source or binary): Google Colab
- TensorFlow version (use command below): Google Colab 2.3.0
- Python version: 3.6.9 (default, Jul 17 2020, 12:50:27) \n[GCC 8.4.0]
- GPU model and memory: Google Colab GPU

I want to freeze a model. That means, set it to non-trainable. I do so by setting model.trainable and the individual layers to non-trainable. However, when I then fit this model (with just 1 epoch) I can see afterwards that the model weights changed. I can see this by checking the model.evaluate output and by comparing the model weights with print(model.trainable_variables) prior and afterwards the model fitting.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

(train_x, train_labels), (test_x, test_labels) = tf.keras.datasets.imdb.load_data(num_words=10000)

x_train_padded = pad_sequences(train_x, maxlen=500)
x_test_padded = pad_sequences(test_x, maxlen=500)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(10000, 128, input_length=500),
    tf.keras.layers.Conv1D(128, 5, activation='relu'),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])

model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),optimizer='adam', metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])

history = model.fit(x=x_train_padded,
                      y=train_labels,
                      validation_data=(x_test_padded , test_labels),
                      epochs=4, batch_size=128)
```

This gives the output:

> Epoch 4/4
> 196/196 [==============================] - 9s 48ms/step - loss: 0.1202 - accuracy: 0.9578 - val_loss: 0.3663 - val_accuracy: 0.8669

I can confirm this with:

`model.evaluate(x_test_padded , test_labels)`

> 782/782 [==============================] - 3s 4ms/step - loss: 0.3663 - accuracy: 0.8669
> 
> [0.36633849143981934, 0.866919994354248]

Now I set the model and individual layers to non-trainable:

```
model.trainable=False

for layer in model.layers:
  layer.trainable=False
```

I check with `model.summary()` that

>  Trainable params: 0

And fit the model again:
```
history = model.fit(x=x_train_padded,
                      y=train_labels,
                      validation_data=(x_test_padded , test_labels),
                      epochs=1, batch_size=128)
```

This gives:



> 196/196 [==============================] - 10s 52ms/step - loss: 0.0911 - accuracy: 0.9677 - val_loss: 0.4298 - val_accuracy: 0.8648

I can again check this with

`model.evaluate(x_test_padded , test_labels)`



> 782/782 [==============================] - 4s 5ms/step - loss: 0.4298 - accuracy: 0.8648
> 
> [0.4298333525657654, 0.8648399710655212]


Which clearly shows that the model was adjusted. The val_loss and val_accuracy is not equal to loss: 0.3663 - accuracy: 0.8669. Moreover, when I check the model weights with 
`print(model.trainable_variables)` before and afterwards I can see that the model weights got adjusted. However, actually all parameters should be set to non-trainable.

"
43303,How about we stop deprecating everything every other release....,"I can't believe how terrible the experience is using these tools.

It seems the whole process is playing jigsaw puzzle games trying to line up all the versions so that something, anything works.

We are sorry but your original working tensorflow code for training is being moved to generic tf.slim for generic training.
We are sorry but your changed code using generic tf.slim was not specific enough, make these changes.
We are sorry but after having you do all this, tf.slim is moving to contrib.
Okay, now contrib is going away and here is a basic script that will just tell you cannot change anything for you, code it all again yourself. All 3rd party guides are now worthless.
(Roughly after about a year of this, I realized what a waste of time and left tensorflow)

Now I'm back only to try to get other frameworks (which work) models converted to tflite since it's the only decent solution for performance on mobile.
Of course the tools to convert to tf-lite require a bunch of tensorflow things to be installed again.
Oops, I'm using Cuda 10.1, watch pip packages try opening libcudart-9.
Install tensorflow 2.x from source for cuda 10.1, whoops it wants newer python now.
Install tensorflow addons, BOOM it doesn't support tensorflow 2.4.0.
Backdate to tensorflow 2.1, rebuild everything.
Install tensorflow addons, BOOM it doesn't support tensorflow <2.2.0.

I know this will just get closed and someone will mention that I'm wrong for being a Gentoo user who expects to build from source and not limp off of a packaging system to fix a project that cannot properly figure out how to handle backwards compatibility. Nor will I use some ""docker"" container....

Do you need some help coding? Is this intentional lock-in related movement to keep everyone rewriting things? Can you stop deprecating things long enough for someone to actually build something longer than a year and have it work when they are done? Thanks."
43302,changes its APIs quite often,There are a lot of very good projects by top international experts (think Google) that are dropping Tensorflow because it changes its APIs quite often we shouldn't expect users to have always a GPU. Please consider this failure.
43301,Should we remove deprecated and ignored field 'version' in graph.proto,"We noticed that the `version` field in `graph.proto` has been deprecated (since 5 years ago). When we tried to generate `GraphDef` scala code by `sbt-protoc` from this proto file, it produced annoying ""deprecation"" warnings during the compilation stage. 

Should we remove it from tensorflow's codebase?

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/graph.proto#L24-L27

According to the description in the codebase:
>  // Deprecated single version field; use versions above instead.  Since all
    // GraphDef changes before ""versions"" was introduced were forward
    // compatible, this field is entirely ignored.

This field seems to be completely not used, no matter old or new versions. 

We appreciate any feedbacks. Thanks!"
43300,Crash when attempting to use xla.conv with complex inputs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10.4
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-41557-gae0a324182 2.4.0-dev20200915
- Python version: Python 3.8.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

TF throws an exception when trying to use `tensorflow.compiler.tf2xla.python.xla.conv` with complex inputs, and the stack trace indicates it may be a bug in the HLO -> LLVM IR lowering.

**Standalone code to reproduce the issue**

```python
import numpy as np
from tensorflow.compiler.tf2xla.python import xla as tfxla
from tensorflow.compiler.xla import xla_data_pb2

proto = xla_data_pb2.ConvolutionDimensionNumbers()
proto.input_batch_dimension = 0
proto.input_feature_dimension = 1
proto.output_batch_dimension = 0
proto.output_feature_dimension = 1
proto.kernel_output_feature_dimension = 0
proto.kernel_input_feature_dimension = 1
proto.input_spatial_dimensions.extend([2, 3])
proto.kernel_spatial_dimensions.extend([2, 3])
proto.output_spatial_dimensions.extend([2, 3])

lhs = np.ones((2,3,9,10), dtype=np.complex64)
rhs = np.ones((3,3,4,5), dtype=np.complex64)

padding = ((0, 0), (0, 0))
window_strides, lhs_dilation, rhs_dilation = (1, 1), (1, 1), (1, 1)
feature_group_count = 1

tfxla.conv(lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
           proto, feature_group_count=feature_group_count, precision_config=None)
```

**Other info / logs**

Stack trace:

```
2020-09-17 16:13:20.385677: E tensorflow/compiler/xla/status_macros.cc:56] Internal: RET_CHECK failure (tensorflow/compiler/xla/service/cpu/cpu_compiler.cc:501) !llvm::verifyModule(llvm_module, &err_stream) Invalid LLVM IR before optimizations:
Floating-point arithmetic operators only work with floating-point types!
  %60 = fmul reassoc nsz contract %complex64 %57, %59
Floating-point arithmetic operators only work with floating-point types!
  %62 = fadd reassoc nsz contract %complex64 %61, %60

This probably indicates a bug in the HLO -> LLVM IR lowering. Rerun with --xla_dump_to to get the IR. 
*** Begin stack trace ***

        xla::status_macros::MakeErrorStream::Impl::GetStatus()

        xla::cpu::CpuCompiler::RunBackend(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*)
        xla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*)
        xla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_2020_02_25::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)
        xla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_2020_02_25::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)
        tensorflow::XlaCompilationCache::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&, std::unique_ptr<xla::LocalExecutable, std::default_delete<xla::LocalExecutable> >*)
        tensorflow::XlaCompilationCache::CompileImpl(tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, absl::lts_2020_02_25::Span<tensorflow::XlaArgument const>, std::function<tensorflow::Status (tensorflow::XlaCompiler*, tensorflow::XlaCompilationResult*)> const&, absl::lts_2020_02_25::optional<long long>, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)
        tensorflow::XlaCompilationCache::CompileSingleOp(tensorflow::XlaCompiler::Options const&, absl::lts_2020_02_25::Span<tensorflow::XlaArgument const>, tensorflow::OpKernelContext*, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)
        tensorflow::XlaCompileOnDemandOp::Compile(tensorflow::OpKernelContext*, tensorflow::XlaCompilationResult const**, tensorflow::XlaCompilationCache**, absl::lts_2020_02_25::flat_hash_map<int, absl::lts_2020_02_25::optional<tensorflow::Tensor>, absl::lts_2020_02_25::hash_internal::Hash<int>, std::equal_to<int>, std::allocator<std::pair<int const, absl::lts_2020_02_25::optional<tensorflow::Tensor> > > >*, xla::LocalExecutable**)
        tensorflow::XlaCompileOnDemandOp::Compute(tensorflow::OpKernelContext*)
        tensorflow::KernelAndDeviceOp::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<absl::lts_2020_02_25::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<absl::lts_2020_02_25::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tensorflow::CancellationManager*, absl::lts_2020_02_25::optional<tensorflow::EagerRemoteFunctionParams> const&)
        tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_2020_02_25::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, absl::lts_2020_02_25::optional<tensorflow::EagerRemoteFunctionParams> const&, std::unique_ptr<tensorflow::KernelAndDevice, tensorflow::core::RefCountDeleter> const&, tensorflow::GraphCollector*, tensorflow::CancellationManager*, absl::lts_2020_02_25::Span<tensorflow::TensorHandle*>)
        tensorflow::ExecuteNode::Run()
        tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*)

        tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*)
        tensorflow::EagerOperation::Execute(absl::lts_2020_02_25::Span<tensorflow::AbstractTensorHandle*>, int*)
        TFE_Execute
        TFE_Py_ExecuteCancelable(TFE_Context*, char const*, char const*, absl::lts_2020_02_25::InlinedVector<TFE_TensorHandle*, 4ul, std::allocator<TFE_TensorHandle*> >*, _object*, TFE_CancellationManager*, absl::lts_2020_02_25::InlinedVector<TFE_TensorHandle*, 2ul, std::allocator<TFE_TensorHandle*> >*, TF_Status*)




        _PyObject_MakeTpCall
        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName
        _PyFunction_Vectorcall
        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName
        _PyFunction_Vectorcall
        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName
        _PyFunction_Vectorcall
        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName
        _PyFunction_Vectorcall
        _PyEval_EvalFrameDefault
        _PyEval_EvalCodeWithName
        PyEval_EvalCode

        PyRun_FileExFlags
        PyRun_SimpleFileExFlags

        Py_BytesMain
        __libc_start_main
        _start
*** End stack trace ***
```"
43298,There is a bug about tf/keras/Sequential#fit(),"For ""tf.keras.Sequential.fit"" API, in TF2.1, documentation said that ""fit_generator"" is deprecated and ""fit"" support generator. But actually, in TF2.0, I find that ""fit"" also support generator. So it means that the deprecated information mislead to users that TF2.0 ""fit"" API cannot support the function of ""fit_generator"" API."
43297,Kernel crash with complex matrices on Windows,"Hello, I'd like to raise this issue again as I am encountering the same problem on TF 2.3.0.

The original issue at https://github.com/tensorflow/tensorflow/issues/40414 attempts to replicate using Colab, which isn't running Windows.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10.0.18362**
- TensorFlow installed from (source or binary): **Binary (pip)**
- TensorFlow version (use command below): **2.3.0 (v2.3.0-rc2-23-gb36436b087)**
- Python version: **3.8.5**
- CUDA/cuDNN version: **10.1.120/7.6.5 (also tried 10.1.243/7.6.5)**
- GPU model and memory: **GeForce RTX 2080 Ti, 11265 MB**

**Describe the current behavior**

Multiplying two matrices of complex type (values themselves can be real) with either `adjoint_a=True` or `adjoint_b=True` results in the following crash. It also seems to crash when running `eig`. This was reproduced on both TF 2.3.0 and 2.2.0, but works on 2.1.0. It also seems to be working on Linux.

I tried doing a fresh install of CUDA with CUDA 10.1.120 and 10.1.243. In both cases the [MNIST tutorial example](https://www.tensorflow.org/tutorials/quickstart/beginner) runs with no problems.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
a = tf.constant([[1, 0], [0, 1]], dtype=tf.complex64)
tf.linalg.matmul(a, a)  # No issues
tf.linalg.matmul(a, a, adjoint_a=True)  # Crash
```

**Other info / logs**:

```
>>> tf.linalg.matmul(a, a)
2020-08-25 18:30:03.799456: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
<tf.Tensor: shape=(2, 2), dtype=complex64, numpy=
array([[1.+0.j, 0.+0.j],
       [0.+0.j, 1.+0.j]], dtype=complex64)>
>>> tf.linalg.matmul(a, a, adjoint_a=True)
2020-08-25 18:30:07.982835: E tensorflow/stream_executor/cuda/cuda_driver.cc:951] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure :: 0x00007FF991B9B115     tensorflow::CurrentStackTrace
0x00007FF9918E989E      tensorflow::CostGraphDef_Node::set_is_final
0x00007FF991A91D7E      stream_executor::StreamExecutor::SetDeviceSharedMemoryConfig
0x00007FF98F622C16      tensorflow::StepStats::internal_default_instance
0x00007FF98F634444      google::protobuf::RepeatedPtrField<tensorflow::InterconnectLink>::Add
0x00007FF9774EF867      std::vector<tensorflow::DtypeAndPartialTensorShape,std::allocator<tensorflow::DtypeAndPartialTensorShape> >::operator=
0x00007FF9774CA7AB      absl::lts_2020_02_25::Span<tensorflow::Tensor const >::end
0x00007FF9774431BF      TFE_TensorHandleResolve
0x00007FF9773E0A33      TFE_Py_TensorShapeSlice
0x00007FF9773DE29A      std::_Tree<std::_Tmap_traits<std::array<std::basic_string<char,std::char_traits<char>,std::allocator<char> >,0>,tensorflow::monitoring::CounterCell,std::less<std::array<std::basic_string<char,std::char_traits<char>,std::allocator<char>
0x00007FF9A51CA3D6      PyList_New
0x00007FF9A51F5626      Py_CheckFunctionResult
0x00007FF9A51F7954      PyEval_EvalFrameDefault
0x00007FF9A51F596E      Py_CheckFunctionResult
0x00007FF9A51F7954      PyEval_EvalFrameDefault
0x00007FF9A51F2EF8      PyEval_EvalCodeWithName
0x00007FF9A51F5C66      Py_CheckFunctionResult
0x00007FF9A51F801E      PyEval_EvalFrameDefault
0x00007FF9A51F3D7D      PyFunction_Vectorcall
0x00007FF9A525FAF9      PyEval_GetFuncDesc
0x00007FF9A525F8DD      PyEval_GetFuncDesc
0x00007FF9A51E7DE4      PyObject_Repr
0x00007FF9A5181E2F      PyFile_WriteObject
0x00007FF9A5181B1B      PyFile_WriteString
0x00007FF9A51EB200      PyFloat_AsDouble
0x00007FF9A51E401E      PyObject_CallFunctionObjArgs
0x00007FF9A51F9E70      PyEval_EvalFrameDefault
0x00007FF9A51F2EF8      PyEval_EvalCodeWithName
0x00007FF9A52038DF      PyEval_EvalCodeEx
0x00007FF9A520383D      PyEval_EvalCode
0x00007FF9A5203526      PyArena_New
0x00007FF9A52034B5      PyArena_New
0x00007FF9A5365760      PyRun_InteractiveOneObject
0x00007FF9A5365373      PyRun_InteractiveLoopFlags
0x00007FF9A53651B1      PyRun_AnyFileExFlags
0x00007FF9A52F917D      Py_FatalError
0x00007FF9A526428F      Py_RunMain
0x00007FF9A5264141      Py_RunMain
0x00007FF9A5264126      Py_Main
0x00007FF9A52640DD      Py_Main
0x00007FF7C5F41268      (unknown)
0x00007FF9F1B57BD4      BaseThreadInitThunk
0x00007FF9F324CE51      RtlUserThreadStart

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\qulab\Anaconda3\envs\tf2-3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1009, in __repr__
    self.shape, self.dtype.name, numpy_text(self, is_repr=True))
  File ""C:\Users\qulab\Anaconda3\envs\tf2-3\lib\site-packages\tensorflow\python\framework\ops.py"", line 225, in numpy_text
    text = repr(tensor._numpy()) if is_repr else str(tensor._numpy())
  File ""C:\Users\qulab\Anaconda3\envs\tf2-3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1031, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: GPU sync failed
```"
43295,Arguments to max_pool are missing correspondance,"https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/python/ops/nn_ops.py#L3820

I have a tensor with shape (N,W,C). When I want to do max_pool1d with stride 2 for W, and then the strides with shape [1,2,1] throws an error and it indicates that the W dimension in the strides parameter is on the first position, so the strides parameter in this case must be [2,1,1]. This should be either changed so that it reflects the shape (N,W,C), or at  least described in the documentation.

```
import tensorflow as tf
import unittest

class TestMethods(unittest.TestCase):

    def test_max_pool(self):
        num_classes = 10
        print('num_classes: ', num_classes)
        batch_size = 2
        y_output = tf.constant(value=[[.1, .3, .2, .0, .8, .4, .1, .1, .1, .2],
                                      [.1, .9, .2, .0, .1, .4, .1, .1, .1, .2]])
        input = tf.reshape(tensor=y_output, shape=[batch_size, num_classes, 1])
        print('input shape: ', input.shape)
        y_max = tf.nn.max_pool1d(input=input, ksize=[num_classes, 1, 1], strides=[1, 1, 1], padding='VALID', data_format='NWC')
        print('y_max: ', y_max)
        y_max = tf.reshape(tensor=y_max, shape=[batch_size])
        print('y_max: ', y_max)


if __name__ == ""__main__"":
    tf.compat.v1.enable_eager_execution()
    unittest.main()
```

Results:
```
Testing started ...

Process finished with exit code 0
num_classes:  10
input shape:  (2, 10, 1)

Ran 1 test in 0.031s

OK
y_max:  tf.Tensor(
[[[0.8]]

 [[0.9]]], shape=(2, 1, 1), dtype=float32)
y_max:  tf.Tensor([0.8 0.9], shape=(2,), dtype=float32)
```"
43294,How to evluate a Model accuracy,"Since there is mentioned to evaluate [Model accuracy](https://www.tensorflow.org/lite/performance/post_training_quantization#model_accuracy) I wonder how to use it for a evaluation, please do you have an example (sample) how to evaluate it?

Thank you."
43293,Usage of intermediate layer causes an exception,"**System information**

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution: Windows 10
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.3
Python version: 3.7.5
CUDA/cuDNN version: 10.1
GPU model and memory: RTX 2080 Ti

**Describe the current behavior**
In toy example (provided below) there is an example of usage intermediate NN layer in loss function and in accuracy validation logic. On TF 2.3 it causes the following exeption:

`Traceback (most recent call last): File ""C:/PyProjects/TF_Issue/main.py"", line 53, in <module> hits += tf.reduce_sum(tf.where((output - target_batch + x_slice) < 0.01, 1, 0)).numpy() AttributeError: 'Tensor' object has no attribute 'numpy'`

I marked the line which causes the error. Without usage of intermediate layer it works.
Please, note these are two lines in the script below:

 ```
 hits += tf.reduce_sum(tf.where((output - target_batch + x_slice) < 0.01, 1, 0)).numpy() # this line causes the error
  # hits += tf.reduce_sum(tf.where(output - target_batch < 0.01, 1, 0)).numpy() # if x_slice is omitted script works fine
```

Commenting-out the first one and uncomment the second to get the script working.

**Standalone code to reproduce the issue**

```
import tensorflow as tf

batch_size = 128

input = tf.keras.Input(shape=(None, 1))
x = tf.keras.layers.Dense(1)(input)
output = tf.keras.layers.Dense(1)(x)

model = tf.keras.Model(inputs=input, outputs=output)

# A toy dataset of points around 3 * x + 2
NUM_EXAMPLES = 2000
inputs = tf.random.normal([NUM_EXAMPLES])
noise = tf.random.normal([NUM_EXAMPLES])
outputs = inputs * 3 + 2 + noise

training_inputs = tf.reshape(inputs[:1500], (1500, 1))
training_outputs = tf.reshape(outputs[:1500], (1500, 1))
training_inputs = tf.data.Dataset.from_tensor_slices(training_inputs).batch(batch_size)
training_outputs = tf.data.Dataset.from_tensor_slices(training_outputs).batch(batch_size)
test_inputs = tf.reshape(inputs[1500:], (500, 1))
test_outputs = tf.reshape(outputs[1500:], (500, 1))
test_inputs = tf.data.Dataset.from_tensor_slices(test_inputs).batch(batch_size)
test_outputs = tf.data.Dataset.from_tensor_slices(test_outputs).batch(batch_size)


def loss(model, inputs, targets):
  outputs = model(inputs)
  output = outputs[:, 0]  # take the first output (in general model can have several outputs)
  global x
  x_slice = x[:, 0]
  error = output - targets + x_slice
  return tf.reduce_mean(tf.square(error))


optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
epoch = 3
for i in range(epoch):
  for input_batch, target_batch in zip(training_inputs, training_outputs):
    with tf.GradientTape() as tape:
      loss_value = loss(model, input_batch, target_batch)
      grads = tape.gradient(loss_value, model.trainable_variables)
      optimizer.apply_gradients(zip(grads, model.trainable_variables))
  print('epoch #:', i)


hits = 0
total = 0
for input_batch, target_batch in zip(test_inputs, test_outputs):
  outputs = model(input_batch)
  output = outputs[:, 0]  # take the first output (in general model can have several outputs)
  x_slice = x[:, 0]
  hits += tf.reduce_sum(tf.where((output - target_batch + x_slice) < 0.01, 1, 0)).numpy() # this line causes the error
  # hits += tf.reduce_sum(tf.where(output - target_batch < 0.01, 1, 0)).numpy() # if x_slice is omitted script works fine
  total += input_batch.shape[0]

print(hits)
print('Accuracy: ', hits/total)

```
Any ideas how to fix the issues?
Thanks in advance!!!
"
43292,Error while trying to save the decoder of the Image-Captioning model into TFLite file.,"**System information**
- Using Google Colaboratory

I am trying to convert this Image-Captioning model into a TFLite file: https://www.tensorflow.org/tutorials/text/image_captioning
For this, I am trying to make two tensorflow lite files- one from the encoder and the other from the decoder objects after training them.

The encoder gets saved using `encoder.save(...)` and then it gets converted into the corresponding TFLite file using:
```
converter = tf.lite.TFLiteConverter.from_saved_model(""saved_encoder/mera_encoder"", signature_keys=None)
tflite_model = converter.convert()
```
but while saving the decoder using `decoder.save(...)` , I get an error saying : _""TypeError: call() missing 2 required positional arguments: 'features' and 'hidden'""_ 

What should I do?

The entire error call traceback: 

```
TypeError                                 Traceback (most recent call last)
<ipython-input-37-d437f5400f07> in <module>()
----> 1 decoder.save(""saved_encoder/mera_decoder"")

24 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)
   1977     """"""
   1978     save.save_model(self, filepath, overwrite, include_optimizer, save_format,
-> 1979                     signatures, options)
   1980 
   1981   def save_weights(self,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)
    132   else:
    133     saved_model_save.save(model, filepath, overwrite, include_optimizer,
--> 134                           signatures, options)
    135 
    136 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)
     78     # we use the default replica context here.
     79     with distribution_strategy_context._get_default_replica_context():  # pylint: disable=protected-access
---> 80       save_lib.save(model, filepath, signatures, options)
     81 
     82   if not include_optimizer:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures, options)
    974 
    975   _, exported_graph, object_saver, asset_info = _build_meta_graph(
--> 976       obj, export_dir, signatures, options, meta_graph_def)
    977   saved_model.saved_model_schema_version = constants.SAVED_MODEL_SCHEMA_VERSION
    978 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in _build_meta_graph(obj, export_dir, signatures, options, meta_graph_def)
   1045   if signatures is None:
   1046     signatures = signature_serialization.find_function_to_export(
-> 1047         checkpoint_graph_view)
   1048 
   1049   signatures, wrapped_functions = (

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_serialization.py in find_function_to_export(saveable_view)
     73   # If the user did not specify signatures, check the root object for a function
     74   # that can be made into a signature.
---> 75   functions = saveable_view.list_functions(saveable_view.root)
     76   signature = functions.get(DEFAULT_SIGNATURE_ATTR, None)
     77   if signature is not None:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in list_functions(self, obj, extra_functions)
    143     if obj_functions is None:
    144       obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access
--> 145           self._serialization_cache)
    146       self._functions[obj] = obj_functions
    147     if extra_functions:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _list_functions_for_serialization(self, serialization_cache)
   2588     self.predict_function = None
   2589     functions = super(
-> 2590         Model, self)._list_functions_for_serialization(serialization_cache)
   2591     self.train_function = train_function
   2592     self.test_function = test_function

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in _list_functions_for_serialization(self, serialization_cache)
   3017   def _list_functions_for_serialization(self, serialization_cache):
   3018     return (self._trackable_saved_model_saver
-> 3019             .list_functions_for_serialization(serialization_cache))
   3020 
   3021   def __getstate__(self):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py in list_functions_for_serialization(self, serialization_cache)
     85         `ConcreteFunction`.
     86     """"""
---> 87     fns = self.functions_to_serialize(serialization_cache)
     88 
     89     # The parent AutoTrackable class saves all user-defined tf.functions, and

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in functions_to_serialize(self, serialization_cache)
     77   def functions_to_serialize(self, serialization_cache):
     78     return (self._get_serialized_attributes(
---> 79         serialization_cache).functions_to_serialize)
     80 
     81   def _get_serialized_attributes(self, serialization_cache):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes(self, serialization_cache)
     93 
     94     object_dict, function_dict = self._get_serialized_attributes_internal(
---> 95         serialization_cache)
     96 
     97     serialized_attr.set_and_validate_objects(object_dict)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)
     49     # cache (i.e. this is the root level object).
     50     if len(serialization_cache[constants.KERAS_CACHE_KEY]) == 1:
---> 51       default_signature = save_impl.default_save_signature(self.obj)
     52 
     53     # Other than the default signature function, all other attributes match with

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in default_save_signature(layer)
    203   original_losses = _reset_layer_losses(layer)
    204   fn = saving_utils.trace_model_call(layer)
--> 205   fn.get_concrete_function()
    206   _restore_layer_losses(original_losses)
    207   return fn

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)
   1165       ValueError: if this object has not yet been called on concrete values.
   1166     """"""
-> 1167     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
   1168     concrete._garbage_collector.release()  # pylint: disable=protected-access
   1169     return concrete

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
   1071       if self._stateful_fn is None:
   1072         initializers = []
-> 1073         self._initialize(args, kwargs, add_initializers_to=initializers)
   1074         self._initialize_uninitialized_variables(initializers)
   1075 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    695     self._concrete_stateful_fn = (
    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 697             *args, **kwds))
    698 
    699     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-> 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3073             arg_names=arg_names,
   3074             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3075             capture_by_value=self._capture_by_value),
   3076         self._function_attributes,
   3077         function_spec=self.function_spec,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    599         # the function a weak reference to itself to avoid a reference cycle.
--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    601     weak_wrapped_fn = weakref.ref(wrapped_fn)
    602 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py in _wrapped_model(*args)
    132     with base_layer_utils.call_context().enter(
    133         model, inputs=inputs, build_graph=False, training=False, saving=True):
--> 134       outputs = model(inputs, training=False)
    135 
    136     # Outputs always has to be a flat dict.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    983 
    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):
--> 985           outputs = call_fn(inputs, *args, **kwargs)
    986 
    987         if self._activity_regularizer:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    300   def wrapper(*args, **kwargs):
    301     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
--> 302       return func(*args, **kwargs)
    303 
    304   if inspect.isfunction(func) or inspect.ismethod(func):

TypeError: call() missing 2 required positional arguments: 'features' and 'hidden'
```
"
43291,Didnt find op for builtin opcode 'RESIZE_NEAREST_NEIGHBOR' version '3',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung A51
- TensorFlow installed from (source or binary): Maven 
- TensorFlow version (use command below): implementation('org.tensorflow:tensorflow-lite:2.3.0'){changing=true}
- Python version: n/a
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a


**Describe the current behavior**

**.tflite generated** using tf.lite.TFLiteConverter.from_saved_model
tf_version = 2.3.0
Python Implementation for Inference works without errors.

**using the same model on Android for inference gives the error**- 
cant create interpreter : Didnt find op for builtin opcode 'RESIZE_NEAREST_NEIGHBOR' version '3'
build-gradle : implementation('org.tensorflow:tensorflow-lite:2.3.0'){changing=true}

**Describe the expected behavior**
the Android code should run.
"
43289,ERROR: Could not find a version that satisfies the requirement tensorflow==1.14.0,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina version 10.15.5, Windows 10 64bit
- TensorFlow installed from (source or binary): pip install tensorflow==1.14.0
- TensorFlow version: 1.14.0
- Python version: 3.7.* (64bit)
- Installed using virtualenv? pip? conda?: pip and conda
- pip version: I have used 19.1.1 and 20.2.3

**Describe the problem**
I have created virtual environments using Anaconda on both OS(mac and windows). Then I tried to install all the modules on that. but I have faced problems that install specific tensorflow version 1.14.0. My pip only supports those version below:
(from versions: 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0)

I also tried to install tensorflow using commands below:
conda install tensorflow==1.14.0

but it wasn't worked. my source can not import tensorflow module.

how can I install that version?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
pip install tensorflow==1.14.0
pip install tensorflow-gpu==1.14.0


**Any other info / logs**
above commands always occurs this error below:
ERROR: Could not find a version that satisfies the requirement tensorflow==1.14.0 (from versions: 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0)
ERROR: No matching distribution found for tensorflow==1.14.0"
43288,edgeTPU compiler error on TF2.3 quantized model,"Hi,
I trained a custom ssd model base on 'ssd_mobilenet_v2_320x320_coco17_tpu-8' in tensorflow 2.3 environment.

If I converted it to .tflite quantized model by TF2.3 / TF2.4 nightly environment, and edgeTPU compiler will show 
""Edge TPU Compiler version 14.1.317412892 
Invalid model: /content/ssd_mobilenet_v2_320x320_coco17_tpu-8_model_quant.tflite 
Model could not be parsed ERROR: Didn't find op for builtin opcode 'CONV_2D' version '5' ERROR: Registration failed.""     

If I converted it to .tflite quantized model by TF2.1 environment, and edgeTPU compiler will show 
""Edge TPU Compiler version 14.1.317412892
Invalid model: /content/ssd_mobilenet_v2_320x320_coco17_tpu-8_model_quant_new.tflite
Model could not be parsed
ERROR: Did not get operators, tensors, or buffers in subgraph 0.""     

#### System information
Linux kernel version:4.19.112+
Distributor ID:Ubuntu
Description:	Ubuntu 18.04.5 LTS
Release:	18.04
Codename:	bionic
Tensorflow python module version 2.3.0

Edge TPU python module version:2.14.1
Edge TPU Compiler version 14.1.317412892
Edge TPU runtime file:
ii  libedgetpu1-std:amd64                  14.1                                              amd64        Support library for Edge TPU
Edge TPU runtime version:
BuildLabel(COMPILER=5.4.0 20160609,DATE=redacted,TIME=redacted,CL_NUMBER=317268237), RuntimeVersion(13)

What is wrong with it actually? Please help me out~ Thank you!
"
43278,My doc build is picking up methods not in the advertised API,"## Description of issue (what needs changing):

This is a bit of a difficult issue to pin down. I have a class that extends `tf.Module`, and I'm trying to build the documentation for that class using sphinx (with a couple of extensions). When I build it, I'm seeing a large number of methods in my doc build that aren't advertised in the [`tf.Module` docs](https://www.tensorflow.org/api_docs/python/tf/Module). (These are mostly inherited from the superclasses of `tf.Module`: `Autotrackable` and `Trackable`, but that might not be too important here.)

I'd like to be able to show those parts of the `tf.Module` API that are intended for use by client code (including those with leading underscores that are intended for use in subclasses, i.e. 'protected' methods), and to omit those that aren't. It's not clear to me what parts of these tensorflow classes are indeed part of the public/protected API. It's also not clear to me, if the public API only includes what is shown on the website, how I can omit the rest from my doc build.

The simplest suggestion for me would be for tensorflow to remove the docstrings for all fully private functionality, but that's a lot of work, and I assume you use those docstrings.

You may be wondering why I'm raising this as an issue with tensorflow. The reason is that with the current structure of docstrings in tensorflow, I'm finding it difficult to manage my downstream doc build. Does the tensorflow team have a clear and consistent way of delineating fully private functionality, and if not, is there anything that can be done to improve docstring usability in this regard?"
43277, Failed to run on the given Interpreter: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.     Node number 62011 (FlexSize) failed to prepare.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip install tf-nightly
- TensorFlow version (or github SHA if from source):
2.4.0-dev20200916

**Provide the text output from tflite_convert**
For **Android Studio project** that gives the following error:

java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
    Node number 62011 (FlexSize) failed to prepare.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:163)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:360)
        at edu.ilab.covid_id.localize.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:202)
        at edu.ilab.covid_id.ir.ConnectFlirActivity$7.run(ConnectFlirActivity.java:673)
        at android.os.Handler.handleCallback(Handler.java:789)
        at android.os.Handler.dispatchMessage(Handler.java:98)
        at android.os.Looper.loop(Looper.java:164)
        at android.os.HandlerThread.run(HandlerThread.java:65)
```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.

**Here is the colab used to convert the saved model to tflite**
https://colab.research.google.com/drive/1_9l_DcyNuVV1NxU3PsmSMjWeI9nTEwNS?usp=sharing

**Here is the drive with the saved model to tflite**
https://drive.google.com/drive/folders/1MCvDti2ygukqw6fGE18WpnFOSzR_AanF?usp=sharing

**Here is the java code which uses the tflite model**
[ConnectFlirActivity.zip](https://github.com/tensorflow/tensorflow/files/5234536/ConnectFlirActivity.zip)

"
43276,Model trains on GPU but not on TPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Mostly stock
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Colab)
- TensorFlow installed from (source or binary): Pre-installed
- TensorFlow version (use command below): 2.3.0
- Python version: 3.x
- Accelerator: TPU

**Describe the current behavior**
I am receiving this error:-
```
InvalidArgumentError: 9 root error(s) found.
  (0) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50
	 [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape_1}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_11288723609953746016/_5]]
	 [[tpu_compile_succeeded_assert/_11288723609953746016/_5/_233]]
  (1) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50
	 [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape_1}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_11288723609953746016/_5]]
	 [[tpu_compile_succeeded_assert/_11288723609953746016/_5/_209]]
  (2) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50
	 [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape_1}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_11288723609953746016/_5]]
	 [[tpu_compile_succeeded_assert/_11288723609953746016/_5/_173]]
  (3) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50
	 [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape_1}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_11288723609953746016/_5]]
	 [[tpu_compile_succeeded_assert/_11288723609953746016/_5/_185]]
  (4) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50
	 [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape_1}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_11288723609953746016/_5]]
	 [[tpu_compile_succeeded_assert/_11288723609953746016/_5/_197]]
  (5) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50
	 [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape_1}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_11288723609953746016/_5]]
	 [[tpu_compile_succeeded_assert/_11288723609953746016/_5/_245]]
  (6) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50
	 [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape_1}}]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_11288723609953746016/_5]]
	 [[tpu_compile_succeeded_assert/_11288723609953746016/_5/_257]]
  (7) Invalid argument: {{function_node __inference_train_function_33888}} Compilation failure: Input to reshape is a tensor with 100 values, but the requested shape has 50
	 [[{{node gradient_tape/sequential_4/embedding_4/embed ... [truncated]
```
Which indicates that there is probably some error with the shapes of my model.

**Describe the expected behavior**
Expected the model to run, as it runs pretty well for considerable time on GPU but ALWAYS results in that error when using TPU. This means that the Model should train and run (since no error is received on GPU and does the training of the first Epoch)

**Standalone code to reproduce the issue**
Here is the `model.fit` block to initiate the training:-
```
# Directory where the checkpoints will be saved
checkpoint_dir = '/content/drive/My Drive/HashPro/checkpoints/'

checkpoint_prefix = os.path.join(checkpoint_dir, ""ckpt_{epoch}"")

checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_prefix)

EPOCHS = 50

history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback], steps_per_epoch=300)  # Comment to evaluate the model
```
Here is how my model looks like:-
```

Model: ""sequential_4""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_4 (Embedding)      (1, None, 8800)           158400    
_________________________________________________________________
dense_42 (Dense)             (1, None, 2500)           22002500  
_________________________________________________________________
dropout_23 (Dropout)         (1, None, 2500)           0         
_________________________________________________________________
dense_43 (Dense)             (1, None, 3500)           8753500   
_________________________________________________________________
dense_44 (Dense)             (1, None, 5500)           19255500  
_________________________________________________________________
dropout_24 (Dropout)         (1, None, 5500)           0         
_________________________________________________________________
dense_45 (Dense)             (1, None, 7500)           41257500  
_________________________________________________________________
dense_46 (Dense)             (1, None, 9500)           71259500  
_________________________________________________________________
batch_normalization_8 (Batch (1, None, 9500)           38000     
_________________________________________________________________
dropout_25 (Dropout)         (1, None, 9500)           0         
_________________________________________________________________
dense_47 (Dense)             (1, None, 7000)           66507000  
_________________________________________________________________
dense_48 (Dense)             (1, None, 7000)           49007000  
_________________________________________________________________
dropout_26 (Dropout)         (1, None, 7000)           0         
_________________________________________________________________
dense_49 (Dense)             (1, None, 1500)           10501500  
_________________________________________________________________
dense_50 (Dense)             (1, None, 500)            750500    
_________________________________________________________________
dropout_27 (Dropout)         (1, None, 500)            0         
_________________________________________________________________
activation_4 (Activation)    (1, None, 500)            0         
_________________________________________________________________
batch_normalization_9 (Batch (1, None, 500)            2000      
=================================================================
Total params: 289,492,900
Trainable params: 289,472,900
Non-trainable params: 20,000

```
Model block for building and applying TPU strategy to run it:-
```
def build_model(vocab_size, embedding_dim, mid_units, batch_size):
  model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),
  tf.keras.layers.Dense(2500, activation='relu'),
  tf.keras.layers.Dropout(0.15),
  tf.keras.layers.Dense(3500, activation='relu'),
  tf.keras.layers.Dense(5500, activation='relu'),
  tf.keras.layers.Dropout(0.15),

  tf.keras.layers.Dense(7500, activation='relu'),
  tf.keras.layers.Dense(9500, activation='relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.Dropout(0.15),
  tf.keras.layers.Dense(mid_units, activation='relu'),
  tf.keras.layers.Dense(mid_units, activation='relu'),
  tf.keras.layers.Dropout(0.15),

  tf.keras.layers.Dense(1500, activation='relu'),
  tf.keras.layers.Dense(500, activation='relu'),
  tf.keras.layers.Dropout(0.15),
  
  tf.keras.layers.Activation('softmax'),
  tf.keras.layers.BatchNormalization()
])
  return model

def loss(labels, logits):
  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

with strategy.scope():
  model = build_model(
    vocab_size = len(vocab),
    embedding_dim=embedding_dim,
    mid_units=7000,
    batch_size=BATCH_SIZE)

  model.compile(optimizer='Adam', loss=loss)

#Let's see the model's organs!
model.summary()
```
TPU strategy and initialization code taken from the TensorFlow website and does not produce any error at all.
> It seems to me that there might be a bug here, since if the model has some shape-related issue, then training on GPU would not have worked at all. But since it works, there might be a problem in that way I am using the TPU

If you want any more info. please comment below"
43275,TensorFlow Lite (2.3.0) Converter Quantization Incompatibility for Coral Edge TPU Compiler,"**System information**
- OS Platform and Distribution: Linux Ubuntu 20.04 LTS with Docker
- TensorFlow installed from:  pulled TensorFlow Docker image (version 19.03.12)
- TensorFlow version (or github SHA if from source): v2.3.0 CPU
- Coral Edge TPU Compiler: version 14.1.317412892


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf 
import numpy as np
from tensorflow import keras

keras_model = tf.keras.models.load_model(""keras_model.h5"")
keras_model.input.set_shape((1,)+keras_model.input.shape[1:])
keras_model.compile(optimizer=""Adam"", loss=""mse"", metrics=[""mae""])
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
#input_arrays=[""x""], output_arrays=[""Identity""],input_shapes={'x':[1,32,32,1]})


def representative_dataset_gen():
  for _ in range(10):
    input_array = np.random.random((1,32,32,1))
    input_array = np.array(input_array,dtype=np.float32)
    yield [input_array]

converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8  # or tf.uint8
converter.inference_output_type = tf.uint8  # or tf.uint8
tflite_model = converter.convert()

with open('keras_model_2-3-0.tflite', 'wb') as f:
  f.write(tflite_model)
```
Note: I am using garbage random data to do the post-training quantization.

**The output from the converter invocation**

```
2020-09-16 18:08:33.821326: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-09-16 18:08:33.825281: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2020-09-16 18:08:38.269950: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-09-16 18:08:38.270738: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-09-16 18:08:38.271532: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (e386413cb5cf): /proc/driver/nvidia/version does not exist
2020-09-16 18:08:38.278651: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-16 18:08:38.337624: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1799995000 Hz
2020-09-16 18:08:38.345643: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4714550 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-16 18:08:38.345866: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
2020-09-16 18:09:01.424650: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
2020-09-16 18:09:28.840811: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-09-16 18:09:28.847221: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-09-16 18:09:28.910112: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-09-16 18:09:28.910190: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 1.162ms.
2020-09-16 18:09:28.910203: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.004ms.
2020-09-16 18:09:31.373431: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-09-16 18:09:31.373507: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.

Attached is the Keras model and tflite model below:
[models.zip](https://github.com/tensorflow/tensorflow/files/5234095/models.zip)

```

**Failure details**
I am attempting to use this post-training quantized TensorFlow Lite model for input to the Coral Edge TPU compiler such that it can run on the Edge TPU.  The Edge TPU compiler fails to compile the quantized TensorFlow Lite model.  I reached to the Coral Edge TPU team to examine why the the TensorFlow Lite model will not compile.  According to their team, ""The compiler rejects the model on purpose because there are some mis matching in quantization parameter which could cause bad prediction:

![image](https://user-images.githubusercontent.com/19961323/93378124-fa01f000-f829-11ea-8fd8-2a88204dd4ac.png)

The are 2 quantized op going to that same Concat layer where one has this:
`scale: 0.048531219363212585 zero_point: 103 num_fxp_values: 256`
and the other one:

`scale: 0.033884394913911819 zero_point: 120 num_fxp_values: 256`
This seems to me like a conversion issue, I suggest reaching out to the tensorflow team for a more appropriate solution.""

Why are there 2 quantized operations with different quantization parameters going to the same concatenation layer?  Is this an error with the TensorFlow Lite converter or an error with the Edge TPU compiler?

"
43273,FP16 not working with NHNet,"I am able to run NHNet code with batch size 8 but the accuracy numbers are no way near to what are reported in the paper. So, I thought increasing the batch size might help. But due to GPU limitation, I am not able to do it. So I tried using Mixed Precision to train the model for larger batch. I tried the following 2 things:

1.  Using `tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt,""dynamic"")`
But this gives the following error.
`AttributeError: 'LossScaleOptimizer' object has no attribute '_hypers_created'`

2.I also tried using:  `tf.keras.mixed_precision.experimental.set_policy('mixed_float16')`
But that gives this error:
`TypeError: Tensors in list passed to 'inputs' of 'Einsum' Op have types [float16, float32] that don't all match.`

**System information**
- Used exact same code from NHNet repo
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- TensorFlow installed from (source or binary): tf-nightly==2.4.0.dev20200724
- TensorFlow version (use command below): v1.12.1-31004-g203aa8b634 2.2.0-dev20200501
- Python version: 3.7.7
- CUDA/cuDNN version: 10.1/7
- GPU model and memory: TITAN RTX 24GB


**Describe the current behavior**

As soon as epoch 1 starts the code throws error as mentioned before.

**Describe the expected behavior**

The training should happen.

**Standalone code to reproduce the issue**

Exact same code mentioned here: https://github.com/tensorflow/models/blob/master/official/nlp/nhnet/trainer.py

Just add one line after line no 146:
`opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt,""dynamic"")`
"
43272,Can't Access Tensorflow Dependencies After Install,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary):  pip install tensorflow
- TensorFlow version:
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1/cudnn-10.1-windows10-x64-v7.6.5.32
- GPU model and memory: nvidia rtx 2060 



**Describe the problem**
Tensorflow not recognizing cudnn is installed. I have verified the files exist in the directories that I have added to my path variable, still says the file doesn't exist.
![image](https://user-images.githubusercontent.com/21284577/93372548-a4bde280-f819-11ea-98aa-ea549bdd06e3.png)
![image](https://user-images.githubusercontent.com/21284577/93372639-c5863800-f819-11ea-8a0c-3ac1c420910c.png)

**Provide the exact sequence of commands / steps that you executed before running into the problem**
 from keras.models import Sequential

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


from keras.models import Sequential
Using TensorFlow backend.
Traceback (most recent call last):

  File ""C:\Users\Luke\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\platform\self_check.py"", line 87, in preload_check
    ctypes.WinDLL(build_info.cudnn_dll_name)

  File ""C:\Users\Luke\anaconda3\lib\ctypes\__init__.py"", line 364, in __init__
    self._handle = _dlopen(self._name, mode)

OSError: [WinError 126] The specified module could not be found


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""<ipython-input-1-9c5e0a19b646>"", line 1, in <module>
    from keras.models import Sequential

  File ""C:\Users\Luke\anaconda3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils

  File ""C:\Users\Luke\anaconda3\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils

  File ""C:\Users\Luke\anaconda3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K

  File ""C:\Users\Luke\anaconda3\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon

  File ""C:\Users\Luke\anaconda3\lib\site-packages\keras\backend\load_backend.py"", line 90, in <module>
    from .tensorflow_backend import *

  File ""C:\Users\Luke\anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf

  File ""C:\Users\Luke\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""C:\Users\Luke\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\Luke\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()

  File ""C:\Users\Luke\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\platform\self_check.py"", line 97, in preload_check
    % (build_info.cudnn_dll_name, build_info.cudnn_version_number))

ImportError: Could not find 'cudnn64_7.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7 from this URL: https://developer.nvidia.com/cudnn"
43271,TFLite converter aborts (dumps core) in BroadcastAdd4DSlow,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Pop! OS 20.04 LTS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.3.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.
https://colab.research.google.com/drive/1rfdr8bu_UfTzpGCXH-tQFVHMIKDreXad?usp=sharing

When running on my Pop! OS system under gdb, I get the backtrace:
```
gdb python
[...]
(gdb) run tflite.py
[...]
(gdb) where
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
#1  0x00007ffff7ddb859 in __GI_abort () at abort.c:79
#2  0x00007ffbf82ccb1f in tflite::reference_ops::BroadcastAdd4DSlow(tflite::ArithmeticParams const&, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*) ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#3  0x00007ffbf82d54ec in void tflite::ops::builtin::add::EvalAdd<(tflite::ops::builtin::add::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteAddParams*, tflite::ops::builtin::add::OpData const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*) ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#4  0x00007ffbf82d846d in TfLiteStatus tflite::ops::builtin::add::Eval<(tflite::ops::builtin::add::KernelType)2>(TfLiteContext*, TfLiteNode*) () from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#5  0x00007ffbf82ab1bf in tflite::optimize::calibration::(anonymous namespace)::LoggingEval(TfLiteContext*, TfLiteNode*) ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#6  0x00007ffbf850f20b in tflite::impl::Subgraph::Invoke() ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#7  0x00007ffbf85121c0 in tflite::impl::Interpreter::Invoke() ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#8  0x00007ffbf827a581 in tflite::calibration_wrapper::CalibrationWrapper::FeedTensor(_object*) ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#9  0x00007ffbf8274285 in pybind11::cpp_function::initialize<pybind11_init__pywrap_tensorflow_lite_calibration_wrapper(pybind11::module&)::{lambda(tflite::calibration_wrapper::CalibrationWrapper&, pybind11::handle&)#4}, pybind11::object, tflite::calibration_wrapper::CalibrationWrapper&, pybind11::handle&, pybind11::name, pybind11::is_method, pybind11::sibling>(pybind11_init__pywrap_tensorflow_lite_calibration_wrapper(pybind11::module&)::{lambda(tflite::calibration_wrapper::CalibrationWrapper&, pybind11::handle&)#4}&&, pybind11::object (*)(tflite::calibration_wrapper::CalibrationWrapper&, pybind11::handle&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call) ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#10 0x00007ffbf8271fe7 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#11 0x00000000005f17e5 in PyCFunction_Call ()
#12 0x00000000005f2406 in _PyObject_MakeTpCall ()
#13 0x000000000050795f in ?? ()
#14 0x000000000056c475 in _PyEval_EvalFrameDefault ()
#15 0x0000000000565972 in _PyEval_EvalCodeWithName ()
#16 0x00000000005f1d85 in _PyFunction_Vectorcall ()
#17 0x00000000005677c7 in _PyEval_EvalFrameDefault ()
#18 0x0000000000565972 in _PyEval_EvalCodeWithName ()
#19 0x00000000005f1d85 in _PyFunction_Vectorcall ()
#20 0x0000000000507729 in ?? ()
#21 0x00000000005f1107 in PyObject_Call ()
#22 0x0000000000568e1f in _PyEval_EvalFrameDefault ()
#23 0x000000000050712e in ?? ()
#24 0x000000000056c475 in _PyEval_EvalFrameDefault ()
#25 0x0000000000565972 in _PyEval_EvalCodeWithName ()
#26 0x00000000005f1d85 in _PyFunction_Vectorcall ()
--Type <RET> for more, q to quit, c to continue without paging--
#27 0x00000000005677c7 in _PyEval_EvalFrameDefault ()
#28 0x0000000000565972 in _PyEval_EvalCodeWithName ()
#29 0x0000000000686053 in PyEval_EvalCode ()
#30 0x00000000006753d1 in ?? ()
#31 0x000000000067544f in ?? ()
#32 0x0000000000675507 in PyRun_FileExFlags ()
#33 0x000000000067758a in PyRun_SimpleFileExFlags ()
#34 0x00000000006ae99e in Py_RunMain ()
#35 0x00000000006aed29 in Py_BytesMain ()
#36 0x00007ffff7ddd0b3 in __libc_start_main (main=0x4ebd20 <main>, argc=2, argv=0x7fffffffe2c8, init=<optimized out>, 
    fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffe2b8) at ../csu/libc-start.c:308
#37 0x00000000005f62ee in _start ()
```
"
43270,[RNN]Cannot get saved .tflite model,"**System information**
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: tf-nightly 2.2.0

**Command used to run the converter or code if you’re using the Python API**

[convert_pb2_tflite.txt](https://github.com/tensorflow/tensorflow/files/5233664/convert_pb2_tflite.txt)

**The output  log from the converter invocation**
[log.txt](https://github.com/tensorflow/tensorflow/files/5233693/log.txt)

**Failure details**
My .pb model is trained and saved in tensorflow 1.15.0 and I want to convert it to .tflite to deploy on CPU for better performance. When I run the uploaded file convert_pb2_tflite.txt(.py actually), the log attached above seems to include no warning and errors. But there is no tflite file in my save path. What's probably going wrong? 
Thank you.

"
