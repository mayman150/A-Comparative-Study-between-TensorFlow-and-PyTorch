Issue Number,Issue Title,Issue Body
32685,tf.function problem when slicing tensor with variable,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: binary
- TensorFlow version: v2.0.0-rc0-101-gd2d2566eef 2.0.0-rc1
- Python version: 3.7.4
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the behavior**
Slicing tensors using slices indexed by a `tf.Variable` does not work with `tf.function`. The problem does not occur when executing eagerly or when slicing with tensors which are not variables.

**Code to reproduce the issue**
Executing the code

	import tensorflow as tf

	pos = tf.Variable(0, dtype=tf.int32)

	def ok():
		return tf.zeros(5)[pos:3]

	@tf.function
	def also_ok():
		return tf.zeros(5)[pos+0:3]

	@tf.function
	def not_ok():
		return tf.zeros(5)[pos:3]

	tf.print(ok())
	tf.print(also_ok())
	tf.print(not_ok())	

produces the output

	[0 0 0]
	[0 0 0]
	StagingError

**Detailed traceback**

	---------------------------------------------------------------------------
	StagingError                              Traceback (most recent call last)
	<ipython-input-29-6470bfb94cb0> in <module>
		 12 
		 13 tf.print(ok())
	---> 14 tf.print(not_ok())

	~\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\eager\def_function.py in __call__(self, *args, **kwds)
		455 
		456     tracing_count = self._get_tracing_count()
	--> 457     result = self._call(*args, **kwds)
		458     if tracing_count == self._get_tracing_count():
		459       self._call_counter.called_without_tracing()

	~\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\eager\def_function.py in _call(self, *args, **kwds)
		501       # This is the first call of __call__, so we have to initialize.
		502       initializer_map = object_identity.ObjectIdentityDictionary()
	--> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)
		504     finally:
		505       # At this point we know that the initialization is complete (or less

	~\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\eager\def_function.py in _initialize(self, args, kwds, add_initializers_to)
		406     self._concrete_stateful_fn = (
		407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
	--> 408             *args, **kwds))
		409 
		410     def invalid_creator_scope(*unused_args, **unused_kwds):

	~\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\eager\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
	   1846     if self.input_signature:
	   1847       args, kwargs = None, None
	-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)
	   1849     return graph_function
	   1850 

	~\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\eager\function.py in _maybe_define_function(self, args, kwargs)
	   2148         graph_function = self._function_cache.primary.get(cache_key, None)
	   2149         if graph_function is None:
	-> 2150           graph_function = self._create_graph_function(args, kwargs)
	   2151           self._function_cache.primary[cache_key] = graph_function
	   2152         return graph_function, args, kwargs

	~\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\eager\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
	   2039             arg_names=arg_names,
	   2040             override_flat_arg_shapes=override_flat_arg_shapes,
	-> 2041             capture_by_value=self._capture_by_value),
	   2042         self._function_attributes,
	   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of

	~\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
		913                                           converted_func)
		914 
	--> 915       func_outputs = python_func(*func_args, **func_kwargs)
		916 
		917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

	~\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\eager\def_function.py in wrapped_fn(*args, **kwds)
		356         # __wrapped__ allows AutoGraph to swap in a converted function. We give
		357         # the function a weak reference to itself to avoid a reference cycle.
	--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)
		359     weak_wrapped_fn = weakref.ref(wrapped_fn)
		360 

	~\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\framework\func_graph.py in wrapper(*args, **kwargs)
		903           except Exception as e:  # pylint:disable=broad-except
		904             if hasattr(e, ""ag_error_metadata""):
	--> 905               raise e.ag_error_metadata.to_exception(e)
		906             else:
		907               raise

	StagingError: in converted code:

		<ipython-input-20-6173bf43c1fe>:11 not_ok  *
			return tf.zeros(5)[pos:3]
		C:\Users\Daniel\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\ops\array_ops.py:748 _slice_helper
			s.start != sys.maxsize):
		C:\Users\Daniel\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\ops\variables.py:1111 __ne__
			return gen_math_ops.not_equal(self, other)
		C:\Users\Daniel\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\ops\gen_math_ops.py:7012 not_equal
			name=name)
		C:\Users\Daniel\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\framework\op_def_library.py:527 _apply_op_helper
			preferred_dtype=default_dtype)
		C:\Users\Daniel\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\framework\ops.py:1296 internal_convert_to_tensor
			ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
		C:\Users\Daniel\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\framework\tensor_conversion_registry.py:52 _default_conversion_function
			return constant_op.constant(value, dtype, name=name)
		C:\Users\Daniel\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\framework\constant_op.py:227 constant
			allow_broadcast=True)
		C:\Users\Daniel\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\framework\constant_op.py:265 _constant_impl
			allow_broadcast=allow_broadcast))
		C:\Users\Daniel\.conda\envs\tf2\lib\site-packages\tensorflow_core\python\framework\tensor_util.py:450 make_tensor_proto
			nparray = np.array(values, dtype=np_dt)

		OverflowError: Python int too large to convert to C long
"
32684,Arguments formatting error in 1.15 keras.model.save docs + incorrect model format info/docstring mismatch?,"## URL(s) with the issue: https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/Model#save

## Description of issue (what needs changing):
[1.15 branch]
It appears that the docstring of keras.model.save does not match the docs in 1.15.

Content is different I think.

The bullet points for the input arguments for the save method are not formatted making it hard to read (although this may be poor formatting on the current source for the documentation as the docstring appears to be correctly formatted.).

A side effect of this is that it also appears that the docs conflicts with the release notes that say the default format is as a `Tensorflow SavedModel ('tf')` however the docs say that the `tf` option is disabled implying that only `.h5` formats can be saved which is contradictory. An update of the docs from the seemingly correct docstring may fix this.

### Correct links

The source code link appears to be correct despite the docstring not matching that of the website docs.

### Parameters defined

Formatting issue and also not updated.

### Returns defined

Depends on model format.

### Raises listed and defined

Depends on model format.

### Submit a pull request?

I don't quite understand why this has happened so no.
"
32682,tf.keras.layers.Conv2D expected axis -1 of input shape to have value 3.,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.14
- Python version: 3.6
- CUDA/cuDNN version: 10.0
- GPU model and memory: 1080TI

**Describe the current behavior**

Here is my code.

```
x = tf.placeholder(tf.float32, [None, 3072])
y = tf.placeholder(tf.int64, [None])

x_img = tf.reshape(x, [-1, 3, 32, 32])
x_img = tf.transpose(x_img, perm=[0, 2, 3, 1])

# Layers Class
convLayer_32_3_3 = tf.keras.layers.Conv2D(filters=32,
                                 kernel_size=(3, 3), 
                                 strides=(1,1),
                                 padding='same',
                                 data_format='channels_last',
                                 activation=tf.nn.relu, name='convLayer_32_3_3')
maxPoolLayer_2_2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), 
                                     strides=(2, 2),
                                     data_format='channels_last')

# conv1: feature_map
conv1_1 = convLayer_32_3_3(x_img)
conv1_2 = convLayer_32_3_3(conv1_1)
```

**Error**

```
<ipython-input-8-c377e720039b> in <module>
     18 # conv1: feature_map
     19 conv1_1 = convLayer_32_3_3(x_img)
---> 20 conv1_2 = convLayer_32_3_3(conv1_1)
     21 
     22 # shape: [16, 16]

~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    584         # the corresponding TF subgraph inside `backend.get_graph()`
    585         input_spec.assert_input_compatibility(self.input_spec, inputs,
--> 586                                               self.name)
    587         graph = backend.get_graph()
    588         with graph.as_default(), backend.name_scope(self._name_scope()):

~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_spec.py in assert_input_compatibility(input_spec, inputs, layer_name)
    157                 ' incompatible with the layer: expected axis ' + str(axis) +
    158                 ' of input shape to have value ' + str(value) +
--> 159                 ' but received input with shape ' + str(shape))
    160     # Check shape.
    161     if spec.shape is not None:

ValueError: Input 0 of layer convLayer_32_3_1 is incompatible with the layer: expected axis -1 of input shape to have value 3 but received input with shape [None, 32, 32, 32]
```

**But when I change the code another style:**

```
x = tf.placeholder(tf.float32, [None, 3072])
y = tf.placeholder(tf.int64, [None])

x_img = tf.reshape(x, [-1, 3, 32, 32])
x_img = tf.transpose(x_img, perm=[0, 2, 3, 1])

# Layers Class
convLayer_32_3_3 = tf.keras.layers.Conv2D(filters=32,
                                 kernel_size=(3, 3), 
                                 strides=(1,1),
                                 padding='same',
                                 data_format='channels_last',
                                 activation=tf.nn.relu, name='convLayer_32_3_3')
maxPoolLayer_2_2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), 
                                     strides=(2, 2),
                                     data_format='channels_last')

# conv1: feature_map
conv1_1 = tf.keras.layers.Conv2D(filters=32,
                                 kernel_size=(3, 3), 
                                 strides=(1,1),
                                 padding='same',
                                 data_format='channels_last',
                                 activation=tf.nn.relu, name='convLayer_32_3_1')(x_img)
conv1_2 = tf.keras.layers.Conv2D(filters=32,
                                 kernel_size=(3, 3), 
                                 strides=(1,1),
                                 padding='same',
                                 data_format='channels_last',
                                 activation=tf.nn.relu, name='convLayer_32_3_2')(conv1_1)

```

**The code is ok.**

So who can tell me why? I don't know why can't i create a class to use this function but it is ok when I change it as same as new instance."
32681,add a custom op in user_ops rebuild tensorflow but no use,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
tensorflow version:1.5.1
plateform:centos cpu
install: source and pip
I have add my op in `tensorflow/core/user_ops/`  zero_out.cc with context:
`#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/op_kernel.h""

using namespace tensorflow;

REGISTER_OP(""ZeroOut"")
    .Input(""to_zero: int32"")
    .Output(""zeroed: int32"")

class ZeroOutOp : public OpKernel {
 public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}
  void Compute(OpKernelContext* context) override {
    const Tensor& input_tensor = context->input(0);
    auto input = input_tensor.flat<int32>();
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));
    auto output = output_tensor->template flat<int32>();
    const int N = input.size();
    for (int i = 1; i < N; i++) {
      output(i) = 0;
    }
    if (N > 0) output(0) = input(0);
  }
};

REGISTER_KERNEL_BUILDER(Name(""ZeroOut"").Device(DEVICE_CPU), ZeroOutOp);
`

and add a BUILD file 
`load(""//tensorflow:tensorflow.bzl"", ""tf_custom_op_library"")
tf_custom_op_library(
        name = ""zero_out.so"",
        srcs = [""zero_out.cc""],
)
`
then,  add
 `# user_ops
Fact
ZeroOut
`  
after Fact in `tensorflow/python/ops/hidden_ops.txt`

add `@tf_export(v1=['user_ops.my_zero_out'])
def leslie_zero_out(input):
  """"""Example of overriding the generated code for an Op.""""""
  return _gen_user_ops._zero_out(input)
` in tensorflow/python/user_ops/user_ops.py
and rebuild tensorflow wtih `bazel build -c opt --copt=-march=native --copt=-mfpmath=both  //tensorflow/tools/pip_package:build_pip_package`, and it complite corrected, but when I installed the *.whl, I cannot find the ""zero_out"" op, I donot know whatis wrong with me?
anyone can tell how to add an op in tensorflow by source?"
32680,autograph=False not respected when calling get_concrete_function(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): reproduced on Ubuntu 18.04 and MacOS 10.14
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0  v1.14.0-rc1-22-gaf24dc91b5
- Python version: 2.7.16

**Describe the current behavior**
When passing in `tf.function(..., autograph=False)`, console output indicates that `autograph` still attempts to autograph the function.

**Describe the expected behavior**
That the function is not going to be autographed.

**Code to reproduce the issue**
This encountered when passing in a function that `autograph` fails on. For example, a python function that is actually a wrapper around an R function.

This example is in R:
```R

> library(reticulate)
> library(tensorflow)
> 
> Sys.setenv(AUTOGRAPH_VERBOSITY=10)
> reticulate::use_virtualenv(""~/tf1"")
> 
> x <- tf$constant(1)
> add1 <- py_func(function() x + 1)
> 
> fn <- tf$`function`(add1, autograph = FALSE)
> 
> fn$get_concrete_function()
ERROR:tensorflow:Error converting <function initialize_variables at 0x134c82140>
Traceback (most recent call last):
  File ""/Users/tomasz/tf1/lib/python2.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 524, in to_graph
    return conversion.convert(entity, program_ctx)
  File ""/Users/tomasz/tf1/lib/python2.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 306, in convert
    entity, program_ctx, free_nonglobal_var_names)
  File ""/Users/tomasz/tf1/lib/python2.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 229, in _convert_with_cache
    entity, program_ctx)
  File ""/Users/tomasz/tf1/lib/python2.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 431, in convert_entity_to_ast
    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)
  File ""/Users/tomasz/tf1/lib/python2.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 624, in convert_func_to_ast
    node = node_to_graph(node, context)
  File ""/Users/tomasz/tf1/lib/python2.7/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 657, in node_to_graph
    node = converter.standard_analysis(node, context, is_initial=True)
  File ""/Users/tomasz/tf1/lib/python2.7/site-packages/tensorflow/python/autograph/core/converter.py"", line 354, in standard_analysis
    node = qual_names.resolve(node)
  File ""/Users/tomasz/tf1/lib/python2.7/site-packages/tensorflow/python/autograph/pyct/qual_names.py"", line 254, in resolve
    return QnResolver().visit(node)
  File ""/usr/local/opt/python@2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ast.py"", line 241, in visit
    return visitor(node)
  File ""/usr/local/opt/python@2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ast.py"", line 297, in generic_visit
    value = self.visit(value)
  File ""/usr/local/opt/python@2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ast.py"", line 241, in visit
    return visitor(node)
  File ""/usr/local/opt/python@2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ast.py"", line 297, in generic_visit
    value = self.visit(value)
  File ""/usr/local/opt/python@2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ast.py"", line 241, in visit
    return visitor(node)
  File ""/usr/local/opt/python@2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ast.py"", line 306, in generic_visit
    new_node = self.visit(old_value)
  File ""/usr/local/opt/python@2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ast.py"", line 241, in visit
    return visitor(node)
  File ""/usr/local/opt/python@2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ast.py"", line 297, in generic_visit
    value = self.visit(value)
  File ""/usr/local/opt/python@2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ast.py"", line 241, in visit
    return visitor(node)
  File ""/Users/tomasz/tf1/lib/python2.7/site-packages/tensorflow/python/autograph/pyct/qual_names.py"", line 236, in visit_Subscript
    if isinstance(s.value, gast.Num):
AttributeError: 'module' object has no attribute 'Num'
WARNING:tensorflow:Entity <function initialize_variables at 0x134c82140> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function initialize_variables at 0x134c82140>: AttributeError: 'module' object has no attribute 'Num'
<tensorflow.python.eager.function.ConcreteFunction>
```"
32679,MobileNet_V3: will you implement it in tensorflow? ,
32678,tensorflow 2.0.0b0 ImportError: cannot import name 'Layer',"When I import the tensorflow, it has show the following errors :
(tensorflow 2.0.0b0)
> import tensorflow as tf
> Traceback (most recent call last):
> 
>   File ""<ipython-input-12-64156d691fe5>"", line 1, in <module>
>     import tensorflow as tf
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 40, in <module>
>     from tensorflow.python.tools import module_util as _module_util
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 83, in <module>
>     from tensorflow.python import keras
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\__init__.py"", line 27, in <module>
>     from tensorflow.python.keras import applications
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\applications\__init__.py"", line 25, in <module>
>     from tensorflow.python.keras import engine
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\__init__.py"", line 23, in <module>
>     from tensorflow.python.keras.engine.base_layer import Layer
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 50, in <module>
>     from tensorflow.python.keras.saving import saved_model
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\saving\__init__.py"", line 20, in <module>
>     from tensorflow.python.keras.saving.hdf5_format import load_attributes_from_hdf5_group
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\saving\hdf5_format.py"", line 32, in <module>
>     from tensorflow.python.keras.utils import conv_utils
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\utils\__init__.py"", line 38, in <module>
>     from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\utils\multi_gpu_utils.py"", line 22, in <module>
>     from tensorflow.python.keras.engine.training import Model
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 39, in <module>
>     from tensorflow.python.keras import metrics as metrics_module
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\metrics.py"", line 33, in <module>
>     from tensorflow.python.keras.engine.base_layer import Layer
> 
> ImportError: cannot import name 'Layer'
> 
> 
> 
> 
> import tensorflow as tf
> Traceback (most recent call last):
> 
>   File ""<ipython-input-13-64156d691fe5>"", line 1, in <module>
>     import tensorflow as tf
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 40, in <module>
>     from tensorflow.python.tools import module_util as _module_util
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 83, in <module>
>     from tensorflow.python import keras
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\__init__.py"", line 27, in <module>
>     from tensorflow.python.keras import applications
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\applications\__init__.py"", line 25, in <module>
>     from tensorflow.python.keras import engine
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\__init__.py"", line 23, in <module>
>     from tensorflow.python.keras.engine.base_layer import Layer
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 50, in <module>
>     from tensorflow.python.keras.saving import saved_model
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\saving\__init__.py"", line 20, in <module>
>     from tensorflow.python.keras.saving.hdf5_format import load_attributes_from_hdf5_group
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\saving\hdf5_format.py"", line 32, in <module>
>     from tensorflow.python.keras.utils import conv_utils
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\utils\__init__.py"", line 38, in <module>
>     from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\utils\multi_gpu_utils.py"", line 22, in <module>
>     from tensorflow.python.keras.engine.training import Model
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 39, in <module>
>     from tensorflow.python.keras import metrics as metrics_module
> 
>   File ""D:\Users\1\Anaconda3\lib\site-packages\tensorflow\python\keras\metrics.py"", line 33, in <module>
>     from tensorflow.python.keras.engine.base_layer import Layer
> 
> ImportError: cannot import name 'Layer'
How to deal with it ? Should I pip the tensorflow 2.0 agnain ?"
32677,gcc: error: unrecognized command line option '-std=c++14',"I'm trying to build on CentOS, but it fails:
```
INFO: Analyzed 4 targets (305 packages loaded, 27773 targets configured).
INFO: Found 4 targets...
INFO: Deleting stale sandbox base /home/xx/.cache/bazel/_bazel_xx/aee6fd7a0bb70a6e710bf5cf6db40de3/sandbox
ERROR: /home/xx/tensorflow-build/tensorflow/core/BUILD:386:1: C++ compilation of rule '//tensorflow/core:util_port' failed (Exit 1)
gcc: error: unrecognized command line option '-std=c++14'
INFO: Elapsed time: 13.712s, Critical Path: 0.21s
INFO: 17 processes: 17 local.
FAILED: Build did NOT complete successfully
```

Why is the C++ option ```-std=c++14``` passed to gcc?

rev.e627965
gcc-9.2.0
CentOS 7.6.1810"
32675,Equivalent of Torch EmbeddingBag,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**
Currently Tensorflow has only tf.nn.embedding_lookup (which, I believe in the backend uses a tf.gather operation). Torch has a torch.nn.EmbeddingBag class which gathers and combines embeddings in specified ways. It allows for a weighted summation of embeddings which they say is much more efficient than getting the embeddings, multiplying (broadcasting) a set of weights across them, and then reduce-summing them. 

I am trying to implement product-key memory layers (https://arxiv.org/pdf/1907.05242.pdf) in tensorflow but they are running very slowly for me - roughly 50% slower than an equivalent BERT-Base sized model without product-key memory layers. Looking at the debugging results, I find that the operations of tf.nn.embedding_lookup, the broadcasted multiplication between scores and values, and the tf.math.reduce_sum operation are taking up a lot of time. A more efficient implementation that performs these operations in one go should reduce the time it takes to run these layers. All product-key memory layers which have a weighted summation of some memory values should benefit from this optimization. 

For a concrete example pseudocode in tensorflow I have to do these operations:

query = tf.Tensor()
key = tf.Tensor()
values = tf.get_variable()
scores, indices = get_scores_and_indices(query,key)
attended_values = tf.nn.embedding_lookup(values,indices)
output = tf.math.reduce_sum(scores*attended_values)  

On torch, you could do
query = torch.Tensor()
key = torch.Tensor()
values = torch.nn.EmbeddingBag()
scores, indices = get_scores_and_indices(query,key)
output = values(indices, per_sample_weights=scores)

Torch claims that using an EmbeddingBag is much more efficient than using a simple Embedding+weighted sum operation. And this does indeed appear to be the case. 
 
**Will this change the current api? How?**
It will add new functionality in a tf.nn.embedding_bag_lookup() method

**Who will benefit with this feature?**
Anybody who wants to implement query-key memory layers based on values saved in embeddings. 

**Any Other info.**
The closest thing I could find to this was tf.nn.embedding_lookup_sparse, but that method requires the indices and scores to be sparse tensors and then just uses a tf.nn.embedding_lookup with some broadcast multiplication and reduce_sum in the backend anyhow. "
32674,Tensorflow Java API and @tf.function signatures,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14
- Python version: 3.7
- Bazel version (if compiling from source): 25.1
- GCC/Compiler version (if compiling from source): 7.3

**Describe the current behavior**

The current Tensorflow Java API (1.14 or 2.0) doesn't seem to be able to do inference on Tensorflow nodes created within a `@tf.function` signature. 

On a related note, https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md mentions that Java is planning to stay `Session` centric. Ideally, a signature inference centric API would be a good way to solve this issue and would be a good addition to the existing Java API.

Concretely, here is the Python snippet defining a `request` and `response` (full Python code attached at the bottom of this ticket):
```
  @tf.function(input_signature=[tf.TensorSpec(shape=[None, None], name=""serving"")])
  def serve(self, request):
    features = tf.identity(self.input_receiver(request), name='request')
    output = self.call(features)
    response = tf.identity(self.response_receiver(output), name='response')
    return response
```
After exporting my model to a `SavedModelBundle` and  trying to do some prediction in Java via the `Session.runner()` API, I am getting:
```
java.lang.IllegalArgumentException: No Operation named [request] in the Graph
  at org.tensorflow.Session$Runner.operationByName(Session.java:380)
  at org.tensorflow.Session$Runner.parseOutput(Session.java:394)
  at org.tensorflow.Session$Runner.feed(Session.java:131)
  ... 34 elided
```
where `request` is defined within a `@tf.function`.

**Describe the expected behavior**

I would expect to not have a runtime failure at inference time on the JVM: either `operationByName` would recognize the `request:0` node or another Java API would exist to fulfill my requirements.

**Code to reproduce the issue**

Python code (model creation):
```
import tensorflow as tf
from tensorflow.keras import layers


class MnistModel(tf.keras.Model):
  def __init__(self):
    super().__init__()
    self.first_dense = layers.Dense(64, input_shape=(784,), activation='relu', name='dense_1')
    self.out = layers.Dense(10, activation='softmax', name='predictions')

  def call(self, inp):
    f_dense = self.first_dense(inp)
    s_dense = self.out(f_dense)
    return s_dense

  def input_receiver(self, inp):
    return inp

  def response_receiver(self, output):
    return output

  @tf.function(input_signature=[tf.TensorSpec(shape=[None, None], name=""serving"")])
  def serve(self, request):
    features = tf.identity(self.input_receiver(request), name='request')
    output = self.call(features)
    response = tf.identity(self.response_receiver(output), name='response')
    return response

model = MnistModel()

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.reshape(60000, 784).astype('float32') / 255
x_test = x_test.reshape(10000, 784).astype('float32') / 255

model.compile(loss='sparse_categorical_crossentropy',
              optimizer=keras.optimizers.RMSprop())
history = model.fit(x_train, y_train,
                    batch_size=64,
                    epochs=1)

keras.experimental.export_saved_model(model, 'local_path', serving_only=True)
```

JVM / Scala code (inference code):
```
import org.tensorflow.{SavedModelBundle, Session, Tensor}
import java.nio.ByteBuffer
import java.lang.{Float => JFloat}

val savedModelBundle = SavedModelBundle.load(""local_path"", ""serve"")

val session = savedModelBundle.session()

val byteBuffer = ByteBuffer.allocate(784*4)

val tensor = Tensor.create(
    classOf[JFloat],
    Array(784),
    byteBuffer)

session.runner()
  .feed(""request:0"", tensor)
  .fetch(""output:0"")
  .run()

tensor.close()
byteBuffer.close()
```

I am getting the following error:
```
java.lang.IllegalArgumentException: No Operation named [request] in the Graph
  at org.tensorflow.Session$Runner.operationByName(Session.java:380)
  at org.tensorflow.Session$Runner.parseOutput(Session.java:394)
  at org.tensorflow.Session$Runner.feed(Session.java:131)
  ... 34 elided
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32672,A keras model containing a tf.tile op layer with a tensor in the `multiples` arg fails when saving to hdf5,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc0-101-gd2d2566eef 2.0.0-rc1
- Python version: Python 3.6.8 :: Anaconda, Inc.
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

A keras model containing a tf.tile op layer with a tensor in the `multiples` arg throws an exception when saving to hdf5.

(I'm using tf.tile because RepeatVector(n) doesn't accept a tensor for n. The goal is to stack a 2d feature batch so it can be concatenated to a variable length 3d batch of sequence features.)

**Describe the expected behavior**

Model.save() should save the model.

**Code to reproduce the issue**

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input
from tensorflow.keras import Model

a = Input(shape=(10,))
out = tf.tile(a, (1, tf.shape(a)[0]))
model = Model(a, out)

x = np.zeros((50,10), dtype=np.float32)
print(model(x).numpy())

model.save('my_model.h5')

**Other info / logs**

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-9b1429243599> in <module>
     11 print(model(x).numpy())
     12 
---> 13 model.save(model_dir + '/my_model.h5')

~/miniconda3/envs/tf20/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)
   1187     """"""
   1188     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,
-> 1189                       signatures, options)
   1190 
   1191   def save_weights(self, filepath, overwrite=True, save_format=None):

~/miniconda3/envs/tf20/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)
    110           'or using `save_weights`.')
    111     hdf5_format.save_model_to_hdf5(
--> 112         model, filepath, overwrite, include_optimizer)
    113   else:
    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,

~/miniconda3/envs/tf20/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in save_model_to_hdf5(model, filepath, overwrite, include_optimizer)
    107     model_weights_group = f.create_group('model_weights')
    108     model_layers = model.layers
--> 109     save_weights_to_hdf5_group(model_weights_group, model_layers)
    110 
    111     # TODO(b/128683857): Add integration tests between tf.keras and external

~/miniconda3/envs/tf20/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in save_weights_to_hdf5_group(f, layers)
    623 
    624   for layer in layers:
--> 625     g = f.create_group(layer.name)
    626     weights = _legacy_weights(layer)
    627     weight_values = K.batch_get_value(weights)

~/miniconda3/envs/tf20/lib/python3.6/site-packages/h5py/_hl/group.py in create_group(self, name, track_order)
     66             name, lcpl = self._e(name, lcpl=True)
     67             gcpl = Group._gcpl_crt_order if track_order else None
---> 68             gid = h5g.create(self.id, name, lcpl=lcpl, gcpl=gcpl)
     69             return Group(gid)
     70 

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/h5g.pyx in h5py.h5g.create()

ValueError: Unable to create group (name already exists)
"
32666,cloudpickle cannot unpickle tf.keras in 1.14: KeyError: 'tensorflow.keras',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS, Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Anaconda binary
- TensorFlow version (use command below): 1.14
- Python version: 3.7.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
unknown 1.14.0

**Describe the current behavior**

cloudpickle cannot unpickle `tf.keras` because the `deprecation_wrapper` introduced in 1.14.

In one python session:

~~~python 
import cloudpickle
import tensorflow.keras as K
with open(""/tmp/K.pkl"", ""wb"") as f:
  cloudpickle.dump(K, f)
~~~

Then start another python session:

~~~python
import cloudpickle
with open(""/tmp/K.pkl"", ""rb"") as f:
  cloudpickle.load(f)
~~~

Error:

~~~
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/Users/meng/conda/envs/tf-1.14/lib/python3.7/site-packages/tensorflow/python/util/deprecation_wrapper.py"", line 148, in __setstate__
    sys.modules[d]._dw_wrapped_module,
KeyError: 'tensorflow.keras'
~~~

**Describe the expected behavior**

The same code worked in TensorFlow 1.13. In TensorFlow 1.13, `K.__module__` does not exist, while in TensorFlow 1.14, `K.__module__` is 

~~~
'tensorflow.python.util.deprecation_wrapper'
~~~

It makes the `tensorflow.keras` module not loaded during unpickling.

There are multiple applications use pickle to ship TensorFlow code to a remote machine to unpickle and execute, e.g., Spark, horovod.spark. So the behavior change would fail the jobs.

Btw, one workaround is to do `from tensorflow import keras as K` instead of `import tensorflow.keras as K`. The diff is that `K.__name__` is `'tensorflow.python.keras.api._v1.keras'` in the first case and `tensorflow.keras` in the second. And pickle uses `__name__` to get global names: https://github.com/python/cpython/blob/3.7/Lib/pickle.py#L952

cc: @alsrgv @hanyucui @annarev

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

See above.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
32664,Broken link in https://www.tensorflow.org/guide/eager#top_of_page,"https://www.tensorflow.org/guide/eager#top_of_page
says
""For a collection of examples running in eager execution, see: tensorflow/contrib/eager/python/examples.""

The link to `https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples` returns a 404 error."
32662,Attention layer not serializable because it takes init args but doesn't implement get_config,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc0-101-gd2d2566eef 2.0.0-rc1
- Python version: Python 3.6.8 :: Anaconda, Inc.
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

Trying to save a model with a tensorflow.keras.layers.Attention layer throws because it doesn't implement get_config().

NotImplementedError: Layers with arguments in `__init__` must override `get_config`.

**Describe the expected behavior**

Model.save() should save the model.

**Code to reproduce the issue**

a = Input(shape=(None, 10))
attn = Attention()([a,a])
model = Model(a, attn)
model(np.zeros((50,10), dtype=np.float32))
model.save('my_model.h5')

**Other info / logs**

~/miniconda3/envs/tf20/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in get_config(self)
    571     # or that `get_config` has been overridden:
    572     if len(extra_args) > 1 and hasattr(self.get_config, '_is_default'):
--> 573       raise NotImplementedError('Layers with arguments in `__init__` must '
    574                                 'override `get_config`.')
    575     # TODO(reedwm): Handle serializing self._dtype_policy.

NotImplementedError: Layers with arguments in `__init__` must override `get_config`."
32661,Documentation about XLA backend is incomplete and inaccurate,"Looking at ```Scenario 3``` in https://github.com/tensorflow/tensorflow/issues/new?labels=type%3Adocs&template=20-documentation-issue.md

It has a link to the class ```StreamExecutor``` but the link leads to the empty class with comments suggesting that it has been removed (?)
"
32660,Performance of tf.io.read_file in graph mode,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.0.0rc1
- Python version: 3.6.0
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Current behavior**
I observed surprising performances of the different ways to read a file, especially when comparing graph mode (by `tf.function`) versus eager mode. I compared two ways to read a file:
`tf.io.gfile.GFile(filepath).read(-1)` versus  `tf.io.read_file(filepath)`.
If the two have similar performances in eager mode, the first option gains a close to 50% speedup when using it in graph mode (wrapped into a `tf.function` call), while the latter one do not show any speed improvement, rather a slowdown.

**Expected behavior**
I do not  understand why such a speed up is observed on `tf.io.gfile.GFile` while `tf.io.read_file` does not benefit any. This is crucial because my data pipeline yields filepaths through `tf.data.Dataset.list_files()`, which are further read and decoded, but tf.io.gfile.GFile does not accept string tensors as input. Hence I am forced to use `tf.io.read_file` (see  [issue](https://github.com/tensorflow/tensorflow/issues/32620))

**Code to reproduce the issue**
See below a MVCE:

```
import numpy as np
import tensorflow as tf
import timeit
import os

filepath = '/tmp/array.npy'
np.save(filepath, np.ones(shape=(400, ) * 3))

def read_Gfile(path):
    f = tf.io.gfile.GFile(name=path, mode='rb')
    s = f.read(-1)
    f.close()
    return s

read_io = lambda x: tf.io.read_file(x)

graph_read_io = tf.function(read_io)
graph_read_Gfile = tf.function(read_Gfile)

print('Reading with numpy: %.3f ms' % (timeit.Timer(lambda: np.load(filepath)).timeit(number=10) * 1000 / 10))
print('Reading in eager mode with GFile: %.3f ms' % (timeit.Timer(lambda: read_Gfile(filepath)).timeit(number=10) * 1000 / 10))
print('Reading in eager mode with io.read_file: %.3f ms' % (timeit.Timer(lambda: read_io(filepath)).timeit(number=10) * 1000 / 10))
print('Reading in graph mode with GFile: %.3f ms' % (timeit.Timer(lambda: graph_read_Gfile(filepath)).timeit(number=10) * 1000 / 10))
print('Reading in graph mode with io.read_file: %.3f ms' % (timeit.Timer(lambda: graph_read_io(filepath)).timeit(number=10) * 1000/ 10))

os.remove(filepath)
```
This code produces the following performances:
```
Reading with numpy: 175.189 ms
Reading in eager mode with GFile: 826.163 ms
Reading in eager mode with io.read_file: 767.376 ms
Reading in graph mode with GFile: 509.069 ms
Reading in graph mode with io.read_file: 864.643 ms
```
"
32659,Potential bug in _compute_sampled_logits when `num_true` > 1,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave (10.14.6)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When logits are computed in `_compute_sampled_logits` for the evaluation of `tf.nn.nce_loss`, the output related to the true labels from `out_labels` might be wrong. 

Let me provide an example. Assume `batch_size = 2`, `num_true = 1000`, `num_sampled = 20`, then `out_labels` will be a Tensor of size `2 x 1020` (1020 is the addition of `num_true` and `num_sampled`).
According to the following line:
`array_ops.ones_like(true_logits) / num_true` (one of the last lines of `_compute_sampled_logits` function),
the first 1000 elements in each row of `out_labels` are: 1./1000 = 0.001, while the last 20 are 0.

The potential problem is that the output of `_compute_sampled_logits` is then fed to a sigmoid cross entropy (**not** softmax cross entropy which requires the labels to sum to 1 in each row). As a result, labels of an example are independent to each other. This means we can have as many labels as possible for each example, and `labels` do not necessarily have to sum to 1 (for each example).

By dividing `array_ops.ones_like(true_logits)` by `num_true`, the weights for the first 1000 (positive) labels of each example would try to produce a `y_hat` that would be close to 0.001 instead of 1. Predictions, which are generated via a sigmoid function, represent the probability that a given label is positive (for this example). Let's call this prediction, `y_hat` for simplicity. Due to the above formulation, `y_hat` would try to be as close to value `1/num_true` (instead of 1) for labels that appeared with the corresponding x feature.

**Describe the expected behavior**
The expected behavior would be that all positive labels in each example be assigned value 1 instead of `1/num_true`. This way, the sigmoid which applies independently to each label will try to produce a `y_hat` for each (positive) label close to 1 instead of `1/num_true`.

The solution would be:
`    out_labels = array_ops.concat([array_ops.ones_like(true_logits), array_ops.zeros_like(sampled_logits)], 1)
`

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
import pandas as pd
import numpy as np
import math

batch_size = 4
num_classes = 2000
dim = 16
num_sampled = 20
num_true = 1000
sampled_values = None
remove_accidental_hits = False
partition_strategy = 'div'

weights = tf.get_variable('weights', initializer=tf.truncated_normal([num_classes, dim], stddev=1.0 / math.sqrt(dim)), trainable=True)
biases = tf.get_variable('biases', initializer=tf.zeros([num_classes]), trainable=True)
inputs = tf.random_normal([batch_size, dim])
df = pd.DataFrame({'labels': [np.array([0, 2], np.int32), np.array([1], np.int32),
                              np.array([6, 4, 1, 0, 3, 2], np.int32), np.array([5, 0, 2, 1, 6, 3, 9, 4], np.int32)]})
labels = tf.keras.preprocessing.sequence.pad_sequences(df['labels'].values, num_true, dtype=np.int64, value=0)
labels = tf.constant(labels)

logits, labels = _compute_sampled_logits(
    weights=weights,
    biases=biases,
    labels=labels,
    inputs=inputs,
    num_sampled=num_sampled,
    num_classes=num_classes,
    num_true=num_true,
    sampled_values=sampled_values,
    subtract_log_q=True,
    remove_accidental_hits=remove_accidental_hits,
    partition_strategy=partition_strategy)
```

Here, `_compute_sampled_logits` is from `tensorflow.python.ops.nn_impl`.
"
32658,Why doesn't tf.matmul work with transposed tensor?,"Why doesn't `tf.matmul` work with transposed tensor?

`transpose_b=True` is ok, but not `tf.transpose(inp)`.

This screenshot was made in Colab with `tensorflow-gpu==2.0.0-rc1`:

![image](https://user-images.githubusercontent.com/35609308/65253964-f2bfdd80-db14-11e9-859f-ca6a38498d20.png)"
32657,Broken link in https://www.tensorflow.org/versions/r2.0/api_docs,"In
https://www.tensorflow.org/versions/r2.0/api_docs
the `Swift (Early Release)`  link points to `https://www.tensorflow.org/versions/r2.0/swift` which shows a 404 page.
"
32656,TF2rc Simple model subclass with Gradient tape error when transformed with autograph,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/quickstart/advanced.ipynb
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): TF2RC binary
- TensorFlow version (use command below): TF2RC
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
A simple toy example using Gradient tape and a subclass-model cannot be transformed into a graph with Autograph.
Error: _**could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4**_

**Describe the expected behavior**
The use of `@tf.function` should correctly create Autograph

**Code to reproduce the issue**
Please, run 
https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/quickstart/advanced.ipynb

> 
> try:
>   # %tensorflow_version only exists in Colab.
>   %tensorflow_version 2.x
> except Exception:
>   pass
> from __future__ import absolute_import, division, print_function, unicode_literals
> 
> import tensorflow as tf
> 
> from tensorflow.keras.layers import Dense, Flatten, Conv2D
> from tensorflow.keras import Model
> mnist = tf.keras.datasets.mnist
> 
> (x_train, y_train), (x_test, y_test) = mnist.load_data()
> x_train, x_test = x_train / 255.0, x_test / 255.0
> 
> # Add a channels dimension
> x_train = x_train[..., tf.newaxis]
> x_test = x_test[..., tf.newaxis]
> train_ds = tf.data.Dataset.from_tensor_slices(
>     (x_train, y_train)).shuffle(10000).batch(32)
> 
> test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)
> class MyModel(Model):
>   def __init__(self):
>     super(MyModel, self).__init__()
>     self.conv1 = Conv2D(32, 3, activation='relu')
>     self.flatten = Flatten()
>     self.d1 = Dense(128, activation='relu')
>     self.d2 = Dense(10, activation='softmax')
> 
>   def call(self, x):
>     x = self.conv1(x)
>     x = self.flatten(x)
>     x = self.d1(x)
>     return self.d2(x)
> 
> # Create an instance of the model
> model = MyModel()
> loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
> 
> optimizer = tf.keras.optimizers.Adam()
> train_loss = tf.keras.metrics.Mean(name='train_loss')
> train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
> 
> test_loss = tf.keras.metrics.Mean(name='test_loss')
> test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')
> @tf.function
> def train_step(images, labels):
>   with tf.GradientTape() as tape:
>     predictions = model(images)
>     loss = loss_object(labels, predictions)
>   gradients = tape.gradient(loss, model.trainable_variables)
>   optimizer.apply_gradients(zip(gradients, model.trainable_variables))
> 
>   train_loss(loss)
>   train_accuracy(labels, predictions)
> @tf.function
> def test_step(images, labels):
>   predictions = model(images)
>   t_loss = loss_object(labels, predictions)
> 
>   test_loss(t_loss)
>   test_accuracy(labels, predictions)
> EPOCHS = 5
> 
> for epoch in range(EPOCHS):
>   for images, labels in train_ds:
>     train_step(images, labels)
> 
>   for test_images, test_labels in test_ds:
>     test_step(test_images, test_labels)
> 
>   template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
>   print(template.format(epoch+1,
>                         train_loss.result(),
>                         train_accuracy.result()*100,
>                         test_loss.result(),
>                         test_accuracy.result()*100))
> 
>   # Reset the metrics for the next epoch
>   train_loss.reset_states()
>   train_accuracy.reset_states()
>   test_loss.reset_states()
>   test_accuracy.reset_states()

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32655,Unable to load tensorflow model in Java that was originally created in python (file not found exception),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): _Yes_
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _Windows 10 Version 1903_
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: _/_
- TensorFlow installed from (source or binary): _installed via pip in virtualenv_
- TensorFlow version (use command below): _1.14_
- Python version: _3.7_
- Bazel version (if compiling from source): _/_
- GCC/Compiler version (if compiling from source): _/_
- CUDA/cuDNN version: 10.0
- GPU model and memory: Nvidia GTX 1050 Ti (3 GB)

**Describe the current behavior**
I train a neural network. After training, I want to export my model in order to use it in Java for predictions. Unfortunately, it always displays an error while trying to load the model in Java, saying the file cannot be found (see error message below). I checked the path a hundred times and made sure via the built-in Java methods that the files exist and I have the rights to read them.

**Describe the expected behavior**
I expect the model to be loaded in order to use it for predictions within my native Java code.

**Code to reproduce the issue**
Below this you can see the Python code to export the model.

```
tf.saved_model.simple_save(sess,
                           path_graph_model_export_dir,
                           inputs=model_inputs, outputs={'sim': similarity})
```

This is the structure of the exported directory.

```
saved_model
---- saved_model.pb
---- variables
-------- variables.data-00000-of-00001
-------- variables.index
```

And the code in Java to load the model..

```
SavedModelBundle model = SavedModelBundle.load(modelPath, ""serve"")
```

results in this exception.

```
2019-09-17 10:59:03.905538: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: saved_model
2019-09-17 10:59:03.905997: I tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: fail. Took 457 microseconds.
org.tensorflow.TensorFlowException: Could not find SavedModel .pb or .pbtxt at supplied export directory path: saved_model
```

**Other info / logs**
I use a tensorflow graph built with Sonnet (version 1.34) but I don't think this is an issue as Sonnet only assists in building more complex graphs.
Tensorboard model log: [tensorboard.zip](https://github.com/tensorflow/tensorflow/files/3631054/tensorboard.zip)
Saved model folder: [saved_model.zip](https://github.com/tensorflow/tensorflow/files/3631055/saved_model.zip)
"
32654,tf.distribute.MirroredStrategy leads to an infinite polling cycle with 4 GPUs,"### System information
A physical tower with 4 GPUs running Ubuntu 18.04 over Kubernetes

- 256 GB of RAM
- TensorFlow: tested on `tf-nightly-gpu-2.0-preview==2.0.0.dev20190902` to `tf-nightly-gpu-2.0-preview==2.0.0.dev20190918`
- Python 3.6.8
- CUDA 10.0, cuDNN 7.6.3.30 (also tested with cuDNN 7.5.0.56)
- NVIDIA GTX 1080

<details>
<summary>nvidia-smi</summary>
<pre>
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |
| 53%   70C    P2    79W / 250W |  10889MiB / 11178MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |
| 52%   69C    P2    76W / 250W |  10893MiB / 11178MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |
| 48%   65C    P2    78W / 250W |  10889MiB / 11178MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 108...  Off  | 00000000:83:00.0 Off |                  N/A |
| 45%   62C    P2    76W / 250W |  10893MiB / 11178MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
</pre>
</details>

### Problem

I run the following sample code:

```python
#!/usr/bin/env python3
import sys
import tensorflow as tf


def main():
    batch_size = 12
    features_shape = 372, 558, 3
    labels = 10
    sample = tf.random.uniform(features_shape)

    def with_shape(t, shape):
        t = tf.squeeze(t)
        t.set_shape(shape)
        return t

    ds_train = tf.data.Dataset.from_tensors([sample]).map(lambda s: (s, tf.ones((labels,)))) \
        .repeat().batch(batch_size).map(lambda s, l: (with_shape(s, (batch_size,) + features_shape),
                                                      with_shape(l, (batch_size, labels))))
    ds_val = tf.data.Dataset.from_tensors([sample]).map(lambda s: (s, tf.ones((labels,)))) \
        .repeat().batch(batch_size).take(10).map(
        lambda s, l: (with_shape(s, (batch_size,) + features_shape), with_shape(l, (batch_size, labels))))
    with tf.distribute.MirroredStrategy().scope():
        model = tf.keras.applications.DenseNet121(
            weights=None, input_shape=features_shape, classes=labels)
        model.build((batch_size,) + features_shape)
        model.summary()
        optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)
        cross_entropy = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)
        model.compile(optimizer=optimizer, loss=cross_entropy, metrics=[""accuracy""])
    model.fit(ds_train, validation_data=ds_val, epochs=1, steps_per_epoch=100)


if __name__ == ""__main__"":
    sys.exit(main())
```

It outputs the following log and hangs for at least 9 hours (I killed it after):

<details>
<summary>log</summary>
<pre>
2019-09-19 11:22:16.548532: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-09-19 11:22:16.553080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:02:00.0
2019-09-19 11:22:16.554064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 1 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:03:00.0
2019-09-19 11:22:16.555051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 2 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:82:00.0
2019-09-19 11:22:16.555890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 3 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:83:00.0
2019-09-19 11:22:16.556021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcudart.so.10.0
2019-09-19 11:22:16.556046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcublas.so.10.0
2019-09-19 11:22:16.556062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcufft.so.10.0
2019-09-19 11:22:16.556079: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcurand.so.10.0
2019-09-19 11:22:16.556095: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcusolver.so.10.0
2019-09-19 11:22:16.556111: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcusparse.so.10.0
2019-09-19 11:22:16.556127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcudnn.so.7
2019-09-19 11:22:16.562745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Adding visible gpu devices: 0, 1, 2, 3
2019-09-19 11:22:16.562815: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcudart.so.10.0
2019-09-19 11:22:16.566634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1173] Device interconnect StreamExecutorwith strength 1 edge matrix:
2019-09-19 11:22:16.566650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1179]      0 1 2 3
2019-09-19 11:22:16.566657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 0:   N Y N N
2019-09-19 11:22:16.566661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 1:   Y N N N
2019-09-19 11:22:16.566666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 2:   N N N Y
2019-09-19 11:22:16.566670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 3:   N N Y N
2019-09-19 11:22:16.571630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
2019-09-19 11:22:16.573706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10470 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)
2019-09-19 11:22:16.575382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10470 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)
2019-09-19 11:22:16.576566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10470 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)
WARNING:tensorflow:Entity <function main.<locals>.<lambda> at 0x7fe776f021e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []
2019-09-19 11:22:17.393146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:02:00.0
2019-09-19 11:22:17.394380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 1 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:03:00.0
2019-09-19 11:22:17.395221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 2 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:82:00.0
2019-09-19 11:22:17.396088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1632] Found device 3 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:83:00.0
2019-09-19 11:22:17.396168: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcudart.so.10.0
2019-09-19 11:22:17.396202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcublas.so.10.0
2019-09-19 11:22:17.396218: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcufft.so.10.0
2019-09-19 11:22:17.396233: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcurand.so.10.0
2019-09-19 11:22:17.396263: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcusolver.so.10.0
2019-09-19 11:22:17.396278: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcusparse.so.10.0
2019-09-19 11:22:17.396293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcudnn.so.7
2019-09-19 11:22:17.402450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Adding visible gpu devices: 0, 1, 2, 3
2019-09-19 11:22:17.402599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1173] Device interconnect StreamExecutorwith strength 1 edge matrix:
2019-09-19 11:22:17.402611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1179]      0 1 2 3
2019-09-19 11:22:17.402619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 0:   N Y N N
2019-09-19 11:22:17.402625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 1:   Y N N N
2019-09-19 11:22:17.402631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 2:   N N N Y
2019-09-19 11:22:17.402637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1192] 3:   N N Y N
2019-09-19 11:22:17.407338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
2019-09-19 11:22:17.408425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/device:GPU:1 with 10470 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)
2019-09-19 11:22:17.409430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/device:GPU:2 with 10470 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)
2019-09-19 11:22:17.410293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1318] Created TensorFlow device (/device:GPU:3 with 10470 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)
Model: ""densenet121""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 372, 558, 3) 0
__________________________________________________________________________________________________
zero_padding2d (ZeroPadding2D)  (None, 378, 564, 3)  0           input_1[0][0]
__________________________________________________________________________________________________
conv1/conv (Conv2D)             (None, 186, 279, 64) 9408        zero_padding2d[0][0]
__________________________________________________________________________________________________
conv1/bn (BatchNormalization)   (None, 186, 279, 64) 256         conv1/conv[0][0]
__________________________________________________________________________________________________
conv1/relu (Activation)         (None, 186, 279, 64) 0           conv1/bn[0][0]
__________________________________________________________________________________________________
zero_padding2d_1 (ZeroPadding2D (None, 188, 281, 64) 0           conv1/relu[0][0]
__________________________________________________________________________________________________
pool1 (MaxPooling2D)            (None, 93, 140, 64)  0           zero_padding2d_1[0][0]
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 93, 140, 64)  256         pool1[0][0]
__________________________________________________________________________________________________
conv2_block1_0_relu (Activation (None, 93, 140, 64)  0           conv2_block1_0_bn[0][0]
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 93, 140, 128) 8192        conv2_block1_0_relu[0][0]
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 93, 140, 128) 512         conv2_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 93, 140, 128) 0           conv2_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 93, 140, 32)  36864       conv2_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block1_concat (Concatenat (None, 93, 140, 96)  0           pool1[0][0]
                                                                 conv2_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block2_0_bn (BatchNormali (None, 93, 140, 96)  384         conv2_block1_concat[0][0]
__________________________________________________________________________________________________
conv2_block2_0_relu (Activation (None, 93, 140, 96)  0           conv2_block2_0_bn[0][0]
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 93, 140, 128) 12288       conv2_block2_0_relu[0][0]
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 93, 140, 128) 512         conv2_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 93, 140, 128) 0           conv2_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 93, 140, 32)  36864       conv2_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block2_concat (Concatenat (None, 93, 140, 128) 0           conv2_block1_concat[0][0]
                                                                 conv2_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block3_0_bn (BatchNormali (None, 93, 140, 128) 512         conv2_block2_concat[0][0]
__________________________________________________________________________________________________
conv2_block3_0_relu (Activation (None, 93, 140, 128) 0           conv2_block3_0_bn[0][0]
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 93, 140, 128) 16384       conv2_block3_0_relu[0][0]
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 93, 140, 128) 512         conv2_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 93, 140, 128) 0           conv2_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 93, 140, 32)  36864       conv2_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block3_concat (Concatenat (None, 93, 140, 160) 0           conv2_block2_concat[0][0]
                                                                 conv2_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block4_0_bn (BatchNormali (None, 93, 140, 160) 640         conv2_block3_concat[0][0]
__________________________________________________________________________________________________
conv2_block4_0_relu (Activation (None, 93, 140, 160) 0           conv2_block4_0_bn[0][0]
__________________________________________________________________________________________________
conv2_block4_1_conv (Conv2D)    (None, 93, 140, 128) 20480       conv2_block4_0_relu[0][0]
__________________________________________________________________________________________________
conv2_block4_1_bn (BatchNormali (None, 93, 140, 128) 512         conv2_block4_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block4_1_relu (Activation (None, 93, 140, 128) 0           conv2_block4_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block4_2_conv (Conv2D)    (None, 93, 140, 32)  36864       conv2_block4_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block4_concat (Concatenat (None, 93, 140, 192) 0           conv2_block3_concat[0][0]
                                                                 conv2_block4_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block5_0_bn (BatchNormali (None, 93, 140, 192) 768         conv2_block4_concat[0][0]
__________________________________________________________________________________________________
conv2_block5_0_relu (Activation (None, 93, 140, 192) 0           conv2_block5_0_bn[0][0]
__________________________________________________________________________________________________
conv2_block5_1_conv (Conv2D)    (None, 93, 140, 128) 24576       conv2_block5_0_relu[0][0]
__________________________________________________________________________________________________
conv2_block5_1_bn (BatchNormali (None, 93, 140, 128) 512         conv2_block5_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block5_1_relu (Activation (None, 93, 140, 128) 0           conv2_block5_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block5_2_conv (Conv2D)    (None, 93, 140, 32)  36864       conv2_block5_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block5_concat (Concatenat (None, 93, 140, 224) 0           conv2_block4_concat[0][0]
                                                                 conv2_block5_2_conv[0][0]
__________________________________________________________________________________________________
conv2_block6_0_bn (BatchNormali (None, 93, 140, 224) 896         conv2_block5_concat[0][0]
__________________________________________________________________________________________________
conv2_block6_0_relu (Activation (None, 93, 140, 224) 0           conv2_block6_0_bn[0][0]
__________________________________________________________________________________________________
conv2_block6_1_conv (Conv2D)    (None, 93, 140, 128) 28672       conv2_block6_0_relu[0][0]
__________________________________________________________________________________________________
conv2_block6_1_bn (BatchNormali (None, 93, 140, 128) 512         conv2_block6_1_conv[0][0]
__________________________________________________________________________________________________
conv2_block6_1_relu (Activation (None, 93, 140, 128) 0           conv2_block6_1_bn[0][0]
__________________________________________________________________________________________________
conv2_block6_2_conv (Conv2D)    (None, 93, 140, 32)  36864       conv2_block6_1_relu[0][0]
__________________________________________________________________________________________________
conv2_block6_concat (Concatenat (None, 93, 140, 256) 0           conv2_block5_concat[0][0]
                                                                 conv2_block6_2_conv[0][0]
__________________________________________________________________________________________________
pool2_bn (BatchNormalization)   (None, 93, 140, 256) 1024        conv2_block6_concat[0][0]
__________________________________________________________________________________________________
pool2_relu (Activation)         (None, 93, 140, 256) 0           pool2_bn[0][0]
__________________________________________________________________________________________________
pool2_conv (Conv2D)             (None, 93, 140, 128) 32768       pool2_relu[0][0]
__________________________________________________________________________________________________
pool2_pool (AveragePooling2D)   (None, 46, 70, 128)  0           pool2_conv[0][0]
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 46, 70, 128)  512         pool2_pool[0][0]
__________________________________________________________________________________________________
conv3_block1_0_relu (Activation (None, 46, 70, 128)  0           conv3_block1_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 46, 70, 128)  16384       conv3_block1_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 46, 70, 128)  0           conv3_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block1_concat (Concatenat (None, 46, 70, 160)  0           pool2_pool[0][0]
                                                                 conv3_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block2_0_bn (BatchNormali (None, 46, 70, 160)  640         conv3_block1_concat[0][0]
__________________________________________________________________________________________________
conv3_block2_0_relu (Activation (None, 46, 70, 160)  0           conv3_block2_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 46, 70, 128)  20480       conv3_block2_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 46, 70, 128)  0           conv3_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block2_concat (Concatenat (None, 46, 70, 192)  0           conv3_block1_concat[0][0]
                                                                 conv3_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block3_0_bn (BatchNormali (None, 46, 70, 192)  768         conv3_block2_concat[0][0]
__________________________________________________________________________________________________
conv3_block3_0_relu (Activation (None, 46, 70, 192)  0           conv3_block3_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 46, 70, 128)  24576       conv3_block3_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 46, 70, 128)  0           conv3_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block3_concat (Concatenat (None, 46, 70, 224)  0           conv3_block2_concat[0][0]
                                                                 conv3_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block4_0_bn (BatchNormali (None, 46, 70, 224)  896         conv3_block3_concat[0][0]
__________________________________________________________________________________________________
conv3_block4_0_relu (Activation (None, 46, 70, 224)  0           conv3_block4_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 46, 70, 128)  28672       conv3_block4_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block4_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 46, 70, 128)  0           conv3_block4_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block4_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block4_concat (Concatenat (None, 46, 70, 256)  0           conv3_block3_concat[0][0]
                                                                 conv3_block4_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block5_0_bn (BatchNormali (None, 46, 70, 256)  1024        conv3_block4_concat[0][0]
__________________________________________________________________________________________________
conv3_block5_0_relu (Activation (None, 46, 70, 256)  0           conv3_block5_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block5_1_conv (Conv2D)    (None, 46, 70, 128)  32768       conv3_block5_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block5_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block5_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block5_1_relu (Activation (None, 46, 70, 128)  0           conv3_block5_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block5_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block5_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block5_concat (Concatenat (None, 46, 70, 288)  0           conv3_block4_concat[0][0]
                                                                 conv3_block5_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block6_0_bn (BatchNormali (None, 46, 70, 288)  1152        conv3_block5_concat[0][0]
__________________________________________________________________________________________________
conv3_block6_0_relu (Activation (None, 46, 70, 288)  0           conv3_block6_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block6_1_conv (Conv2D)    (None, 46, 70, 128)  36864       conv3_block6_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block6_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block6_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block6_1_relu (Activation (None, 46, 70, 128)  0           conv3_block6_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block6_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block6_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block6_concat (Concatenat (None, 46, 70, 320)  0           conv3_block5_concat[0][0]
                                                                 conv3_block6_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block7_0_bn (BatchNormali (None, 46, 70, 320)  1280        conv3_block6_concat[0][0]
__________________________________________________________________________________________________
conv3_block7_0_relu (Activation (None, 46, 70, 320)  0           conv3_block7_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block7_1_conv (Conv2D)    (None, 46, 70, 128)  40960       conv3_block7_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block7_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block7_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block7_1_relu (Activation (None, 46, 70, 128)  0           conv3_block7_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block7_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block7_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block7_concat (Concatenat (None, 46, 70, 352)  0           conv3_block6_concat[0][0]
                                                                 conv3_block7_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block8_0_bn (BatchNormali (None, 46, 70, 352)  1408        conv3_block7_concat[0][0]
__________________________________________________________________________________________________
conv3_block8_0_relu (Activation (None, 46, 70, 352)  0           conv3_block8_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block8_1_conv (Conv2D)    (None, 46, 70, 128)  45056       conv3_block8_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block8_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block8_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block8_1_relu (Activation (None, 46, 70, 128)  0           conv3_block8_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block8_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block8_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block8_concat (Concatenat (None, 46, 70, 384)  0           conv3_block7_concat[0][0]
                                                                 conv3_block8_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block9_0_bn (BatchNormali (None, 46, 70, 384)  1536        conv3_block8_concat[0][0]
__________________________________________________________________________________________________
conv3_block9_0_relu (Activation (None, 46, 70, 384)  0           conv3_block9_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block9_1_conv (Conv2D)    (None, 46, 70, 128)  49152       conv3_block9_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block9_1_bn (BatchNormali (None, 46, 70, 128)  512         conv3_block9_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block9_1_relu (Activation (None, 46, 70, 128)  0           conv3_block9_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block9_2_conv (Conv2D)    (None, 46, 70, 32)   36864       conv3_block9_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block9_concat (Concatenat (None, 46, 70, 416)  0           conv3_block8_concat[0][0]
                                                                 conv3_block9_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block10_0_bn (BatchNormal (None, 46, 70, 416)  1664        conv3_block9_concat[0][0]
__________________________________________________________________________________________________
conv3_block10_0_relu (Activatio (None, 46, 70, 416)  0           conv3_block10_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block10_1_conv (Conv2D)   (None, 46, 70, 128)  53248       conv3_block10_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block10_1_bn (BatchNormal (None, 46, 70, 128)  512         conv3_block10_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block10_1_relu (Activatio (None, 46, 70, 128)  0           conv3_block10_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block10_2_conv (Conv2D)   (None, 46, 70, 32)   36864       conv3_block10_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block10_concat (Concatena (None, 46, 70, 448)  0           conv3_block9_concat[0][0]
                                                                 conv3_block10_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block11_0_bn (BatchNormal (None, 46, 70, 448)  1792        conv3_block10_concat[0][0]
__________________________________________________________________________________________________
conv3_block11_0_relu (Activatio (None, 46, 70, 448)  0           conv3_block11_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block11_1_conv (Conv2D)   (None, 46, 70, 128)  57344       conv3_block11_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block11_1_bn (BatchNormal (None, 46, 70, 128)  512         conv3_block11_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block11_1_relu (Activatio (None, 46, 70, 128)  0           conv3_block11_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block11_2_conv (Conv2D)   (None, 46, 70, 32)   36864       conv3_block11_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block11_concat (Concatena (None, 46, 70, 480)  0           conv3_block10_concat[0][0]
                                                                 conv3_block11_2_conv[0][0]
__________________________________________________________________________________________________
conv3_block12_0_bn (BatchNormal (None, 46, 70, 480)  1920        conv3_block11_concat[0][0]
__________________________________________________________________________________________________
conv3_block12_0_relu (Activatio (None, 46, 70, 480)  0           conv3_block12_0_bn[0][0]
__________________________________________________________________________________________________
conv3_block12_1_conv (Conv2D)   (None, 46, 70, 128)  61440       conv3_block12_0_relu[0][0]
__________________________________________________________________________________________________
conv3_block12_1_bn (BatchNormal (None, 46, 70, 128)  512         conv3_block12_1_conv[0][0]
__________________________________________________________________________________________________
conv3_block12_1_relu (Activatio (None, 46, 70, 128)  0           conv3_block12_1_bn[0][0]
__________________________________________________________________________________________________
conv3_block12_2_conv (Conv2D)   (None, 46, 70, 32)   36864       conv3_block12_1_relu[0][0]
__________________________________________________________________________________________________
conv3_block12_concat (Concatena (None, 46, 70, 512)  0           conv3_block11_concat[0][0]
                                                                 conv3_block12_2_conv[0][0]
__________________________________________________________________________________________________
pool3_bn (BatchNormalization)   (None, 46, 70, 512)  2048        conv3_block12_concat[0][0]
__________________________________________________________________________________________________
pool3_relu (Activation)         (None, 46, 70, 512)  0           pool3_bn[0][0]
__________________________________________________________________________________________________
pool3_conv (Conv2D)             (None, 46, 70, 256)  131072      pool3_relu[0][0]
__________________________________________________________________________________________________
pool3_pool (AveragePooling2D)   (None, 23, 35, 256)  0           pool3_conv[0][0]
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 23, 35, 256)  1024        pool3_pool[0][0]
__________________________________________________________________________________________________
conv4_block1_0_relu (Activation (None, 23, 35, 256)  0           conv4_block1_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 23, 35, 128)  32768       conv4_block1_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 23, 35, 128)  0           conv4_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block1_concat (Concatenat (None, 23, 35, 288)  0           pool3_pool[0][0]
                                                                 conv4_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block2_0_bn (BatchNormali (None, 23, 35, 288)  1152        conv4_block1_concat[0][0]
__________________________________________________________________________________________________
conv4_block2_0_relu (Activation (None, 23, 35, 288)  0           conv4_block2_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 23, 35, 128)  36864       conv4_block2_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 23, 35, 128)  0           conv4_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block2_concat (Concatenat (None, 23, 35, 320)  0           conv4_block1_concat[0][0]
                                                                 conv4_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block3_0_bn (BatchNormali (None, 23, 35, 320)  1280        conv4_block2_concat[0][0]
__________________________________________________________________________________________________
conv4_block3_0_relu (Activation (None, 23, 35, 320)  0           conv4_block3_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 23, 35, 128)  40960       conv4_block3_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 23, 35, 128)  0           conv4_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block3_concat (Concatenat (None, 23, 35, 352)  0           conv4_block2_concat[0][0]
                                                                 conv4_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block4_0_bn (BatchNormali (None, 23, 35, 352)  1408        conv4_block3_concat[0][0]
__________________________________________________________________________________________________
conv4_block4_0_relu (Activation (None, 23, 35, 352)  0           conv4_block4_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 23, 35, 128)  45056       conv4_block4_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block4_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 23, 35, 128)  0           conv4_block4_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block4_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block4_concat (Concatenat (None, 23, 35, 384)  0           conv4_block3_concat[0][0]
                                                                 conv4_block4_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block5_0_bn (BatchNormali (None, 23, 35, 384)  1536        conv4_block4_concat[0][0]
__________________________________________________________________________________________________
conv4_block5_0_relu (Activation (None, 23, 35, 384)  0           conv4_block5_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 23, 35, 128)  49152       conv4_block5_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block5_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 23, 35, 128)  0           conv4_block5_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block5_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block5_concat (Concatenat (None, 23, 35, 416)  0           conv4_block4_concat[0][0]
                                                                 conv4_block5_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block6_0_bn (BatchNormali (None, 23, 35, 416)  1664        conv4_block5_concat[0][0]
__________________________________________________________________________________________________
conv4_block6_0_relu (Activation (None, 23, 35, 416)  0           conv4_block6_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 23, 35, 128)  53248       conv4_block6_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block6_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 23, 35, 128)  0           conv4_block6_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block6_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block6_concat (Concatenat (None, 23, 35, 448)  0           conv4_block5_concat[0][0]
                                                                 conv4_block6_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block7_0_bn (BatchNormali (None, 23, 35, 448)  1792        conv4_block6_concat[0][0]
__________________________________________________________________________________________________
conv4_block7_0_relu (Activation (None, 23, 35, 448)  0           conv4_block7_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block7_1_conv (Conv2D)    (None, 23, 35, 128)  57344       conv4_block7_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block7_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block7_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block7_1_relu (Activation (None, 23, 35, 128)  0           conv4_block7_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block7_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block7_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block7_concat (Concatenat (None, 23, 35, 480)  0           conv4_block6_concat[0][0]
                                                                 conv4_block7_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block8_0_bn (BatchNormali (None, 23, 35, 480)  1920        conv4_block7_concat[0][0]
__________________________________________________________________________________________________
conv4_block8_0_relu (Activation (None, 23, 35, 480)  0           conv4_block8_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block8_1_conv (Conv2D)    (None, 23, 35, 128)  61440       conv4_block8_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block8_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block8_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block8_1_relu (Activation (None, 23, 35, 128)  0           conv4_block8_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block8_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block8_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block8_concat (Concatenat (None, 23, 35, 512)  0           conv4_block7_concat[0][0]
                                                                 conv4_block8_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block9_0_bn (BatchNormali (None, 23, 35, 512)  2048        conv4_block8_concat[0][0]
__________________________________________________________________________________________________
conv4_block9_0_relu (Activation (None, 23, 35, 512)  0           conv4_block9_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block9_1_conv (Conv2D)    (None, 23, 35, 128)  65536       conv4_block9_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block9_1_bn (BatchNormali (None, 23, 35, 128)  512         conv4_block9_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block9_1_relu (Activation (None, 23, 35, 128)  0           conv4_block9_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block9_2_conv (Conv2D)    (None, 23, 35, 32)   36864       conv4_block9_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block9_concat (Concatenat (None, 23, 35, 544)  0           conv4_block8_concat[0][0]
                                                                 conv4_block9_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block10_0_bn (BatchNormal (None, 23, 35, 544)  2176        conv4_block9_concat[0][0]
__________________________________________________________________________________________________
conv4_block10_0_relu (Activatio (None, 23, 35, 544)  0           conv4_block10_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block10_1_conv (Conv2D)   (None, 23, 35, 128)  69632       conv4_block10_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block10_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block10_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block10_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block10_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block10_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block10_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block10_concat (Concatena (None, 23, 35, 576)  0           conv4_block9_concat[0][0]
                                                                 conv4_block10_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block11_0_bn (BatchNormal (None, 23, 35, 576)  2304        conv4_block10_concat[0][0]
__________________________________________________________________________________________________
conv4_block11_0_relu (Activatio (None, 23, 35, 576)  0           conv4_block11_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block11_1_conv (Conv2D)   (None, 23, 35, 128)  73728       conv4_block11_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block11_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block11_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block11_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block11_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block11_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block11_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block11_concat (Concatena (None, 23, 35, 608)  0           conv4_block10_concat[0][0]
                                                                 conv4_block11_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block12_0_bn (BatchNormal (None, 23, 35, 608)  2432        conv4_block11_concat[0][0]
__________________________________________________________________________________________________
conv4_block12_0_relu (Activatio (None, 23, 35, 608)  0           conv4_block12_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block12_1_conv (Conv2D)   (None, 23, 35, 128)  77824       conv4_block12_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block12_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block12_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block12_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block12_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block12_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block12_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block12_concat (Concatena (None, 23, 35, 640)  0           conv4_block11_concat[0][0]
                                                                 conv4_block12_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block13_0_bn (BatchNormal (None, 23, 35, 640)  2560        conv4_block12_concat[0][0]
__________________________________________________________________________________________________
conv4_block13_0_relu (Activatio (None, 23, 35, 640)  0           conv4_block13_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block13_1_conv (Conv2D)   (None, 23, 35, 128)  81920       conv4_block13_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block13_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block13_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block13_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block13_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block13_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block13_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block13_concat (Concatena (None, 23, 35, 672)  0           conv4_block12_concat[0][0]
                                                                 conv4_block13_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block14_0_bn (BatchNormal (None, 23, 35, 672)  2688        conv4_block13_concat[0][0]
__________________________________________________________________________________________________
conv4_block14_0_relu (Activatio (None, 23, 35, 672)  0           conv4_block14_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block14_1_conv (Conv2D)   (None, 23, 35, 128)  86016       conv4_block14_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block14_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block14_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block14_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block14_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block14_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block14_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block14_concat (Concatena (None, 23, 35, 704)  0           conv4_block13_concat[0][0]
                                                                 conv4_block14_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block15_0_bn (BatchNormal (None, 23, 35, 704)  2816        conv4_block14_concat[0][0]
__________________________________________________________________________________________________
conv4_block15_0_relu (Activatio (None, 23, 35, 704)  0           conv4_block15_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block15_1_conv (Conv2D)   (None, 23, 35, 128)  90112       conv4_block15_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block15_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block15_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block15_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block15_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block15_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block15_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block15_concat (Concatena (None, 23, 35, 736)  0           conv4_block14_concat[0][0]
                                                                 conv4_block15_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block16_0_bn (BatchNormal (None, 23, 35, 736)  2944        conv4_block15_concat[0][0]
__________________________________________________________________________________________________
conv4_block16_0_relu (Activatio (None, 23, 35, 736)  0           conv4_block16_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block16_1_conv (Conv2D)   (None, 23, 35, 128)  94208       conv4_block16_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block16_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block16_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block16_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block16_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block16_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block16_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block16_concat (Concatena (None, 23, 35, 768)  0           conv4_block15_concat[0][0]
                                                                 conv4_block16_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block17_0_bn (BatchNormal (None, 23, 35, 768)  3072        conv4_block16_concat[0][0]
__________________________________________________________________________________________________
conv4_block17_0_relu (Activatio (None, 23, 35, 768)  0           conv4_block17_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block17_1_conv (Conv2D)   (None, 23, 35, 128)  98304       conv4_block17_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block17_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block17_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block17_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block17_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block17_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block17_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block17_concat (Concatena (None, 23, 35, 800)  0           conv4_block16_concat[0][0]
                                                                 conv4_block17_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block18_0_bn (BatchNormal (None, 23, 35, 800)  3200        conv4_block17_concat[0][0]
__________________________________________________________________________________________________
conv4_block18_0_relu (Activatio (None, 23, 35, 800)  0           conv4_block18_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block18_1_conv (Conv2D)   (None, 23, 35, 128)  102400      conv4_block18_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block18_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block18_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block18_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block18_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block18_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block18_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block18_concat (Concatena (None, 23, 35, 832)  0           conv4_block17_concat[0][0]
                                                                 conv4_block18_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block19_0_bn (BatchNormal (None, 23, 35, 832)  3328        conv4_block18_concat[0][0]
__________________________________________________________________________________________________
conv4_block19_0_relu (Activatio (None, 23, 35, 832)  0           conv4_block19_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block19_1_conv (Conv2D)   (None, 23, 35, 128)  106496      conv4_block19_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block19_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block19_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block19_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block19_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block19_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block19_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block19_concat (Concatena (None, 23, 35, 864)  0           conv4_block18_concat[0][0]
                                                                 conv4_block19_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block20_0_bn (BatchNormal (None, 23, 35, 864)  3456        conv4_block19_concat[0][0]
__________________________________________________________________________________________________
conv4_block20_0_relu (Activatio (None, 23, 35, 864)  0           conv4_block20_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block20_1_conv (Conv2D)   (None, 23, 35, 128)  110592      conv4_block20_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block20_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block20_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block20_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block20_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block20_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block20_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block20_concat (Concatena (None, 23, 35, 896)  0           conv4_block19_concat[0][0]
                                                                 conv4_block20_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block21_0_bn (BatchNormal (None, 23, 35, 896)  3584        conv4_block20_concat[0][0]
__________________________________________________________________________________________________
conv4_block21_0_relu (Activatio (None, 23, 35, 896)  0           conv4_block21_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block21_1_conv (Conv2D)   (None, 23, 35, 128)  114688      conv4_block21_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block21_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block21_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block21_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block21_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block21_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block21_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block21_concat (Concatena (None, 23, 35, 928)  0           conv4_block20_concat[0][0]
                                                                 conv4_block21_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block22_0_bn (BatchNormal (None, 23, 35, 928)  3712        conv4_block21_concat[0][0]
__________________________________________________________________________________________________
conv4_block22_0_relu (Activatio (None, 23, 35, 928)  0           conv4_block22_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block22_1_conv (Conv2D)   (None, 23, 35, 128)  118784      conv4_block22_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block22_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block22_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block22_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block22_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block22_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block22_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block22_concat (Concatena (None, 23, 35, 960)  0           conv4_block21_concat[0][0]
                                                                 conv4_block22_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block23_0_bn (BatchNormal (None, 23, 35, 960)  3840        conv4_block22_concat[0][0]
__________________________________________________________________________________________________
conv4_block23_0_relu (Activatio (None, 23, 35, 960)  0           conv4_block23_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block23_1_conv (Conv2D)   (None, 23, 35, 128)  122880      conv4_block23_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block23_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block23_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block23_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block23_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block23_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block23_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block23_concat (Concatena (None, 23, 35, 992)  0           conv4_block22_concat[0][0]
                                                                 conv4_block23_2_conv[0][0]
__________________________________________________________________________________________________
conv4_block24_0_bn (BatchNormal (None, 23, 35, 992)  3968        conv4_block23_concat[0][0]
__________________________________________________________________________________________________
conv4_block24_0_relu (Activatio (None, 23, 35, 992)  0           conv4_block24_0_bn[0][0]
__________________________________________________________________________________________________
conv4_block24_1_conv (Conv2D)   (None, 23, 35, 128)  126976      conv4_block24_0_relu[0][0]
__________________________________________________________________________________________________
conv4_block24_1_bn (BatchNormal (None, 23, 35, 128)  512         conv4_block24_1_conv[0][0]
__________________________________________________________________________________________________
conv4_block24_1_relu (Activatio (None, 23, 35, 128)  0           conv4_block24_1_bn[0][0]
__________________________________________________________________________________________________
conv4_block24_2_conv (Conv2D)   (None, 23, 35, 32)   36864       conv4_block24_1_relu[0][0]
__________________________________________________________________________________________________
conv4_block24_concat (Concatena (None, 23, 35, 1024) 0           conv4_block23_concat[0][0]
                                                                 conv4_block24_2_conv[0][0]
__________________________________________________________________________________________________
pool4_bn (BatchNormalization)   (None, 23, 35, 1024) 4096        conv4_block24_concat[0][0]
__________________________________________________________________________________________________
pool4_relu (Activation)         (None, 23, 35, 1024) 0           pool4_bn[0][0]
__________________________________________________________________________________________________
pool4_conv (Conv2D)             (None, 23, 35, 512)  524288      pool4_relu[0][0]
__________________________________________________________________________________________________
pool4_pool (AveragePooling2D)   (None, 11, 17, 512)  0           pool4_conv[0][0]
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 11, 17, 512)  2048        pool4_pool[0][0]
__________________________________________________________________________________________________
conv5_block1_0_relu (Activation (None, 11, 17, 512)  0           conv5_block1_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 11, 17, 128)  65536       conv5_block1_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block1_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 11, 17, 128)  0           conv5_block1_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block1_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block1_concat (Concatenat (None, 11, 17, 544)  0           pool4_pool[0][0]
                                                                 conv5_block1_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block2_0_bn (BatchNormali (None, 11, 17, 544)  2176        conv5_block1_concat[0][0]
__________________________________________________________________________________________________
conv5_block2_0_relu (Activation (None, 11, 17, 544)  0           conv5_block2_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 11, 17, 128)  69632       conv5_block2_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block2_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 11, 17, 128)  0           conv5_block2_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block2_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block2_concat (Concatenat (None, 11, 17, 576)  0           conv5_block1_concat[0][0]
                                                                 conv5_block2_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block3_0_bn (BatchNormali (None, 11, 17, 576)  2304        conv5_block2_concat[0][0]
__________________________________________________________________________________________________
conv5_block3_0_relu (Activation (None, 11, 17, 576)  0           conv5_block3_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 11, 17, 128)  73728       conv5_block3_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block3_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 11, 17, 128)  0           conv5_block3_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block3_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block3_concat (Concatenat (None, 11, 17, 608)  0           conv5_block2_concat[0][0]
                                                                 conv5_block3_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block4_0_bn (BatchNormali (None, 11, 17, 608)  2432        conv5_block3_concat[0][0]
__________________________________________________________________________________________________
conv5_block4_0_relu (Activation (None, 11, 17, 608)  0           conv5_block4_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block4_1_conv (Conv2D)    (None, 11, 17, 128)  77824       conv5_block4_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block4_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block4_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block4_1_relu (Activation (None, 11, 17, 128)  0           conv5_block4_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block4_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block4_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block4_concat (Concatenat (None, 11, 17, 640)  0           conv5_block3_concat[0][0]
                                                                 conv5_block4_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block5_0_bn (BatchNormali (None, 11, 17, 640)  2560        conv5_block4_concat[0][0]
__________________________________________________________________________________________________
conv5_block5_0_relu (Activation (None, 11, 17, 640)  0           conv5_block5_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block5_1_conv (Conv2D)    (None, 11, 17, 128)  81920       conv5_block5_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block5_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block5_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block5_1_relu (Activation (None, 11, 17, 128)  0           conv5_block5_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block5_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block5_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block5_concat (Concatenat (None, 11, 17, 672)  0           conv5_block4_concat[0][0]
                                                                 conv5_block5_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block6_0_bn (BatchNormali (None, 11, 17, 672)  2688        conv5_block5_concat[0][0]
__________________________________________________________________________________________________
conv5_block6_0_relu (Activation (None, 11, 17, 672)  0           conv5_block6_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block6_1_conv (Conv2D)    (None, 11, 17, 128)  86016       conv5_block6_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block6_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block6_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block6_1_relu (Activation (None, 11, 17, 128)  0           conv5_block6_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block6_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block6_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block6_concat (Concatenat (None, 11, 17, 704)  0           conv5_block5_concat[0][0]
                                                                 conv5_block6_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block7_0_bn (BatchNormali (None, 11, 17, 704)  2816        conv5_block6_concat[0][0]
__________________________________________________________________________________________________
conv5_block7_0_relu (Activation (None, 11, 17, 704)  0           conv5_block7_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block7_1_conv (Conv2D)    (None, 11, 17, 128)  90112       conv5_block7_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block7_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block7_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block7_1_relu (Activation (None, 11, 17, 128)  0           conv5_block7_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block7_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block7_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block7_concat (Concatenat (None, 11, 17, 736)  0           conv5_block6_concat[0][0]
                                                                 conv5_block7_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block8_0_bn (BatchNormali (None, 11, 17, 736)  2944        conv5_block7_concat[0][0]
__________________________________________________________________________________________________
conv5_block8_0_relu (Activation (None, 11, 17, 736)  0           conv5_block8_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block8_1_conv (Conv2D)    (None, 11, 17, 128)  94208       conv5_block8_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block8_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block8_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block8_1_relu (Activation (None, 11, 17, 128)  0           conv5_block8_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block8_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block8_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block8_concat (Concatenat (None, 11, 17, 768)  0           conv5_block7_concat[0][0]
                                                                 conv5_block8_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block9_0_bn (BatchNormali (None, 11, 17, 768)  3072        conv5_block8_concat[0][0]
__________________________________________________________________________________________________
conv5_block9_0_relu (Activation (None, 11, 17, 768)  0           conv5_block9_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block9_1_conv (Conv2D)    (None, 11, 17, 128)  98304       conv5_block9_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block9_1_bn (BatchNormali (None, 11, 17, 128)  512         conv5_block9_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block9_1_relu (Activation (None, 11, 17, 128)  0           conv5_block9_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block9_2_conv (Conv2D)    (None, 11, 17, 32)   36864       conv5_block9_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block9_concat (Concatenat (None, 11, 17, 800)  0           conv5_block8_concat[0][0]
                                                                 conv5_block9_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block10_0_bn (BatchNormal (None, 11, 17, 800)  3200        conv5_block9_concat[0][0]
__________________________________________________________________________________________________
conv5_block10_0_relu (Activatio (None, 11, 17, 800)  0           conv5_block10_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block10_1_conv (Conv2D)   (None, 11, 17, 128)  102400      conv5_block10_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block10_1_bn (BatchNormal (None, 11, 17, 128)  512         conv5_block10_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block10_1_relu (Activatio (None, 11, 17, 128)  0           conv5_block10_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block10_2_conv (Conv2D)   (None, 11, 17, 32)   36864       conv5_block10_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block10_concat (Concatena (None, 11, 17, 832)  0           conv5_block9_concat[0][0]
                                                                 conv5_block10_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block11_0_bn (BatchNormal (None, 11, 17, 832)  3328        conv5_block10_concat[0][0]
__________________________________________________________________________________________________
conv5_block11_0_relu (Activatio (None, 11, 17, 832)  0           conv5_block11_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block11_1_conv (Conv2D)   (None, 11, 17, 128)  106496      conv5_block11_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block11_1_bn (BatchNormal (None, 11, 17, 128)  512         conv5_block11_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block11_1_relu (Activatio (None, 11, 17, 128)  0           conv5_block11_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block11_2_conv (Conv2D)   (None, 11, 17, 32)   36864       conv5_block11_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block11_concat (Concatena (None, 11, 17, 864)  0           conv5_block10_concat[0][0]
                                                                 conv5_block11_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block12_0_bn (BatchNormal (None, 11, 17, 864)  3456        conv5_block11_concat[0][0]
__________________________________________________________________________________________________
conv5_block12_0_relu (Activatio (None, 11, 17, 864)  0           conv5_block12_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block12_1_conv (Conv2D)   (None, 11, 17, 128)  110592      conv5_block12_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block12_1_bn (BatchNormal (None, 11, 17, 128)  512         conv5_block12_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block12_1_relu (Activatio (None, 11, 17, 128)  0           conv5_block12_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block12_2_conv (Conv2D)   (None, 11, 17, 32)   36864       conv5_block12_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block12_concat (Concatena (None, 11, 17, 896)  0           conv5_block11_concat[0][0]
                                                                 conv5_block12_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block13_0_bn (BatchNormal (None, 11, 17, 896)  3584        conv5_block12_concat[0][0]
__________________________________________________________________________________________________
conv5_block13_0_relu (Activatio (None, 11, 17, 896)  0           conv5_block13_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block13_1_conv (Conv2D)   (None, 11, 17, 128)  114688      conv5_block13_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block13_1_bn (BatchNormal (None, 11, 17, 128)  512         conv5_block13_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block13_1_relu (Activatio (None, 11, 17, 128)  0           conv5_block13_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block13_2_conv (Conv2D)   (None, 11, 17, 32)   36864       conv5_block13_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block13_concat (Concatena (None, 11, 17, 928)  0           conv5_block12_concat[0][0]
                                                                 conv5_block13_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block14_0_bn (BatchNormal (None, 11, 17, 928)  3712        conv5_block13_concat[0][0]
__________________________________________________________________________________________________
conv5_block14_0_relu (Activatio (None, 11, 17, 928)  0           conv5_block14_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block14_1_conv (Conv2D)   (None, 11, 17, 128)  118784      conv5_block14_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block14_1_bn (BatchNormal (None, 11, 17, 128)  512         conv5_block14_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block14_1_relu (Activatio (None, 11, 17, 128)  0           conv5_block14_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block14_2_conv (Conv2D)   (None, 11, 17, 32)   36864       conv5_block14_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block14_concat (Concatena (None, 11, 17, 960)  0           conv5_block13_concat[0][0]
                                                                 conv5_block14_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block15_0_bn (BatchNormal (None, 11, 17, 960)  3840        conv5_block14_concat[0][0]
__________________________________________________________________________________________________
conv5_block15_0_relu (Activatio (None, 11, 17, 960)  0           conv5_block15_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block15_1_conv (Conv2D)   (None, 11, 17, 128)  122880      conv5_block15_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block15_1_bn (BatchNormal (None, 11, 17, 128)  512         conv5_block15_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block15_1_relu (Activatio (None, 11, 17, 128)  0           conv5_block15_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block15_2_conv (Conv2D)   (None, 11, 17, 32)   36864       conv5_block15_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block15_concat (Concatena (None, 11, 17, 992)  0           conv5_block14_concat[0][0]
                                                                 conv5_block15_2_conv[0][0]
__________________________________________________________________________________________________
conv5_block16_0_bn (BatchNormal (None, 11, 17, 992)  3968        conv5_block15_concat[0][0]
__________________________________________________________________________________________________
conv5_block16_0_relu (Activatio (None, 11, 17, 992)  0           conv5_block16_0_bn[0][0]
__________________________________________________________________________________________________
conv5_block16_1_conv (Conv2D)   (None, 11, 17, 128)  126976      conv5_block16_0_relu[0][0]
__________________________________________________________________________________________________
conv5_block16_1_bn (BatchNormal (None, 11, 17, 128)  512         conv5_block16_1_conv[0][0]
__________________________________________________________________________________________________
conv5_block16_1_relu (Activatio (None, 11, 17, 128)  0           conv5_block16_1_bn[0][0]
__________________________________________________________________________________________________
conv5_block16_2_conv (Conv2D)   (None, 11, 17, 32)   36864       conv5_block16_1_relu[0][0]
__________________________________________________________________________________________________
conv5_block16_concat (Concatena (None, 11, 17, 1024) 0           conv5_block15_concat[0][0]
                                                                 conv5_block16_2_conv[0][0]
__________________________________________________________________________________________________
bn (BatchNormalization)         (None, 11, 17, 1024) 4096        conv5_block16_concat[0][0]
__________________________________________________________________________________________________
relu (Activation)               (None, 11, 17, 1024) 0           bn[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 1024)         0           relu[0][0]
__________________________________________________________________________________________________
fc1000 (Dense)                  (None, 10)           10250       avg_pool[0][0]
==================================================================================================
Total params: 7,047,754
Trainable params: 6,964,106
Non-trainable params: 83,648
__________________________________________________________________________________________________
Train for 100 steps, validate for 10 steps
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/normalization.py:477: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2019-09-19 11:25:34.482086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcublas.so.10.0
2019-09-19 11:25:34.711640: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamiclibrary libcudnn.so.7
2019-09-19 11:25:35.685779: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
</pre>
</details>

If I remove the `MirroredStrategy` scope, the code runs successfully and does not hang (doing meaningless training).

### Investigation

<details>
<summary><code>top</code></summary>
<pre>
 3161 root      20   0  0.112t 0.013t 948384 S  24.0  5.3 181:17.23 python3
</pre>
</details>

`nvidia-smi`'s output is the same that I used in the ""System information"": all the GPUs are constantly 100% busy.

<details>
<summary><code>top -H -p 3161</code> - threads of the running process</summary>
<pre>
 Threads: 155 total,   0 running, 155 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.9 us,  0.8 sy,  0.0 ni, 97.8 id,  0.0 wa,  0.3 hi,  0.2 si,  0.0 st
KiB Mem : 26408952+total, 99229216 free, 21207464 used, 14365283+buff/cache
KiB Swap:        0 total,        0 free,        0 used. 20145740+avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND
 3261 root      20   0  0.112t 0.013t 948360 S  6.3  5.3  42:18.36 python3
 3255 root      20   0  0.112t 0.013t 948360 S  6.0  5.3  41:49.75 python3
 3259 root      20   0  0.112t 0.013t 948360 S  6.0  5.3  42:09.41 python3
 3257 root      20   0  0.112t 0.013t 948360 S  5.6  5.3  42:10.03 python3
 3161 root      20   0  0.112t 0.013t 948360 S  0.0  5.3   2:11.62 python3
 3165 root      20   0  0.112t 0.013t 948360 S  0.0  5.3   0:00.00 python3
 3166 root      20   0  0.112t 0.013t 948360 S  0.0  5.3   0:15.45 python3
...
</pre>
</details>

<details>
<summary><code>bt</code> in <code>gdb --pid 3161</code> - trace of the main thread</summary>
<pre>
#0  0x00007f26924c5839 in syscall () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007f264b30e53b in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#2  0x00007f264b30db59 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#3  0x00007f264b30b11b in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#4  0x00007f264b30b5f3 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#5  0x00007f264344f60c in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, absl::InlinedVector<tensorflow::TensorValue, 4ul, std::allocator<tensorflow::TensorValue> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::NodeExecStats*, tensorflow::StepStats*, tensorflow::GraphCollector*, tensorflow::CancellationManager*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#6  0x00007f264344fa06 in tensorflow::KernelAndDeviceFunc::Run(absl::InlinedVector<tensorflow::TensorValue, 4ul, std::allocator<tensorflow::TensorValue> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::NodeExecStats*, tensorflow::StepStats*, tensorflow::GraphCollector*, tensorflow::CancellationManager*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#7  0x00007f26434313f6 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::unique_ptr<tensorflow::KernelAndDevice, tensorflow::core::RefCountDeleter> const&, tensorflow::NodeExecStats*, tensorflow::StepStats*, tensorflow::GraphCollector*, tensorflow::CancellationManager*, absl::Span<tensorflow::TensorHandle*>) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#8  0x00007f2643431aed in tensorflow::ExecuteNode::Run() ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#9  0x00007f264346ca85 in tensorflow::EagerExecutor::RunItem(std::unique_ptr<tensorflow::EagerExecutor::NodeItem, tensorflow::core::RefCountDeleter>) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#10 0x00007f264346d18d in tensorflow::EagerExecutor::AddOrExecute(std::unique_ptr<tensorflow::EagerNode, std::default_delete<tensorflow::EagerNode> >) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#11 0x00007f264342cd86 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#12 0x00007f264342ed00 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) ()
---Type <return> to continue, or q <return> to quit---
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#13 0x00007f26432bc05d in TFE_Execute ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#14 0x00007f264324640c in TFE_Py_ExecuteCancelable(TFE_Context*, char const*, char const*, absl::InlinedVector<TFE_TensorHandle*, 4ul, std::allocator<TFE_TensorHandle*> >*, _object*, TFE_CancellationManager*, absl::InlinedVector<TFE_TensorHandle*, 2ul, std::allocator<TFE_TensorHandle*> >*, TF_Status*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#15 0x00007f2643246941 in TFE_Py_Execute(TFE_Context*, char const*, char const*, absl::InlinedVector<TFE_TensorHandle*,4ul, std::allocator<TFE_TensorHandle*> >*, _object*, absl::InlinedVector<TFE_TensorHandle*, 2ul, std::allocator<TFE_TensorHandle*> >*, TF_Status*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#16 0x00007f2642ddeb34 in _wrap_TFE_Py_Execute ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#17 0x00000000005097cf in _PyCFunction_FastCallDict (kwargs=<optimized out>, nargs=<optimized out>,
    args=<optimized out>, func_obj=<built-in method TFE_Py_Execute of module object at remote 0x7f26805d2778>)
    at ../Objects/methodobject.c:234
#18 _PyCFunction_FastCallKeywords (kwnames=<optimized out>, nargs=<optimized out>, stack=<optimized out>,
    func=<optimized out>) at ../Objects/methodobject.c:294
#19 call_function.lto_priv () at ../Python/ceval.c:4851
#20 0x000000000050b4a9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335
#21 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0, f=
    Frame 0x62d109a8, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py, line 61,in quick_execute (op_name='__inference_distributed_function_164755', num_outputs=3, inputs=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f256431f198>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f256431f2e8>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f25642d2c18>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f263badc6d8>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f260c506cc0>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f260c50f8d0>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f260c506780>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f260c49d2e8>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f260c50fc18>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f260c420d68>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7f260c420630>, <tensorflow.python.frame...(truncated)) at ../Python/ceval.c:754
#22 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166
#23 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992
#24 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872
#25 0x000000000050c36e in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3351
#26 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,
    f=Frame 0x71ccbef8, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py, line 4---Type <return> to continue, or q <return> to quit---
95, in call (self=<_EagerDefinedFunction(name=b'__inference_distributed_function_164755', _function_deleter=<_EagerDefinedFunctionDeleter(name=b'__inference_distributed_function_164755') at remote 0x7f1e0e0df438>, _registered_on_context=True, definition=<FunctionDef at remote 0x7f24bc06bfa8>, signature=<OpDef at remote 0x7f24bc06bef8>, _num_outputs=3, _output_types=[9, 1, 1], _output_shapes=[<TensorShape(_dims=[]) at remote 0x7f2384537a90>, <TensorShape(_dims=[]) at remote 0x7f2384537518>, <TensorShape(_dims=[]) at remote 0x7f2384537e80>], _control_captures=set(), _func_graph_outputs=[<Tensor(_op=<Operation(_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f25642c78d0>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lock at remote 0x7f24c4746288>, acquire=<built-in method acquire of _thread.lock object at remote 0x7f24c4746288>, release=<built-in method release of _thread.lock object at...(truncated)) at ../Python/ceval.c:754
#27 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166
#28 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992
#29 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872
#30 0x000000000050c36e in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3351
#31 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,
    f=Frame 0x71ccb5b8, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py, line 1600, in _call_flat (self=<ConcreteFunction(_arg_keywords=None, _num_positional_args=None, _func_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f25642c78d0>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lock at remote 0x7f24c4746288>, acquire=<built-in method acquire of _thread.lock object at remote 0x7f24c4746288>, release=<built-in method release of _thread.lock object at remote 0x7f24c4746288>, _waiters=<collections.deque at remote 0x7f24e44428d0>) at remote0x7f2384537f60>, _num_groups=2, _group_member_counts=[0, 0]) at remote 0x7f2384537c88>, _nodes_by_id={1: <Operation(_graph=<...>, _inputs_val=(), _id_value=1, _original_op=None, _traceback=<tensorflow_core.python._tf_stack.StackSummary at remote 0x7f23844c6fb8>, _device_code_locations=[<TraceableObject(obj='/job:localhost/replica:0/task:0/device:GPU:0', filename='/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/fr...(truncated)) at ../Python/ceval.c:754
#32 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166
#33 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992
#34 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872
#35 0x000000000050b4a9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335
#36 0x0000000000508c69 in PyEval_EvalFrameEx (throwflag=0,
    f=Frame 0x7f18b8000b38, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py, line 1515, in _filtered_call (self=<ConcreteFunction(_arg_keywords=None, _num_positional_args=None, _func_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f25642c78d0>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lock at remote0x7f24c4746288>, acquire=<built-in method acquire of _thread.lock object at remote 0x7f24c4746288>, release=<built-in method release of _thread.lock object at remote 0x7f24c4746288>, _waiters=<collections.deque at remote 0x7f24e44428d0>) at remote 0x7f2384537f60>, _num_groups=2, _group_member_counts=[0, 0]) at remote 0x7f2384537c88>, _nodes_by_id={1: <Operation(_graph=<...>, _inputs_val=(), _id_value=1, _original_op=None, _traceback=<tensorflow_core.python._tf_stack.StackSummary at remote 0x7f23844c6fb8>, _device_code_locations=[<TraceableObject(obj='/job:localhost/replica:0/task:0/device:GPU:0', filename='/usr/local/lib/python3.6/dist-packages/tensorflow_core/p...(truncated)) at ../Python/ceval.c:754
---Type <return> to continue, or q <return> to quit---
#37 _PyFunction_FastCall (globals=<optimized out>, nargs=139744142953272, args=<optimized out>, co=<optimized out>)
    at ../Python/ceval.c:4933
#38 fast_function.lto_priv () at ../Python/ceval.c:4968
#39 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872
#40 0x000000000050b4a9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335
#41 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,
    f=Frame 0x1d37bb48, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py, line 2237, in __call__ (self=<Function(_python_function=<function at remote 0x7f2635ff3a60>, _function_spec=<FunctionSpec(_fullargspec=<FullArgSpec at remote 0x7f24942b4eb8>, _is_method=False, _default_values=None, _args_to_indices={'input_iterator': 0}, arg_names=['input_iterator'], vararg_name=None, _arg_indices_to_default_values={}, _input_signature=None) at remote 0x7f25642e3630>, _name='distributed_function', _autograph=False, _autograph_options=None, _experimental_relax_shapes=False, _function_cache=<FunctionCache(missed={<CacheKey at remote 0x7f244a21be28>}, primary={<CacheKey at remote 0x7f244a21bd68>: <ConcreteFunction(_arg_keywords=None, _num_positional_args=None, _func_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f25642c78d0>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lock at remote 0x7f24c4746288>, acquire=<built-in method acquire of _thread.lock object at remote 0x7f24c4...(truncated)) at ../Python/ceval.c:754
#42 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166
#43 0x0000000000508794 in _PyFunction_FastCallDict () at ../Python/ceval.c:5084
#44 0x00000000005940d1 in _PyObject_FastCallDict (kwargs={}, nargs=2, args=0x7ffcaa451a50,
    func=<function at remote 0x7f263bd949d8>) at ../Objects/abstract.c:2310
#45 _PyObject_Call_Prepend (kwargs={}, args=<optimized out>, obj=<optimized out>,
    func=<function at remote 0x7f263bd949d8>) at ../Objects/abstract.c:2373
#46 method_call.lto_priv () at ../Objects/classobject.c:314
#47 0x0000000000549f41 in PyObject_Call (kwargs={},
    args=(<DistributedIterator(_enable_get_next_as_optional=False, _iterators=[<_SingleWorkerDatasetIterator(_dataset=<_AutoShardDataset(_input_dataset=<_OptionsDataset(_input_dataset=<_OptionsDataset(_input_dataset=<PrefetchDataset(_input_dataset=<_RebatchDataset(_input_dataset=<MapDataset(_input_dataset=<BatchDataset(_input_dataset=<RepeatDataset(_input_dataset=<MapDataset(_input_dataset=<TensorDataset(_structure=<TensorSpec at remote 0x7f26295ffe10>, _tensors=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d514438>], _variant_tensor_attr=<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d5148d0>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[<TrackableReference at remote 0x7f26295ffd80>], _self_unconditional_dependency_names={'_variant_tracker': <_VariantTracker(_resource_handle=<...>, _resource_device='CPU', _resource_deleter=<CapturableResourceDeleter(_destroy_resource=None) at remote 0x7f263afb4400>, _create_resource=<function at remote 0x7f263bb23620>, _sel...(truncated),
    func=<method at remote 0x7f25643a5d88>) at ../Objects/abstract.c:2261
#48 slot_tp_call () at ../Objects/typeobject.c:6207
#49 0x000000000059f50e in PyObject_Call () at ../Objects/abstract.c:2261
#50 0x000000000050c854 in do_call_core (kwdict={},
---Type <return> to continue, or q <return> to quit---
    callargs=(<DistributedIterator(_enable_get_next_as_optional=False, _iterators=[<_SingleWorkerDatasetIterator(_dataset=<_AutoShardDataset(_input_dataset=<_OptionsDataset(_input_dataset=<_OptionsDataset(_input_dataset=<PrefetchDataset(_input_dataset=<_RebatchDataset(_input_dataset=<MapDataset(_input_dataset=<BatchDataset(_input_dataset=<RepeatDataset(_input_dataset=<MapDataset(_input_dataset=<TensorDataset(_structure=<TensorSpec at remote 0x7f26295ffe10>, _tensors=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d514438>], _variant_tensor_attr=<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d5148d0>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[<TrackableReference at remote 0x7f26295ffd80>], _self_unconditional_dependency_names={'_variant_tracker': <_VariantTracker(_resource_handle=<...>, _resource_device='CPU', _resource_deleter=<CapturableResourceDeleter(_destroy_resource=None) at remote 0x7f263afb4400>, _create_resource=<function at remote 0x7f263bb23620>, _sel...(truncated),
    func=<Function(_python_function=<function at remote 0x7f2635ff3a60>, _function_spec=<FunctionSpec(_fullargspec=<FullArgSpec at remote 0x7f24942b4eb8>, _is_method=False, _default_values=None, _args_to_indices={'input_iterator': 0}, arg_names=['input_iterator'], vararg_name=None, _arg_indices_to_default_values={}, _input_signature=None) at remote 0x7f25642e3630>, _name='distributed_function', _autograph=False, _autograph_options=None, _experimental_relax_shapes=False, _function_cache=<FunctionCache(missed={<CacheKey at remote 0x7f244a21be28>}, primary={<CacheKey at remote 0x7f244a21bd68>: <ConcreteFunction(_arg_keywords=None, _num_positional_args=None, _func_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f25642c78d0>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lock at remote 0x7f24c4746288>, acquire=<built-inmethod acquire of _thread.lock object at remote 0x7f24c4746288>, release=<built-in method release of _thread.lock object at remote 0x7f24c4746288>, _waiters=<collections.deque at remote 0x7f24e...(truncated)) at ../Python/ceval.c:5120
#51 _PyEval_EvalFrameDefault () at ../Python/ceval.c:3404
#52 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,
    f=Frame 0x68702018, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py, line 543, in _call (args=(<DistributedIterator(_enable_get_next_as_optional=False, _iterators=[<_SingleWorkerDatasetIterator(_dataset=<_AutoShardDataset(_input_dataset=<_OptionsDataset(_input_dataset=<_OptionsDataset(_input_dataset=<PrefetchDataset(_input_dataset=<_RebatchDataset(_input_dataset=<MapDataset(_input_dataset=<BatchDataset(_input_dataset=<RepeatDataset(_input_dataset=<MapDataset(_input_dataset=<TensorDataset(_structure=<TensorSpec at remote 0x7f26295ffe10>, _tensors=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d514438>], _variant_tensor_attr=<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d5148d0>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[<TrackableReference at remote 0x7f26295ffd80>], _self_unconditional_dependency_names={'_variant_tracker': <_VariantTracker(_resource_handle=<...>, _resource_device='CPU', _resource_deleter...(truncated)) at ../Python/ceval.c:754
#53 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166
#54 0x0000000000508794 in _PyFunction_FastCallDict () at ../Python/ceval.c:5084
#55 0x00000000005940d1 in _PyObject_FastCallDict (kwargs={}, nargs=2, args=0x7ffcaa451e10,
    func=<function at remote 0x7f263bdae048>) at ../Objects/abstract.c:2310
#56 _PyObject_Call_Prepend (kwargs={}, args=<optimized out>, obj=<optimized out>,
    func=<function at remote 0x7f263bdae048>) at ../Objects/abstract.c:2373
#57 method_call.lto_priv () at ../Objects/classobject.c:314
---Type <return> to continue, or q <return> to quit---
#58 0x000000000059f50e in PyObject_Call () at ../Objects/abstract.c:2261
#59 0x000000000050c854 in do_call_core (kwdict={},
    callargs=(<DistributedIterator(_enable_get_next_as_optional=False, _iterators=[<_SingleWorkerDatasetIterator(_dataset=<_AutoShardDataset(_input_dataset=<_OptionsDataset(_input_dataset=<_OptionsDataset(_input_dataset=<PrefetchDataset(_input_dataset=<_RebatchDataset(_input_dataset=<MapDataset(_input_dataset=<BatchDataset(_input_dataset=<RepeatDataset(_input_dataset=<MapDataset(_input_dataset=<TensorDataset(_structure=<TensorSpec at remote 0x7f26295ffe10>, _tensors=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d514438>], _variant_tensor_attr=<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d5148d0>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[<TrackableReference at remote 0x7f26295ffd80>], _self_unconditional_dependency_names={'_variant_tracker': <_VariantTracker(_resource_handle=<...>, _resource_device='CPU', _resource_deleter=<CapturableResourceDeleter(_destroy_resource=None) at remote 0x7f263afb4400>, _create_resource=<function at remote 0x7f263bb23620>, _sel...(truncated),
    func=<method at remote 0x7f25b05c7f88>) at ../Python/ceval.c:5120
#60 _PyEval_EvalFrameDefault () at ../Python/ceval.c:3404
#61 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,
    f=Frame 0x7f2564359dd8, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py, line 480, in __call__ (self=<Function(_lock=<_thread.lock at remote 0x7f2564374df0>, _python_function=<function at remote 0x7f2564495f28>, _function_spec=<FunctionSpec(_fullargspec=<FullArgSpec at remote 0x7f25644326d8>, _is_method=False, _default_values=None, _args_to_indices={'input_iterator': 0}, arg_names=['input_iterator'], vararg_name=None, _arg_indices_to_default_values={}, _input_signature=None) at remote 0x7f256435b400>, _autograph=False, _experimental_autograph_options=None, experimental_relax_shapes=False, _experimental_compile=None, _created_variables=[<weakref at remote 0x7f256418ea48>, <weakref at remote 0x7f256418eae8>, <weakref at remote 0x7f256418ebd8>, <weakref at remote 0x7f256418ed18>, <weakref at remote 0x7f256418ed68>, <weakref at remote 0x7f256418eef8>, <weakref at remote 0x7f252832d098>, <weakref at remote 0x7f252832d188>, <weakref at remote 0x7f252832d228>, <weakref at r...(truncated)) at ../Python/ceval.c:754
#62 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166
#63 0x0000000000508537 in _PyFunction_FastCallDict () at ../Python/ceval.c:5075
#64 0x00000000005940d1 in _PyObject_FastCallDict (kwargs=0x0, nargs=2, args=0x7ffcaa452190,
    func=<function at remote 0x7f263bdbef28>) at ../Objects/abstract.c:2310
#65 _PyObject_Call_Prepend (kwargs=0x0, args=<optimized out>, obj=<optimized out>,
    func=<function at remote 0x7f263bdbef28>) at ../Objects/abstract.c:2373
#66 method_call.lto_priv () at ../Objects/classobject.c:314
#67 0x0000000000549f41 in PyObject_Call (kwargs=0x0,
    args=(<DistributedIterator(_enable_get_next_as_optional=False, _iterators=[<_SingleWorkerDatasetIterator(_dataset=<_AutoShardDataset(_input_dataset=<_OptionsDataset(_input_dataset=<_OptionsDataset(_input_dataset=<PrefetchDataset(_input_dataset=<_RebatchDataset(_input_dataset=<MapDataset(_input_dataset=<BatchDataset(_input_dataset=<RepeatDataset(_input_dataset=<MapDataset(_input_dataset=<TensorDataset(_structure=<TensorSpec at remote 0x7f26295ffe10>, _tensors=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d514438>], _variant_tensor_attr=<tensorflow.python.framework.ops.Eager---Type <return> to continue, or q <return> to quit---
Tensor at remote 0x7f263d5148d0>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[<TrackableReference at remote 0x7f26295ffd80>], _self_unconditional_dependency_names={'_variant_tracker': <_VariantTracker(_resource_handle=<...>, _resource_device='CPU', _resource_deleter=<CapturableResourceDeleter(_destroy_resource=None) at remote 0x7f263afb4400>, _create_resource=<function at remote 0x7f263bb23620>, _sel...(truncated),
    func=<method at remote 0x7f26914e20c8>) at ../Objects/abstract.c:2261
#68 slot_tp_call () at ../Objects/typeobject.c:6207
#69 0x00000000005a95fc in _PyObject_FastCallDict (kwargs=<optimized out>, nargs=1, args=0x7f25642fdc98,
    func=<Function(_lock=<_thread.lock at remote 0x7f2564374df0>, _python_function=<function at remote 0x7f2564495f28>,_function_spec=<FunctionSpec(_fullargspec=<FullArgSpec at remote 0x7f25644326d8>, _is_method=False, _default_values=None, _args_to_indices={'input_iterator': 0}, arg_names=['input_iterator'], vararg_name=None, _arg_indices_to_default_values={}, _input_signature=None) at remote 0x7f256435b400>, _autograph=False, _experimental_autograph_options=None, experimental_relax_shapes=False, _experimental_compile=None, _created_variables=[<weakref at remote 0x7f256418ea48>, <weakref atremote 0x7f256418eae8>, <weakref at remote 0x7f256418ebd8>, <weakref at remote 0x7f256418ed18>, <weakref at remote 0x7f256418ed68>, <weakref at remote 0x7f256418eef8>, <weakref at remote 0x7f252832d098>, <weakref at remote 0x7f252832d188>,<weakref at remote 0x7f252832d228>, <weakref at remote 0x7f252832d278>, <weakref at remote 0x7f252832d1d8>, <weakref atremote 0x7f252832d318>, <weakref at remote 0x7f252832d4a8>, <weakref at r...(truncated))
    at ../Objects/tupleobject.c:131
#70 _PyObject_FastCallKeywords () at ../Objects/abstract.c:2496
#71 0x0000000000509ad3 in call_function.lto_priv () at ../Python/ceval.c:4875
#72 0x000000000050b4a9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335
#73 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,
    f=Frame 0x7f25642fdaf8, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py, line 86, in execution_function (input_fn=<DistributedIterator(_enable_get_next_as_optional=False, _iterators=[<_SingleWorkerDatasetIterator(_dataset=<_AutoShardDataset(_input_dataset=<_OptionsDataset(_input_dataset=<_OptionsDataset(_input_dataset=<PrefetchDataset(_input_dataset=<_RebatchDataset(_input_dataset=<MapDataset(_input_dataset=<BatchDataset(_input_dataset=<RepeatDataset(_input_dataset=<MapDataset(_input_dataset=<TensorDataset(_structure=<TensorSpec at remote 0x7f26295ffe10>, _tensors=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d514438>], _variant_tensor_attr=<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d5148d0>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[<TrackableReference at remote 0x7f26295ffd80>], _self_unconditional_dependency_names={'_variant_tracker': <_VariantTracker(_resource_handle=<...>, _resource_...(truncated)) at ../Python/ceval.c:754
#74 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166
#75 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992
#76 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872
#77 0x000000000050b4a9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335
#78 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,
    f=Frame 0x689353d8, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.---Type <return> to continue, or q <return> to quit---
py, line 123, in run_one_epoch (model=<Model(_self_setattr_tracking=True, _nested_outputs=<Tensor(_op=<Operation(_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f262967f690>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lockat remote 0x7f260c4a7f30>, acquire=<built-in method acquire of _thread.lock object at remote 0x7f260c4a7f30>, release=<built-in method release of _thread.lock object at remote 0x7f260c4a7f30>, _waiters=<collections.deque at remote 0x7f260c594730>) at remote 0x7f260c5101d0>, _num_groups=2, _group_member_counts=[0, 0]) at remote 0x7f260c510160>, _nodes_by_id={1: <Operation(_graph=<...>, _inputs_val=None, _id_value=1, _original_op=None, _traceback=<tensorflow_core.python._tf_stack.StackSummary at remote 0x7f260c510f48>, _device_code_locations=[<TraceableObject(obj='', filename='/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py', ...(truncated)) at ../Python/ceval.c:754
#79 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166
#80 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992
#81 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872
#82 0x000000000050c36e in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3351
#83 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,
    f=Frame 0x68693178, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py, line 331, in fit (self=<Loop at remote 0x7f260c5102b0>, model=<Model(_self_setattr_tracking=True, _nested_outputs=<Tensor(_op=<Operation(_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f262967f690>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lock at remote 0x7f260c4a7f30>, acquire=<built-in method acquire of _thread.lock object at remote 0x7f260c4a7f30>, release=<built-in method release of _thread.lock object at remote 0x7f260c4a7f30>, _waiters=<collections.deque at remote 0x7f260c594730>) at remote 0x7f260c5101d0>, _num_groups=2, _group_member_counts=[0, 0]) at remote 0x7f260c510160>, _nodes_by_id={1: <Operation(_graph=<...>, _inputs_val=None, _id_value=1, _original_op=None, _traceback=<tensorflow_core.python._tf_stack.StackSummary at remote 0x7f260c510f48>, _device_code_locations=[<TraceableObject(obj='',filename='/usr/local/lib/python3.6/dist-packages/tensorflow_core/pytho...(truncated)) at ../Python/ceval.c:754
#84 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166
#85 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992
#86 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872
#87 0x000000000050c36e in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3351
#88 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,
    f=Frame 0x7f20bc0086b8, for file /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py, line 766, in fit (self=<Model(_self_setattr_tracking=True, _nested_outputs=<Tensor(_op=<Operation(_graph=<FuncGraph(_lock=<_thread.RLock at remote 0x7f262967f690>, _group_lock=<GroupLock(_ready=<Condition(_lock=<_thread.lock at remote0x7f260c4a7f30>, acquire=<built-in method acquire of _thread.lock object at remote 0x7f260c4a7f30>, release=<built-in method release of _thread.lock object at remote 0x7f260c4a7f30>, _waiters=<collections.deque at remote 0x7f260c594730>) at remote 0x7f260c5101d0>, _num_groups=2, _group_member_counts=[0, 0]) at remote 0x7f260c510160>, _nodes_by_id={1: <Operation(_graph=<...>, _inputs_val=None, _id_value=1, _original_op=None, _traceback=<tensorflow_core.python._tf_stack.StackSummary at remote 0x7f260c510f48>, _device_code_locations=[<TraceableObject(obj='', filename='/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py', lineno=390...(truncated)) at ../Python/ceval.c:754
---Type <return> to continue, or q <return> to quit---
#89 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166
#90 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992
#91 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872
#92 0x000000000050c36e in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3351
#93 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,
    f=Frame 0x52a7658, for file /user/vmarkovtsev/images/hang.py, line 31, in main (sample=<tensorflow.python.framework.ops.EagerTensor at remote 0x7f26295f78d0>, ds_train=<MapDataset(_input_dataset=<BatchDataset(_input_dataset=<RepeatDataset(_input_dataset=<MapDataset(_input_dataset=<TensorDataset(_structure=<TensorSpec at remote 0x7f26295ffe10>, _tensors=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d514438>], _variant_tensor_attr=<tensorflow.python.framework.ops.EagerTensor at remote 0x7f263d5148d0>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[<TrackableReference at remote 0x7f26295ffd80>], _self_unconditional_dependency_names={'_variant_tracker': <_VariantTracker(_resource_handle=<...>, _resource_device='CPU', _resource_deleter=<CapturableResourceDeleter(_destroy_resource=None)at remote 0x7f263afb4400>, _create_resource=<function at remote 0x7f263bb23620>, _self_setattr_tracking=True, _self_unconditional_checkpoint_dependencies=[], _self_unconditional_dependency_n...(truncated)) at ../Python/ceval.c:754
#94 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166
#95 0x0000000000508fa0 in fast_function.lto_priv () at ../Python/ceval.c:4992
#96 0x000000000050999d in call_function.lto_priv () at ../Python/ceval.c:4872
#97 0x000000000050b4a9 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335
#98 0x0000000000507125 in PyEval_EvalFrameEx (throwflag=0,
    f=Frame 0x20509a8, for file /user/vmarkovtsev/images/hang.py, line 35, in <module> ()) at ../Python/ceval.c:754
#99 _PyEval_EvalCodeWithName.lto_priv.1821 () at ../Python/ceval.c:4166
#100 0x000000000050a3b3 in PyEval_EvalCodeEx (closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0,
    argcount=0, args=0x0, locals=<optimized out>, globals=<optimized out>, _co=<optimized out>)
    at ../Python/ceval.c:4187
#101 PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>) at ../Python/ceval.c:731
#102 0x00000000006349e2 in run_mod () at ../Python/pythonrun.c:1025
#103 0x0000000000634a97 in PyRun_FileExFlags () at ../Python/pythonrun.c:978
#104 0x000000000063824f in PyRun_SimpleFileExFlags () at ../Python/pythonrun.c:419
#105 0x0000000000638425 in PyRun_AnyFileExFlags () at ../Python/pythonrun.c:81
#106 0x0000000000638df1 in run_file (p_cf=0x7ffcaa45361c, filename=<optimized out>, fp=<optimized out>)
    at ../Modules/main.c:340
#107 Py_Main () at ../Modules/main.c:810
#108 0x00000000004b0de0 in main (argc=2, argv=0x7ffcaa453818) at ../Programs/python.c:69
</pre>
</details>

<details>
<summary><code>bt</code> of each of the 4 running threads</summary>
<pre>
#0  0x00007fa23e7989d0 in nanosleep () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007fa1ec03cffd in tensorflow::(anonymous namespace)::PosixEnv::SleepForMicroseconds(long long) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2
#2  0x00007fa1f5d2dcd5 in tensorflow::EventMgr::PollLoop() ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
#3  0x00007fa1ec0528d1 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2
#4  0x00007fa1ec04feb8 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2
#5  0x00007fa1ec6a58df in std::execute_native_thread_routine (__p=0x6360ed0)
    at /dt7-src/libstdc++-v3/src/nonshared11/../c++11/thread.cc:83
#6  0x00007fa23e49c6db in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0
#7  0x00007fa23e7d588f in clone () from /lib/x86_64-linux-gnu/libc.so.6
</pre>
</details>

### Speculation

As we see, there are 4 threads - I guess one for each of my GPUs - which are polling something. They make 25-30% CPU load together. There are more than a hundred other threads, so I don't know which ones I should `bt` additionally. I tried with different batch sizes, which ofc influences the memory consumption, but does not change anything with the hang.

I can provide the access to the hardware or execute arbitrary commands if needed."
32653,Keras model.save() is extremely slow under MirroredStrategy context when keras.layers.BatchNormalization is in use.,"**System information**
- Have I written custom code: yes, the training script is written from scratch but the network resnet_v1.5-50 is copied from official example tensorflow/models
- OS Platform and Distribution: Linux Debian 9
- TensorFlow installed from (source or binary): yes
- GCC version for compiling: gcc 6.3 git commit 1eea9f905c4b4ac4844855eb64e2f9e415babe56
- TensorFlow version (use command below): 1.15
- Python version: python 3.5
- CUDA/cuDNN version: cuda 10.0 cudnn 7.6.2.24
- GPU model and memory: V100 32GB

**Describe the current behavior**
I try to use keras API for image classification model training and I'm testing the performance of `MirroredStrategy`. Everything works ordinarily except for keras model saving, which is extremely slow. Typically saving a model with resnet_v1.5-50 as backbone can elapsed hundreds of seconds.

Here's the saving time for each layers in resnet_v1.5-50, I found that saving `keras.layers.BatchNormalization` is the bottleneck:
```
Layer res5c_branch2b K.batch_get_value() for weight aggregation elapsed 0.00420689582824707s
Save layer res5c_branch2b weights elapsed 0.010241508483886719s
Layer bn5c_branch2b K.batch_get_value() for weight aggregation elapsed 6.323298692703247s
Save layer bn5c_branch2b weights elapsed 6.325744867324829s
Layer activation_47 K.batch_get_value() for weight aggregation elapsed 3.337860107421875e-05s
Save layer activation_47 weights elapsed 0.002166748046875s
```
After checking source codes of `BatchNormalization` and `MirroredStrategy`, I guess this bottleneck may be caused by synchronization between multiple replicas for parameters `moving_mean` and `moving_variance`. These two parameters use `SyncOnReadVariable` with `aggregation=VariableAggregation.MEAN` under `MirroredStrategy` context, and saving these two parameters needs synchronization, which is slow in my runtime environment.

The source codes which may be related to this problem:
https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/layers/normalization.py#L393
https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/distribute/values.py#L1230
https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/distribute/values.py#L1151
https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/distribute/mirrored_strategy.py#L524
https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/distribute/mirrored_strategy.py#L734

related issue: https://github.com/keras-team/keras/issues/9298

I didn't test tensorflow 2.0 version, but I found no changes in terms of `moving_mean`, `moving_variance` and `SyncOnReadVariable`, so I guess this problem may still exists in 2.0 version.
"
32651,[TF1.14][TPU]Can not use custom TFrecord dataset on Colab using TPU,"I have created a TFRecord dataset file consisting elements and their corresponding labels. I want to use it for training model on Colab using free TPU. I can load the TFRecord file and even run an iterator just to see the contents however, before the beginning of the epoch it throws following error- 
````
UnimplementedError: From /job:worker/replica:0/task:0:
File system scheme '[local]' not implemented (file: '/content/gdrive/My Drive/data/encodeddata_inGZIP.tfrecord')
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional_1]]
````
In my understanding, it wants the TFRecord file on the TPU bucket, I don't know how to do that on Colab. How can one use a TFRecord file directly on Colab TPU?"
32650,Converting unsupported operation ??,"- OS Platform and Distribution ( Linux Ubuntu 18.04):
- TensorFlow installed from (source):
- TensorFlow version (https://github.com/tensorflow/tensorflow):


`tensorflow-master$ bazel-bin/tensorflow/lite/toco/toco --input_file=../my_freeze12xshell.pb --output_file=../my12.tflite --output_format=TFLITE --input_shapes=2,4,513 --input_arrays=x_mixed --output_arrays=y_out1,y_out2 --inference_type=QUANTIZED_UINT8 --inference_input_type=FLOAT --std_dev_values=1 --mean_values=0`

my pb file is here

[my_freeze12xshell.zip](https://github.com/tensorflow/tensorflow/files/3630369/my_freeze12xshell.zip)

any one could help me ?
"
32647,"TF2.0 Convolution  [10,128,128,9] -> [10,128,128,32]->[10,128,128,3] doesn't work.","**System information**

Python version: 3.6.8
TensorFlow-gpu version 2.0.rc0
CUDA/cuDNN version: 10.0
GPU model and memory: GeForce GTX 1080 Ti
**Describe the current behavior**
Running the provided code on GPUs leads to error message tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'lambda_26_1/Identity:0' shape=(50, 128, 128, 3) dtype=float32>].

I created **keras** model with multiple keras layers and write  images in summary for tensorboard in function with attribute @tf.function: 
```
@tf.function
def write_to_summary(outputs, targets, writer)
    with writer.as_default():
        tf.summary.image(name=""losses_aux/log_outputs"",
                         data=outputs)
   ...
   ...
```
and get the error with such stack:
```
Traceback (most recent call last):
  File ""D:/Work/1.0/tf2/train.2.0.py"", line 255, in <module>
    _main()
  File ""D:/Work/1.0/tf2/train.2.0.py"", line 84, in _main
    _train_and_eval(config, train_dataset, eval_dataset, model, model_path)
  File ""D:/Work/1.0/tf2/train.2.0.py"", line 170, in _train_and_eval
    loss_value = loss_fn(outputs, target_batch_train, writer)
  File ""D:\Work\1.0\tf2\autoencoder_losses2.py"", line 62, in loss_fn
    gamma=losses_config.get_float('hfmae.gamma')),
  File ""D:\Work\1.0\tf2\loss2.py"", line 47, in high_frequency_mean_abs_error
    outputs, targets, writer, size, sigma, gamma)
  File ""C:\venv_rc2.0\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 445, in __call__
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""C:\venv_rc2.0\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1141, in _filtered_call
    self.captured_inputs)
  File ""C:\venv_rc2.0\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""C:\venv_rc2.0\lib\site-packages\tensorflow_core\python\eager\function.py"", line 511, in call
    ctx=ctx)
  File ""C:\venv_rc2.0\lib\site-packages\tensorflow_core\python\eager\execute.py"", line 75, in quick_execute
    ""tensors, but found {}"".format(keras_symbolic_tensors))
tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'lambda_26_1/Identity:0' shape=(50, 128, 128, 3) dtype=float32>]
```

Why keras tensor can't be input to eager execution function?!! 
Please, fix this bug, because images are very important for tensorboard.

"
32646,Tried to convert 'y' to a tensor and failed. Error: None values not supported.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0rc1

**Describe the current behavior**

Crash when fit. It works just fine in `1.14.0`, `2.0.0b0` & `2.0.0b1`.

**Describe the expected behavior**

Not to crash when fit.

**Code to reproduce the issue**

```python
import numpy as np
import tensorflow as tf
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.optimizers import Adam

model = Sequential([
    Dense(24, input_dim=4, activation='relu'),
    Dense(24, activation='relu'),
    Dense(2, activation='linear')
])
model.compile(loss='mse', optimizer=Adam(lr=0.001))

x = np.array([[-0.08623559, -0.79897248,  0.03606475,  1.09068178]])
y = np.array([[ 1.0449973,  -0.14471795]])
model.fit(x, y)
```

Here is the [colab link](https://colab.research.google.com/drive/1HevwenWthBQAnRRj8z7sJzBjuZV9F_JZ)

**Other info / logs**

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    526                 as_ref=input_arg.is_ref,
--> 527                 preferred_dtype=default_dtype)
    528           except TypeError as err:

22 frames
ValueError: None values not supported.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
ValueError: None values not supported.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    543               raise ValueError(
    544                   ""Tried to convert '%s' to a tensor and failed. Error: %s"" %
--> 545                   (input_name, err))
    546             prefix = (""Input '%s' of '%s' Op has type %s that does not match"" %
    547                       (input_name, op_type_name, observed))

ValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.
```
"
32645,import source code in Clion failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS (e.g., Linux Ubuntu 18.04):
- TensorFlow installed from (source or binary):
- TensorFlow version:r1.15
- Python version:2.7
- Bazel version (if compiling from source):0.24.1
- GCC/Compiler version (if compiling from source):7
- CUDA/cuDNN version:10.4



**Describe the problem**

I want to try to import the tensorflow source using clion's bazel plugin, and tried many versions, but none of them succeeded. Has anyone succeeded? Please give me advice.

![image](https://user-images.githubusercontent.com/33537489/65225648-1b85aa00-daf8-11e9-863d-954f0f248f78.png)

"
32642,Unable to convert .ckpt to .pb using tensorflow,"I have been trying to use BERT for question answer prediction but the response time of it quite high.

The model size is too heavy and is in .ckpt format.

I want to save it to .pb format but I am struglling with the output_node_name encountered with below error:

tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'InfeedEnqueueTuple' used by node input_pipeline_task0/while/InfeedQueue/enqueue/0 (defined at Tensorflowckpt2pb.py:98) with these attrs: [shapes=[[1], [1,384], [1,384], [1,384], [1], [1]], device_ordinal=0, layouts=[], dtypes=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32], _class=[""loc:@input_pipeline_task0/while/IteratorGetNext""]]
Registered devices: [CPU, XLA_CPU]
Registered kernels:
  <no registered kernels>

         [[input_pipeline_task0/while/InfeedQueue/enqueue/0]]

The script code is as below:

# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""Converts checkpoint variables into Const ops in a standalone GraphDef file.
This script is designed to take a GraphDef proto, a SaverDef proto, and a set of
variable values stored in a checkpoint file, and output a GraphDef with all of
the variable ops converted into const ops containing the values of the
variables.
It's useful to do this when we need to load a single file in C++, especially in
environments like mobile or embedded where we may not have access to the
RestoreTensor ops and file loading calls that they rely on.
An example of command-line usage is:
bazel build tensorflow/python/tools:freeze_graph && \
bazel-bin/tensorflow/python/tools/freeze_graph \
--input_graph=some_graph_def.pb \
--input_checkpoint=model.ckpt-8361242 \
--output_graph=/tmp/frozen_graph.pb --output_node_names=softmax
You can also look at freeze_graph_test.py for an example of how to use it.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf

from google.protobuf import text_format
#from tensorflow.python.client import graph_util
from tensorflow.python.framework import graph_util

FLAGS = tf.app.flags.FLAGS

tf.app.flags.DEFINE_string(""input_graph"", """",
                           """"""TensorFlow 'GraphDef' file to load."""""")
tf.app.flags.DEFINE_string(""input_saver"", """",
                           """"""TensorFlow saver file to load."""""")
tf.app.flags.DEFINE_string(""input_checkpoint"", """",
                           """"""TensorFlow variables file to load."""""")
tf.app.flags.DEFINE_string(""output_graph"", """",
                           """"""Output 'GraphDef' file name."""""")
tf.app.flags.DEFINE_boolean(""input_binary"", False,
                            """"""Whether the input files are in binary format."""""")
tf.app.flags.DEFINE_string(""output_node_names"", """",
                           """"""The name of the output nodes, comma separated."""""")
tf.app.flags.DEFINE_string(""restore_op_name"", ""save/restore_all"",
                           """"""The name of the master restore operator."""""")
tf.app.flags.DEFINE_string(""filename_tensor_name"", ""save/Const:0"",
                           """"""The name of the tensor holding the save path."""""")
tf.app.flags.DEFINE_boolean(""clear_devices"", True,
                            """"""Whether to remove device specifications."""""")
tf.app.flags.DEFINE_string(""initializer_nodes"", """", ""comma separated list of ""
                           ""initializer nodes to run before freezing."")


def freeze_graph(input_graph, input_saver, input_binary, input_checkpoint,
                 output_node_names, restore_op_name, filename_tensor_name,
                 output_graph, clear_devices, initializer_nodes):
  """"""Converts all variables in a graph and checkpoint into constants.""""""

  if not tf.gfile.Exists(input_graph):
    print(""Input graph file '"" + input_graph + ""' does not exist!"")
    return -1

  if input_saver and not tf.gfile.Exists(input_saver):
    print(""Input saver file '"" + input_saver + ""' does not exist!"")
    return -1

  if not tf.gfile.Glob(input_checkpoint):
    print(""Input checkpoint '"" + input_checkpoint + ""' doesn't exist!"")
    return -1

  if not output_node_names:
    print(""You need to supply the name of a node to --output_node_names."")
    return -1

  input_graph_def = tf.GraphDef()
  mode = ""rb"" if input_binary else ""r""
  with tf.gfile.FastGFile(input_graph, mode) as f:
    if input_binary:
      input_graph_def.ParseFromString(f.read())
    else:
      text_format.Merge(f.read(), input_graph_def)
  # Remove all the explicit device specifications for this node. This helps to
  # make the graph more portable.
  if clear_devices:
    for node in input_graph_def.node:
      node.device = """"
  _ = tf.import_graph_def(input_graph_def, name="""")

  with tf.Session() as sess:
    if input_saver:
      with tf.gfile.FastGFile(input_saver, mode) as f:
        saver_def = tf.train.SaverDef()
        if input_binary:
          saver_def.ParseFromString(f.read())
        else:
          text_format.Merge(f.read(), saver_def)
        saver = tf.train.Saver(saver_def=saver_def)
        saver.restore(sess, input_checkpoint)
    else:
      sess.run([restore_op_name], {filename_tensor_name: input_checkpoint})
      if initializer_nodes:
        sess.run(initializer_nodes)
    output_graph_def = graph_util.convert_variables_to_constants(
        sess, input_graph_def, output_node_names.split("",""))

  with tf.gfile.GFile(output_graph, ""wb"") as f:
    f.write(output_graph_def.SerializeToString())
  print(""%d ops in the final graph."" % len(output_graph_def.node))


def main(unused_args):
  freeze_graph(FLAGS.input_graph, FLAGS.input_saver, FLAGS.input_binary,
               FLAGS.input_checkpoint, FLAGS.output_node_names,
               FLAGS.restore_op_name, FLAGS.filename_tensor_name,
               FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)

if __name__ == ""__main__"":
  tf.app.run()

"
32641,Tensorflow Memory Error,"Dear Team,

i have 8 GB RAM in my system is it possible to train it in divided rule so that it would take less RAM to train it to large data set with less RAM i mean 8GB with 1GPU .i am going to train open_images_v4  data set "
32640,How to build tf1.14 then build tf2.0?,"I try to build 2 different pip package with 2 versions. First:

```
bazel build --config=opt  //tensorflow/tools/pip_package:build_pip_package
# this generates 1.14 by default
./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg

# then I try build v2
bazel build --config=v2 //tensorflow/tools/pip_package:build_pip_package
./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg

```

But anyway it generates only 1.14, not 2.0 in the next step......"
32638,load custom op in C API got error,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
I have added my op by following [https://www.tensorflow.org/guide/extend/op](url)
and I can load it in python by `tf.load_op_library(""./myop.so"")`, and then I freeze the graph and saved to `graph.pb`. Now I want to load the graph.pb in C API,and do inference, by the following codes to load the op 
`TF_Library* lib_handle = TF_LoadLibrary(""/home/yx.wang/hello_tf/myop.so"", status);
if(TF_GetCode(status) != TF_OK)
    {
        printf(""Can't encode string: %s\n"", TF_Message(status));
        return NULL;
    }
`
But I got an error `2019-09-19 10:39:14.505244: F tensorflow/core/framework/variant_op_registry.cc:102] Check failed: existing == nullptr (0x1e80700 vs. nullptr)Unary VariantDecodeFn for type_name: tensorflow::Tensor already registered`
I donot know what's wrong, anyone can help me to ckeck the question?"
32637,No such file or directory in bazel build,"- OS Platform and Distribution ( Linux Ubuntu 18.04, python3.5):
- TensorFlow installed from (tensorflow-master,that is, I download the https://github.com/tensorflow/tensorflow)
- TensorFlow version (source,I don't know, this is the link https://github.com/tensorflow/tensorflow):

`tensorflow-master$ bazel build tensorflow/lite/toco:toco`
`Starting local Bazel server and connecting to it...
FATAL: Cannot get start time of process 0: (error: 2): No such file or directory`

So what's wrong ??
thx"
32630,Cannot import tensorflow.image since 2019-09-06,"I am running Linux. 

Executing the following works:
```
pip3 install tf-nightly-gpu-2.0-preview==2.0.0.dev20190903
python3 -c ""import tensorflow.image""
```

There were no nightly builds on 4th and 5th. The following does not work:
```
pip3 install tf-nightly-gpu-2.0-preview==2.0.0.dev20190906
python3 -c ""import tensorflow.image""
```

I get

```
ModuleNotFoundError: No module named 'tensorflow.image'
```

This still reproduces on the most recent nightly 2.0.0.dev20190918."
32629,tflite_runtime supports for arm (pi3 model b+),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linuz rpi 4.19.66-v7+ #1253 SMP Thu Ag 15 11:49:46 BST 2019 arm71 GNU/Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary):
I installed tflite_runtime via 
pip3 install tflite_runtime-1.15.0rc0-cp35-cp35m-linux_armv7l.whl package
- TensorFlow version:
1.15.0
- Python version:
3.5.3
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the problem**
$ pip3 install tflite_runtime-1.15.0rc0-cp35-cp35m-linux_armv7l.whl
in my script, I have:

from tflite_runtime.interpreter import Interpreter
from tflite_runtime.interpreter import load_delegate
// more codes here
file_name = 'my_model.tflite'
interpreter = Interpreter(file_name, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])

Running the script:
$ python3 script.py
...
OSError: /usr/bin/arm-linux-gnueabihf/libc++abi.so.1: undefined symbol: _Unwind_SetIP
Exception ignored in: <bound method Delegate.__del__ of <tflite_runtime.interpreter..Delegate object at 0x769be550>>
Traceback (most recent call last):
  File ""/home/pi/.local/lib/python3.5/site-packages/tflite_runtime/iterpreter.py"", line 124, in __del__
    if self.__library is not None:
AttributeError: 'Delegate' object has no attribute '_library'

Keep in mind that I've been able to run the same exact code on my x86_64 Ubunbu 18.10 host machine, downloading the package from here:
https://www.tensorflow.org/lite/guide/python#install_just_the_interpreter
and ran pip install on the package.whl

"
32628,add new variables to a pretrain model in keras,"I encounter some problems when using keras. I get a network which has been pretrained, such as vgg16 or resnet34. The network use the keras default naming for all variables. Now i want to change some layers of the network which will change the variables of current layers. But when i directly change the layers, the name of variables in other layers change, too. Then i can not load the variables from the pretrained model. Can anyone tell me how to solve the problem."
32627, Support for 32 bits architecture #32315 ,"Several users are still using Python32 bits and they cannot install TensorFlow. For them, pip install TensorFlow fails as no wheel matches the tags expected by their environment (to debug, pip debug --verbose shows only tags that don't math the filenames of our wheels).

There is some requests to support 32 bits, see for example #31431

This is not going to be easy as we need to also compile the C++ codebase in 32 bits mode and that would cause issues with code written assuming types have a certain bit width.

There is no change in the user visible API, just a new set of wheels to support more users.

Opening this to reference in all similar issues.

After closing my thread, i just reopen it. Google employees trying to tell me this is not the roadmap for the tool. The day i am going to care about a Google employee opinion is the day hell froze. 

If you close this issue I will keep posting daily, until issue is fixed. 

"
32625,ImportError: DLL load failed: The specified module could not be found.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not application
- TensorFlow installed from (source or binary): PIP Command
- TensorFlow version:1.14.0
- Python version:Python 3.7.3 (default, Apr 24 2019, 15:29:51)
- Installed using virtualenv? pip? conda?: PIP
- Bazel version (if compiling from source): Not applicable
- GCC/Compiler version (if compiling from source): Not applicable
- CUDA/cuDNN version: cudnn-10.1-windows10-x64-v7.6.3.30
- GPU model and memory:NVIDIA GetForce GTX 1660 Ti anf 6GB



**Describe the problem**
When I am try to import the tensorflow I am getting the error ImportError: DLL load failed: The specified module could not be found.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Followed the installation guide provided for CUDA Installation Guide for Microsoft Windows

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

>>> import tensorflow as t
Traceback (most recent call last):
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Anaconda\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Anaconda\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Anaconda\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Anaconda\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
32624,Inference in C++ Tensorflow: Session->Run hangs when ran with input and output parameters supplied,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source C++ API
- TensorFlow version (use command below):1.12.0
- Python version:3.6.7
- Bazel version (if compiling from source):0.18.1
- GCC/Compiler version (if compiling from source):7.4.0
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I trained a FasterRCNN for point detection using TensorFlow and its python API. I was able to do the inference successfully in the same without any issues.
Now for production requirements, I need to build a standalone application or library which can perform the inference in C++.

In order to achieve that I successfully managed to build the TensorFlow library from scratch for C++ API and ran a few test programs.

Steps followed for inference on my trained model:

 1. Method #1: freeze the graph to *.pb file using modified [freeze_graph.py][1] and restore it.
 2. Method #2: Directly restore from checkpoint using MetaGraphDef and ReadBinaryProto.

I tried both the methods and the loading of the model seems to happen without any error. I printed the node names as well.

After loading the graph I created a handler for image input, previously the input during training was getting handled by QueueRunner, but because it's just inference I created the handler myself with appropriate modifications to it as can be seen below.

        Status fasterRcnn::CreateGraphForImage(bool unstack)
    {
        fileNameVar=Placeholder(iRoot.WithOpName(""input""),DT_STRING);
        auto fileReader=ReadFile(iRoot.WithOpName(""fileReader""),fileNameVar);
        auto imageReader=DecodeJpeg(iRoot.WithOpName(""jpegReader""),fileReader,DecodeJpeg::Channels(imageChannels));
        auto floatCaster=Cast(iRoot.WithOpName(""floatCaster""),imageReader,DT_FLOAT);
        auto dimsExpander=ExpandDims(iRoot.WithOpName(""dim""),floatCaster,0);
        //auto resized=ResizeBilinear(iRoot.WithOpName(""size""),dimsExpander,Const(iRoot,{imageSide,imageSide}));
        //auto div=Div(iRoot.WithOpName(""normalized""),resized,{255.f});
        imageTensorVar=dimsExpander;//div;
        return iRoot.status();

    }
NOTE: iRoot is the private variable of fasterRcnn class-> Scope used by the graph for loading images into tensors

 

       class fasterRcnn
    {
    private:
        Scope iRoot;//graph for loading images into tensors
        const int imageSide;
        const int imageChannels;//rgb
        //load image vars
        Output fileNameVar;
        Output imageTensorVar;
        //
        std::unique_ptr<Session> fSession;//file session
        std::unique_ptr<GraphDef> graphDef;
    public:
        fasterRcnn(int side,int channels):iRoot(Scope::NewRootScope()),imageSide(side),imageChannels(channels){}
        Status CreateGraphForImage(bool unstack);
        Status LoadSavedModel(string &fileName);//via frozen graph
        Status LoadSavedModel(std::string graph_fn,std::string checkpoint_fn);//via checkpoints
        Status PredictFromFrozen(Tensor &image,int&results);//
        Status ReadTensorFromImageFile(string& file_name, Tensor& outTensor);
        Status ReadFileTensors(string& folder_name,vector<Tensor>& file_tensors);
    
    };

**Now the when the time comes for Prediction when I run Session->Run with particular input node name and output node name the program hangs**:

    Status fasterRcnn::PredictFromFrozen(Tensor& image, int& result)
    {
        vector<Tensor> outTensors;
        cout<<""Prediction about to start""<<endl;
        TF_CHECK_OK(fSession->Run({{""fasterrcnn/truncated_base_network/sub"",image}},{""fasterrcnn/rcnn/rcnn_proposal/GatherV2_1""},{},&outTensors));
        cout<<""Prediction done""<<endl;
        auto output_c = outTensors[0].scalar<float>();
        for (int i=0;i<outTensors.size();i++)
        {
            cout << ""Output dimension of the image"" << outTensors[i].DebugString()<<""\n"";
        }
        cout << ""Output dimension of the image"" << outTensors[0].DebugString()<<""\n"";
        return Status::OK();
    }

**TF_CHECK_OK(fSession->Run({{""fasterrcnn/truncated_base_network/sub"",image}},{""fasterrcnn/rcnn/rcnn_proposal/GatherV2_1""},{},&outTensors));**

***It hangs at the above statement.***

**Up until a few Debug thoughts:**

 1. I thought maybe the QueueRunner might be creating the issue, but according to [this answer][2] I handled that as I created the image handler graph.
 2. I thought maybe I didn't freeze the graph well, that's I loaded directly from the checkpoints, but still, it hangs.
 3. The input to the VGG_16 truncated network in graph expects a 4D float tensor, which I am providing, double-checked that.



**Links to Code and Graph:**

 1. [CodeBase and GraphPng and GIF hosted on GDrive][3]

**Question:**

 1. Is there any step that I missing, for successful inferencing in C++.
 2. How to verify if my input and output node names are correct? I referred to the python code and they seem right.
 3. Any steps for debugging that I should follow to provide more info on this?

  [1]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py
  [2]: https://stackoverflow.com/a/35346656/8250471
  [3]: https://drive.google.com/drive/folders/1iO7JIitD_BfHnnLqgzvUygFwAAW6v_5G?usp=sharing

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32623,"""failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error"" unless running with sudo","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ClearLinux 31030
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  -
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tensorflow-gpu 1.14.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: pyenv's pip
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: cuda_10.0.130_410.48_linux, cudnn-10.0-linux-x64-v7.6.3.30
- GPU model and memory: Geforce RTX 2060, 6GB RAM


**Describe the problem**

The error ""failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error"" is thrown when initializing tensorflow-gpu, falling back to CPU instead of GPU.

When running python with sudo, GPU is detected but libraries cannot be opened. A subsequent run without sudo works, enabling GPU being used. I don't understand why running with sudo is needed to enable future calls without sudo work.

The specific output is:

    $ python -c ""import tensorflow as tf; tf.Session(config=tf.ConfigProto(log_device_placement=True))""
    2019-09-18 17:49:26.342297: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
    2019-09-18 17:49:26.364789: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192000000 Hz
    2019-09-18 17:49:26.365673: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b4f7008c70 executing computations on platform Host. Devices:
    2019-09-18 17:49:26.365685: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
    2019-09-18 17:49:26.387440: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
    2019-09-18 17:49:26.397897: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
    2019-09-18 17:49:26.397918: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: linux
    2019-09-18 17:49:26.397923: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: linux
    2019-09-18 17:49:26.397950: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.50.0
    2019-09-18 17:49:26.397965: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.50.0
    2019-09-18 17:49:26.397969: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.50.0
    2019-09-18 17:49:26.399634: I tensorflow/core/common_runtime/direct_session.cc:296] Device mapping:
    /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
    Device mapping:
    /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device

    $ sudo python -c ""import tensorflow as tf; tf.Session(config=tf.ConfigProto(log_device_placement=True))""
    2019-09-18 17:49:33.476640: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
    2019-09-18 17:49:33.492804: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192000000 Hz
    2019-09-18 17:49:33.493345: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b8b43539a0 executing computations on platform Host. Devices:
    2019-09-18 17:49:33.493356: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
    2019-09-18 17:49:33.494037: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
    2019-09-18 17:49:33.525519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    2019-09-18 17:49:33.525827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
    name: GeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.755
    pciBusID: 0000:01:00.0
    2019-09-18 17:49:33.525900: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory
    2019-09-18 17:49:33.525942: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory
    2019-09-18 17:49:33.525980: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory
    2019-09-18 17:49:33.526017: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory
    2019-09-18 17:49:33.526055: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory
    2019-09-18 17:49:33.526092: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory
    2019-09-18 17:49:33.526130: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory
    2019-09-18 17:49:33.526135: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
    2019-09-18 17:49:33.614070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
    2019-09-18 17:49:33.614090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
    2019-09-18 17:49:33.614095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
    2019-09-18 17:49:33.615437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    2019-09-18 17:49:33.615752: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b8b71df500 executing computations on platform CUDA. Devices:
    2019-09-18 17:49:33.615761: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2060, Compute Capability 7.5
    2019-09-18 17:49:33.616523: I tensorflow/core/common_runtime/direct_session.cc:296] Device mapping:
    /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
    /job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
    Device mapping:
    /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
    /job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device

    $ python -c ""import tensorflow as tf; tf.Session(config=tf.ConfigProto(log_device_placement=True))""
    2019-09-18 17:49:38.343247: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
    2019-09-18 17:49:38.359840: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192000000 Hz
    2019-09-18 17:49:38.360790: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d522b77c70 executing computations on platform Host. Devices:
    2019-09-18 17:49:38.360803: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
    2019-09-18 17:49:38.361478: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
    2019-09-18 17:49:38.377924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    2019-09-18 17:49:38.378224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
    name: GeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.755
    pciBusID: 0000:01:00.0
    2019-09-18 17:49:38.382955: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
    2019-09-18 17:49:38.426461: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
    2019-09-18 17:49:38.452107: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
    2019-09-18 17:49:38.468904: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
    2019-09-18 17:49:38.517258: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
    2019-09-18 17:49:38.545852: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
    2019-09-18 17:49:38.660617: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
    2019-09-18 17:49:38.660684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    2019-09-18 17:49:38.661018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    2019-09-18 17:49:38.661283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
    2019-09-18 17:49:38.661304: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
    2019-09-18 17:49:38.727136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
    2019-09-18 17:49:38.727157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
    2019-09-18 17:49:38.727164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
    2019-09-18 17:49:38.727262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    2019-09-18 17:49:38.727564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    2019-09-18 17:49:38.727848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    2019-09-18 17:49:38.728115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5451 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5)
    2019-09-18 17:49:38.729363: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d525ddbe50 executing computations on platform CUDA. Devices:
    2019-09-18 17:49:38.729373: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2060, Compute Capability 7.5
    2019-09-18 17:49:38.730199: I tensorflow/core/common_runtime/direct_session.cc:296] Device mapping:
    /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
    /job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5
    /job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
    Device mapping:
    /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
    /job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5
    /job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device

**Any other info / logs**

CUDA and Nvidia drivers installed at /opt following [ClearLinux guide](https://docs.01.org/clearlinux/latest/tutorials/nvidia.html).

    $ echo $LD_LIBRARY_PATH
    /usr/local/cuda/lib64:

    $ nvcc --version
    nvcc: NVIDIA (R) Cuda compiler driver
    Copyright (c) 2005-2018 NVIDIA Corporation
    Built on Sat_Aug_25_21:08:01_CDT_2018
    Cuda compilation tools, release 10.0, V10.0.130

    $ nvidia-smi
    Wed Sep 18 17:58:36 2019       
    +-----------------------------------------------------------------------------+
    | NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |
    |-------------------------------+----------------------+----------------------+
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |===============================+======================+======================|
    |   0  GeForce RTX 2060    Off  | 00000000:01:00.0  On |                  N/A |
    |  0%   50C    P8     9W / 170W |    152MiB /  5931MiB |      0%      Default |
    +-------------------------------+----------------------+----------------------+
                                                                                   
    +-----------------------------------------------------------------------------+
    | Processes:                                                       GPU Memory |
    |  GPU       PID   Type   Process name                             Usage      |
    |=============================================================================|
    |    0       651      G   /usr/bin/X                                    39MiB |
    |    0       789      G   /usr/bin/gnome-shell                         111MiB |
    +-----------------------------------------------------------------------------+"
32622,[TF 2.0] tf.assign fails with int32 tensors using a multi-device distribution strategy,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0rc0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10/7
- GPU model and memory: 2 GPUs

**Describe the current behavior**

If you create a distribution strategy like MultiWorkerMirroredDistributionStrategy, then create a tf.int32 variable var, then call var.assign(some_int32_tensor_to_assign), TF will crash. It crashes in values.py, line 1220,

```
 # To preserve the sum across save and restore, we have to divide the
        # total across all devices when restoring a variable that was summed
        # when saving.
        tensor = args[0]
  if self._aggregation == vs.VariableAggregation.SUM:
          tensor *= 1. / len(self.devices)
```
So what's happening here appears to be that TF tries to divide the int32 tensor by a float with no cast and blows up.

**Describe the expected behavior**
It seems like being able to assign integers to tensors while using distributed training is something that should be possible, given the goal of abstracting away distribution as much as possible. I know the distribution strategies may be kind of in flux right now, and I'm mainly just posting to make more knowledgeable people away of the issue; I'm just going to make all my tensors floats in the meantime :). I don't have the context to know how this should be approached anyway. Cast everything to floats? Make it so that the tensors don't always have to be divided? Divide integer tensors by assigning everything to one worker and giving zeros to everyone else? Who knows.

"
32621,Estimator 1.x nightlies break tf.contrib.*,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux (like Debian)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-9365-gff401a6 1.15.0-dev20190821
- Python version: Python 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

On TF 1.x nightlies, `tf.contrib.summary` fails to import due to an error
importing something from estimator:

```
$ cd ""$(mktemp -d)""
$ virtualenv -q -p python3.6 ./ve
$ . ./ve/bin/activate
(ve) $ pip install -q tf-nightly==1.15.0.dev20190821
(ve) $ pip freeze | grep -e tensor -e tf- -e tb-
tb-nightly==1.15.0a20190911
tf-estimator-nightly==1.14.0.dev2019091801
tf-nightly==1.15.0.dev20190821
(ve) $ python -c 'import tensorflow as tf; tf.contrib.summary'
WARNING:tensorflow:

  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.

  Please upgrade your code to TensorFlow 2.0:
    * https://www.tensorflow.org/beta/guide/migration_guide

  Or install the latest stable TensorFlow 1.X release:
    * `pip install -U ""tensorflow==1.*""`

  Otherwise your code may be broken by the change.

  
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/tmp/tmp.pTt6GXwXDC/ve/lib/python3.6/site-packages/tensorflow_core/python/util/lazy_loader.py"", line 63, in __getattr__
    return getattr(module, item)
  File ""/tmp/tmp.pTt6GXwXDC/ve/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/tmp/tmp.pTt6GXwXDC/ve/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/tmp/tmp.pTt6GXwXDC/ve/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/tmp/tmp.pTt6GXwXDC/ve/lib/python3.6/site-packages/tensorflow_core/contrib/__init__.py"", line 48, in <module>
    from tensorflow.contrib import estimator
  File ""/tmp/tmp.pTt6GXwXDC/ve/lib/python3.6/site-packages/tensorflow_core/contrib/estimator/__init__.py"", line 30, in <module>
    from tensorflow_estimator.contrib import estimator
ModuleNotFoundError: No module named 'tensorflow_estimator.contrib'
```

**Describe the expected behavior**

It should be possible to force `tf.contrib`, as was possible with yesterday’s
`tensorflow-estimator-nightly`.

```
$ pip freeze | grep -e tensor -e tf- -e tb-
tb-nightly==1.15.0a20190911
tf-estimator-nightly==1.14.0.dev2019091701
tf-nightly==1.15.0.dev20190821
$ python -c 'import tensorflow as tf; tf.contrib.summary'  # works
```

**Code to reproduce the issue**

```
python -c 'import tensorflow as tf; tf.contrib.summary'
```

**Other info / logs**

This is blocking TensorBoard builds and nightlies.

Googlers, see <http://b/140485848#comment35>.
"
32620,Use tf.Tensor as input to tf.io.gfile.GFile,"**System information**
- TensorFlow version (you are using): 2.0.0rc1


**Describe the feature and the current behavior/state.**
Currently, file readers such as `tf.io.gfile.GFile` expect plain strings as input. As a consequence, one can hardly use those file readers inside `tf.data.Dataset` pipelines which yield string Tensors (typically through `tf.data.Dataset.list_files`)

Doing so raises `TypeError: Expected binary or unicode string, got <tf.Tensor 'EagerPyFunc:0' shape=<unknown> dtype=string>`, at the line `compat.as_bytes(self.__name)` (tensorflow_core/python/lib/io/file_io.py:84).

**Will this change the current api? How?**
From a user point of view, the current API would not see any significant changes.

**Who will benefit with this feature?**
Any one eager to read custom file formats, where it is for instance needed to decode part of the header file (first few bytes) to decode the actual content. For instance, image formats such as TIFF where the dtype and the image shape are encoded inside the header.

Below pipelines would then be easier to build:
```
files = tf.data.Dataset.list_files('/**/regex*')
tensors = files.map(MyCustomReader)
```

**Any Other info.**
See below an example of such custom format file reader:

```
import tensorflow as tf

@tf.function
def CustomReader(path):
    # path must here a string in the current API
    with tf.io.gfile.GFile(name=path, mode='rb') as f:
        
        # Shape information is encoded  on 16 bytes starting at position 40
        f.seek(40)
        shape = tf.io.decode_raw(f.read(16), out_type=tf.int16)
        shape = tf.cast(shape, tf.int32)
       
        # dtype information is encoded on 2 bytes at position 70
        f.seek(70)
        dtype = tf.io.decode_raw(f.read(2), out_type=tf.int16)[0]

        # tensor values populate the  rest of the buffer
        values = tf.io.decode_raw(f.read(-1), out_type=dtype)
        tensor = tf.reshape(values, shape)

        return tensor
```"
32619,Missing output shape for inverse stft,"## Link
https://www.tensorflow.org/api_docs/python/tf/signal/inverse_stft

## Description of issue:
I am missing what the output dimensions are.

### Clear description

I do not know what the output dimensions of inverse_stft are. And I do not know how they come to be.

### Example Code

```
audioNp = np.random.random((1,120800)).astype(np.float32)
frame_length = 2048
frame_step = 2048//4
stft = tf.contrib.signal.stft(
    audioNp[0], 
    frame_length, 
    frame_step)
invstft = tf.contrib.signal.inverse_stft(
    stft, 
    frame_length, 
    frame_step, 
    window_fn = tf.contrib.signal.inverse_stft_window_fn(frame_step))
sess = tf.Session()
stft, invstft = sess.run((stft, invstft))
print(invstft.shape)
```
The output shape of invstft is then (120320,). And I don't know how it got there. In my opinion it should be again (120800).
"
32618,"Use of fit_generator and evaluate_generator result in ""builtins.TypeError: 'NoneType' object is not subscriptable""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
https://colab.research.google.com/gist/isaacgerg/03717f55f020f36aa7b5138fc04cfc43/tf-keras-generator-issue.ipynb

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 7, Python 3.7, tensorflow 1.13.1

- TensorFlow installed from (source or binary):
via pip

- TensorFlow version (use command below): 1.13,1

- Python version: 3.7
- CUDA/cuDNN version: 10
- GPU model and memory: RTX

**Describe the current behavior**
```
The code in the colab, when run on a GPU, results in teh following error:
File ""c:\python37\Lib\threading.py"", line 885, in _bootstrap
  self._bootstrap_inner()
File ""c:\python37\Lib\threading.py"", line 917, in _bootstrap_inner
  self.run()
File ""c:\python37\Lib\threading.py"", line 865, in run
  self._target(*self._args, **self._kwargs)
File ""c:\python37\Lib\multiprocessing\pool.py"", line 121, in worker
  result = (True, func(*args, **kwds))
File ""c:\python37\Lib\site-packages\tensorflow\python\keras\utils\data_utils.py"", line 445, in get_index
  return _SHARED_SEQUENCES[uid][i]

builtins.TypeError: 'NoneType' object is not subscriptable
```

**Describe the expected behavior**
No error

**Code to reproduce the issue**
See colab link in description.  This is the offending code.  It only appears to happen on windows.  I am unsure in colab how to select a windows based environment.

EDIT 1: Added windows caveat."
32617,[TF.1.14] Cannot load weights under bfloat16 using tf.keras,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14
- Python version: 3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):-
- CUDA/cuDNN version:-
- GPU model and memory: Using TPU

I ve trained a custom CNN with TF 1.14 using keras, and trained under TPU strategy scope and by settting:  
```
K.set_floatx('float16')
keras.backend.set_epsilon(1e-4)
```
For traininng with fp16. The training procedure was allright, since I would like to stop and load the trained weights. I compile the model under tpu_strategy.scope, and when trying to load weights using model.load_weigths, reaises the following error 

```
WARNING: Logging before flag parsing goes to stderr.
W0918 11:36:24.372959 139742800717568 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0918 11:37:32.353564 139742800717568 deprecation_wrapper.py:119] From 6dense_inv_frp16.py:326: The name tf.losses.mean_squared_error is deprecated. Please use tf.compat.v1.losses.mean_squared_error instead.

W0918 11:37:32.370291 139742800717568 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Traceback (most recent call last):
  File ""6dense_inv_frp16.py"", line 364, in <module>
    model.load_weights(filepath)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py"", line 162, in load_weights
    return super(Model, self).load_weights(filepath, by_name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py"", line 1424, in load_weights
    saving.load_weights_from_hdf5_group(f, self.layers)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 759, in load_weights_from_hdf5_group
    K.batch_set_value(weight_value_tuples)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py"", line 3058, in batch_set_value
    value = np.asarray(value, dtype=dtype(x))
  File ""/usr/local/lib/python3.5/dist-packages/numpy/core/_asarray.py"", line 85, in asarray
    return array(a, dtype, copy=False, order=order)
ValueError: setting an array element with a sequence.
```
"
32616,Dear team how could I use open images dataset v4 with tensorflow to detect a objects,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32614,[TF2.0] can't restore path with brackets,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX
- TensorFlow installed from (source or binary): `pip install tensorflow==2.0.0-rc1`
- TensorFlow version (use command below): v2.0.0-rc0-101-gd2d2566eef 2.0.0-rc1
- Python version: 3.6.8
- CUDA/cuDNN version: `None`
- GPU model and memory: `None`

**Describe the current behavior**
I can't restore a model stored ina directory containing a bracket

**Describe the expected behavior**
I can restore a model stored ina directory containing a bracket

**Code to reproduce the issue**
```python
import os
import shutil
import tensorflow as tf

current_dir = os.path.realpath(os.path.dirname(__file__))
save_dir = os.path.join(current_dir, 'testhop')
save_dir_wbracket = os.path.join(current_dir, 'test[hop')


if os.path.isdir(save_dir):
    shutil.rmtree(save_dir)
os.mkdir(save_dir)
assert os.path.isdir(save_dir)

if os.path.isdir(save_dir_wbracket):
    shutil.rmtree(save_dir_wbracket)
os.mkdir(save_dir_wbracket)
assert os.path.isdir(save_dir_wbracket)

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(
        16, batch_input_shape=[256, 1], activation='relu'),
    tf.keras.layers.Dense(16, activation='tanh'),
    tf.keras.layers.Dense(1),
])

lr = tf.Variable(1.)
reg_param = tf.Variable(1.)
optim = tf.keras.optimizers.SGD(lr)

ckpt = tf.train.Checkpoint(
    model=model,
    lr=lr,
    reg_param=reg_param,
)

manager = tf.train.CheckpointManager(ckpt, save_dir, max_to_keep=1)
manager_wbracket = tf.train.CheckpointManager(ckpt, save_dir_wbracket, max_to_keep=1)

manager.save()
assert os.path.isfile(os.path.join(save_dir, 'checkpoint'))
manager_wbracket.save()
assert os.path.isfile(os.path.join(save_dir_wbracket, 'checkpoint'))

# Works
print(manager.latest_checkpoint)
ckpt.restore(manager.latest_checkpoint)

# Does not work
print(manager_wbracket.latest_checkpoint)
ckpt.restore(manager_wbracket.latest_checkpoint)
```

**Other info / logs**
Note: Saving is working

Outputs:
```
/Users/morgangiraud/Sites/git/ray/python/ray/tune/examples/pbt/PBTTrain/testhop/ckpt-1
/Users/morgangiraud/Sites/git/ray/python/ray/tune/examples/pbt/PBTTrain/test[hop/ckpt-2
Traceback (most recent call last):
  File ""debug.py"", line 51, in <module>
    ckpt.restore(manager_wbracket.latest_checkpoint)
  File ""/Users/morgangiraud/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py"", line 1971, in restore
    status = self._saver.restore(save_path=save_path)
  File ""/Users/morgangiraud/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py"", line 1228, in restore
    reader = pywrap_tensorflow.NewCheckpointReader(save_path)
  File ""/Users/morgangiraud/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 873, in NewCheckpointReader
    return CheckpointReader(compat.as_bytes(filepattern))
  File ""/Users/morgangiraud/miniconda3/envs/ray/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 885, in __init__
    this = _pywrap_tensorflow_internal.new_CheckpointReader(filename)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files
```"
32612,Keras can not load custom Loss functions.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: Cuda 10.0
- GPU model and memory: RTX 2080 Titan

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
If overloading the loss function in `tf.keras.losses.Loss` the created model can be compiled, trained and saved but not loaded.
**Describe the expected behavior**
Custom loss function should be loadable.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow as tf

print(tf.__version__)
class MyCustomLoss(tf.keras.losses.Loss):
    def __init__(self):
        super().__init__()

    def call(self, y_true, y_pred):
        return 1.



a = tf.keras.layers.Input(shape=(32,))
b = tf.keras.layers.Dense(32)(a)
model = tf.keras.models.Model(inputs=a, outputs=b)

model.compile('sgd', MyCustomLoss())
model.save('/tmp/model.h5')
model_new = tf.keras.models.load_model('/tmp/model.h5', custom_objects={'MyCustomLoss': MyCustomLoss}))
```
Output:  
```
2.0.0-beta1
2019-09-18 10:10:54.994663: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-09-18 10:10:55.075823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:65:00.0
2019-09-18 10:10:55.077214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:b3:00.0
2019-09-18 10:10:55.077363: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-09-18 10:10:55.078368: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-09-18 10:10:55.079236: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-09-18 10:10:55.079453: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-09-18 10:10:55.080500: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-09-18 10:10:55.081301: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-09-18 10:10:55.083761: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-18 10:10:55.089032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
2019-09-18 10:10:55.089311: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-09-18 10:10:55.420058: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4d766e0 executing computations on platform CUDA. Devices:
2019-09-18 10:10:55.420103: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2019-09-18 10:10:55.420116: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5
2019-09-18 10:10:55.442162: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3300000000 Hz
2019-09-18 10:10:55.444231: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ef370 executing computations on platform Host. Devices:
2019-09-18 10:10:55.444265: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-09-18 10:10:55.445930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:65:00.0
2019-09-18 10:10:55.447265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:b3:00.0
2019-09-18 10:10:55.447316: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-09-18 10:10:55.447336: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-09-18 10:10:55.447352: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-09-18 10:10:55.447369: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-09-18 10:10:55.447385: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-09-18 10:10:55.447402: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-09-18 10:10:55.447419: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-18 10:10:55.452337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1
2019-09-18 10:10:55.452385: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-09-18 10:10:55.455253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-18 10:10:55.455270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 
2019-09-18 10:10:55.455279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N N 
2019-09-18 10:10:55.455286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   N N 
2019-09-18 10:10:55.458601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9913 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5)
2019-09-18 10:10:55.460320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10311 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:b3:00.0, compute capability: 7.5)
Traceback (most recent call last):
  File ""/home/ragnar/projects/mbda/mbda_infrared/sandbox.py"", line 19, in <module>
    model_new = tf.keras.models.load_model('/tmp/model.h5')
  File ""/home/ragnar/projects/mbda/mbda_infrared/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py"", line 137, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File ""/home/ragnar/projects/mbda/mbda_infrared/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 178, in load_model_from_hdf5
    training_config, custom_objects))
  File ""/home/ragnar/projects/mbda/mbda_infrared/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/saving_utils.py"", line 222, in compile_args_from_training_config
    loss_config = losses.get(loss_config)
  File ""/home/ragnar/projects/mbda/mbda_infrared/venv/lib/python3.6/site-packages/tensorflow/python/keras/losses.py"", line 1124, in get
    return deserialize(identifier)
  File ""/home/ragnar/projects/mbda/mbda_infrared/venv/lib/python3.6/site-packages/tensorflow/python/keras/losses.py"", line 1113, in deserialize
    printable_module_name='loss function')
  File ""/home/ragnar/projects/mbda/mbda_infrared/venv/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 181, in deserialize_keras_object
    config, module_objects, custom_objects, printable_module_name)
  File ""/home/ragnar/projects/mbda/mbda_infrared/venv/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 166, in class_and_config_for_serialized_keras_object
    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)
ValueError: Unknown loss function: MyCustomLoss

Process finished with exit code 1
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32611,How to set the four parameters in toco,"Linux Ubuntu 18.04:
- TensorFlow installed from `pip install tensorflow-gpu`:
- TensorFlow version 1.12.0:

In ckpt to pb I have used the 
`tf.contrib.quantize.create_eval_graph()`
So I use the type QUANTIZED_UINT8 in toco as 
`toco --graph_def_file=./my_freeze_graph_stft.pb --output_file=my_stft.tflite --output_format=TFLITE --input_shape=1024 --input_array=x_mixed --output_array=y_out2 --inference_type=QUANTIZED_UINT8 --inference_input_type=QUANTIZED_UINT8`

but I find the error
`    input_array.mean_value, input_array.std_value = quantized_input_stats[idx]
TypeError: 'NoneType' object is not subscriptable`
SO I must set the four parameters
`--std_dev_values`
`--mean_values`
`--default_ranges_min`
`--default_ranges_max`

SO how to set them ?any idea or advice/suggestuion ?

thx

"
32609,Support for TensorForest,"**System information**
- OS Platform and Distribution Linux Ubuntu 18.04
- TensorFlow installed from binary
- TensorFlow version 1.14

Text from TFLite converter:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ARG_MAX, DIV, PACK, SUM. Here is a list of operators for which you will need custom implementations: DecisionTreeResourceHandleOp, TreePredictionsV4.

"
32608,Failed to convert an RNN built with tf.keras by TFLiteConverter,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): tensorflow-gpu 2.0.0rc1


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: RESHAPE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.
```

Also, please include a link to a GraphDef or the model if possible.

Code which would reproduce the error log:
```
from tensorflow.keras import layers, models
import tensorflow as tf

ipt = layers.Input((10, 5))
x = layers.SimpleRNN(1, return_sequences=True)(ipt)
model = models.Model(ipt, x)

cvt = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = cvt.convert()
```

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32607,"""node optimization/gradients has inputs from different frames"" on GPU, but works fine on CPU","

------------------------

### System information
- **I'm writing a customer LSTMCell, I referred the source code in tensorflow/python/ops/rnn_cell_impl.py. I only add one line of operations on top of the source code.**:
- **Linux Ubuntu**: 16.04
- **TensorFlow version**: 1.13.1
- **Python version**: 3.5.2
- **CUDA/cuDNN version**: v10.1.168
- **GPU**: Tesla v100


### Describe the problem
I'm writing a customer LSTMCell, I followed the source code in tensorflow/python/ops/rnn_cell_impl.py, just add one more line ""d = tf.multiply(f, (tf.ones_like(f)-f))"". Everything works fine on CPU, but raise ""node optimization/gradients has inputs from different frames"" error while running on GPU. I didn't use any operation that's invalid on CUDA, only regular tensor operations like multiplication.

### Source code / logs
""defined customer LSTM Cell"":


    from __future__ import absolute_import
    from __future__ import division
    from __future__ import print_function


    from tensorflow.python.framework import constant_op
    from tensorflow.python.framework import dtypes

    from tensorflow.python.layers import base as base_layer
    from tensorflow.python.ops import array_ops

    from tensorflow.python.ops import init_ops
    from tensorflow.python.ops import math_ops
    from tensorflow.python.ops import nn_ops
    from tensorflow.python.platform import tf_logging as logging
    from tensorflow.contrib.rnn import BasicLSTMCell as BasicLSTMCell
    from tensorflow.contrib.rnn import LSTMStateTuple
    import tensorflow as tf


    _BIAS_VARIABLE_NAME = ""bias""
    _WEIGHTS_VARIABLE_NAME = ""kernel""


    class ProxLSTMCell(BasicLSTMCell):

      def __init__(self, num_units, forget_bias=1.0,
                   state_is_tuple=True, activation=None, reuse=None, name='basic_lstm_cell', lamb 
          = 1.0, delta = 1.0):
          super(ProxLSTMCell, self).__init__(num_units = num_units, reuse=reuse, name=name)
          if not state_is_tuple:
          logging.warn(""%s: Using a concatenated state is slower and will soon be ""
                       ""deprecated.  Use state_is_tuple=True."", self)

         # Inputs must be 2-dimensional.
         self.input_spec = base_layer.InputSpec(ndim=2)

         self._num_units = num_units
         self._forget_bias = forget_bias
         self._state_is_tuple = state_is_tuple
         self.lamb = lamb
         self.delta = delta

     @property
     def state_size(self):
       return (LSTMStateTuple(self._num_units, self._num_units)
              if self._state_is_tuple else 2 * self._num_units)

      @property
      def output_size(self):
        return self._num_units

      def build(self, inputs_shape):
        if inputs_shape[1].value is None:
         raise ValueError(""Expected inputs.shape[-1] to be known, saw shape: %s""
                       % inputs_shape)

         input_depth = inputs_shape[1].value
         self.input_size = input_depth
         h_depth = self._num_units
         self._kernel = self.add_variable(
         _WEIGHTS_VARIABLE_NAME,
         shape=[input_depth + h_depth, 4 * self._num_units])
         self._bias = self.add_variable(
         _BIAS_VARIABLE_NAME,
         shape=[4 * self._num_units],
         initializer=init_ops.zeros_initializer(dtype=self.dtype))

         self.built = True

      def call(self, inputs, state):
                 """"""Long short-term memory cell (LSTM).
    sigmoid = math_ops.sigmoid
    one = constant_op.constant(1, dtype=dtypes.int32)
    # Parameters of gates are concatenated into one multiply for efficiency.
    if self._state_is_tuple:
      c, h = state
    else:
      c, h = array_ops.split(value=state, num_or_size_splits=2, axis=one)

    gate_inputs = math_ops.matmul(
        array_ops.concat([inputs, h], 1), self._kernel)
    gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)

    # i = input_gate, j = new_input, f = forget_gate, o = output_gate
    i, j, f, o = array_ops.split(
        value=gate_inputs, num_or_size_splits=4, axis=one)

    forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype)
    # Note that using `add` and `multiply` instead of `+` and `*` gives a
    # performance improvement. So using those at the cost of readability.
    add = math_ops.add
    multiply = math_ops.multiply
    new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))),
                multiply(sigmoid(i), self._activation(j)))
    new_h = multiply(self._activation(new_c), sigmoid(o))
    d = multiply(f, (tf.ones_like(f)-f))

    new_c = new_c + d
    if self._state_is_tuple:
      new_state = LSTMStateTuple(new_c, new_h)
    else:
      new_state = array_ops.concat([new_c, new_h], 1)
    return new_h, new_state

''Calling defined LSTM Cell'':
```
def LSTM_graph():

........

prox_cell = ProxLSTM.ProxLSTMCell(
        num_units=self.cell_size,
        forget_bias=0.0,
        reuse=tf.get_variable_scope().reuse,
        lamb=FLAGS.lamb,
        delta=FLAGS.delta)
cell = tf.contrib.rnn.MultiRNNCell([
              prox_cell
              for _ in xrange(self.num_layers)
          ])

lstm_out, next_state = tf.nn.dynamic_rnn(
          cell, x, initial_state=initial_state, sequence_length=seq_length)

return lstm_out, next_state



""Error message"":

tensorflow.python.framework.errors_impl.InvalidArgumentError: node optimization/gradients/LSTM/rnn/while/rnn/multi_
rnn_cell/cell_0/basic_lstm_cell/MatMul_grad/MatMul (defined at /workspace/adversarial_text/layers.py:327)  has inpu
ts from different frames. The input node optimization/gradients/LSTM/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm
_cell/split_grad/concat (defined at /workspace/adversarial_text/layers.py:327)  is in frame ''. The input node opti
mization/gradients/LSTM/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul_grad/MatMul/Enter (defined at /w
orkspace/adversarial_text/layers.py:327)  is in frame 'optimization/gradients/LSTM/rnn/while/while_context'.
```"
32606,[model_to_estimator] estimator not evaluate all outputs defined in keras model but only one,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0
- GPU model and memory: 

**Describe the current behavior**
When using keras model with **multiple outputs**, and convert it to estimator by `model_to_estimator`, it only evaluate one output by `estimator.evaluate` or `tf.estimator.train_and_evaluate`. 
And don't know which output it evaluate (it doesn't display output name but only metrics name).

**Describe the expected behavior**
It should only evaluate all outputs defined in keras model, not only pick one.

**Code to reproduce the issue**
A example here:
```python
    ...

    model = tf.keras.Model(inputs=input_layer, outputs=[output_layer1, output_layer2])

    model.compile(optimizer=optimizer_builder.build(config.optimizer),
                  loss={
                      'output_layer1': 'sparse_categorical_crossentropy',
                      'output_layer2': 'sparse_categorical_crossentropy'},
                  metrics={
                      'output_layer1': 'accuracy',
                      'output_layer2': 'accuracy'})

    estimator = tf.keras.estimator.model_to_estimator(
        keras_model=model,
        config=run_config,
    )
    train_spec = tf.estimator.TrainSpec(
        input_fn=lambda: train_input_fn(config))
    eval_spec = tf.estimator.EvalSpec(
        input_fn=lambda: eval_input_fn(config))
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

    # Only evaluate one output and don't know which output it evaluate
```

**Other info / logs**
tf log here, except two outputs log, but only get one:
```
# Training is OK
INFO:tensorflow:global_step/sec: 1.02106
INFO:tensorflow:loss = 0.16959098, step = 33600 (97.938 sec)
INFO:tensorflow:global_step/sec: 0.943188
INFO:tensorflow:loss = 0.17629223, step = 33700 (106.023 sec)
INFO:tensorflow:global_step/sec: 0.848641
INFO:tensorflow:loss = 0.1101246, step = 33800 (117.835 sec)
INFO:tensorflow:global_step/sec: 0.852682
INFO:tensorflow:loss = 0.104427785, step = 33900 (117.277 sec)
INFO:tensorflow:Saving checkpoints for 34000 into /summary/model.ckpt.
...

# Evaluating only evaluate one output
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Evaluation [150/1500]
INFO:tensorflow:Finished evaluation at 2019-09-17-14:14:45
INFO:tensorflow:Saving dict for global step 34000: accuracy = 0.6914724, sparse_categorical_crossentropy = 1.0857606, global_step = 34000, loss = 1.1107497
```

TensorBoard except two outputs scalar, but only get one:
![2019-09-18 11 27 54](https://user-images.githubusercontent.com/7609173/65105491-aaff6000-da07-11e9-916f-473ba7847ba6.png)
"
32604,"EmptyTensorList, TensorListFromTensor, TensorListReserve, TensorListStack, While.","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32603,[model_to_estimator] summary in keras loss/metrics function get duplicate scalars with same value in TensorBoard,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0
- GPU model and memory: 

**Describe the current behavior**
Using tf.summary.scalar in keras loss/metrics function, then use model_to_estimator to convert keras model to estimator, tensorboard get duplicate scalars with totally same value. (e.g., `metrics/accuracy/train` and `metrics/accuracy/train_1`)

**Describe the expected behavior**
It should only summary one scalar.

**Code to reproduce the issue**
A example here:
```python
    ...

    model = tf.keras.Model(inputs=input_layer, outputs=[output_layer1, output_layer2])

    def accuracy(y_true, y_pred):
        y_pred, y_true = reshape_keras_tensor(y_pred, y_true)
        acc = tf.reduce_mean(
            tf.cast(tf.equal(tf.cast(y_true, tf.int64), tf.argmax(input=y_pred, axis=1)), tf.float32))
        # Where I summary, without this line, it will not summary anything during training
        tf.summary.scalar('train', acc)
        return acc

    def total_loss(y_true, y_pred):
        y_pred, y_true = reshape_keras_tensor(y_pred, y_true)
        loss =  tf.nn.sparse_softmax_cross_entropy_with_logits(
            labels=y_true,
            logits=y_pred)
        loss = tf.reduce_mean(cross_entropy)
        # Where I summary, without this line, it will not summary anything during training
        tf.summary.scalar('train', loss)
        return loss

    ...

    model.compile(optimizer=optimizer_builder.build(config.optimizer),
                  loss={output_layer1: total_loss},
                  metrics={output_layer1: accuracy})

    estimator = tf.keras.estimator.model_to_estimator(
        keras_model=model,
        config=run_config,
    )
    train_spec = tf.estimator.TrainSpec(
        input_fn=lambda: train_input_fn(config))
    eval_spec = tf.estimator.EvalSpec(
        input_fn=lambda: eval_input_fn(config))
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
```

**Other info / logs**
TensorBoard with duplicate scalars:
![2019-09-18 10 28 17](https://user-images.githubusercontent.com/7609173/65102919-27da0c00-d9ff-11e9-8a46-6d00ea39a4eb.png)
![2019-09-18 10 27 56](https://user-images.githubusercontent.com/7609173/65102920-2872a280-d9ff-11e9-9d85-b626593688a7.png)

"
32602,tensorrt6 compatibility,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.3 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.0-rc2
- Python version: 3.7.4 x64
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.1, 7.6
- GPU model and memory:
GTX1080Ti GDDR5X 11GB X 7


**Describe the problem**
bazel build failed

**Provide the exact sequence of commands / steps that you executed before running into the problem**
./configure
add cuda,tensorrt
bazel build --config=opt --config=v2 //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
ERROR: /home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/external/local_config_tensorrt/BUILD:43:1: Executing genrule @local_config_tensorrt//:tensorrt_include failed (Exit 1)
cp: cannot stat '/usr/include/x86_64-linux-gnu/NvInferPlugin.h': No such file or directory
```"
32601,Is there any documents of TF Core,"Such as:

1. Global Flow Chart of Tensorflow
2. Graph modify for Model Optimization(Insert an op in backend)
3.  Basic Data Structure(Data management)
4. Detail of Communication for Distribute Strategy"
32600,Tensorflow seems lost some module No module named 'numpy.core._multiarray_umath',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**


Requirement already satisfied: markdown>=2.6.8 in c:\programdata\anaconda3\lib\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.1.1)
Requirement already satisfied: setuptools>=41.0.0 in c:\programdata\anaconda3\lib\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (41.0.1)
Requirement already satisfied: h5py in c:\programdata\anaconda3\lib\site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)
Installing collected packages: tensorflow
Successfully installed tensorflow-1.14.0

(base) C:\Users\Liu>python
Python 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'
ImportError: numpy.core.multiarray failed to import

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<frozen importlib._bootstrap>"", line 980, in _find_and_load
SystemError: <class '_frozen_importlib._ModuleLockManager'> returned a result with an error set
ImportError: numpy.core._multiarray_umath failed to import
ImportError: numpy.core.umath failed to import
2019-09-18 08:38:07.989471: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32598,pix2pix tutorial is broken with tf2 rc,"The tutorial given here: 
[https://www.tensorflow.org/beta/tutorials/generative/pix2pix](https://www.tensorflow.org/beta/tutorials/generative/pix2pix) no longer works on a clean install of the current tf2 rc. 

it breaks around the 16th block. 

I wasn't sure where to report this problem

```

WARNING:tensorflow:Entity <function load_image_train at 0x10e71cf80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4
WARNING: Entity <function load_image_train at 0x10e71cf80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4

---------------------------------------------------------------------------
OperatorNotAllowedInGraphError            Traceback (most recent call last)
<ipython-input-16-e1fc1fbf2b89> in <module>
      2 train_dataset = train_dataset.shuffle(BUFFER_SIZE)
      3 train_dataset = train_dataset.map(load_image_train,
----> 4                                   num_parallel_calls=tf.data.experimental.AUTOTUNE)
      5 train_dataset = train_dataset.batch(1)

~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls)
   1902       return DatasetV1Adapter(
   1903           ParallelMapDataset(
-> 1904               self, map_func, num_parallel_calls, preserve_cardinality=False))
   1905 
   1906   @deprecation.deprecated(None, ""Use `tf.data.Dataset.map()"")

~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)
   3452         self._transformation_name(),
   3453         dataset=input_dataset,
-> 3454         use_legacy_function=use_legacy_function)
   3455     self._num_parallel_calls = ops.convert_to_tensor(
   3456         num_parallel_calls, dtype=dtypes.int32, name=""num_parallel_calls"")

~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
   2693       resource_tracker = tracking.ResourceTracker()
   2694       with tracking.resource_tracker_scope(resource_tracker):
-> 2695         self._function = wrapper_fn._get_concrete_function_internal()
   2696         if add_to_graph:
   2697           self._function.add_to_graph(ops.get_default_graph())

~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal(self, *args, **kwargs)
   1852     """"""Bypasses error checking when getting a graph function.""""""
   1853     graph_function = self._get_concrete_function_internal_garbage_collected(
-> 1854         *args, **kwargs)
   1855     # We're returning this concrete function to someone, and they may keep a
   1856     # reference to the FuncGraph without keeping a reference to the

~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1846     if self.input_signature:
   1847       args, kwargs = None, None
-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1849     return graph_function
   1850 

~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2148         graph_function = self._function_cache.primary.get(cache_key, None)
   2149         if graph_function is None:
-> 2150           graph_function = self._create_graph_function(args, kwargs)
   2151           self._function_cache.primary[cache_key] = graph_function
   2152         return graph_function, args, kwargs

~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2039             arg_names=arg_names,
   2040             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2041             capture_by_value=self._capture_by_value),
   2042         self._function_attributes,
   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of

~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    913                                           converted_func)
    914 
--> 915       func_outputs = python_func(*func_args, **func_kwargs)
    916 
    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in wrapper_fn(*args)
   2687           attributes=defun_kwargs)
   2688       def wrapper_fn(*args):  # pylint: disable=missing-docstring
-> 2689         ret = _wrapper_helper(*args)
   2690         ret = structure.to_tensor_list(self._output_structure, ret)
   2691         return [ops.convert_to_tensor(t) for t in ret]

~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in _wrapper_helper(*args)
   2632         nested_args = (nested_args,)
   2633 
-> 2634       ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
   2635       # If `func` returns a list of tensors, `nest.flatten()` and
   2636       # `ops.convert_to_tensor()` would conspire to attempt to stack

~/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    235       except Exception as e:  # pylint:disable=broad-except
    236         if hasattr(e, 'ag_error_metadata'):
--> 237           raise e.ag_error_metadata.to_exception(e)
    238         else:
    239           raise

OperatorNotAllowedInGraphError: in converted code:

    <ipython-input-14-e5f2b44984ba>:3 load_image_train
        input_image, real_image = random_jitter(input_image, real_image)
    <ipython-input-12-b7170a9df479>:8 random_jitter
        if tf.random.uniform(()) > 0.5:
    /Users/nathan/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:765 __bool__
        self._disallow_bool_casting()
    /Users/nathan/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:528 _disallow_bool_casting
        ""using a `tf.Tensor` as a Python `bool`"")
    /Users/nathan/anaconda3/envs/playground/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:513 _disallow_when_autograph_disabled
        "" Try decorating it directly with @tf.function."".format(task))

    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph is disabled in this function. Try decorating it directly with @tf.function.

```"
32594,2 x WARNING: Entity <function ... initialize_variables at ...>could not be transformed and will be executed as-is,"Hi! I'm getting two `Entity ... could not be transformed and will be executed as-is` errors while training a Keras model. Here's a Colab notebook illustrating the problem:

https://colab.research.google.com/drive/1muFh7JjN6AJcWmF0Kp3zRwE043Uh-Tk6

This may be related to #32319 which I raised last week. Since that's been resolved, this might be, too. I don't know how to check that, though, so I thought I'd file to be on the safe side."
32592,In tf.Estimator training metric is not divided by the batch size,"**System information**
- TensorFlow version: TF 1.12
- Python version: 3.6

**Describe the current behavior**

<img width=""345"" alt=""metric"" src=""https://user-images.githubusercontent.com/23496841/65050585-e9ddd900-d967-11e9-82b5-b8d0a04164db.png"">

I am training a model with tf.Estimator and it seems that the training metric (root mean squared error) is not divided by the batch size (for both training and validation the batch size is 100). Indeed, for comparison this is the mean squared error loss for training and validation:

<img width=""364"" alt=""loss"" src=""https://user-images.githubusercontent.com/23496841/65050797-450fcb80-d968-11e9-9a67-565efced4684.png"">

**Code to reproduce the issue**
I am using the standard structure for `model_fn` in `tf.Estimator`. Here's the main part of the code:

``` Python
def model_fn(features, labels, mode, params):

    # model layers 
    # ...

    rmse = tf.metrics.root_mean_squared_error(labels, predictions)

    if mode == tf.estimator.ModeKeys.PREDICT:
        predictions = {
            'new_states': predictions
        }
        return tf.estimator.EstimatorSpec(mode, predictions=predictions)

    loss = tf.losses.mean_squared_error(labels, predictions)

    metrics = {'rmse': rmse}

    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)

    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])
   
    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())

    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)

```"
32588,tf.compat.v2.summary should be used in /guide/eager#summaries_and_tensorboard,"## URL(s) with the issue:

https://www.tensorflow.org/guide/eager#summaries_and_tensorboard

## Description of issue (what needs changing):

We should not recommend the usage of `tf.contrib.summary` in eager mode.
`tf.compat.v2.summary` (or `tf.summary` in TF2) should be used in this section.

With `tf.contrib.summary`, we need to use `always_record_summaries()` or `record_summaries_every_n_global_steps()` (#32587) to record events even in eager mode. This is very unnatural and confusing.

When I read this section, I thought `record_summaries_every_n_global_steps()` is optional and spent a lot of time to notice why `tf.contrib.summary.scalar` operations in my code was no-op.

In eager mode, I think we should recommend code like:

```
for i in range(num_steps):
  loss = train_one_step()
  step = i + 1
  if step % 100 == 0:
    tf_summary.scalar('loss', loss, step=step)
```

with `tf.compat.v2.summary`.

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? 

**Yes**

See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
32587,tf.contrib.summary.scalar does not record anything outside of always_record_summaries in eager execution,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): Colab builtin
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.8
- GCC/Compiler version (if compiling from source): [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]

**Describe the current behavior**

https://colab.research.google.com/drive/1c1SUEet_jCxvW8D_ZlmOCPJvutB3PhCD
This code does not record anything in eager execution mode.

```
writer = tf.contrib.summary.create_file_writer(os.path.join(logdir, 'contrib'))
with writer.as_default():
  for i in range(1000):
    if i % 10 == 0:
      tf.contrib.summary.scalar('sin', math.sin(2*math.pi/500), step=i+1)
writer.close()
```

**Describe the expected behavior**

`tf.contrib.summary.scalar` should save records even out side of `always_record_summaries()`  by default.

It's very unnatural that we need to call `always_record_summaries` or `record_summaries_every_n_global_steps` to save records in eager execution mode.

https://www.tensorflow.org/guide/eager#summaries_and_tensorboard

**Code to reproduce the issue**
https://colab.research.google.com/drive/1c1SUEet_jCxvW8D_ZlmOCPJvutB3PhCD"
32586,RNN does not forward the training flag to StackedRNNCells,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: 2.0.0rc0
- Python version: 3.6.6

**Describe the current behavior**

When using `tf.keras.layers.StackedRNNCells` with `tf.keras.layers.RNN`, the RNN layer does not forward the `training` flag to the cell. This is because the RNN code checks that cell explictly defines the `training` flag as argument, which `tf.keras.layers.StackedRNNCells` does not.

https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/recurrent.py#L709-L710

**Describe the expected behavior**

The `training` flag should be passed to `tf.keras.layers.StackedRNNCells`, and to each stacked cell.

**Code to reproduce the issue**

The code below should not raise the `AssertionError`:

```python
import tensorflow as tf

class CellWrapper(tf.keras.layers.AbstractRNNCell):

    def __init__(self, cell):
        super(CellWrapper, self).__init__()
        self.cell = cell

    @property
    def state_size(self):
        return self.cell.state_size

    @property
    def output_size(self):
        return self.cell.output_size

    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):
        return self.cell.get_initial_state(
            inputs=inputs, batch_size=batch_size, dtype=dtype)

    def call(self, inputs, states, training=None, **kwargs):
        assert training is not None


cell = tf.keras.layers.LSTMCell(32)
cell = CellWrapper(cell)
cell = tf.keras.layers.StackedRNNCells([cell])

rnn = tf.keras.layers.RNN(cell)
inputs = tf.random.uniform([4, 7, 16])
rnn(inputs, training=True)
```"
32585,Horrible Energy Impact when opening tensorflow.org with Safari,"<img width=""801"" alt=""image"" src=""https://user-images.githubusercontent.com/22335780/65037849-25d16800-d981-11e9-8167-e17cbdae72d0.png"">

Not sure it appropriate issue this kind problem here, but got nothing with google. 
Every time opening https://www.tensorflow.org, the battery consuming speed is unreasonably high, why?"
32583,tensorflow2 Custom training fine tuning issue,"**I used  ResNet101V2 pretrained model training my dataset.**
Below is my code:

```
    batch_size = 16
    buffer_size = 5000
    image_size = 224
    classes = 9
    num_epochs = 200
    learning_rate = 0.000
    train_image_paths, train_image_labels = util.get_image_paths_and_labels(train_dir)
    train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, train_image_labels))
    train_dataset = train_dataset.map(
        lambda path, label: weather.load_and_preprocess_image_and_label(path, label, image_size))
    train_dataset = train_dataset.shuffle(buffer_size=buffer_size)
    train_dataset = train_dataset.batch(batch_size=batch_size)
    val_image_paths, val_image_labels = util.get_image_paths_and_labels(val_dir)
    val_dataset = tf.data.Dataset.from_tensor_slices((val_image_paths, val_image_labels))
    # val_dataset = weather.load_data(val_tfrecords_path)
    val_dataset = val_dataset.map(
        lambda path, label: weather.load_and_preprocess_image_and_label(path, label, image_size))
    val_dataset = val_dataset.batch(batch_size=batch_size)
    base_model = ResNet101V2(input_shape=(image_size, image_size, 3),
                             include_top=False,
                             weights='imagenet')  # 'imagenet'
    base_model.trainable = True
    
    model = keras.Sequential([
        base_model,
        keras.layers.GlobalAveragePooling2D(),
        keras.layers.Dense(classes, activation='softmax', name='predictions')
    ])
    model.summary()
    
    loss_fn = keras.losses.CategoricalCrossentropy()
    # optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)
    
    train_loss = keras.metrics.Mean()
    train_accuracy = keras.metrics.CategoricalAccuracy()
    
    val_loss = keras.metrics.Mean()
    val_accuracy = keras.metrics.CategoricalAccuracy()

    for epoch in range(num_epochs):
        for step, (images, labels) in enumerate(train_dataset):
            labels = tf.one_hot(labels, depth=classes)
            with tf.GradientTape() as tape:
                y_ = model(images)
                loss = loss_fn(labels, y_)
            grads = tape.gradient(loss, model.trainable_variables)

            optimizer.apply_gradients(zip(grads, model.trainable_variables))

            train_loss(loss)
            train_accuracy(labels, y_)  # model(images)
            if step % 50 == 0:
                print(""Step {:03d}: TrainLoss: {:.3f}, TrainAccuracy: {:.3%}"".format(step,
                                                                                     train_loss.result(),
                                                                                     train_accuracy.result()))
        # Display metrics at the end of each epoch
        print(""Epoch {:03d}: TrainLoss: {:.3f}, TrainAccuracy: {:.3%}"".format(epoch,
                                                                              train_loss.result(),
                                                                              train_accuracy.result()))
        train_loss.reset_states()
        train_accuracy.reset_states()

        for images, labels in val_dataset:
            labels = tf.one_hot(labels, depth=classes)
            y_ = model(images)
            loss = loss_fn(labels, y_)
            val_loss(loss)
            val_accuracy(labels, y_)
        print(""Epoch {:03d}: ValLoss: {:.3f}, ValAccuracy: {:.3%}"".format(epoch,
                                                                          val_loss.result(),
                                                                          val_accuracy.result()))
        val_loss.reset_states()
        val_accuracy.reset_states()
```


I set base_model.trainable = True, the training result is bad. 

```
    Step 000: TrainLoss: 2.219, TrainAccuracy: 18.750%
    Step 050: TrainLoss: 2.122, TrainAccuracy: 21.446%
    Step 100: TrainLoss: 2.134, TrainAccuracy: 20.854%
    Step 150: TrainLoss: 2.139, TrainAccuracy: 19.826%
    Step 200: TrainLoss: 2.140, TrainAccuracy: 19.652%
    Step 250: TrainLoss: 2.137, TrainAccuracy: 20.045%
    Step 300: TrainLoss: 2.133, TrainAccuracy: 20.328%
    Step 350: TrainLoss: 2.133, TrainAccuracy: 20.495%
    Epoch 005: TrainLoss: 2.134, TrainAccuracy: 20.396%
    Epoch 005: ValLoss: 2.137, ValAccuracy: 19.844%
    2019-09-17 10:38:11.732802: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:143] Filling 
    up shuffle buffer (this may take a while): 1707 of 5000
    2019-09-17 10:38:21.722296: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:143] Filling 
    up shuffle buffer (this may take a while): 3409 of 5000
    2019-09-17 10:38:31.856501: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:143] Filling 
    up shuffle buffer (this may take a while): 4311 of 5000
    2019-09-17 10:38:41.166725: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:193] 
    Shuffle buffer filled.
    Step 000: TrainLoss: 2.057, TrainAccuracy: 18.750%
    Step 050: TrainLoss: 2.102, TrainAccuracy: 21.324%
    Step 100: TrainLoss: 2.123, TrainAccuracy: 20.050%
    Step 150: TrainLoss: 2.124, TrainAccuracy: 20.530%
    Step 200: TrainLoss: 2.127, TrainAccuracy: 19.963%
    Step 250: TrainLoss: 2.130, TrainAccuracy: 20.568%
    Step 300: TrainLoss: 2.132, TrainAccuracy: 20.619%
    Step 350: TrainLoss: 2.132, TrainAccuracy: 20.388%
```

However，I use keras training way

```
        model.compile(loss='sparse_categorical_crossentropy',
                      optimizer=tf.keras.optimizers.RMSprop(lr=learning_rate),
                      metrics=['accuracy'])

        model.fit(train_dataset,
                  epochs=num_epochs,
                  validation_data=val_dataset)
```

the training result is good. 

I am very confused，anyone help me？
Thanks！"
32581,Need to save tf model in .h5 format,"I have a trained model in ONNX and i have ported that to TF using below code programmaticaly.
(TensorflowRep). (onnx_tf.backend.prepare ).
A TensorflowRep class object representing the ONNX model
onnx_tf.backend_rep.TensorflowRep.export_graph
How can I save the model to disk?
Can I save this in .h5 format.

"
32580,tf.test.is_gpu_available() and tf.Session() blocking indefinitely,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Nope
- OS Platform and Distribution: Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not tried
- TensorFlow installed from (source or binary): don't remember
- TensorFlow version: 1.14
- Python version: 3.6.8
- CUDA/cuDNN version: 10.1 and driver version 418.56
- GPU model and memory: 2x Nvidia 1080 and 1x Nvidia Titan 

**Describe the current behavior**
I found similar issue #[29273](https://github.com/tensorflow/tensorflow/issues/22730) but for me it never unfreezes the system. Strange that yesterday my docker container was working perfectly fine, and today something strange happened( I guess something happened after reboot?) . This two commands tf.test.is_gpu_available() and tf.Session() are blocking my machine so that I need to restart it. 
The last thing I see in the console is:
```
Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA
CPU Frequency: 3499975000 Hz
XLA service 0x559960dc29d0 executing computations on platform Host.Devices:
StreamExecutor device (0):<undefined>, <undefined>
Successfully opened dynamic library libcuda.so.1
``` 
Also I tried to make a conda env with tf via `conda create -n tensorflow_env tensorflow` and the problem remains. According to the https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html the drivers are matching, but the weirdest thing is that it was working for quite a while! Any ideas how to debug it?

**Describe the expected behavior**
Not to block.

"
32579,ONNX to Keras(.h5 format),I have to export trained model from ONNX to Keras. How can i do this?or  How we can convert it to tensorflow & then convert it to .h5 format.
32578,sparse_softmax_cross_entropy_with_logits fails for symbolic tensors in eager mode,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0-rc0
- Python version: 3.6

This code:
```
tf.nn.sparse_softmax_cross_entropy_with_logits(
    tf.keras.layers.Input((None,), dtype=tf.int32),
    tf.keras.layers.Input((None,None)))
```

Fails with this exception:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 3477, in sparse_softmax_cross_entropy_with_logits_v2
    labels=labels, logits=logits, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 3410, in sparse_softmax_cross_entropy_with_logits
    array_ops.shape(logits)[:-1]))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/check_ops.py"", line 506, in assert_equal
    if not condition:
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 765, in __bool__
    self._disallow_bool_casting()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 534, in _disallow_bool_casting
    self._disallow_in_graph_mode(""using a `tf.Tensor` as a Python `bool`"")
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 523, in _disallow_in_graph_mode
    "" this function with @tf.function."".format(task))
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
```

But this works:
```
tf.compat.v1.disable_eager_execution()
tf.nn.sparse_softmax_cross_entropy_with_logits(
    tf.keras.layers.Input((None,), dtype=tf.int32),
    tf.keras.layers.Input((None,None)))
```

Seems similar to #31848, although the underlying issue here is `assert_equal` not faring well with symbolic tensors in eager mode. The issue seems to be from here:

https://github.com/tensorflow/tensorflow/blob/d5efc0e9fd431586e4ebd63a9716577f6a59ba02/tensorflow/python/ops/check_ops.py#L330-L334

Here a tensor is cast to a boolean in eager mode, but that is not possible in the case of a symbolic keras tensor - even though tensorflow is executing eagerly."
32577,Unable to find pip package,"I have been using bazel build for my im2txt model from TensorFlow and it shows me

`ModuleNotFoundError: No module named 'nltk'`

I have installed the package nltk and tried to even create an environment and run the bazel script

Is there anyway i need to link my python to bazel for the python modules to be installed separately on an environment ?

Repo i am trying to run : im2txt

Code sample which iam running :
```
# Location to save the MSCOCO data.
MSCOCO_DIR=""${HOME}/im2txt/data/mscoco""

# Build the preprocessing script.
cd research/im2txt
bazel build //im2txt:download_and_preprocess_mscoco

# Run the preprocessing script.
bazel-bin/im2txt/download_and_preprocess_mscoco ""${MSCOCO_DIR}""   ##Error here <----
```"
32575,TF2.0RC1/RC0  the output shape of  custom layer is None. The same code works in TF2.0 Beta1,"**System information**
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): TF2.0.0beta1/rc0/rc1
- Python version: 3.6
- CUDA/cuDNN version: 10.0
- GPU model and memory: TitanV


"
32574,tensorflow2rc1 import bug  ,"Hi,
I was trying to convert one of my training templates to TensorFlow2 but encountered several issues. I can not reach a point where my code is even executed because many imports already terminate the application at start. 
```python
from tensorflow_core.python.client.session import InteractiveSession

def main():
    print('hello world')

if __name__ == ""__main__"":
    main()
```
This code results instantly in an error and the print is never reached.
```
2019-09-17 09:18:51.362815: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/api/python/session_create_counter
Traceback (most recent call last):
  File ""/home/xxx/dev/tf2xxx/bug.py"", line 1, in <module>
    from tensorflow_core.python.client.session import InteractiveSession
  File ""/home/xxx/.virtualenv/tf2/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 49, in <module>
    'Counter for number of sessions created in Python.')
  File ""/home/xxx/.virtualenv/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/monitoring.py"", line 183, in __init__
    name, description, *labels)
  File ""/home/xxx/.virtualenv/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/monitoring.py"", line 121, in __init__
    self._metric = self._metric_methods[self._label_length].create(*args)
tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.
```
Next to _InteractiveSession_ there are multiple other imports that cause problems with monitoring.py. Changing to tf2's eager mode and not worry about the session did not do the trick.

Keras however runs fine and does not cause any problems (except the conv2d layer + CUDA what causes problems for a while already). 

**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: pip3
- TensorFlow version: 2.0.0rc1
- Python version: 3.6.8"
32573,TFTRT: CUDA_ERROR_ILLEGAL_ADDRESS,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.14
- Python version:2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:RTX2080

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Seeing the following error:
```
2019-09-16 23:41:12.835022: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:749] failed to record completion event; therefore, failed to create inter-stream dependency
2019-09-16 23:41:12.835047: I tensorflow/stream_executor/stream.cc:4813] [stream=0x5591077fbc60,impl=0x5590ffc9bd90] did not memcpy host-to-device; source: 0x7f7e8425bc80
2019-09-16 23:41:12.835058: E tensorflow/stream_executor/stream.cc:331] Error recording event in stream: error recording CUDA event on stream 0x5590ffc9b740: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.
2019-09-16 23:41:12.835076: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1ailed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
```
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32571,Expose core/kernels/linalg_ops_common to user_ops,"**System information**
- TensorFlow version r1.14:
- Are you willing to contribute it (Yes/No): No.

**Describe the feature and the current behavior/state.**

Expose core/kernels/linalg_ops_common to user applications in core/user_ops.

**Will this change the current api? How?**

It will allow user ops to access [`class LinearAlgebraOp : public OpKernel`](https://github.com/tensorflow/tensorflow/blob/512a3c3a6c0b36f6750394103b7eb316fe4ce853/tensorflow/core/kernels/linalg_ops_common.h#L40)

**Who will benefit with this feature?**

Users writing their own broadcasting ops.

**Any Other info.**

[here's an explanation of the whole thing](https://stackoverflow.com/questions/57912754/linking-into-the-tensorflow-framework-libraries/57966486#57966486)
"
32570,Assertion error when using mask with unrolled stacked LSTM,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14
- Python version: 3.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: no GPU, 32 GB ram

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I receive an assertion error when creating a forward pass for an unrolled multi-layer LSTM while using a mask.

**Describe the expected behavior**
No assertion error, or at least a better explanation as to the cause.

**Code to reproduce the issue**
```python
import tensorflow as tf

inputs = tf.placeholder(tf.float32, (3,4,5))
mask = tf.placeholder(tf.bool, (3,4))

single_cells = [tf.keras.layer.LSTMCell(10) for _ in range(3)]
multi_cell = tf.keras.layers.StackedRNNCell(cells=single_cells)
lstm = tf.keras.layers.RNN(cell=multi_cell, unroll=True)
output, state = lstm(inputs=inputs, mask=mask)  # <- assertion error occurs here
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32569,TF GPU doesn't recognise 2nd GPUs,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 pro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.14 GPU
- Python version: 3.7 64 bit
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: RTX 1070 (8GB) and GTX 1050 (2GB) , 32 GB Ram, Core i7 9700 k

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
TF GPu wont identify two GPUs, only GPU 0 appears

**Describe the expected behavior**
I want to see GPU0 and GPU 1 when run the device list command

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

After I install all, The TF GPU does not recognise my 2nd GPU (Nvidia GTX 1050) as GPU:01
![1](https://user-images.githubusercontent.com/45702408/65003983-57204880-d92d-11e9-8706-45c0f98179b1.JPG)
I can switch to that GPU, using Cuda visible devices, however can not use two GPUs  (for multi GPU calculations), any recommendations? 
"
32564,model.add_loss no longer works for tf.image.ssim_multiscale in tf 2.1,"This tf.keras code was valid in tf 1.13.1:

`model.add_loss(tf.image.ssim_multiscale(reconstruction.inputs[0], reconstruction.outputs[0], 1.0))`
with types of reconstruction.inputs[0], reconstruction.outputs[0] being:
```
(<tf.Tensor 'input_1:0' shape=(?, 256, 256, 1) dtype=float32>,
 <tf.Tensor 'reconstruction_/truediv:0' shape=(?, 256, 256, 1) dtype=float32>)
```

but is no longer valid in tf 1.14.0 or 1.15.0rc0:

```
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
(0) Invalid argument: You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,256,256,1]
[[{{node input_1}}]]
[[MS-SSIM_1/Scale1/cond/Pad/paddings/_3891]]
(1) Invalid argument: You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,256,256,1]
[[{{node input_1}}]]
0 successful operations.
0 derived errors ignored.
```

I can not find any documentation on this change or how to fix it in the newer tf versions."
32563,can't interpret primitive type objects correctly,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.0.0-dev20190915
- Python version:3.6.7
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

when i use primitive type objects during building model with tf.keras functional api. tensorflow mistake primtive type objects as tensors.

**Describe the expected behavior**

interpret primitive type as tensor at the right time.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import tensorflow as tf;
import numpy as np;

def Model(shape):
  input_shapes = [np.array(shape,dtype=np.int32), np.array(shape,dtype=np.int32) // 2, np.array(shape,dtype=np.int32) // 4];
  print(input_shapes)
  inputs = [tf.keras.Input(input_shape) for input_shape in input_shapes];
  outputs = list();
  for input in inputs:
    output = tf.keras.layers.Conv2D(filters=10,kernel_size=(3,3), padding='same')(input);
    outputs.append(output);
  return tf.keras.Model(inputs = inputs, outputs = outputs);
  
model = Model([32,32,32]);
model.save('model.h5');
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

>Traceback (most recent call last):            File ""test.py"", line 14, in <module>
    model = Model([32,32,32]);
  File ""test.py"", line 7, in Model
    inputs = [tf.keras.Input(input_shape) for input_shape in input_shapes];               File ""test.py"", line 7, in <listcomp>
    inputs = [tf.keras.Input(input_shape) for input_shape in input_shapes];
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/input_layer.py"", line 251, in Input
    if shape and batch_input_shape:
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
32562,Ungooglable error: dataset map over dictionaries with multiple input paths and float outputs,"The error in TF2b: ""ValueError: elems must be a 1+ dimensional Tensor, not a scalar""

A StackOverflow bounty is a few days from expiring; docs and code.  At my wits' end.

[SO 100+ bounty and gunga galunga](https://stackoverflow.com/questions/57894869/tf-dataset-multiple-path-inputs-and-mapping-per-batch-to-load-images)"
32561,[TF2.0] Cannot place the graph after loading weights trained with multiple GPUs and tf.distribute.MirroredStrategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: CentOS Linux 7.2.1511
- TensorFlow installed from: binary (pip)
- TensorFlow version: 2.0.0.dev20190915
- Python version: 3.6.8
- CUDA/cuDNN version: CUDA 10.0.130 / cuDNN 7.6.0
- GPU model and memory: 2x Nvidia Tesla V100-SXM2-16GB

**Describe the current behavior**
I am using tensorflow 2 on a cluster which limits the GPU hours per job. Therefore, I want to be able to load model weights and resume training the model from a checkpoint. The model in question is a `tf.keras` model compiled with `tf.keras.optimizers.SGD` as optimizer, with momentum. When attempting to resume training after loading weights that were trained with multiple GPUs and `tf.distribute.MirroredStrategy`, tensorflow crashes with the following error:
```text
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:GPU:1. The edge src node is sgd_sgd_update_update_0_resourceapplykerasmomentum_accum , and the dst node is SGD/SGD/update/update_1/ResourceApplyKerasMomentum [Op:__inference_distributed_function_1355]
```
The error does not occur when the model has been pre-trained on CPU, or with only one GPU.

**Describe the expected behavior**
The model should be able to resume training after loading weights trained on multiple GPUs.

**Code to reproduce the issue**
A minimal reproducible example is below. With access to at least 2 GPUs, begin training the model with `initial_run=True`. Interrupt training after a few epochs have completed, then re-run the script with `initial_run=False`. The model will attempt to load the previously trained weights and resume training from the last completed epoch, leading to the error.
```python
import tensorflow as tf

initial_run = True

batch_size = 1000

(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()
train_images = train_images / 255.0
train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
train_dataset = train_dataset.batch(batch_size).repeat()

strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])

    model.compile(
            optimizer=tf.keras.optimizers.SGD(momentum=0.9),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy'])

if not initial_run:
    model.load_weights(""latest_weights"")

model.fit(
        train_dataset,
        steps_per_epoch=len(train_images) / batch_size,
        epochs=1000,
        initial_epoch=int(model.optimizer.iterations.numpy() // (len(train_images) / batch_size)),
        callbacks=[
            tf.keras.callbacks.ModelCheckpoint(
                filepath=""latest_weights"",
                save_weights_only=True)])
```
**Other info / logs**
A log of the output from attempting to load weights and resume training is available here: https://pastebin.com/raw/Dmn2Jt0i"
32558,Loading weights from checkpoint file fails [python/c api],"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock example and costume code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Model created with pip package, training performed with C api from compiled libtensorflow
- TensorFlow version (use command below): 2.0rc
- Python version: 3.6
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
**My environment**
[tf_env.txt](https://gist.github.com/7PintsOfCherryGarcia/a04c7fe26ed2b546fe6bfc3ebc6b1d2f)


**Describe the current behavior**
I am using tensorflow's c API to train models created with tf.keras in python. After training in my C program a checkpoint file is created from which a model's weights can be restored for more training or prediction within my C program.
When trying to restore my model in python, trained weights can not be loaded neither by:
```python
model.load_weights(""path_to_checkpoint"")
```
nor with
```python
checkpoint = tf.train.Checkpoint(model=model)
checkpoint.restore(""path_to_checkpoint"")
```
The model loads successfully with:
```python
model = keras.experimental.load_from_saved_model(""path_to_model_folder"")
```
But this is an untrained model. To be precise, this model has the same weights as when the model was first defined.

**Describe the expected behavior**
In my C program I can load/save/restore my model with no problem, I am expecting to be able to load trained model and make predictions in python as well from the model trained in my C program.

**Code to reproduce the issue**
There are several steps that I will describe to accurately reproduce my problem. 

**1. Create model with:
[keras_model.py](https://gist.github.com/7PintsOfCherryGarcia/f0483137a952c3d5d2907d49d723ccf7)
```bash
python keras_model.py
``` 
This will create a keras model and export it in keras_model, this folder name **is hard coded in the C program**

**2. Get data with:**
[getData.py](https://gist.github.com/7PintsOfCherryGarcia/455370c9550a6c5c10f9135155a8b282)
```bash
python getData.py
```
This will download the fashion MNIST dataset and stored the training examples and labels as text files in:
data.txt and labels.txt **These names are hardcoded in the C program**

**3. Compile C program**
The only requirement for my C program is the tensorflow library shared object files libtensorflow.so and libtensorflow_framework.so and the c API header file c_api.h. These were compiled with bazel via:
```bash
./configure
bazel build -c opt //tensorflow/tools/lib_package:libtensorflow
```  
Extracting the shared object files from libtensorflow.tar.gz

My [C program](https://gist.github.com/7PintsOfCherryGarcia/d6d3df8e7282f2e635bf9362c81f9292) can then be compiled with:
```bash
gcc -Wall -I path_to_libtensorflow/include -L path_to_libtensorflow/lib -o BellyTF BellyTF.c -ltensorflow
```
Assuming data.txt, labels.txt and keras_model are in the same directory as the executing command run:
```bash
LD_LIBRARY_PATH=path_to_libtensorflow_lib ./BellyTF
```
This will load saved model, and training data, print predictions from untrained model, train for a number of epochs, print predictions from trained model and save weights as a checkpoint file in the variables folder inside keras_model

**4. Load trained mdel in python from checkpoints file**
This is where I encounter problems
```python
import tensorflow as tf
from tensorflow import keras
#Get data
fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
#scale and reshape
train_images = train_images/255
train_images = np.reshape(train_images,(60000,784))

#Load model
model = keras.experimental.load_from_saved_model(""keras_model"")
#Predictions are wrong
model.predict([[train_images[0]]])

#Try to load weights This produces AssertError (traceback at end of issue)
model.load_weights(""keras_model/variables/belly"")
AssertionError: Some objects had attributes which were not restored:

#Try load model via tf.train.Checkpoint
checkpoint = tf.train.Checkpoint(model=model)
status =  checkpoint.restore(""keras_model/variables/belly"")

#Prediction is wrong
model.predict([[train_images[0]]])
 ```

**Other info / logs**
Traceback from
```python
model.load_weights(""keras_model/variables/belly"")
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-39-db6e544ddcae> in <module>
----> 1 model.load_weights(""keras_model/variables/belly"")

~/Projects/tensorflow/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in load_weights(self, filepath, by_name)
    160         raise ValueError('Load weights is not yet supported with TPUStrategy '
    161                          'with steps_per_run greater than 1.')
--> 162     return super(Model, self).load_weights(filepath, by_name)
    163
    164   @trackable.no_automatic_dependency_tracking

~/Projects/tensorflow/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py in load_weights(self, filepath, by_name)
   1396         # streaming restore for any variables created in the future.
   1397         trackable_utils.streaming_restore(status=status, session=session)
-> 1398       status.assert_nontrivial_match()
   1399       return status
   1400     if h5py is None:

~/Projects/tensorflow/env/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py in assert_nontrivial_match(self)
    915     # assert_nontrivial_match and assert_consumed (and both are less
    916     # useful since we don't touch Python objects or Python state).
--> 917     return self.assert_consumed()
    918
    919   def _gather_saveable_objects(self):

~/Projects/tensorflow/env/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py in assert_consumed(self)
    892       raise AssertionError(
    893           ""Some objects had attributes which were not restored: %s"" %
--> 894           (unused_attributes,))
    895     for trackable in self._graph_view.list_objects():
    896       # pylint: disable=protected-access

AssertionError: Some objects had attributes which were not restored: {<tf.Variable 'dense_2/kernel:0' shape=(784, 128) dtype=float32, numpy=
array([[ 0.01530321, -0.07596322, -0.01604562, ...,  0.03402978,
         0.07713103, -0.05052958],
       [ 0.00116051,  0.04680964, -0.03951511, ..., -0.03624759,
        -0.04315009,  0.06628483],
       [ 0.0188334 , -0.06348058, -0.04110793, ..., -0.02473556,
        -0.04467853, -0.00360004],
       ...,
       [-0.01828651, -0.07760178, -0.06535166, ..., -0.04309935,
        -0.08069813,  0.00902149],
       [ 0.04599006,  0.02525605,  0.06842359, ..., -0.03162104,
        -0.02693111,  0.06349993],
       [-0.02491769,  0.0611873 , -0.00206578, ..., -0.01271369,
         0.00109962, -0.07020341]], dtype=float32)>: ['dense_2/kernel'], <tf.Variable 'dense_2/bias:0' shape=(128,) dtype=float32, numpy=
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['dense_2/bias'], <tf.Variable 'dense_1_1/kernel:0' shape=(128, 10) dtype=float32, numpy=
array([[ 0.16446821, -0.17563525,  0.18357025, ...,  0.08780123,
         0.07823227,  0.10018779],
       [ 0.0915112 , -0.17932694,  0.12889476, ...,  0.20635314,
        -0.09271443,  0.11488943],
       [-0.08094949, -0.14578271, -0.01433699, ..., -0.11150974,
        -0.19056591, -0.01099543],
       ...,
       [-0.08208692,  0.06081717,  0.0688145 , ..., -0.08729573,
         0.20750098, -0.08333559],
       [-0.15593694,  0.14401685, -0.20091254, ..., -0.15566093,
        -0.06369022, -0.12938659],
       [-0.18293834,  0.08427195, -0.02965383, ...,  0.02141935,
        -0.05810072, -0.04387315]], dtype=float32)>: ['dense_1_1/kernel'], <tf.Variable 'dense_1_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['dense_1_1/bias']}
```
"
32557,python kernel restart when training xgboost estimator,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- I am using a  stock example script provided in TensorFlow):
https://www.tensorflow.org/beta/tutorials/estimators/boosted_trees_model_understanding

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
debian linux intel x86_64 

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
NA

- TensorFlow installed from (source or binary):
binary via
pip install of tf2.0.0.rc1

- TensorFlow version (use command below):
tf2.0.0.rc1

- Python version:
3.7.4

- CUDA/cuDNN version:
How to determine?
- GPU model and memory:
GeForce RTX 2070 8GB

v2.0.0-rc0-101-gd2d2566 2.0.0-rc1


> == check python ===================================================
> python version: 3.7.4
> python branch: 
> python build version: ('default', 'Aug 13 2019 20:35:49')
> python compiler version: GCC 7.3.0
> python implementation: CPython
> 
> 
> == check os platform ===============================================
> os: Linux
> os kernel version: #1 SMP Debian 4.19.37-5+deb10u2 (2019-08-08)
> os release version: 4.19.0-5-amd64
> os platform: Linux-4.19.0-5-amd64-x86_64-with-debian-10.1
> linux distribution: ('debian', '10.1', '')
> linux os distribution: ('debian', '10.1', '')
> mac version: ('', ('', '', ''), '')
> uname: uname_result(system='Linux', node='gsd', release='4.19.0-5-amd64', version='#1 SMP Debian 4.19.37-5+deb10u2 (2019-08-08)', machine='x86_64', processor='')
> architecture: ('64bit', '')
> machine: x86_64
> 
> 
> == are we in docker =============================================
> No
> 
> == compiler =====================================================
> c++ (Debian 8.3.0-6) 8.3.0
> Copyright (C) 2018 Free Software Foundation, Inc.
> This is free software; see the source for copying conditions.  There is NO
> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
> 
> 
> == check pips ===================================================
> numpy                1.17.2              
> protobuf             3.9.1               
> tensorflow           2.0.0rc1            
> 
> == check for virtualenv =========================================
> False
> 
> == tensorflow import ============================================
> tf.version.VERSION = 2.0.0-rc1
> tf.version.GIT_VERSION = v2.0.0-rc0-101-gd2d2566
> tf.version.COMPILER_VERSION = 7.3.1 20180303
>      22191:	find library=libpthread.so.0 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/bin/../lib/tls/haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/bin/../lib/tls/haswell:/home/davis/anaconda3/envs/py3tf2/bin/../lib/tls/x86_64:/home/davis/anaconda3/envs/py3tf2/bin/../lib/tls:/home/davis/anaconda3/envs/py3tf2/bin/../lib/haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/bin/../lib/haswell:/home/davis/anaconda3/envs/py3tf2/bin/../lib/x86_64:/home/davis/anaconda3/envs/py3tf2/bin/../lib		(RPATH from file /home/davis/anaconda3/envs/py3tf2/bin/python)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/tls/haswell/x86_64/libpthread.so.0
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/tls/haswell/libpthread.so.0
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/tls/x86_64/libpthread.so.0
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/tls/libpthread.so.0
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/haswell/x86_64/libpthread.so.0
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/haswell/libpthread.so.0
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/x86_64/libpthread.so.0
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/libpthread.so.0
>      22191:	 search cache=/etc/ld.so.cache
>      22191:	  trying file=/lib/x86_64-linux-gnu/libpthread.so.0
>      22191:	
>      22191:	find library=libc.so.6 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/bin/../lib		(RPATH from file /home/davis/anaconda3/envs/py3tf2/bin/python)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/libc.so.6
>      22191:	 search cache=/etc/ld.so.cache
>      22191:	  trying file=/lib/x86_64-linux-gnu/libc.so.6
>      22191:	
>      22191:	find library=libdl.so.2 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/bin/../lib		(RPATH from file /home/davis/anaconda3/envs/py3tf2/bin/python)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/libdl.so.2
>      22191:	 search cache=/etc/ld.so.cache
>      22191:	  trying file=/lib/x86_64-linux-gnu/libdl.so.2
>      22191:	
>      22191:	find library=libutil.so.1 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/bin/../lib		(RPATH from file /home/davis/anaconda3/envs/py3tf2/bin/python)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/libutil.so.1
>      22191:	 search cache=/etc/ld.so.cache
>      22191:	  trying file=/lib/x86_64-linux-gnu/libutil.so.1
>      22191:	
>      22191:	find library=librt.so.1 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/bin/../lib		(RPATH from file /home/davis/anaconda3/envs/py3tf2/bin/python)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/librt.so.1
>      22191:	 search cache=/etc/ld.so.cache
>      22191:	  trying file=/lib/x86_64-linux-gnu/librt.so.1
>      22191:	
>      22191:	find library=libm.so.6 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/bin/../lib		(RPATH from file /home/davis/anaconda3/envs/py3tf2/bin/python)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/libm.so.6
>      22191:	 search cache=/etc/ld.so.cache
>      22191:	  trying file=/lib/x86_64-linux-gnu/libm.so.6
>      22191:	
>      22191:	
>      22191:	calling init: /lib/x86_64-linux-gnu/libpthread.so.0
>      22191:	
>      22191:	
>      22191:	calling init: /lib/x86_64-linux-gnu/libc.so.6
>      22191:	
>      22191:	
>      22191:	calling init: /lib/x86_64-linux-gnu/libm.so.6
>      22191:	
>      22191:	
>      22191:	calling init: /lib/x86_64-linux-gnu/librt.so.1
>      22191:	
>      22191:	
>      22191:	calling init: /lib/x86_64-linux-gnu/libutil.so.1
>      22191:	
>      22191:	
>      22191:	calling init: /lib/x86_64-linux-gnu/libdl.so.2
>      22191:	
>      22191:	
>      22191:	initialize program: /home/davis/anaconda3/envs/py3tf2/bin/python
>      22191:	
>      22191:	
>      22191:	transferring control: /home/davis/anaconda3/envs/py3tf2/bin/python
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_heapq.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_opcode.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	find library=libffi.so.6 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../tls/haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../tls/haswell:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../tls/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../tls:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../haswell:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../..		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../tls/haswell/x86_64/libffi.so.6
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../tls/haswell/libffi.so.6
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../tls/x86_64/libffi.so.6
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../tls/libffi.so.6
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../haswell/x86_64/libffi.so.6
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../haswell/libffi.so.6
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../x86_64/libffi.so.6
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../libffi.so.6
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../libffi.so.6
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_struct.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	find library=libmkl_rt.so [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../tls/haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../tls/haswell:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../tls/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../tls:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../haswell:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../..		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/_mklinit.cpython-37m-x86_64-linux-gnu.so)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../tls/haswell/x86_64/libmkl_rt.so
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../tls/haswell/libmkl_rt.so
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../tls/x86_64/libmkl_rt.so
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../tls/libmkl_rt.so
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../haswell/x86_64/libmkl_rt.so
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../haswell/libmkl_rt.so
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../x86_64/libmkl_rt.so
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../libmkl_rt.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../libmkl_rt.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/_mklinit.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/_mklinit.cpython-37m-x86_64-linux-gnu.so: error: symbol lookup error: undefined symbol: omp_get_num_threads (fatal)
>      22191:	find library=libiomp5.so [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../..		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/_mklinit.cpython-37m-x86_64-linux-gnu.so)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../libiomp5.so
>      22191:	
>      22191:	find library=libgcc_s.so.1 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/bin/../lib		(RPATH from file /home/davis/anaconda3/envs/py3tf2/bin/python)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/libgcc_s.so.1
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/bin/../lib/libgcc_s.so.1
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../libiomp5.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/math.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_datetime.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_pickle.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/core/_multiarray_tests.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/linalg/lapack_lite.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/linalg/_umath_linalg.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	find library=libz.so.1 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../..		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../libz.so.1
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../libz.so.1
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/zlib.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_bz2.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	find library=liblzma.so.5 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../..		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../liblzma.so.5
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../liblzma.so.5
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_lzma.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/grp.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_decimal.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/fft/fftpack_lite.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/mkl_fft/_pydfti.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/random/mtrand.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	find library=libcrypto.so.1.1 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../..		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../libcrypto.so.1.1
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../libcrypto.so.1.1
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_hashlib.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_blake2.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_sha3.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_bisect.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_random.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../libmkl_core.so
>      22191:	
>      22191:	find library=libittnotify.so [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/bin/../lib		(RPATH from file /home/davis/anaconda3/envs/py3tf2/bin/python)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/libittnotify.so
>      22191:	 search cache=/etc/ld.so.cache
>      22191:	 search path=/lib/x86_64-linux-gnu/tls/haswell/x86_64:/lib/x86_64-linux-gnu/tls/haswell:/lib/x86_64-linux-gnu/tls/x86_64:/lib/x86_64-linux-gnu/tls:/lib/x86_64-linux-gnu/haswell/x86_64:/lib/x86_64-linux-gnu/haswell:/lib/x86_64-linux-gnu/x86_64:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/tls/haswell/x86_64:/usr/lib/x86_64-linux-gnu/tls/haswell:/usr/lib/x86_64-linux-gnu/tls/x86_64:/usr/lib/x86_64-linux-gnu/tls:/usr/lib/x86_64-linux-gnu/haswell/x86_64:/usr/lib/x86_64-linux-gnu/haswell:/usr/lib/x86_64-linux-gnu/x86_64:/usr/lib/x86_64-linux-gnu:/lib/tls/haswell/x86_64:/lib/tls/haswell:/lib/tls/x86_64:/lib/tls:/lib/haswell/x86_64:/lib/haswell:/lib/x86_64:/lib:/usr/lib/tls/haswell/x86_64:/usr/lib/tls/haswell:/usr/lib/tls/x86_64:/usr/lib/tls:/usr/lib/haswell/x86_64:/usr/lib/haswell:/usr/lib/x86_64:/usr/lib		(system search path)
>      22191:	  trying file=/lib/x86_64-linux-gnu/tls/haswell/x86_64/libittnotify.so
>      22191:	  trying file=/lib/x86_64-linux-gnu/tls/haswell/libittnotify.so
>      22191:	  trying file=/lib/x86_64-linux-gnu/tls/x86_64/libittnotify.so
>      22191:	  trying file=/lib/x86_64-linux-gnu/tls/libittnotify.so
>      22191:	  trying file=/lib/x86_64-linux-gnu/haswell/x86_64/libittnotify.so
>      22191:	  trying file=/lib/x86_64-linux-gnu/haswell/libittnotify.so
>      22191:	  trying file=/lib/x86_64-linux-gnu/x86_64/libittnotify.so
>      22191:	  trying file=/lib/x86_64-linux-gnu/libittnotify.so
>      22191:	  trying file=/usr/lib/x86_64-linux-gnu/tls/haswell/x86_64/libittnotify.so
>      22191:	  trying file=/usr/lib/x86_64-linux-gnu/tls/haswell/libittnotify.so
>      22191:	  trying file=/usr/lib/x86_64-linux-gnu/tls/x86_64/libittnotify.so
>      22191:	  trying file=/usr/lib/x86_64-linux-gnu/tls/libittnotify.so
>      22191:	  trying file=/usr/lib/x86_64-linux-gnu/haswell/x86_64/libittnotify.so
>      22191:	  trying file=/usr/lib/x86_64-linux-gnu/haswell/libittnotify.so
>      22191:	  trying file=/usr/lib/x86_64-linux-gnu/x86_64/libittnotify.so
>      22191:	  trying file=/usr/lib/x86_64-linux-gnu/libittnotify.so
>      22191:	  trying file=/lib/tls/haswell/x86_64/libittnotify.so
>      22191:	  trying file=/lib/tls/haswell/libittnotify.so
>      22191:	  trying file=/lib/tls/x86_64/libittnotify.so
>      22191:	  trying file=/lib/tls/libittnotify.so
>      22191:	  trying file=/lib/haswell/x86_64/libittnotify.so
>      22191:	  trying file=/lib/haswell/libittnotify.so
>      22191:	  trying file=/lib/x86_64/libittnotify.so
>      22191:	  trying file=/lib/libittnotify.so
>      22191:	  trying file=/usr/lib/tls/haswell/x86_64/libittnotify.so
>      22191:	  trying file=/usr/lib/tls/haswell/libittnotify.so
>      22191:	  trying file=/usr/lib/tls/x86_64/libittnotify.so
>      22191:	  trying file=/usr/lib/tls/libittnotify.so
>      22191:	  trying file=/usr/lib/haswell/x86_64/libittnotify.so
>      22191:	  trying file=/usr/lib/haswell/libittnotify.so
>      22191:	  trying file=/usr/lib/x86_64/libittnotify.so
>      22191:	  trying file=/usr/lib/libittnotify.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../libmkl_intel_thread.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../libmkl_intel_lp64.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../libmkl_avx2.so
>      22191:	
>      22191:	find library=libtensorflow_framework.so.2 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/haswell:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/haswell:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/tls/haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/tls/haswell:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/tls/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/tls:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/haswell:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../tls/haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../tls/haswell:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../tls/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../tls:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../haswell:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/..		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/haswell/x86_64/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/haswell/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/haswell/x86_64/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/haswell/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/tls/haswell/x86_64/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/tls/haswell/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/tls/x86_64/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/tls/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/haswell/x86_64/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/haswell/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/x86_64/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../tls/haswell/x86_64/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../tls/haswell/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../tls/x86_64/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../tls/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../haswell/x86_64/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../haswell/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../x86_64/libtensorflow_framework.so.2
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.2
>      22191:	
>      22191:	find library=libstdc++.so.6 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/..		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/libstdc++.so.6
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../libstdc++.so.6
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/bin/../lib		(RPATH from file /home/davis/anaconda3/envs/py3tf2/bin/python)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/libstdc++.so.6
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/bin/../lib/libstdc++.so.6
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.2
>      22191:	
>      22191:	find library=libhdfs.so [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/..		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../libhdfs.so
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/..		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/libhdfs.so
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../libhdfs.so
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/bin/../lib		(RPATH from file /home/davis/anaconda3/envs/py3tf2/bin/python)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/bin/../lib/libhdfs.so
>      22191:	 search cache=/etc/ld.so.cache
>      22191:	 search path=/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/lib:/usr/lib		(system search path)
>      22191:	  trying file=/lib/x86_64-linux-gnu/libhdfs.so
>      22191:	  trying file=/usr/lib/x86_64-linux-gnu/libhdfs.so
>      22191:	  trying file=/lib/libhdfs.so
>      22191:	  trying file=/usr/lib/libhdfs.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/binascii.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_posixsubprocess.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/select.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/pyexpat.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_socket.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	
>      22191:	
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_csv.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/fcntl.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/termios.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/fast_tensor_util.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_queue.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/wrapt/_wrappers.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_json.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	find library=libssl.so.1.1 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../..		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../libssl.so.1.1
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../libssl.so.1.1
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_ssl.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/array.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	find library=libhdf5-978d01a9.so.103.0.0 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/tls/haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/tls/haswell:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/tls/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/tls:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/haswell:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/_errors.cpython-37m-x86_64-linux-gnu.so)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/tls/haswell/x86_64/libhdf5-978d01a9.so.103.0.0
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/tls/haswell/libhdf5-978d01a9.so.103.0.0
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/tls/x86_64/libhdf5-978d01a9.so.103.0.0
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/tls/libhdf5-978d01a9.so.103.0.0
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/haswell/x86_64/libhdf5-978d01a9.so.103.0.0
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/haswell/libhdf5-978d01a9.so.103.0.0
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/x86_64/libhdf5-978d01a9.so.103.0.0
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/libhdf5-978d01a9.so.103.0.0
>      22191:	
>      22191:	find library=libhdf5_hl-db841637.so.100.1.1 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/_errors.cpython-37m-x86_64-linux-gnu.so)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1
>      22191:	
>      22191:	find library=libsz-1c7dd0cf.so.2.0.1 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./tls/haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./tls/haswell:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./tls/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./tls:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./haswell/x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./haswell:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./x86_64:/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/.		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/libhdf5-978d01a9.so.103.0.0)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./tls/haswell/x86_64/libsz-1c7dd0cf.so.2.0.1
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./tls/haswell/libsz-1c7dd0cf.so.2.0.1
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./tls/x86_64/libsz-1c7dd0cf.so.2.0.1
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./tls/libsz-1c7dd0cf.so.2.0.1
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./haswell/x86_64/libsz-1c7dd0cf.so.2.0.1
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./haswell/libsz-1c7dd0cf.so.2.0.1
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./x86_64/libsz-1c7dd0cf.so.2.0.1
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1
>      22191:	
>      22191:	find library=libaec-2147abcd.so.0.0.4 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/.		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/libhdf5-978d01a9.so.103.0.0)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4
>      22191:	
>      22191:	find library=libz-a147dcb0.so.1.2.3 [0]; searching
>      22191:	 search path=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/.		(RPATH from file /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/libhdf5-978d01a9.so.103.0.0)
>      22191:	  trying file=/home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/libhdf5-978d01a9.so.103.0.0
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/_errors.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/defs.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/_objects.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/_conv.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5r.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5t.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/utils.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22192:	find library=libc.so.6 [0]; searching
>      22192:	 search cache=/etc/ld.so.cache
>      22192:	  trying file=/lib/x86_64-linux-gnu/libc.so.6
>      22192:	
>      22192:	
>      22192:	calling init: /lib/x86_64-linux-gnu/libc.so.6
>      22192:	
>      22192:	
>      22192:	initialize program: /bin/sh
>      22192:	
>      22192:	
>      22192:	transferring control: /bin/sh
>      22192:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5z.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5a.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5s.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5p.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5ac.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/_proxy.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5d.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5ds.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5f.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5g.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5i.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5fd.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5pl.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5o.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5l.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/conversion.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/c_timestamp.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/nattype.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/np_datetime.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/timezones.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/tzconversion.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/timedeltas.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/offsets.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/ccalendar.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/strptime.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/fields.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/parsing.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/period.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/frequencies.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/timestamps.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/resolution.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/hashtable.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/missing.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/lib.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslib.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/algos.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/interval.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/properties.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/hashing.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/ops.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/index.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/join.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/sparse.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/indexing.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/internals.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/unicodedata.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/mmap.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/reshape.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/window.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/skiplist.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/groupby.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/reduction.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/parsers.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/json.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/writers.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/util/_move.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/io/msgpack/_packer.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/io/msgpack/_unpacker.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling init: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/testing.cpython-37m-x86_64-linux-gnu.so
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/bin/python [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_heapq.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_opcode.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../libffi.so.6 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_struct.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/_mklinit.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../libiomp5.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/math.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_datetime.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_pickle.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/core/_multiarray_tests.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/linalg/lapack_lite.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/linalg/_umath_linalg.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/zlib.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_bz2.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_lzma.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../liblzma.so.5 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/grp.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_decimal.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/fft/fftpack_lite.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/mkl_fft/_pydfti.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/random/mtrand.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_hashlib.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_blake2.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_sha3.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_bisect.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_random.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../libmkl_intel_lp64.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../libmkl_avx2.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../libmkl_intel_thread.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../libmkl_core.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/numpy/../../../libmkl_rt.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/binascii.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../libz.so.1 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_posixsubprocess.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/select.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/pyexpat.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_socket.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	
>      22191:	
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_csv.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/fcntl.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/termios.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/fast_tensor_util.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_queue.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/wrapt/_wrappers.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_json.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/_ssl.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../libssl.so.1.1 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/../../libcrypto.so.1.1 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/array.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/_errors.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/defs.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/_objects.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/_conv.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5r.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5t.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/utils.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5z.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5a.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5s.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5p.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5ac.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/_proxy.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5d.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5ds.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5f.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5g.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5i.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5fd.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5pl.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5o.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/h5l.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/libhdf5-978d01a9.so.103.0.0 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/../libtensorflow_framework.so.2 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/conversion.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/c_timestamp.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/nattype.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/np_datetime.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/timezones.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/tzconversion.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/timedeltas.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/offsets.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/ccalendar.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/strptime.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/fields.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/parsing.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/period.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/frequencies.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/timestamps.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslibs/resolution.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/hashtable.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/missing.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/lib.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/tslib.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/algos.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/interval.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/properties.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/hashing.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/ops.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/index.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/join.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/sparse.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/indexing.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/internals.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/unicodedata.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/lib-dynload/mmap.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/reshape.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/window.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/bin/../lib/libstdc++.so.6 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/bin/../lib/libgcc_s.so.1 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /lib/x86_64-linux-gnu/libm.so.6 [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/skiplist.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/groupby.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/reduction.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/parsers.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/json.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/writers.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/util/_move.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/io/msgpack/_packer.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/io/msgpack/_unpacker.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/pandas/_libs/testing.cpython-37m-x86_64-linux-gnu.so [0]
>      22191:	
>      22191:	
>      22191:	calling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]
>      22191:	
> 
> == env ==========================================================
> LD_LIBRARY_PATH is unset
> DYLD_LIBRARY_PATH is unset
> 
> == nvidia-smi ===================================================
> Mon Sep 16 13:02:01 2019       
> +-----------------------------------------------------------------------------+
> | NVIDIA-SMI 418.74       Driver Version: 418.74       CUDA Version: 10.1     |
> |-------------------------------+----------------------+----------------------+
> | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
> | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
> |===============================+======================+======================|
> |   0  GeForce RTX 2070    On   | 00000000:01:00.0  On |                  N/A |
> | 38%   30C    P8    14W / 175W |   1249MiB /  7949MiB |      6%      Default |
> +-------------------------------+----------------------+----------------------+
>                                                                                
> +-----------------------------------------------------------------------------+
> | Processes:                                                       GPU Memory |
> |  GPU       PID   Type   Process name                             Usage      |
> |=============================================================================|
> |    0       814      G   /usr/lib/xorg/Xorg                            34MiB |
> |    0       948      G   /usr/bin/gnome-shell                          24MiB |
> |    0      1171      G   /usr/lib/xorg/Xorg                           217MiB |
> |    0      1250      G   /usr/bin/gnome-shell                         176MiB |
> |    0      1551      G   ...uest-channel-token=12821291265308298198     8MiB |
> |    0      2551      G   ...equest-channel-token=447657113308850378   786MiB |
> +-----------------------------------------------------------------------------+
> 
> == cuda libs  ===================================================
> 
> == tensorflow installed from info ==================
> Name: tensorflow
> Version: 2.0.0rc1
> Summary: TensorFlow is an open source machine learning framework for everyone.
> Home-page: https://www.tensorflow.org/
> Author-email: packages@tensorflow.org
> License: Apache 2.0
> Location: /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages
> Required-by: 
> 
> == python version  ==============================================
> (major, minor, micro, releaselevel, serial)
> (3, 7, 4, 'final', 0)
> 
> == bazel version  ===============================================
> 

**Describe the current behavior**

During the train of the xgboosted tree, the kernel will abort and halt.

**Describe the expected behavior**

That the python kernel operate normally.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

> import numpy as np
> import pandas as pd
> import tensorflow as tf
> import ssl
> dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')
> dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')
> y_train = dftrain.pop('survived')
> y_eval = dfeval.pop('survived')
> try:
>   %tensorflow_version 2.x
> except Exception:
>   pass
> import tensorflow as tf
> tf.random.set_seed(123)
> print( ""tensorflow version = {}"".format(tf.version))
> import platform
> print(platform.python_version())
> vocabulary = dftrain['sex'].unique()
> fc = tf.feature_column
> CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck', 'embark_town', 'alone']
> NUMERIC_COLUMNS = ['age', 'fare']
>   
> def one_hot_cat_column(feature_name, vocab):
>     return tf.feature_column.indicator_column(
>         tf.feature_column.categorical_column_with_vocabulary_list(feature_name,
>                                                                 vocab))
> feature_columns = []
> for feature_name in CATEGORICAL_COLUMNS:
>     # Need to one-hot encode categorical features.
>     # He finds the unique values in each column and uses that
>     # array as a vocabulary
>     vocabulary = dftrain[feature_name].unique()
>     feature_columns.append(one_hot_cat_column(feature_name, vocabulary))
> 
> for feature_name in NUMERIC_COLUMNS:
>     feature_columns.append(tf.feature_column.numeric_column(feature_name,dtype=tf.float32))
> 
> example = dict(dftrain.head(1))
> class_fc = tf.feature_column.indicator_column(
>     tf.feature_column.categorical_column_with_vocabulary_list('class', ('First', 'Second', 'Third')))
> print('Feature value: ""{}""'.format(example['class'].iloc[0]))
> print('One-hot encoded: ', tf.keras.layers.DenseFeatures([class_fc])(example).numpy())
> 
> # Use entire batch since this is such a small dataset.
> NUM_EXAMPLES = len(y_train)
> 
> def make_input_fn(X, y, n_epochs=None, shuffle=True):
>   def input_fn():
>     dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))
>     if shuffle:
>       dataset = dataset.shuffle(NUM_EXAMPLES)
>     # For training, cycle thru dataset as many times as need (n_epochs=None).    
>     dataset = dataset.repeat(n_epochs)
>     # In memory training doesn't use batching.
>     dataset = dataset.batch(NUM_EXAMPLES)
>     return dataset
>   return input_fn
> 
> # Training and evaluation input functions.
> train_input_fn = make_input_fn(dftrain, y_train)
> eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)
> linear_est = tf.estimator.LinearClassifier(feature_columns)
> 
> # Train model.
> linear_est.train(train_input_fn, max_steps=100)
> 
> # Evaluation.
> result = linear_est.evaluate(eval_input_fn)
> 
> # Since data fits into memory, use entire dataset per layer. It will be faster.
> # Above one batch is defined as the entire dataset. 
> n_batches = 1
> est = tf.estimator.BoostedTreesClassifier(feature_columns,
>                                           n_batches_per_layer=n_batches)
> 
> # The model will stop training once the specified number of trees is built, not 
> # based on the number of steps.
> est.train(train_input_fn, max_steps=100)   ##### <<<<<---- this kills notebook kernel
> 
> 


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Output of the est.train call at end of sample script.  This output is generated and then the jupyter notebook will pop up a dialog which says kernel has stopped.

> INFO:tensorflow:Calling model_fn.
> WARNING:tensorflow:From /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py:2158: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
> Instructions for updating:
> The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
> WARNING:tensorflow:From /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py:2158: IndicatorColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
> Instructions for updating:
> The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
> WARNING:tensorflow:From /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4302: VocabularyListCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
> Instructions for updating:
> The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
> WARNING:tensorflow:From /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py:2158: VocabularyListCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
> Instructions for updating:
> The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
> WARNING:tensorflow:From /home/davis/anaconda3/envs/py3tf2/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py:161: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
> Instructions for updating:
> Use `tf.cast` instead.
> INFO:tensorflow:Done calling model_fn.
> INFO:tensorflow:Create CheckpointSaverHook.
> WARNING:tensorflow:Issue encountered when serializing resources.
> Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
> '_Resource' object has no attribute 'name'
> INFO:tensorflow:Graph was finalized.
> INFO:tensorflow:Running local_init_op.
> INFO:tensorflow:Done running local_init_op.
> WARNING:tensorflow:Issue encountered when serializing resources.
> Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
> '_Resource' object has no attribute 'name'
> INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpr2q3edmx/model.ckpt.
> WARNING:tensorflow:Issue encountered when serializing resources.
> Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
> '_Resource' object has no attribute 'name'
> INFO:tensorflow:loss = 0.6931468, step = 0
> WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
> ​"
32556,Bazel workspace.bzl requires a dependency on rules_closure,"Per https://stackoverflow.com/questions/52482972/running-load-within-skylark-macro
users of Tensorflow are adding rules_closure to their `WORKSPACE` file even though they don't use it.

rules_closure is minimally maintained (my team has just acquired it). We don't want new dependencies on it and have discovered that most dependencies are due to this issue.

I suspect the Tensorflow users don't actually want to build a Closure Compiler-minified UI, so any `load()` statement from rules_closure doesn't belong in the tensorflow bazel rules distribution.

Note, #15997 suggests customizing the dependencies, but in this issue I propose that users should never observe this dependency at all."
32555,Can't import from subpackages,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0rc1
- Python version: 3.7.2
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory: GeForce GTX Titan X, 12GB

**Describe the current behavior**

As of v2.0.0rc1, I can't import anything from subpackages. Attempting to do so results in the error ""ModuleNotFoundError: No module named ..."".

I had upgraded to rc1 using pip. Uninstalling and reinstalling tensorflow did not help. The issue is present in both tensorflow and tensorflow-gpu. This issue is not present in v2.0.0-rc0, including after reverting to it.

**Describe the expected behavior**

Previously, it has always been possible to import definitions directly from subpackages.

**Code to reproduce the issue**

Imports like the following previously worked and now both result in the error specified above:

    from tensorflow.data import *
    from tensorflow.io import TFRecordWriter

The following approaches still work:

    import tensorflow as tf
    writer = tf.io.TFRecordWriter

    from tensorflow import io
    writer = io.TFRecordWriter
"
32554,Survey on Pull Request Prioritization,"Dear Pull Requests integrators,
We are an international group of researchers investigating Pull Requests management activities. We implemented an automated approach, named CARTESIAN, for prioritizing Pull Requests (PRs) received by an open-source project according to the likelihood of acceptance/response. We would like to evaluate whether CARTESIAN can help integrators when reviewing PRs. 
As we noticed that your project receives many PRs daily, we have taken the liberty of contacting you. Thus, we experimented CARTESIAN on Your projects, to help you in prioritizing PRs. To know more about these results, please fill in the form below. 
Survey form link: https://forms.gle/bGkj3nRH9i6ArjUx8 
Your participation is voluntary and confidential. We kindly request you, ONLY to INTEGRATORS, to participate in this study which is expected to take about 15 minutes of your time. You might withdraw at any time.
Best regards,
Muhammad Ilyas Azeem, National Engineering Research Center of Fundamental Software, Chinese Academy of Sciences, China
Andrea Di Sorbo, University of Sannio, Italy
Sebastiano Panichella, Zurich University of Applied Science, Switzerland
Alexander Serebrenik, Eindhoven University of Technology, The Netherlands
"
32553,FutureWarning in TensorFlow 1.14 after NumPy update.,"System information:
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
    TensorFlow installed from (source or binary): binaries from pip
    TensorFlow version (use command below):1.14
    Python version: 3.6.1
    CUDA/cuDNN version: 10.0 with any 7.5.0
    GPU model and memory: Geforce RTX 2060


After updating the NumPy to the latest version (1.17.2+mkl),
I receive the following warnings in TensorFlow 1.14:

```
2019-09-16T13:46:07.976+0200	WARNING	C:\python\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
	py.warnings:99
```
```
2019-09-16T13:46:07.976+0200	WARNING	C:\python\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
	py.warnings:99
```
```
2019-09-16T13:46:07.977+0200	WARNING	C:\python\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
	py.warnings:99
```
```
2019-09-16T13:46:07.977+0200	WARNING	C:\python\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
	py.warnings:99
```
```
2019-09-16T13:46:07.977+0200	WARNING	C:\python\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
	py.warnings:99
```
```
2019-09-16T13:46:07.977+0200	WARNING	C:\python\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
	py.warnings:99
```
```
2019-09-16T13:46:09.194+0200	WARNING	C:\python\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
	py.warnings:99
```
```
2019-09-16T13:46:09.194+0200	WARNING	C:\python\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
	py.warnings:99
```
```
2019-09-16T13:46:09.195+0200	WARNING	C:\python\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
	py.warnings:99
```
```
2019-09-16T13:46:09.195+0200	WARNING	C:\python\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
	py.warnings:99
```
```
2019-09-16T13:46:09.195+0200	WARNING	C:\python\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
	py.warnings:99
```
```
2019-09-16T13:46:09.195+0200	WARNING	C:\python\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
	py.warnings:99
```"
32552,CUDNN_STATUS_ALLOC_FAILED with minimal network and data using CUDA 10.0 and CuDNN 7.6.x,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binaries from pip
- TensorFlow version (use command below):1.13.1 through 2.0.0rc1 (e.g., v2.0.0-rc0-101-gd2d2566eef 2.0.0-rc1)
- Python version: 3.6.8 and 3.7.4
- Bazel version (if compiling from source): none
- GCC/Compiler version (if compiling from source): none
- CUDA/cuDNN version: 10.0 with any 7.6.x
- GPU model and memory: Geforce RTX 2060 (Laptop)

**Describe the current behavior**
Code stops with CUDNN_STATUS_ALLOC_FAILED if using CuDNN 7.6 on CUDA 10.0.
Code works fine if using CuDNN <=7.5 or if using CUDA 9.0 or on different computer (Quadro M5000).
(But TF 2.0 binaries require CUD 10.0, CuDNN >= 7.6.)
Code also works when adding a dense layer (`model.add(layers.Dense(1))`) or changing the input shape to `(3,1)`.

**Describe the expected behavior**
Code finishes.

**Code to reproduce the issue**
```
import numpy as np
from tensorflow.keras import layers, models, optimizers

model = models.Sequential()
model.add(layers.Conv1D(1, 3, input_shape=(8, 1)))


optimizer = optimizers.Adam(lr=1e-6)
model.compile(optimizer=optimizer, loss='mse')
x = np.zeros((1, *model.input.shape[1:]))
y = np.zeros((1, *model.output.shape[1:]))
model.fit(x, y)
```

**Other info / logs**
Here is some more detailed information: https://stackoverflow.com/questions/57872336/should-i-be-able-to-use-precompiled-tensorflow-2-on-a-laptop-geforce-rtx-2060-gp"
32551,Session() freezes the machine.,"## System info:

- Ubuntu 18.04
- Nvidia Drivers: 390.116
- TF version: v1.13.1-0-g6612da8951 (installed using pip3)
- Cuda: 10.0
- CUDNN: 7.5.1
- Python: 3.6.8
- GPU: 2 x GeForce GTX 1080 Ti

## Description

I have 5 alienware machines that have been working for over one year with the above configuration. Last week I noticed that tensorflow does not work anymore when I try to open a session:
```
import tensorflow as tf 
sess = tf.Session()
```
If I do so it freezes completely my pc. I'll try to add a screen after I post this message. I insist that for all the machines I am experiencing the very same problem.

"
32550,Timeline reports incorrect GPU memory usage,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): conda (binary)
- TensorFlow version (use command below): 1.14 gpu
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

My objective is to measure the amount of GPU memory needed by BERT during the inference process. I have already set the TF_FORCE_GPU_ALLOW_GROWTH (or a paraphrase thereof, I don't remember the exact flag) to true. NVIDIA-SMI shows a usage of ~2500 MB for a batch size of 227. 

1. Even if the allow growth is true, the tensorflow allocation is taking place in steps. I see the same peak usage in nvidia-smi for batch sizes 16, 32, 64 and then it jumps up suddenly. Usage for batch sizes 128 through 227 is as above, while for batch size 228, it is suddenly 4495MB. Hence, this measurement does not serve my purpose. It clearly depends more on the allocator behaviour than the network size + activations size.

2. Timeline visualisation in chrome trace format shows a peak allocation on GPU_0_bfc as ~418MB for a batch size of 227. This is way too low. If I compare it with nvidia-smi trace (peaking at ~2500MB) this makes no sense.

3. Tensorboard visualisation somehow shows the BERT node as using 175MB. Again, it makes no sense.

**Describe the expected behavior**
Please help me get an accurate measurement of how much memory my current model and batch size is really taking to run on a GPU with TF. The only trustworthy metric I see in this labyrinth is nvidia-smi, but only for the batch sizes at which it suddenly steps up.


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Can use the google research bert and profile for memory using ProfilerHooks.


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32549,Chief-worker should waitting for all other workers.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): tensorflow1.12
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**
Use Ps-worker mode.
When I use Estimator or MonitoredTrainingSession, I should have a chief-worker.
Chief-worker will do some chief-only hooks, like CheckpointSaverHook.
But now, I can't guarantee that chief-worker will quit before all other workers.
If chief-worker is quitted before all others workers, nobody will do the CheckpointSaverHook.

**Will this change the current api? How?**
I have no elegant idea so far.

**Who will benefit with this feature?**
All.

**Any Other info.**
So how can I fix the problem? Maybe you can offer some good ideas. 

Thanks so much :)
"
32548,"tensorflow 2rc1 Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): - Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2rc1
- Python version: 3.6.7
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce RTX 2070 - 8GRAM


**Describe the current behavior**
My program crashes when I try to execute it.

**Describe the expected behavior**
I want to fit my model to the dataset. 

**Code to reproduce the issue**

```
from __future__ import absolute_import, division, print_function, unicode_literals

import numpy as np
import tensorflow as tf

DATA_URL = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'

path = tf.keras.utils.get_file('mnist.npz', DATA_URL)
with np.load(path) as data:
  train_examples = data['x_train']
  train_labels = data['y_train']
  test_examples = data['x_test']
  test_labels = data['y_test']


train_examples = train_examples.reshape((-1, 28, 28, 1))
train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))
test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))
BATCH_SIZE = 64
SHUFFLE_BUFFER_SIZE = 100

train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dense(10, activation='softmax'))


model.compile(optimizer=tf.keras.optimizers.RMSprop(),
                loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

model.fit(train_dataset, epochs=10)
```

Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**

I've tried to do use these options which didn't help:


```
# 1st option
physical_devices = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)
```


```
# 2nd option
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.compat.v1.Session(config=config)
```

I would be grateful for help 



"
32547,tensorflow.contrib.integrate.odeint does not work with the default __call__ method of subclass of tf.Keras.layers.Layer,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): both 1.14 and 1.15-rc0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1/7.0
- GPU model and memory: Titan RTX, 24 GB

**Describe the current behavior**
__For Tensorflow 1.14:__
When defining ODE function using the default `__call__` method of any subclass of `tf.Keras.layers.Layer`, an `ValueError` is raised, stating `VarIsInitializedOp` has been marked as not fetchable. To avoid this issue, the ODE function has to be defined using either pure TensorFlow functions, or the `call` method of the subclass instead of `__call__`. 
__For Tensorflow 1.15-rc0:__
The same code will run forever without error message. The above solution (using `call` instead of `__call__`) also applies here. 

**Describe the expected behavior**
`tensorflow.contrib.integrate.odeint` should work with the default `__call__` method of any subclass of `tf.Keras.layers.Layer` under both TensorFlow 1.14 and 1.15-rc0.

**Code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow.python.keras import backend, layers, Input
backend.clear_session()
from tensorflow.contrib.integrate import odeint

x = Input(shape=(10,))
l0 = layers.Dense(units=10)
l0.build(x.shape)  # this line is necessary only to make call work

def ode_func(h, t, *ode_params):
    return l0(h)  # l0.call(h) avoids the error which is hacky

ts = tf.constant([0., 1.], dtype=tf.float64)
h_ts = odeint(ode_func, x, ts)
```

**Other info / logs**
<details><summary>For TensorFlow 1.14: ValueError: '.../VarIsInitializedOp' not fetchable; click here for detailed logs.</summary>
<p>

```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-ad0e95799922> in <module>()
     13 print('code start')
     14 
---> 15 h_ts = odeint(ode_func, x, ts)
     16 
     17 print('code finished')

23 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/integrate/python/ops/odes.py in odeint(func, y0, t, rtol, atol, method, options, full_output, name)
    538         full_output=full_output,
    539         name=scope,
--> 540         **options)
    541 
    542 

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/integrate/python/ops/odes.py in _dopri5(func, y0, t, rtol, atol, full_output, first_step, safety, ifactor, dfactor, max_num_steps, name)
    403         lambda _, __, ___, i: i < num_times,
    404         interpolate, (solution, history, rk_state, 1),
--> 405         name='interpolate_loop')
    406 
    407     y = solution.stack(name=scope)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)
   3499       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)
   3500     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,
-> 3501                                     return_same_structure)
   3502     if maximum_iterations is not None:
   3503       return result[1]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants, return_same_structure)
   3010       with ops.get_default_graph()._mutation_lock():  # pylint: disable=protected-access
   3011         original_body_result, exit_vars = self._BuildLoop(
-> 3012             pred, body, original_loop_vars, loop_vars, shape_invariants)
   3013     finally:
   3014       self.Exit()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)
   2935         expand_composites=True)
   2936     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
-> 2937     body_result = body(*packed_vars_for_body)
   2938     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
   2939     if not nest.is_sequence_or_composite(body_result):

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/integrate/python/ops/odes.py in interpolate(solution, history, rk_state, i)
    381             lambda rk_state, *_: t[i] > rk_state.t1,
    382             adaptive_runge_kutta_step, (rk_state, history, 0),
--> 383             name='integrate_loop')
    384         y = _interp_evaluate(rk_state.interp_coeff, rk_state.t0, rk_state.t1,
    385                              t[i])

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)
   3499       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)
   3500     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,
-> 3501                                     return_same_structure)
   3502     if maximum_iterations is not None:
   3503       return result[1]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants, return_same_structure)
   3010       with ops.get_default_graph()._mutation_lock():  # pylint: disable=protected-access
   3011         original_body_result, exit_vars = self._BuildLoop(
-> 3012             pred, body, original_loop_vars, loop_vars, shape_invariants)
   3013     finally:
   3014       self.Exit()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)
   2935         expand_composites=True)
   2936     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
-> 2937     body_result = body(*packed_vars_for_body)
   2938     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
   2939     if not nest.is_sequence_or_composite(body_result):

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/integrate/python/ops/odes.py in adaptive_runge_kutta_step(rk_state, history, n_steps)
    345       with ops.control_dependencies(
    346           [check_underflow, check_max_num_steps, check_numerics]):
--> 347         y1, f1, y1_error, k = _runge_kutta_step(func, y0, f0, t0, dt)
    348 
    349       with ops.name_scope('error_ratio'):

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/integrate/python/ops/odes.py in _runge_kutta_step(func, y0, f0, t0, dt, tableau, name)
    121       ti = t0 + alpha_i * dt
    122       yi = y0 + _scaled_dot_product(dt_cast, beta_i, k)
--> 123       k.append(func(yi, ti))
    124 
    125     if not (tableau.c_sol[-1] == 0 and tableau.c_sol[:-1] == tableau.beta[-1]):

<ipython-input-3-ad0e95799922> in ode_func(h, t, *ode_params)
      8 
      9 def ode_func(h, t, *ode_params):
---> 10     return l0(h)  # l0.call(h) avoids the error which is hacky
     11 
     12 ts = tf.constant([0., 1.], dtype=tf.float64)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    559       # framework.
    560       if base_layer_utils.needs_keras_history(inputs):
--> 561         base_layer_utils.create_keras_history(inputs)
    562 
    563     # Handle Keras mask propagation from previous layer to current layer.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py in create_keras_history(tensors)
    198     keras_tensors: The Tensors found that came from a Keras Layer.
    199   """"""
--> 200   _, created_layers = _create_keras_history_helper(tensors, set(), [])
    201   return created_layers
    202 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py in _create_keras_history_helper(tensors, processed_ops, created_layers)
    244             constants[i] = backend.function([], op_input)([])
    245       processed_ops, created_layers = _create_keras_history_helper(
--> 246           layer_inputs, processed_ops, created_layers)
    247       name = op.name
    248       node_def = op.node_def.SerializeToString()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py in _create_keras_history_helper(tensors, processed_ops, created_layers)
    244             constants[i] = backend.function([], op_input)([])
    245       processed_ops, created_layers = _create_keras_history_helper(
--> 246           layer_inputs, processed_ops, created_layers)
    247       name = op.name
    248       node_def = op.node_def.SerializeToString()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py in _create_keras_history_helper(tensors, processed_ops, created_layers)
    242             constants[i] = op_input
    243           else:
--> 244             constants[i] = backend.function([], op_input)([])
    245       processed_ops, created_layers = _create_keras_history_helper(
    246           layer_inputs, processed_ops, created_layers)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3251     inputs = nest.flatten(inputs)
   3252 
-> 3253     session = get_session(inputs)
   3254     feed_arrays = []
   3255     array_vals = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in get_session(op_input_list)
    460   if not _MANUAL_VAR_INIT:
    461     with session.graph.as_default():
--> 462       _initialize_variables(session)
    463   return session
    464 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in _initialize_variables(session)
    877     # marked as initialized.
    878     is_initialized = session.run(
--> 879         [variables_module.is_variable_initialized(v) for v in candidate_vars])
    880     uninitialized_vars = []
    881     for flag, v in zip(is_initialized, candidate_vars):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    948     try:
    949       result = self._run(None, fetches, feed_dict, options_ptr,
--> 950                          run_metadata_ptr)
    951       if run_metadata:
    952         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1156     # Create a fetch handler to take care of the structure of fetches.
   1157     fetch_handler = _FetchHandler(
-> 1158         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
   1159 
   1160     # Run request and get response.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in __init__(self, graph, fetches, feeds, feed_handles)
    485         self._ops.append(True)
    486       else:
--> 487         self._assert_fetchable(graph, fetch.op)
    488         self._fetches.append(fetch)
    489         self._ops.append(False)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _assert_fetchable(self, graph, op)
    498     if not graph.is_fetchable(op):
    499       raise ValueError(
--> 500           'Operation %r has been marked as not fetchable.' % op.name)
    501 
    502   def fetches(self):

ValueError: Operation 'odeint/interpolate_loop/interpolate/integrate_loop/runge_kutta_step/VarIsInitializedOp' has been marked as not fetchable.
```

</p>
</details>



"
32546,[autograph List comprehension issue] OperatorNotAllowedInGraphError error in Tensorflow 2.0,"**System information**
- OS Platform and Distribution 
Mac os (10.14.6)
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
 2.0.0rc1
- Python version:
Python 3.6.4

I'm learn tensorflow2.0 from [official tutorials](https://www.tensorflow.org/beta/guide/autograph#batching).I can understand the result from below code.
```
def square_if_positive(x):
  return [i ** 2 if i > 0 else i for i in x]
square_if_positive(range(-5, 5))

# result
[-5, -4, -3, -2, -1, 0, 1, 4, 9, 16]
```
But if I change the inputs with tensor not python code, just like this, I got a error
```
@tf.function
def square_if_positive(x):
  return [i ** 2 if i > 0 else i for i in x]
square_if_positive(tf.range(-5, 5))
```
```

~/tf2_workspace/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   1984             arg_names=arg_names,
   1985             override_flat_arg_shapes=override_flat_arg_shapes,
-> 1986             capture_by_value=self._capture_by_value),
   1987         self._function_attributes,
   1988         # Tell the ConcreteFunction to clean up its graph once it goes out of

~/tf2_workspace/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    851                                           converted_func)
    852 
--> 853       func_outputs = python_func(*func_args, **func_kwargs)
    854 
    855       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/tf2_workspace/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    323         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    324         # the function a weak reference to itself to avoid a reference cycle.
--> 325         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    326     weak_wrapped_fn = weakref.ref(wrapped_fn)
    327 

~/tf2_workspace/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)
    841           except Exception as e:  # pylint:disable=broad-except
    842             if hasattr(e, ""ag_error_metadata""):
--> 843               raise e.ag_error_metadata.to_exception(type(e))
    844             else:
    845               raise

OperatorNotAllowedInGraphError: in converted code:

    <ipython-input-37-6c17f29a3443>:3 square_if_positive  *
        return [i**2 if i > 0 else i for i in x]
    /Users/zhangpan/tf2_workspace/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:547 __iter__
        self._disallow_iteration()
    /Users/zhangpan/tf2_workspace/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:540 _disallow_iteration
        self._disallow_when_autograph_enabled(""iterating over `tf.Tensor`"")
    /Users/zhangpan/tf2_workspace/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:518 _disallow_when_autograph_enabled
        "" decorating it directly with @tf.function."".format(task))

    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.
```
I can't find any specifications about this error. I think the real reason is not ""iterating over tf.Tensor is not allowed"" . Becase I can write like this.

```
@tf.function
def square_if_positive(x):
    for i in x:
        if i>0:
            tf.print(i**2)
        else:
            tf.print(i)

square_if_positive(tf.range(10))
```
iterate over tensor just like above code.
So my question is what's the real reason about this error? Any suggestions will help me. I really can't understand this error through I read a lot of materials.

"
32545,tf.keras.layers.Add can add tensors with dimension (),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-dev20190731
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

when I add multiple tensors with shape (), tf.keras.layers.Add complains that
>  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 680, in __call__
    self._maybe_build(inputs)
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1905, in _maybe_build
    self.build(input_shapes)
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/tf_utils.py"", line 299, in wrapper
    output_shape = fn(instance, input_shape)
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/merge.py"", line 95, in build
    batch_sizes = [s[0] for s in input_shape if s is not None]
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/merge.py"", line 95, in <listcomp>
    batch_sizes = [s[0] for s in input_shape if s is not None]
IndexError: tuple index out of range

**Describe the expected behavior**

I expect tf.keras.layers.Add can process tensors of any dimension.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
#!/usr/bin/python3

import tensorflow as tf;

a = tf.constant(1);
b = tf.constant(2);
c = tf.constant(3);
d = tf.keras.layers.Add()([a,b,c]);
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32544,tf.keras batch normalization is batch dependent at test time,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Both
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13
- Python version: 3.7
- CUDA/cuDNN version: 10
- GPU model and memory: RTX

**Describe the current behavior**

When doing inference using tf.keras batchnormalization,  for a given input sample x, the output is dependent on the other samples in the batch.  I can modify the other samples in the batch to influence the output of x.  This should not be the case as all samples should be processed independently.  Whether I run a sample x as a batch size of 1 or with other samples, its output should be exactly the same.  I am seeing differences of 0.3 at times (see my output below).

It appears keras is also having this issue: https://github.com/keras-team/keras/issues/12400

**Describe the expected behavior**
Upon using tf.keras batch normalization at test time, a given input sample x should have the same output regardless of the other samples in the batch

**Code to reproduce the issue**
Note: I put the code into colab and was able to verify the bug.

```
import tensorflow as tf
import numpy as np

input1 = tf.keras.layers.Input(shape=(128,128,1))
x = tf.keras.layers.BatchNormalization()(input1)
x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(5, activation = 'softmax')(x)

model = tf.keras.models.Model(input1, x)

batchSize = 64
x = np.random.rand(1, 128,128,1)
x = np.repeat(x, batchSize, axis=0)

print('The following rows should all be equal...')
for k in range(1, batchSize):  
    y = model.predict(x[0:k,:,:,:], batch_size=8)
    print(y[0,:])
```
I get the following output:

```
The following rows should all be equal...
2019-09-15 16:46:29.463998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-09-15 16:46:29.463998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-15 16:46:29.463998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-09-15 16:46:29.463998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-09-15 16:46:29.464999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11334 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:09:00.0, compute capability: 6.1)
2019-09-15 16:46:30.597112: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally
[0.38215435 0.66578805 0.45068434 0.85698235 0.7698223 ]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215438 0.66578805 0.45068428 0.85698223 0.76982236]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
[0.38215443 0.66578794 0.4506843  0.85698223 0.76982224]
```
When i run on the CPU, the problem goes away.

When I run on my own data, I get the following code / output which shows the problem more severely:
    
```
    x,y = testGenerator.next(0)
    score = []
    for kk in range(1, 16):
        s = model.predict(x[0:kk,:,:,:])
        print(s[0][0,:])
```
Gives output...
```
[9.3195647e-01 4.5844557e-04 6.6464258e-05 6.7509413e-02 2.0588086e-06
 1.3121526e-06 1.7270798e-06 2.0374323e-06 2.1472629e-06]
[9.3195647e-01 4.5844595e-04 6.6464192e-05 6.7509346e-02 2.0587986e-06
 1.3121462e-06 1.7270781e-06 2.0374266e-06 2.1472567e-06]
[9.3195623e-01 4.5845023e-04 6.6464687e-05 6.7509525e-02 2.0588097e-06
 1.3121534e-06 1.7270876e-06 2.0374339e-06 2.1472645e-06]
[9.1518945e-01 5.7900354e-04 8.6429020e-05 8.4134214e-02 2.4391386e-06
 1.5495950e-06 2.0541604e-06 2.3649241e-06 2.5034813e-06]
[9.1518945e-01 5.7900301e-04 8.6429180e-05 8.4134296e-02 2.4391431e-06
 1.5496009e-06 2.0541527e-06 2.3649197e-06 2.5034813e-06]
[9.1442782e-01 5.8447709e-04 8.7354209e-05 8.4889315e-02 2.4558769e-06
 1.5600473e-06 2.0685882e-06 2.3791872e-06 2.5190409e-06]
[9.1442782e-01 5.8447709e-04 8.7354209e-05 8.4889315e-02 2.4558769e-06
 1.5600473e-06 2.0685882e-06 2.3791872e-06 2.5190409e-06]
[9.1442782e-01 5.8447709e-04 8.7354209e-05 8.4889315e-02 2.4558769e-06
 1.5600473e-06 2.0685882e-06 2.3791872e-06 2.5190409e-06]
[9.1442782e-01 5.8447709e-04 8.7354209e-05 8.4889315e-02 2.4558769e-06
 1.5600473e-06 2.0685882e-06 2.3791872e-06 2.5190409e-06]
[2.7099210e-01 5.2615767e-03 1.4831917e-03 7.2222257e-01 9.9957060e-06
 6.1737474e-06 8.7498120e-06 7.3227852e-06 8.3933583e-06]
[2.7099210e-01 5.2615767e-03 1.4831917e-03 7.2222257e-01 9.9957060e-06
 6.1737474e-06 8.7498120e-06 7.3227852e-06 8.3933583e-06]
[2.7099210e-01 5.2615767e-03 1.4831917e-03 7.2222257e-01 9.9957060e-06
 6.1737474e-06 8.7498120e-06 7.3227852e-06 8.3933583e-06]
[2.7099210e-01 5.2615767e-03 1.4831917e-03 7.2222257e-01 9.9957060e-06
 6.1737474e-06 8.7498120e-06 7.3227852e-06 8.3933583e-06]
[2.7099210e-01 5.2615767e-03 1.4831917e-03 7.2222257e-01 9.9957060e-06
 6.1737474e-06 8.7498120e-06 7.3227852e-06 8.3933583e-06]
[1.9735371e-01 5.5795610e-03 1.7620743e-03 7.9526496e-01 9.8536966e-06
 6.0714729e-06 8.6973323e-06 7.0008568e-06 8.0944292e-06]
```


**EDIT 1:** Added code to replicate issue.  Also, If you remove the batch norm, the problem still persists. 

**EDIT 2:** Displayed results from running on my own data."
32543,RNN layer does not reset dropout masks of RNNCell,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  v1.12.1-9392-gf3c7314d83 1.15.0-rc0
- Python version: 3.7.4

**Describe the current behavior**
The RNN layer with an RNNCell does not reset the states of dropout masks compared to the layer implementations of the cells. Thus the behavior of `tf.keras.layers.GRU(10)` != `tf.keras.layers.RNN(tf.keras.layers.GRUCell(10))`.

This is especially problematic, because the [Keras RNN API tutorial](https://www.tensorflow.org/beta/guide/keras/rnn#rnn_layers_and_rnn_cells) states both approaches are mathematically equivalent.

**Describe the expected behavior**
The RNN layer should check the type of the RNNCell and, if it is a subclass of `DropoutRNNCellMixin`, reset the dropout masks after each call. By calling `cell.reset_recurrent_dropout_mask()` and `cell.reset_dropout_mask`.

**Code to reproduce the issue**
Partially copied from https://github.com/tensorflow/tensorflow/issues/29391
```python
from __future__ import absolute_import, division, print_function
import numpy as np

import tensorflow as tf
tf.enable_eager_execution()


tf.enable_eager_execution()
print(tf.__version__)
data = np.random.normal(0, 1, (1, 10, 2)).astype(np.float32)
rnn = tf.keras.layers.GRU(units=10, dropout=0.5,
                                   recurrent_dropout=0.5)
print(set([rnn(data, training=True).numpy()[0, 0] for _ in range(5)]))

rnn_cell = tf.keras.layers.GRUCell(units=10, dropout=0.5,
                                   recurrent_dropout=0.5)
rnn = tf.keras.layers.RNN(rnn_cell)
print(set([rnn(data, training=True).numpy()[0, 0] for _ in range(5)]))
```

Output:
```
WARNING:tensorflow:From check_dropout.py:5: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

1.15.0-rc0
{0.04537238, 0.15487108, 0.0, 0.08881481, 0.055508718}  # Different dropout mask was used for each call
{-0.34464198}  # Same dropout mask was used for each call
```"
32540,tf.map_fn doesn't support unpacking multiple tensors of different dimensions,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0.0-dev20190731
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**

tf.map_fn support unpacking elements from multiple tensors of the same dimension, but it doesn't support unpacking from multiple tensors of different dimensions. the following code is an instance.

```python
import tensorflow as tf;
import numpy as np;

a = tf.constant(np.random.normal(size = (10,3,4)));
b = tf.constant(np.random.normal(size = (10,4,5)));
c = tf.map_fn(lambda x: tf.linalg.matmul(x[0], x[1]), (a, b));
```
please support the feature to make batch processing easily.

**Will this change the current api? How?**

the api can be remained the same.

**Who will benefit with this feature?**

people who want to process batch of data

**Any Other info.**
"
32539,tf-Java 1.14.0 can not open shared object file libtensorflow_frameword.so.1,"**System information**
- OS system: CentOS 7.3
- TensorFlow installed from binary: import tensorflow from maven repo
- TensorFlow version: 1.14.0
- Python version: 3.6.5
- CUDA/cuDNN version: cuda10-cudnn7


**Describe the current behavior**
We are using tensorflow java api to loaded a tensorflow model and a program exception is thrown. The exception information shows that the dependent resource file (libtensorflow_jni.so : libtensorflow_frameword.so.1) is missing.  We find that the tensorflow jar file contains this resource file, but the file failed to extract, resulting in an exception. We suspect this is due to the program filtering the files to be extracted by suffix names (.so).
**Describe the expected behavior**
The program can load model file successfully. 

**Code to reproduce the issue**
The maven setting:
```
<dependency>
            <groupId>org.tensorflow</groupId>
            <artifactId>tensorflow</artifactId>
            <version>1.14.0</version>
</dependency>
```
```
import org.tensorflow.SavedModelBundle;
import org.tensorflow.Tensor;

public static void main(String[] args){
        String dir = ""xxx"" // model path 
        savedModelBundle = SavedModelBundle.load(dir, ""serve"");
    }
}
```

**Other info / logs**
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/tensorflow_native_libraries-1568533726609-0/libtensorflow_jni.so: libtensorflow_framework.so.1: cannot open shared object file: No such file or directory
        at java.lang.ClassLoader$NativeLibrary.load(Native Method)
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824)
        at java.lang.Runtime.load0(Runtime.java:809)
        at java.lang.System.load(System.java:1086)
        at org.tensorflow.NativeLibrary.load(NativeLibrary.java:101)
        at org.tensorflow.TensorFlow.init(TensorFlow.java:66)
        at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)
        at org.tensorflow.SavedModelBundle.<clinit>(SavedModelBundle.java:170)"
32535,Build TensorFlow Lite for Raspberry Pi with SELECT_TF_OPS,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian GNU/Linux 9.1 (stretch)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

Hi, I am trying to use TF Lite with SELECT_TF_OPS on my Raspberry Pi 3B. According to your docs, Python API is currently not fully supported, so I tried to use the C++ API. Then I have to compile a static library for TF Lite [https://www.tensorflow.org/lite/guide/build_rpi#cross-compile_for_raspberry_pi](url) . The problem is how can I config it to support SELECT_TF_OPS? The only information I found is on [https://www.tensorflow.org/lite/guide/ops_select#c](url), but it is about compiling using Bazel, which is not compatible with the compiling instructions on Raspberry Pi? Does anyone has any suggestions or ideas about this issue?

Thanks!"
32534,import packages of tf2.0.0rc in pycharm,"i'm using tf2.0.0rc, win10.

when i write the below code in pycharm, `Dataset` is underlined with a red line, and shows a message `cannot find reference 'Dataset' in '__init__.py' `, and pycharm can't auto-complete `Dataset`'s functions, but the script can run well.

```python
from tensorflow_core.python.data import Dataset

dataset = Dataset.from_tensors([2])
```

when i write the below code, there is no red line, and pycharm can auto-complete `Dataset`'s functions, but the script has error:`KeyError: ""Registering two gradient with name 'ReduceDataset'! (Previous registration was in register /home/xiefangyuan/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/registry.py:66)""`

```python
from tensorflow_core.python.data.ops.dataset_ops import Dataset

dataset = Dataset.from_tensors([2])
```"
32532,Improving compatibility with Scikit-learn,"The original issue in keras-team/keras is [this](https://github.com/keras-team/keras/issues/12832
)

Scikit-learn estimators are instanced only with hyper-parameter information. No knowledge on the data (X or y) is needed until the estimator is fitted. On the other hand, KerasRegressor and KerasClassifier receive as argument a build_fn that instantiates and compiles the estimator. At this point, the input and output shapes are needed to define the network's architecture. This is a subtle but important difference between Scikit-learn estimators and KerasClassifier/KerasRegressor.

Also, Scikit-learn's GridSearchCV and RandomizedSearchCV (and probably Scikit-optimize's BayesSearchCV) automatically infer the most appropriate CV folding (stratified or not) from the base_estimator. KerasClassifier and KerasRegressor, however, do not have the attribute needed for this to work.

My proposed solution (with some additional tests) is in [this](https://github.com/daviddiazvico/tensorflow/commit/e33e1c5f88e9832b2578f495ee85bcae7e9a5a4f) commit (I'll open a PR now).

Thank you!"
32529,Keras code freezes on GPU ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Ubuntu 18.04

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:

binary (pip)

- **TensorFlow version (use command below)**:

1.14

- **Python version**:

3.6.3

- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:


Cuda: 10.2
cuDNN: 7.6.2.24-1+cuda10.0

- **GPU model and memory**:

Geforce GTX 1080Ti

- **Exact command to reproduce**:

It is a lot of code and data that I am trying to dumb down to something easily reproducible.

### Describe the problem

My keras model training freezes up.  By freeze up I mean I cannot ctrl-c out of the code, I can ctrl-z and kill.  The training never proceeds, it just sits at that point and the python process goes to 100% and the GPU goes to 100%.  Stack trace included below seems to indicate the code is waiting on something.

```+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      3499      C   python                                      6545MiB |
+-----------------------------------------------------------------------------+
Sat Sep 14 12:29:32 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 430.26       Driver Version: 430.26       CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |
| 38%   65C    P2    82W / 250W |   6555MiB / 11177MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      3499      C   python                                      6545MiB |
+-----------------------------------------------------------------------------+
```

```
top - 12:30:48 up 9 days, 15:45,  3 users,  load average: 1.04, 1.48, 1.44
Tasks: 224 total,   1 running, 156 sleeping,   0 stopped,   1 zombie
%Cpu(s):  0.0 us,  5.8 sy,  6.8 ni, 87.4 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem : 65913412 total, 40978484 free,  9833572 used, 15101356 buff/cache
KiB Swap: 67048444 total, 67048444 free,        0 used. 55203584 avail Mem 

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                    
 3499 jostheim  25   5 28.628g 4.935g 761640 S  99.7  7.9  22:56.33 python       
```

### Source code / logs

GDB traces are here:

[https://pastebin.com/qcpHGadT](https://pastebin.com/qcpHGadT)"
32528,"EstimatorSpec documentation removed from 1.14, 1.15","Looks like the documentation for tf.estimator.EstimatorSpec got lost along the way from 1.13 to 1.14. Can we please add it back in?

## URL(s) with the issue: https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/estimator/EstimatorSpec
https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/estimator/EstimatorSpec

## Description of issue (what needs changing): Reinstate documentation (see https://www.tensorflow.org/versions/r1.13/api_docs/python/tf/estimator/EstimatorSpec)

### Submit a pull request? no"
32525,invalid_creator_scope,
32524,unicode decode error when import tensorflow.contrib.slim,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- Linux Ubuntu 16.04
- TensorFlow installed via pip install tensorflow-gpu==1.2
- TensorFlow version 1.2.0
- Python version 3.6
- CUDA/cuDNN version CUDA8.0 cuDNN5.1
- GPU model and memory: GTX1080TI

There is something wrong when I import slim
`import tensorflow.contrib.slim as slim`

`Traceback (most recent call last):
  File ""finetune-sintel.py"", line 22, in <module>
    from model_pwcnet import ModelPWCNet, _DEFAULT_PWCNET_FINETUNE_OPTIONS
  File ""/home/sxl/ext4-2T-3/code/tfoptflow-master/tfoptflow/model_pwcnet.py"", line 21, in <module>
    from model_base import ModelBase
  File ""/home/sxl/ext4-2T-3/code/tfoptflow-master/tfoptflow/model_base.py"", line 15, in <module>
    import tensorflow.contrib.slim as slim
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 22, in <module>
    from tensorflow.contrib import bayesflow
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/bayesflow/__init__.py"", line 24, in <module>
    from tensorflow.contrib.bayesflow.python.ops import entropy
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/bayesflow/python/ops/entropy.py"", line 23, in <module>
    from tensorflow.contrib.bayesflow.python.ops.entropy_impl import *
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/bayesflow/python/ops/entropy_impl.py"", line 30, in <module>
    from tensorflow.contrib.bayesflow.python.ops import variational_inference
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/bayesflow/python/ops/variational_inference.py"", line 26, in <module>
    from tensorflow.contrib.bayesflow.python.ops.variational_inference_impl import *
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/bayesflow/python/ops/variational_inference_impl.py"", line 29, in <module>
    from tensorflow.contrib.bayesflow.python.ops import stochastic_graph_impl as sg
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/bayesflow/python/ops/stochastic_graph_impl.py"", line 28, in <module>
    from tensorflow.contrib.bayesflow.python.ops import stochastic_tensor_impl
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/bayesflow/python/ops/stochastic_tensor_impl.py"", line 51, in <module>
    from tensorflow.contrib.distributions.python.ops import distribution
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/distributions/__init__.py"", line 91, in <module>
    from tensorflow.contrib.distributions.python.ops.bernoulli import *
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/bernoulli.py"", line 21, in <module>
    from tensorflow.contrib.distributions.python.ops import distribution
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/distribution.py"", line 29, in <module>
    from tensorflow.contrib import framework as contrib_framework
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/framework/__init__.py"", line 81, in <module>
    from tensorflow.contrib.framework.python.ops import *
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/__init__.py"", line 26, in <module>
    from tensorflow.contrib.framework.python.ops.variables import *
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py"", line 26, in <module>
    from tensorflow.contrib.framework.python.ops import gen_variable_ops
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/gen_variable_ops.py"", line 40, in <module>
    _ops.RegisterShape(""ZeroInitializer"")(None)
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1697, in __call__
    self._op_type)
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/site-packages/tensorflow/python/framework/registry.py"", line 67, in register
    stack = traceback.extract_stack()
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/traceback.py"", line 211, in extract_stack
    stack = StackSummary.extract(walk_stack(f), limit=limit)
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/traceback.py"", line 364, in extract
    f.line
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/traceback.py"", line 286, in line
    self._line = linecache.getline(self.filename, self.lineno).strip()
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/linecache.py"", line 16, in getline
    lines = getlines(filename, module_globals)
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/linecache.py"", line 47, in getlines
    return updatecache(filename, module_globals)
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/linecache.py"", line 137, in updatecache
    lines = fp.readlines()
  File ""/home/sxl/anaconda3/envs/tfpy36/lib/python3.6/codecs.py"", line 321, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xcf in position 4307: invalid continuation byte
`
"
32523,iOS `TensorFlowLiteC` using old C API,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS 13
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 6
- TensorFlow installed from (source or binary): binary (TensorFlowLiteC)
- TensorFlow version: 1.14.0

**Describe the problem**
https://github.com/tensorflow/tensorflow/blob/v1.15.0-rc0/tensorflow/lite/experimental/ios/TensorFlowLiteC.podspec does not have the pre-compiled binary yet, considering we have a significant backward compatible API change introduce by https://github.com/tensorflow/tensorflow/commit/4cc4425aee000b6358fbf219fa3dd64710b7aed4

Currently, it's a problem for us at https://github.com/dart-lang/tflite_native/pull/22"
32522, Config value android_arm is not defined in any .rc file,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): conda
- TensorFlow version: Tensorflow 1.13.1
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.19.2

When I convert my TensorFlow model into TFLite model, as suggested from:https://github.com/tensorflow/tensorflow/issues/32112, I use tf.lite.OpsSet.SELECT_TF_OPS to covert it, then according to 
[running the model](https://www.tensorflow.org/lite/guide/ops_select#running_the_model), i installed bazel to get Android AAR:
```
bazel build --cxxopt='--std=c++11' -c opt             \
  --config=android_arm --config=monolithic          \
  //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops
```
But a error occurs although I have already set up my environment by `export PATH=""$PATH:$HOME/bin""`:
```
ERROR: Config value android_arm is not defined in any .rc file
```
Thanks for you reading
"
32521,can not tflite squeezedet graph by freeze_graph.py,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):
- TensorFlow installed from (binary):
- TensorFlow version (1.15.0-dev20190707):


Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ARG_MAX, CAST, CONCATENATION, CONV_2D, DIV, EXP, FLOOR, GREATER, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, REDUCE_MAX, RESHAPE, SOFTMAX, STRIDED_SLICE, SUB, TRANSPOSE, UNPACK. Here is a list of operators for which you will need custom implementations: RandomUniform.


"
32520,Training stuck on Allocation exceeds 10% of system memory,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.9.0
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: running on CPU
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
The training is stuck after throwing the following warning: 
2019-09-13 23:25:57.221146: W tensorflow/core/framework/allocator.cc:108] Allocation of 603979776 exceeds 10% of system memory.
start training iteration: 0 elapse: 1568442370.016824
2019-09-13 23:26:10.230583: W tensorflow/core/framework/allocator.cc:108] Allocation of 2147483648 exceeds 10% of system memory.
2019-09-13 23:26:11.860365: W tensorflow/core/framework/allocator.cc:108] Allocation of 2147483648 exceeds 10% of system memory.

**Describe the expected behavior**
Expect the training to slowly progress given the memory is not big enough to hold the tensors then the swap should be able to help. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

MN_REDUCED.py
    
    import tensorflow as tf
    from tensorflow.python.framework import ops
    from tensorflow.python.framework import dtypes
    import random
    import numpy as np
      
    BATCH_SIZE = 128
    OUTPUT=4096
    NUM_CLASSES = OUTPUT
    
    class MN_REDUCED(object):
     
          def __init__(self, trainable=True, dropout=0.5):
              self.trainable = trainable
              self.dropout = dropout
              self.parameters = []
    
          def build(self,rgb,train_mode=None):
              with tf.name_scope('conv_1') as scope:
                  kernel = tf.Variable(tf.truncated_normal([3, 3, 3, OUTPUT], dtype=tf.float32,
                                          stddev=1e-2), name='weights')
                  conv = tf.nn.conv2d(rgb, kernel, [1, 1, 1, 1], padding='SAME')
                  biases = tf.Variable(tf.constant(0.0, shape=[OUTPUT], dtype=tf.float32),
                                           trainable=True, name='biases')
                  conv1 = tf.nn.bias_add(conv, biases)
               #   shape = int(np.prod(out.get_shape()))
               #   flat = tf.reshape(out, [BATCH_SIZE, -1])
               #   self.out_0 = flat[:,0:OUTPUT]
              with tf.name_scope('conv_2') as scope:
                  kernel = tf.Variable(tf.truncated_normal([3, 3, OUTPUT, OUTPUT], type=tf.float32,
                                          stddev=1e-2), name='weights')
                  conv = tf.nn.conv2d(conv1, kernel, [1, 1, 1, 1], padding='SAME')
                  biases = tf.Variable(tf.constant(0.0, shape=[OUTPUT], dtype=tf.float32),
                                           trainable=True, name='biases')
                  out = tf.nn.bias_add(conv, biases)
                  shape = int(np.prod(out.get_shape()))
                  flat = tf.reshape(out, [BATCH_SIZE, -1])
                  self.out_0 = flat[:,0:OUTPUT]
     
          def loss(self, labels):
              labels = tf.cast(labels, tf.int32)
              oneHot = tf.one_hot (labels, NUM_CLASSES)
              loss = tf.reduce_mean(tf.square(self.out_0 - oneHot), name='loss')
              return loss
     
          def training(self, loss):
              optimizer = tf.train.GradientDescentOptimizer(0.001)
              train_op = optimizer.minimize(loss);
              return train_op


main.py

     import tensorflow as tf
     import random
     import os
     from DataInput import DataInput
     #from CNN_FULL_CPU import CNN_FULL_CPU
     from MN_REDUCED import MN_REDUCED
     import pdb
     from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file
     import time
     import numpy as np
     
     dataset_path = ""./""
     train_labels_file = ""dataset.txt""
     
     IMAGE_HEIGHT = 32
     IMAGE_WIDTH = 32
     NUM_CHANNELS = 3
     BATCH_SIZE = 128
     NUM_ITERATIONS = 1000
     #NUM_ITERATIONS = 10
     LEARNING_RATE = 0.001
     SUMMARY_LOG_DIR=""./summary-log""
     lasttime = 0
   
       def placeholder_inputs(batch_size):
               images_placeholder = tf.placeholder(tf.float32,
                                                                       shape=(batch_size, IMAGE_HEIGHT,
                                                                                  IMAGE_WIDTH, NUM_CHANNELS))
               labels_placeholder = tf.placeholder(tf.int32,
                                                                       shape=(batch_size))
       
               return images_placeholder, labels_placeholder
   
     def fill_feed_dict(images_pl, labels_pl, sess):
               #images_feed, labels_feed = sess.run([data_input.example_batch, data_input.label_batch])
       
       
               #feed_dict = {
               #       images_pl: images_feed,
               #       labels_pl: labels_feed,
               #}
               n = BATCH_SIZE * IMAGE_WIDTH * IMAGE_HEIGHT * NUM_CHANNELS
               k = IMAGE_WIDTH
               a = np.empty(n, dtype=np.float32)
               np.random.seed(0)
       
           for i in range(0, n, k):
                   a[i:i+k] = np.random.normal(loc=0, scale=1, size=k)
               rand = np.reshape(a, (BATCH_SIZE, IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS))
       
       
               n1 = BATCH_SIZE
               a1 = np.empty(n1, dtype=np.float32)
               for i in range(0, n1):
                   a1[i] = np.random.normal(loc=0, scale=1)
       
       
               feed_dict = {
                       images_pl: rand,
                       labels_pl: a1,
               }
       
               return feed_dict
   
       def do_eval(sess,
                           eval_correct,
                           logits,
                           images_placeholder,
                           labels_placeholder,
                           dataset):
   
           true_count = 0
           # // is flooring division
           steps_per_epoch = dataset.num_examples // BATCH_SIZE
           num_examples = steps_per_epoch * BATCH_SIZE
   
           for step in xrange(steps_per_epoch):
                   #feed_dict = fill_feed_dict(dataset, images_placeholder,        labels_placeholder)
                   feed_dict = fill_feed_dict(dataset, images_placeholder, labels_placeholder,sess)
                   count = sess.run(eval_correct, feed_dict=feed_dict)
                   true_count = true_count + count
   
           precision = float(true_count) / num_examples
           print ('  Num examples: %d, Num correct: %d, Precision @ 1: %0.04f' %
                           (num_examples, true_count, precision))

    def placeholder_inputs(batch_size):
        images_placeholder = tf.placeholder(tf.float32,
        shape=(batch_size, IMAGE_HEIGHT,
        IMAGE_WIDTH, NUM_CHANNELS))
        labels_placeholder = tf.placeholder(tf.int32,
        shape=(batch_size))  
       def main():
               with tf.Graph().as_default():
   
                   #data_input = DataInput(dataset_path, train_labels_file, BATCH_SIZE)
                   images_placeholder, labels_placeholder = placeholder_inputs(BATCH_SIZE)
                   cnn_full_cpu = MN_REDUCED()
                   cnn_full_cpu.build(images_placeholder)
   
                   summary = tf.summary.merge_all()
                   saver = tf.train.Saver()
                   sess = tf.Session()
                   #sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
                   summary_writer = tf.summary.FileWriter(SUMMARY_LOG_DIR, sess.graph)
                   coord = tf.train.Coordinator()
                   threads = tf.train.start_queue_runners(sess=sess, coord=coord)
   
                   loss = cnn_full_cpu.loss(labels_placeholder)
                   train_op = cnn_full_cpu.training(loss)
   
                   init = tf.global_variables_initializer()
                   sess.run(init)
                   eval_correct = evaluation(cnn_full_cpu.out_0, labels_placeholder)
   
                   try:
                           for i in range(NUM_ITERATIONS):
                                   feed_dict = fill_feed_dict(images_placeholder,
                                                                   labels_placeholder, sess)
                                   _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)
   
                                   print ('Step %d: loss = %.6f' % (i, loss_value))
   
                           coord.request_stop()
                           coord.join(threads)
                   except Exception as e:
                           print(e)
                   infer = sess.run([cnn_full_cpu.out_0], feed_dict=feed_dict)
           sess.close().   


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I checked the memory utilization of the virtual machine. 
Although the memory is almost fully utilized, but there is still swap area. The program should run at a cost of performance. 
              total        used        free      shared  buff/cache   available
Mem:           3945        3712         178           0          54          79
Swap:          4092        2169        1923

In order to simulate a memory constrained environment, I setup a VirtualBox with 4GB memory and 4 GB swap size. Only one cpu core. 
"
32519,Failed to get convolution algorithm,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes / no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04 LTS Ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 1.14
- Python version: 2.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version:  9.0
- GPU model and memory: Quadro GP100

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I have a simple sequential `tf.keras` model with convolution that runs on 
a tfrecords Dataset. Suddenly, convolutions stopped working. 
No known changes made to system.

**Describe the expected behavior**
It runs as per usual


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Won't help reproduce what may be system's issue.
```
ds = tf.data.TFRecordsDataset(...).shuffle(...)
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(...),
     ...
])

model.compile(...)
model.fit(...)

```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


 name: Quadro GP100, pci bus id: 0000:af:00.0, compute capability: 6.0)
2019-09-14 08:13:43.313823: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56244eb0db50 executing computations on platform CUDA. Devices:
2019-09-14 08:13:43.313866: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Quadro GP100, Compute Capability 6.0

2019-09-14 08:14:35.273134: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:111] Filling up shuffle buffer (this may take a while): 4255 of 129446
....


2019-09-14 08:14:36.878088: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:162] Shuffle buffer filled.
2019-09-14 08:14:48.210416: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-09-14 08:14:49.011180: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-09-14 08:14:49.016488: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-09-14 08:14:49.016616: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node conv1d/conv1d}}]]
```

no warning message was printed above."
32518,"DefaultLogger Internal error: could not find any implementation for node (Unnamed Layer* 1) [TopK], try increasing the workspace size with IBuilder::setMaxWorkspaceSize()","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.14
- Python version:2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory: RTX 2080 , 8 GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
See the following error.  

2019-09-13 20:44:30.298493: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger Internal error: could not find any implementation for node (Unnamed Layer* 1) [TopK], try increasing the workspace size with IBuilder::setMaxWorkspaceSize()
2019-09-13 20:44:30.301481: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger ../builder/tacticOptimizer.cpp (1330) - OutOfMemory Error in computeCosts: 0

**Describe the expected behavior**
Work around  so as to not see this error. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32517,undeclared inclusion(s) in rule '//tensorflow/contrib/lite/kernels:eigen_support',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): Source
- TensorFlow version: r1.12
- Python version: 2.7
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: Cuda 9.0/ cuDNN 7
- GPU model and memory: Mobile GTX 1070



**Describe the problem**

sudo bazel --output_base=/opt/tensorflow build --config=cuda --config=opt --define framework_shared_object=false tensorflow:libtensorflow_cc.so //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
`./configure`
build --action_env PYTHON_BIN_PATH=""/usr/bin/python""
build --action_env PYTHON_LIB_PATH=""/home/ryan/catkin_ws/devel/lib/python2.7/dist-packages""
build --python_path=""/usr/bin/python""
build:ignite --define with_ignite_support=true
build:xla --define with_xla_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda""
build --action_env TF_CUDA_VERSION=""9.0""
build --action_env CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_NCCL_VERSION=""1""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env LD_LIBRARY_PATH=""/usr/local/cuda-9.0/lib64:/home/ryan/catkin_ws/devel/lib:/opt/ros/kinetic/lib:/opt/ros/kinetic/lib/x86_64-linux-gnu""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
build --config=cuda
test --config=cuda
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build:v2 --define=tf_api_version=2

`build log`
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
DEBUG: /opt/tensorflow/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
WARNING: /opt/tensorflow/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /opt/tensorflow/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /opt/tensorflow/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /opt/tensorflow/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /opt/tensorflow/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /opt/tensorflow/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/ryan/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/ryan/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /home/ryan/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/ryan/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/ryan/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/ryan/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/ryan/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/ryan/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed 2 targets (0 packages loaded).
INFO: Found 2 targets...
ERROR: /home/ryan/tensorflow/tensorflow/contrib/lite/kernels/BUILD:57:1: undeclared inclusion(s) in rule '//tensorflow/contrib/lite/kernels:eigen_support':
this rule is missing dependency declarations for the following files included by 'tensorflow/contrib/lite/kernels/eigen_support.cc':
  '/opt/tensorflow/external/eigen_archive/Eigen/Core'
INFO: Elapsed time: 3.144s, Critical Path: 2.54s
INFO: 18 processes: 18 local.
FAILED: Build did NOT complete successfully

"
32516,keras.fit not working with conv2D layer,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am using the fashion_mnist dataset to train a convolutional network,
`fashion_mnist = keras.datasets.fashion_mnist

(train_images, train_labels),(test_images, test_labels) = fashion_mnist.load_data()`
when I use a model with conv2D as first layer, more specifically:
`model = keras.models.Sequential([
    keras.layers.Conv2D(64, (3,3), activation=tf.nn.relu, input_shape=(28,28,1)),
...`
then use the fit function:
`model.fit(train_images, train_labels, batch_size=1, epochs=5)`
an error pops up:
`
ValueError: Error when checking input: expected conv2d_2_input to have 4 dimensions, but got array with shape (60000, 28, 28)`

so I changed the input shape to (28,28,1,0), but another error pops up:
`ValueError: Input 0 of layer conv2d_6 is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [None, 28, 28, 1, 0]`

**Describe the expected behavior**
The conv nn should have worked just fine

**Code to reproduce the issue**
`
import tensorflow as tf
from tensorflow import keras
fashion_mnist = keras.datasets.fashion_mnist

(train_images, train_labels),(test_images, test_labels) = fashion_mnist.load_data()

model = keras.models.Sequential([
    keras.layers.Conv2D(64, (3,3), activation=tf.nn.relu, input_shape=(28,28,1)),
    keras.layers.MaxPooling2D(2,2),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation=tf.nn.relu),
    keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer=tf.train.AdamOptimizer(),
              loss='sparse_categorical_crossentropy')

model.fit(train_images, train_labels, epochs=5)
'

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

`
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-16-4abc8fa24c0d> in <module>()
     10     keras.layers.Flatten(),
     11     keras.layers.Dense(128, activation=tf.nn.relu),
---> 12     keras.layers.Dense(10, activation=tf.nn.softmax)
     13 ])
     14 
/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)
    108       tf_utils.assert_no_legacy_layers(layers)
    109       for layer in layers:
--> 110         self.add(layer)
    111 
    112   @property

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in add(self, layer)
    172           # and create the node connecting the current layer
    173           # to the input layer we just created.
--> 174           layer(x)
    175           set_inputs = True
    176 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    584         # the corresponding TF subgraph inside `backend.get_graph()`
    585         input_spec.assert_input_compatibility(self.input_spec, inputs,
--> 586                                               self.name)
    587         graph = backend.get_graph()
    588         with graph.as_default(), backend.name_scope(self._name_scope()):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py in assert_input_compatibility(input_spec, inputs, layer_name)
    121                          'expected ndim=' + str(spec.ndim) + ', found ndim=' +
    122                          str(ndim) + '. Full shape received: ' +
--> 123                          str(x.shape.as_list()))
    124     if spec.max_ndim is not None:
    125       ndim = x.shape.ndims

ValueError: Input 0 of layer conv2d_6 is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [None, 28, 28, 1, 0]
`
"
32512,Expand checkpoint + SavedModel file formats to include provenance (origin) meta data,"I wrote a really nice issue description. Then github made it all disappear when I clicked their shiny new ""Similar issues - Try it!"" feature. I'm too frustrated to retype it. The gist is really in the title: current provenance practice is basically in the filename of the model, which sucks. 

We should add a field (that should get as auto-populated as possible) to the model outputs to keep track of the story behind the model. "
32510,Build doesn't respect LD_LIBRARY_PATH necessary for some rules.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
RHEL 6
- TensorFlow installed from (source or binary):
r1.14 from git
- TensorFlow version:
1.14
- Python version:
Python 3.5.6 :: Anaconda, Inc.
- Installed using virtualenv? pip? conda?:
Using conda's python
- Bazel version (if compiling from source):
0.26.1
- GCC/Compiler version (if compiling from source):
7.3.0
- CUDA/cuDNN version:
CUDA - 10.1
cuDNN - 7.6
- GPU model and memory:
Nvidia V100 32GB


**Describe the problem**
I'm trying to build TensorFlow on a RHEL 6 system, and run into the following error:

```
ERROR: /scratch/users/mkrafcz2/bazel_output/external/nccl_archive/BUILD.bazel:67:1: nvlink external/nccl_archive/device_dlink_hdrs_register_sm_70.h failed (Exit 1): nvlink failed: error ex 1uting command
  (cd /scratch/users/mkrafcz2/bazel_output/execroot/org_tensorflow && \
  exec env - \
  bazel-out/host/bin/external/local_config_cuda/cuda/cuda/bin/nvlink '--cpu-arch=X86_64' '--arch=sm_70' '--register-link-binaries=bazel-out/k8-opt/bin/external/nccl_archive/device_dlink_hdrs_register_sm_70.h' '--output-file=bazel-out/k8-opt/bin/external/nccl_archive/device_dlink_hdrs_sm_70.cubin' bazel-out/k8-opt/bin/external/nccl_archive/libdevice_lib.pic.a)
Execution platform: @bazel_tools//platforms:host_platform
/ui/ncsa/mkrafcz2/.cache/bazel/_bazel_mkrafcz2/install/316fe1c1cb66caaa37fdaa8c2ebb1ffd/_embedded_binaries/process-wrapper: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /ui/ncsa/mkrafcz2/.cache/bazel/_bazel_mkrafcz2/install/316fe1c1cb66caaa37fdaa8c2ebb1ffd/_embedded_binaries/process-wrapper)
```

Because the native gcc is 4.4.7, I need to use a custom compiler which I get by running 
`module load gcc/7.3.0`.

I then run the bazel build with --action_env=PATH and --action_env=LD_LIBRARY_PATH thinking that this will be appropriately passed into tests and such through the build, but from the error above, this is not the case.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Environment script:

```
source ~/conda3/bin/activate

conda activate tf-build-test

module load gcc/7.3.0
module load cuda-10.0
module load java-1.8.112

export PATH=""${PWD}/bazel-0.26.1/bin:${PATH}""

export CUDNN_PATH=""/usr/apps/deeplearning/cuDNNv7.6.3.30_10.1""
export SRC_DIR=""${PWD}/tensorflow-git""
export BAZEL_OUTPUT=${HOME}/scratch-global/bazel_outpu
```

Build script:

```
# This build script is populated initially from the conda tensorflow recipe

source scripts/env.sh

cd tensorflow-git

mkdir -p ${BAZEL_OUTPUT}

export BAZEL_OPTS=""--batch --output_base=${BAZEL_OUTPUT}""

# Compile tensorflow from source
export PYTHON_BIN_PATH=$(which python)
export PYTHON_LIB_PATH=$(python -c 'import sys; print([path for path in sys.path if ""site-packages"" in path][0])')
export USE_DEFAULT_PYTHON_LIB_PATH=1

# additional settings
# disable jemmloc (needs MADV_HUGEPAGE macro which is not in glib <= 2.12)
export TF_NEED_JEMALLOC=0

# do not build with MKL support
export TF_NEED_MKL=0

# Ivybridge
export CC_OPT_FLAGS=""-march=ivybridge -mtune=intel""

export TF_NEED_GCP=1
export TF_NEED_HDFS=1
export TF_NEED_S3=1
export TF_ENABLE_XLA=1
export TF_NEED_GDR=0
export TF_NEED_VERBS=0
export TF_NEED_OPENCL=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_ROCM=0
export TF_NEED_MPI=0
export TF_NEED_TENSORRT=0

# CUDA details
export TF_NEED_CUDA=1
export CUDA_PATH=$(which nvcc | rev | cut -d '/' -f 3- | rev)
export TF_CUDA_PATHS=""${CUDA_PATH},${CUDNN_PATH}""
export TF_CUDA_VERSION=$(cat ${CUDA_PATH}/version.txt | cut -d ' ' -f 3 | cut -d '.' -f -2)
export CUDNN_MAJOR=$(cat ${CUDNN_PATH}/include/cudnn.h | grep CUDNN_MAJOR | grep -v CUDNN_VERSION | cut -d ' ' -f 3)
export CUDNN_MINOR=$(cat ${CUDNN_PATH}/include/cudnn.h | grep CUDNN_MINOR | grep -v CUDNN_VERSION | cut -d ' ' -f 3)
export CUDNN_PATCHLEVEL=$(cat ${CUDNN_PATH}/include/cudnn.h | grep CUDNN_PATCHLEVEL | grep -v CUDNN_VERSION | cut -d ' ' -f 3)
export TF_CUDNN_VERSION=""${CUDNN_MAJOR}""

export TF_NCCL_VERSION=""""

# libcuda.so.1 needs to be symlinked to libcuda.so
# ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1
# on a ""real"" system the so.1 library is typically in /usr/local/nvidia/lib64
# add the stubs directory to LD_LIBRARY_PATH so libcuda.so.1 can be found
#export LD_LIBRARY_PATH=""/usr/local/cuda/lib64/stubs/:${LD_LIBRARY_PATH}""

yes """" | ./configure

bazel ${BAZEL_OPTS} build \
    --copt=-march=ivybridge \
    --copt=-mtune=ivybridge \
    --copt=-ftree-vectorize \
    --copt=-fPIC \
    --copt=-fstack-protector-strong \
    --copt=-O3 \
    --cxxopt=-fvisibility-inlines-hidden \
    --cxxopt=-fmessage-length=0 \
    --verbose_failures \
    --config=opt \
    --config=cuda \
    --color=yes \
    --curses=yes \
    --action_env=PATH \
    --action_env=LD_LIBRARY_PATH \
    //tensorflow/tools/pip_package:build_pip_package

# build a whl file
mkdir -p $SRC_DIR/tensorflow_pkg
bazel-bin/tensorflow/tools/pip_package/build_pip_package $SRC_DIR/tensorflow_pkg
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32506,UnboundLocalError: local variable `packed` referenced before assignment,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.7.3

**Describe the current behavior**
Some specific set of circumstances skip over assigning `packed` to a value.

https://github.com/tensorflow/tensorflow/blob/551b67c8dff2d3656cb1858f1cb11beb049dafaf/tensorflow/python/util/nest.py#L472

**Describe the expected behavior**
The local variable `packed` should have a value set.

**Code to reproduce the issue**
Throw an `IndexError` in [this call](https://github.com/tensorflow/tensorflow/blob/551b67c8dff2d3656cb1858f1cb11beb049dafaf/tensorflow/python/util/nest.py#L461) and make sure the [if statement](https://github.com/tensorflow/tensorflow/blob/551b67c8dff2d3656cb1858f1cb11beb049dafaf/tensorflow/python/util/nest.py#L467) is not entered.

I wish I had a portable example for you but I'm not quite sure why my code triggering this path and I don't have much time to investigate the root cause, but I can confirm that is an error that I have encountered naturally. It isn't just hypothetical.

**Other info / logs**
TF portion of the stack trace:
```
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/nest.py"", line 516, in map_structure
    expand_composites=expand_composites)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/nest.py"", line 450, in pack_sequence_as
    return _sequence_like(structure, packed)
UnboundLocalError: local variable 'packed' referenced before assignment
```
"
32505,Keras ModelCheckpoint callback fails to create directory when saving as SavedModel,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10 Pro Version 1903
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
TF 2.0.0rc1
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Using CPU
- GPU model and memory:

**Describe the current behavior**
Using tf.keras.callbacks.ModelCheckpoint results in Failed to create a directory. I believe it's an issue with how paths are created in Tensorflow. Error does not occur if I use double backslash to join checkpoint_folder and model_filename instead of os.path.join. 
**Describe the expected behavior**
Directory for saved model is created and saved model is saved correctly
**Code to reproduce the issue**
```
model_filename = str(args.net) + ""-Epoch-{epoch:02d}-Loss-{val_loss:.2f}""\
if args.save_format == 'h5':
    model_filename += '.h5'
checkpoint_path = os.path.join(args.checkpoint_folder, model_filename)

model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_path,
        monitor='val_loss',
        verbose=1,
        save_best_only=True,
        save_weights_only=False,
        mode='auto')
```

**Other info / logs**
```
Traceback (most recent call last):
  File ""C:/Users/iden/PycharmProjects/PytorchSSD/tf_translate/train_ssd.py"", line 307, in <module>
    workers=args.num_workers, max_queue_size=args.max_queue_size)
  File ""C:\Users\iden\PycharmProjects\PytorchSSD\tf2venv\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 1297, in fit_generator
    steps_name='steps_per_epoch')
  File ""C:\Users\iden\PycharmProjects\PytorchSSD\tf2venv\lib\site-packages\tensorflow_core\python\keras\engine\training_generator.py"", line 332, in model_iteration
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""C:\Users\iden\PycharmProjects\PytorchSSD\tf2venv\lib\site-packages\tensorflow_core\python\keras\callbacks.py"", line 297, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""C:\Users\iden\PycharmProjects\PytorchSSD\tf2venv\lib\site-packages\tensorflow_core\python\keras\callbacks.py"", line 964, in on_epoch_end
    self._save_model(epoch=epoch, logs=logs)
  File ""C:\Users\iden\PycharmProjects\PytorchSSD\tf2venv\lib\site-packages\tensorflow_core\python\keras\callbacks.py"", line 1000, in _save_model
    self.model.save(filepath, overwrite=True)
  File ""C:\Users\iden\PycharmProjects\PytorchSSD\tf2venv\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 1189, in save
    signatures, options)
  File ""C:\Users\iden\PycharmProjects\PytorchSSD\tf2venv\lib\site-packages\tensorflow_core\python\keras\saving\save.py"", line 115, in save_model
    signatures, options)
  File ""C:\Users\iden\PycharmProjects\PytorchSSD\tf2venv\lib\site-packages\tensorflow_core\python\keras\saving\saved_model\save.py"", line 74, in save
    save_lib.save(model, filepath, signatures, options)
  File ""C:\Users\iden\PycharmProjects\PytorchSSD\tf2venv\lib\site-packages\tensorflow_core\python\saved_model\save.py"", line 899, in save
    utils_impl.get_or_create_variables_dir(export_dir)
  File ""C:\Users\iden\PycharmProjects\PytorchSSD\tf2venv\lib\site-packages\tensorflow_core\python\saved_model\utils_impl.py"", line 183, in get_or_create_variables_dir
    file_io.recursive_create_dir(variables_dir)
  File ""C:\Users\iden\PycharmProjects\PytorchSSD\tf2venv\lib\site-packages\tensorflow_core\python\lib\io\file_io.py"", line 438, in recursive_create_dir
    recursive_create_dir_v2(dirname)
  File ""C:\Users\iden\PycharmProjects\PytorchSSD\tf2venv\lib\site-packages\tensorflow_core\python\lib\io\file_io.py"", line 453, in recursive_create_dir_v2
    pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(path))
tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: models/mb1-ssd-Epoch-01-Loss-32.98\variables; No such file or directory

Process finished with exit code 1
```
"
32502,Plain English explanation of CLA?,"
## URL(s) with the issue:

https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#contributor-license-agreements

## Description of issue (what needs changing):

I have no idea what any of this legal mumbo-jumbo actually entails.  ""Grant of Patent License"", ""Grant of Copyright License"".  Do I own my contributions?  Does Google own my contributions?  What does all of this mess mean?  If I create something and ""give it away"", I want to make it free as in gratis and as in libre widely, and not just to Google.  Is that happening here?  Does Google charge/restrict people (e.g. corporations) using tensorflow?  *Can* Google charge/restrict people using tensorflow and/or my contributions? 

### Submit a pull request?
No.
"
32501,Error when using stateful RNN with multiple inputs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0rc0
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0.130/7.6.0
- GPU model and memory: GTX 980 Ti

**Describe the current behavior**

The stock example of RNNs with multiple inputs from here https://www.tensorflow.org/beta/guide/keras/rnn#rnns_with_listdict_inputs_or_nested_inputs produces an error if you set `stateful=True`.  This seems to be a problem with any multi-input RNN with stateful=True.

**Describe the expected behavior**

There should be no error, multi-input RNNs with stateful=True should work the same as with stateful=False (other than preserving state).

**Code to reproduce the issue**

Note, this code is copied from https://www.tensorflow.org/beta/guide/keras/rnn#rnns_with_listdict_inputs_or_nested_inputs, with the exception that I changed the line
```
rnn = tf.keras.layers.RNN(cell)
```
to
```
rnn = tf.keras.layers.RNN(cell, stateful=True)
```

``` python
import collections

import tensorflow as tf

NestedInput = collections.namedtuple(""NestedInput"", [""feature1"", ""feature2""])
NestedState = collections.namedtuple(""NestedState"", [""state1"", ""state2""])


class NestedCell(tf.keras.layers.Layer):
    def __init__(self, unit_1, unit_2, unit_3, **kwargs):
        self.unit_1 = unit_1
        self.unit_2 = unit_2
        self.unit_3 = unit_3
        self.state_size = NestedState(
            state1=unit_1, state2=tf.TensorShape([unit_2, unit_3])
        )
        self.output_size = (unit_1, tf.TensorShape([unit_2, unit_3]))
        super(NestedCell, self).__init__(**kwargs)

    def build(self, input_shapes):
        # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]
        input_1 = input_shapes.feature1[1]
        input_2, input_3 = input_shapes.feature2[1:]

        self.kernel_1 = self.add_weight(
            shape=(input_1, self.unit_1), initializer=""uniform"", name=""kernel_1""
        )
        self.kernel_2_3 = self.add_weight(
            shape=(input_2, input_3, self.unit_2, self.unit_3),
            initializer=""uniform"",
            name=""kernel_2_3"",
        )

    def call(self, inputs, states):
        # inputs should be in [(batch, input_1), (batch, input_2, input_3)]
        # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]
        input_1, input_2 = tf.nest.flatten(inputs)
        s1, s2 = states

        output_1 = tf.matmul(input_1, self.kernel_1)
        output_2_3 = tf.einsum(""bij,ijkl->bkl"", input_2, self.kernel_2_3)
        state_1 = s1 + output_1
        state_2_3 = s2 + output_2_3

        output = [output_1, output_2_3]
        new_states = NestedState(state1=state_1, state2=state_2_3)

        return output, new_states


unit_1 = 10
unit_2 = 20
unit_3 = 30

input_1 = 32
input_2 = 64
input_3 = 32
batch_size = 64
num_batch = 100
timestep = 50

cell = NestedCell(unit_1, unit_2, unit_3)
rnn = tf.keras.layers.RNN(cell, stateful=True)

inp_1 = tf.keras.Input((None, input_1))
inp_2 = tf.keras.Input((None, input_2, input_3))

outputs = rnn(NestedInput(feature1=inp_1, feature2=inp_2))

model = tf.keras.models.Model([inp_1, inp_2], outputs)

model.compile(optimizer=""adam"", loss=""mse"", metrics=[""accuracy""])
```

**Other info / logs**
```
Traceback (most recent call last):
  File "".../tmp2.py"", line 70, in <module>
    outputs = rnn(NestedInput(feature1=inp_1, feature2=inp_2))
  File ""...\site-packages\tensorflow_core\python\keras\layers\recurrent.py"", line 623, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""...\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 777, in __call__
    self._maybe_build(inputs)
  File ""...\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 2099, in _maybe_build
    self.build(input_shapes)
  File ""...\site-packages\tensorflow_core\python\keras\layers\recurrent.py"", line 561, in build
    self.reset_states()
  File ""...\site-packages\tensorflow_core\python\keras\layers\recurrent.py"", line 809, in reset_states
    spec_shape = None if self.input_spec is None else self.input_spec[0].shape
AttributeError: 'NestedInput' object has no attribute 'shape'
```
"
32500,"Memory continues to grow after repeated calls to model.predict(tf.one_hot(states, dtype='float32', depth=3))","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): tensorflow==2.0.0-rc1
- TensorFlow version (use command below): v2.0.0-rc0-101-gd2d2566 2.0.0-rc1
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Intel Iris Plus Graphics 640 1536 MB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Running the below code in Docker (version 19.03.2) causes the memory to grow without limit. This is visible in `docker stats`, eventually crashing docker.

**Describe the expected behavior**

The memory should not grow indefinitely

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python3
import tensorflow as tf

rows = 6
columns = 7

model = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=[rows * columns, 3]),
  tf.keras.layers.Dense(7, input_shape=[rows * columns * 3]),
])

model.compile(
  optimizer=tf.keras.optimizers.SGD(lr=0.01),
  loss='mean_squared_error',
  metrics=['accuracy']
)

states = [ [ 1 ] * rows * columns for i in range(20) ]

for iteration in range(1000000):
    print('iteration', iteration)
    model.predict(tf.one_hot(states, dtype='float32', depth=3))

```

The aforementioned code runs in an image generated by the following Dockerfile

```Dockerfile
FROM centos:7

ENV SOURCE_DIRECTORY /tmp/tf-connect4

ENV PYTHON_VERSION 3.7.4

RUN yum -y groupinstall -y ""Development Tools"" && \
    yum -y update && \
    yum -y install openssl-devel zlib-devel libffi libffi-devel wget && \
    wget https://www.python.org/ftp/python/$PYTHON_VERSION/Python-$PYTHON_VERSION.tar.xz && \
    tar -xJf Python-$PYTHON_VERSION.tar.xz && \
    cd Python-$PYTHON_VERSION && \
    ./configure && \
    make && \
    make install && \
    pip3 install --upgrade pip && \
    pip3 install tensorflow==2.0.0-rc1 tensorflow_probability==0.8.0-rc0 numpy falcon jsonschema
```"
32497,ModuleNotFoundError: No module named 'official.wide_deep',"## URL(s) with the issue:

https://www.tensorflow.org/tutorials/estimators/linear

## Description of issue (what needs changing):
`https://github.com/tensorflow/models` has `wide_deep` in `official.r1.wide_deep` but the documentation says it's in `official.wide_deep`.
"
32496,[r2.0.0-rc1] Converting to TFLite format: <type == kTfLiteInt32/64 condition> was not true. ONE_HOT failed to prepare,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.2 LTS**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): **Source**
- TensorFlow version (use command below): **2.0.0-rc1 commit 59bf33**
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0.130
- GPU model and memory: NVidia 1080Ti / 11G

**Describe the current behavior**

Conversion of TF2.0 function containing `reshape` and `one_hot` ops to TFLite format fails with the following RuntimeError. Source code of the program is listed in the `Code` section below.

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-2-ec9775ede022> in <module>
----> 1 run()

~/mironov/hbtest/tflite_one_hot_bug_v2.py in run()
     25
     26   interpreter = tf.lite.Interpreter(model_path=model_file)
---> 27   interpreter.allocate_tensors()
     28
     29 if __name__ == '__main__':

/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter.py in allocate_tensors(self)
    242   def allocate_tensors(self):
    243     self._ensure_safe()
--> 244     return self._interpreter.AllocateTensors()
    245
    246   def _safe_to_run(self):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)                                                                                         
    104
    105     def AllocateTensors(self):
--> 106         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
    107
    108     def Invoke(self):

RuntimeError: tensorflow/lite/kernels/one_hot.cc:141 op_context.indices->type == kTfLiteInt32 || op_context.indices->type == kTfLiteInt64 was not true.Node number 3 (ONE_HOT) failed to prepare.
```

**Describe the expected behavior**

`converter.convert()`  finishes without errors

**Code to reproduce the issue**

File: `tflite_one_hot_bug_v2.py`
```
import tensorflow as tf
import numpy as np

@tf.function(input_signature=[
    tf.TensorSpec(shape=[10,10], dtype=tf.int32, name='inp0')])
def model(inp0):
  res = tf.reshape(inp0, [100])
  res = tf.one_hot(res, depth=10)
  print(res.shape)
  return res

def run():
  def _representative_dataset_gen():
    for i in range(10):
      yield [np.random.random_integers(0, 9, size=[10,10]).astype('int32')]

  cfunc = model.get_concrete_function()
  converter = tf.lite.TFLiteConverter.from_concrete_functions([cfunc])
  converter.representative_dataset = _representative_dataset_gen
  converter.optimizations = [tf.lite.Optimize.DEFAULT]

  tflite_model = converter.convert()
  model_file = ""/tmp/tflite_one_hot_bug_v2.tflite""
  open(model_file, ""wb"").write(tflite_model)

  interpreter = tf.lite.Interpreter(model_path=model_file)
  interpreter.allocate_tensors()

if __name__ == '__main__':
  run()
```

**Other info / logs**
N/A"
32495,[r2.0.0-rc1] Converting to TFLite format: Invalid quantization params for op RESHAPE,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.2 LTS**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): **Source**
- TensorFlow version (use command below): **2.0.0-rc1 commit 59bf33**
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0.130
- GPU model and memory: NVidia 1080Ti / 11G

**Describe the current behavior**

Conversion of TF2.0 function containing `softmax` and `reshape` ops to TFLite format fails with the following RuntimeError. Source code of the program is listed in the `Code` section below.

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-4-ec9775ede022> in <module>
----> 1 run()

~/mironov/hbtest/tflite_softmax_bug_v2.py in run()
     20   converter.optimizations = [tf.lite.Optimize.DEFAULT]
     21
---> 22   tflite_model = converter.convert()
     23   model_file = ""/tmp/tflite_softmax_bug_v2.tflite""
     24   open(model_file, ""wb"").write(tflite_model)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py in convert(self)
    448     if self._is_calibration_quantize():
    449       result = self._calibrate_quantize_model(result, constants.FLOAT,
--> 450                                               constants.FLOAT)
    451
    452     return result

/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type)                                                                             
    237     return calibrate_quantize.calibrate_and_quantize(
    238         self.representative_dataset.input_gen, inference_input_type,
--> 239         inference_output_type, allow_float)
    240
    241   def _get_base_converter_args(self):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float)                                                                   
     76     return self._calibrator.QuantizeModel(
     77         np.dtype(input_type.as_numpy_dtype()).num,
---> 78         np.dtype(output_type.as_numpy_dtype()).num, allow_float)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py in QuantizeModel(self, input_py_type, output_py_type, allow_float)                                                     
    113
    114     def QuantizeModel(self, input_py_type, output_py_type, allow_float):
--> 115         return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)                                                                                            
    116 CalibrationWrapper_swigregister = _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_swigregister
    117 CalibrationWrapper_swigregister(CalibrationWrapper)

RuntimeError: Invalid quantization params for op RESHAPE at index 1 in subgraph 0

```

**Describe the expected behavior**

`converter.convert()` call finishes without errors


**Code to reproduce the issue**

File: `tflite_softmax_bug_v2.py`
```
import tensorflow as tf
import numpy as np

@tf.function(input_signature=[
    tf.TensorSpec(shape=[10], dtype=tf.float32, name='inp0')])
def model(inp0):
  res = tf.nn.softmax(inp0)
  res = tf.reshape(res, [1, 10])
  print(res.shape)
  return res

def run():
  def _representative_dataset_gen():
    for i in range(10):
      yield [np.random.random_integers(0, 9, size=[10]).astype('float32'),]

  cfunc = model.get_concrete_function()
  converter = tf.lite.TFLiteConverter.from_concrete_functions([cfunc])
  converter.representative_dataset = _representative_dataset_gen
  converter.optimizations = [tf.lite.Optimize.DEFAULT]

  tflite_model = converter.convert()
  model_file = ""/tmp/tflite_softmax_bug_v2.tflite""
  open(model_file, ""wb"").write(tflite_model)

  interpreter = tf.lite.Interpreter(model_path=model_file)
  interpreter.allocate_tensors()

if __name__ == '__main__':
  run()

```

**Other info / logs**
N/A"
32493,How to determine the shape of input and output nodes in graph?  (C++) ,"**System information**
- TensorFlow version (you are using): 1.14
- Operation system: Ubuntu 18.04
- Are you willing to contribute it (Yes/No): No

**Describe the current behavior**

At first, I tried to implement the following pipeline:

- 1a) I converted the model from keras (.hdf5) to tensorflow (.pb) and saved using the `tf.saved_model.simple_save` function in Python.
- 2a) I loaded the model using `tensorflow::LoadSavedModel`
- 3a) I determined a shape using the following code:

```C++
const string export_dir = ""/home/user/MyBuild/build_tftest2/models"";
 SavedModelBundle bundle; RunOptions run_options;
status = LoadSavedModel(sopt, run_options, export_dir,
               {kSavedModelTagServe}, &bundle);
if (status.ok())
{
    GraphDef graph = bundle.meta_graph_def.graph_def();
    auto shape = graph.node().Get(0).attr().at(""shape"").shape();
    for (int i = 0; i < shape.dim_size(); i++) {
        std::cout << shape.dim(i).size()<<std::endl;
    }
}
```
Later I was wishing to use optimal model, therefore I used another pipeline:

 - 1b) I converted the model from keras (.hdf5) to tensorflow (.pb) using 
```python
frozen_graph = convert_variables_to_constants(session, input_graph_def,
output_names, freeze_var_names)
```
 and saved in Python using:

```python
tf.train.write_graph(frozen_graph, ""model"", ""tf_model.pb"", as_text=False)
```
- 2b) I optimized the model using the `transform_graph` utility from tensorflow package.
- 3b) I loaded my graph using the following code (C++):
```c++
status = ReadBinaryProto(Env::Default(), ""/home/user/MySoftware/foreign code/netology_JN/Diplom/Models/optimized/optim_model.pb"", &graph_def);
```
- 4b) I tried to determine a shape using 3a) and an exception was thrown:


```
[libprotobuf FATAL /usr/local/include/google/protobuf/map.h:1064]
CHECK failed: it != end(): key not found: shape terminate called after
throwing an instance of 'google::protobuf::FatalException' what():
CHECK failed: it != end(): key not found: shape 14:21:36: The program
has unexpectedly finished.
```
"
32492,SIGSEGV,"```
import tensorflow as tf
from tensorflow_probability import distributions as tfd

mu = [1., 2., 3.]
mu = tf.stack([mu, mu], 0)
cov = [[ 0.36,  0.12,  0.06],
       [ 0.12,  0.29, -0.13],
       [ 0.06, -0.13,  0.26]]
cov = tf.stack([cov, cov], 0)
scale = tf.linalg.cholesky(cov)

mvn = tfd.MultivariateNormalTriL(
    loc=mu,
    scale_tril=scale)

l = mvn.bijector.scale.to_dense()
l = tf.linalg.triangular_solve(l, tf.eye(3))
```

**System information**
- Ubuntu 18.04
- TensorFlow installed from binary
- TensorFlow version: 2.0.0-dev20190913
- Python version: 3.7
- No GPU

git version: v1.12.1-11024-ge8506e0
tensorflow-probability version: 0.9.0-dev20190913

**Describe the current behavior**
`Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)`

Sometimes it hangs for a bit, sometimes it doesn't crash. But 2/3 times I get a SIGSEGV.
Most modifications to the posted code somehow solve the crash.

**Describe the expected behavior**
It should complain about dimensions that mismatch at the last operation.


"
32491,Getting SIGSEGV when passing Tensor to sample_from_datasets during checkpoint save,"**System information**
- Have I written custom code: 
yes
- OS Platform and Distribution:
Linux Ubuntu 16.04
- TensorFlow installed from:
`pip`
- TensorFlow version (use command below):
Exact Tensorflow version: v1.12.0-0-ga6d8ffae09 1.12.0
*Update:* Same in v1.12.3-0-g41e0a4f56c 1.12.3
```
$ pip freeze | grep tensorflow
mesh-tensorflow==0.0.5
tensorflow-datasets==1.2.0
tensorflow-gan==1.0.0.dev0
tensorflow-gpu==1.12.0
tensorflow-metadata==0.14.0
tensorflow-probability==0.5.0
```
- Python version: 3.5
- CUDA/cuDNN version: 9.0
- GPU model and memory: GeForce GTX 1080

**Describe the current behavior**

Passing a tensor to `sample_from_datasets` which depends on the global step causes a SIGSEGV.

**Describe the expected behavior**

Either don't produce a SIGSEGV but a more meaningful error message or simply don't fail at all.

**Code to reproduce the issue**

```python
# ...
elif pretrain_cfg.schedule == PretrainSchedule.CONVERGE_LINEARLY:
    a = tf.minimum(tf.constant(1.0, dtype=tf.float64, shape=(1,)), global_step / max_pretrain_steps)
    b = tf.maximum(tf.constant(0.0, dtype=tf.float64, shape=(1,)), 1 - global_step / max_pretrain_steps)
    weights = a * const_task_weights + b * pretrain_task_weights

return tf.data.experimental.sample_from_datasets(datasets, weights=weights)
```

The following setup works in comparison:

```python
if pretrain_cfg.schedule == PretrainSchedule.CONSTANT:
    weights = tf.cond(
        tf.greater(global_step, max_pretrain_steps),
        true_fn=lambda: const_task_weights,
        false_fn=lambda: pretrain_task_weights
    )
```

**Other info / logs**

```
...
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into /data/itranslate-translation/multi-problem/hi2en/model/512-3-1-1024/de2en.hi2en/c19cfad259cad911/model.ckpt.
bash: line 1: 14168 Segmentation fault      (core dumped) env ""CUDA_VISIBLE_DEVICES""=""0"" ""PYCHARM_MATPLOTLIB_PORT""=""49858"" ""JETBRAINS_REMOTE_RUN""=""1"" ""PYTHONIOENCODING""=""UTF-8"" ""PYTHONPATH""=""/home/sfalk/.pycharm_helpers/pycharm..
Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)
```
"
32490,//tensorflow/contrib/distributions/python/kernel_tests/independent_test.py test fails with Assertion error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04 s390x
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.14.0
- Python version: 2.7.15
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the current behavior**
```
FAIL: testMnistLikeDynamicShape (__main__.ProductDistributionTest)
testMnistLikeDynamicShape (__main__.ProductDistributionTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor
    yield
  File ""/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py"", line 162, in run
    testMethod()
  File ""tensorflow/contrib/distributions/python/kernel_tests/independent_test.py"", line 275, in testMnistLikeDynamicShape
    self._testMnistLike(static_shape=False)
  File ""tensorflow/contrib/distributions/python/kernel_tests/independent_test.py"", line 269, in _testMnistLike
    rtol=1e-6, atol=0.)
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 1073, in decorated
    return f(*args, **kwds)
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 2303, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 2272, in _assertAllCloseRecursive
    (path_str, path_str, msg)))
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 2207, in _assertArrayLikeAllClose
    a, b, rtol=rtol, atol=atol, err_msg=""\n"".join(msgs), equal_nan=True)
  File ""/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py"", line 1501, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py"", line 827, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Not equal to tolerance rtol=1e-06, atol=0
Mismatched value: a is different from b.
not close where = (array([1, 2, 2, 2, 3]), array([3, 1, 2, 4, 4]), array([6, 1, 1, 8, 6]))
not close lhs = [-463.41407785 -467.87059292 -444.45405599 -469.33429804 -457.64799854]
not close rhs = [-463.41458 -467.87006 -444.45358 -469.3348  -457.6486 ]
not close dif = [0.00050345 0.00053677 0.00047323 0.00051031 0.00059155]
not close tol = [0.00046341 0.00046787 0.00044445 0.00046933 0.00045765]
dtype = float64, shape = (4, 5, 10)
Mismatch: 2.5%
Max absolute difference: 0.00059155
Max relative difference: 1.29257756e-06
 x: array([[[-465.912459, -448.916315, -457.207675, -486.805523,
         -456.784984, -448.14827 , -453.583166, -486.295655,
         -468.533898, -481.740375],...
 y: array([[[-465.9126 , -448.9159 , -457.20764, -486.8053 , -456.7849 ,
         -448.1483 , -453.5835 , -486.2955 , -468.53412, -481.74048],
        [-472.38965, -483.41187, -464.7721 , -467.14288, -478.4115 ,...

======================================================================
FAIL: testMnistLikeStaticShape (__main__.ProductDistributionTest)
testMnistLikeStaticShape (__main__.ProductDistributionTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor
    yield
  File ""/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py"", line 162, in run
    testMethod()
  File ""tensorflow/contrib/distributions/python/kernel_tests/independent_test.py"", line 272, in testMnistLikeStaticShape
    self._testMnistLike(static_shape=True)
  File ""tensorflow/contrib/distributions/python/kernel_tests/independent_test.py"", line 269, in _testMnistLike
    rtol=1e-6, atol=0.)
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 1073, in decorated
    return f(*args, **kwds)
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 2303, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 2272, in _assertAllCloseRecursive
    (path_str, path_str, msg)))
  File ""/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py"", line 2207, in _assertArrayLikeAllClose
    a, b, rtol=rtol, atol=atol, err_msg=""\n"".join(msgs), equal_nan=True)
  File ""/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py"", line 1501, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py"", line 827, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Not equal to tolerance rtol=1e-06, atol=0
Mismatched value: a is different from b.
not close where = (array([1, 2, 2, 2, 3]), array([3, 1, 2, 4, 4]), array([6, 1, 1, 8, 6]))
not close lhs = [-463.41407785 -467.87059292 -444.45405599 -469.33429804 -457.64799854]
not close rhs = [-463.41458 -467.87006 -444.45358 -469.3348  -457.6486 ]
not close dif = [0.00050345 0.00053677 0.00047323 0.00051031 0.00059155]
not close tol = [0.00046341 0.00046787 0.00044445 0.00046933 0.00045765]
dtype = float64, shape = (4, 5, 10)
Mismatch: 2.5%
Max absolute difference: 0.00059155
Max relative difference: 1.29257756e-06
 x: array([[[-465.912459, -448.916315, -457.207675, -486.805523,
         -456.784984, -448.14827 , -453.583166, -486.295655,
         -468.533898, -481.740375],...
 y: array([[[-465.9126 , -448.9159 , -457.20764, -486.8053 , -456.7849 ,
         -448.1483 , -453.5835 , -486.2955 , -468.53412, -481.74048],
        [-472.38965, -483.41187, -464.7721 , -467.14288, -478.4115 ,...

----------------------------------------------------------------------
Ran 10 tests in 1.019s

FAILED (failures=2)
```
**Describe the expected behavior**
```
The test should pass on s390x.
```
**Code to reproduce the issue**

```
python tensorflow/contrib/distributions/python/kernel_tests/independent_test.py
```

"
32489,tensorflow does not detect 2nd GPU,"Hi,

I am trying to use 2 GPUs, tensorflow does not recognise the 2nd one. the 2nd GPU is working fine (in widows environment).

using tensorflow example: tf.keras.utils.multi_gpu_model, https://www.tensorflow.org/api_docs/python/tf/keras/utils/multi_gpu_model

I see the follwing error

ValueError: To call `multi_gpu_model` with `gpus=2`, we expect the following devices to be available: ['/cpu:0', '/gpu:0', '/gpu:1']. However this machine only has: ['/cpu:0', '/gpu:0']. Try reducing `gpus`.

I can switch between the two graphic cards, with CUDA VISIBLE DEVICE , which sets one of the GPUs to GPU 0. however both can not be used. device 0 and device 1 are recognised however one GPU is only recognised.

The main GPU is RTX2070 (8GB) and 2nd GPU is GTX1050 (2GB). Before i submit i spent sometime searching for solution and did whatever I could find on the internet. drivers are up to date, 64bit version and latest versions of the software are installed. I dont see any issue, beside not appearing the 2nd GPU. The codes are working fine on first GPU, both have > 3.5 computational capacity."
32488,Issue: Structure of Python function inputs does not match input_signature while trying to save a subclassed model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.14.0-0-g87989f6959 1.14.0 / tensorflow-gpu==2.0.0-rc0
- Python version: 3.6.8
- GPU model and memory: Google Colab (Nvidia T4)

Similar to this issue #28165, I have defined the following Encoder class with a tf.function signature hoping to be able to save it using the SavedModel format. In my case, I'm using a LSTM layer though instead of GRUs:

```
class Encoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding_size, lstm_size):
        super(Encoder, self).__init__()
        self.lstm_size = lstm_size
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)
        self.lstm = tf.keras.layers.LSTM(
            lstm_size, return_sequences=True, return_state=True)

    @tf.function(input_signature=(
        tf.TensorSpec([None, None], tf.int32, name='sequence'),
        (tf.TensorSpec([None, 64], tf.float32, name='states_1'), tf.TensorSpec([None, 64], tf.float32, name='states_2'))
    )) 
    def call(self, sequence, states):
        embed = self.embedding(sequence)
        output, state_h, state_c = self.lstm(embed, initial_state=states)

        return output, state_h, state_c

    def init_states(self, batch_size):
        return (tf.zeros([batch_size, self.lstm_size]),
                tf.zeros([batch_size, self.lstm_size]))

# Some more code for training the seq2seq model...

tf.saved_model.save(
    encoder,  # instance of Encoder
    './some/directory/',
    signatures=encoder.call
)
```
Here's also the Colab notebook to reproduce the results: [Google Colab Link](https://colab.research.google.com/drive/11rgiI7oYT9uiRWG3ZT9NvRhuRJ5ubEQY)

Since the issue was closed I hope it would work now but now I get this weird error that the Python function input does not match the input_signature although they clearly match each other:

```
ValueError: Structure of Python function inputs does not match input_signature:
  inputs: (
    [<tf.Tensor 'sequence:0' shape=(None, None) dtype=int32>, (<tf.Tensor 'states_1:0' shape=(None, 64) dtype=float32>, <tf.Tensor 'states_2:0' shape=(None, 64) dtype=float32>)])
  input_signature: (
    TensorSpec(shape=(None, None), dtype=tf.int32, name='sequence'),
    (TensorSpec(shape=(None, 64), dtype=tf.float32, name='states_1'), TensorSpec(shape=(None, 64), dtype=tf.float32, name='states_2')))
```
"
32487,[TF -2] Multi gpu training error,"I am trying to train a keras model on two k80. 
**
- Have I written custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): SMP Debian 4.9.144-3.1
- TensorFlow version (use command below): 2.0.0-rc0
- Python version: 3.6.9
- CUDA/cuDNN version: 10.1 
- GPU model and memory: Tesla K80

Here is the the keras model that I am trying to fit:
```
import tensorflow as tf
import numpy as np

class SparseSlice(tf.keras.layers.Layer):
    def __init__(self, feature_column):
        super(SparseSlice, self).__init__()
        self.fc = feature_column

    def build(self, input_shape):

        self.kernel = self.add_weight('{}_kernel'.format(self.fc.name), shape=(self.fc.num_buckets, ), dtype=tf.float32)

    def call(self, input):
        ids = self.fc._transform_input_tensor(input)
        return tf.expand_dims(tf.gather(self.kernel, ids.values), axis=1)


strategy = tf.distribute.MirroredStrategy()
with strategy.scope():

    batch_size = 10
    sparse_col = tf.feature_column.categorical_column_with_hash_bucket('sparse_col', 10000, dtype=tf.int64)
    dense_col = tf.feature_column.numeric_column('dense_col', dtype=tf.float32)
    example_spec = tf.feature_column.make_parse_example_spec([sparse_col, dense_col])

    sparse_inputs = tf.keras.layers.Input(name=sparse_col.name, shape=(None, ), batch_size=batch_size, sparse=True, dtype=tf.int64)
    dense_inputs = {dense_col.name: tf.keras.layers.Input(name=dense_col.name, shape=(1, ), dtype=tf.float32)}

    sparse_out = SparseSlice(sparse_col)(sparse_inputs)
    output = tf.keras.layers.Dense(1, activation='sigmoid')(sparse_out)
    num = tf.keras.layers.DenseFeatures(dense_col)(dense_inputs)

    concats = tf.keras.layers.Concatenate()([output, num])
    output = tf.keras.layers.Dense(1, activation='sigmoid')(concats)

    model = tf.keras.Model([dense_inputs, {'sparse_output': sparse_inputs}], output)

    model.compile(optimizer='adam',
                  loss='mse')

    np.random.random(())

    features = {dense_col.name: tf.constant(np.random.random((batch_size, )))}
    features.update({sparse_col.name: tf.sparse.SparseTensor(indices=[[i, 0] for i in range(batch_size)], values=np.random.randint(0, 1000, (batch_size, )), dense_shape=(batch_size, 1))})
    ys = tf.constant(np.random.rand(batch_size), dtype=tf.float32)

    dataset = tf.data.Dataset.from_tensor_slices((features, ys)).batch(batch_size)

    model.fit(x=dataset,
              epochs=1
              )
```
but I am getting the following error:

```
2019-09-13 06:48:10.524592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-13 06:48:10.525159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
2019-09-13 06:48:10.525252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-13 06:48:10.525673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:05.0
2019-09-13 06:48:10.525737: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-13 06:48:10.525763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-13 06:48:10.525798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-13 06:48:10.525835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-13 06:48:10.525869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-13 06:48:10.525904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-13 06:48:10.525937: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-13 06:48:10.526033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-13 06:48:10.526541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-13 06:48:10.527021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-13 06:48:10.527491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-13 06:48:10.527907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1
2019-09-13 06:48:10.528002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-13 06:48:10.528023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 
2019-09-13 06:48:10.528036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y 
2019-09-13 06:48:10.528054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N 
2019-09-13 06:48:10.528240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-13 06:48:10.528714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-13 06:48:10.529244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-13 06:48:10.529670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2019-09-13 06:48:10.529763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-13 06:48:10.530226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:1 with 10805 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:00:05.0, compute capability: 3.7)
      1/Unknown - 0s 75ms/stepTraceback (most recent call last):
  File ""/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-7-9c71ae70d829>"", line 33, in <module>
    epochs=1
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 734, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 324, in fit
    total_epochs=epochs)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 86, in execution_function
    distributed_function(input_fn))
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 427, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 370, in _initialize
    *args, **kwds))
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1847, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2147, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2038, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 320, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 66, in distributed_function
    model, input_iterator, mode)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 112, in _prepare_feed_values
    inputs, targets, sample_weights = _get_input_from_iterator(inputs)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 132, in _get_input_from_iterator
    next_element = next(iterator)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 275, in __next__
    return self.get_next()
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 304, in get_next
    global_has_value, replicas = _get_next_as_optional(self, self._strategy)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 200, in _get_next_as_optional
    iterator._iterators[i].get_next_as_list(new_name))  # pylint: disable=protected-access
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 878, in get_next_as_list
    lambda: _dummy_tensor_fn(data.value_structure))
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 1174, in cond
    return cond_v2.cond_v2(pred, true_fn, false_fn, name)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/cond_v2.py"", line 91, in cond_v2
    op_return_value=pred)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 878, in <lambda>
    lambda: _dummy_tensor_fn(data.value_structure))
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 801, in _dummy_tensor_fn
    result.append(create_dummy_tensor(feature_shape, feature_type))
  File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 784, in create_dummy_tensor
    for dim in feature_shape.dims:
TypeError: 'NoneType' object is not iterable
```
Everything runs fine if I exclude the `with strategy.scope()`"
32484,tflite model inaccurate on python and not loading on android ,"
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes I'm using a custom model (I can send you the original model and the converted tflite model)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):1.14
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
When running the tflite model through python the results are very different to the original model when no quantisation has been applied. Also, the model does not load at all on android.


**Describe the expected behavior**
The same output as the original model
**Code to reproduce the issue**
I'm converting the original ckpt model to pb using frozen graph 
```
tf.train.write_graph(graph_or_graph_def=sess.graph_def, logdir=checkpoint_dir+""\\pb"", name=pbtxt_filename, as_text=True)

freeze_graph.freeze_graph(input_graph=pbtxt_filepath, input_saver='', input_binary=False,
                          input_checkpoint=checkpoint_dir+""model.ckpt"", output_node_names='g_conv10/BiasAdd',
                          restore_op_name='save/restore_all', filename_tensor_name='save/Const:0',
                          output_graph=checkpoint_dir+""pb\\modelpy.pb"", clear_devices=True, initializer_nodes='')
```

Then I convert the .pb to tflite using :

```
 converter = tf.lite.TFLiteConverter.from_frozen_graph(""""""pb_model_dir""""""
                                                      ,input_arrays=[""image_in""]  ,output_arrays=[""g_conv10/BiasAdd""])
 ```

Any help would be appreciated.
"
32480,ODR violations between cuDNN and TensorRT,"_This reports an issue with an NVIDIA library and **not** a bug in TensorFlow. This issue serves as a public description and permalink._

There are [One Definition Rule (ODR)](https://en.wikipedia.org/wiki/One_Definition_Rule) violations between TensorRT and cuDNN that can cause binaries that link in both these libraries to crash or misbehave.  We observed this with the combination of TensorRT 5.1.5 and cuDNN 7.6.2 linked statically. We don’t know if builds that use shared libraries are affected.

For instance, both TensorRT 5.1.5 and cuDNN 7.6.2 define `_Z22first_layer_fwd_kernelILi4ELi7ELi7ELi64EEv19FirstLayerFwdParams` (demangled: `void first_layer_fwd_kernel<4, 7, 7, 64>(FirstLayerFwdParams)`) but SASS blobs corresponding to these kernels are not the same in TensorRT and cuDNN.  This means that in an application that links in both TensorRT and cuDNN, one of the two libraries will launch the incorrect kernel.  Observed failures manifest as GPU side crash due to misaligned memory access or data corruption.

We have informed NVIDIA and they are investigating the issue."
32479,[TF 2.0] AutoGraph: Jacobian docs example could not be transformed,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian stretch in Crostini ❤️ 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): `pip install --upgrade tf-nightly-2.0-preview`
- TensorFlow version (use command below): v1.12.1-10936-g3c7062c 2.0.0-dev20190912
- Python version: 3.7.4

**Describe the current behavior**

When I run the example code from https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape#example_usage_2

I get this: 

```
(libsbn) x86_64-conda_cos6-linux-gnu libsbn/python ‹103-branch-length*› » export AUTOGRAPH_VERBOSITY=10 && python autograph_issue.py 
2019-09-12 14:14:23.681818: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-12 14:14:23.689486: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1607975000 Hz
2019-09-12 14:14:23.689916: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c529e0a3cf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-09-12 14:14:23.689979: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:Entity <function pfor.<locals>.f at 0x7ebbe2511e60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'
```

**Describe the expected behavior**

To compute the Jacobian without complaining.

**Code to reproduce the issue**

From the example, 

```python
import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()

with tf.GradientTape() as g:
    x  = tf.constant([1.0, 2.0])
    g.watch(x)
    y = x * x
jacobian = g.jacobian(y, x)
```

**Other info / logs**
"
32478,optimizer.apply_gradients does not accept tape.gradient information,"**System information**
Running inside colab

**Describe the current behavior**

I was expecting to be able to apply the same list of variables in apply_gradient as in t.gradient. But I get an error.

from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf
tf.enable_eager_execution()
optimizer = tf.train.GradientDescentOptimizer(0.001)
x = tf.Variable(2*tf.ones((5,1)))  
with tf.GradientTape(persistent=True) as t:
  y = x**2
  dy_dx = t.gradient(y, x)
  optimizer.apply_gradients(dy_dx,x)  # optimizer = Adam

"
32477,BatchNormalization doesn't work in graph mode in tf2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow:
- OS: MacOS
- TensorFlow version (use command below): '2.0.0-rc0'
- Python version: 3.7

tf.keras.layers.BatchNormalization layer does not work in graph mode

code to reproduce:
```
import tensorflow as tf
from tensorflow.keras.layers import BatchNormalization
import numpy as np

keras = tf.keras

class check_bn_model(keras.Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.bn = BatchNormalization()
    
    @tf.function
    def call(self, x):
        x = self.bn(x)
        return x
    
X = np.ones((10,5)).astype('float32')
model = check_bn_model()
model.compile('adam', 'mse')
model.fit(X, X, batch_size=2, epochs=2)
```

Error stack:
```
InaccessibleTensorError                   Traceback (most recent call last)
<ipython-input-142-0234ba2796e1> in <module>
     12 model = check_bn_model()
     13 model.compile('adam', 'mse')
---> 14 model.fit(X, X, batch_size=2, epochs=2)

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    732         max_queue_size=max_queue_size,
    733         workers=workers,
--> 734         use_multiprocessing=use_multiprocessing)
    735 
    736   def evaluate(self,

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    222           validation_data=validation_data,
    223           validation_steps=validation_steps,
--> 224           distribution_strategy=strategy)
    225 
    226       total_samples = _get_total_number_of_samples(training_data_adapter)

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _process_training_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)
    545         max_queue_size=max_queue_size,
    546         workers=workers,
--> 547         use_multiprocessing=use_multiprocessing)
    548     val_adapter = None
    549     if validation_data:

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _process_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)
    591         batch_size=batch_size,
    592         check_steps=False,
--> 593         steps=steps)
    594   adapter = adapter_cls(
    595       x,

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2382     # First, we build the model on the fly if necessary.
   2383     if not self.inputs:
-> 2384       all_inputs, y_input, dict_inputs = self._build_model_with_inputs(x, y)
   2385       is_build_called = True
   2386     else:

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _build_model_with_inputs(self, inputs, targets)
   2585     else:
   2586       cast_inputs = inputs
-> 2587     self._set_inputs(cast_inputs)
   2588     return processed_inputs, targets, is_dict_inputs
   2589 

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _set_inputs(self, inputs, outputs, training)
   2672           kwargs['training'] = training
   2673       try:
-> 2674         outputs = self(inputs, **kwargs)
   2675       except NotImplementedError:
   2676         # This Model or a submodel is dynamic and hasn't overridden

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    800                     not base_layer_utils.is_in_eager_or_tf_function()):
    801                   with auto_control_deps.AutomaticControlDependencies() as acd:
--> 802                     outputs = call_fn(cast_inputs, *args, **kwargs)
    803                     # Wrap Tensors in `outputs` in `tf.identity` to avoid
    804                     # circular dependencies.

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    437         # Lifting succeeded, so variables are initialized and we can run the
    438         # stateless function.
--> 439         return self._stateless_fn(*args, **kwds)
    440     else:
    441       canon_args, canon_kwds = \

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   1820     """"""Calls a graph function specialized to the inputs.""""""
   1821     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 1822     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1823 
   1824   @property

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-> 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1228           {""PartitionedCall"": gradient_name,
   1229            ""StatefulPartitionedCall"": gradient_name}):
-> 1230         flat_outputs = forward_function.call(ctx, args)
   1231     if isinstance(flat_outputs, ops.Operation) or flat_outputs is None:
   1232       # We only record function calls which have outputs.

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    538                 executing_eagerly=executing_eagerly,
    539                 config=config,
--> 540                 executor_type=executor_type)
    541 
    542     if executing_eagerly:

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/ops/functional_ops.py in partitioned_call(args, f, tout, executing_eagerly, config, executor_type)
    857           f=f,
    858           config_proto=config,
--> 859           executor_type=executor_type)
    860     else:
    861       outputs = gen_functional_ops.partitioned_call(

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_functional_ops.py in stateful_partitioned_call(args, Tout, f, config, config_proto, executor_type, name)
    670         ""StatefulPartitionedCall"", args=args, Tout=Tout, f=f, config=config,
    671                                    config_proto=config_proto,
--> 672                                    executor_type=executor_type, name=name)
    673   _result = _op.outputs[:]
    674   if not _result:

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    791         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,
    792                          input_types=input_types, attrs=attr_protos,
--> 793                          op_def=op_def)
    794       return output_structure, op_def.is_stateful, op
    795 

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in create_op(***failed resolving arguments***)
    542       if ctxt is not None and hasattr(ctxt, ""AddValue""):
    543         inp = ctxt.AddValue(inp)
--> 544       inp = self.capture(inp)
    545       inputs[i] = inp
    546     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access

~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in capture(self, tensor, name)
    601               "" explicit Python locals or TensorFlow collections to access""
    602               "" it. Defined in: %s; accessed from: %s.\n""
--> 603               % (tensor, tensor.graph, self))
    604         inner_graph = inner_graph.outer_graph
    605       return self._capture_helper(tensor, name)

InaccessibleTensorError: The tensor 'Tensor(""batch_normalization_194/batch_normalization_194_trainable:0"", dtype=bool)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=call, id=5982930640); accessed from: FuncGraph(name=keras_graph, id=5269648784).
```

If you remove the `@tf.function` decorator or if you pass `dynamic=True` to the model instantialization it will work. Otherwise it fails. Note that this is specific to BatchNormalization, if you replace it with any other layer it will work (even other normalization layers like layer norm/instancenorm)
"
32476,Unexpected output shape on custom keras dynamic layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.6
- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0rc0
- TensorFlow version (use command below): 2.0.0-rc0
- Python version: 3.7.4

**Describe the current behavior**

Upon attempting to create a custom dynamic keras layer, keras seems to incorrectly interpret the output of `compute_output_shape`.

**Describe the expected behavior**

In the example code below, `model.summary()` outputs `[(None, (2,))]` for the output shape. According to the docs/examples, I would expect that to be `[(None, 2)]`. When attempting to place layers after this, it returns two placeholders, despite the output shape only defining one.

**Code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

class Example(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        kwargs[""dynamic""] = True
        super(Example, self).__init__(**kwargs)

    def call(self, inputs):
        return inputs

    def compute_output_shape(self, input_shape):
        return [(None, 2)]

inp = tf.keras.layers.Input(batch_shape=(None, 1))
comp = Example()(inp)

model = tf.keras.models.Model(inputs=[inp], outputs=[comp])
model.summary()
```
In my code, the input layer's `batch_shape` and the content of `call` are arbitrary. If I remove `dynamic=True`, then it gives the expected shape based on the contents of `call`. 

There seems to be no semantic difference in output if `compute_output_shapes` returns `[(None, 2)]`, `(None, 2)`, or `[None, 2]`

**Other info / logs**

Here's what I am seeing from model.summary()
```
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 1)]               0
_________________________________________________________________
example (Example)            [(None, (2,))]            0
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________
```
"
32475,fpath = keras_utils.get_file( AttributeError: 'NoneType' object has no attribute 'get_file',"<em>Several Problems with importing ResNeXt101 from keras_applications in TF 2.0</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Following Instructions from Keras.io and this [Issue#54](https://github.com/keras-team/keras-applications/issues/54)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS with Kernel 5.2.10-050210-generic

- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0b1
- Python version: 3.6


**Describe the current behavior**
First Issue was keras_application.resnext.py in `def preprocess_input(x, **kwargs):` was set hard to `return imagenet_utils.preprocess_input(x, mode='torch'), **kwargs) `
setting mode to 'tf' caused an error. 

Main Issue now is that in keras_applications.imagenet_utils.py on line 223 `fpath= keras_utils.get_file` collides with namespace of TF2, which is tf.keras.utils.

Error is
`    fpath = keras_utils.get_file(
AttributeError: 'NoneType' object has no attribute 'get_file'`


**Code to reproduce the issue**
import tensorflow as tf
import numpy as np
from keras_applications.resnext import ResNeXt101
from keras_applications.resnext import preprocess_input, decode_predictions
from keras_preprocessing import image


model = ResNeXt101(weights='imagenet',
                   backend=tf.keras.backend,
                   layers=tf.keras.layers,
                   models=tf.keras.models,
                   utils=tf.keras.utils)

img_path ='image.jpg'
img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x, backend=tf.keras.backend, utils=tf.keras.utils)

preds = model.predict(x)
print('Predicted:', decode_predictions(preds, top=3)[0])


**Other info / logs**
`Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""<string>"", line 23, in <module>
  File ""/venv/lib/python3.6/site-packages/keras_applications/imagenet_utils.py"", line 224, in decode_predictions
    fpath = keras_utils.get_file(
AttributeError: 'NoneType' object has no attribute 'get_file'`
"
32470,Custom Optimizer keeps throwing `no attribute _create_slots` error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.0rc1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory: P100

**Describe the current behavior**
When creating a custom optimizer, the optimizer keeps throwing `object has no attribute _create_slots` error.

**Code to reproduce the issue**
```python
!pip install tensorflow-gpu==2.0.0-rc1
import math
import numpy as np
import tensorflow as tf
from tensorflow.keras import backend as K

class CustomOptimizer(tf.keras.optimizers.Optimizer):
  def __init__(self, 
               optimizer, 
               cool_period = 10,
               **kwargs):
    super(CustomOptimizer, self).__init__(""CustomOptimizer"", **kwargs)
    self.optimizer = tf.keras.optimizers.get(optimizer)
    
    self.cool_period = K.variable(cool_period,
                                  name = ""cool_period"",
                                  dtype = K.floatx())
    self.cool_period_slot = self.add_slot(self.cool_period, ""cool_period"")
 
 
  def get_updates(self, loss, params):
    if self.optimizer.iterations==self.get_slot(cool_period):
      self.updates = self.optimizer.get_updates(loss, params)


    
  def get_config(self):
    config = {
              'cool_period': K.get_value(self.cool_period),
              'optimizer': tf.keras.optimizers.serialize(self.optimizer),
            }
    base_config = super(CustomOptimizer, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))

x = np.random.rand(100,100)
y = np.random.randint(2, size=(100))
op = CustomOptimizer('adam')
input_layer = tf.keras.layers.Input(shape=x.shape[1:])
fc = tf.keras.layers.Dense(1)(input_layer)

model = tf.keras.models.Model(input_layer, fc)
model.compile(optimizer=op, loss='mse')
model.fit(x, y, epochs=5)

```
"
32468,TF Micro requires CONV_2D version '2' when applying quantization,"- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow version: 1.14, 2.0-rc0, nightly-preview 2.0.0.dev20190911
- Python version: 3.7.4

I am trying to run a simple Keras model with TensorFlow version for microcontrollers (modified micro_vision example with several convolutions, dense layers and batch normalization). While the model works correctly when using floats, the quantized version generated with `tf.lite.Optimize.DEFAULT` outputs wrong values and `'Didn't find op for builtin opcode 'CONV_2D' version '2' Invoke failed.'` error.

After changing `CONV_2D` version with `AddBuiltin(BuiltinOperator_CONV_2D, Register_CONV_2D(), 1, 2)` in `all_ops_resolver.cc` the error disappeared, but now the model outputs NaN instead of normal output and I'm not sure if it actually uses a different micro kernel.

After trying multiple conversion techniques (from .h5, model itself, SavedModel folder etc.) with multiple TF versions and BatchNormalization removal, the situation remains the same. As far as I understand, each way to quantize the model (post-training quantization, quantization aware training) requires this version of `CONV2D`, but it seems to work incorrectly or there is no such version in TF micro.

Is there any way to deploy such a model with quantization?"
32465,[tflite] Build issue: failed by \execroot\org_tensorflow\bin\false in Windows,"**System information**
- Windows 10 x64:
- TensorFlow installed from source:
- TensorFlow version: master
- Python version: 3.7
- Bazel version: 0.29.1

**Describe the problem**
```
ERROR: E:/tools/tensorflow/tensorflow/lite/BUILD:89:1: C++ compilation of rule '//tensorflow/lite:external_cpu_backend_context' failed (Exit -1). Note: Remote connection/protocol failed with: execution failed
Action failed to execute: java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(""C:\users\???\_bazel_???\svyyfdgn\execroot\org_tensorflow\bin\false"" -MD -MF bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/_objs/external_cpu_backend_context/external_cpu_backend_context.pic.d -frandom-seed=bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/_objs/external_cpu_backend_context/external_cpu_backend_context.pic.o -fPIC -iquote . -iquote bazel-out/armeabi-v7a-opt/bin -std=c++14 --std=c++11 -Wall -Wno-comment -Wno-extern-c-compat -c tensorflow/lite/external_cpu_backend_context.cc -o b(...)): The system cannot find the file specified.
 (error: 2)
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`bazel build -c opt --cxxopt=--std=c++11 --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a //tensorflow/lite/experimental/c:libtensorflowlite_c.so`
"
32464,[tflite] Support INT8 quantisation for SPLIT with TFLITE_BUILTINS_INT8 OpsSet,"**System information**
- TensorFlow version (you are using): 1.14 and built from sources, master branch
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

The new TFLiteConverter post-training quantisation flow, as described in https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations, does not support quantisation of SPLIT operation when only integer operations are requested in the output model. When this conversion is forced, the Runtime Error ""Quantization not yet supported for op: SPLIT"" is raised. The following code can be used to recreate the error
```
import tensorflow as tf
import numpy as np

def representative_dataset_gen():
	input = np.array(np.random.random_sample([2,10]), dtype=np.float32)
	for _ in range(10):
		yield [input]

# tf Graph Input 
in_intact = tf.compat.v1.placeholder(""float32"", [2, 10])
out_split = tf.split(in_intact,2,axis=0)

with tf.compat.v1.Session() as sess:
	tf.io.write_graph(tf.compat.v1.get_default_graph(), '.','split.pb', as_text=False)

input_name = [""Placeholder""]
output_name = [""split"", ""split:1""]

tflite_model_name = ""int8_split.tflite""
converter = tf.lite.TFLiteConverter.from_frozen_graph(""split.pb"", input_name, output_name)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.representative_dataset = representative_dataset_gen
tflite_model = converter.convert()
open(tflite_model_name, ""wb"").write(tflite_model)
```

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Anyone who attempts full 8-bit fixed point quantisation of models containinig SPLIT operation, present in some versions of Deepspeech.

**Any Other info.**
Note that there are separate TFLite implementations, SPLIT and SPLIT_V, for two different behaviors of Tensorflow's tf.split. This request is related only to SPLIT, although after implementing it, SPLIT_V can probably similarly be implemented."
32462,.h5 file to pb,is there any API to convert tf.keras.models.save() .h5  file to pb? 
32461,"TensorShapeBase parameter list in .so differs from header, undefined reference ","**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Mint 18.3 Xfce 64-bit, 4.15.0-45-generic #48~16.04.1-Ubuntu
- TensorFlow installed from: source
- TensorFlow version: r1.10 and r1.12
- Python version: Python 2.7.12
- Bazel version: 0.19.2
- GCC/Compiler version: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
- GPU model and memory: Integrated Intel HD inside a virtual machine

I have compiled the tensorflow C++ api libraries from source for versions r1.10 and r1.12 on my Linux machine, thus I get a libtensorflow_cc.so (or libtensorflow.so) and libtensorflow_framework.so. I simply used `bazel build --config=opt //tensorflow:libtensorflow_cc.so` to do this.

The problem arises when I try to include the libraries in a project of mine. When compiling I get the following linker errors:
`mydir/myfile.o: In function 'tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(std::initializer_list<long long>)':
~/myproject/include/tensorflow/tensorflow/core/framework/tensor_shape.h:172: undefined reference to 'tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(tensorflow::gtl::ArraySlice<long long>)'
~/myproject/include/tensorflow/tensorflow/core/framework/tensor_shape.h:172: undefined reference to 'tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(tensorflow::gtl::ArraySlice<long long>)'
~/myproject/include/tensorflow/tensorflow/core/framework/tensor_shape.h:172: undefined reference to 'tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(tensorflow::gtl::ArraySlice<long long>)'
collect2: error: ld returned 1 exit status`

And it's true. When I do
`nm -D libtensorflow_cc.so | c++filt | grep TensorShapeBase`
I get
`U tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(absl::Span<long long const>)`
and the same for `libtensorflow_framework.so`:
`000000000049deb0 W tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(absl::Span<long long const>)`.

However, when looking in `tensorflow/core/framework/tensor_shape.h` it does say starting at line 171:
`TensorShapeBase(std::initializer_list<int64> dim_sizes)
: TensorShapeBase(gtl::ArraySlice<int64>(dim_sizes)) {}` 

How can this issue be fixed? Why does it compile the library with different parameters than what is inside the header?

Cheers

btw: I also compiled a 800 MB libtensorflow.so with cmake but it doesn't work due to runtime problems concerning some estimator - from what I can see, though, it does have the correct parameter in the call."
32460,Executor error message in GradientTape.jacobian,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-4.14.141-1-MANJARO-x86_64-with-arch-Manjaro-Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-beta1-5101-gc75bb66 2.0.0-rc0
- Python version: 3.7.4
- Bazel version (if compiling from source): /
- GCC/Compiler version (if compiling from source): /
- CUDA/cuDNN version: /
- GPU model and memory: /

**Describe the current behavior**
I am calculating the Jacobian of a model with respect to its parameters using a gradient tape. The method `GradientTape.jacobian` outputs an error message about a missing function library. The program does not abort however and the computed Jacobian is correct.

**Describe the expected behavior**
There should be no error message.

**Code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(2, input_shape=(2,)),
])
inputs = tf.Variable([[1, 2]], dtype=tf.float32)

with tf.GradientTape() as gtape:
    outputs = model(inputs)
gtape.jacobian(outputs, model.trainable_variables)
```

**Other info / logs**
The full output of the program shown above is:
```
2019-09-12 11:01:04.479226: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-12 11:01:04.498159: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304500000 Hz
2019-09-12 11:01:04.498584: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556260300c60 executing computations on platform Host. Devices:
2019-09-12 11:01:04.498609: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-09-12 11:01:04.630899: E tensorflow/core/common_runtime/executor.cc:642] Executor failed to create kernel. Internal: No function library
	 [[{{node loop_body/MatMul_1/pfor/cond}}]]
```
With additional layers, more error messages like the one quoted above appear, one per matrix multiplication.
The problem persists when the Jacobian calculation is moved into a `tf.function` decorated function."
32459,TFRecordWriter doesn't work from TFRecordsDataset,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): barely
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 1.14
- Python version: 3.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10
- GPU model and memory: quadro 16gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Would like to write `TFRecordsDataset` to files using [`TFRecordWriter`](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/data/experimental/ops/writers.py#L30-L61), however it complains that it does not match the right format... which is odd as given the [tutorial](https://www.tensorflow.org/tutorials/load_data/tf_records) one would think this is possible


**Describe the expected behavior**

It ""just works""


**Code to reproduce the issue**

```python
files = ['file_with_n.tfrecords', 'aother_file_with.tfrecords', ...]
n_records = # 

ds = tf.data.TFRecordDataset(files)
# pre-shuffle all data and then use smaller shuffle buffer size and shuffle files during training
ds = ds.shuffle(n_records)
ds = ds.batch(records_per_file)
for i, batch in enumerate(ds):
    o_file = 'part_{}.tfrecord'.format(i)   
    batch_ds = tf.data.Dataset.from_tensors(batch) # <--- batch is back as a DataSet
    batch_ds = batch_ds.batch(1).map(lambda e: tf.reshape(e,(-1,)))
    writer = tf.data.experimental.TFRecordWriter(o_file)
    writer.write(batch_ds)
```
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32458,how to open tensorcore in TFS1.13,"I have a language model, which has some layers as ""embedding+lstm+2dnn"", and I want to test the fp16 performance on TFS. 

By the way,I have tested that it can speed up 2times In pytorch on 2080Ti.

But when I do test on 2080Ti in TFS1.13GPU(cuda version=10.0),  fp16 is slower than float32 when batch is small, the test result as followings:

batch | 1080Ti(tfsv1.5) | 2080Ti(tfsv1.5) | 2080Ti(tfsv1.13)
    -  |float32 | fp16 | float32 | fp16 | float32 | fp16
50   | 31ms | 56ms | 48ms | 60ms | 51ms | 60ms
500 | 48ms | 42ms | 73ms | 62ms | 81ms | 61ms
1000 | 71ms | 42ms | 129ms | 60ms | 123ms | 58ms
1500 | 100ms | 47ms | 179ms | 76ms | 175ms | 60ms

I have checked that the CUDNN math type is CUDNN_TENSOR_OP_MATH, but I donot know if it has opened tensorcores or how to open it?

thanks a lot for any suggestions!"
32457,[TF 2.0] Feature request: let autograph accept collection inputs,"
**System information**
- TensorFlow version (you are using): 2.0.0-dev20190901
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

Currently it seems to me that there is no way to convert a function to graph if the input of the function is a variant length list of elements (except the simplest case when the elements themselves are all tensors with the same shape, for which we can use a TensorArray). 

Assume we have a function that runs well in graph mode for a certain type of input, say a tensor tuple containing 2 tensors with arbitrary shape:

```
@tf.function(input_signature=[(tf.TensorSpec(None), tf.TensorSpec(None))])
def do_something(input):
    ....
    return some_result
```

We want to have another function that takes undetermined number of `input`s and merge the result, so we have:

```
def merge(list_of_inputs):
    return tf.reduce_sum(tf.stack([do_something(input) for input in list_of_inputs]))
```

This function works well in eager mode, but we cannot add a `@tf.function` decorator to it, because `list_of_inputs` has no fixed length and the function will retrace for every calls, which is very slow. Even if we can pad the `list_of_inputs` to have the same length, it's still not working as the shapes of the elements might vary across different runs, so retracing will still be triggered.

So it would be great if we have something like `tf.Array`and `tf.ArraySpec`, which is defined as an array of objects that share the same specs. Then we can give the `merge` function a decorator like:

```
@tf.function(input_signature=[
    tf.ArraySpec((tf.TensorSpec(None), tf.TensorSpec(None)))])
def merge(list_of_inputs):
    return tf.reduce_sum(tf.stack([do_something(input) for input in list_of_inputs]))
```
 and run it without retracing.

**Will this change the current api? How?**

As described above, we propose to add `tf.Array`and `tf.ArraySpec`.

**Who will benefit with this feature?**

Anyone uses autograph will benefit. It would be much easier to write code that handles the case when a data example contains a set/list of variant number of objects, which is very common in NLP / vision / knowledge graph and so on.
"
32454,Calling tf.function from tf.py_function in dataset.map hangs.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS or Windows.
- TensorFlow installed from (source or binary): binary.
- TensorFlow version (use command below):2.0.0b1/rc0/rc1
- Python version: 3.6.
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla K80

**Describe the current behavior**
Calling tf.function from tf.py_function in dataset.map hangs the program.
By removing tf.function decorator or enable run_functions_eagerly, program runs as expected.

**Describe the expected behavior**
Calling tf.function from tf.py_function does not hang the program.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf

@tf.function
def generate_feature(key):
    if key > tf.constant(-1):
        x = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)
        y = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)
    else:
        x = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)
        y = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)
    x = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(x)
    y = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(y)
    return tf.stack([x, y])

def generate_feature_and_label(key):
    feature = generate_feature(key)
    if key > -1:
        label = tf.constant(1)
    else:
        label = tf.constant(0)
    return feature, label

def dataset_map(key):
    feature, label = tf.py_function(func=generate_feature_and_label,
                                    inp=[key],
                                    Tout=[tf.float32, tf.int32])
    feature = tf.ensure_shape(feature, [2])
    label = tf.ensure_shape(label, [])
    return feature, label

if __name__ == '__main__':
    print(tf.__version__)
    # remove tf.function decorator or enable run_functions_eagerly to run successfully

    # tf.config.experimental_run_functions_eagerly(True)
    keys = list(range(-1000, 1000))
    dataset = tf.data.Dataset.from_tensor_slices(keys)
    dataset = dataset.repeat()
    dataset = dataset.shuffle(buffer_size=len(keys))
    dataset = dataset.map(map_func=dataset_map,
                          num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.batch(4)

    it = iter(dataset)
    while True:
        features, labels = next(it)
        print(f'labels={labels}, features={features}')
        input('press any key to continue...\r\n')
```
"
32453,Add tfrecord support to saved_model_cli,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
``` bash
saved_model_cli run --inputs=""x=/tmp/example.[npy,npz]"" --dir=<model-dir> --tag_set=serve --signature_def=serving_default
```
saved_model_cli does not support tfrecord as inputs 
**Will this change the current api? How?**
``` bash
saved_model_cli run --inputs=/tmp/example.tfrecord --dir=<model-dir> --tag_set=serve --signature_def=serving_default
```
**Who will benefit with this feature?**
Developers always use tfrecord to warm up serving, it will be more convenient for them to use tfrecord as saved_model_cli run inputs.
**Any Other info.**
"
32452,Unimplemented: this graph contains an operator of type Abs for which the quantized form is not yet implemented,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No. Got Insightface trained model directly from https://drive.google.com/open?id=1Iw2Ckz_BnHZUi78USlaFreZXylJj7hnP
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.12.1-9365-gff401a6 1.15.0-dev20190821
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: Not installed
- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 8.1.0-5ubuntu1~16.04) 8.1.0
- **CUDA/cuDNN version**: CPU only
- **GPU model and memory**: CPU only
- **Exact command to reproduce**: tflite_convert \
--output_file=insightface_quant.tflite \
--graph_def_file=insightface.pb \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_dev_values=128 \
--input_arrays=img_inputs \
--output_arrays=resnet_v1_50/E_BN2/Identity \
--default_range_min=0 \
--default_range_max=127


### Describe the problem
I am trying to convert insightface pre-trained model so that I can run it on EdgeTPU. First, I am trying to convert the .pb model into tflite model with both weights and activations quantized to 8bit integer. That's when I get the error below. Is the support for operator Abs coming soon?
2019-09-11 16:41:12.872535: W tensorflow/lite/toco/graph_transformations/quantize.cc:139] Constant array resnet_v1_50/conv1/W_conv2d lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.
2019-09-11 16:41:12.872690: F ./tensorflow/lite/toco/toco_tooling.h:38] Check failed: s.ok() Unimplemented: this graph contains an operator of type Abs for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
Fatal Python error: Aborted


### Source code / logs
WARNING: Logging before flag parsing goes to stderr.
W0911 16:39:50.219565 140227897026304 __init__.py:690] 

  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.

  Please upgrade your code to TensorFlow 2.0:
    * https://www.tensorflow.org/beta/guide/migration_guide

  Or install the latest stable TensorFlow 1.X release:
    * `pip install -U ""tensorflow==1.*""`

  Otherwise your code may be broken by the change.

  
2019-09-11 16:39:50.225242: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-11 16:39:50.246574: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3504000000 Hz
2019-09-11 16:39:50.246796: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x490a580 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-09-11 16:39:50.246817: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Traceback (most recent call last):
  File ""/home/crossbar/.local/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 515, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/crossbar/.local/lib/python3.5/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/crossbar/.local/lib/python3.5/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 511, in run_main
    _convert_tf1_model(tflite_flags)
  File ""/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/tflite_convert.py"", line 199, in _convert_tf1_model
    output_data = converter.convert()
  File ""/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py"", line 983, in convert
    **converter_kwargs)
  File ""/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/convert.py"", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/convert.py"", line 200, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
WARNING: Logging before flag parsing goes to stderr.
W0911 16:39:52.657464 140689256613632 __init__.py:690] 

  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.

  Please upgrade your code to TensorFlow 2.0:
    * https://www.tensorflow.org/beta/guide/migration_guide

  Or install the latest stable TensorFlow 1.X release:
    * `pip install -U ""tensorflow==1.*""`

  Otherwise your code may be broken by the change.

  
2019-09-11 16:39:53.139500: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1363 operators, 1866 arrays (0 quantized)
2019-09-11 16:39:53.171158: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1363 operators, 1866 arrays (0 quantized)
2019-09-11 16:39:53.395100: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting ""resnet_v1_50/E_Dropout/random_uniform/RandomUniform"" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set ""seed"" or ""seed2"" attr non-zero to fix this
2019-09-11 16:39:53.398795: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 292 operators, 508 arrays (1 quantized)
2019-09-11 16:39:53.536832: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting ""resnet_v1_50/E_Dropout/random_uniform/RandomUniform"" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set ""seed"" or ""seed2"" attr non-zero to fix this
2019-09-11 16:39:53.545258: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 291 operators, 507 arrays (1 quantized)
2019-09-11 16:39:53.549759: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting ""resnet_v1_50/E_Dropout/random_uniform/RandomUniform"" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set ""seed"" or ""seed2"" attr non-zero to fix this
2019-09-11 16:39:53.561449: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 288 operators, 501 arrays (1 quantized)
2019-09-11 16:39:53.562454: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting ""resnet_v1_50/E_Dropout/random_uniform/RandomUniform"" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set ""seed"" or ""seed2"" attr non-zero to fix this
2019-09-11 16:39:53.567925: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 288 operators, 501 arrays (1 quantized)
2019-09-11 16:39:53.570066: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 288 operators, 501 arrays (1 quantized)
2019-09-11 16:39:53.573091: F tensorflow/lite/toco/tooling_util.cc:1728] Array resnet_v1_50/bn0/batchnorm/add_1, which is an input to the Abs operator producing the output array resnet_v1_50/prelu0/Abs, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Fatal Python error: Aborted

Current thread 0x00007ff4c52a0700 (most recent call first):
  File ""/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52 in execute
  File ""/home/crossbar/.local/lib/python3.5/site-packages/absl/app.py"", line 251 in _run_main
  File ""/home/crossbar/.local/lib/python3.5/site-packages/absl/app.py"", line 300 in run
  File ""/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89 in main
  File ""/home/crossbar/.local/bin/toco_from_protos"", line 10 in <module>
Aborted (core dumped)


"
32450,node softmax_cross_entropy_with_logits,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab

- TensorFlow version:1.14.0
- Python version:3.6.8


**Describe the problem**

** I have created custom model for cifar 10 dataset. I m unable to use tf.nn.softmax_cross_entropy_with_logits_v2 gets   Invalid argument: logits and labels must be broadcastable: logits_size=[128,10] labels_size=[1,128]

The entire source code can be viewed at 
https://colab.research.google.com/drive/11tyipxzE9qkmlwkiPhs-A8I2OvT5Jebi"
32449,C API: Unexpected behaviour when multithreading calls of c_api's TF_SessionRun,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): [Official download URL from TF's website. ](https://www.tensorflow.org/install/lang_c),Which is then downloaded via nuget package.
- TensorFlow version (use command below): Unable, we use c_api, the dll is downloaded from [here](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-1.14.0.zip)
- Python version: -
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: We use CPU
- GPU model and memory: -

**Describe the current behavior**
I'm working with our team on [Tensorflow.NET](https://github.com/SciSharp/TensorFlow.NET/) and we wanted to exploit C#'s multithreading capabilities with Tensorflow. 
Our library is a port of close to 1-1 from python to C# with couple of changes to allow multithreading to work.
We use a different Session and Graph for every Thread so every thread is actually isolated from the other Threads.
When calling `c_api.TF_SessionRun` from 10 parallel threads in an infinite loop for a minute or so - some sort of corruption occurs that causes Tensorflow to terminate with the following error code 

`Code -1073740791 (0xc0000409).` which is:
> Stack buffer overflow / overrun. Error can indicate a bug in the executed software that causes > stack overflow, leading to abnormal termination of the software.

This does not occur every run, in-fact it might occur 1/10 and only after 1-5 minutes of running.

In most cases, the program will just terminate but even more rarely the following message is printed:
> 2019-09-12 02:47:15.298642: F tensorflow/core/framework/tensor.cc:928] Check failed: buf_ null buf_ with non-zero shape size 15

We discussed the issue in our repository: https://github.com/SciSharp/TensorFlow.NET/issues/380

**Describe the expected behavior**
Run smoothly, `c_api.TF_SessionRun` expected to be thread-safe when used with a separate Session and Graph for every thread.

**Code to reproduce the issue**
https://github.com/Nucs/TensorFlowNetMultithreading
Requirements: Visual Studio 2017+
Happens both in Debug or Release builds.

The tensorflow.dll we use is not built on Debug, in order to use a custom tensorflow.dll with a .pdb, replace the one in `CalcEventsTFS\bin\Debug\net461 or netcoreapp2.2\` and then run `CalcEventsTFS.exe`

Simply build the solution and run the only project there is. 
I recommend to restart the program if it did not occur within 1 minute.

If you are getting `Unable to load DLL 'tensorflow': The specified module could not be found.`, 
Copy your own tensorflow.dll of version 1.40 CPU for windows to the output directory: 
> CalcEventsTFS\bin\Debug\net461 or netcoreapp2.2\

**Other info / logs**
We managed to get a dump after the termination of the program available from [here]( https://mega.nz/#!VVF3HKrS!r5sxOBsbccdVn93KpwS7EtxWSvtLlim2GwExDF8L9h4).

![image](https://user-images.githubusercontent.com/649919/64742960-c1d51c80-d506-11e9-9f2d-c4f7de2234b8.png)

"
32448,AutoGraph could not transform pfor when using GradientTape.jacobian,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04.6 LTS**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (use command below): **v2.0.0-rc0-0-gc75bb66**
- Python version: **3.7**
- Bazel version (if compiling from source): **0.25.2**
- GCC/Compiler version (if compiling from source): **5.4.0 20160609**
- CUDA/cuDNN version: **No**
- GPU model and memory: **No**

**Describe the current behavior**
`GradientTape.jacobian` is showing warnings about `WARNING:tensorflow:Entity <function pfor.<locals>.f at 0x7f5e7807af28> could not be transformed and will be executed as-is. `

**Describe the expected behavior**
No warnings shown.

**Code to reproduce the issue**

```python
import tensorflow as tf

W = tf.random.normal((6,1), dtype=tf.float32)
with tf.GradientTape() as tape:
    y = tf.reduce_sum(W ** 3)
tape.jacobian(y, W)
```
**Other info / logs**

logs by setting `AUTOGRAPH_VERBOSITY=10`

```
INFO:tensorflow:Converted call: <function pfor.<locals>.f at 0x7f5e7807af28>
    args: ()
    kwargs: {}

Converted call: <function pfor.<locals>.f at 0x7f5e7807af28>
    args: ()
    kwargs: {}

INFO:tensorflow:Entity <function pfor.<locals>.f at 0x7f5e7807af28> is not cached for key <code object f at 0x7f5deb7a2390, file ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py"", line 183> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f5dd8004eb8>, frozenset({'loop_fn', 'parallel_iterations', 'iters'}))
Entity <function pfor.<locals>.f at 0x7f5e7807af28> is not cached for key <code object f at 0x7f5deb7a2390, file ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py"", line 183> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f5dd8004eb8>, frozenset({'loop_fn', 'parallel_iterations', 'iters'}))
INFO:tensorflow:Converting <function pfor.<locals>.f at 0x7f5e7807af28>
Converting <function pfor.<locals>.f at 0x7f5e7807af28>
INFO:tensorflow:Source code of <function pfor.<locals>.f at 0x7f5e7807af28>:

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
def f():
  return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)


Source code of <function pfor.<locals>.f at 0x7f5e7807af28>:

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
def f():
  return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)


INFO:tensorflow:Error transforming entity <function pfor.<locals>.f at 0x7f5e7807af28>
Traceback (most recent call last):
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 506, in converted_call
    converted_f = conversion.convert(target_entity, program_ctx)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 322, in convert
    free_nonglobal_var_names)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 240, in _convert_with_cache
    entity, program_ctx)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 469, in convert_entity_to_ast
    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 669, in convert_func_to_ast
    node = node_to_graph(node, context)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 699, in node_to_graph
    node = converter.apply_(node, context, function_scopes)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py"", line 409, in apply_
    node = converter_module.transform(node, context)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py"", line 120, in transform
    return FunctionBodyTransformer(ctx).visit(node)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py"", line 346, in visit
    return super(Base, self).visit(node)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/transformer.py"", line 480, in visit
    result = super(Base, self).visit(node)
  File ""/home/youngw/.pyenv/versions/3.7.2/lib/python3.7/ast.py"", line 262, in visit
    return visitor(node)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py"", line 87, in visit_FunctionDef
    node = self.generic_visit(node)
  File ""/home/youngw/.pyenv/versions/3.7.2/lib/python3.7/ast.py"", line 317, in generic_visit
    value = self.visit(value)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py"", line 346, in visit
    return super(Base, self).visit(node)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/transformer.py"", line 480, in visit
    result = super(Base, self).visit(node)
  File ""/home/youngw/.pyenv/versions/3.7.2/lib/python3.7/ast.py"", line 262, in visit
    return visitor(node)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py"", line 44, in visit_Return
    value=node.value)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/templates.py"", line 261, in replace
    replacements[k] = _convert_to_ast(replacements[k])
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/templates.py"", line 223, in _convert_to_ast
    return gast.Name(id=n, ctx=None, annotation=None)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/gast/gast.py"", line 19, in create_node
    format(Name, nbparam, len(Fields))
AssertionError: Bad argument number for Name: 3, expecting 4
Error transforming entity <function pfor.<locals>.f at 0x7f5e7807af28>
WARNING:tensorflow:Entity <function pfor.<locals>.f at 0x7f5e7807af28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4
WARNING: Entity <function pfor.<locals>.f at 0x7f5e7807af28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4
Traceback (most recent call last):
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 506, in converted_call
    converted_f = conversion.convert(target_entity, program_ctx)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 322, in convert
    free_nonglobal_var_names)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 240, in _convert_with_cache
    entity, program_ctx)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 469, in convert_entity_to_ast
    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 669, in convert_func_to_ast
    node = node_to_graph(node, context)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 699, in node_to_graph
    node = converter.apply_(node, context, function_scopes)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py"", line 409, in apply_
    node = converter_module.transform(node, context)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py"", line 120, in transform
    return FunctionBodyTransformer(ctx).visit(node)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py"", line 346, in visit
    return super(Base, self).visit(node)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/transformer.py"", line 480, in visit
    result = super(Base, self).visit(node)
  File ""/home/youngw/.pyenv/versions/3.7.2/lib/python3.7/ast.py"", line 262, in visit
    return visitor(node)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py"", line 87, in visit_FunctionDef
    node = self.generic_visit(node)
  File ""/home/youngw/.pyenv/versions/3.7.2/lib/python3.7/ast.py"", line 317, in generic_visit
    value = self.visit(value)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py"", line 346, in visit
    return super(Base, self).visit(node)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/transformer.py"", line 480, in visit
    result = super(Base, self).visit(node)
  File ""/home/youngw/.pyenv/versions/3.7.2/lib/python3.7/ast.py"", line 262, in visit
    return visitor(node)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py"", line 44, in visit_Return
    value=node.value)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/templates.py"", line 261, in replace
    replacements[k] = _convert_to_ast(replacements[k])
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/templates.py"", line 223, in _convert_to_ast
    return gast.Name(id=n, ctx=None, annotation=None)
  File ""/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/gast/gast.py"", line 19, in create_node
    format(Name, nbparam, len(Fields))
AssertionError: Bad argument number for Name: 3, expecting 4
```"
32447,How to use Install TensorFlow for C,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): https://tensorflow.google.cn/install/lang_c
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Use Visual Studio 2017 Import TensorFlow C library Windows CPU only.  run code Example program , tensorflow.dll : fatal error LNK1107: 0x358
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32446,Bug in NonMaxSuppressionV3 GPU op added by PR #30893,It broke a test with some large network at head. I'll provide a repro later.
32437,Cannot cross-compile ARM64 on Mac host,"Following https://www.tensorflow.org/lite/guide/build_arm64 is not possible on Mac, `./tensorflow/lite/tools/make/build_aarch64_lib.sh` returns

```
...
/bin/bash: aarch64-linux-gnu-g++: command not found
...
```"
32434,Int32 overflow in sparse_reshape on Windows,"**System information**
- OS Platform and Distribution: Windows 10 64-bit
- TensorFlow installed from: Source
- TensorFlow version: 1.14.0
- Python version: 3.6.7

**Describe the current behavior**
When a large sparse tensor is reshaped using `sparse_reshape` on Windows it fails, due to the fact that it uses `np.prod` to determine whether the number of elements in the old and new SparseTensor are the same. This can fail on Windows, due to the fact that numpy converts python ints to dtype np.int32 on Windows, which will overflow for large dimensions.

**Describe the expected behavior**
`sparse_reshape` should work similarly on both Windows and Unix-based systems.

**Code to reproduce the issue**
```python
import tensorflow as tf
tf.enable_eager_execution()

idxs = tf.constant([[1, 0, 1],
                             [1, 4, 2]], dtype=tf.int64)
values = tf.constant([12, 42])

A = tf.sparse.SparseTensor(idxs, values, (8, 42000, 40000))
tf.sparse.reshape(A, (8, 40000, 42000))
```

Output:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\thomasoerkild\Anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\ops\sparse_ops.py"", line 769, in sparse_reshape
    reshaped_size))
ValueError: Cannot reshape a tensor with 555098112 elements to shape [8, 40000, 42000] (13440000000 elements).
```

**Other info / logs**
The issue might be resolved, by forcing the shape inputted to `np.prod` to be of type np.int64.
"
32429,TensorBoard: gracefully handle deleted event files,"as far as i can tell this is exactly the same as this issue https://github.com/tensorflow/tensorflow/issues/3267

if i delete files from the logdir while tensorboard is running i get things like

```
E0911 11:27:19.441699 139989077399296 plugin_event_multiplexer.py:226] Unable to reload accumulator 'srresnet_voc_2x': [Errno 2] No such file or directory: b'/home/maksim/data/tensorboard/srresnet_voc_2x/events.out.tfevents.1568215513.maksim-desktop.105092.0'
E0911 11:27:24.447706 139989077399296 plugin_event_multiplexer.py:226] Unable to reload accumulator 'srresnet_voc_2x': [Errno 2] No such file or directory: b'/home/maksim/data/tensorboard/srresnet_voc_2x/events.out.tfevents.1568215513.maksim-desktop.105092.0'
E0911 11:27:29.453577 139989077399296 plugin_event_multiplexer.py:226] Unable to reload accumulator 'srresnet_voc_2x': [Errno 2] No such file or directory: b'/home/maksim/data/tensorboard/srresnet_voc_2x/events.out.tfevents.1568215513.maksim-desktop.105092.0'
E0911 11:27:34.459517 139989077399296 plugin_event_multiplexer.py:226] Unable to reload accumulator 'srresnet_voc_2x': [Errno 2] No such file or directory: b'/home/maksim/data/tensorboard/srresnet_voc_2x/events.out.tfevents.1568215513.maksim-desktop.105092.0'
```

i'm using `tb-nightly==1.15.0a20190911` through pytorch.

i'm not sure when reaping is supposed to happen e.g. as in https://github.com/tensorflow/tensorflow/issues/3267#issuecomment-292911229
or how to manually force

```
WARNING:tensorflow:Deleting accumulator 'run1/test'
WARNING:tensorflow:Deleting accumulator 'run1'
WARNING:tensorflow:Deleting accumulator 'run2/test'
WARNING:tensorflow:Deleting accumulator 'run2'
```"
32428,How to convert speech_recognition_frozen_graph.pb to FP32 or FP16?,"How to convert speech_recognition_frozen_graph.pb to FP32 or FP16?
Can you list convert command line for Intel openvino?

"
32427,Exception in tflite with missing operators that model uses,"**System information**
- OS Platform and Distribution : Ubuntu 18.04.2 LTS
- TensorFlow installed from binary
- TensorFlow version tf-1.0.0 (or github SHA if from source):

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CAST, CONCATENATION, CONV_2D, EXPAND_DIMS, FULLY_CONNECTED, MAX_POOL_2D, RESIZE_BILINEAR, SOFTMAX. Here is a list of operators for which you will need custom implementations: DecodeJpeg.

link to a GraphDef or the model.
https://github.com/RaghavPrabhu/Deep-Learning/tree/master/dogs_breed_classification
"
32426,Diagnose,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32425,undefined reference to `tensorflow::str_util::EndsWith,"On Kubuntu 18.08 
TensorFlow 1.13 installed from source.
Python 3.6
Bazel 19.2
GCC 7.4.0
 
In the past, i have a tested program in ubuntu 16.04 and it run fine. I make the migration to ubuntu 18.04, recompile tensorflow for c++ and obtain the libraries libtensorflow_cc.so and libtensorflow_framework.so.
When i tried to compile my program using ""cmake .. && make""  see the following messages:

tensorflowlabelimageclassification.cpp:(.text+0x2bff): undefined reference to `tensorflow::str_util::EndsWith(std::basic_string_view<char, std::char_traits<char> >, std::basic_string_view<char, std::char_traits<char> >)'
tensorflowlabelimageclassification.cpp:(.text+0x2d3c): undefined reference to `tensorflow::str_util::EndsWith(std::basic_string_view<char, std::char_traits<char> >, std::basic_string_view<char, std::char_traits<char> >)'
tensorflowlabelimageclassification.cpp:(.text+0x2ee3): undefined reference to `tensorflow::str_util::EndsWith(std::basic_string_view<char, std::char_traits<char> >, std::basic_string_view<char, std::char_traits<char> >)'

Any help?
Thanks.
Dibet


**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32424,"No rule to make target '/home/ai002/tensorflow_Cpp/add0/tensorflow_cpp2/tensorflow-1.11.0/tensorflow/contrib/makefile/gen/proto/tensorflow/core/util/test_log.pb.cc', needed by '/home/ai002/tensorflow_Cpp/add0/tensorflow_cpp2/tensorflow-1.11.0/tensorflow/contrib/makefile/gen/obj/tensorflow/contrib/boosted_trees/proto/learner.pb.o'.  Stop.","tensorflow 1.11.0
bazel 0.18.0
When i run sudo ./tensorflow/contrib/makefile/build_all_linux.sh...... help me ! thank you hardly!!!
No rule to make target '/home/ai002/tensorflow_Cpp/add0/tensorflow_cpp2/tensorflow-1.11.0/tensorflow/contrib/makefile/gen/proto/tensorflow/core/util/test_log.pb.cc', needed by '/home/ai002/tensorflow_Cpp/add0/tensorflow_cpp2/tensorflow-1.11.0/tensorflow/contrib/makefile/gen/obj/tensorflow/contrib/boosted_trees/proto/learner.pb.o'.  Stop."
32422,[lite/micro] quantized dense layer is not supported.,"The following error is thrown while running a quantized tf lite model converted from a tensorflow model using tf lite **micro**.

```
Quantized FullyConnected expects output data type uint8 or int16
Node FULLY_CONNECTED (number 1) failed to invoke with status 1
```

# Environment information
- The master branch is used to compile lite/micro.
- OS: Ubuntu 16.04
- Gcc version: 5.4.0

# tensorflow version
1.13.1

The script to convert the tensorflow model to a tflite model is as follows:

```python
#!/usr/bin/env python

from __future__ import print_function, absolute_import, division

import argparse
import os
import warnings

import numpy as np

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.simplefilter(action='ignore', category=FutureWarning)

import tensorflow as tf
tf.logging.set_verbosity(tf.logging.ERROR)


def test():
    # first build a graph
    g = tf.Graph()

    with g.as_default():
        in_shape = [1, 10, 9, 3]
        x = tf.placeholder(tf.float32, shape=in_shape, name=""input"")

        w = np.random.rand(3, 3)
        b = np.random.rand(9)

        layer = tf.layers.conv2d(inputs=x,
                                 filters=9,
                                 kernel_size=w.shape,
                                 name=""layer1"",
                                 kernel_initializer=tf.constant_initializer(w),
                                 bias_initializer=tf.constant_initializer(b))
        relu = tf.nn.relu(layer)

        relu = tf.reshape(relu, (1, -1))

        w2 = np.random.rand(relu.shape[-1])
        b2 = np.random.rand(9)
        y = tf.layers.dense(inputs=relu,
                            units=9,
                            kernel_initializer=tf.constant_initializer(w2),
                            bias_initializer=tf.constant_initializer(b2),
                            name=""output"")

    with tf.Session(graph=g) as sess:
        sess.run(tf.global_variables_initializer())
        converter = tf.lite.TFLiteConverter.from_session(sess,
                                                         input_tensors=[x],
                                                         output_tensors=[y])

        print(""before conversion"")
        in_data = np.arange(270).reshape(x.shape)
        out = sess.run(y, feed_dict={x: in_data})
        print(out)

        converter.post_training_quantize = True  # for 1.13.1

        tflite_model = converter.convert()
        model_filename = ""xxx.tflite""
        open(model_filename, ""wb"").write(tflite_model)

        cc_src = ""xxx.h""
        os.system(""xxd -i {} > {}"".format(model_filename, cc_src))

    # after conversion
    interpreter = tf.lite.Interpreter(model_path=model_filename)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    input_shape = input_details[0]['shape']
    interpreter.set_tensor(input_details[0]['index'],
                           in_data.astype(np.float32))
    interpreter.invoke()
    output_data = interpreter.get_tensor(output_details[0]['index'])
    print(""after conversion"")
    print(output_data)


if __name__ == '__main__':
    print(tf.__version__)
    print(tf.__git_version__)
    print(tf.__compiler_version__)
    np.random.seed(777)
    test()
```

Output of the above script is:

```
1.13.1
b'v1.13.1-0-g6612da8951'
4.8.5
before conversion
[[327913.94 327228.56 327169.47 327523.12 326526.3  327757.97 328415.53
  326741.97 327555.22]]
after conversion
[[327600.28 326899.12 326834.88 327196.44 326194.53 327430.94 328063.88
  326408.12 327227.72]]
```"
32421,Do I have to compile a TF.keras model that's part of a compiled model in order for its added losses to be taken into account during training?,"I have already tried to get an answer [here](https://stackoverflow.com/questions/57851525) to this question because I cannot figure it out based on the documentation:

I have a VAE tf.keras model and it consists of an encoder and decoder model. I want to regularize the encoder and the decoder with different custom terms (using add_loss()) while training VAE (using VAE.fit()).

In pseudocode:

```
encoder = model(input, encoding)
decoder = model(encoding, output)
encoder.add_loss(custom_loss_1)
decoder.add_loss(custom_loss_2)
vae = model(input, output)
vae.compile(optimizer, standard_keras_loss_function)
vae.fit(args)
```

Note that I can't add the custom losses to the wrapper model (vae) because I want them to be backpropagated only in their respective submodels (encoder's loss shouldnt affect decoder's gradients and vice versa). I also can't fit encoder and decoder separately, VAEs benefit from jointly training the encoder and decoder.

I'm facing two challenges:

Although I see no error during training, I have no easy way to heck if encoder and decoder losses are taken into account when fitting bar wrapper model. Do I need to compile encoder and decoder for the added custom losses to be taken into account?

I don't want the 'standard_keras_loss' (binary crossentropy) to be backpropagated beyond the decoder, it should only affect the decoder's gradients. However, vae.compile() method requires me to add at least one loss to the VAE model. Thus, the only way to prevent this loss from backpropagating to encoder is to try and add a stop_gradient lambda layer after the encoder but this would disconnect my encoder from my decoder (in fact I'm getting disconnected graph error wherever I try to add the stop_gradient layer).

Is there a TF.keras way to do this without having to write custom training loop? I'm trying to keep a clean code."
32420,TF2.0 - Multiple calls to Keras .fit and .evaluate makes RAM explode and is 25x slower,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0-dev20190909
- Python version: 3.6.5
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**
(Everything is done on the CPU)
Consecutive calls to either `.fit()` or `.evaluate()` increase the RAM used, even if calling with the same data. The calls takes approximately 10 times longer than with TF1.x

**Describe the expected behavior**
I expect the RAM usage to remain constant just like in TF1.x

**Code to reproduce the issue**

```python
from memory_profiler import profile
from time import time
import numpy as np
import tensorflow as tf

model = tf.keras.Sequential([tf.keras.layers.Dense(100, activation=tf.nn.softmax)])
model.compile(loss='mse', optimizer='sgd')

@profile
def eval(x, y):
    model.evaluate(x, y)

x = np.random.normal(size=(1,100))
y = np.random.normal(size=(1,100))

for i in range(100000):
    print('iteration', i)
    tic = time()
    eval(x, y)
    print('timeit', time() - tic)
```

Using TF2.0. 229MB RAM used and evaluate completed in 99ms
```
iteration 20
Train on 1 samples
1/1 [==============================] - 0s 10ms/sample - loss: 1.0597
Filename: reproduce_keras_oom.py

Line #    Mem usage    Increment   Line Contents
================================================
     9    228.8 MiB    228.8 MiB   @profile
    10                             def eval(x, y):
    11    229.1 MiB      0.2 MiB       model.evaluate(x, y)

timeit 0.09978580474853516
```

Using TF2.0 1508MB RAM used after calling `.evaluate()` 3312 times.
```
iteration 3312
1/1 [==============================] - 0s 4ms/sample - loss: 1.0205
Filename: reproduce_keras_oom.py

Line #    Mem usage    Increment   Line Contents
================================================
     9   1508.3 MiB   1508.3 MiB   @profile
    10                             def eval(x, y):
    11   1508.7 MiB      0.4 MiB       model.evaluate(x, y)

timeit 0.09004998207092285
```

Using TF1.x, the RAM used is not increasing over consecutive calls of `.evaluate()`. RAM stays at 176MB indefinitely (iteration 5100 below). **Also note that it is 25 times faster!**
```
iteration 5100
1/1 [==============================] - 0s 1ms/sample - loss: 1.2716
Filename: reproduce_keras_oom.py

Line #    Mem usage    Increment   Line Contents
================================================
     9    176.0 MiB    176.0 MiB   @profile
    10                             def eval(x, y):
    11    176.0 MiB      0.0 MiB       model.evaluate(x, y)

timeit 0.004405021667480469
```

I just discovered that wrapping `(x, y)` into a `tf.data.Dataset` does not have this issue!
Modified code:
```python
from memory_profiler import profile
from time import time
import numpy as np
import tensorflow as tf

model = tf.keras.Sequential([tf.keras.layers.Dense(100, activation=tf.nn.softmax)])
model.compile(loss='mse', optimizer='sgd')

@profile
def eval(dataset):
    model.evaluate(dataset)

x = np.random.normal(size=(1,100))
y = np.random.normal(size=(1,100))

dataset = tf.data.Dataset.from_tensor_slices((x, y))
dataset = dataset.batch(1)

for i in range(100000):
    print('iteration', i)
    tic = time()
    eval(dataset)
    print('timeit', time() - tic)
```

Using `tf.data.Dataset`, there is no exploding RAM. 217 MB RAM used after calling `.evaluate()` 8154 times. It is also only 2.5 times slower than TF1.x
```
iteration 8154
1/1 [==============================] - 0s 3ms/step - loss: 0.9972
Filename: reproduce_keras_oom.py

Line #    Mem usage    Increment   Line Contents
================================================
     9    217.6 MiB    217.6 MiB   @profile
    10                             def eval(dataset):
    11    217.6 MiB      0.0 MiB       model.evaluate(dataset)

timeit 0.010456085205078125
```

"
32419,Masking in BERT,"In the original paper of BERT it is said:

> Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine-tuning, as the [MASK] symbol never appears during the fine-tuning stage.

Let's consider a sentence ""I am a Liverpool fan"" which with 40% masking will be transformed into ""I [MASK] a [MASK] fan"". When predicting the first [MASK], will it be predicted by a phrase ""I [MASK] a fan"", excluding the second [MASK] or ""I [MASK] a [MASK] fan"", by a full sentence?

And what is the purpose of replacing 10% of masked tokens with themselves? Does it mean they will not be predicted? Or we will predict them, having themselves in the context (like predicting the first [MASK] by ""I am a [MASK] fan""?

Will be very grateful for any help!"
32416,[lite/micro] missing delete() in GreedyMemoryPlanner,"https://github.com/tensorflow/tensorflow/blob/8c0df1fa0b0490d8b1e54d7b019e2b2242ad6718/tensorflow/lite/experimental/micro/memory_planner/greedy_memory_planner.h#L43

does not override `void operator delete(void *p)` which results in link time error."
32415,TensorFlow Lite for micro controllers support BatchNorm?,"I'm looking on this file that should list supported operations for micro controllers:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/kernels/all_ops_resolver.cc

I don't see an explicit support in batch norm layer.

Does TF Lite for micro controllers support batch normalization?
generally, does it support basic architectures like resnet and mobilenet v2?"
32413,How to split neural network into different GPU devices ?,"Hi, I am working on a Video Segmentation project. Due to the limit of GPU memory (GTX 1080TI),  OOM error has occurred. I do know this error can be avoided if I squeeze my model or just use other GPUs with higher memory. But, does Tensorflow support splitting one neural network into different GPU devices? i.e. , I have 2 GPUs(A and B) and the model definitation is shown as following:
```
    input = keras.layers.Input((28, 28))
    f = keras.layers.Flatten()(input)
    #I want GPU A finish this part
    part1 = keras.layers.Dense(128, activation=tf.nn.relu)(f)    
     #After part1 is finished, GPU B will finish this part
    part2 = keras.layers.Dense(10, activation=tf.nn.softmax)(part1)   
```
I want  GPU A finish `part1 = keras.layers.Dense(128, activation=tf.nn.relu)(f)  ` and GPU B finish `part2 = keras.layers.Dense(10, activation=tf.nn.softmax)(part1) `."
32412,[TF 2.0.0-rc0] Keras Model subclassing `dynamic=True` throws Error. Possible Regression bug of rc0,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
    - Mac OSX 10.13.6, intel i7, MacBook Pro (Retina, 13-inch, Early 2015), Intel Iris Graphics 6100 1536 MB, 
    - Google Colab (Both CPU and GPU)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary (pip)
- TensorFlow version (use command below): 2.0.0-rc0
` pip install tensorflow==2.0.0-rc0`
- Python version: 3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA
- GPU model and memory: Intel Iris Graphics 6100 1536 MB (Stock Intel i7 CPU bundled)

**Describe the current behavior**
In 2.0.0-rc0, While using Model Subclassing of tf,keras.Model and passing `dynamic=True`. Then when I call model.fit it throws `AttributeError: 'NoneType' object has no attribute 'dtype'`

**Describe the expected behavior**
model.fit should not throw any error. Note this was working fine in `2.0.0-beta1`

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf
import os

print(tf.__version__)


class ConvBn2D(tf.keras.Model):
    def __init__(self, c_out, kernel_size=3):
        super().__init__()
        self.conv = tf.keras.layers.Conv2D(filters=c_out, kernel_size=kernel_size,
                                           strides=1, padding=""SAME"",
                                           use_bias=False)
        self.bn = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-7)

    def call(self, inputs):
        res = tf.nn.relu(self.bn(self.conv(inputs)))
        return res


class FNet(tf.keras.Model):
    def __init__(self, start_kernels=64, weight=0.125, **kwargs):
        super().__init__(**kwargs)
        c = start_kernels
        self.max_pool = tf.keras.layers.MaxPooling2D()
        self.init_conv_bn = ConvBn2D(c, kernel_size=3)
        self.c0 = ConvBn2D(c, kernel_size=3)

        self.c1 = ConvBn2D(c * 2, kernel_size=3)
        self.c2 = ConvBn2D(c * 2, kernel_size=3)

        self.c3 = ConvBn2D(c * 2, kernel_size=3)
        self.c4 = ConvBn2D(c * 2, kernel_size=3)

        self.pool = tf.keras.layers.GlobalMaxPool2D()
        self.linear = tf.keras.layers.Dense(10, use_bias=False)
        self.weight = weight

    def call(self, x):
        h = self.max_pool(self.c0(self.init_conv_bn(x)))
        h = self.max_pool(self.c2(self.c1(h)))
        h = self.max_pool(self.c4(self.c3(h)))
        h = self.pool(h)
        h = self.linear(h) * self.weight
        return h


(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
train = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train = train.map(lambda x,y:(tf.cast(x,tf.float32),tf.cast(y,tf.int64))).map(lambda x,y:(x/255.0,y)).batch(512)

# model = FNet(start_kernels=8)

model = FNet(start_kernels=8,dynamic=True)

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),
              loss=loss)

callbacks=[]
model.fit(train, epochs=2, callbacks=callbacks,verbose=1)
```


**Other info / logs**
Run code in `2.0.0-beta1` it works. in rc0 fails.

[Colab Notebook with same issue](https://colab.research.google.com/drive/1dZaIwlgKRk_8FSKkdyQlwXsT3auqV6VX)
"
32410,Possible issue in tf.scatter_nd documentation.,"Thank you for explaining about tf.scatter_nd using some wonderful visualizations. I have a doubt whether the cubes used for the higher dimensional explanation of tf.scatter_nd is right or not.
Please check the visualization of the Cube tagged as 'output' in [tf.scatter_nd](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/scatter_nd#) documentation, the second and fourth indices are shaded whereas the given indices according to the example is tf.constant([[0], [2]]) so the first and third indices (0 , 2) should be shaded instead of second and fourth. Please correct me if I'm wrong
"
32409,keras.backend.learning_phase() broken when sample_weights are used,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04

- TensorFlow installed from (source or binary):
binary

- TensorFlow version (use command below):
1.14.0

- Python version:
3.7

**Describe the current behavior**

There are multiple issues:

- *Issue 1* keras.backend.learning_phase() does not always return a scalar value (as stated by the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/backend/learning_phase) but it returns multiple values.
- *Issue 2* Even when though it returns multiple values, the shape says it is a 0d tensor indicating it is a scalar
- *Issue 3* Reduce does not do anything even though the tensor has multiple values.

**Describe the expected behavior**

- *Issue 1* Documentation is fixed to reflect the correct behavior
- *Issue 2* The right shape is set for the tensor
- *Issue 3* `reduce_any` should reduce all values when no axis is given irrespective of the shape.

**Code to reproduce the issue**
```
import os
import numpy as np
import tensorflow as tf
import tensorflow.keras as keras

BATCH_SIZE = 4

class SimpleModel(keras.Model):

  def __init__(self):
    super(SimpleModel, self).__init__()
    self._layer = keras.layers.Dense(1)

  def call(self, inputs):
    learning_phase = keras.backend.learning_phase()
    inputs = tf.Print(inputs, [tf.constant(learning_phase.shape.ndims)],
                      ""Looking at the ndims says this is a salar: "",
                      summarize=BATCH_SIZE)
    inputs = tf.Print(inputs, [learning_phase], ""But this is not a scalar: "",
                      summarize=BATCH_SIZE)
    inputs = tf.Print(inputs,
                      [tf.reduce_any(learning_phase)],
                      ""Even reduce_any does not create a scalar: "", summarize=BATCH_SIZE)
    return self._layer(inputs)


def get_dataset(batch_size=BATCH_SIZE):
  num_batches = 16
  num_features = 5
  inputs = np.random.random((num_batches, num_features))
  labels = np.random.random((num_batches))
  sample_weights = (np.random.random((num_batches)) > 0.5).astype(np.float32)
  dataset = tf.data.Dataset.from_tensor_slices((inputs, labels, sample_weights))
  return dataset.batch(batch_size).take(1)


if __name__ == ""__main__"":
  tf.logging.set_verbosity(tf.logging.ERROR)
  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

  print(""-------------"")
  print(tf.__version__)
  print(""-------------"")

  model = SimpleModel()
  optimizer = tf.keras.optimizers.SGD(learning_rate=0.005)
  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
  model.compile(optimizer, loss)
  model.fit(get_dataset(), verbose=0)
```

**Other info / logs**
```
-------------
1.14.0
-------------
Looking at the ndims says this is a salar: [0]
But this is not a scalar: [0 1 1 0]
Even reduce_any does not create a scalar: [0 1 1 0]
```"
32408,"The name ""resnet50"" is used 2 times in the model.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **tensorflow-gpu 1.14.0**
- Python version: **3.7.3**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 

---

when i try to use more than one `keras.application.ResNet50` in my model, there shows a `ValueError: The name ""resnet50"" is used 2 times in the model. All layer names should be unique.` and i checked tensorflow's source code, the model name is unchangable.

here is my test code:

```python
from tensorflow.compat.v1 import enable_eager_execution
from tensorflow.keras.layers import Concatenate, Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras.applications import ResNet50

import os

enable_eager_execution()
os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""2""

res_net = ResNet50(include_top=False, input_shape=(224, 224, 3))
res_net2 = ResNet50(include_top=False, input_shape=(224, 224, 3))

x = Input(shape=(224, 224, 3), name='input')
y = res_net(x)
z = res_net2(x)
z = Dense(1, name='output')(Concatenate(axis=-1)([y, z]))

model = Model([x], [z])
```

and here is the logs:

```text
Traceback (most recent call last):
  File ""draft.py"", line 19, in <module>
    model = Model([x], [z])
  File ""/home/xiefangyuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 129, in __init__
    super(Model, self).__init__(*args, **kwargs)
  File ""/home/xiefangyuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 162, in __init__
    self._init_graph_network(*args, **kwargs)
  File ""/home/xiefangyuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/xiefangyuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 315, in _init_graph_network
    self.inputs, self.outputs)
  File ""/home/xiefangyuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1862, in _map_graph_network
    str(all_names.count(name)) + ' times in the model. '
ValueError: The name ""resnet50"" is used 2 times in the model. All layer names should be unique.
```

and hee is tensorflow's source code at line 274 in `keras_applications\resnet50.py`:

```pyton
# Create model.
model = models.Model(inputs, x, name='resnet50')
```

i think sometimes we need to change the model's name."
32405,Failed to build on Ubuntu 18.04 with Ryzen CPU,"<em>Error when build from source on Ubuntu 18.04 tag:build_template</em>

**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from source 1.14
- TensorFlow version: 1.14
- Python version: 3.6
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 7.4
- CUDA/cuDNN version: 10.0
- GPU model and memory: RTX 2060
- CPU: Ryzen 5 2600

**Describe the problem**
I follow tutorial https://www.tensorflow.org/install/source, but It always throw error like this
**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazel build --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" -- define=tensorflow_mkldnn_contraction_kernel=0 --verbose_failures  //tensorflow/tools/pip_package:build_pip_package
or 
bazel build --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package
or 
bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package
**Any other info / logs**
ERROR: /home/tuan/Downloads/tensorflow-1.14.0/tensorflow/core/BUILD:3079:1: C++ compilation of rule '//tensorflow/core:core_cpu_base' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/tuan/.cache/bazel/_bazel_tuan/5cac32750bec0be81932199159ef575e/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    PATH=/home/tuan/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=7.5 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
    TF_NEED_TENSORRT=1 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/_objs/core_cpu_base/graph_constructor.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/_objs/core_cpu_base/graph_constructor.pic.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote bazel-out/k8-opt/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote bazel-out/k8-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote bazel-out/k8-opt/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote bazel-out/k8-opt/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote bazel-out/k8-opt/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem bazel-out/k8-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '-D_GLIBCXX_USE_CXX11_ABI=0' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -msse3 -pthread '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -c tensorflow/core/graph/graph_constructor.cc -o bazel-out/k8-opt/bin/tensorflow/core/_objs/core_cpu_base/graph_constructor.pic.o)
Execution platform: @bazel_tools//platforms:host_platform
In file included from external/protobuf_archive/src/google/protobuf/arena_impl.h:40:0,
                 from external/protobuf_archive/src/google/protobuf/arena.h:51,
                 from bazel-out/k8-opt/genfiles/tensorflow/core/framework/graph.pb.h:24,
                 from ./tensorflow/core/graph/graph_constructor.h:19,
                 from tensorflow/core/graph/graph_constructor.cc:16:
external/protobuf_archive/src/google/protobuf/map.h: In instantiation of 'void google::protobuf::Map<Key, T>::InnerMap::iterator_base<KeyValueType>::SearchFrom(google::protobuf::Map<Key, T>::size_type) [with KeyValueType = google::protobuf::Map<unsigned int, std::basic_string<char> >::KeyValuePair; Key = unsigned int; T = std::basic_string<char>; google::protobuf::Map<Key, T>::size_type = long unsigned int]':
external/protobuf_archive/src/google/protobuf/map.h:400:19:   required from 'google::protobuf::Map<Key, T>::InnerMap::iterator_base<KeyValueType>::iterator_base(const google::protobuf::Map<Key, T>::InnerMap*) [with KeyValueType = google::protobuf::Map<unsigned int, std::basic_string<char> >::KeyValuePair; Key = unsigned int; T = std::basic_string<char>]'
external/protobuf_archive/src/google/protobuf/map.h:516:31:   required from 'google::protobuf::Map<Key, T>::InnerMap::iterator google::protobuf::Map<Key, T>::InnerMap::begin() [with Key = unsigned int; T = std::basic_string<char>; google::protobuf::Map<Key, T>::InnerMap::iterator = google::protobuf::Map<unsigned int, std::basic_string<char> >::InnerMap::iterator_base<google::protobuf::Map<unsigned int, std::basic_string<char> >::KeyValuePair>]'
external/protobuf_archive/src/google/protobuf/map.h:1037:29:   required from 'google::protobuf::Map<Key, T>::iterator google::protobuf::Map<Key, T>::begin() [with Key = unsigned int; T = std::basic_string<char>]'
external/protobuf_archive/src/google/protobuf/map.h:1147:29:   required from 'void google::protobuf::Map<Key, T>::clear() [with Key = unsigned int; T = std::basic_string<char>]'
external/protobuf_archive/src/google/protobuf/map_field_inl.h:188:3:   required from 'void google::protobuf::internal::MapField<Derived, Key, T, key_wire_type, value_wire_type, default_enum_value>::Clear() [with Derived = tensorflow::DeviceStepStats_ThreadNamesEntry_DoNotUse; Key = unsigned int; T = std::basic_string<char>; google::protobuf::internal::WireFormatLite::FieldType kKeyFieldType = (google::protobuf::internal::WireFormatLite::FieldType)13; google::protobuf::internal::WireFormatLite::FieldType kValueFieldType = (google::protobuf::internal::WireFormatLite::FieldType)9; int default_enum_value = 0]'
bazel-out/k8-opt/genfiles/tensorflow/core/framework/step_stats.pb.h:2329:23:   required from here
external/protobuf_archive/src/google/protobuf/map.h:423:9: internal compiler error: Segmentation fault
         GOOGLE_DCHECK(m_->index_of_first_non_null_ == m_->num_buckets_ ||
         ^
Please submit a full bug report,
```"
32404,Successfully opened dynamic library libcuda.so.1,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
32401,GPU NMS kernel in TF 1.15rc0 was not fixed,"This commit:
https://github.com/tensorflow/tensorflow/commit/9480262cbbfc2430b0c53424f0fc133418d7ae3f
was included in TF 1.15rc0.

However, this commit has a bug as pointed out in https://github.com/tensorflow/tensorflow/pull/28745#issuecomment-512949342.

The bug is fixed in https://github.com/tensorflow/tensorflow/commit/5d6158e0d4a736a8ad2fc98b717fed519e4080f0, which is not included in TF 1.15rc0.

I expect TF 1.15rc0 to include the bugfix for the GPU version of NMS kernel."
32398,ValueError from invalid weights while loading older .h5 model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.13.1 & v1.12.1-10753-g1c2ae57 2.0.0-dev20190910
- Python version: 3.7.4
- CUDA/cuDNN version: 10.1
- GPU model and memory:

**Describe the current behavior**
I created a tfkeras model and saved it to .h5 format using TF version 1.13.1. The model can be loaded and used for inference just fine in 1.13.1. After upgrading to TF 2.0 (nightly build), loading the model results in a ValueError (see traceback below).
I am using tf.compat.v1.disable_v2_behavior() in case that might make a difference.

**Describe the expected behavior**
V1 models should load correctly, or present the user with a way to migrate the model to a more compatible format.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Relevant traceback info:
```
  File ""*/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py"", line 146, in load_mode
l
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File ""*/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 171, in lo
ad_model_from_hdf5
    load_weights_from_hdf5_group(f['model_weights'], model.layers)
  File ""*/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 697, in lo
ad_weights_from_hdf5_group
    str(len(weight_values)) + ' elements.')
ValueError: Layer #1 (named ""encoder_bn_0"" in the current model) was found to correspond to layer encoder_bn_0 in the save file. However t
he new layer encoder_bn_0 expects 7 weights, but the saved weights have 8 elements.
```

I added some print statements to get info on the weights for this batchnorm (w. renorm) layer. The names for the weights in the original model are:
```
['encoder_bn_0/gamma:0', 'encoder_bn_0/beta:0', 'encoder_bn_0/moving_mean:0', 'encoder_bn_0/moving_variance:0', 'encoder_bn_0/renorm_mean:
0', 'encoder_bn_0/renorm_mean_weight:0', 'encoder_bn_0/renorm_stddev:0', 'encoder_bn_0/renorm_stddev_weight:0']
```
The expected weight placeholders are:
```
[<tf.Variable 'encoder_bn_0/gamma:0' shape=(96,) dtype=float32>, <tf.Variable 'encoder_bn_0/beta:0' shape=(96,) dtype=float32>, <tf.Variab
le 'encoder_bn_0/moving_mean:0' shape=(96,) dtype=float32>, <tf.Variable 'encoder_bn_0/moving_variance:0' shape=(96,) dtype=float32>, <tf.
Variable 'encoder_bn_0/moving_stddev:0' shape=(96,) dtype=float32>, <tf.Variable 'encoder_bn_0/renorm_mean:0' shape=(96,) dtype=float32>,
<tf.Variable 'encoder_bn_0/renorm_stddev:0' shape=(96,) dtype=float32>]
```

I'm guessing the format for saving batchnorm (w.renorm) parameters changed at some point? Is there a way to make this backwards compatible? Or perhaps a way to migrate the save file?"
32396,The SessionOptions doesn't control the usage of CPU or Thread,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, only use C++ API
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): SUSE 12.4
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14.0
- Python version: 3.5
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc 4.8
- CUDA/cuDNN version: only CPU
- GPU model and memory: only CPU

**Describe the current behavior**
I created a session option with inter=1 and intra=1. Then I used this option to create a session and load a model.
When I run the model, the program used almost all the CPU cores on my server, and thread spawned to several hundreds.
And No matter what value I set to ""inter"" and ""intra"", there are still many CPU cores were used and many threads were created.

**Describe the expected behavior**
TF only use 1 thread and 1 cpu core

**Code to reproduce the issue**
  std::unique_ptr<tensorflow::Session> session;
  tensorflow::SessionOptions options;
  tensorflow::ConfigProto & config = options.config;
  int coresToUse = 1;
  if (coresToUse > 0)
    {
		config.set_use_per_session_threads(false);
		config.set_inter_op_parallelism_threads(coresToUse);
                config.set_intra_op_parallelism_threads(coresToUse);	
    }
  status = tensorflow::NewSession(options, session);
tensorflow::MetaGraphDef graph_def;
tensorflow::ReadBinaryProto(tensorflow::Env::Default(), metamodel_path, &graph_def);
session->Create(graph_def.graph_def());
run_model();

**Other info / logs**
When I use the python API of TensorFlow 1.14 do the same thing, it seems like the CPU and thread usage can be controlled by the inter and intra parameter.

How to control the computing resources used by tensorflow C++ API?

#32009 @JiayiFu @ezhulenev I saw the similar problem described here, but don't know its solution."
32395,AssertionError: Unreachable when adding or subtracting Dataset range element and tf.constant,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home Version	10.0.18362 Build 18362
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 1.14
- Python version: 3.6.9

**Describe the current behavior**
I'm trying to calculate the minimum index to take for extracting windows out of the dataset. I've got a constant `window_size` defined of which dtype defaults to `int32`. After running the following code it throws Assertion Error.

**Code to reproduce the issue**

```
import tensorflow as tf

tf.enable_eager_execution()

ds = tf.data.Dataset.range(10)
window_size = tf.constant(3)

def mapper(idx):
    test = idx - window_size
    return test

ds = ds.map(mapper)
```

the same issue happens when I add or multiply (but not divide!).
When dividing with this mapper:
```
def mapper(idx):
    test = idx / window_size
    return test
```
I get `TypeError: x and y must have the same dtype, got tf.int64 != tf.int32`.

Then I've tried casting `window_size` to `tf.int64`: `window_size = tf.constant(3, dtype=tf.int64)` and that fixed the issue in all cases. 

**Describe the expected behavior**
I'd expect the error to be the same in case of addition/subtraction/multiplication as in case of division as it turns out to be the dtype issue.

**Other info / logs**

```
  File ""C:/source/test_python_project/main.py"", line 14, in <module>
    ds = ds.map(mapper)
  File ""C:\ProgramData\Anaconda3\envs\test_python_project\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1772, in map
    MapDataset(self, map_func, preserve_cardinality=False))
  File ""C:\ProgramData\Anaconda3\envs\test_python_project\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3190, in __init__
    use_legacy_function=use_legacy_function)
  File ""C:\ProgramData\Anaconda3\envs\test_python_project\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2555, in __init__
    self._function = wrapper_fn._get_concrete_function_internal()
  File ""C:\ProgramData\Anaconda3\envs\test_python_project\lib\site-packages\tensorflow\python\eager\function.py"", line 1355, in _get_concrete_function_internal
    *args, **kwargs)
  File ""C:\ProgramData\Anaconda3\envs\test_python_project\lib\site-packages\tensorflow\python\eager\function.py"", line 1349, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""C:\ProgramData\Anaconda3\envs\test_python_project\lib\site-packages\tensorflow\python\eager\function.py"", line 1652, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\ProgramData\Anaconda3\envs\test_python_project\lib\site-packages\tensorflow\python\eager\function.py"", line 1545, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""C:\ProgramData\Anaconda3\envs\test_python_project\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 715, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\ProgramData\Anaconda3\envs\test_python_project\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2549, in wrapper_fn
    ret = _wrapper_helper(*args)
  File ""C:\ProgramData\Anaconda3\envs\test_python_project\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2489, in _wrapper_helper
    ret = func(*nested_args)
  File ""C:/source/test_python_project/main.py"", line 10, in mapper
    test = idx - window_size
  File ""C:\ProgramData\Anaconda3\envs\test_python_project\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 884, in binary_op_wrapper
    return func(x, y, name=name)
  File ""C:\ProgramData\Anaconda3\envs\test_python_project\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 11574, in sub
    ""Sub"", x=x, y=y, name=name)
  File ""C:\ProgramData\Anaconda3\envs\test_python_project\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 621, in _apply_op_helper
    assert False, ""Unreachable""
AssertionError: Unreachable
```
"
32394,Memory Leak in tf.keras.Model.fit ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
During training of a keras model with `fit`, the memory consumption is constantly increasing until the machine eventually runs out of memory.
**Note:** The observed issue is only observed with `tensorflow` installed with `conda` or the one used in the AWS Deep Learning AMI that was optimized for the respective compute instance. If I just install `tensorflow` with `pip` the memory consumption does not increase until failure. Since I am not an expert on building `tensorflow` from source I can only guess that it has something to do with the way tensorflow was build from source by the package provider.

**Describe the expected behavior**
For a small training dataset of a couple of MB, a machine with 16 GiB memory should not run out of memory.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
I used this code and a fresh conda environment to reproduce the issue:
```bash
$ conda create -n myenv python=3.6 tensorflow
$ conda activate myenv
```

```python
""""""Example that reproduces the memory consumption increase during training.""""""
import numpy as np
import tensorflow as tf


def build_model():
    """"""Build a simple logistic regression model.""""""
    x = tf.keras.Input((100,))
    h = tf.keras.layers.Flatten()(x)
    output = tf.keras.layers.Dense(1, activation='sigmoid')(h)
    return tf.keras.Model(inputs=[x], outputs=[output])


def load_data():
    """"""Load some dummy data.""""""
    x = np.random.randn(1000000, 100).astype(np.float32)
    y = np.random.choice([0, 1], size=(1000000, 1)).astype(np.float32)
    return x, y


def train(model, x, y):
    """"""Train the model on some data.""""""
    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),
                  loss=tf.keras.losses.binary_crossentropy)
    model.fit(x, y, epochs=10000, batch_size=1280)


if __name__ == '__main__':
    x, y = load_data()
    model = build_model()
    train(model, x, y)
```
**Other info / logs**
Here are the logs from the tensorflow version installed with `conda` maybe it helps someone to reproduce how tensorflow was compiled and reproduce the issue:
```
OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #213: KMP_AFFINITY: cpuid leaf 11 not supported - decoding legacy APIC ids.
OMP: Info #149: KMP_AFFINITY: Affinity capable, using global cpuid info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3
OMP: Info #156: KMP_AFFINITY: 4 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #159: KMP_AFFINITY: 1 packages x 1 cores/pkg x 4 threads/core (1 total cores)
OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 thread 2
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 thread 3
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20360 thread 0 bound to OS proc set 0
WARNING: Logging before flag parsing goes to stderr.
W0910 21:18:07.612579 139883462330112 deprecation.py:506] From /home/ubuntu/anaconda3/envs/tfkerasbug/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0910 21:18:07.634243 139883462330112 deprecation.py:323] From /home/ubuntu/anaconda3/envs/tfkerasbug/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2019-09-10 21:18:07.824063: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-09-10 21:18:07.829994: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199910000 Hz
2019-09-10 21:18:07.830112: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e5e924d9b0 executing computations on platform Host. Devices:
2019-09-10 21:18:07.830142: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-09-10 21:18:07.830228: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2019-09-10 21:18:07.848855: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20372 thread 1 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20374 thread 2 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20375 thread 3 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20376 thread 4 bound to OS proc set 0
KMP_AFFINITY: pid 20360 tid 20373 thread 5 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20377 thread 6 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20378 thread 7 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20379 thread 8 bound to OS proc set 0
```
"
32388,tensorflow installation windows 10 python idle 3.6,"C:\WINDOWS\system32>python
Python 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\RAHUL\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\RAHUL\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\RAHUL\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\RAHUL\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\RAHUL\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\RAHUL\AppData\Roaming\Python\Python36\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\RAHUL\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\RAHUL\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\RAHUL\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\RAHUL\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\RAHUL\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\RAHUL\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.
>>> exit()

C:\WINDOWS\system32>rm -rf
'rm' is not recognized as an internal or external command,
operable program or batch file."
32387,"TypeError: 'Tensor' object does not support item assignment ,  in tf.while_loop","i am training an unsupervised CNN, for this, I defined a loss function in which CNN input is mapped to the cnn output through a complicated expression.  i am using  tf.while_loop to update the variables of ""gamma_tilde_tensor_local "" and ""D_mat_tensor_local "".  i have uploaded my code  & the error below.
please suggest me how to update tensor variables in while_loop. 

to avoid confusion, i have avoided the complex loops. originally  ""gamma_tilde_tensor_local "" size (batch_size, M,M,K,K).   


```
def function(self):
        gamma_tilde_tensor_local = tf.Variable(tf.zeros(shape = [self.mini_batchSize, self.num_users]),dtype=tf.float32)
        D_mat_tensor_local = tf.Variable(tf.zeros(shape = [self.mini_batchSize, , self.num_users]), dtype=tf.float32)
        condition_g1  = lambda k_iter1, gamma_tilde_tensor_local, D_mat_tensor_local : k_iter1 < self.num_users
        def body_gm1( k_iter1, gamma_tilde_tensor_local, D_mat_tensor_local):
            tf_x_k = tf.expand_dims(self.PHI_batch[:,:,k_iter1],axis=-1)          
            tf_x_kd = tf.expand_dims(self.PHI_batch[:,:,k_iter2],axis=-1)        
            phi_to_reduce =  tf.matmul( tf.transpose(tf_x_kd, perm=[0, 2, 1]), tf_x_k)
            phi_t = tf.squeeze(phi_to_reduce,[1,2])
            gamma_till = tf.divide(  tf.multiply( self.gammafun[:,1, 2 ], self.channel_Gain[:,1, k_iter1]), self.channel_Gain[:,1, 2])
            
            gamma_tilde_tensor_local[: k_iter1] = tf.multiply(phi_t, gamma_till )    
            D_mat_tensor_local[:,k_iter1] = tf.sqrt(  tf.multiply( self.gammafun[1, 2], self.channel_Gain[1,k_iter1] ))                 
              
            return tf.add(k_iter1, 1), gamma_tilde_tensor_local, D_mat_tensor_local
        k_iter1, gamma_tilde_tensor_local, D_mat_tensor_local = tf.while_loop(condition_g1, body_gm1, [0, gamma_tilde_tensor_local, D_mat_tensor_local])
```

**> TypeError: 'Tensor' object does not support item assignment
>         gamma_till = tf.divide(  tf.multiply( self.gammafun[:,1, 2 ], self.channel_Gain[:,1, k_iter1]), self.channel_Gain[:,1, 2])**"
32385,TF 2.0 save-load SequenceFeatures layer issue,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution: **WIndows 10 Pro build 1903**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **2.0.0-alpha0**, **2.0.0-beta0**, **2.0.0-beta1**, **2.0.0-rc0**
- Python version: **Python 3.7.3**

**Describe the current behavior**

1. I create model using Keras Functional API. The model includes `tf.keras.experimental.SequenceFeatures` layer and `sequence_categorical_column_with_vocabulary_list` feature_column.
2. Then I save model by `tf.keras.models.model.save('path_to_model.h5')`. 
3. Finally, I load model by `tf.keras.models.load_model(path_to_model.h5)`. While loading I get the error message :

> Unknown layer: SequenceFeatures

**Describe the expected behavior**
The model containing SequenceFeatures layer and any corresponding feature_columns is loaded by `tf.keras.models.load_model(MODEL_PATH.h5)` without errors.

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow import feature_column

print(tf.__version__)

!python -V

# Define categorical colunm for our text feature, which is preprocessed into sequence of tokens
text_column = feature_column.sequence_categorical_column_with_vocabulary_list(key='text', 
                                                                     vocabulary_list=list(['asd', 'asdf']))

text_embedding = feature_column.embedding_column(text_column, dimension=8)

# Then define the layers and model it self
# This example uses Keras Functional API instead of Sequential just for more generallity

# Define SequenceFeatures layer to pass feature_columns into Keras model
sequence_feature_layer = tf.keras.experimental.SequenceFeatures(text_embedding)

max_length = 6
# Define inputs for each feature column. See
# см. https://github.com/tensorflow/tensorflow/issues/27416#issuecomment-502218673
sequence_feature_layer_inputs = {}
sequence_feature_layer_inputs['text'] = tf.keras.Input(shape=(max_length,), name='text', dtype=tf.string)

sequence_feature_layer_outputs, _ = sequence_feature_layer(sequence_feature_layer_inputs)
print(sequence_feature_layer_outputs)

# Define outputs of SequenceFeatures layer 
# And accually use them as first layer of the model

# note here that SequenceFeatures layer produce tuple of two tensors as output. We need just first to pass next.
sequence_feature_layer_outputs, _ = sequence_feature_layer(sequence_feature_layer_inputs)

x = tf.keras.layers.Conv1D(8,4)(sequence_feature_layer_outputs)
x = tf.keras.layers.MaxPooling1D(2)(x)
x = tf.keras.layers.Dense(256, activation='relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)
x= tf.keras.layers.GlobalAveragePooling1D()(x)
# This example supposes binary classification, as labels are 0 or 1
x = tf.keras.layers.Dense(1, activation='sigmoid')(x)

model = tf.keras.models.Model(inputs=[v for v in sequence_feature_layer_inputs.values()], outputs=x)

model.summary()

# This example supposes binary classification, as labels are 0 or 1
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy']
              #run_eagerly=True
             )

model.save('model.h5')
model_loaded = tf.keras.models.load_model('model.h5')

model_loaded.summary()
# expected result: model_loaded is instantinated
# actual result: exception with text: ""Unknown layer: SequenceFeatures""

#other way to save model structure also finishes with error

model_json_string = model.to_json()
# import pprint
# pprint.pprint(json.loads(model_json_string))
fresh_model = tf.keras.models.model_from_json(model_json_string)
fresh_model.summary()
# expected result: model_loaded is instantinated
# actual result: exception with text: ""Unknown layer: SequenceFeatures""
```

**Other info / logs**
I have found that this problem is fixed by changes in commit [a29555a8fed1b2a1eb495e220eca7b3e3161ea26](https://github.com/tensorflow/tensorflow/commit/a29555a8fed1b2a1eb495e220eca7b3e3161ea26#diff-66f6b57cc123613d5703a087b48cdb48) and do not reproduced in **tf-nightly-2.0-preview** build. But it still reproduced in **2.0.0-alpha0**, **2.0.0-beta0**, **2.0.0-beta1**, **2.0.0-rc0**
"
32384,tensorboard gives ValueError: Duplicate plugins for name projector,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.5
- TensorFlow installed from (source or binary): `pip install tensorflow==1.14.0`
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.4 with virtualenv


**Describe the current behavior**
When I try to run `tensorboard --logdir=.` I get this ValueError: Duplicate plugins for name projector
**Describe the expected behavior**
Tensorboard should launch
**Code to reproduce the issue**
`tensorboard --logdir=.` 

**Other info / logs**
Ideally, I would ultimately like to run ` tensorboard --logdir=gs://[bucket_name]`, but I have not even been able to use tensorboard for models with logdirs that are local. 
```
Traceback (most recent call last):
  File ""/Users/len/research/.venv/bin/tensorboard"", line 10, in <module>
    sys.exit(run_main())
  File ""/Users/len/research/.venv/lib/python3.7/site-packages/tensorboard/main.py"", line 64, in run_main
    app.run(tensorboard.main, flags_parser=tensorboard.configure)
  File ""/Users/len/research/.venv/lib/python3.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/Users/len/research/.venv/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/Users/len/research/.venv/lib/python3.7/site-packages/tensorboard/program.py"", line 228, in main
    server = self._make_server()
  File ""/Users/len/research/.venv/lib/python3.7/site-packages/tensorboard/program.py"", line 309, in _make_server
    self.assets_zip_provider)
  File ""/Users/len/research/.venv/lib/python3.7/site-packages/tensorboard/backend/application.py"", line 161, in standard_tensorboard_wsgi
    reload_task)
  File ""/Users/len/research/.venv/lib/python3.7/site-packages/tensorboard/backend/application.py"", line 194, in TensorBoardWSGIApp
    return TensorBoardWSGI(plugins, path_prefix)
  File ""/Users/len/research/.venv/lib/python3.7/site-packages/tensorboard/backend/application.py"", line 245, in __init__
    raise ValueError('Duplicate plugins for name %s' % plugin.plugin_name)
ValueError: Duplicate plugins for name projector
```"
32383,tf Function could not able to transformed into graph.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32382,Audio WAV files from iOS are not read by Tensorflow audio_ops.decode_wav: Header mismatch: Expected fmt  but found JUNK,"I have an iOS app that records audio as wav files and sends the files to the server for audio recognition. The iOS app records audio as wav files with the following configurations 
```
let settings = [
                    AVFormatIDKey: Int(kAudioFormatLinearPCM),
                    AVSampleRateKey: 16000,
                    AVNumberOfChannelsKey: 1,
                    AVLinearPCMBitDepthKey: 16,
                    AVEncoderAudioQualityKey:AVAudioQuality.max.rawValue
                    ] as [String : Any]
```
The files are playing with any media player. But when i try to read the files with tensorflow ops in the server i am getting the following error

```
InvalidArgumentError: Header mismatch: Expected fmt  but found JUNK
	 [[node DecodeWav_3 (defined at <ipython-input-5-36afd53919f4>:4) ]]

Errors may have originated from an input operation.
Input Source operations connected to node DecodeWav_3:
 ReadFile_3 (defined at <ipython-input-5-36afd53919f4>:3)

Original stack trace for 'DecodeWav_3':
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 505, in start
    self.io_loop.start()
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/platform/asyncio.py"", line 148, in start
    self.asyncio_loop.run_forever()
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/asyncio/base_events.py"", line 438, in run_forever
    self._run_once()
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/asyncio/base_events.py"", line 1451, in _run_once
    handle._run()
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/asyncio/events.py"", line 145, in _run
    self._callback(*self._args)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/ioloop.py"", line 690, in <lambda>
    lambda f: self._run_callback(functools.partial(callback, future))
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/ioloop.py"", line 743, in _run_callback
    ret = callback()
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/gen.py"", line 787, in inner
    self.run()
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/gen.py"", line 748, in run
    yielded = self.gen.send(value)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 365, in process_one
    yield gen.maybe_future(dispatch(*args))
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/gen.py"", line 209, in wrapper
    yielded = next(result)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 272, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/gen.py"", line 209, in wrapper
    yielded = next(result)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 542, in execute_request
    user_expressions, allow_stdin,
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/gen.py"", line 209, in wrapper
    yielded = next(result)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 294, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 536, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2855, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2881, in _run_cell
    return runner(coro)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
    coro.send(None)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3058, in run_cell_async
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3249, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-5-36afd53919f4>"", line 4, in <module>
    data = audio_ops.decode_wav(audio_binary, desired_channels=1)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tensorflow/python/ops/gen_audio_ops.py"", line 227, in decode_wav
    desired_samples=desired_samples, name=name)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()
```

I couldn't find the reason for the crash. My TensorFlow code to read the files are as below

```
import tensorflow as tf
from tensorflow.contrib.framework.python.ops import audio_ops
file = ""/Users/minimaci73/Downloads/1567690231185.wav""  #wav file path
audio_binary = tf.read_file(file)
data = audio_ops.decode_wav(audio_binary, desired_channels=1)
pcm16_data = tf.Session().run(data)
pcm16_data = pcm16_data[0] 
print(pcm16_data)
```

OS - OSX Mojave 10.14.6
Tensorflow Version: 14
Environment - Anaconda VM"
32381,cudaGetDevice() failed. Status: cudaGetErrorString symbol not found.,"After installing Cuda 10.1 and CuDNN, I am getting above error when testing if tensorflow 2.0 can recognize my GPU, I am using a GTX 1060, on Windows 10.

I am trying to run:
`tf.test.is_gpu_available(
    cuda_only=False,
    min_cuda_compute_capability=None
)`"
32380,BatchNormalization virtual_batch_size does not work with None in input shape,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 
v2.0.0-beta1-5101-gc75bb66 2.0.0-rc0
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 10.0, cuDNN 7.6.2.24-1
- GPU model and memory: Nvidia RTX 2070 8 GB

**Describe the current behavior**
A constructor of a tf.keras Model that uses `tf.keras.layers.BatchNormalization` with `virtual_batch_size` set and unspecified input shape dimensions throws an exception.

**Describe the expected behavior**
Such a model should be usable.

**Code to reproduce the issue**
```python
import tensorflow as tf

inp = tf.keras.layers.Input(shape=(None, None, 3))
net = tf.keras.layers.BatchNormalization(virtual_batch_size=8)(inp)

model = tf.keras.Model(inputs=inp, outputs=net)
```

**Other info / logs**
Traceback of the exception:
```
Traceback (most recent call last):
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py"", line 541, in make_tensor_proto
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py"", line 541, in <listcomp>
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/util/compat.py"", line 71, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got 8

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/test_virtual_batch.py"", line 6, in <module>
    net = tf.keras.layers.BatchNormalization(virtual_batch_size=8)(inp)
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 802, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py"", line 652, in call
    inputs = array_ops.reshape(inputs, expanded_shape)
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py"", line 131, in reshape
    result = gen_array_ops.reshape(tensor, shape, name)
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py"", line 8117, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 530, in _apply_op_helper
    raise err
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 527, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 1296, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py"", line 286, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py"", line 227, in constant
    allow_broadcast=True)
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py"", line 265, in _constant_impl
    allow_broadcast=allow_broadcast))
  File ""/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py"", line 545, in make_tensor_proto
    ""supported type."" % (type(values), values))
TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [8, -1, None, None, 3]. Consider casting elements to a supported type.
```"
32378,*del*,*del*
32377,TF2.0  AutoGraph issue,"OS:Ubuntu 18.04
TensorFlow Version:tensorflow-gpu == 2.0.0-rc0
issue:AutoGraph
WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd89803cef0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'
WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd89803cef0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3594563/tf_env.txt)"
32376,[TF2.0]Shuffle function on tf.data fills up RAM memory.,"In TF2.0rc, using `shuffle(buffer_size= #_of_elements)` on `tf.data.Dataset` type dataset, it fills up shuffle buffer which also fills up RAM memory so if the data is huge and RAM is not enough then it hangs up the system which leads to system restart. This memory filling happens before every epoch. It of course releases the memory once the epoch is done. To reproduce this problem, I followed [this](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches) TF tutorial but with more amount of data.
Since `tf.data` is for handling the dataset  that can not be filled up in the memory then why does shuffle function shows this behaviour and fills up the memory.?"
32375,tf.concat not accepting arbitrary dtype with functional API model,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Windows 10 Pro, version 1903, OS build 18362.295
- TensorFlow installed via: pip
- TensorFlow version: v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0
- Python version: Python 3.6.9 |Anaconda, Inc.| [MSC v.1915 64 bit (AMD64)] on win32

**Describe the current behavior**
```tf.concat``` only accepts ```float32``` unless explicitly passing ```dtype``` arg to ```tf.keras.Input```.

**Describe the expected behavior**
```tf.concat``` implicitly accepts any dtype (as long as input dtypes match).

**Code to reproduce the issue**
```
# Build concat model
x1 = tf.keras.Input([5])
x2 = tf.keras.Input([6])
y = tf.concat([x1, x2], axis=1)
concat_model = tf.keras.Model(inputs=[x1, x2], outputs=y)
# Generate inputs
x1_ = np.random.random(size=[2, 5])
x2_ = np.random.random(size=[2, 6])
assert x1_.dtype == 'float64'
assert x2_.dtype == 'float64'
# Run model
y_ = concat_model([x1_, x2_])  # EXCEPTION HAPPENS HERE
```

**Full traceback**
```Traceback (most recent call last):
  File ""C:/Users/wwill/Downloads/tf_bug_report_test.py"", line 10, in <module>
    y_ = concat_model([x1_, x2_])  # Expected output of dtype float64 to match inputs
  File ""C:\Users\wwill\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 851, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""C:\Users\wwill\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 697, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)
  File ""C:\Users\wwill\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 842, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File ""C:\Users\wwill\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 851, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""C:\Users\wwill\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 2525, in call
    return self._defun_call(inputs)
  File ""C:\Users\wwill\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1821, in __call__
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File ""C:\Users\wwill\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2147, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\Users\wwill\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2038, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""C:\Users\wwill\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\framework\func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Users\wwill\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2655, in bound_method_wrapper
    return wrapped_fn(*args, **kwargs)
  File ""C:\Users\wwill\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python\framework\func_graph.py"", line 905, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in converted code:
    relative to C:\Users\wwill\Anaconda3\envs\tf2\lib\site-packages\tensorflow_core\python:

    keras\engine\base_layer.py:2571 _defun_call  *
        return self._make_op(inputs)
    keras\engine\base_layer.py:2549 _make_op
        c_op = ops._create_c_op(graph, node_def, inputs, control_inputs=[])
    framework\ops.py:1613 _create_c_op
        raise ValueError(str(e))

    ValueError: Inconsistent values for attr 'T' DT_DOUBLE vs. DT_FLOAT while building NodeDef 'concat' using Op<name=ConcatV2; signature=values:N*T, axis:Tidx -> output:T; attr=N:int,min=2; attr=T:type; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>```"
32373,is tf.signal.stft same as librosa.stft ?,"hi,
Dear, if I use the tf.signal.stft instead of librosa.stft, will have any difference ?
and tf.signal.inverse_stft could be librosa.istft ?

thanks a lot
"
32371,Inconsistent tf.Print() results,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.14
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 10/cudnn7.1
- GPU model and memory:

**Describe the current behavior**
```python
tf.reset_default_graph()
use_resource = False
v = tf.Variable(0, trainable=False, use_resource=use_resource, name='v')
v_op1 = v.assign_add(1, name='v_op1')
v_op2 = v.assign_add(2, name='v_op2')
with tf.control_dependencies([v_op1]):
    w = tf.Print(v,[v],'msg1:', name='v--w')
with tf.control_dependencies([v_op2]):
    w = tf.Print(w,[v],'msg2:', name='w--w')
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(w)
```
When I run the above code multiple times, the print messages are inconsistent. Sometime it prints

```python
msg1:[1]
msg2:[3]
```
Sometimes it prints

```python
msg1:[3]
msg2:[3]
```
I though when we execute `sess.run(w)`, `v_op2` will be executed first, but in the `v_op2`, we need to get `w` first, so actually we are depend on `v_op1`, that means we actually execute `v_op1` first. So `v` will be 1 and then `v` was assigned to `w`. In the end,  `v` was increased by 2 via `v_op2`. So my expectation output of `tf.Print()` should be:

```python
msg1:[1]
msg2:[3]
```
But why does it print the second results sometimes? Why are the print results inconsistent? One possibility I could think of, is that `tf.Print()` sometimes read the value of `v` before `v_op2` is done, sometimes it reads the value of `v` after `v_op2` is done. But I'm not sure actually what's going on under the hood. "
32369,tf.keras.layers.LSTM does not pass its trainable values to its cell,"**System information**
- TensorFlow version 1.12.0 (I believe the issue appears in 1.14.0 as well).
- Python version: 3.5.2

```
import tensorflow as tf

lstm_input = tf.placeholder(dtype=tf.float64, shape=[3, 1, 4])
lstm = tf.keras.layers.LSTM(512, dtype=tf.float64)
print(lstm.cell.dtype)  # prints None!!!
result = lstm(lstm_input)
```


When running this, the last line raises the following error:
TypeError: Input 'b' of 'MatMul' Op has type float32 that does not match type float64 of argument 'a'.

The LSTM constructor is not passing the dtype to the LSTMCell constructor, so `lstm.cell.dtype` is just None. So the weights created in `LSTMCell.build` have their dtype default to tf.float32, which seems to cause the error. Adding `lstm.cell._dtype = 'float64'` right before the last line seems to ""fix"" the issue. There may be other mismatches between LSTM and LSTMCell, but the dtype mismatch is the one I ran into."
32368,Problem in tensorboard callback,"-> tf-nightly-gpu-2.0-preview==2.0.0.dev20190909
-> Python 3.7.4 (default, Jul  9 2019, 03:52:42)  [GCC 5.4.0 20160609] on linux
-> Ubuntu 16.04 LTS
-> Geforce GTX 1080 Ti

I have an autoencoder with Tensorboard callback and tf.datasets (whose code I've omitted here, since the iteration on the patches are working)

``` python
tensorboard = tf.keras.callbacks.TensorBoard(log_dir=str(output_folder / 'logs'), histogram_freq=1, write_graph=True, update_freq=500)
checkpoints = tf.keras.callbacks.ModelCheckpoint(str(output_folder / 'checkpoints'/ 'val_loss{val_loss:.3g}-epoch{epoch:02d}-loss{loss:.3g}'), verbose=1, save_best_only=True, mode='min', monitor='val_loss', save_freq='epoch')

train_iter = create_db_loader(get_file_paths(g_dict['run_glob']), g_dict['train'])
valid_iter = create_db_loader(get_file_paths(g_dict['val_glob']), 0)

autoencoder.fit(x=train_iter, y=None, validation_data=valid_iter, epochs=g_dict['epochs'], steps_per_epoch=g_dict['steps_per_epoch'], validation_steps=g_dict['validation_steps'], validation_freq=1, callbacks=[tensorboard, checkpoints])
```

Apparently, everything runs ok

``` python
Train for 1000 steps, validate for 100 steps
Epoch 1/10
2019-09-09 17:33:56.224374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-09 17:33:56.979317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-09 17:33:57.194187: I tensorflow/core/profiler/lib/profiler_session.cc:206] Profiler session started.
2019-09-09 17:33:57.194313: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.0'; dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory
2019-09-09 17:33:57.194322: W tensorflow/core/profiler/lib/profiler_session.cc:214] Encountered error while starting profiler: Unavailable: CUPTI error: CUPTI could not be loaded or symbol could not be found.
   1/1000 [..............................] - ETA: 27:33 - loss: 0.21982019-09-09 17:33:57.202954: I tensorflow/core/platform/default/device_tracer.cc:590] Collecting 0 kernel records, 0 memcpy records.
2019-09-09 17:33:57.203136: E tensorflow/core/platform/default/device_tracer.cc:70] CUPTI error: CUPTI could not be loaded or symbol could not be found.
 993/1000 [============================>.] - ETA: 0s - loss: 0.0086
Epoch 00001: loss improved from inf to 0.00862, saving model to ../output/run0/checkpoints/val_loss0.00754-epoch01-loss0.00862
2019-09-09 17:34:04.410966: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:tensorflow:From /home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1782: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
1000/1000 [==============================] - 9s 9ms/step - loss: 0.0086 - val_loss: 0.0075
Epoch 2/10
 998/1000 [============================>.] - ETA: 0s - loss: 0.0077
Epoch 00002: loss improved from 0.00862 to 0.00767, saving model to ../output/run0/checkpoints/val_loss0.00718-epoch02-loss0.00767
1000/1000 [==============================] - 8s 8ms/step - loss: 0.0077 - val_loss: 0.0072
Epoch 3/10
 993/1000 [============================>.] - ETA: 0s - loss: 0.0065
Epoch 00003: loss improved from 0.00767 to 0.00646, saving model to ../output/run0/checkpoints/val_loss0.00623-epoch03-loss0.00646
1000/1000 [==============================] - 8s 8ms/step - loss: 0.0065 - val_loss: 0.0062
Epoch 4/10
 995/1000 [============================>.] - ETA: 0s - loss: 0.0052
Epoch 00004: loss improved from 0.00646 to 0.00524, saving model to ../output/run0/checkpoints/val_loss0.00642-epoch04-loss0.00524
1000/1000 [==============================] - 8s 8ms/step - loss: 0.0052 - val_loss: 0.0064
Epoch 5/10
 993/1000 [============================>.] - ETA: 0s - loss: 0.0062
Epoch 00005: loss did not improve from 0.00524
1000/1000 [==============================] - 7s 7ms/step - loss: 0.0061 - val_loss: 0.0056
Epoch 6/10
 992/1000 [============================>.] - ETA: 0s - loss: 0.4702
Epoch 00006: loss did not improve from 0.00524
1000/1000 [==============================] - 7s 7ms/step - loss: 0.4728 - val_loss: 0.7266
Epoch 7/10
 999/1000 [============================>.] - ETA: 0s - loss: 0.7511
Epoch 00007: loss did not improve from 0.00524
1000/1000 [==============================] - 7s 7ms/step - loss: 0.7511 - val_loss: 0.7195
Epoch 8/10
 994/1000 [============================>.] - ETA: 0s - loss: 0.7476
Epoch 00008: loss did not improve from 0.00524
1000/1000 [==============================] - 7s 7ms/step - loss: 0.7476 - val_loss: 0.7171
Epoch 9/10
 994/1000 [============================>.] - ETA: 0s - loss: 0.7546
Epoch 00009: loss did not improve from 0.00524
1000/1000 [==============================] - 7s 7ms/step - loss: 0.7517 - val_loss: 0.2810
Epoch 10/10
 998/1000 [============================>.] - ETA: 0s - loss: 0.2786
Epoch 00010: loss did not improve from 0.00524
1000/1000 [==============================] - 7s 7ms/step - loss: 0.2786 - val_loss: 0.2764
```

However, tensorboard simply doesn't show the activations:

![Screenshot from 2019-09-09 17-43-23](https://user-images.githubusercontent.com/35977339/64565143-80d3e100-d329-11e9-83d4-bb4bf323f16f.png)

`write_images=True` is not working either. 

So, is there anything wrong with this piece of code? Or is it a bug?

Another thing: why isn't there a write_grads option in 2.0? I've seen a message in 1.x saying it's not supported with eager execution enabled. Is there a special reason for this removal? 

I was trying to use as much functionalities as I could from keras default routines, for fast prototyping. Being capable of printing the gradients is a good thing, specially using the `fit` method.

It seems I'll have to implement a manual iteration loop."
32366,"LINK : fatal error LNK1201: error writing to program database ""..\x64_windows-opt\bin\tensorflow\python\_pywrap_tensorflow_internal.pdb'; check for insufficient disk space, invalid path, or insufficient privilege","#31610 : The same kind of issue. 

**System information**
- OS Platform and Distribution:  Windows 10
- TensorFlow installed from (source or binary): Source. Master branch. SHA Key: 97ff26f
- TensorFlow version: 1.13.1
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: Pip
- Bazel version (if compiling from source): 0.25.1 (I modified the configure.py to support 0.25.1 Bazel version)
- GCC/Compiler version (if compiling from source): Using Microsoft Visual Studio 2019(No 2017 or 2015 Build tools)
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the problem**
 I am trying to build Tensorflow sources in optimised mode, with debug information, so that I can get some debug information in the form of pdb files. However, I am getting the error:
LINK : fatal error LNK1201: error writing to program database 'C:\users\tensorflow_bazel_tensorflow\a7oebymx\execroot\org_tensorflow\bazel-out\x64_windows-opt\bin\tensorflow\python_pywrap_tensorflow_internal.pdb'; check for insufficient disk space, invalid path, or insufficient privilege
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3959.292s, Critical Path: 1017.43s
INFO: 4150 processes: 4150 local.
FAILED: Build did NOT complete successfully

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. python ./configure.py
2. bazel build --define=tensorflow_mkldnn_contraction_kernel=0 --local_ram_resources=2048 -c opt --copt=/Z7 --copt=/FS --linkopt=/DEBUG --strip=never --config=opt //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
The size of the pdb: C:\users\tensorflow_bazel_tensorflow\a7oebymx\execroot\org_tensorflow\bazel-out\x64_windows-opt\bin\tensorflow\python\_pywrap_tensorflow_internal.pdb was just 4GB and there was enough disk space available.
CAn somebody help me give a better insight of the issue?"
32365,Reduce sum should work on heterogeneous shapes,"**System information**
- TensorFlow version (you are using): 2 rc
- Are you willing to contribute it (Yes/No): Not sure how

**Describe the feature and the current behavior/state.**
tf.math.reduce_sum fails if given an array of differently shaped tensors

**Will this change the current api? How?**
reduce sum should ""just work"" in the case where user wants to total a bunch of different shapes. 

**Who will benefit with this feature?**
anyone who wants to tally the losses

**Any Other info.**
just want to tally my losses, but this doesn't work:
`total = tf.math.reduce_sum(LIST_OF_TENSORS)`
```
    @tf.function
    def tally(losses):
        total = 0.
        for loss in losses:
            current = tf.math.reduce_sum(loss)
            tf.print(tf.shape(current))
            total = total + current
        return total
```
this is a simple common task, `total = tf.math.reduce_sum(LIST_OF_TENSORS)` should just work even if the tensors are differently shaped

>     ValueError: Shapes must be equal rank, but are 3 and 0"
32363,Error building tensorflow graph transform tools from source using bazel,"
**System information**
**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**
Microsoft Windows [Version 10.0.17763.615]
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**
No
- **TensorFlow installed from (source or binary):**
Source
- TensorFlow version:

- **Python version**:
3.6
- **Installed using virtualenv? pip? conda?:**
No
- **Bazel version (if compiling from source):**
0.26.1
- GCC/Compiler version (if compiling from source):

- CUDA/cuDNN version:
10.1/7
- GPU model and memory:
Nvidia GeForce GTX 1050, 10123MB
**Describe the problem**
Trying to use tensorflow graph transform tools by building it using bazel. Configuration is done fine, but while building it using the command :
`bazel build tensorflow/tools/graph_transforms:transform_graph`

**receiving the error**;
`INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Options provided by the client:
  'build' options: --python_path=C:/Windows/system32/python.exe
INFO: Reading rc options for 'build' from c:\users\rajat\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include
INFO: Reading rc options for 'build' from c:\users\rajat\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Windows/system32/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Rajat/AppData/Local/Programs/Python/Python36/lib/site-packages --python_path=C:/Windows/system32/python.exe --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --config=cuda --config monolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --verbose_failures --distinct_host_configuration=false --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:cuda in file c:\users\rajat\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\users\rajat\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
INFO: Found applicable config definition build:monolithic in file c:\users\rajat\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Call stack for the definition of repository 'local_config_cuda' which is a cuda_configure (rule definition at C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl:1268:18):
 - C:/users/rajat/tensorflow/tensorflow/workspace.bzl:70:5
 - C:/users/rajat/tensorflow/WORKSPACE:27:1
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1266
                _create_local_cuda_repository(repository_ctx)
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 988, in _create_local_cuda_repository
                _get_cuda_config(repository_ctx)
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 714, in _get_cuda_config
                find_cuda_config(repository_ctx, [""cuda"", ""cudnn""])
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 694, in find_cuda_config
                auto_configure_fail((""Failed to run find_cuda_config...))
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 325, in auto_configure_fail
                fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Failed to run find_cuda_config.py:
ERROR: Skipping 'tensorflow/tools/graph_transforms:transform_graph': error loading package 'tensorflow/tools/graph_transforms': in C:/users/rajat/tensorflow/tensorflow/tensorflow.bzl: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1266
                _create_local_cuda_repository(repository_ctx)
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 988, in _create_local_cuda_repository
                _get_cuda_config(repository_ctx)
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 714, in _get_cuda_config
                find_cuda_config(repository_ctx, [""cuda"", ""cudnn""])
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 694, in find_cuda_config
                auto_configure_fail((""Failed to run find_cuda_config...))
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 325, in auto_configure_fail
                fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Failed to run find_cuda_config.py:
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/graph_transforms': in C:/users/rajat/tensorflow/tensorflow/tensorflow.bzl: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1266
                _create_local_cuda_repository(repository_ctx)
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 988, in _create_local_cuda_repository
                _get_cuda_config(repository_ctx)
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 714, in _get_cuda_config
                find_cuda_config(repository_ctx, [""cuda"", ""cudnn""])
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 694, in find_cuda_config
                auto_configure_fail((""Failed to run find_cuda_config...))
        File ""C:/users/rajat/tensorflow/third_party/gpus/cuda_configure.bzl"", line 325, in auto_configure_fail
                fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Failed to run find_cuda_config.py:
INFO: Elapsed time: 0.238s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/graph_transforms
`

No ideas to go any further, kindly suggest any solution.

Thanks
"
32361,load custom op library when using c++?,"I have build a very simple custom op `zero_out.dll` with bazel, it works when using python.
```
import tensorflow as tf
zero_out_module = tf.load_op_library('./zero_out.dll')
with tf.Session(''):
  zero_out_module.zero_out([[1, 2], [3, 4]]).eval()
```
However I have to run the inference using C++, is there any c++ api which has the similar function as `tf.load_op_library`, or it seems that a lot of registeration work has been done in `tf.load_op_library`, TF has not counterpart c++ API?"
32360,Tensorflow juste use 0 - 5% GPU,"When i'm using tensorflow for face recognition with facenet, it's very very slow. And when i check for GPU during execution, i saw that the big part of the time only 1% of GPU is ised and sometimes 29% but just one 1second. 
I made my installation without any problem. 
during the code execution, it show a lot of warning like use tf.data but really don't know how to do that and if its because of it
Thanks

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux ubunutu 18.4.1
- TensorFlow version (use command below):1.14.0
- Python version: 3.6.8
- GCC/Compiler version (if compiling from source): gcc-7
- CUDA/cuDNN version: 10.0/7.5.1
- GPU model and memory:Nvidia quadro P2000
"
32359,[TPU][TF 1.14]Can not reload the trained model for training on Colab TPU.,"I trained a model on Colab TPU and then saved it using callbacks in `model.fit()`. Now, I want to continue training model after restoring it from the last checkpoint saved as h5 format. This file has layer weights and optimizer weights i.e. also the architecture. On loading the model inside `TPUStrategy scope()` as follows-
````
TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']# Name or Address of TPU
resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)
tf.contrib.distribute.initialize_tpu_system(resolver)
strategy = tf.contrib.distribute.TPUStrategy(resolver)

with strategy.scope():
  saved_model_file_name = os.path.join(""/content/gdrive/My Drive/data"", 'GDmodel09.h5')# saved model is on my Google Drive
  model = tf.keras.models.load_model(saved_model_file_name)
````
the last line throws following error-
`AttributeError: 'NoneType' object has no attribute 'merge_call'`
However, if I load the model outside scope then it gets loaded without any error.
Please help. Thank you!"
32358,Does tf consider to support mac os + amd gpu?,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS, Mojave, 10.14.6
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.0-rc0
- Python version: python 3.7


**Describe the problem**
I try to compile tensorflow from source code with opencl, while it requires computecpp support. However, the official website of computecpp shows that it does not mac os.
Thus, as described by tf official docs, the gpu version cannot run on mac os. 
I just want to ask whether tf considers to add gpu support for mac os, now I can only develop models by keras with plaidml backend, or run tf on cpu.

"
32357,Keras 'fit_generator' - Zip object is not considered generator or sequence in ,"**Description**
Keras Model `fit_generator` does not work with `zip` objects.
I followed the image segmentation example from https://keras.io/preprocessing/image/ of loading and transforming images and masks together and then training a model with `fit_generator`.

I'm getting `AttributeError: 'zip' object has no attribute 'shape'` (see stack trace below)

**System information**
- TensorFlow version: 1.14.0 and 2.0.0-RC
- Python version: 3.7.3
- Occurs on both CPU and GPU execution

**Code to reproduce the issue**
Code taken from https://keras.io/preprocessing/image/

```
# we create two instances with the same arguments
data_gen_args = dict(featurewise_center=True,
                     featurewise_std_normalization=True,
                     rotation_range=90,
                     width_shift_range=0.1,
                     height_shift_range=0.1,
                     zoom_range=0.2)
image_datagen = ImageDataGenerator(**data_gen_args)
mask_datagen = ImageDataGenerator(**data_gen_args)

# Provide the same seed and keyword arguments to the fit and flow methods
seed = 1
image_datagen.fit(images, augment=True, seed=seed)
mask_datagen.fit(masks, augment=True, seed=seed)

image_generator = image_datagen.flow_from_directory(
    'data/images',
    class_mode=None,
    seed=seed)

mask_generator = mask_datagen.flow_from_directory(
    'data/masks',
    class_mode=None,
    seed=seed)

# combine generators into one which yields image and masks
train_generator = zip(image_generator, mask_generator)

model.fit_generator(
    train_generator,
    steps_per_epoch=2000,
    epochs=50)
```

**Stacktrace**
```
File ""/home/kevin/dev/code/image-gen/src/train.py"", line 47, in main
    validation_steps=1)
  File ""/home/kevin/dev/venvs/image-gen/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1303, in fit_generator
    steps_name='steps_per_epoch')
  File ""/home/kevin/dev/venvs/image-gen/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py"", line 144, in model_iteration
    shuffle=shuffle)
  File ""/home/kevin/dev/venvs/image-gen/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py"", line 477, in convert_to_generator_like
    num_samples = int(nest.flatten(data)[0].shape[0])
AttributeError: 'zip' object has no attribute 'shape'
```

**Workaround**:
Create generator from zip object
`train_generator = (pair for pair in zip(image_generator, mask_generator))`
"
32356,Keras 'Zip object is not considered generator or sequence in ,Duplicate - can be deleted
32355,Custom Distributed Strategy,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

As Tensorflow stands, there is no easy and intuitive way to implement a new distribution strategy. The ones available, (MirroredStrategy, MultiWorkerMirroredStrategy, ...), work fine but the code seems very complex, and there isn't a tutorial/guide on how to develop a new one

**Will this change the current api? How?**

The api cloud be restructured to ease the support of new distributed strategy. A tutorial/guide on how to develop one would also be appreciated 

**Who will benefit with this feature?**

Researchers who want to reduce the time spent training distributed tensorflow models

**Any Other info.**"
32354,"Tensorflow 1.14 + CUDA 10.1, nvcc fatal   : Unsupported gpu architecture 'compute_101'","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14 branch
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): gcc 7.4.0
- CUDA/cuDNN version:10.1
- GPU model and memory: gtx 1080, 8GB Ram



**Describe the problem**
nvcc fatal   : Unsupported gpu architecture 'compute_101'

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32353,Issue with basic_classification tutorial,"## URL(s) with the issue:

https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/tutorials/keras/basic_classification.ipynb#scrollTo=9ODch-OFCaW4&line=2&uniqifier=1
## Description of issue (what needs changing):

I was browsing the beginner tutorial for basic_classification. It was at the above link. In the section **Setup the Layers** it is throwing a warning when running:

`WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor`
"
32352,Error while importing Tensorflow,"**When I try to import Tensorflow I'm getting this error tried a lot of solutions but nothing could help**
 **I'm using Python 3.6 in 64 bit pc**

```
Traceback (most recent call last):
  File ""C:\Users\NEW\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\NEW\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\NEW\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\NEW\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\NEW\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\NEW\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\NEW\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\NEW\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\NEW\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\NEW\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\NEW\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\NEW\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\NEW\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

```
```"
32351,403 Forbidden error in 'Classify structured data' tutorial,"Hi,

I got a 403 Forbidden error in the following tutorial section.

https://www.tensorflow.org/beta/tutorials/keras/feature_columns#use_pandas_to_create_a_dataframe

The code is as follows:
```
URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
dataframe = pd.read_csv(URL)
dataframe.head()
```

The output:
```
---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
<ipython-input-4-bbbf4ab1b496> in <module>()
      1 URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
----> 2 dataframe = pd.read_csv(URL)
      3 dataframe.head()

8 frames
/usr/lib/python3.6/urllib/request.py in http_error_default(self, req, fp, code, msg, hdrs)
    648 class HTTPDefaultErrorHandler(BaseHandler):
    649     def http_error_default(self, req, fp, code, msg, hdrs):
--> 650         raise HTTPError(req.full_url, code, msg, hdrs, fp)
    651 
    652 class HTTPRedirectHandler(BaseHandler):

HTTPError: HTTP Error 403: Forbidden
```"
32350,cuDNN 7.2 linker errors,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14.0
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): 7.3.1
- CUDA/cuDNN version: 9.2.148 / 7.2.1
- GPU model and memory: GeForce GTX 1080



**Describe the problem**

When compiling tensorflow 1.14 from source with cuDNN version 7.2 linker errors occur:

```
ERROR: /nfs/freenas/tuph/e18/project/e18sat/poeschl_neuron/tensorinstall/shredderPy3/tensorflow/tf_test/tensorflow-1.14.0/tensorflow/python/BUILD:2189:1: Linking of rule '//tensorflow/python:gen_tpu_ops_py_wrappers_cc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /tmp/ga58pas/bazel/_bazel_ga58pas/afa3e28b79d056ac590a6714d65f8145/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/nfs/mnemosyne/sys/cc7/sw/root/x86-64/6.14.06/devtoolset-7/root/lib:/opt/rh/python27/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/nfs/freenas/globalSoftware/CC7/geant4/install/lib64:/nfs/freenas/globalSoftware/CC7/CADMesh/build/lib:/nfs/freenas/globalSoftware/CC7/CGAL/build/lib::/nfs/freenas/geant4.10.2/geant4.10.2-install/lib64 \
    PATH=/nfs/mnemosyne/user/ga58pas/private/bazel/bazel-0.25.2/bin:/nfs/mnemosyne/sys/cc7/sw/root/x86-64/6.14.06/devtoolset-7/root/bin:/opt/rh/python27/root/usr/bin:/opt/rh/devtoolset-7/root/usr/bin:/nfs/freenas/globalSoftware/CC7/geant4/install/bin:/nfs/freenas/globalSoftware/CC7/cmake/build/bin:/nfs/mnemosyne/user/ga58pas/private/scripts:/usr/sue/bin:/usr/lib64/qt-3.3/bin:/usr/lib64/ccache:/opt/sge/bin/lx-amd64:/usr/local/bin:/usr/bin:/bin:/usr/games:/opt/puppetlabs/bin/ \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/tensorflow/python/gen_tpu_ops_py_wrappers_cc '-Wl,-rpath,$ORIGIN/../../_solib_local/_U_S_Stensorflow_Spython_Cgen_Utpu_Uops_Upy_Uwrappers_Ucc___Utensorflow' -Lbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Utpu_Uops_Upy_Uwrappers_Ucc___Utensorflow -Wl,-ldl '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -pthread -pthread -pthread -pthread -Wl,-S -Wl,-no-as-needed -pie -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/host/bin/tensorflow/python/gen_tpu_ops_py_wrappers_cc-2.params)
Execution platform: @bazel_tools//platforms:host_platform
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Utpu_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.1: undefined reference to `cudnnSetRNNPaddingMode'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Utpu_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.1: undefined reference to `cudnnRNNForwardInferenceEx'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Utpu_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.1: undefined reference to `cudnnCreateRNNDataDescriptor'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Utpu_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.1: undefined reference to `cudnnRNNForwardTrainingEx'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Utpu_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.1: undefined reference to `cudnnSetRNNDataDescriptor'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Utpu_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.1: undefined reference to `cudnnRNNBackwardDataEx'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Utpu_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.1: undefined reference to `cudnnRNNBackwardWeightsEx'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Utpu_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.1: undefined reference to `cudnnDestroyRNNDataDescriptor'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1529.552s, Critical Path: 187.94s
INFO: 5504 processes: 5504 local.
FAILED: Build did NOT complete successfully
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I configured tensorflow to compile with CUDA 9.2 and cuDNN 7.2 and started the build:
```
./configure
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

My .tf_configure.bazelrc looks like this
```
build --action_env PYTHON_BIN_PATH=""/nfs/freenas/virtenv/TensorPy3/bin/python""
build --action_env PYTHON_LIB_PATH=""/nfs/freenas/virtenv/TensorPy3/lib/python3.6/site-packages""
build --python_path=""/nfs/freenas/virtenv/TensorPy3/bin/python""
build:xla --define with_xla_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env TF_NEED_TENSORRT=""0""
build --action_env TF_CUDA_VERSION=""9""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_NCCL_VERSION=""""
build --action_env TF_CUDA_PATHS=""/usr/local/cuda-9.2,/usr/local/cuda-9.2/targets/x86_64-linux""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda-9.2""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env LD_LIBRARY_PATH=""/nfs/mnemosyne/sys/cc7/sw/root/x86-64/6.14.06/devtoolset-7/root/lib:/opt/rh/python27/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/nfs/freenas/globalSoftware/CC7/geant4/install/lib64:/nfs/freenas/globalSoftware/CC7/CADMesh/build/lib:/nfs/freenas/globalSoftware/CC7/CGAL/build/lib::/nfs/freenas/geant4.10.2/geant4.10.2-install/lib64""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/opt/rh/devtoolset-7/root/usr/bin/gcc""
build --config=cuda
build:opt --copt=-march=native
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build:v2 --define=tf_api_version=2
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_tag_filters=-benchmark-test,-no_oss,-oss_serial
test --build_tag_filters=-benchmark-test,-no_oss
test --test_tag_filters=-gpu
test --build_tag_filters=-gpu
build --action_env TF_CONFIGURE_IOS=""0""
```
"
32349,Distributed strategy TF2.0 rc0 error,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):tf 2.0.0 rc0
- TensorFlow version (use command below):tf 2.0.0 rc
- Python version:3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory: tesla v100

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**


i run training code with 4 gpus using Distributed stratage..
when i finish 1 epoch.. end of epoch.. always got the mesage.
but if i run this code in local cpu.., everythings is okay..
i don't understand what's wrlong


epoch: 0, step: 19, loss = 51.801109313964844
epoch: 0, step: 20, loss = 51.188865661621094
epoch: 0, step: 21, loss = 51.24317932128906
epoch: 0, step: 22, loss = 51.97450256347656
Traceback (most recent call last):
  File ""/root/arcface_test/train_multigpu.py"", line 259, in <module>
    main()
  File ""/root/arcface_test/train_multigpu.py"", line 211, in main
    loss = distributed_train_step(images,labels)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py"", line 415, in __call__
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py"", line 1821, in __call__
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py"", line 2147, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py"", line 2038, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py"", line 320, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py"", line 905, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in converted code:

    /root/arcface_test/train_multigpu.py:187 distributed_train_step  *
        per_replica_losses = strategy.experimental_run_v2(train_step, args=(images,labels))
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py:760 experimental_run_v2
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /root/arcface_test/train_multigpu.py:173 train_step  *
        prelogits,  norm_dense = model(images, training=True)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py:807 __call__
        outputs = call_fn(cast_inputs, *args, **kwargs)
    /root/arcface_test/models/model.py:36 call  *
        prelogits = self.backbone(inputs, training=training)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py:807 __call__
        outputs = call_fn(cast_inputs, *args, **kwargs)
    /root/arcface_test/backbones/L_resnet_E_IR.py:118 call  *
        x = self.conv(inputs)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py:772 __call__
        self.name)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/input_spec.py:213 assert_input_compatibility
        ' but received input with shape ' + str(shape))

    ValueError: Input 0 of layer conv2d is incompatible with the layer: expected axis -1 of input shape to have value 3 but received input with shape [0, 112, 112, 0]


**Describe the expected behavior**

**Code to reproduce the issue**
from __future__ import absolute_import, division, print_function, unicode_literals

import argparse
import datetime
import os
import platform
import sys
import time
import math
import tensorflow as tf
import yaml

from backbones.L_resnet_E_IR import ResNet_v1_101_IR
from data.generate_data import GenerateData
from models.model import MyModel
from valid import Valid_Data


tf.executing_eagerly()


def parse_args(argv):
    parser = argparse.ArgumentParser(description='Train face network')
    parser.add_argument('--config_path', type=str, help='path to config path', default='configs/config_local.yaml')

    args = parser.parse_args(argv)

    return args

#lr decay 설정...
def decay(step):
    if step < 100000:
        return 0.1
    if step >= 100000 and step < 160000:
        return 0.01
    if step >= 160000:
        return 0.001
def main():


    args = parse_args(sys.argv[1:])
    # logger.info(args)

    with open(args.config_path) as cfg:
        config = yaml.load(cfg, Loader=yaml.FullLoader)

    num_of_gpu = config['num_of_gpu']

    print(num_of_gpu)
    strategy = tf.distribute.MirroredStrategy()
    print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))

    epoch_num = config['epoch_num']
    image_size_width = config['image_size_width']
    image_size_height = config['image_size_height']
    image_channel = config['image_channel']


    m3 = config['logits_margin3']
    s = config['logits_scale']
    below_fpr = config['below_fpr']
    learning_rate = config['learning_rate']
    batch_size = config['batch_size']



    with strategy.scope():

        gd = GenerateData(config)

        train_data, cat_num = gd.get_train_data()

        print(cat_num)

        model = MyModel(ResNet_v1_101_IR, embedding_size=config['embedding_size'], classes=cat_num)

        #model.build((None, image_size_height , image_size_width, image_channel))

        valid_data = gd.get_val_data(num=config['valid_num'], valid_data_type=config['valid_data_type'])


        ckpt_dir = os.path.expanduser(config['ckpt_dir'])

        summary_dir = os.path.expanduser(config['summary_dir'])
        current_time = datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
        train_log_dir = os.path.join(summary_dir, current_time, 'train')
        valid_log_dir = os.path.join(summary_dir, current_time, 'valid')
        # self.graph_log_dir = os.path.join(summary_dir, current_time, 'graph')

        if platform.system() == 'Windows':
            train_log_dir = train_log_dir.replace('/', '\\')
            valid_log_dir = valid_log_dir.replace('/', '\\')
            # self.graph_log_dir = self.graph_log_dir.replace('/', '\\')

        # self.train_summary_writer = tf.summary.create_file_writer(train_log_dir)
        # self.valid_summary_writer = tf.summary.create_file_writer(valid_log_dir)
        train_summary_writer = tf.compat.v2.summary.create_file_writer(train_log_dir)
        valid_summary_writer = tf.compat.v2.summary.create_file_writer(valid_log_dir)

        #self.graph_writer = tf.compat.v2.summary.create_file_writer(self.graph_log_dir)
        #tf.compat.v2.summary.trace_on(graph=True, profiler=True)
        #with self.graph_writer.as_default():
        #    tf.compat.v2.summary.trace_export(name=""graph_trace"", step=0, profiler_outdir=graph_log_dir)


        optimizer = config['optimizer']



        #boundaries = [2000, 100000, 1000000, 160000]
        #values = [0.05, 0.01, 0.001, 0.005, 0.0001]
        #learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(
        #    boundaries, values)



        if optimizer == 'ADADELTA':
            optimizer = tf.keras.optimizers.Adadelta(learning_rate)
        elif optimizer == 'ADAGRAD':
            optimizer = tf.keras.optimizers.Adagrad(learning_rate)
        elif optimizer == 'ADAM':
            optimizer = tf.keras.optimizers.Adam(learning_rate)
        elif optimizer == 'ADAMAX':
            optimizer = tf.keras.optimizers.Adamax(learning_rate)
        elif optimizer == 'FTRL':
            optimizer = tf.keras.optimizers.Ftrl(learning_rate)
        elif optimizer == 'NADAM':
            optimizer = tf.keras.optimizers.Nadam(learning_rate)
        elif optimizer == 'RMSPROP':
            optimizer = tf.keras.optimizers.RMSprop(learning_rate)
        elif optimizer == 'SGD':
            optimizer = tf.keras.optimizers.SGD(learning_rate, momentum=0.9)
            #optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate, momentum=0.90)
        else:
            raise ValueError('Invalid optimization algorithm')



        ckpt = tf.train.Checkpoint(backbone=model.backbone, model=model, optimizer=optimizer)
        ckpt_manager = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep=5, checkpoint_name='mymodel')

        if ckpt_manager.latest_checkpoint:
            ckpt.restore(ckpt_manager.latest_checkpoint)
            print(""Restored from {}"".format(ckpt_manager.latest_checkpoint))
        else:
            print(""Initializing from scratch."")
        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction=tf.losses.Reduction.NONE)

        def arcface_loss_multi(x, normx_cos, labels, batch_size, m=0.5, s=64):
            norm_x = tf.norm(x, axis=1, keepdims=True)
            cos_theta = normx_cos / norm_x
            theta = tf.acos(cos_theta)
            mask = tf.one_hot(labels, depth=normx_cos.shape[-1])
            zeros = tf.zeros_like(mask)
            cond = tf.where(tf.greater(theta + m, math.pi), zeros, mask)
            cond = tf.cast(cond, dtype=tf.bool)
            m1_theta_plus_m3 = tf.where(cond, theta + m, theta)
            cos_m1_theta_plus_m3 = tf.cos(m1_theta_plus_m3)
            prelogits = tf.where(cond, cos_m1_theta_plus_m3, cos_m1_theta_plus_m3) * s

            loss = loss_object(labels, prelogits)

            #return tf.reduce_sum(loss) * (1. / batch_size)
            return tf.nn.compute_average_loss(loss, global_batch_size=batch_size)
        #test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
        #    ""test_accuracy"", dtype=tf.float32)


        def train_step(images,labels):


            with tf.GradientTape() as tape:
                prelogits,  norm_dense = model(images, training=True)

                loss = arcface_loss_multi(prelogits, norm_dense, labels, batch_size, m3, s)

            gradients = tape.gradient(loss, model.trainable_variables)

            optimizer.apply_gradients(zip(gradients, model.trainable_variables))
            return loss




        @tf.function
        def distributed_train_step(images,labels):
            per_replica_losses = strategy.experimental_run_v2(train_step, args=(images,labels))
            return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)


        train_dist_dataset = strategy.experimental_distribute_dataset(train_data)

        vd = Valid_Data(model, valid_data)
        max_step = 0
        for epoch in range(epoch_num):

            start = time.time()
            # Train
            print(""Starting epoch {}"".format(epoch))
            total_loss = 0.0
            num_batches = 0

            for step, (images, labels) in enumerate(train_dist_dataset):

                if step > max_step:
                    max_step = step

                optimizer.learning_rate = decay((max_step*epoch) + step)


                loss = distributed_train_step(images,labels)

                with train_summary_writer.as_default():
                    tf.compat.v2.summary.scalar('loss', loss, step=((max_step*epoch) + step))

                    print('epoch: {}, step: {}, loss = {}'.format(epoch, ((max_step*epoch) + step), loss))
                total_loss += loss
                num_batches += 1

            train_loss = total_loss / num_batches

            print(""Training loss: {:0.4f}"".format(train_loss))

            with train_summary_writer.as_default():
                tf.compat.v2.summary.scalar('loss_epoch', train_loss, step=epoch)


            #if epoch % 1 == 0:
            # valid

            #with tf.device('/CPU:0'):
            '''
            acc, p, tpr, fpr, thresh, acc_fpr, p_fpr, tpr_fpr, thresh_fpr = vd.get_metric(below_fpr)

            with valid_summary_writer.as_default():
                tf.compat.v2.summary.scalar('acc', acc, step=epoch)
                tf.compat.v2.summary.scalar('p', p, step=epoch)
                tf.compat.v2.summary.scalar('tpr', tpr, step=epoch)
                tf.compat.v2.summary.scalar('fpr', fpr, step=epoch)
                tf.compat.v2.summary.scalar('thresh', thresh, step=epoch)
                tf.compat.v2.summary.scalar('acc_fpr', acc_fpr, step=epoch)
                tf.compat.v2.summary.scalar('p_fpr', p_fpr, step=epoch)
                tf.compat.v2.summary.scalar('tpr_fpr', tpr_fpr, step=epoch)
                tf.compat.v2.summary.scalar('thresh_fpr', thresh_fpr, step=epoch)
            print('epoch: {}, acc: {:.3f}, p: {:.3f}, r=tpr: {:.3f}, fpr: {:.3f} \n'
                  ',thresh: {:.3f}fix fpr <= {}, acc: {:.3f}, p: {:.3f}, r=tpr: {:.3f}, thresh: {:.3f}'
                  .format(epoch, acc, p, tpr, fpr, thresh, below_fpr, acc_fpr, p_fpr, tpr_fpr, thresh_fpr))
            '''
        # ckpt
        # if epoch % 5 == 0:
            save_path = ckpt_manager.save()
            print('Saving checkpoint for epoch {} at {}'.format(epoch, save_path))

            print('Time taken for epoch {} is {} sec\n'.format(epoch, time.time() - start))


if __name__ == '__main__':

    main()




Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32348,Unable to load model with custom loss function with tf.keras.models.load_model,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow==2.0.0rc0
- Python version: 2.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**

I have saved a simple feed-forward Keras model. When I try to load it with the following code, I get an error. 
```
from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape=(784,), name='digits')
x = layers.Dense(64, activation='relu', name='dense_1')(inputs)
x = layers.Dense(64, activation='relu', name='dense_2')(x)
outputs = layers.Dense(10, activation='softmax', name='predictions')(x)
model = keras.Model(inputs=inputs, outputs=outputs, name='3_layer_mlp')

# Useless custom loss here
def custom_loss(y_true, y_pred):
    return keras.backend.mean(keras.backend.square(y_true - y_pred), axis=-1)

model.compile(loss=custom_loss, optimizer=keras.optimizers.RMSprop())

model.save(""model/"", save_format='tf')

# Here comes the bug:
new_model = keras.models.load_model('model/', custom_objects={'loss': custom_loss})
```
The error:
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File "".env/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py"", line 147, in load_model
>     return saved_model_load.load(filepath, compile)
>   File "".env/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py"", line 93, in load
>     model._training_config))  # pylint: disable=protected-access
>   File "".env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
>     result = method(self, *args, **kwargs)
>   File "".env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 340, in compile
>     self.loss, self.output_names)
>   File "".env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py"", line 1350, in prepare_loss_functions
>     loss_functions = [get_loss_function(loss) for _ in output_names]
>   File "".env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py"", line 1350, in <listcomp>
>     loss_functions = [get_loss_function(loss) for _ in output_names]
>   File "".env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py"", line 1086, in get_loss_function
>     loss_fn = losses.get(loss)
>   File "".env/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py"", line 1166, in get
>     return deserialize(identifier)
>   File "".env/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py"", line 1157, in deserialize
>     printable_module_name='loss function')
>   File "".env/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 210, in deserialize_keras_object
>     raise ValueError('Unknown ' + printable_module_name + ':' + object_name)
> ValueError: Unknown loss function:loss

**My understanding of the issue**

I checked in the TF code and I found the following:
- The function `deserialize_keras_object` from [generic_utils.py](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/utils/generic_utils.py#L169-L218) has indeed a `custom_objects` argument
- `deserialize` from [losses.py](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/losses.py#L1169-L1174) has this argument too
- `get` (from [losses.py](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/losses.py#L1177-L1190)), the function that calls `deserialize` does not fills in this argument
- So that, even though I give `custom_objects` to `load_model` function, it is not passed to `deserialize_keras_object` at the end.

Could someone check this issue and implement the needed changes for this to work?

Thank you for your help!

"
32347,[lite/micro] incorrect link of pre-generated files for STM32F746G Discovery Board,"The link <https://drive.google.com/open?id=1u46mTtAMZ7Y1aD-He1u3R8AE4ZyEpnOl>
on page `tensorflow/lite/experimental/micro/README.md` for `STM32F746G Discovery Board` links to keil project based files instead of `Make/GCC` based files."
32345,[lite/micro] Huge size of libtensorflow-microlite.a for arm,"
The file `tensorflow/lite/experimental/micro/README.md` says 
>The core runtime fits in 16KB on a Cortex M3, and with enough operators to run a speech keyword detection model, takes up a total of 22KB.

But after compiling `libtensorflow-microlite.a`, I get

```
ls -lh
total 512
-rw-r--r--  1 xxx  xxx   254K Sep  9 18:11 libtensorflow-microlite.a
```

which is an order of magnitude larger than `22KB`.

The command I used to compile it is:

```
make -j2 -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=foo microlite
```

The configuration file `lite/experimental/micro/tools/make/targets/foo_makefile.inc` is 
```
ifeq ($(TARGET), foo)

TARGET_TOOLCHAIN_PREFIX := arm-none-eabi-

COMMON_FLAGS := \
-O2 \
-fno-unwind-tables \
-fstack-reuse=all -ffunction-sections -fdata-sections -Wl,--gc-sections -nostartfiles \
-mthumb -mcpu=cortex-m4 -march=armv7e-m -mfloat-abi=hard -mfpu=fpv4-sp-d16 \
-mno-thumb-interwork -ffast-math  \
-Wall -Werror -Wlogical-op -Waddress -Wempty-body -Wpointer-arith \
-Wenum-compare  -fno-strict-aliasing \
-Wno-sign-compare \
-nostdlib \
-fno-exceptions \
-fno-unwind-tables \
-fno-builtin

CXXFLAGS += $(COMMON_FLAGS) -fno-rtti
CCFLAGS += $(COMMON_FLAGS)

endif
```

The compiler toolchain information is as follows:

```
$ arm-none-eabi-gcc --version
arm-none-eabi-gcc (GNU Tools for ARM Embedded Processors) 5.3.1 20160307 (release) [ARM/embedded-5-branch revision 234589]
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```"
32344,[Tensorflow 2.0.0-rc0] Cannot save model on AI Platform: Not JSON Serializable,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google AI Platform
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary 
- TensorFlow version (use command below): Tensorflow 2.0.0-rc0
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When I try to save a trained model (Keras) locally with `keras.models.save_model(model, params['output_dir'] + '/savedmodel', save_format='tf')` it runs like expected. However, when I try to do the same on AI platform, it fails: `Not JSON Serializable`.

**Describe the expected behavior**
To be able to save a model on AI platform. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```The replica master 0 exited with a non-zero status of 1. Traceback (most recent call last): [...] File ""/root/.local/lib/python3.5/site-packages/model/model.py"", line 204, in start train_and_evaluate_model(data, params) File ""/root/.local/lib/python3.5/site-packages/model/model.py"", line 153, in train_and_evaluate_model '/savedmodel', save_format='tf') File ""/root/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/saving/save.py"", line 112, in save_model signatures) File ""/root/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py"", line 92, in save save_lib.save(model, filepath, signatures) File ""/root/.local/lib/python3.5/site-packages/tensorflow_core/python/saved_model/save.py"", line 874, in save saveable_view, asset_info.asset_index) File ""/root/.local/lib/python3.5/site-packages/tensorflow_core/python/saved_model/save.py"", line 629, in _serialize_object_graph _write_object_proto(obj, obj_proto, asset_file_def_index) File ""/root/.local/lib/python3.5/site-packages/tensorflow_core/python/saved_model/save.py"", line 668, in _write_object_proto metadata=obj._tracking_metadata) File ""/root/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training.py"", line 2839, in _tracking_metadata metadata = json.loads(super(Model, self)._tracking_metadata) File ""/root/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2410, in _tracking_metadata return json.dumps(metadata, default=serialization.get_json_type) File ""/usr/lib/python3.5/json/__init__.py"", line 237, in dumps **kw).encode(obj) File ""/usr/lib/python3.5/json/encoder.py"", line 198, in encode chunks = self.iterencode(o, _one_shot=True) File ""/usr/lib/python3.5/json/encoder.py"", line 256, in iterencode return _iterencode(o, 0) File ""/root/.local/lib/python3.5/site-packages/tensorflow_core/python/util/serialization.py"", line 68, in get_json_type raise TypeError('Not JSON Serializable:', obj) TypeError: ('Not JSON Serializable:', b'011') To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=675828645530&resource=ml_job%2Fjob_id%2Frgs_ml_v1_1567614514&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22rgs_ml_v1_1567614514%22```"
32343,[Tensorflow 2.0.0-rc0] Hyperparameter tuning with Keras fails ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOs 10.14.6, but trying to do hyperparameter tuning on Google AI platform
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): Tensorflow 2.0.0-rc0
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When I try to run a hyperparameter tuning job on AI Platform, the job fails. I use tensorflow callbacks: `callbacks = [
		keras.callbacks.TensorBoard(log_dir=params[""output_dir""],
									   histogram_freq=1)]`. Tensorboard runs just fine. However, I get the error: 
```AttributeError: module 'tensorflow._api.v2.train' has no attribute 'SummaryWriter' ```. Is there another way in Tensorflow 2.0 to use hyperparameter tuning?

**Describe the expected behavior**
Able to do hyperparameter tuning. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Just simply trying to do hyperparameter tuning on AI platform with the callback in the model above. 

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```Hyperparameter Tuning Trial #1 Failed before any other successful trials were completed. The failed trial had parameters: amsgrad=0, batch_size=16, beta_1=0.85743346214294436, learning_rate=0.037045532170408492, . The trial's error message was: The replica master 0 exited with a non-zero status of 1. Traceback (most recent call last): [...] During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/usr/lib/python3.5/runpy.py"", line 184, in _run_module_as_main ""__main__"", mod_spec) File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code exec(code, run_globals) File ""/root/.local/lib/python3.5/site-packages/model/task.py"", line 11, in <module> from . import model File ""/root/.local/lib/python3.5/site-packages/model/model.py"", line 7, in <module> import tensorflow as tf File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked File ""<frozen importlib._bootstrap>"", line 664, in _load_unlocked File ""<frozen importlib._bootstrap>"", line 634, in _load_backward_compatible File ""/usr/local/lib/python3.5/dist-packages/wrapt/importer.py"", line 158, in load_module notify_module_loaded(module) File ""/usr/local/lib/python3.5/dist-packages/wrapt/decorators.py"", line 440, in _synchronized return wrapped(*args, **kwargs) File ""/usr/local/lib/python3.5/dist-packages/wrapt/importer.py"", line 136, in notify_module_loaded hook(module) File ""/var/sitecustomize/sitecustomize.py"", line 192, in run_post_import_hook self.new_add_summary) File ""/usr/local/lib/python3.5/dist-packages/wrapt/wrappers.py"", line 827, in wrap_function_wrapper return wrap_object(module, name, FunctionWrapper, (wrapper,)) File ""/usr/local/lib/python3.5/dist-packages/wrapt/wrappers.py"", line 773, in wrap_object (parent, attribute, original) = resolve_path(module, name) File ""/usr/local/lib/python3.5/dist-packages/wrapt/wrappers.py"", line 765, in resolve_path original = getattr(original, attribute) File ""/root/.local/lib/python3.5/site-packages/tensorflow_core/python/util/module_wrapper.py"", line 169, in __getattr__ attr = getattr(self._tfmw_wrapped_module, name) AttributeError: module 'tensorflow._api.v2.train' has no attribute 'SummaryWriter' To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=675828645530&resource=ml_job%2Fjob_id%2Frgs_ml_hyper_v1_1567614786&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22rgs_ml_hyper_v1_1567614786%22```
"
32341,Keras model with sequence feature columns fails to convert to estimator,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-beta1-5101-gc75bb66 2.0.0-rc0
- Python version: 3
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: Unknown, from Colab
- GPU model and memory: Unknown, from Colab


**Describe the current behavior**
When i rewrote my estimator-based model (Dataset + Feature columns) to keras i was able to run (train) it.

But when i converted it to estimator as shown in migration guide https://www.tensorflow.org/beta/guide/migration_guide#estimators it fails with error
```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-7-6538390f0054> in <module>()
      1 estimator = tf.keras.estimator.model_to_estimator(
----> 2   keras_model = model
      3 )

9 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    235       except Exception as e:  # pylint:disable=broad-except
    236         if hasattr(e, 'ag_error_metadata'):
--> 237           raise e.ag_error_metadata.to_exception(e)
    238         else:
    239           raise

ValueError: in converted code:
    relative to /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/feature_column:

    sequence_feature_column.py:140 call
        transformation_cache, self._state_manager)
    feature_column_v2.py:3205 get_sequence_dense_tensor
        self.categorical_column))

    ValueError: In embedding_column: words_embedding. categorical_column must be of type SequenceCategoricalColumn to use SequenceFeatures. Suggested fix: Use one of sequence_categorical_column_with_*. Given (type <class 'tensorflow.python.feature_column.feature_column_v2.HashedCategoricalColumn'>): HashedCategoricalColumn(key='words', hash_bucket_size=1000, dtype=tf.string)
```

**Describe the expected behavior**
Working keras model should always be convertable to estimator.

**Code to reproduce the issue**
Here is an example https://colab.research.google.com/drive/11pWyltRzvQPPvM2dWQ8EqxafB9ByCxNn
There are 2 cases shown. First i show that keras model with sequence features is working. See `model.fit_generator`.
And then i show that conversion to estimator fails.
"
32340,AttributeError: 'RefVariable' object has no attribute 'ref',"with tf.device('/cpu:0'):
  activations = tf.Variable(tf.zeros([ACT_LEN, FLAGS.wvs, 1], FTYPE), name='activations',
                            trainable=False)
logits = batch_logits(i_ss, activations.ref())
AttributeError: 'RefVariable' object has no attribute 'ref'"
32336,pip install tensorflow from .whl not working,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Windows 7
- TensorFlow installed from .whl file downloaded: tensorflow-1.14.0-cp36-cp36m-win_amd64.whl
- I am behind a firewall (hence downloaded .whl)
- Python version 3.6.4
- Anaconda navigator environment

**Describe the problem**
I am now getting the error:
Could not find a version that satisfies the requirement keras-preprocessing >=1.0.5 (from tensorflow ==1.14.0) (from versions: )
No matching distribution found for keras-preprocessing >= 1.0.5 (from tensorflow == 1.14.0)

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I tried 'pip install tensorflow-1.14.0-cp36-cp36m-win_amd64.whl'

**Any other info / logs**
Before I was getting an error about google-pasta instead of keras-preprocessing. 
"
32335,"For long labels / logits, torch.nn.CTCloss is 30-100x faster than tf.nn.ctc_loss","**System information**
Default Google Colab GPU runtime

**Describe the current behavior**
For long labels / logits, tf.nn.ctc_loss is unbearably slow, in a way that makes whole training very slow (tf.nn.ctc_loss_v2 is even slower)
A standard GPU implementation with unbounded label length such as one offered by torch.nn.CTCloss is many many times faster

**Code to reproduce the issue**
The following code is used for the benchmark

```
from torch import nn
import torch
import tensorflow as tf
import math
import time

time_step  = 5  # Input sequence length
vocab_size = 200  # Number of classes
batch_size = 4  # Batch size
target_sql = 3  # Target sequence length

iters = 6
warm  = 3
DENSE = False

def dense_to_sparse(dense_tensor, sequence_length):
    indices = tf.where(tf.sequence_mask(sequence_length))
    values = tf.gather_nd(dense_tensor, indices)
    shape = tf.shape(dense_tensor, out_type=tf.int64)
    return tf.SparseTensor(indices, values, shape)


ctc_loss = nn.CTCLoss(blank=vocab_size - 1, reduction='none')

for j in range(25):
  time_step  *= 2
  target_sql *= 2
  print('Logits length : ',time_step,'Label length : ',target_sql)
  print('-------------------')
  
  x = torch.randn(time_step, batch_size, vocab_size).log_softmax(2).detach().requires_grad_().cuda()
  y = torch.randint(low=0, high=vocab_size-2, size=(batch_size, target_sql),dtype=torch.long).cuda()

  x_lengths = torch.full(size=(batch_size,), fill_value=time_step, dtype=torch.long).cuda()
  y_lengths = torch.full(size=(batch_size,), fill_value=target_sql, dtype=torch.long).cuda()

  s=0
  tlss=0.
  for i in range(iters):
      torch.cuda.synchronize()
      a = time.perf_counter()
      loss = ctc_loss(x.log_softmax(2), y, x_lengths, y_lengths).mean()
      torch.cuda.synchronize()
      b = time.perf_counter()
      tlss += float(loss)
      del loss
  
      if i>=warm:
          s += b - a

  torch.cuda.empty_cache()
  avtorch = s/(iters-warm)
  print('Average torch Time : %.6f'%(avtorch),'Loss : %.3f'%(tlss/iters))

  ###################################
  ###################################
  
  x = x.detach().cpu().numpy()
  y = y.detach().cpu().numpy()
  x_lengths = x_lengths.detach().cpu().numpy()
  y_lengths = y_lengths.detach().cpu().numpy()

  if not DENSE:
    y = tf.cast(dense_to_sparse(y, y_lengths), dtype=tf.int32)
  else:
    y = tf.cast(y, dtype=tf.int32)


  _x = tf.placeholder(tf.float32, [None, None, None])

  if not DENSE:
    ctclosses = tf.nn.ctc_loss(y, _x, x_lengths)
  else:
    ctclosses = tf.nn.ctc_loss_v2(y, _x, [target_sql]*batch_size, x_lengths, blank_index=-1)

  ctclosses = tf.reduce_mean(ctclosses)

  with tf.Session() as sess:
      s=0
      flss = 0
      for i in range(iters):
          a = time.perf_counter()
          loss = sess.run(ctclosses, feed_dict={_x: x})
          b = time.perf_counter()
          flss += float(loss)
          if i>=warm:
            s += b - a
      
      avtf = s/(iters-warm)
      print('Average tf Time    : %.6f'%(avtf),'Loss : %.3f'%(flss/iters))

  print('Loss diff : %.6f'%(abs(tlss-flss)))
  print('torch.nn.CTCloss Speed-up : %.2f'%(avtf/avtorch),'\n\n')

```
and get the following results
```
Logits length :  10 Label length :  6
-------------------
Average torch Time : 0.000162 Loss : 47.414
Average tf Time    : 0.001151 Loss : 47.414
Loss diff : 0.000046
torch.nn.CTCloss Speed-up : 7.09 


Logits length :  20 Label length :  12
-------------------
Average torch Time : 0.000173 Loss : 95.054
Average tf Time    : 0.001166 Loss : 95.054
Loss diff : 0.000092
torch.nn.CTCloss Speed-up : 6.74 


Logits length :  40 Label length :  24
-------------------
Average torch Time : 0.000172 Loss : 183.101
Average tf Time    : 0.001589 Loss : 183.101
Loss diff : 0.000092
torch.nn.CTCloss Speed-up : 9.23 


Logits length :  80 Label length :  48
-------------------
Average torch Time : 0.000252 Loss : 363.414
Average tf Time    : 0.003907 Loss : 363.414
Loss diff : 0.000000
torch.nn.CTCloss Speed-up : 15.52 


Logits length :  160 Label length :  96
-------------------
Average torch Time : 0.000580 Loss : 727.621
Average tf Time    : 0.008144 Loss : 727.621
Loss diff : 0.000366
torch.nn.CTCloss Speed-up : 14.05 


Logits length :  320 Label length :  192
-------------------
Average torch Time : 0.001026 Loss : 1446.162
Average tf Time    : 0.025436 Loss : 1446.162
Loss diff : 0.001465
torch.nn.CTCloss Speed-up : 24.80 


Logits length :  640 Label length :  384
-------------------
Average torch Time : 0.001742 Loss : 2894.562
Average tf Time    : 0.098221 Loss : 2894.562
Loss diff : 0.004395
torch.nn.CTCloss Speed-up : 56.38 


Logits length :  1280 Label length :  768
-------------------
Average torch Time : 0.006307 Loss : 5772.526
Average tf Time    : 0.379303 Loss : 5772.524
Loss diff : 0.011719
torch.nn.CTCloss Speed-up : 60.14 


Logits length :  2560 Label length :  1536
-------------------
Average torch Time : 0.025078 Loss : 11538.377
Average tf Time    : 1.546986 Loss : 11538.381
Loss diff : 0.023438
torch.nn.CTCloss Speed-up : 61.69 


Logits length :  5120 Label length :  3072
-------------------
Average torch Time : 0.043300 Loss : 23038.340
Average tf Time    : 6.084921 Loss : 23038.336
Loss diff : 0.023438
torch.nn.CTCloss Speed-up : 140.53 


Logits length :  10240 Label length :  6144
-------------------
Average torch Time : 0.312641 Loss : 46052.559
Average tf Time    : 23.677528 Loss : 46052.535
Loss diff : 0.140625
torch.nn.CTCloss Speed-up : 75.73 
```"
32333,Tensorflow 2.0.0-rc0 - incorrect data shape for SparseCategoricalCrossentropy,"Hi,

I'm trying to train a classifier using tensorflow.keras that predicts categorical labels (0/1/2). Input data is an ndarray of tf.float32. I used tensorflow=2.0.0-beta0 and then tensorflow=2.0.0-rc0 to produce the error messages below.

The output seems to be in the correct form as per [SparseCategoricalCrossentropy](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) :   
Input : [7 1]    
Output: [7 3]  
Where batch size is 7.


tensorflow=2.0.0-rc0:  
```python
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-5-fe1cc3e1dca1> in <module>
     47 print(model.summary())
     48 
---> 49 model.fit(train.batch(BATCH_SIZE), epochs=EPOCHS, verbose=2)
     50 model.evaluate(train, steps=None, verbose=1)

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    732         max_queue_size=max_queue_size,
    733         workers=workers,
--> 734         use_multiprocessing=use_multiprocessing)
    735 
    736   def evaluate(self,

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    322                 mode=ModeKeys.TRAIN,
    323                 training_context=training_context,
--> 324                 total_epochs=epochs)
    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    326 

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--> 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     84     # `numpy` translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---> 86                               distributed_function(input_fn))
     87 
     88   return execution_function

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    437         # Lifting succeeded, so variables are initialized and we can run the
    438         # stateless function.
--> 439         return self._stateless_fn(*args, **kwds)
    440     else:
    441       canon_args, canon_kwds = \

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   1820     """"""Calls a graph function specialized to the inputs.""""""
   1821     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 1822     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1823 
   1824   @property

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-> 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1222     if executing_eagerly:
   1223       flat_outputs = forward_function.call(
-> 1224           ctx, args, cancellation_manager=cancellation_manager)
   1225     else:
   1226       gradient_name = self._delayed_rewrite_functions.register()

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    509               inputs=args,
    510               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 511               ctx=ctx)
    512         else:
    513           outputs = execute.execute_with_cancellation(

/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

~/Library/Python/3.7/lib/python/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError:  assertion failed: [] [Condition x == y did not hold element-wise:] [x (loss/dense_3_loss/SparseSoftmaxCrossEntropyWithLogits/Shape_1:0) = ] [7 1] [y (loss/dense_3_loss/SparseSoftmaxCrossEntropyWithLogits/strided_slice:0) = ] [7 3]
	 [[node loss/dense_3_loss/SparseSoftmaxCrossEntropyWithLogits/assert_equal/Assert/Assert (defined at /usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_2031]

Function call stack:
distributed_function
```


With tensorflow=2.0.0-beta0:  
```python
InvalidArgumentError:  logits and labels must have the same first dimension, got logits shape [7,3] and labels shape [7]
	 [[node loss/dense_2_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-3-4fa88a5fad5e>:70) ]] [Op:__inference_keras_scratch_graph_5038]
```




code to reproduce:
```python
import numpy as np
import pandas as pd
import random
import tensorflow as tf
import time as tm

INPUT_SHAPE=[3, 5]
NUM_POINTS=20
BATCH_SIZE=7
EPOCHS=4

def data_gen(num, in_shape):
    for i in range(num):
        x = np.random.rand(in_shape[0], in_shape[1])
        y = random.randint(0,2)
        yield x, y
        
def data_gen_all(num, in_shape, num_labels):
    x = np.zeros([num]+in_shape)
    y = np.zeros([num]+[num_labels])
    for i in range(num):
        x[i,:,:]= np.random.rand(in_shape[0], in_shape[1])
        y[i]= tf.one_hot(random.randint(0, num_labels), num_labels).numpy()
    return x, y

train = tf.data.Dataset.from_generator(
    generator=data_gen,
    output_types=(tf.float32, tf.int32),
#     output_shapes=(tf.TensorShape([None, INPUT_SHAPE[1]]), tf.TensorShape(None)),
#     output_shapes=(tf.TensorShape(INPUT_SHAPE), tf.TensorShape(())),
    output_shapes=([None, INPUT_SHAPE[1]],()),
    args=([NUM_POINTS, INPUT_SHAPE])
)

def create_model(input_shape):
    model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(100, activation=""tanh"",input_shape=input_shape),        
        tf.keras.layers.Dense(3, activation=""softmax"", kernel_regularizer= tf.keras.regularizers.l2(0.001))
    ])
    return model

model = create_model(input_shape=INPUT_SHAPE)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, clipvalue=1.0),
    loss= tf.keras.losses.SparseCategoricalCrossentropy(),
#     loss= tf.keras.losses.CategoricalCrossentropy()
    )
print(model.summary())
model.fit(train.batch(BATCH_SIZE), epochs=EPOCHS, verbose=2)
model.evaluate(train, steps=None, verbose=1)

### CategoricalCrossentropy
x,y = data_gen_all(num=20, in_shape=INPUT_SHAPE, num_labels=3)
print(x.shape)
model.fit(x=x, y=y, epochs=EPOCHS, verbose=2)
```


https://stackoverflow.com/questions/57842734/tensorflow-2-0-incrrect-data-shape-for-sparsecategoricalcrossentropy

Platform: MacOS 10.14.6  
tensorflow=2.0.0-rc0  
tf_env_collect.sh produced:
```
== check python ===================================================
python version: 3.7.4
python branch: 
python build version: ('default', 'Jul  9 2019 18:13:23')
python compiler version: Clang 10.0.1 (clang-1001.0.46.4)
python implementation: CPython


== check os platform ===============================================
os: Darwin
os kernel version: Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64
os release version: 18.7.0
os platform: Darwin-18.7.0-x86_64-i386-64bit
linux distribution: ('', '', '')
linux os distribution: ('', '', '')
mac version: ('10.14.6', ('', '', ''), 'x86_64')
uname: uname_result(system='Darwin', node='chnb.local', release='18.7.0', version='Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64', machine='x86_64', processor='i386')
architecture: ('64bit', '')
machine: x86_64


== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 10.0.1 (clang-1001.0.46.4)
Target: x86_64-apple-darwin18.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== check pips ===================================================
numpy                1.17.2              
protobuf             3.9.1               
tensorflow           2.0.0b0             

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.0.0-beta0
tf.version.GIT_VERSION = v1.12.1-3259-gf59745a381
tf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.5)
/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./collect.sh: line 147: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 2.0.0b0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /usr/local/lib/python3.7/site-packages
Required-by: 

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(2, 7, 16, 'final', 0)

== bazel version  ===============================================
Build label: 0.29.0-homebrew
Build time: Wed Aug 28 18:07:41 2019 (1567015661)
Build timestamp: 1567015661
Build timestamp as int: 1567015661
```
"
32331,"Poisson random variable samples float instead of int, fails in hierarchical modeling as result","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, derived from tf example.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14.4
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- **Python version**: Python 3.7.3
- **Exact command to reproduce**: I am implementing a hierarchical model, where the value of the Poisson sample is used to determine the batch size of the normal distribution. Since the Poisson sample is a float, the MCMC step function treats it as a float. This results in incorrect mathematics for MCMC to work. In general, I cannot get this code to run.  

```python
import tensorflow as tf
import tensorflow_probability as tfp
tfd = tfp.distributions
tfb = tfp.bijectors

rv_m = tfd.Poisson(rate=1.)
m    = rv_m.sample()

def joint_log_prob(m):
    rv_m    = tfd.Poisson(rate=1.)
    m_int   = tf.cast(m,'int32')
    rv_norm = tfd.Normal(loc=tf.zeros(m_int),scale=1.)
    return (-1.)

number_of_steps = 1000
burnin          = 0

initial_chain_state      = [m]
log_prob                 = lambda *args: joint_log_prob(*args)
[post_m], kernel_results = tfp.mcmc.sample_chain(num_results=number_of_steps,num_burnin_steps=burnin,current_state=initial_chain_state,kernel=tfp.mcmc.RandomWalkMetropolis(target_log_prob_fn=log_prob,seed=4))
```
### Source code / logs
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow_probability/python/mcmc/sample.py"", line 361, in sample_chain
    parallel_iterations=parallel_iterations)
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow_probability/python/mcmc/internal/util.py"", line 370, in trace_scan
    parallel_iterations=parallel_iterations)
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3501, in while_loop
    return_same_structure)
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3012, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2937, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow_probability/python/mcmc/internal/util.py"", line 359, in _body
    state = loop_fn(state, elems_array.read(i))
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow_probability/python/mcmc/sample.py"", line 345, in _trace_scan_fn
    parallel_iterations=parallel_iterations)
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow_probability/python/mcmc/internal/util.py"", line 286, in smart_for_loop
    parallel_iterations=parallel_iterations
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3501, in while_loop
    return_same_structure)
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3012, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2937, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow_probability/python/mcmc/internal/util.py"", line 284, in <lambda>
    body=lambda i, *args: [i + 1] + list(body_fn(*args)),
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow_probability/python/mcmc/random_walk_metropolis.py"", line 440, in one_step
    return self._impl.one_step(current_state, previous_kernel_results)
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow_probability/python/mcmc/metropolis_hastings.py"", line 194, in one_step
    previous_kernel_results.accepted_results)
  File ""/Users/dcunha/mutatio/lib/python3.7/site-packages/tensorflow_probability/python/mcmc/random_walk_metropolis.py"", line 533, in one_step
    dtype=next_target_log_prob.dtype.base_dtype),
AttributeError: 'float' object has no attribute 'dtype'
```
"
32330,java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: TfLiteGpuDelegate Invoke: Delegate should run on the same thread where it was initialized.Node number 175 (TfLiteGpuDelegate) failed to invoke,"In the android image classifier demo, I tried to run the Interpreter with GPU from start so I changed [this line](https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/android/app/src/main/java/org/tensorflow/lite/examples/classification/tflite/Classifier.java#L175-L189):

```
  protected Classifier(Activity activity, Device device, int numThreads) throws IOException {
    tfliteModel = loadModelFile(activity);
    switch (device) {
      case NNAPI:
        tfliteOptions.setUseNNAPI(true);
        break;
      case GPU:
        gpuDelegate = new GpuDelegate();
        tfliteOptions.addDelegate(gpuDelegate);
        break;
      case CPU:
        break;
    }

    tfliteOptions.setNumThreads(numThreads);
    tflite = new Interpreter(tfliteModel, tfliteOptions);
```

to this:

```
  protected Classifier(Activity activity, Device device, int numThreads) throws IOException {
    tfliteModel = loadModelFile(activity);
    gpuDelegate = new GpuDelegate();
    tfliteOptions.addDelegate(gpuDelegate);
    tfliteOptions.setNumThreads(numThreads);
    tflite = new Interpreter(tfliteModel, tfliteOptions);
```

sounds simple, but it crashes with the above mentioned error.
Weirdly: When I start the app in CPU mode and switch to GPU while running (via the options-sheet) it does work.

Is the GpuDelegate class using some custom thread? "
32329,TensorFlow installation apt commands is wrong,"## URL(s) with the issue:

https://www.tensorflow.org/install/gpu

## Description of issue (what needs changing):

1. change your language to chinese simplified from the upright corner.
2. in the apt installation commands for Ubuntu 18.04 (CUDA 10), last step shown below is wrong:

```
  # Install TensorRT. Requires that libcudnn7 is installed above.
    sudo apt-get update && \
            && sudo apt-get install -y --no-install-recommends libnvinfer-dev=5.1.5-1+cuda10.0
```

1.  there is a syntax error--> && \ &&
2.  libnvinfer5=5.1.5-1+cuda10.0  should be installed first before libnvinfer-dev=5.1.5-1+cuda10.0
3.  just change the commands to the same as what is shown in english-language website.


"
32328,Use keras .fit() method to train adversarial models,"TF2.0.0-RC0 keras question:

Because GANs and adversarial models require training two models with different optimizers simultaneously, I don't think this can be achieved currently without a custom training loop, applying only the model.fit() methods. Is this feature already available because I haven't found it in the documentations? To clarify, the feature I would need is to create the generative and the discriminator models, compile the generative model with optimizer_A, compile the discriminator model with optimizer_B, now train them simultaneously but alternately, eg. one batch with discriminator then one batch with generative (using the output of the discriminator model of course for the loss), and do this without having to write a custom training loop. This is the way to go with GANs, but the examples I see all use custom train steps. The problem with custom train steps have limited access to many of the keras features such as saving models, monitoring, etc."
32327,I don't like TensorFlow,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
I used tensorflow and PyTorch and Facebook is the winner of this race
PyTorch is the most simple library and tensorFlow is the most difficult"
32326,cannot import tensorflow,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `import tensorflow as tf`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): `pip install tensorflow`
- TensorFlow version (use command below): 
- Python version: 3.7
- CUDA/cuDNN version:cuda 8.0
- GPU model and memory:quadro 4000

**Describe the current behavior**
i have installed cuda 8.0 because my GPU support 8.0 but i think this version of tensor flow require cuda 10.0 so i want to downgrade my tensorflow which support cuda 8.0
**Describe the expected behavior**

**Code to reproduce the issue**
ImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive

Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
```
Traceback (most recent call last):
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\first\lib\site-packages\tensorflow\python\platform\self_check.py"", line 75, in preload_check
    ctypes.WinDLL(build_info.cudart_dll_name)
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\first\lib\ctypes\__init__.py"", line 364, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/Mohsin Ali/PycharmProjects/first/tensor.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\first\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\first\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\first\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Users\Mohsin Ali\Anaconda3\envs\first\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive
```
"
32323,Build fails on latest master with clang and CUDA,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: b78ed2c5973ddebb2589558d722ea78da238e815
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): clang 6.0.0-1ubuntu2
- CUDA/cuDNN version: 10.2
- GPU model and memory: NVIDIA Tesla K80 11441MiB

**Describe the problem**
> **FAILED: Build did NOT complete successfully**
**Provide the exact sequence of commands / steps that you executed before running into the problem**

Build script:
https://github.com/offscale/offregister-tensorflow/blob/5bf7a19f6b5056362442177128dbae92304ef6c3/offregister_tensorflow/ubuntu/utils.py#L148

Specifically:
```
./configure
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```
(fails on that second line)

Environment variables set (+ `PYTHON_LIB_PATH` and `PYTHON_BIN_PATH`):
```
{'CC_OPT_FLAGS': '-march=native',
 'CLANG_CUDA_COMPILER_PATH': '/usr/bin/clang',
 'LD_LIBRARY_PATH': '/usr/local/cuda/extras/CUPTI/lib64',
 'TF_CUDA_CLANG': '1',
 'TF_CUDA_COMPUTE_CAPABILITIES': '3.7',
 'TF_DOWNLOAD_CLANG': '0',
 'TF_DOWNLOAD_MKL': '1',
 'TF_ENABLE_XLA': '0',
 'TF_NEED_COMPUTECPP': '0',
 'TF_NEED_CUDA': '1',
 'TF_NEED_GCP': '0',
 'TF_NEED_GDR': '0',
 'TF_NEED_HDFS': '0',
 'TF_NEED_JEMALLOC': '1',
 'TF_NEED_KAFKA': '0',
 'TF_NEED_MKL': '1',
 'TF_NEED_MPI': '0',
 'TF_NEED_OPENCL': '0',
 'TF_NEED_OPENCL_SYCL': '0',
 'TF_NEED_ROCM': '0',
 'TF_NEED_S3': '0',
 'TF_NEED_TENSORRT': '1',
 'TF_NEED_VERBS': '0',
 'TF_SET_ANDROID_WORKSPACE': '0'}
```


**Any other info / logs**
```

ERROR: /home/ubuntu/repos/tensorflow-for-py3/tensorflow/core/kernels/BUILD:2518:1: output 'tensorflow/core/kernels/_objs/dynamic_stitch_op_gpu/dynamic_stitch_op_gpu.cu.pic.o' was not created
[8,641 / 8,655] 4 actions running
    Compiling tensorflow/core/kernels/sparse_matmul_op.cc; 16s local
    Compiling tensorflow/core/kernels/dynamic_stitch_op_gpu.cu.cc; 12s local
    Compiling tensorflow/core/kernels/data/cache_dataset_ops.cc; 7s local
    Compiling tensorflow/core/kernels/fifo_queue_op.cc; 4s local





ERROR: /home/ubuntu/repos/tensorflow-for-py3/tensorflow/core/kernels/BUILD:2518:1: not all outputs were created or valid
[8,641 / 8,655] 4 actions running
    Compiling tensorflow/core/kernels/sparse_matmul_op.cc; 16s local
    Compiling tensorflow/core/kernels/dynamic_stitch_op_gpu.cu.cc; 12s local
    Compiling tensorflow/core/kernels/data/cache_dataset_ops.cc; 7s local
    Compiling tensorflow/core/kernels/fifo_queue_op.cc; 4s local





[8,645 / 8,655] checking cached actions

Target //tensorflow/tools/pip_package:build_pip_package failed to build
[8,645 / 8,655] checking cached actions

Use --verbose_failures to see the command lines of failed build steps.
[8,645 / 8,655] checking cached actions

INFO: Elapsed time: 3087.912s, Critical Path: 114.34s
[8,645 / 8,655] checking cached actions

INFO: 5399 processes: 5399 local.
[8,645 / 8,655] checking cached actions

FAILED: Build did NOT complete successfully

FAILED: Build did NOT complete successfully
```"
32322,compute_mask not called when custom layer does not alter inputs and mask is not None,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-beta
- Python version: 3.7.3-2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When the layer does not alter the inputs (but alters the mask), if `mask` is not `None`, the layer does not execute the `compute_mask` method. 
**Describe the expected behavior**
Call `compute_mask` and attach the mask to output regardless.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import tensorflow as tf
import numpy as np

class RandomMaskingNoAlter(tf.keras.layers.Layer):

  def call(self, inputs):
    return inputs

  def compute_mask(self, inputs, mask=None):
    print('no alter executed')
    if mask is None:
      return None
    random_mask = tf.cast(tf.random.uniform(tf.shape(mask), 0, 1, dtype=tf.int32), tf.bool)
    return tf.math.logical_and(random_mask, mask)

class RandomMaskingAlter(tf.keras.layers.Layer):
    
  def call(self, inputs):
    return inputs + 0

  def compute_mask(self, inputs, mask=None):
    print('alter executed')
    if mask is None:
      return None
    random_mask = tf.cast(tf.random.uniform(tf.shape(mask), 0, 1, dtype=tf.int32), tf.bool)
    return tf.math.logical_and(random_mask, mask)

x = np.array([[1, 4, 2, 2, 0, 0], [1, 1, 1, 0, 0, 0], [3, 2, 2, 3, 4, 1]], dtype='i4')
y = tf.keras.layers.Embedding(5, 5, mask_zero=True)(x)

z1 = RandomMaskingNoAlter()(x)
z2 = RandomMaskingNoAlter()(y)
z3 = RandomMaskingAlter()(x)
z4 = RandomMaskingAlter()(y)
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32320,keras model.evaluate() progress bar WAY too long by default,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): just testing this https://www.tensorflow.org/tutorials/keras/basic_classification
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 fully updated
- TensorFlow installed from (source or binary): `pip3 install --user tensorflow-gpu==2.0.0-rc0`
- TensorFlow version (use command below): `v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0`
- Python version: 3.7.4
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce GTX 1660 Ti, 6 GB

**Describe the current behavior**
Just running a basic image classifier with Keras. Import data, create model, train, evaluate.
I run `python script-name.py` in the Command Prompt.
`model.evaluate()` prints out an insanely long progress bar at the end. It's many, MANY pages long, with Command Prompt already maximized (so one page is already a lot of characters). I have to scroll WAAAY UP to see the previous output.

**Describe the expected behavior**
I know I could turn off verbosity, but I would expect sane defaults for the progress bars printed by TF/Keras. And with `verbose=1` that thing is so huge, it's useless.

**Code to reproduce the issue**
```python
from __future__ import absolute_import, division, print_function, unicode_literals

# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt
from pprint import pprint

# CUDA vs CPU
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""

print(tf.__version__)

# load train/test data
fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# print shape/size for train/test data
print(train_images.shape, len(train_labels), test_images.shape, len(test_labels))

# show first image
plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
#plt.show()

# normalize pixel values (0...1)
train_images = train_images / 255.0
test_images = test_images / 255.0

# show first 25 images, sanity check
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
#plt.show()

# build the model
# flat 1D layer
# dense 128-node layer
# dense softmax output layer
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation=tf.nn.relu),
    keras.layers.Dense(10, activation=tf.nn.softmax)
])

# compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# train the model
model.fit(train_images, train_labels, epochs=5)

# evaluate the model
test_loss, test_acc = model.evaluate(test_images, test_labels)
print('Test accuracy:', test_acc)
```
"
32319,Module 'gast' has no attribute 'Num',"I got an error message including a request that I report a bug.

Here's the code:
```python
import numpy as np
import tensorflow as tf
import platform

print()
print(f""PLATFORM:\n---------\n{platform.platform()}"")
print(""\nTENSORFLOW:\n----------"")
for a in tf.version.__all__:
    print(f""{a}: {getattr(tf.version, a)}"")

print(f""\nNUMPY:\n-----\n{np.version.version}"")

print(f""\nPYTHON:\n-------\n{sys.version}\n"")

np.random.seed(0)
tf.random.set_seed(0)


model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(1, activation=""linear"")
])
model.compile(optimizer=""sgd"", loss=""mse"")

x = np.random.uniform(size=(1,1))
y = np.random.uniform(size=(1,))
model.fit(x, y, epochs=1)
```

And here's the output, including system info, module versions, etc.
```

PLATFORM:
---------
Darwin-18.7.0-x86_64-i386-64bit

TENSORFLOW:
----------
COMPILER_VERSION: 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.5)
GIT_VERSION: v2.0.0-beta1-5101-gc75bb66a99
GRAPH_DEF_VERSION: 119
GRAPH_DEF_VERSION_MIN_CONSUMER: 0
GRAPH_DEF_VERSION_MIN_PRODUCER: 0
VERSION: 2.0.0-rc0

NUMPY:
-----
1.17.1

PYTHON:
-------
3.7.4 (default, Jul  9 2019, 18:13:23) 
[Clang 10.0.1 (clang-1001.0.46.4)]

Train on 1 samples
WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1492748c0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'
WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x1492748c0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'
1/1 [==============================] - 0s 157ms/sample - loss: 1.2336

<tensorflow.python.keras.callbacks.History at 0x14927b5d0>
```"
32318,AttributeError: 'str' object has no attribute 'keys',"

**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: v2.0.0-beta1-5101-gc75bb66 2.0.0-rc0
- Python version: 3.7
- CUDA/cuDNN version: 10.0
- GPU model and memory: Nvidia rtx 2070

**Describe the current behavior**
I want to visualize the embeddings of my model but when i add:
```
embeddings_freq=1,
embeddings_metadata='metadata.tsv'
```
in the tensorboard callbacks
i am getting an error.

**Code to reproduce the issue**
```
import tensorflow as tf
from os import makedirs
from os.path import exists
import numpy as np

model_dir = ""tmp/model_mnist/""
log_dir = ""logs_mnist/""
if not exists(log_dir):
    makedirs(log_dir)
data_format = ('channels_first' if tf.test.is_built_with_cuda() else 'channels_last')

batch_size = 128
num_classes = 10
epochs = 12

# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

if data_format == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# save class labels to disk to color data points in TensorBoard accordingly
with open(log_dir + 'metadata.tsv', 'w') as f:
    np.savetxt(f, y_test, fmt=""%d"")

# convert class vectors to binary class matrices
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),
                                 activation='relu',
                                 input_shape=input_shape))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(0.25))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))

model.compile(loss=tf.keras.losses.categorical_crossentropy,
              optimizer=tf.keras.optimizers.Adadelta(lr=1.),
              metrics=['accuracy'])

callbacks = tf.keras.callbacks.TensorBoard(log_dir=log_dir,
                                           embeddings_freq=1,
                                           embeddings_metadata='metadata.tsv')

model.fit(x_train, y_train,
          batch_size=batch_size,
          callbacks=[callbacks],
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

```

**Other info / logs**

> Traceback (most recent call last):
  File ""/home/chris/Desktop/tests/mnist.py"", line 75, in <module>
    validation_data=(x_test, y_test))
  File ""/home/chris/miniconda3/envs/mdl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 734, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/home/chris/miniconda3/envs/mdl/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 292, in fit
    mode=ModeKeys.TRAIN)
  File ""/home/chris/miniconda3/envs/mdl/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py"", line 103, in configure_callbacks
    callback_list.set_model(callback_model)
  File ""/home/chris/miniconda3/envs/mdl/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py"", line 217, in set_model
    callback.set_model(model)
  File ""/home/chris/miniconda3/envs/mdl/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py"", line 1509, in set_model
    self._configure_embeddings()
  File ""/home/chris/miniconda3/envs/mdl/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py"", line 1536, in _configure_embeddings
    'argument: ' + str(self.embeddings_metadata.keys()))
AttributeError: 'str' object has no attribute 'keys'
"
32315,Support for 32 bits architecture,"Several users are still using Python32 bits and they cannot install TensorFlow. For them, `pip install tensorflow` fails as no wheel matches the tags expected by their environment (to debug, `pip debug --verbose` shows only tags that don't math the filenames of our wheels).

There is some requests to support 32 bits, see for example #31431 

This is not going to be easy as we need to also compile the C++ codebase in 32 bits mode and that would cause issues with code written assuming types have a certain bit width.

There is no change in the user visible API, just a new set of wheels to support more users.

Opening this to reference in all similar issues."
32314,Cannot load fashion_mnist dataset,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12
- Python version: 3.6.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0/7.5
- GPU model and memory: GeForce 940M



**Describe the problem**
when I try to use the fashion_mnist dataset, it starts to read the data from the website, but after a while, the website shuts down my connection because I am connecting too often (WinError 10054).


**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
import tensorflow as tf
from tensorflow import keras

fashion_mnist = keras.datasets.fashion_mnist

(train_images, train_labels),(test_images, test_labels) = fashion_mnist.load_data()
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

(it's loading...)

```
Traceback (most recent call last):
  File ""C:\Users\admin\Desktop\working as intended\python pls\love_copying_code_instead_of_writing_it_urself.py"", line 6, in <module>
    (train_images, train_labels),(test_images, test_labels) = fashion_mnist.load_data()
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\datasets\fashion_mnist.py"", line 52, in load_data
    paths.append(get_file(fname, origin=base + fname, cache_subdir=dirname))
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\utils\data_utils.py"", line 249, in get_file
    urlretrieve(origin, fpath, dl_progress)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\urllib\request.py"", line 277, in urlretrieve
    block = fp.read(bs)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\http\client.py"", line 449, in read
    n = self.readinto(b)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\http\client.py"", line 493, in readinto
    n = self.fp.readinto(b)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\socket.py"", line 586, in readinto
    return self._sock.recv_into(b)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\ssl.py"", line 1009, in recv_into
    return self.read(nbytes, buffer)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\ssl.py"", line 871, in read
    return self._sslobj.read(len, buffer)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\ssl.py"", line 631, in read
    v = self._sslobj.read(len, buffer)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。
```"
32313,tensorflow.python.framework.errors_impl.FailedPreconditionError: TensorRT is not enabled!,"I am trying to use tensorflow-1.10 to optimize my pb model.
trt_graph = trt.create_inference_graph(
   input_graph_def = graph_def,
   outputs = outputs,
   max_batch_size=1,
   max_workspace_size_bytes=workspace_size,
   precision_mode='INT8',
   minimum_segment_size=3)
"
32312,tensorflow.python.framework.errors_impl.FailedPreconditionError: TensorRT is not enabled!,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
32311,LayerNormalization fails when given tuple as axis input,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-4.14.137+-x86_64-with-Ubuntu-18.04-bionic
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Google colab has it preinstalled
- TensorFlow version (use command below): 1.14.0
- Python version: (major, minor, micro, releaselevel, serial) (3, 6, 8, 'final', 0)
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: CUDA Version: 10.1
- GPU model and memory: Tesla K80 11441MiB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
```python
Sequential([Input(shape=(64, 64, 3), dtype=np.float32), LayerNormalization(axis=(-3, -2, -1))])
```
fails with error
```text
---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

<ipython-input-22-18267fdf3265> in <module>()
----> 1 Sequential([Input(shape=(64, 64, 3), dtype=np.float32), LayerNormalization(axis=(-3, -2, -1))])

6 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in build(self, input_shape)
    957     for idx, x in enumerate(self.axis):
    958       if x < 0:
--> 959         self.axis[idx] = ndims + x
    960 
    961     # Validate axes

TypeError: 'tuple' object does not support item assignment
```

This is because the lines:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/normalization.py#L929-L930
```python
    if isinstance(axis, (list, tuple)):
      self.axis = axis[:]
```
make a copy of the tuple instead of converting it to a list and later the lines:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/normalization.py#L954-L956
```python
    # Convert axis to list and resolve negatives
    if isinstance(self.axis, int):
      self.axis = [self.axis]
```
do not take care of the case when axis is a tuple.

**Describe the expected behavior**
Fix is simple, replace the lines:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/normalization.py#L954-L956
with:
```python
    # Convert axis to list and resolve negatives
    if isinstance(self.axis, int):
      self.axis = [self.axis]
    elif isinstance(self.axis, tuple):
      self.axis = list(axis)
```
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
Sequential([Input(shape=(64, 64, 3), dtype=np.float32), LayerNormalization(axis=(-3, -2, -1))])
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32310,ModuleNotFoundError: No module named 'tensorflow' ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WIndows 7 64 bit
- TensorFlow installed from (source or binary): from : https://www.tensorflow.org/install/pip
------------------------------------------------------------------------------------------------------
the error generated while running -python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""- code mentioned in the site to verify the install. 

tensorflow —Latest stable release for CPU-only (recommended for beginners), from tesnorflow website
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here

(venv) C:\Users\sharone\AppData\Local\Programs\Python\Python36>python -c ""import
 tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow'
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32309,Tensorflow Micro: Build issue,"`micro_vision` and `micro_speech` examples are showing following error on invoking makefile to build
```
/bin/sh: 1: [[: not found
```

My build command is:
```
make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=linux micro_vision
```
The patch to fix this issue is:
```
diff --git a/tensorflow/lite/experimental/micro/tools/make/Makefile b/tensorflow/lite/experimental/micro/tools/make/Makefile
index d2ed2e6234..2779530d02 100644
--- a/tensorflow/lite/experimental/micro/tools/make/Makefile
+++ b/tensorflow/lite/experimental/micro/tools/make/Makefile
@@ -1,3 +1,4 @@
+SHELL := /bin/bash
 
 ifneq (3.82,$(firstword $(sort $(MAKE_VERSION) 3.82)))
   $(error ""Requires make version 3.82 or later (current is $(MAKE_VERSION))"")
```

Tensorflow repo info:
```
commit 92fc3d02fd3cd086e17659842bfee1bbd62d1838 (HEAD -> master)
Author: Tiezhen WANG <wangtz@google.com>
Date:   Thu Sep 5 07:10:01 2019 -0700

    TFLM: Better format for debug log in `screen`
    
    Otherwise, it's showing up like
    msg1
        msg2
            msg3
    
    PiperOrigin-RevId: 267365236
```
OS Platform and Distribution: Linux Ubuntu 18.04.3 LTS
GCC/Compiler version: 7.3.1
"
32299,RNN stateful is incompatible with initial_state,"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): binary via conda (1.14.0) and pip (2.0.0-rc0)
- TensorFlow version (use command below): 1.14.0 and 2.0.0-rc0
- Python version: 3.6
- CUDA/cuDNN version: 10.0
- GPU model and memory: 1080 Ti

**Describe the current behavior**
When a stateful RNN is created and is given initial_state, it effectively resets the state to the initial state for every prediction. See the ""NOT EXPECTED"" example in the code below.

**Describe the expected behavior**
The stateful RNN should use the initial state the first time, and then update the state and use it for each following time.

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import backend as K
import numpy as np

INPUT_SIZE = 4
BATCH_SIZE = 1
OUTPUT_SIZE = 2

# Define a model without any state initialization
input_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)
rnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer)
model = keras.Model(input_layer, rnn_layer)
model.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')

test = np.full((BATCH_SIZE,1,INPUT_SIZE), 0.5)

# This generates different predictions for the two time steps, as expected
model.reset_states()
print(model.predict(test))
print(model.predict(test))
# [[[0.15479106 0.3035699 ]]]
# [[[0.22833839 0.4250579 ]]]

# This generates the same prediction after resetting the state, as expected
model.reset_states()
print(model.predict(test))
model.reset_states()
print(model.predict(test))
# [[[0.15479106 0.3035699 ]]]
# [[[0.15479106 0.3035699 ]]]

# Define a model with a constant state initialization
input_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)
initial_state_layer = K.constant(np.ones(OUTPUT_SIZE), shape=(1, OUTPUT_SIZE))
rnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer, initial_state=initial_state_layer)
model = keras.Model(input_layer, rnn_layer)
model.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')

# This generates the same prediction for the two time steps, NOT EXPECTED
model.reset_states()
print(model.predict(test))
print(model.predict(test))
# [[[0.23247536 0.8585994 ]]]
# [[[0.23247536 0.8585994 ]]]

# This generates the same prediction after resetting the state, as expected
model.reset_states()
print(model.predict(test))
model.reset_states()
print(model.predict(test))
# [[[0.23247536 0.8585994 ]]]
# [[[0.23247536 0.8585994 ]]]
```
"
32294,Unclear error,"When I ran converter from tf to tf lite I have this error:
`F tensorflow/lite/toco/graph_transformations/propagate_array_data_types.cc:305] Check failed: op->inputs.size() > 0 (0 vs. 0)`
Or sometimes this error:
`F tensorflow/lite/toco/tooling_util.cc:644] Check failed: dim >= 1 (0 vs. 1)`

Can you explain, what is this errors mean?
Can you make errors in tflite converter more readable?
"
32293,AttributeError: module 'tensorflow' has no attribute 'optimizers',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): tf-nightly-2.0-preview
- TensorFlow version (use command below):
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
```
>>> import tensorflow as tf
>>> tf.optimizers
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'optimizers'
>>> tf.losses
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'losses'```
```

**Describe the expected behavior**
`tf.optimizers`, `tf.losses` were present.

cc @martinwicke "
32291,[TF.1.14] Train on TPU Pod with tf.keras ,"Can I train on a TPU Pod -not a single device- with tf.keras and how on TF1.14?

Thank you"
32290,UnsatisfiedLinkError for libtensorflow_jni.so when trying to load a newer model ,"### System information
See attached [tf_env.txt](https://github.com/tensorflow/tensorflow/files/3584174/tf_env.txt) generated by `tf_env_collect.sh`.

tf.version.VERSION **1.14.0**
tf.version.GIT_VERSION unknown

### Describe the problem
We have been successfully training Tensorflow models and loading them into Java project for inference. We have been training models using Tensorflow [1.8.0 - 1.10.0] and loading them into a gradle project compiled with 1.8.0. It worked.
```
dependencies {

    compile(
            ""org.tensorflow:tensorflow:"" + ""1.8.0"",
            ""com.google.guava:guava:"" + guavaVersion,
            ""com.opencsv:opencsv:"" + openCSVVersion,
    )
```
Recently we have retrained the models using Tensorflow **1.14.0**. After replacing the older models and raising the gradle version number to 1.14.0
```
dependencies {

    compile(
            ""org.tensorflow:tensorflow:"" + ""1.14.0"",
            ""com.google.guava:guava:"" + guavaVersion,
            ""com.opencsv:opencsv:"" + openCSVVersion,
    )
```

one of the tests that attempts to load the model started throwing the following exception below:
```
java.lang.UnsatisfiedLinkError: /tmp/tensorflow_native_libraries-1567776737834-0/libtensorflow_jni.so: /tmp/tensorflow_native_libraries-1567776737834-0/libtensorflow_jni.so: undefined symbol: _ZN11tensorflow10FileSystem20RecursivelyCreateDirERKSs

	at java.lang.ClassLoader$NativeLibrary.load(Native Method)
	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824)
	at java.lang.Runtime.load0(Runtime.java:809)
	at java.lang.System.load(System.java:1086)
	at org.tensorflow.NativeLibrary.load(NativeLibrary.java:101)
	at org.tensorflow.TensorFlow.init(TensorFlow.java:66)
	at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)
	at org.tensorflow.SavedModelBundle.<clinit>(SavedModelBundle.java:170)
```

This does not look like a bug issue but it would be nice if you could provide some tips on how to fix this on our end. Thank you.
"
32289,Training a trivial keras model is not even close to deterministic despite call to set_random_seed.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): pip (binary)
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.5
- CUDA/cuDNN version:
- GPU model and memory:
output of tf_env_collect.sh:
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3584142/tf_env.txt)

**Describe the current behavior**
Training a trivial one layer keras model is not deterministic.

**Describe the expected behavior**
I would like to initialize the seeds and train deterministically, at least up to floating point precision. 

We are having severe problems with non-deterministic network behavior with more complex models (for both training an inference), and I minimized my example all the way down to a single layer. 

I have included a failing test case:

**Code to reproduce the issue**

```
import numpy
import tensorflow
from tensorflow import Session, global_variables_initializer, local_variables_initializer
from tensorflow.python import debug as tf_debug
from tensorflow.compat.v1 import Session
from tensorflow.compat.v1.keras.backend import set_session
from tensorflow.keras.layers import Conv2D, Input
from tensorflow.keras.models import Model

def call_in_new_session():
    import random
    random.seed(0)

    tensorflow.set_random_seed(1234)

    config = tensorflow.ConfigProto(
        device_count={'CPU': 1},
      intra_op_parallelism_threads=1,
      inter_op_parallelism_threads=1)
    session = Session(config=config)

    set_session(session)
    with session.as_default():
        print('---------------New Tensorflow Sesssion--------------------')
        model_shape = [1024,1024,3]

        numpy.random.seed(0)
        X = numpy.random.randn(1,*model_shape) # batch dimension 1
        y = numpy.random.randn(1,*model_shape)
        #print(X.sum()) # Numpy is determinisic here.

        tensorflow.compat.v1.random.set_random_seed(1234)

        i = Input(shape=model_shape, name='i')
        c = Conv2D(filters=1,kernel_size=(1,1))(i)
        model_2d = Model(inputs=i,outputs=c)

        model_2d.compile(loss='mean_squared_error', optimizer='adam')
        # Weights still not initialized; contain garbage data.
        
        loss = model_2d.train_on_batch([X,y])
        # loss is not determinisic here.

        weights = model_2d.get_weights()[0]
        print(""OOOOOO weights sum:"",weights.sum())
        print(""OOOOOO loss sum:"",loss.sum())

        return model_2d.predict_on_batch(X)

def test_deterministic_between_sessions():
    result1 = call_in_new_session()
    result2 = call_in_new_session()
    assert numpy.allclose(result1, result2, atol=1e-5), ""Results were different when run in a fresh session!""

if __name__=='__main__':
    test_deterministic_between_sessions()

```
"
32288,"C api - Program failing during training when compiling with optimizations flag (-O2) or when libtensorflow is built with optimized instructions (AVX SSE), or when keras model is compiled with ""sparse_categorical_crossentropy""","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Mixed setup. Keras model is built based on stock example. Training happens in custom C code that adapts an existing TF example
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04, CentOS 7.6 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Both, binary and source. Keras model is built using python package installed with pip. Training program uses libtensorflow compiled from source
- TensorFlow version (use command below): 2.0
- Python version: 3.6(Ubuntu), 3.7(CentOS)
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4(Ubuntu) and 9.1(CentOS)
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
My env:
    [tf_env.txt](https://gist.github.com/7PintsOfCherryGarcia/31247fa9e421bddd158272c01786911e)


**Describe the current behavior**
I am using TF 2.0 [C api](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/c/c_api.h) to recreate the tutorial in [here](https://www.tensorflow.org/beta/tutorials/keras/basic_classification)

Keras model is built using python package as indicated in the tutorial with slight modifications. ""categorical_crossentropy"" is used as loss function instead of ""sparse_categorical_crossentropy"" as the latter produces the error referenced in this issue. Model is saved using keras.experimental.export_saved_model.


Model is saved in a folder called ""saved_model"", **name of this folder** is **hard coded** in C program.

I link against libtensorflow.so.2.0.0 and libtensorflow_framework.so.2.0.0 as compiled with the following command:

`./configure; bazel build -c opt //tensorflow/tools/lib_package:libtensorflow` (no cuda support)

**Describe the expected behavior**
If compiled **without** optimization (-O<num>) program runs normally and model can be trained successfully. As seen can be observed in the output.
If compiled **with** optimization (-O<num>) program fails at training step.
If libtensorflow is **compiled with AVX,SSE** support, program fails at training step.
If keras model is compiled using ""sparse_categorical_crossentropy"", program fails at training step.
All modes of failure end with the same error (see below)

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Model is build using 

The steps I follow are:
1) Create keras model and save it to a file.
python [createModel.py](https://gist.github.com/7PintsOfCherryGarcia/094e68585be228e42efc5764a97ab5d7)

2) Download training data and store in two files: data.txt and labels.txt, the names of the input data are **hard coded in C program**. To download data:
python [getData.py](https://gist.github.com/7PintsOfCherryGarcia/455370c9550a6c5c10f9135155a8b282)

3) Run C [program](https://gist.github.com/7PintsOfCherryGarcia/a5935bc0410e64bca2a8f252620eff92), the program will read the saved model, load training data and labels. Make predictions using the untrained model, train the model, run predictions again.
LD_LIBRARY_PATH=/path/to/libtensorflow/lib ./[CFashionMnist](https://gist.github.com/7PintsOfCherryGarcia/a5935bc0410e64bca2a8f252620eff92) The program will print predicted values as well as loss values during training.

Program is compiled with:
For normal execution
  gcc -Wall -I /path/to/libtensorflow/include -L /path/to/libtensorflow/lib -o CFashionMnist CFashionMnist.c -ltensorflow
For failing execution
  gcc -Wall **-O2** -I /path/to/libtensorflow/include -L /path/to/libtensorflow/lib -o CFashionMnist CFashionMnist.c -ltensorflow

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

When compiled with optimizations flags (-O<num>) I get:

2019-09-06 14:16:51.617721: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2019-09-06 14:16:51.642262: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1696290000 Hz
2019-09-06 14:16:51.642842: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557c6f2b89a0 executing computations on platform Host. Devices:
2019-09-06 14:16:51.642893: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-09-06 14:16:51.643335: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: keras_model
2019-09-06 14:16:51.648699: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { train }
2019-09-06 14:16:51.662135: I tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.
2019-09-06 14:16:51.693512: I tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: keras_model
2019-09-06 14:16:51.712088: I tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { train }; Status: success. Took 68754 microseconds.
**ERROR: Node 'dense_1_target' (type: 'Placeholder', num of outputs: 1) does not have output 2044864112**

If I build libtensorflow with support for optimized instructions I get the same error minus the optimized instructions warning. "
32286,Simple `model.evaluate()` example floods output with `=` characters,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  - **Fedora 30**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
  - **(binary): pip install tensorflow==2.0.0-rc0**
- TensorFlow version (use command below):
  - **v2.0.0-beta1-5101-gc75bb66 2.0.0-rc0**
- Python version:
  - **Python 3.7.4**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Running the code, I get an output flooded with hundreds of thousands of `=` characters when calling `model.evaluate()`.

```
Train on 60000 samples
Epoch 1/5
60000/60000 [==============================] - 3s 43us/sample - loss: 0.5010 - accuracy: 0.8228
Epoch 2/5
60000/60000 [==============================] - 2s 38us/sample - loss: 0.3766 - accuracy: 0.8639
Epoch 3/5
60000/60000 [==============================] - 2s 38us/sample - loss: 0.3408 - accuracy: 0.8753
Epoch 4/5
60000/60000 [==============================] - 2s 37us/sample - loss: 0.3155 - accuracy: 0.8839
Epoch 5/5
60000/60000 [==============================] - 2s 38us/sample - loss: 0.2980 - accuracy: 0.8903
10000/1 [==================================================================
===========================================================================
===========================================================================
===========================================================================
===========================================================================
===========================================================================
===========================================================================
===========================================================================
...
... Literally hundreds of thousands of `=` ...
...
===========================================================================
===========================================================================
===========================================================================
===========================================================================
===========================================================================
===========================================================================
===========================================] - 0s 26us/sample - loss: 0.2803 - accuracy: 0.8673

```

**Describe the expected behavior**

```
Train on 60000 samples
Epoch 1/5
60000/60000 [==============================] - 3s 43us/sample - loss: 0.5010 - accuracy: 0.8228
Epoch 2/5
60000/60000 [==============================] - 2s 38us/sample - loss: 0.3766 - accuracy: 0.8639
Epoch 3/5
60000/60000 [==============================] - 2s 38us/sample - loss: 0.3408 - accuracy: 0.8753
Epoch 4/5
60000/60000 [==============================] - 2s 37us/sample - loss: 0.3155 - accuracy: 0.8839
Epoch 5/5
60000/60000 [==============================] - 2s 38us/sample - loss: 0.2980 - accuracy: 0.8903
10000/10000 [==============================] - 0s 26us/sample - loss: 0.2803 - accuracy: 0.8673
```

**Code to reproduce the issue**

```python
import tensorflow as tf

mnist = tf.keras.datasets.fashion_mnist
(training_images, training_labels), (test_images, test_labels) = mnist.load_data()
training_images = training_images / 255.0
test_images = test_images / 255.0
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(training_images, training_labels, epochs=5)

test_loss = model.evaluate(test_images, test_labels)
```

**Other info / logs**

Running this code in a Jupyter Notebook results in a performance penalty for the huge, unnecessary output.
"
32285,Model unable to take input from dictionary(with input layer names as key) after loading it using tf.keras.models.load_model,"**System information**
- OS Platform and Distribution (Linux Ubuntu 16.04):
device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0 rc
- Python version: 3.6

My model works fine the first time when i create the architecture and train it, but once i save the model using **tf.keras.model.save_model**  and then load it again using **tf.keras.models.load_model** it fails to take input from the  dictionary(with input layer names as key) as if the names of layers are changed.

Example:
training_mask = tf.keras.layers.Input(shape=(None, None, 1), name='training_mask')
target_score_map = tf.keras.layers.Input(shape=(None, None, 1), name='target_score_map')
target_geo_map = tf.keras.layers.Input(shape = (None,None,5), name='target_geo_map')

These are three input layers that i am using to take input for training but once i save the model and later load it again to train it gives me the error "" No data provided for ""input_1"". Need data for each key in: ['input_1', 'input_2', 'input_3']""

I am feeding data by specifying layer names :

        inputs = {
                'target_score_map': score_maps,
                'target_geo_map': geo_maps,
                'training_mask': training_masks
            }

Therefore, it works the first time when create I the architecture from code then train and save it but later when i load the model from disk it fails.

Regards
Shubham

"
32284,"how to use java implement functions,such as:Tensor、Graph、Operation etc.","@ry Could I implement funtions（Tensor） using java?
If yes, would you like to give me an example.
Thanks very much!"
32283,Commit 2f38d92fa854bb173bc866a89f5123e501cf35bd breaking unit test builds.,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: https://github.com/tensorflow/tensorflow/commit/2f38d92fa854bb173bc866a89f5123e501cf35bd problem occurs on master branch with and without --config=v2
- Python version: py2.7
- Installed using virtualenv? pip? conda?: docker tensorflow/tensorflow:devel
- Bazel version (if compiling from source): 0.26.1 from docker tensorflow/tensorflow:devel
- GCC/Compiler version (if compiling from source): tensorflow/tensorflow:devel (7.4.0)
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
> SUBCOMMAND: # //tensorflow/core/platform:windows_env_time_impl [action 'Compiling tensorflow/core/platform/windows/env_time.cc']
> (cd /jenkins/workspace/Nightly-Private-Tensorflow-Eigen/eigen_build/a638235ef1b5f50137189224e7b73e40/execroot/org_tensorflow && \
>   exec env - \
>     PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
>     PWD=/proc/self/cwd \
>     PYTHON_BIN_PATH=/usr/local/bin/python \
>     PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
>     TF_CONFIGURE_IOS=0 \
>   /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/tensorflow/core/platform/_objs/windows_env_time_impl/env_time.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/platform/_objs/windows_env_time_impl/env_time.pic.o' -fPIC -iquote . -iquote bazel-out/k8-opt/bin '-march=haswell' '-mtune=broadwell' -O3 -Wformat -Wformat-security -fstack-protector -fPIC -fpic '-std=c++14' '-D_GLIBCXX_USE_CXX11_ABI=0' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/core/platform/windows/env_time.cc -o bazel-out/k8-opt/bin/tensorflow/core/platform/_objs/windows_env_time_impl/env_time.pic.o)
> ERROR: /jenkins/workspace/Nightly-Private-Tensorflow-Eigen/tensorflow/tensorflow/core/platform/BUILD:51:1: C++ compilation of rule '//tensorflow/core/platform:windows_env_time_impl' failed (Exit 1)
> tensorflow/core/platform/windows/env_time.cc:19:10: fatal error: windows.h: No such file or directory
>  #include <windows.h>
>           ^~~~~~~~~~~


**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
$ docker run tensorflow/tensorflow:devel
# cd /tensorflow_src
# git pull
# yes """" | python configure.py
# bazel --nosystem_rc --nohome_rc --output_user_root=/root/eigen_build test --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-march=haswell --copt=-mtune=broadwell --copt=-O3 --copt=-Wformat --copt=-Wformat-security --copt=-fstack-protector --copt=-fPIC --copt=-fpic --linkopt=-znoexecstack --linkopt=-zrelro --linkopt=-znow --linkopt=-fstack-protector --test_timeout 300,450,1200,3600 --test_env=KMP_BLOCKTIME=0 -s --cache_test_results=no --test_size_filters=small,medium,large,enormous -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/... -//tensorflow/core:test_lite_main -//tensorflow/contrib/tensorrt/... -//tensorflow/stream_executor/cuda/... -//tensorflow/python/autograph/pyct/... -//tensorflow/core/kernels:eigen_mkldnn_contraction_kernel_test -//tensorflow/python/debug:dist_session_debug_grpc_test

```
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32281,The TF function for the TRT segment could not be empty,"--
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary):  when I convert pb to tensorrt, I use tf-nightly-gpu-1.15 from pip install. When I do infer, I use tf-1.14.0 from source.
- TensorFlow version (use command below): tf-1.14.0
- Python version: 3.5
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: cuda 10 cudnn 17
- GPU model and memory: 1060Ti


**Describe the current problem**
Hi, 
I successfully used trt_convert to my pb model to tensorRT plan 
and I want to use c++ to infer my model. 
So I use bazel complie tf with **tensorrt together**. 
In my code, pb model can run successfully. 
I use the same code, and TensorRT model can load successfully but as session running, tf give me the following error: 

2019-09-06 06:56:37.455586: E tensorflow/core/common_runtime/executor.cc:642] Executor failed to create kernel. Invalid argument: The TF function for the TRT segment could not be empty
	 [[{{node fa_layer4_c0/TRTEngineOp_108}}]]



When I convert pb to tensorrt, it is shown as following:

2019-09-06 06:25:59.150320: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-09-06 06:25:59.183609: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:831] TensorRT node fa_layer4/TRTEngineOp_107 added for segment 107 consisting of 3 nodes succeeded.
2019-09-06 06:25:59.189007: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:831] TensorRT node fa_layer4_c0/TRTEngineOp_108 added for segment 108 consisting of 2 nodes succeeded.
2019-09-06 06:25:59.189378: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.
2019-09-06 06:25:59.189403: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger Network must have at least one output
2019-09-06 06:25:59.189426: W tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:834] TensorRT node fa_layer4_c0/conv0/bn/cond_1/TRTEngineOp_109 added for segment 109 consisting of 4 nodes failed: Internal: Failed to build TensorRT engine. Fallback to TF...



Additionally, I can use the tensorrt model in python code to infer. (python installed from pip) 
Anyone could provide some ideas how to solve it?


"
32279,Clarification in the validation_data argument of fit() in nsl.keras.AdversarialRegularization,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0-rc0 
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**:

I am trying the newly released `neural_structured_learning` API. In order to do an experiment, I thought of starting with fine-tuning a VGG16 model and seeing the API's action in that case. I am interested in using the `validation_data` argument while calling `fit()` on a `nsl.keras.AdversarialRegularization` model. 

I tried to do two variants:

First one: 
```python
adv_model.fit({'feature': X_train, 'label': y_train},
                   validation_data=(X_val, y_val),
                   batch_size=128, epochs=2, verbose=1)
```

It throws:

```python
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-63-6a9e8b7f90e8> in <module>()
      2 h = adv_model.fit({'feature': X_train, 'label': y_train},
      3                   validation_data={'feature': X_val, 'label': y_val},
----> 4                   batch_size=128, epochs=2, verbose=1)
      5 print(""Took {0:.2f} seconds"".format(time.time() - start))

6 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/data_adapter.py in <genexpr>(.0)
    223       inputs = (x,)
    224 
--> 225     num_samples = set(int(i.shape[0]) for i in nest.flatten(inputs))
    226     if len(num_samples) > 1:
    227       msg = ""Data cardinality is ambiguous:\n""

IndexError: tuple index out of range
```

Second one:
```python
adv_model.fit({'feature': X_train, 'label': y_train},
                  validation_data={'feature': X_val, 'label': y_val},
                   batch_size=128, epochs=2, verbose=1)
```

It throws:

```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-68-158557e95fb0> in <module>()
      2 h = adv_model.fit({'feature': X_train, 'label': y_train},
      3                   validation_data=(X_val, y_val),
----> 4                   batch_size=128, epochs=2, verbose=1)
      5 print(""Took {0:.2f} seconds"".format(time.time() - start))

5 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    732         max_queue_size=max_queue_size,
    733         workers=workers,
--> 734         use_multiprocessing=use_multiprocessing)
    735 
    736   def evaluate(self,

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    222           validation_data=validation_data,
    223           validation_steps=validation_steps,
--> 224           distribution_strategy=strategy)
    225 
    226       total_samples = _get_total_number_of_samples(training_data_adapter)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in _process_training_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)
    561                                     class_weights=class_weights,
    562                                     steps=validation_steps,
--> 563                                     distribution_strategy=distribution_strategy)
    564     elif validation_steps:
    565       raise ValueError('`validation_steps` should not be specified if '

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in _process_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)
    591         batch_size=batch_size,
    592         check_steps=False,
--> 593         steps=steps)
    594   adapter = adapter_cls(
    595       x,

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2435           feed_input_shapes,
   2436           check_batch_axis=False,  # Don't enforce the batch size.
-> 2437           exception_prefix='input')
   2438 
   2439     # Get typespecs for the input data and sanitize it if necessary.

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    528                        'Expected to see ' + str(len(names)) + ' array(s), '
    529                        'but instead got the following list of ' +
--> 530                        str(len(data)) + ' arrays: ' + str(data)[:200] + '...')
    531     elif len(names) > 1:
    532       raise ValueError('Error when checking model ' + exception_prefix +

ValueError: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[[[132., 140.,  64.],
         [135., 142.,  64.],
         [140., 145.,  64.],
         ...,
         [145., 141.,  70.],
         [146., 134.,  86.],
         [148., 128., 100.]],

        [...
```

But when I fit the model without `validation_data`, it works fine. 

**Describe the expected behavior**

Either throw some light on the usage in the documentation or in the example.

**Code to reproduce the issue**

For experimentation, I use Colab, so here's the [Colab notebook](https://colab.research.google.com/drive/1vEIpwdmS9Uj_QVlZVqLT5ZQu77BgKNwX). "
32278,tf.keras.datasets.cifar10.load_data() cannot load lcoal files when cifar-10-batches-py mannully put to ~/.keras/dataset/,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0-beta1
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
- tf.keras.datasets.cifar10.load_data() cannot load lcoal files when cifar-10-batches-py mannully put to ~/.keras/dataset/. 
- It still try to download dataset online, while my network connection is restricted.
- similar behaviour happens when using tfds.load with data_dir setted
**Describe the expected behavior**
Load local cifarl dataset to numpy array without reporting any errors
**Code to reproduce the issue**
1. put downloaded  cifar-10-batches-py to ~/.keras/datasets/
2. code:
import tensorflow as tf
(train_images,train_labels), (_, _) = tf.keras.datasets.cifar10.load_data()

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32276,Memory keeps increasing with GradientTape,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0rc
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0.130
- GPU model and memory: GeForce RTX 208, memory 10G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I create a subclass model with a encoder layer and decoder layer. Encoder is to call tf bidirectional LSTM and decoder is just a dense layer. I train the model with GradientTape, looping to pass data with generator. The GPU memory keep increasing.  

**Describe the expected behavior**
The GPU memory usage should be stable after the 1st epoch.

**Code to reproduce the issue**
I minimize my code and save it to https://github.com/ChenYang-ChenYang/tf-memory-increase
You can just run train.py to reproduce.

The memory increase from 700M at epoch 1 to 1080M at epoch 6 in the sample project. My actual project have much more data and more complex model. The memory increase to 10G after some epochs and OOM, so I can't continue to train.

**Other info / logs**
If I change the sample project to use model.compile() and then fit(), instead of using GradientTape, the memory is stable.  But my actual project is complex, including seq2seq addons, multiple outputs and losses, so it is very difficult to change to use model.compile() and fit(). I have to use GradientTape.
It seems the memory is re-created and not released in each epoch or step.
By the way, I searched most of the tf memory issues, like https://github.com/tensorflow/tensorflow/issues/32052, https://github.com/tensorflow/tensorflow/issues/19385, https://github.com/tensorflow/tensorflow/issues/19671, none of them are the same or the solution doesn't work. The most common solution to release memory cache is to call tf.set_random_seed(1) in tf version 1.x. I tried to call tf.random.set_seed(1) in tf 2.0 but it didn't work. 

"
32273,SSD anchors in Tensorflow detection API,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Non
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: python2.7
- CUDA/cuDNN version: CUDA 10.0/ cuDNN 7.4.1
- GPU model and memory: 1080Ti, 12GB

**Describe the current behavior**
Default SSD_mobilenet_V2_config is below.
```
anchor_generator {
  ssd_anchor_generator {
    num_layers: 6
    min_scale: 0.2
    max_scale: 0.9
    aspect_ratios: 1.0
    aspect_ratios: 2.0
    aspect_ratios: 0.5
    aspect_ratios: 3.0
    aspect_ratios: 0.33
    }
}
image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
}
```
If I set image_resizer {width:600, height:450} (4:3 input size), would base_anchor_size be set square or rectangle? [link](https://github.com/tensorflow/models/blob/1af55e018eebce03fb61bba9959a04672536107d/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py#L213)
```
image_resizer {
      fixed_shape_resizer {
        height: 450
        width: 600
      }
}
```
I trace the ssd_anchor_create func code, it seems to set rectangle-like base_anchor_size, right?
```
    min_im_shape = tf.minimum(im_height, im_width)
    scale_height = min_im_shape / im_height
    scale_width = min_im_shape / im_width
    base_anchor_size = [
        scale_height * self._base_anchor_size[0],
        scale_width * self._base_anchor_size[1]
    ]
```

I want to ask how could I set specific size of anchor box for each layer?
How to check details of anchor bboxes after training model?
Thanks a lot

"
32271,XLA inference slower than plain TF inference for ResNet50 (0.084s vs 0.115s) .,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory: T4

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Do not see any  inference speedup  with  XLA enabled through TF_XLA_FLAGS=--tf_xla_auto_jit=2

**Describe the expected behavior**
Expected to see some improvement.  

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
# to run python ./xla_resnet2x.py --runtime TFXLACOMP
from tensorflow.contrib.slim.nets import resnet_v1
import tensorflow as tf
import tensorflow.contrib.slim as slim
import numpy as np
import time
import os
from PIL import Image
from tensorflow.contrib.compiler import xla
import argparse

PATH_TO_CKPT = ""resnet_v1_50.ckpt""
TEST_IMAGE_PATHS = [ ""elephant_small.jpg"", ""tabby_tiger_cat.jpg""]
BATCH_SIZE=25
MAX_BATCH_SIZE=1000
HEIGHT=224
WIDTH=224
CHANNELS=3


def load_image_into_numpy_array(image, batch_size=1):
  (im_width, im_height) = image.size
  x = np.array(image.getdata()).reshape(
      (HEIGHT, WIDTH, CHANNELS)).astype(np.uint8)
  x = np.expand_dims(x, axis=0)
  xsl = list (x.shape)
  xsl[0] = batch_size#MAX_BATCH_SIZE
  x = np.broadcast_to(x[0,:,:,:], xsl)
  return x

def run_resnet_50():
    # Create graph
    inputs = tf.placeholder(tf.float32, shape=[BATCH_SIZE, HEIGHT, WIDTH, CHANNELS])
    with slim.arg_scope(resnet_v1.resnet_arg_scope()):
            net, end_points = resnet_v1.resnet_v1_50(inputs, is_training=False)

    saver = tf.train.Saver()

    with tf.Session() as sess:
            saver.restore(sess, PATH_TO_CKPT)
            representation_tensor = sess.graph.get_tensor_by_name('resnet_v1_50/pool5:0') # if you don't know names like these, consider referring to corresponding model file or generate .pbtxt file as mentioned in  @civilman628 's answer above
            img = np.ones((batch_size, height, width, channels))   #load image here with size [1, 224,224, 3]
            features = sess.run(representation_tensor, {'Placeholder:0': img})
            print ( ""features"", features)

def renamed_ckpt_save(name):
    with tf.Session() as sess:# Restore the TF checkpoint

        for var_name, var_shape in tf.contrib.framework.list_variables(PATH_TO_CKPT):
            var = tf.contrib.framework.load_variable(PATH_TO_CKPT, var_name)
            new_name_parts = [name] + var_name.split('/')[1:]
            new_name = '/'.join(new_name_parts)
            var = tf.Variable(var, name=new_name)
            print var_name, var_shape, var.name

        ckpt_dir = ""/tmp/""+name
        if not os.path.exists(ckpt_dir):
            os.mkdir(ckpt_dir)
        sess.run(tf.global_variables_initializer())
        saver = tf.train.Saver()
        saver.save(sess, ckpt_dir)

def renamed_ckpt_mem(sess, name):

        for var_name, var_shape in tf.contrib.framework.list_variables(PATH_TO_CKPT):
            var = tf.contrib.framework.load_variable(PATH_TO_CKPT, var_name)
            new_name_parts = [name] + var_name.split('/')[1:]
            new_name = '/'.join(new_name_parts)
            var = tf.Variable(var, name=new_name)
            print var_name, var_shape, var.name

def build_graph_xla (inputs):

    with slim.arg_scope(resnet_v1.resnet_arg_scope()):
            net, end_points = resnet_v1.resnet_v1_50(inputs, is_training=False, scope=name)

    if True: #with input_graph.as_default():
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'pool5',
      ]:
        if name == '':
            tensor_name = key + ':0'
        else :
            tensor_name = name+'/'+key + ':0'
        if tensor_name in all_tensor_names:
                tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
                tensor_name)
                print ( ""tensor_name"", tensor_name, tensor_dict[key])
        else:
           print( ""tensor_name "", tensor_name, "" not found"")

    return tensor_dict

def restore_graph_xla (sess, name):
    # Restore the TF checkpoint
    saver = tf.train.Saver()
    saver.restore(sess, ""/tmp/""+name)

def build_graph (sess, input_graph, name='graph1'):

    inputs = tf.placeholder(tf.float32, shape=[BATCH_SIZE, HEIGHT, WIDTH, CHANNELS])

    with slim.arg_scope(resnet_v1.resnet_arg_scope()):
            net, end_points = resnet_v1.resnet_v1_50(inputs, is_training=False, scope=name)

    with input_graph.as_default():
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'pool5',
      ]:
        if name == '':
            tensor_name = key + ':0'
        else :
            tensor_name = name+'/'+key + ':0'
        if tensor_name in all_tensor_names:
                tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
                tensor_name)
                print ( ""tensor_name"", tensor_name, tensor_dict[key])
        else:
           print( ""tensor_name "", tensor_name, "" not found"")


    # Restore the TF checkpoint
    saver = tf.train.Saver()
    saver.restore(sess, ""/tmp/""+name)
    return tensor_dict

class TfXla():
    name = ''

    @staticmethod
    def build_graph_xla (inputs):

        with slim.arg_scope(resnet_v1.resnet_arg_scope()):
                net, end_points = resnet_v1.resnet_v1_50(inputs, is_training=False, scope=TfXla.name)

        if True: #with input_graph.as_default():
          # Get handles to input and output tensors
          ops = tf.get_default_graph().get_operations()
          all_tensor_names = {output.name for op in ops for output in op.outputs}
          tensor_dict = {}
          for key in [
              'pool5',
          ]:
            if TfXla.name == '':
                tensor_name = key + ':0'
            else :
                tensor_name = TfXla.name+'/'+key + ':0'
            if tensor_name in all_tensor_names:
                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
                    tensor_name)
                    print ( ""tensor_name"", tensor_name, tensor_dict[key])
            else:
               print( ""tensor_name "", tensor_name, "" not found"")

        return tensor_dict

    @staticmethod
    def restore_graph_xla (sess, name):
        # Restore the TF checkpoint
        saver = tf.train.Saver()
        saver.restore(sess, ""/tmp/""+name)

    @staticmethod
    def build(sess, input_graph, name='', import_name='import'):
        TfXla.name = name
        with input_graph.as_default():
            inputs = tf.placeholder(tf.float32, shape=[BATCH_SIZE, HEIGHT, WIDTH, CHANNELS])
            tensor_dict = xla.compile (TfXla.build_graph_xla, [inputs,])
            restore_graph_xla(sess, name)
        return tensor_dict

    @staticmethod
    def run(sess, tensor_dict, image_tensor, image_np_expanded):
        output_dict = sess.run(tensor_dict, feed_dict={image_tensor: image_np_expanded})
        return output_dict

class Tf():

    @staticmethod
    def build(sess, graph, name='', import_name=None):
        return build_graph(sess, graph, name=name)

    @staticmethod
    def run(sess, tensor_dict, image_tensor, image_np_expanded):
        output_dict = sess.run(tensor_dict, feed_dict={image_tensor: image_np_expanded})
        return output_dict
def run (runtime='TF', amp=False):

    if runtime=='TF':
        RunClass = Tf
        import_name1= ''
        import_name2= ''
        placeholder_name1 = 'Placeholder:0'
        placeholder_name2 = 'Placeholder_1:0'
    elif runtime=='TFXLACOMP':
        print (""Setting XLA compile"")
        RunClass = TfXla
        #import_name1 = 'import1'
        #import_name2 = 'import2'
        #placeholder_name1 = 'import1/Placeholder:0'
        #placeholder_name2 = 'import2/Placeholder_1:0'
        import_name1= ''
        import_name2= ''
        placeholder_name1 = 'Placeholder:0'
        placeholder_name2 = 'Placeholder_1:0'
    elif runtime=='TFXLA':
        os.environ['TF_XLA_FLAGS'] = ""--tf_xla_auto_jit=2""
        print (""Setting XLA auto clustering"")
        RunClass = Tf
        #import_name1 = 'import1'
        #import_name2 = 'import2'
        #placeholder_name1 = 'import1/Placeholder:0'
        #placeholder_name2 = 'import2/Placeholder_1:0'
        import_name1= ''
        import_name2= ''
        placeholder_name1 = 'Placeholder:0'
        placeholder_name2 = 'Placeholder_1:0'

    graph = tf.Graph()

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    #config.gpu_options.per_process_gpu_memory_fraction = 0.33
    if amp:
        config.graph_options.rewrite_options.auto_mixed_precision = 1
        print ""Enabling AMP""
    with tf.Session(graph=graph, config=config) as sess:
        #renamed_ckpt_mem(sess, 'resnet_v1_50_1')
        #renamed_ckpt_mem(sess, 'resnet_v1_50_2')
        tensor_dict1 = RunClass.build(sess, graph, name='resnet_v1_50_1', import_name=import_name1)
        #tensor_dict2 = tensor_dict1 RunClass.build(sess, graph, name='resnet_v1_50_2', import_name=import_name2)
        tensorboard_dir = os.environ['TENSORBOARD_DIR']
        file_writer = tf.summary.FileWriter(tensorboard_dir, sess.graph)
        image_path = TEST_IMAGE_PATHS[0]
        print ( ""image_path {}"".format(image_path))
        image = Image.open(image_path)
        image = image.resize((WIDTH, HEIGHT))
        image_np_expanded = load_image_into_numpy_array(image, batch_size=BATCH_SIZE)
        image_tensor1 = graph.get_tensor_by_name(placeholder_name1)
        #image_tensor2 = graph.get_tensor_by_name(placeholder_name2)
        time0 = time.time()
        for i in range(1,2001):
            output_dict1 = RunClass.run(sess, tensor_dict1, image_tensor1, image_np_expanded)
            #output_dict2 = RunClass.run(sess, tensor_dict2, image_tensor2, image_np_expanded)
            if i % 100 == 0:
                time_taken = (time.time() - time0 )/(i * 1.0)
                print (i, time_taken)
        time_taken = (time.time() - time0 )/(i * 1.0)
        print (""time_taken pb "", time_taken, "" pf "", time_taken/BATCH_SIZE , ""output_dict"", output_dict1, 'output_dict2')

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--runtime', default='TF', help='TF or TFXLA or TFXLACOMP')
    parser.add_argument('--amp', action='store_true', help='enable AMP')
    parser.add_argument('--rewrite_ckpt', action='store_true',  help='rename checkpoints')
    args = parser.parse_args()
    if args.rewrite_ckpt:
        renamed_ckpt_save('resnet_v1_50_1')
        renamed_ckpt_save('resnet_v1_50_2')

    run (args.runtime, args.amp)


```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32270,`tf.estimator` missing in 2019-09-05 nightlies (and broken in 2019-09-04),"#### System information

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux (like Debian)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `v1.12.1-10423-g11e22c0 2.0.0-dev20190905` (`tf-nightly-2.0-preview==2.0.0.dev20190905`)
- Python version: 3.6.8rc1
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

#### Describe the current behavior

The `tf.estimator` module appears not to exist:

```python
>>> import tensorflow as tf
>>> tf.estimator
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'estimator'
>>> tf.compat.v2.estimator
<module 'tensorflow_estimator.python.estimator.api._v2.estimator' from '/tmp/tmp.hdyrTvpKDw/ve/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/api/_v2/estimator/__init__.py'>
>>> tf.compat.v1.estimator
<module 'tensorflow_estimator.python.estimator.api._v1.estimator' from '/tmp/tmp.hdyrTvpKDw/ve/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py'>
>>> tf.estimator
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'estimator'
```

#### Describe the expected behavior

The `tf.estimator` module should exist, given that it [is documented in
the TF 2.x APIs][1] and has been in previous nightlies.

[1]: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator

#### Code to reproduce the issue

```
python -c '__import__(""tensorflow"").estimator`
```

#### Other info / logs

N/A
"
32268,TF.Keras model creation results in different output node when eager is enabled or not,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Collab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): No
- TensorFlow version (use command below):Google Collab default runtime / Tensorflow 1.14.0
tf.keras 2.2.4-tf
- Python version:Google Collab default runtime: 3.6.8 (default, Jan 14 2019, 11:02:34)
[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]

**Describe the current behavior**

building a tf.keras sequence model when eager mode is enabled vs not results in different output nodes.

**Describe the expected behavior**

Graph execution does not effect graph construction, the model should be the same.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

With eager enabled:
```
# load tensorflow
from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf
import numpy as np
print(tf.__version__)
AUTOTUNE = tf.data.experimental.AUTOTUNE
#enable eager
tf.enable_eager_execution()
assert tf.multiply(6, 7).numpy() == 42
print(""Eager execution: {}"".format(tf.executing_eagerly()))

# build our model and print its outputs and summary
base_model = tf.keras.applications.NASNetMobile(input_shape=(IMG_SIZE, IMG_SIZE, 3),
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False
output = tf.keras.layers.Dense(num_labels, activation = 'sigmoid', name=""cinemanet_output"", input_shape=(None, 1056))

model = tf.keras.Sequential([
  base_model,
  tf.keras.layers.GlobalAveragePooling2D(),
  output])
model.summary()
print(model.input.op.name)
print(model.output.op.name)
```

Results in:

```
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
NASNet (Model)               (None, 7, 7, 1056)        4269716   
_________________________________________________________________
global_average_pooling2d (Gl (None, 1056)              0         
_________________________________________________________________
cinemanet_output (Dense)     (None, 229)               242053    
=================================================================
Total params: 4,511,769
Trainable params: 242,053
Non-trainable params: 4,269,716
_________________________________________________________________
NASNet_input
cinemanet_output/Identity
```

Without Eager:

```
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
NASNet (Model)               (None, 7, 7, 1056)        4269716   
_________________________________________________________________
global_average_pooling2d (Gl (None, 1056)              0         
_________________________________________________________________
cinemanet_output (Dense)     (None, 229)               242053    
=================================================================
Total params: 4,511,769
Trainable params: 242,053
Non-trainable params: 4,269,716
_________________________________________________________________
NASNet_input
cinemanet_output/Sigmoid
```

Which results in a model with no output when saved to pb.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32265,tf.keras model.evaluate (and fit with validation) possible leak when running on TPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **'Linux-4.14.137+-x86_64-with-Ubuntu-18.04-bionic'** (Google Colab)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **No**
- TensorFlow installed from (source or binary): **binary** (I think, it's the one already installed in Colab)
- TensorFlow version (use command below): **1.14.0**
- Python version: **3.6.8**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: **Google Colab's TPU**

**Describe the current behavior**
When running `model.evaluate` in TPU, each time it gets executed it takes longer and longer. I started seeing this behavior when training (`model.fit()`) but only when providing validation data. I guess `model.evaluate` gets called when processing the validation set, so I tried to have a minimum reproduction sample with just directly calling `model.evaluate`.

But please note this is also a problem when running `fit` with validation data. Each epoch takes longer and longer. I can start with 10 seconds on epoch 1 and have like 160 seconds on epoch 50.

Other *strange* and related behavior I can see:
- Just when calling `evaluate`, you get two identical lines as output instead of just 1 as I would expect (calling `fit` without validation data outputs 1 line per epoch, but when calling it with validation data it outputs that same 1 line plus two identical lines corresponding to the `evaluation`)
- each time `evaluate` is called it seems to incur in a ""setup"" time, since the step itself takes considerably less than than the total `evaluate` execution time
- as you call it again and again, it seems both, this ""setup time"" and the ""evaluation time"" itself increase, but not exactly as much. It seems the setup time increases more than the evaluation time
- the reproduction example below has very simple data and model, and you can still see this behavior with that. But it seems that with more complex models the time it takes for `evaluate` increases at a higher rate each time it gets called

**Describe the expected behavior**
Each time evaluate is called with the same data, it should take roughly the same time. Also, when running fit with validation, each epoch (after the first one at least) should also take roughly the same time (this does happen without validation data, but doesn't happen with validation)

**Code to reproduce the issue**

I tried to create a minimal example, so I'm not even training the model here before evaluating some data. But note this is also a problem when training, so I don't think using an untrained model here is related to the issue. In the same way, I'm using randomly generated data, but this happened to me when training with real images and masks, and also with higher batch size (I was using the recommended 1024, but I used 128 here so the example executes quicker)

```python
import os
import time
import numpy as np
import tensorflow as tf

tf.keras.backend.clear_session()

TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])

with tf.Session(TF_MASTER) as session:
  print(session.list_devices())

print(""Tensorflow Version:"", tf.VERSION)
print(""Tensorflow Keras Version:"", tf.keras.__version__)

amount = 128
size = [256, 256]
images = np.array([np.random.rand(*size, 3).astype('float32') for i in range(amount)])
masks = np.array([np.random.rand(*size, 1).astype('float32') for i in range(amount)])

ds = tf.data.Dataset.from_tensor_slices((images, masks)).batch(amount, drop_remainder=True) # we need Dataset to run on TPU
print(ds)

# very minimal model to demostrate the issue
def make_model(batch_size=None):
  src = tf.keras.layers.Input(shape=(*size,3), batch_size=batch_size, dtype=tf.float32, name='Input')
  outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(src)

  model = tf.keras.Model(inputs=[src], outputs=[outputs])
  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
  return model

resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.contrib.distribute.initialize_tpu_system(resolver)
strategy = tf.contrib.distribute.TPUStrategy(resolver)
with strategy.scope():
  model = make_model(batch_size = 128)
model.summary()

for i in range(30):
  start_time = time.time()
  model.evaluate(ds, steps=1)
  print(""--- %s seconds ---"" % (time.time() - start_time))
```

Output:
```
1/1 [==============================] - 2s 2s/step
1/1 [==============================] - 2s 2s/step
--- 4.957882404327393 seconds ---
1/1 [==============================] - 2s 2s/step
1/1 [==============================] - 2s 2s/step
--- 4.556092977523804 seconds ---
1/1 [==============================] - 2s 2s/step
1/1 [==============================] - 2s 2s/step
--- 4.7304792404174805 seconds ---
1/1 [==============================] - 2s 2s/step
1/1 [==============================] - 2s 2s/step
--- 5.154953479766846 seconds ---
1/1 [==============================] - 3s 3s/step
1/1 [==============================] - 3s 3s/step
--- 5.432474136352539 seconds ---
1/1 [==============================] - 3s 3s/step
1/1 [==============================] - 3s 3s/step
--- 5.930710315704346 seconds ---
1/1 [==============================] - 3s 3s/step
1/1 [==============================] - 3s 3s/step
--- 5.949801683425903 seconds ---
1/1 [==============================] - 3s 3s/step
1/1 [==============================] - 3s 3s/step
--- 6.352793455123901 seconds ---
1/1 [==============================] - 3s 3s/step
1/1 [==============================] - 3s 3s/step
--- 7.153834581375122 seconds ---
1/1 [==============================] - 4s 4s/step
1/1 [==============================] - 4s 4s/step
--- 7.319533348083496 seconds ---
1/1 [==============================] - 4s 4s/step
1/1 [==============================] - 4s 4s/step
--- 7.719737529754639 seconds ---
1/1 [==============================] - 4s 4s/step
1/1 [==============================] - 4s 4s/step
--- 8.089992761611938 seconds ---
1/1 [==============================] - 4s 4s/step
1/1 [==============================] - 4s 4s/step
--- 8.87194037437439 seconds ---
1/1 [==============================] - 5s 5s/step
1/1 [==============================] - 5s 5s/step
--- 9.02499771118164 seconds ---
1/1 [==============================] - 5s 5s/step
1/1 [==============================] - 5s 5s/step
--- 9.39006233215332 seconds ---
1/1 [==============================] - 5s 5s/step
1/1 [==============================] - 5s 5s/step
--- 9.687021970748901 seconds ---
1/1 [==============================] - 5s 5s/step
1/1 [==============================] - 5s 5s/step
--- 10.596904754638672 seconds ---
1/1 [==============================] - 6s 6s/step
1/1 [==============================] - 6s 6s/step
--- 10.745074272155762 seconds ---
1/1 [==============================] - 6s 6s/step
1/1 [==============================] - 6s 6s/step
--- 11.3403902053833 seconds ---
1/1 [==============================] - 6s 6s/step
1/1 [==============================] - 6s 6s/step
--- 11.945271015167236 seconds ---
1/1 [==============================] - 7s 7s/step
1/1 [==============================] - 7s 7s/step
--- 12.67944622039795 seconds ---
1/1 [==============================] - 7s 7s/step
1/1 [==============================] - 7s 7s/step
--- 13.195830345153809 seconds ---
1/1 [==============================] - 7s 7s/step
1/1 [==============================] - 7s 7s/step
--- 14.408772230148315 seconds ---
1/1 [==============================] - 7s 7s/step
1/1 [==============================] - 7s 7s/step
--- 14.38786506652832 seconds ---
1/1 [==============================] - 8s 8s/step
1/1 [==============================] - 8s 8s/step
--- 14.985581636428833 seconds ---
1/1 [==============================] - 8s 8s/step
1/1 [==============================] - 8s 8s/step
--- 15.89274287223816 seconds ---
1/1 [==============================] - 9s 9s/step
1/1 [==============================] - 9s 9s/step
--- 16.433870792388916 seconds ---
1/1 [==============================] - 9s 9s/step
1/1 [==============================] - 9s 9s/step
--- 17.08533024787903 seconds ---
[...]
```

**Other info / logs**

I created a Google Colab with this example, so you can execute it and check this behavior: https://colab.research.google.com/drive/1YgSSdlrfVpTfufGXNNXckYzOd-dzEbVI"
32262,TF2.0 : Tensorflow object detection model zoo inference not working on gpu,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-rc0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0
- GPU model and memory: GeForce RTX 2080 Ti

**Describe the current behavior**
Running any of the saved models from [tensorflow model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) leads to a segmentation fault.

**Code to reproduce the issue**
```
import os
import tensorflow as tf
import numpy as np

os.system('wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz')
os.system('tar -xvf ssd_mobilenet_v2_coco_2018_03_29.tar.gz')

with tf.device('/device:GPU:0'):
    loaded_model = tf.saved_model.load('ssd_mobilenet_v2_coco_2018_03_29/saved_model')
    infer = loaded_model.signatures['serving_default']
    sample_img = np.zeros((1,128,128,3))
    predicted = infer(tf.constant(sample_img, tf.uint8))
    print(predicted)
```

**gdb output**
```
Thread 89 ""python3"" received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7ffbfbfff700 (LWP 7857)]
0x00007fff4372b741 in tensorflow::NonMaxSuppressionV2GPUOp::Compute(tensorflow::OpKernelContext*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
```"
32261,TF2.0 : NonMaxSuppressionV2GPUOp segmentation fault,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): TF2.0.0-rc0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0
- GPU model and memory: GeForce RTX 2080 Ti

**Describe the current behavior**
TF2.0 has a Non Max Suppression V2 op gpu implementation which seg faults when it is run.
Due to this [TF object detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) saved models cannot be run in TF2.0.

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.python.ops import gen_image_ops

with tf.device('/device:GPU:0'):
    boxes = tf.random_uniform((10,4), dtype=tf.float32)
    scores = tf.constant([1]*10, tf.float32)
    max_output_size = tf.constant(1)
    iou_threshold = tf.constant(0.5)

    nms = gen_image_ops.non_max_suppression_v2(boxes, scores, max_output_size, iou_threshold)
    print(nms)
```

**gdb output** 
```
Thread 1 ""python3"" received signal SIGSEGV, Segmentation fault.                                                                                                                          
0x00007fff4372b741 in tensorflow::NonMaxSuppressionV2GPUOp::Compute(tensorflow::OpKernelContext*) ()                                                                                     
   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so
```"
32255,TPU with tensorflow 2.0 -- 'DeleteIterator' OpKernel missing,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution: Debian GNU/Linux 9
- TensorFlow installed from (source or binary): /usr/bin/pip3
- TensorFlow version (use command below): 2.0.0b1
- Python version: 3.5.3
- TPU type: v2-8
- TPU software version: 1.14 

**Describe the current behavior**
I am running a TPU allocated by `ctpu up` in tensorflow 2.0 (I'm aware this isn't fully supported atm). I have a simple training loop functioning mostly following the guidelines here:
https://www.tensorflow.org/beta/guide/distribute_strategy#using_tfdistributestrategy_with_custom_training_loops

To my surprise I have encountered few issues along the way, but one that I can't seem to remedy on my end is this:
`tensorflow.python.framework.errors_impl.NotFoundError: No registered 'DeleteIterator' OpKernel for TPU devices compatible with node {{node DeleteIterator}}`

This error doesn't seem to actually break anything, but I'm worried there could be a TPU memory leak or something and that is difficult to verify without any useable TPU profiling tools for TF 2.0.
EDIT: The bug does actually cause the program to crash after a few iterations.

**Describe the expected behavior**
I expect to be able to delete the iterator object within the TPU strategy scope.

**Code to reproduce the issue**
```
for epoch in range(self.train_epochs):
    with tf.device(self.device), self.distribution_strategy.scope():
        dataset = self.fill_experience_buffer()
        exp_buff = iter(dataset)

        for step in tqdm(range(self.train_steps), ""Training epoch {}"".format(epoch)):
            train_step(next(exp_buff))
```

The issue occurs the second time around in the loop when the `exp_buff` variable is rewritten with `iter(dataset)`. I've tried explicitly freeing the object with 'del exp_buff' within and outside of the scope():, but the same error occurs regardless.

**Other info / logs**
Full error message here (the message appears 8 times, once for each TPU device, but the messages are identical:
```
Exception ignored in: <bound method IteratorResourceDeleter.__del__ of <tensorflow.python.data.ops.iterator_ops.IteratorResourceDeleter object at 0x7f0bac1883c8>>
Traceback (most recent call last):
  File ""/home/youngalou/.local/lib/python3.5/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 531, in __del__
    handle=self._handle, deleter=self._deleter)
  File ""/home/youngalou/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 800, in delete_iterator
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'DeleteIterator' OpKernel for TPU devices compatible with node {{node DeleteIterator}}
        .  Registered:  device='CPU'
  device='GPU'

Additional GRPC error information:
{""created"":""@1567559533.240191338"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""No registered 'DeleteIterator' OpKernel for TPU devices compatible with node {{node DeleteIterator}}\n\t.  Registered:  device='CPU'\n  device='GPU'\n"",""grpc_status"":5} [Op:DeleteIterator]
Exception ignored in: <bound method IteratorResourceDeleter.__del__ of <tensorflow.python.data.ops.iterator_ops.IteratorResourceDeleter object at 0x7f0bac188518>>
Traceback (most recent call last):
  File ""/home/youngalou/.local/lib/python3.5/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 531, in __del__
    handle=self._handle, deleter=self._deleter)
  File ""/home/youngalou/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 800, in delete_iterator
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'DeleteIterator' OpKernel for TPU devices compatible with node {{node DeleteIterator}}
        .  Registered:  device='CPU'
  device='GPU'
```"
32254,Input placeholder with shape [1] of saved_model.pb accepts scaler input but throws error in later graph.,"Env:
* Mac OS
* python 2.7
* tensorflow 1.13.1 installed from pip

Description:
I'm using estimator API to build model. Inside serving input function, my model has an int64 input placeholder with shape [1]. And this placeholder is later concatenated with another tensor. Then I trained the model and saved it to `*.pb` graph. However during serving time I mistakenly feed a scaler value to this placeholder:
```
dtype: DT_INT64
tensor_shape {
}
int64_val: 182
```
As you can see, there is no dimension info in this proto. It didn't immediately throw an error and it seems that the placeholder with shape [1] is compatible with it. However later during concatenating op I got the error:
```
*** _Rendezvous: <_Rendezvous of RPC that terminated with:
        status = StatusCode.INVALID_ARGUMENT
        details = ""ConcatOp : Expected concatenating dimensions in the range [0, 0), but got 0
         [[{{node concat_77}}]]""
        debug_error_string = ""{""created"":""@1567706789.859604000"",""description"":""Error received from peer ipv6:[::1]:9000"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1041,""grpc_message"":""ConcatOp : Expected concatenating dimensions in the range [0, 0), but got 0\n\t [[{{node concat_77}}]]"",""grpc_status"":3}""
```
Later I realized I did something wrong in the dimension so I add the shape info to my input proto. The error is gone. The correct proto looks like:
```
dtype: DT_INT64
tensor_shape {
  dim {
    size: 1
  }
}
int64_val: 182
```
In one word, a `saved_model.pb` with input placeholder of shape [1] accepts a scaler input and not throwing any error. However later in the concat operation, since we are doing `tf.concat([scaler, tf.constant([1])], axis=0)`, it throws an error since the scaler has 0 dimension.

Code to reproduce the error:
```
def serve_input_fn():
    receiver_tensor = {}
    serving_features = {}
    receiver_tensor['my_input'] = tf.placeholder(shape=[1], dtype=tf.int64)
    serving_features['my_feature'] = tf.concat([receiver_tensor['my_input'], [1]], axis=0)
    return tf.estimator.export.ServingInputReceiver(serving_features, receiver_tensors)

# create any estimater
estimator.export(""/tmp/my_model"", ""serve_input_fn"", mode=""infer"")

# serve the model and feed in the following input
request.inputs['my_input'].CopyFrom(tf.contrib.util.make_tensor_proto(100, dtype=tf.int64))
output = stup.Predict(request, 10)
```
"
32250,tf.saved_model.save() broken for my subclassed model in tf-2.0.0rc0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.02 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf2.0.0rc0
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

n [4]: tf.version.GIT_VERSION                                                           
Out[4]: 'v2.0.0-beta1-5101-gc75bb66'

In [5]: tf.version.VERSION                                                               
Out[5]: '2.0.0-rc0'

**Describe the current behavior**

I have a subclassed model that I have been saving and deploying since tf-2.0.0b0 and after upgrading to rc0, it errored out with this message.

[save.py:136] Skipping full serialization of object <__main__.MyModel object at 0x7f76815add30> because an error occurred while tracing layer functions. Error message: in converted code:

TypeError: tf__call() takes from 2 to 3 positional arguments but 4 were given

This is how the call method is defined in my class.  Can someone tell me what went wrong, or what I need to change to make this work?

Thanks, David

class MyModel(Model):

    def __init__(self, n_layers, h_dim, b_dim,
                       activation_fn,
                       kernel_init,
                       batch_norm):
        super().__init__()
        ... skipping ...

    @tf.function
    def call(self, inputs, training=False):
        y = inputs[0]
        h = inputs[1]
        n_var = inputs[2]
        x = layers.concatenate([y, h])
        if self.n_layers >= 1: x = self.d_hidden_1(x,training=training)
        if self.n_layers >= 2: x = self.d_hidden_2(x,training=training)
        if self.n_layers >= 3: x = self.d_hidden_3(x,training=training)
        if self.n_layers >= 4: x = self.d_hidden_4(x,training=training)
        if self.n_layers >= 5: x = self.d_hidden_5(x,training=training)
        # scale output by 1/n_var
        #return self.d_out(x) / n_var
        return self.d_out(x,training=training)

**Describe the expected behavior**
It should just work

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
See above

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32249,Is tensorflow lite malloc free?,"I read that TFL Micro is malloc free, but is Tensorflow Lite also malloc free? I wondering if its safe to use inside an audio thread?

"
32248,Performance regression in sparse_dense_matmul,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.14.0
- Python version:
3.7

**Describe the current behavior**

Performance of `sparse_dense_matmul` is significantly slower compared to 1.12
 
**Describe the expected behavior**

Performance does not degrade between versions. If there is a regression, it should be included in the release notes.

**Code to reproduce the issue**
```
import os
import time

import numpy as np
import tensorflow as tf


def create_sparse_tensor(batch_size, num_features, num_samples):
  total_samples = num_samples * batch_size
  batch_indices = np.sort(np.random.randint(0, batch_size, total_samples))
  feature_indices = np.random.randint(0, num_features, total_samples)
  indices = np.stack([batch_indices, feature_indices], axis=1)
  values = np.random.random(total_samples).astype(np.float32)
  return tf.SparseTensorValue(indices=indices, values=values,
                              dense_shape=[batch_size, num_features])


def bench(batch_size=32, num_bits=18, num_samples=1000, num_runs=2000):
  num_features = 1 << num_bits
  experiment = (""batch_size_%d_num_bits_%d"" % (batch_size, num_bits))
  sparse_input = tf.sparse_placeholder(dtype=tf.float32,
                                       shape=(batch_size, num_features),
                                       name=""sparse_input_"" + experiment)
  dense_weight = tf.get_variable(name=""dense_weight_"" + experiment,
                                 shape=(num_features, 50),
                                 dtype=tf.float32)

  output_sparse = tf.sparse_tensor_dense_matmul(sparse_input, dense_weight)

  with tf.Session() as sess:
    print(experiment)
    sparse_input_values = [
      create_sparse_tensor(batch_size, num_features, num_samples)
      for n in range(num_runs)
    ]

    sess.run(tf.initializers.global_variables())

    start = time.time()
    for n in range(num_runs):
      sess.run(output_sparse, feed_dict={sparse_input: sparse_input_values[n]})
    elapsed = time.time() - start
    print(""Avg time taken for sparse_matmul: %.4f"" % (elapsed / num_runs))

if __name__ == ""__main__"":
  tf.logging.set_verbosity(tf.logging.ERROR)
  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
  print(""--------------------"")
  print(tf.__version__)
  print(""--------------------"")
  for b in [4, 16, 32]:
    for n in [18, 22]:
      bench(batch_size=b, num_bits=n)
  print(""--------------------"")
```
**Other info / logs**
```
--------------------
1.14.0
--------------------
batch_size_4_num_bits_18
Avg time taken for sparse_matmul: 0.0015
batch_size_4_num_bits_22
Avg time taken for sparse_matmul: 0.0020
batch_size_16_num_bits_18
Avg time taken for sparse_matmul: 0.0049
batch_size_16_num_bits_22
Avg time taken for sparse_matmul: 0.0072
batch_size_32_num_bits_18
Avg time taken for sparse_matmul: 0.0100
batch_size_32_num_bits_22
Avg time taken for sparse_matmul: 0.0117
--------------------
--------------------
1.12.0
--------------------
batch_size_4_num_bits_18
Avg time taken for sparse_matmul: 0.0009
batch_size_4_num_bits_22
Avg time taken for sparse_matmul: 0.0012
batch_size_16_num_bits_18
Avg time taken for sparse_matmul: 0.0027
batch_size_16_num_bits_22
Avg time taken for sparse_matmul: 0.0035
batch_size_32_num_bits_18
Avg time taken for sparse_matmul: 0.0048
batch_size_32_num_bits_22
Avg time taken for sparse_matmul: 0.0064
--------------------
```"
32246,tf.train.SingularMonitoredSession() Error:tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a NewWriteableFile,"hi,
I'm new with tensorflow, I'm facing an issue while executing a [project](https://github.com/hcmlab/vadnet/blob/master/train/code/main.py) on window 10.

Using :
python-3.5.4
tensorboard-gpu=1.14.0
cuda-10

ERROR:
tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a NewWriteableFile: C:\Users\MY-PC\Desktop\tools\vadnet\train\nets\vad_Model[48000,48000,24000,512,None]_Conv7[2,True,False,4,2]_SceAdam[1e-04,0.9,0.999,1e-08]\ckpt\model.ckpt-0_temp_df1af0f684d24212bc1ebde45848fe3f/part-00000-of-00001.data-00000-of-00001.tempstate18107421248045726006 : The system cannot find the path specified.

path is correct and directory is created ""nets\vad_Model[48000,48000,24000,512,None]_Conv7[2,True,False,4,2]_SceAdam[1e-04,0.9,0.999,1e-08]\ckpt"" 
by script but 
""part-00000-of-00001.data-00000-of-00001.tempstate18107421248045726006"" 
file not creating.

I read somewhere, need to use an absolute path. I had tried that also but still facing the same error.
thank you!

"
32244,Please modify cuda-bin in cuda_configure.bzl to copy just needed files instead of directory,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 30
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: master branch 
- Python version: 3.7
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): 7.4.1
- CUDA/cuDNN version: 10.1 / 7.6
- GPU model and memory: Quadro M2200

If you don't have CUDA installed in `/usr/local/cuda`, but in the ""normal RPM way"" (so that binaries end up in `/usr/bin`, libraries in `/usr/lib64` etc), the following `cuda-bin` rule

```
# in third_party/gpus/cuda_configure.bzl
copy_rules.append(make_copy_dir_rule(
    repository_ctx,
    name = ""cuda-bin"",
    src_dir = cuda_config.cuda_toolkit_path + ""/bin"",
    out_dir = ""cuda/bin"",
))
```

will end up trying to copy all the binaries in `/usr/bin`, which on some systems (where these commands are not other-readable) may lead to errors like this one:

```
ERROR: /home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/external/local_config_cuda/cuda/BUILD:1399:1: Executing genrule @local_config_cuda//cuda:cuda-bin failed (Exit 1)
cp: cannot open '/usr/bin/./sudoedit' for reading: Permission denied
cp: cannot open '/usr/bin/./locate' for reading: Permission denied
cp: cannot open '/usr/bin/./sudo' for reading: Permission denied
cp: cannot open '/usr/bin/./sudoreplay' for reading: Permission denied
cp: cannot open '/usr/bin/./chsh' for reading: Permission denied
cp: cannot open '/usr/bin/./chfn' for reading: Permission denied

```

Evidently these files should in fact not be copied ... looking for what binaries really are wanted, I found this:

```
# in nccl/build_defs.bzl.tpl
_device_link = rule(
    implementation = _device_link_impl,
    attrs = {
        ""deps"": attr.label_list(),
        ""out"": attr.output(mandatory = True),
        ""gpu_archs"": attr.string_list(),
        ""nvlink_args"": attr.string_list(),
        ""_nvlink"": attr.label(
            default = Label(""@local_config_cuda//cuda:cuda/bin/nvlink""),
            allow_single_file = True,
            executable = True,
            cfg = ""host"",
        ),
        ""_fatbinary"": attr.label(
            default = Label(""@local_config_cuda//cuda:cuda/bin/fatbinary""),
            allow_single_file = True,
            executable = True,
            cfg = ""host"",
        ),
        ""_bin2c"": attr.label(
            default = Label(""@local_config_cuda//cuda:cuda/bin/bin2c""),
            allow_single_file = True,
            executable = True,
            cfg = ""host"",
        ),
        ""_link_stub"": attr.label(
            default = Label(""@local_config_cuda//cuda:cuda/bin/crt/link.stub""),
            allow_single_file = True,
        ),
    },
```

In fact when I hard-code these 4 files, modifying the `cuda-bin` rule like this

```
copy_rules.append(make_copy_files_rule(
         repository_ctx,
         name = ""cuda-bin"",
         srcs = [
            cuda_config.cuda_toolkit_path + ""/bin/"" + ""crt/link.stub"",
            cuda_config.cuda_toolkit_path + ""/bin/"" + ""nvlink"",
            cuda_config.cuda_toolkit_path + ""/bin/"" + ""fatbinary"",
            cuda_config.cuda_toolkit_path + ""/bin/"" + ""bin2c""
        ],
        outs = [
            ""cuda/bin/"" + ""crt/link.stub"",
            ""cuda/bin/"" + ""nvlink"",
            ""cuda/bin/"" + ""fatbinary"",
            ""cuda/bin/"" + ""bin2c""
        ],
     ))
```

all works great.

Would you be interested in adapting the build rule accordingly? Not only would it not fail, it'd be also cleaner not to try to copy files like `sudo` :-)

I'd be happy to submit a PR but would probably need some guidance (assuming you don't want the file names hard coded as I'm doing for my workaround). Thanks!


"
32243,SpatialDropout2D,"
**System information**
- TensorFlow version (you are using): 1.11
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

SpatialDropout2D is often used in CNNs (https://www.tensorflow.org/api_docs/python/tf/keras/layers/SpatialDropout2D).  At inference time, the layer should be treated just like a regular dropout layer where it just scales the coefficients by the dropout term. However, when I take a Keras model with SpatialDropout2D and saved it as a TF protobuf, TensorFlow converts SpatialDropout2D into a set of primitive operators to do the random dropout instead-- even for a TensorFlow serving graph where the model has been frozen and optimized for inference.  

**Will this change the current api? How?**

I'd like to see the SpatialDropout layers behave the same as the regular Dropout layers during inference.  So the TensorFlow Serving protobuf should not include SpatialDropout2D (or any set of random/slice ops to reconstruct it), but instead should just scale the convolutional filters by the dropout rate as explained in the original paper (https://arxiv.org/abs/1411.4280).

**Who will benefit with this feature?**

If the SpatialDropout layers behave the same as the regular Dropout layers, it will reduce the number of operations during inference and speed up inference on these graphs.

**Any Other info.**
"
32242,[TF2.0] Checkpoint doesn't store non-TF objects,"Hello all,

I found that `tf.train.Checkpoint` doesn't save non-TF objects and tf.Tensors. The MWE below shows the case:

```python
import tensorflow as tf

class A(tf.Module):
    def __init__(self):
        super().__init__()
        self.scalar = 1.
        self.variable = tf.Variable(1.)
        self.tensor = tf.convert_to_tensor(1.)
        self.list = [tf.convert_to_tensor(10.0), 20.0, tf.Variable(0.0)]

    def __str__(self):
        return f""scalar={self.scalar}, variable={self.variable.numpy()}, tensor={self.tensor}, list={self.list}""


a_module = A()
checkpoint = tf.train.Checkpoint(a=a_module)
manager = tf.train.CheckpointManager(checkpoint, '/tmp/example', max_to_keep=3)
manager.save()

print(f""1. {a_module}"")

#### modify

a_module.scalar = -100.
a_module.variable.assign(123.)
a_module.tensor = tf.convert_to_tensor(-12.)
a_module.list = [3., 3.]

print(f""2. {a_module}"")

#### restore
checkpoint.restore(manager.latest_checkpoint)

print(f""3. {a_module}"")
```

Output:
```python
1. scalar=1.0, variable=1.0, tensor=1.0, list=ListWrapper([<tf.Tensor: id=9, shape=(), dtype=float32, numpy=10.0>, 20.0, <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>])
2. scalar=-100.0, variable=123.0, tensor=-12.0, list=ListWrapper([3.0, 3.0])
3. scalar=-100.0, variable=1.0, tensor=-12.0, list=ListWrapper([3.0, 3.0])
```

In the output you can see that the checkpointer restores only the variable and completely ignores other fields of the A object.

I expected that at least pythonic objects, numpy and TF tensors should be stored by the checkpointer on the disk. Is that a normal behavior?

If the answer is **yes**: I think that this feature should be implemented, otherwise it is very confusing and not straightforward why the checkpoint saves some objects and others not.
**no**: this is a bug, must be fixed.

**System information**
- OS Platform and Distribution: `macOS 10.14.6`
- TensorFlow version: `v1.12.1-10419-g5138353309 2.0.0-dev20190905`
- Python version: `3.6.8`
"
32241,tf.keras Sequence model looses final output when saved as frozen pb graph?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes and No. Both cases fail.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux / Google Collaboratory runtime
- TensorFlow installed from (source or binary): No : Google Collab default runtime
- TensorFlow version (use command below): Google Collab default runtime / Tensorflow  1.14.0
tf.keras 2.2.4-tf
- Python version: Google Collab default runtime: 3.6.8 (default, Jan 14 2019, 11:02:34) 
[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]

**Describe the current behavior**

I am able to successfully build a model train, evaluate and run prediticions on a Sequence model using NasNetMobile. I can save my model using model.save in tf.keras and have a well formed h5 file with a graph, weights and inputs and outputs as expected and verified in Netron.

I wish to convert this model to pb. Using various techniques I will describe below, I load the model using tf.keras, and introspecting the model summary and model inputs and outputs. All is as to be expected (save a slight difference in output tensor name).

Saving the PB does not error, and creates a valid PB file but it lacks the final graph output, and tools like tf_coreml and Netron complain of missing output.

**Describe the expected behavior**

Loading a save h5 file and saving the resulting Tensorflow pb results in a well formed PB that can be used for inference.

**Code to reproduce the issue**

**Model definition:**

```

base_model = tf.keras.applications.NASNetMobile(input_shape=(IMG_SIZE, IMG_SIZE, 3),
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

output = tf.keras.layers.Dense(num_labels, activation = 'sigmoid', name=""output"")
model = tf.keras.Sequential([
  base_model,
  tf.keras.layers.GlobalAveragePooling2D(),
  output])
model.summary()
```

Which prints:
```
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
NASNet (Model)               (None, 7, 7, 1056)        4269716   
_________________________________________________________________
global_average_pooling2d (Gl (None, 1056)              0         
_________________________________________________________________
output (Dense)               (None, 229)               242053    
=================================================================
Total params: 4,511,769
Trainable params: 242,053
Non-trainable params: 4,269,716
_________________________________________________________________
```

**Saving the model:**

```
#verify inputs and outputs
print(model.input.op.name)
print(model.output.op.name) 
tf.keras.models.save_model(model, model_path, overwrite=True,save_format=""h5"")
```

or alternatively;
```
#verify inputs and outputs
print(model.input.op.name)
print(model.output.op.name) 
 model.save(model_path)
```

Both mechanisms print the following inputs and output tensor names:

```
NASNet_input
output/Identity
```

**Loading said model in a different ipynb:**

```

import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.python.tools import freeze_graph

# unclear if strictly necessary but git issues imply it may be defensive to add these lines
tf.reset_default_graph()
K.clear_session()
K.set_learning_phase(0)

restored_model = tf.keras.models.load_model(model_path, compile=True)
print(restored_model.inputs)
print(restored_model.outputs)

restored_model.summary()
```

Which prints:

```
[<tf.Tensor 'NASNet_input:0' shape=(?, 224, 224, 3) dtype=float32>]
[<tf.Tensor 'output/Sigmoid:0' shape=(?, 229) dtype=float32>]
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
NASNet (Model)               (None, 7, 7, 1056)        4269716   
_________________________________________________________________
global_average_pooling2d (Gl (None, 1056)              0         
_________________________________________________________________
output (Dense)               (None, 229)               242053    
=================================================================
Total params: 4,511,769
Trainable params: 242,053
Non-trainable params: 4,269,716
```

Notice the output tensor has changed its name / no longer identity / placeholder?

**Saving to PB**

The simplest incantation which does not do any graph cleaning / optimization

```
import datetime

output_model_name = model_name + "".pb""
output_model_path = ""/tmp/"" + output_model_name
save_dir = ""./tmp_{:%Y-%m-%d_%H%M%S}"".format(datetime.datetime.now())
tf.saved_model.simple_save(K.get_session(),
                           save_dir,
                           inputs={""input"": restored_model.inputs[0]},
                           outputs={""output"": restored_model.outputs[0]})

freeze_graph.freeze_graph(None,
                          None,
                          None,
                          None,
                          restored_model.outputs[0].op.name,
                          None,
                          None,
                           output_model_path,
                          False,
                          """",
                          input_saved_model_dir=save_dir)
```

Results in a PB that is missing the output layer.

**Other PB saving methods ive tried:**

All result with a PB missing output:
```
with tf.keras.backend.get_session() as session:
  graph = session.graph
  input_graph_def = graph.as_graph_def()
  
  # For demonstration purpose we show the first 15 ops the TF model
with graph.as_default() as g:
    tf.import_graph_def(input_graph_def, name='')
    
    ops = g.get_operations()
    for op in ops[0:15]:
        print('op name: {}, op type: ""{}""'.format(op.name, op.type));
    for op in ops[::-1][0:15]:
        print('op name: {}, op type: ""{}""'.format(op.name, op.type));

  

input_node_names = ['NASNet_input']
output_node_names = ['output/Sigmoid']
gdef = strip_unused_lib.strip_unused(
      input_graph_def = input_graph_def,
      input_node_names = input_node_names,
      output_node_names = output_node_names,
      placeholder_type_enum = dtypes.float32.as_datatype_enum)
  
with gfile.GFile(output_model_path, ""wb"") as f:
    f.write(gdef.SerializeToString())
```

And :

```
# from tensorflow.python.framework.graph_util import convert_variables_to_constants


def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):
    """"""
    Freezes the state of a session into a pruned computation graph.

    Creates a new computation graph where variable nodes are replaced by
    constants taking their current value in the session. The new graph will be
    pruned so subgraphs that are not necessary to compute the requested
    outputs are removed.
    @param session The TensorFlow session to be frozen.
    @param keep_var_names A list of variable names that should not be frozen,
                          or None to freeze all the variables in the graph.
    @param output_names Names of the relevant graph outputs.
    @param clear_devices Remove the device directives from the graph for better portability.
    @return The frozen graph definition.
    """"""
    from tensorflow.python.framework.graph_util import convert_variables_to_constants
    graph = session.graph
    with graph.as_default():
        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))
        output_names = output_names or []
        output_names += [v.op.name for v in tf.global_variables()]
        # Graph -> GraphDef ProtoBuf
        input_graph_def = graph.as_graph_def()
        if clear_devices:
            for node in input_graph_def.node:
                node.device = """"
        frozen_graph = convert_variables_to_constants(session, input_graph_def,
                                                      output_names, freeze_var_names)
        return frozen_graph



      
frozen_graph = freeze_session(K.get_session(),
                              output_names=[out.op.name for out in restored_model.outputs], 
                             clear_devices=True)

tf.train.write_graph(frozen_graph, ""/tmp"", model_name+"".pb"", as_text=False)

```


**Other info / logs**

Here is a link the my H5 file:

https://storage.cloud.google.com/synopsis_cinemanet/Synopsis_Models/Cinemanet-2019-09-04_21_57_48.h5


Here is a link to a resulting PB file ive tried to make, which lacks output:

https://storage.cloud.google.com/synopsis_cinemanet/Synopsis_Models/Cinemanet-2019-09-04_21_57_48.h5.pb

Thank you very much!
"
32240,"TPU has unsupported tensorflow op ""LogUniformCandidateSampler"" using XLA","Using NCE loss for an NLP model and ran into an issue where XLA doesn't compile using TPU.
```
No registered 'LogUniformCandidateSampler' OpKernel for XLA_CPU_JIT devices compatible with node node LogUniformCandidateSampler
```

**System information**
- Using Google's cloud TPU with Tensorflow 1.14
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3579968/tf_env.txt)

**Describe the current behavior**
I get an error when running the code on TPU because XLA doesn't support candidate samplers.

image:
![Screen Shot 2019-09-05 at 8 29 54 AM](https://user-images.githubusercontent.com/44978436/64356249-60b5c200-cfb7-11e9-9a09-9f9a35057f6e.png)


**Describe the expected behavior**
I expect the model to be able to run and calculate loss on TPU using XLA

**Code to reproduce the issue**
```
import os
import tensorflow as tf
from tensorflow.python.compiler.xla import xla

def my_model(features, labels):
  nce_weights = tf.get_variable(
    name=""nce_weights"",
    shape=[10, 4],
    initializer=tf.random_uniform_initializer(-1.0, 1.0)
  )
  nce_biases = tf.get_variable(
    name=""nce_biases"",
    shape=[10],
    initializer=tf.zeros_initializer()
  )

  num_sampled = 1
  num_classes = 10

  sampler_func = tf.random.log_uniform_candidate_sampler
  sampled_values = sampler_func(
    true_classes=tf.cast(labels, tf.dtypes.int64),
    num_sampled=num_sampled,
    range_max=num_classes,
    num_true=1,
    unique=True,
    seed=None,
  )

  loss = tf.reduce_mean(
    tf.nn.nce_loss(
      weights=nce_weights,
      biases=nce_biases,
      labels=labels,
      inputs=features,
      num_sampled=num_sampled,
      num_classes=num_classes,
      sampled_values=sampled_values,
    )
  )

  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)
  return loss, optimizer

sess = tf.Session()
features = tf.constant(1.2, shape=[1, 4], dtype=tf.float32)
labels = tf.constant(1, shape=[1, 1], dtype=tf.float32)

[y] = xla.compile(my_model, inputs=[features, labels])

sess.run(tf.global_variables_initializer())
sess.run(y)
```

**Other info / logs**
```
WARNING: Logging before flag parsing goes to stderr.
W0905 08:26:12.392647 140303472117184 deprecation_wrapper.py:119] From issue.py:45: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-09-05 08:26:12.529939: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-05 08:26:12.908472: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2019-09-05 08:26:12.908636: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557f84258f10 executing computations on platform Host. Devices:
2019-09-05 08:26:12.908650: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
W0905 08:26:13.021475 140303472117184 deprecation_wrapper.py:119] From issue.py:6: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

W0905 08:26:13.398072 140303472117184 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_impl.py:180: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0905 08:26:13.421444 140303472117184 deprecation_wrapper.py:119] From issue.py:42: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

W0905 08:26:13.709804 140303472117184 deprecation_wrapper.py:119] From issue.py:51: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

2019-09-05 08:26:14.238822: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-09-05 08:26:14.645355: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at xla_ops.cc:343 : Invalid argument: Detected unsupported operations when trying to compile graph cluster_18305510630026920806[] on XLA_CPU_JIT: LogUniformCandidateSampler (No registered 'LogUniformCandidateSampler' OpKernel for XLA_CPU_JIT devices compatible with node {{node LogUniformCandidateSampler}}
	.  Registered:  device='CPU'
){{node LogUniformCandidateSampler}}
	This error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=""tf_xla_auto_jit=2"" which will attempt to use xla to compile as much of the graph as the compiler is able to.
Traceback (most recent call last):
  File ""issue.py"", line 52, in <module>
    sess.run(y)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph cluster_18305510630026920806[] on XLA_CPU_JIT: LogUniformCandidateSampler (No registered 'LogUniformCandidateSampler' OpKernel for XLA_CPU_JIT devices compatible with node node LogUniformCandidateSampler (defined at issue.py:27) 
	.  Registered:  device='CPU'
)node LogUniformCandidateSampler (defined at issue.py:27) 
	This error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=""tf_xla_auto_jit=2"" which will attempt to use xla to compile as much of the graph as the compiler is able to.
	 [[cluster]]

Errors may have originated from an input operation.
Input Source operations connected to node LogUniformCandidateSampler:
 Cast (defined at issue.py:22)

Input Source operations connected to node LogUniformCandidateSampler:
 Cast (defined at issue.py:22)
```

"
32239,"ModelCheckpoint can't use ""val_acc"" even if fit() function receives validation_data argument","**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-rc0-gpu
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0
- GPU model and memory: NVidia Tesla P100 (16 gb)

**Describe the current behavior**
Recently I switched from TF1 wrapped with native Keras to TF2 using the built-in `tf.keras` implementation. In my code I train my model like so:

    checkpoint = ModelCheckpoint(auto_save_path, monitor=""val_acc"", save_best_only=True)
    callbacks.append(checkpoint)

    keras_model.fit_generator(training_img_generator,
        steps_per_epoch=100, epochs=20, validation_steps=100,
        validation_data=validation_img_generator, callbacks=callbacks)

However, with that code I do get the following warning message:

    WARNING:tensorflow:Can save best model only with val_acc available, skipping.

A full working example can be found as [Github Gist here](https://gist.github.com/haimat/65e9dddc9cd8004ea8870853f84d7b02).

**Describe the expected behavior**
I don't understand this error, because I do provide the `validation_data` argument to the `fit_generator()` function. Also, I used exactly the same code with Keras on top of TF1, which worked fine there. This warning came now after switching to TF2 and built-in Keras API."
32237,iOS framework build of tf_lite has missing _TFLGpuDelegateCreate object,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- TF Version: b50852ccac3d9e90af0568391ace90fc1da440e1 (master on Sep. 5 2019)
- build type: built with `sh tensorflow/lite/tools/make/build_ios_universal_lib.sh`
- trying to compile for iOS on Xcode 10.2


**Describe the problem**

- Add TensorFlow_lite.framework to target, link and add header search path as no module header avaiable
- Following error on build: `""Undefined symbols for architecture arm64:
  ""_TFLGpuDelegateCreate"", referenced from:""` in `.mm` class calling `TFLGpuDelegateCreate(&options)`"
32236,boolean_mask does not accept a Tensor as axis,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): unknown 1.14.0
- Python version: 3.6.8 (Anaconda)
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**

[`tf.boolean_mask`](https://www.tensorflow.org/api_docs/python/tf/boolean_mask) does not accept a scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) object as axis parameter.

**Describe the expected behavior**

As per the docs, [`tf.boolean_mask`](https://www.tensorflow.org/api_docs/python/tf/boolean_mask) should accept a scalar  [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) object as axis parameter.

> * `axis`: A 0-D int Tensor representing the axis in `tensor` to mask from. By default, axis is 0 which will mask from the first dimension. Otherwise K + axis <= N.

**Code to reproduce the issue**

The following snippet:

```py
import tensorflow as tf
tf.boolean_mask([1, 2, 3], [True, False, True], axis=tf.constant(0, dtype=tf.int32))
```

Causes the exception:

```none
TypeError: slice indices must be integers or None or have an __index__ method
```

For comparison, the equivalent operation with [`tf.gather`](https://www.tensorflow.org/api_docs/python/tf/gather) works correctly:

```py
import tensorflow as tf
with tf.Session() as sess:
    print(sess.run(tf.gather([1, 2, 3], [0, 1], axis=tf.constant(0, dtype=tf.int32))))
    # [1 2]
```


**Other info / logs**

Full traceback:

```none
TypeError                                 Traceback (most recent call last)
<ipython-input-3-4beb5ed72842> in <module>
      1 import tensorflow as tf
----> 2 tf.boolean_mask([1, 2, 3], [True, False, True], axis=tf.constant(0, dtype=tf.int32))

~\AppData\Local\Continuum\anaconda3\envs\tf_test\lib\site-packages\tensorflow\python\ops\array_ops.py in boolean_mask(tensor, mask, name, axis)
   1369           "" are None.  E.g. shape=[None] is ok, but shape=None is not."")
   1370     axis = 0 if axis is None else axis
-> 1371     shape_tensor[axis:axis + ndims_mask].assert_is_compatible_with(shape_mask)
   1372
   1373     leading_size = gen_math_ops.prod(shape(tensor)[axis:axis + ndims_mask], [0])

~\AppData\Local\Continuum\anaconda3\envs\tf_test\lib\site-packages\tensorflow\python\framework\tensor_shape.py in __getitem__(self, key)
    861     if self._dims is not None:
    862       if isinstance(key, slice):
--> 863         return TensorShape(self._dims[key])
    864       else:
    865         if self._v2_behavior:

TypeError: slice indices must be integers or None or have an __index__ method
```"
32235,tf.scatter_nd sums updates if indices are duplicated,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): tensorflow-gpu==2.0.0rc0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
tf.scatter_nd sums update values if indices are present multiple times. I use this function for moving pixels in an image. Some pixels are present twice and this gives wrong pixel values in scatter.

tf.Tensor([ 0 11  0 10  9  0  0 12], shape=(8,), dtype=int32)
tf.Tensor([ 0 11  0 10  9  0  0 24], shape=(8,), dtype=int32)
tf.Tensor([ 0 11  0 10  9  0  0 36], shape=(8,), dtype=int32)
**Describe the expected behavior**
I expect this behavior:

tf.Tensor([ 0 11  0 10  9  0  0 12], shape=(8,), dtype=int32)
tf.Tensor([ 0 11  0 10  9  0  0 12], shape=(8,), dtype=int32)
tf.Tensor([ 0 11  0 10  9  0  0 12], shape=(8,), dtype=int32)

Otherwise I need to find unique indices and delete multiples. And why it is adding? Maybe other options are possible then too.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
indices1 = tf.constant([[4], [3], [1], [7]])
updates1 = tf.constant([9, 10, 11, 12])

indices2 = tf.constant([[4], [3], [1], [7], [7]])
updates2 = tf.constant([9, 10, 11, 12, 12])

indices3 = tf.constant([[4], [3], [1], [7], [7], [7]])
updates3 = tf.constant([9, 10, 11, 12, 12, 12])

shape = tf.constant([8])

scatter1 = tf.scatter_nd(indices1, updates1, shape)
scatter2 = tf.scatter_nd(indices2, updates2, shape)
scatter3 = tf.scatter_nd(indices3, updates3, shape)

print(scatter1)
print(scatter2)
print(scatter3)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32234,TF-2.0: Keras model save load memory leakage,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tensorflow-gpu==2.0.0-beta1
- Python version: Python 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0.130/ cuDNN 7.6
- GPU model and memory: GeForce GTX 1050 Ti, 4036 MiB

**Describe the current behavior**
During saving and loading model I face memory leakage. Eventually it crashes with `OSError: [Errno 12] Cannot allocate memory`. During real training it happens quite fast.


**Describe the expected behavior**
I expect that memory should be cleaned.

**Code to reproduce the issue**
```
import tensorflow as tf
import numpy as np
from tqdm import tqdm
from memory_profiler import profile

data_array = np.random.random_sample((1, 1024))
tf_array = tf.constant(data_array, dtype=tf.float32)

input = tf.keras.Input((1, 1024))
hidden_layer = tf.keras.layers.Dense(1024)(input)
output = tf.keras.layers.Dense(1)(hidden_layer)
model = tf.keras.Model(inputs=[input], outputs=[output])

pred = model([tf_array])
print(pred)


@profile
def func():
    export_path = ""temp_export""
    tf.saved_model.save(model, export_path)
    imported = tf.saved_model.load(export_path)


for i in tqdm(range(1000000), total=1000000):
    func()
```

**Other info / logs**
Profiler logs (2 steps)
![Снимок экрана от 2019-09-05 15-14-15](https://user-images.githubusercontent.com/54936869/64340897-de95bf80-cfef-11e9-8069-4a8c532cd059.png)

"
32233,"Why tfrecord only support Int64, Float, Bytes, not support Int32 or Double？",for example， i only need Int32 ， it seems to need more space to store the Int64 data？ also i need Double but using Float will cut off the precision of data.
32232,Distributed Training is not working with input signature,"Distributed training is not working when I am passing input signature in tf.function


```
@tf.function(input_signature=input_signature)
def _distributed_train_step(self, inputs, targets):
        def step_fn(inp, tar):
            with tf.GradientTape() as tape:
                logits = self(inp, training=True)
                cross_entropy = self.get_loss(tar, logits)
                loss = tf.reduce_sum(cross_entropy) * (1.0 / self.batch_size)

            with tf.name_scope(""gradients""):
                gradients = tape.gradient(loss, self.trainable_variables)
                if self.grad_clip:
                    gradients = [(tf.clip_by_value(grad, -self.clip_value, self.clip_value))
                                 for grad in gradients]
                self.optimizer.apply_gradients(list(zip(gradients, self.trainable_variables)))
            return cross_entropy

        per_example_losses = self.mirrored_strategy.experimental_run_v2(
            step_fn, args=(inputs, targets))

        mean_loss = self.mirrored_strategy.reduce(
            tf.distribute.ReduceOp.SUM, per_example_losses, axis=0)

        mean_loss = mean_loss / self.batch_size
        perplexity = self.get_perplexity(mean_loss)
        step = self.optimizer.iterations
        self._log_model_summary_data(self.train_writer,
                                     step,
                                     mean_loss,
                                     perplexity)

        return step, mean_loss, perplexity
```
Please have a look at traceback.

`Traceback (most recent call last):
  File ""train_gpt2.py"", line 86, in <module>
    train()
  File ""/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""train_gpt2.py"", line 59, in train
    model.fit([dist_train_dataset, dist_test_dataset])
  File ""/home/abhay/edge_gpt2/model_gpt.py"", line 359, in fit
    step, train_loss, perplexity = self.distributed_train_step(inputs, targets)
  File ""/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 439, in __call__
    return self._stateless_fn(*args, **kwds)
  File ""/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1821, in __call__
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File ""/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2104, in _maybe_define_function
    *args, **kwargs)
  File ""/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1650, in canonicalize_function_inputs
    self._flat_input_signature)
  File ""/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1710, in _convert_inputs_to_signature
    format_error_message(inputs, input_signature))
ValueError: When input_signature is provided, all inputs to the Python function must be convertible to tensors:
  inputs: (
    PerReplica:{
  0 /job:localhost/replica:0/task:0/device:GPU:0: tf.Tensor`
`ValueError: When input_signature is provided, all inputs to the Python function must be convertible to tensors:`"
32231,Bug in categorical_crossentropy loss (label smoothing),"Possible bug found in categorical_crossentropy's label smoothing argument in [line](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/losses.py#L940-L941) and for TF2 its in [line](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/losses.py#L960-L961)

The axis specified in num_class is 1 which works fine for a  Rank 2 tensor but fails for higher rank tensors.

def _smooth_labels():
    num_classes = math_ops.cast(array_ops.shape(y_true)[1], y_pred.dtype)   #  -1 instead of 1 
    return y_true * (1.0 - label_smoothing) + (label_smoothing / num_classes)

  y_true = smart_cond.smart_cond(label_smoothing,
                                 _smooth_labels, lambda: y_true)
  return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)"
32230,tf 2.0.0-rc0 tutorial colab (hub_with_keras) fails when batching images for simple model training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab
- TensorFlow version (use command below): 2.0.0-rc0
- Python version: 3.6.8

**Describe the current behavior**
Original bug report: https://github.com/tensorflow/hub/issues/359

Using tf 2.0.0-rc0 and constructing a simple Keras model fails to train. The issue seems to be due to batching in the tf.keras.preprocessing.image.ImageDataGenerrator().flow_from_dictionary(). 
Maybe some default parameters were changed.
If this is the case, the tutorial colab has to be adapted:
https://www.tensorflow.org/beta/tutorials/images/hub_with_keras

**Code to reproduce the issue**
Original official colab: https://www.tensorflow.org/beta/tutorials/images/hub_with_keras
Issue first reported based on this colab: https://colab.research.google.com/gist/MokkeMeguru/788255c93d4a4d8d4485098a7ba5e9f5/hub_with_keras.ipynb
Simplified colab reproducing the error with a simple model:
https://colab.sandbox.google.com/drive/1YI0XnFOgt9OEUzpvlAP4Tcb1W9NkPMdN#scrollTo=OGNpmn43C0O6

"
32229,why tensorflow 1.14 does not have contrib module?,"```
    hparams = tf.contrib.training.HParams(
AttributeError: module 'tensorflow' has no attribute 'contrib'
(env) [16:29:19] fagangjin:tf2_tacotron2_chinese git:(master*) $ python
Python 3.5.2 (default, Nov 12 2018, 13:43:14) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
t>>> tf.__version__
'1.14.0'
>>> 

```

Does this normal?"
32228,Build error C2589 on x64-Windows-static,"Hi Developers,

I recently tried to build tensorflow using vcpkg, and I have encountered some compilation errors.
After two days of research this issue, I gave up. And I am not familiar with the bazel compiler.
So can someone help me?

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 x64 build 1903**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version: **v1.14.0**
- Python version: **3.7.3**
- Installed using virtualenv? pip? conda?: **vcpkg**
- Bazel version (if compiling from source): **0.25.2**
- GCC/Compiler version (if compiling from source): **Visual Studio 2017 ver 15.9.13**
- CUDA/cuDNN version:  **N/A**
- GPU model and memory: **CPU model, 32GB Memory**



**Describe the problem**
First build, error like:
`.\tensorflow/stream_executor/rng.h(66): error C2589: 'constant': illegal token on right side of '::'`
Tried to build **again**, the build was **successful**. I don't know why. 



**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Clone vcpkg from `https://github.com/microsoft/vcpkg.git`.
2. Build vcpkg with command `.\bootstrap.bat`.
3. Install tensorflow with command `.\vcpkg.exe install tensorflow-cc:x64-windows-static`.



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
[build-x64-windows-static-err.log](https://github.com/tensorflow/tensorflow/files/3577942/build-x64-windows-static-err.log)
[build-x64-windows-static-out.log](https://github.com/tensorflow/tensorflow/files/3577943/build-x64-windows-static-out.log)
[config-x64-windows-static-err.log](https://github.com/tensorflow/tensorflow/files/3577944/config-x64-windows-static-err.log)
[config-x64-windows-static-out.log](https://github.com/tensorflow/tensorflow/files/3577945/config-x64-windows-static-out.log)


Thanks."
32227,`tf.strings.regex_replace` cannot add special characters on rewrite pattern,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.4
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0b1 (CPU version)
- Python version: 3.7.4

**Describe the current behavior**
Google's re2 library seems to be unable to handle rewrite patterns with characters like `\n`, `\t`, etc.

**Describe the expected behavior**
The expected output of the code below should be `<tf.Tensor: id=4, shape=(), dtype=string, numpy=b'hello\tworld'>`

**Code to reproduce the issue**
```
>>> s = ""hello world""
>>> tf.strings.regex_replace(s, r"" "", r""\t"")
external/com_googlesource_code_re2/re2/re2.cc:925: invalid rewrite pattern: \t
<tf.Tensor: id=4, shape=(), dtype=string, numpy=b'helloworld'>
```"
32224,Deeplab V3 TFLite starter model has wrong input details shape dtype,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

Current Behavior:
When running the starter model found [here](https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/deeplabv3_257_mv_gpu.tflite) and printing out the input details, the dtype in the shape is labeled as ""int32"" but the model expects ""float32""
input details = [{'name': 'sub_7',
  'index': 183,
  'shape': array([  1, 257, 257,   3], dtype=int32),
  'dtype': numpy.float32,
  'quantization': (0.0, 0)}]

When feeding an image of dtype int32:
ValueError: Cannot set tensor: Got tensor of type INT32 but expected type FLOAT32 for input 183, name: sub_7 

TLDR: dtype and shape dtype are inconsistent. The dtype in the input details shape should be float32."
32222,AttributeError: module 'tensorflow' has no attribute 'indices',"**System information**
- TensorFlow version (you are using): 1.14 (colab)
- Are you willing to contribute it (Yes/No): not sure how

**Describe the feature and the current behavior/state.**
np.indices is useful to quickly get coordinates for each element in a shape, tensorflow doesn't have this so user must import numpy for this

**Will this change the current api? How?**
sure, `coordinates = tf.indices(x.shape[1:-1], dtype=DTYPE)` would allow one to get coordinates for a tensor. Ideally it would have a `normalize=True` kwarg where user can get normalized coordinates for coordinate convolution https://arxiv.org/abs/1807.03247

**Who will benefit with this feature?**
anyone who uses convolution, and wants code to run faster (i.e. by not importing numpy)

**Any Other info.**
```
import tensorflow as tf
import numpy as np
coordinates_np = np.indices((28, 28))  # works
coordinates_tf = tf.indices((28, 28))  # crashes
```

> AttributeError: module 'tensorflow' has no attribute 'indices'

[https://docs.scipy.org/doc/numpy/reference/generated/numpy.indices.html](np.indices docs)"
32221,"AttributeError: 'Tensor' object has no attribute ['order', 'degree', 'rank']","**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No): Yes. `return len(self.shape) - 1`

**Describe the feature and the current behavior/state.**

from wikipedia:
> The order (also degree or rank) of a tensor is thus the sum of the orders of its arguments plus the order of the resulting tensor. This is also the dimensionality of the array of numbers needed to represent the tensor with respect to a specific basis, or equivalently, the number of indices needed to label each component in that array.

I assumed I could do x.rank ; x.order, x.degree to get this value for a tensor. This is useful when we build a layer we need to do different functions for different order of tensors (e.g. image vs sequence data) 

**Will this change the current api? How?**
User can quickly access the rank of a tensor without having to recalculate it for each tensor. Tensors by definition have an order, so this change allows TF tensors to have the normal attributes a tensor would have in linear algebra discussion

**Who will benefit with this feature?**
Users making custom layers often need to do boilerplate: `x_rank = len(x.shape) - 1 or such, and  might do it wrong. 

I made some neural interfaces to resize/reshape data to a common code, but convolution is a lot more effective if you concat coords to the tensor (https://arxiv.org/abs/1807.03247); logic often differs for different tensor ranks, so rather than users rewrite the 'len(x.shape) - N' 123456789 times, maybe we can just add this attribute to the Tensor class?   

**Any Other info.**
tf.keras.layers.Input mutates input shape to add batch size. I'm not sure how to handle the [mandatory] shape mutation in keras, I personally detest that, but it's not going away, guess we can just increment the rank, or not, you guys r smart, what do you think, is a batch of tensors a 1-higher-rank tensor? cc @alextp @fchollet

Note: TFP lib has a ""batch_shape"" tuple, not integer, not sure if that causes issues, they have a 'param_shape' or such function to help with it 

example with image data sort of shape
```
import tensorflow as tf
import numpy as np

print('tf:', tf.__version__)
print('np:', np.__version__)

SHAPE_AFTER_KERAS_MUTATES_IT = (1, 123, 123, 4)
y = tf.random.normal(SHAPE_AFTER_KERAS_MUTATES_IT)

SHAPE_WITHOUT_BATCH_OR_CHANNELS = y.shape[1:-1]
coords_np = np.indices(SHAPE_WITHOUT_BATCH_OR_CHANNELS)   # no tf.indices ? :(
coords = tf.convert_to_tensor(coords_np, dtype=tf.float32)

if y.rank is 3:  # <--- this fails and we have to manually compute rank
     coords = tf.transpose(coords, [1, 2, 0])
     coords = tf.expand_dims(coords, 0)
     coords.shape

y_with_coords = tf.concat([y, coords], -1)
```"
32219,TFP 0.8-rc0 AttributeError: 'MultivariateNormalTriL' object has no attribute 'type',"I am using TFP 0.8-rc0 with TF2.0.0-rc0 for a VAE composed with keras functional API. I am able to train the VAE however when I want to retrieve the latent code by calling the encoder only (using .predict()), it throws 

> AttributeError: 'MultivariateNormalTriL' object has no attribute 'type' 

which is not very informative in itself and I am not sure what it refers to. I am a bit confused why the decoder input seems to accept the output of the TFP layer without problem but can't seem to call predict on the encoder alone. Note that calling predict on encoder alone works when replacing the TFP layer with an equivalent keras sampling class. I understand that the tfp layer, when no convert_to_tensor_fn is defined, should call sample() by default so returning its output with encoder.predict(samples) should be possible.

Here is a minimal reproduction code:



```

import numpy as np
import os
import tensorflow_probability as tfp
tfd = tfp.distributions
tfpl = tfp.layers.distribution_layer
import tensorflow as tf


load_model = 1

latent_dim = 8
learning_rate = 1e-4

BATCH_SIZE = 100
TEST_BATCH_SIZE = 10


color_channels = 1
(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()

train_images = train_images[:5000,::]
n_trainsamples = np.shape(train_images)[0]
imsize = np.shape(train_images)[1]
train_images = train_images.reshape(-1, imsize, imsize, 1).astype('float32')
train_images /= 255.
train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_images)).shuffle(n_trainsamples).repeat().batch(BATCH_SIZE)

image_input = tf.keras.Input(shape=(imsize, imsize, color_channels), name='encoder_input')
x = tf.keras.layers.Flatten()(image_input)
x = tf.keras.layers.Dense(500, activation='softplus', name=""Inference-l1_Dense"")(x)
x = tf.keras.layers.Dense(tfpl.MultivariateNormalTriL.params_size(latent_dim))(x)
z = tfpl.MultivariateNormalTriL(latent_dim)(x)
prior = tfd.Independent(tfd.Normal(loc=[0., 0], scale=1), reinterpreted_batch_ndims=1)
tfpl.KLDivergenceAddLoss(prior, weight=1.0)

encoder = tf.keras.Model(inputs=image_input, outputs=z, name='encoder')

latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')
x = tf.keras.layers.Dense(500, activation='softplus', name=""Generative-l1_Dense"")(latent_inputs)
x = tf.keras.layers.Dense(imsize ** 2 * color_channels, activation='sigmoid', name=""Generative-l3_Dense_out"")(x)  

output_probs = tf.keras.layers.Reshape(target_shape=(imsize, imsize, color_channels), name=""Generative-output_probs"")(x)

decoder = tf.keras.Model(inputs=latent_inputs, outputs=output_probs, name='decoder')
output_probs = decoder(z)

vae_model = tf.keras.Model(inputs=image_input, outputs=output_probs, name='vae')

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
vae_model.compile(optimizer, tf.keras.losses.BinaryCrossentropy())

vae_model.fit(train_dataset, steps_per_epoch=n_trainsamples // BATCH_SIZE, epochs=4)

latents = encoder.predict(train_images[:4,::])
print('latent shape: ' + latents.shape())


```



And the full stack with the error:

> 
> 2019-09-04 21:56:37.276275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
> 2019-09-04 21:56:37.276332: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
> 2019-09-04 21:56:37.278632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2019-09-04 21:56:37.278665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
> 2019-09-04 21:56:37.278682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
> 2019-09-04 21:56:37.280141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: eaaf:00:00.0, compute capability: 3.7)
> Train for 50 steps
> Epoch 1/4
> WARNING: Logging before flag parsing goes to stderr.
> W0904 21:56:38.184839 140359104337664 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
> Instructions for updating:
> Use tf.where in 2.0, which has the same broadcast rule as np.where
> 2019-09-04 21:56:39.006765: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
> 50/50 [==============================] - 1s 29ms/step - loss: 0.2975
> Epoch 2/4
> 50/50 [==============================] - 0s 5ms/step - loss: 0.2114
> Epoch 3/4
> 50/50 [==============================] - 0s 5ms/step - loss: 0.1730
> Epoch 4/4
> 50/50 [==============================] - 0s 5ms/step - loss: 0.1590
> Traceback (most recent call last):
>   File ""/home/pycharm_project/VAE/save_issue_reproduction.py"", line 55, in <module>
>     latents = encoder.predict(train_images[:4,::])
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py"", line 915, in predict
>     use_multiprocessing=use_multiprocessing)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 722, in predict
>     callbacks=callbacks)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 189, in model_iteration
>     f = _make_execution_function(model, mode)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 565, in _make_execution_function
>     return model._make_execution_function(mode)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py"", line 2155, in _make_execution_function
>     self._make_predict_function()
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py"", line 2145, in _make_predict_function
>     **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/backend.py"", line 3658, in function
>     return EagerExecutionFunction(inputs, outputs, updates=updates, name=name)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/backend.py"", line 3555, in __init__
>     base_graph=source_graph)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/lift_to_graph.py"", line 260, in lift_to_graph
>     add_sources=add_sources))
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/op_selector.py"", line 404, in map_subgraph
>     elif op.type == ""Placeholder"":
> AttributeError: 'MultivariateNormalTriL' object has no attribute 'type'
"
32218,tf.signal.fft2d speed is slow and unstable in RTX2080Ti,"**System information**
- OS Platform: Linux Ubuntu 18.04 (server)
- TensorFlow installed from: docker tensorflow/tensorflow  1.14.0-gpu-py3-jupyter 
- TensorFlow version: 1.14.0
- Python version: 3.6.8
- CUDA:  v10.0
- GPU model: RTX 2080Ti 

**Describe the current behavior**
The speed of fft2d operation `tf.signal.fft2d` is very unstable at different iterations. Here is an example output of time every 100 iterations (code is shown below):
```
2019-09-04 19:03:57.731947
2019-09-04 19:04:33.715335
2019-09-04 19:05:10.976109
2019-09-04 19:05:44.012072
2019-09-04 19:06:15.616308
2019-09-04 19:07:14.961716
2019-09-04 19:08:12.324199
2019-09-04 19:09:11.560423
2019-09-04 19:10:08.877960
2019-09-04 19:11:08.102977
```
During the training, there is no other programs running.

Bur when running the same code in my local machine (GTX 1080Ti) with the same TensorFlow docker image. The speed is fast and stable.
```
2019-09-04 14:12:00.387114
2019-09-04 14:12:07.363174
2019-09-04 14:12:14.355784
2019-09-04 14:12:21.384377
2019-09-04 14:12:28.378524
```

**Describe the expected behavior**
The speed should be always very fast (about 7s/100iterations).

**Code to reproduce the issue**
```
import os
import datetime
import tensorflow as tf
import numpy as np

os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""

def main(_):
    dp_train = tf.placeholder(tf.float32, shape=(41, 300, 300, 1))
    dp = tf.complex(dp_train, 0.0)
    dp_fft = tf.abs(tf.signal.fft2d(dp))
    W = tf.get_variable('W_conv', [1, 1, 1, 1])  # not important, just want to make the training process to run.
    cost_train = tf.reduce_mean(tf.nn.conv2d(dp_fft, W, strides=[1, 1, 1, 1], padding='SAME'))

    opt = tf.train.AdamOptimizer(1e-4)
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):
        train_op = opt.minimize(cost_train)  # var_list=vars_digital

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)

        for i in range(0, 5000):
            data = np.random.rand(41,300,300,1)
            sess.run(train_op, feed_dict={dp_train:data})
            if i % 100 ==0:
                old_time = datetime.datetime.now()
                print(old_time)

        coord.request_stop()
        coord.join(threads)

if __name__ == '__main__':
    tf.app.run()

```
"
32215,Weights that are SparseTensor values do not get gradients in eager execution mode,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0 also reproduces on v2.0.0-beta1-5101-gc75bb66 2.0.0-rc0
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: None
- GPU model and memory: None

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When trying to train a subclassed Keras model that has a trainable parameter that is the values Tensor of a SparseTensor weight matrix, the training fails with an error message saying that the parameter does not have gradients. This problem reproduces only in eager mode.

**Describe the expected behavior**

The values of non-zero elements in sparse weight matrices with fixed indices should be trainable.

**Code to reproduce the issue**

The code for the two versions only differs in enabling eager execution and the loss function.

```python
import tensorflow as tf
import numpy as np

#Without this line the script works in v1.14.
#comment out this line for v2.0-rc0
tf.enable_eager_execution()


def matmul_dense_sparse(a, b):
    ta = tf.transpose(a)
    tb = tf.sparse.transpose(b)
    return tf.transpose(tf.sparse.sparse_dense_matmul(tb, ta))


class SparseLayer(tf.keras.layers.Layer):
    def __init__(self, indices, shape):
        super().__init__()
        self.indices = indices
        self.shape = shape

    def build(self, input_shape):
        self.w = self.add_weight(name='w',
                        shape=(self.indices.shape[0],),
                        trainable=True)
        self.sparse_mat = tf.sparse.reorder(tf.sparse.SparseTensor(self.indices, self.w, self.shape))
        super().build(input_shape)

    def call(self, inputs):
        return matmul_dense_sparse(inputs, self.sparse_mat)


class SparseModel(tf.keras.Model):
    def __init__(self, indices, shape):
        super().__init__()
        self.l1 = SparseLayer(indices, shape)

    def call(self, inputs):
        return self.l1(inputs)

indices = np.array([[1, 2], [30, 1], [30, 3], [45, 2], [56, 2], [32, 4]])

ex_x = np.random.rand(20, 100)
ex_y = np.random.rand(20, 5)

model = SparseModel(indices, (100, 5))

#compile for v1.14
model.compile(tf.keras.optimizers.SGD(), tf.losses.mean_squared_error)
#compile for v2.0-rc0
#model.compile(tf.keras.optimizers.SGD(), tf.losses.MeanSquaredError())

model.fit(ex_x, ex_y)
```

**Other info / logs**
Traceback in v1.14:
```
Traceback (most recent call last):
  File ""bug_reprod.py"", line 43, in <module>
    model.fit(ex_x, ex_y)
  File ""/home/mero/.ve/tfsim-P0VFtrg_/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 780, in fit
    steps_name='steps_per_epoch')
  File ""/home/mero/.ve/tfsim-P0VFtrg_/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 157, in model_iteration
    f = _make_execution_function(model, mode)
  File ""/home/mero/.ve/tfsim-P0VFtrg_/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 532, in _make_execution_function
    return model._make_execution_function(mode)
  File ""/home/mero/.ve/tfsim-P0VFtrg_/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2276, in _make_execution_function
    self._make_train_function()
  File ""/home/mero/.ve/tfsim-P0VFtrg_/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 2219, in _make_train_function
    params=self._collected_trainable_weights, loss=self.total_loss)
  File ""/home/mero/.ve/tfsim-P0VFtrg_/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 491, in get_updates
    grads = self.get_gradients(loss, params)
  File ""/home/mero/.ve/tfsim-P0VFtrg_/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 398, in get_gradients
    ""K.argmax, K.round, K.eval."".format(param))
ValueError: Variable <tf.Variable 'sparse_model/sparse_layer/w:0' shape=(6,) dtype=float32> has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.
```

Traceback in 2.0-rc0:

```
Traceback (most recent call last):
  File ""bug_reprod.py"", line 42, in <module>
    model.fit(ex_x, ex_y)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 734, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 324, in fit
    total_epochs=epochs)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 86, in execution_function
    distributed_function(input_fn))
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 427, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 370, in _initialize
    *args, **kwds))
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1847, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2147, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2038, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 320, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 73, in distributed_function
    per_replica_function, args=(model, x, y, sample_weights))
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 760, in experimental_run_v2
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 1787, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 2132, in _call_for_each_replica
    return fn(*args, **kwargs)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 292, in wrapper
    return func(*args, **kwargs)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 264, in train_on_batch
    output_loss_metrics=model._output_loss_metrics)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py"", line 311, in train_on_batch
    output_loss_metrics=output_loss_metrics))
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py"", line 272, in _process_single_batch
    model.optimizer.apply_gradients(zip(grads, trainable_weights))
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py"", line 427, in apply_gradients
    grads_and_vars = _filter_grads(grads_and_vars)
  File ""/home/mero/.ve/tfsim-8nc9BEC-/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py"", line 1025, in _filter_grads
    ([v.name for _, v in grads_and_vars],))
ValueError: No gradients provided for any variable: ['sparse_model/sparse_layer/w:0'].
```


Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32214, Error installing tensorflow centos 7,"
hi guys, i'm trying to install tensorflow on centos 7, i have python 3.7.4 installed.

When trying to give the pip looks like the message:
Could not find a version that satisfies the requirement tensorflow (from versions:) No matching distribution found for tensorflow


Could you please help me?"
32213,[micro] TFLite optimization failed with TFLite micro interpreter,"**System information**
- custom code: Modified TFLite micro example, **hello_world**, to use MNIST.
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: v1.12.1-3259-gf59745a381 2.0.0-beta0
- Python version: 3.6.8

**Describe the current behavior**
Without TFL converter optimization, prediction on **micro interpreter** works.
With any TFL converter optimization, prediction on **micro interpreter** gets wild and produce **random** values.
For example,
`converter.optimizations = [tf.lite.Optimize.DEFAULT]`

**Describe the expected behavior**
Regardless of TFL converter optimization, prediction on **micro interpreter** should work.

**Code to reproduce the issue**
https://github.com/ehirdoy/tflm-helloworld/tree/bug

NG: https://github.com/ehirdoy/tflm-helloworld/commit/13fd1c39e368729f574f44daf93299470ec1649d HEAD
OK: https://github.com/ehirdoy/tflm-helloworld/commit/a025d2a5f5c2487b551b6bfd8414926ebf864bfb HEAD^

> $ git checkout -f bug
> $ make input_data.h
> $ make
> $ ./hello_world # NG
> $ git checkout HEAD^
> $ make
> $ ./hello_world # OK

**Other info / logs**
Logs are in the git commit mesage in the above, {OK,NG} commits.

> commit 13fd1c39e368729f574f44daf93299470ec1649d (HEAD, origin/bug, github/bug)
> Author: Hiroshi Doyu <hiroshi.doyu@ericsson.com>
> Date:   Wed Sep 4 14:14:33 2019 +0300
> 
>     [BUG?] store TFLite file optimized for size
> 
>     Without the following
>       'converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]',
>     MNIST prediction works OK as seen in the previous commit log.
> 
>     With the above OPTIMIZE_FOR_SIZE inserted right before convert(),
>     MNIST prediction seems to go wild with random values as seen below:
> 
>       $ touch train-mnist-tflite.py
>       $ make
>       $ ./hello_world
> 
>           ......
>           ................
>           ................
>                ...........
>                       ....
>                      ....
>                      ....
>                     ....
>                     ....
>                    ....
>                    ...
>                   ....
>                  ....
>                 .....
>                 ....
>                .....
>                ....
>               .....
>               .....
>               ....
> 
>     0: 0.096435
>     1: 0.083848
>     2: 0.109924
>     3: 0.096781
>     4: 0.103029
>     5: 0.100698
>     6: 0.095319
>     7: 0.095055
>     8: 0.114907
>     9: 0.104004
>
>     0: 0.000000
>     1: 0.000000
>     2: 0.000000
>     3: 0.000000
>     4: 0.000000
>     5: 0.000000
>     6: 0.000000
>     7: 0.000000
>     8: 1.000000
>     9: 0.000000
> "
32212,"Raise undefined symbol in ""tensorflow.contrib.fused_conv.python.ops.fused_conv2d_bias_activation_op"" module","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `CentOS Linux release 7.6.1810 (Core)`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): `source`
- TensorFlow version: `1.14.0`
- Python version: `3.6.5`
- Installed using virtualenv? pip? conda?: `conda`
- Bazel version (if compiling from source): `0.25.0`
- GCC/Compiler version (if compiling from source): `4.8.5`
- CUDA/cuDNN version: `CUDA==10.1 / CUDNN=7`
- GPU model and memory: `GeForce GTX 1080   8119MiB`



**Describe the problem**

```
>>> from tensorflow.contrib.fused_conv.python.ops.fused_conv2d_bias_activation_op import *
2019-09-04 13:57:26.493744: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
WARNING: Logging before flag parsing goes to stderr.
W0904 13:57:28.321362 140054143231808 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/root/conda/lib/python3.6/site-packages/tensorflow/contrib/fused_conv/__init__.py"", line 22, in <module>
    from tensorflow.contrib.fused_conv.python.ops.fused_conv2d_bias_activation_op import *
  File ""/root/conda/lib/python3.6/site-packages/tensorflow/contrib/fused_conv/python/ops/fused_conv2d_bias_activation_op.py"", line 26, in <module>
    resource_loader.get_path_to_datafile(""_fused_conv2d_bias_activation_op.so""))
  File ""/root/conda/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py"", line 56, in load_op_library
    ret = load_library.load_op_library(path)
  File ""/root/conda/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py"", line 61, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: /root/conda/lib/python3.6/site-packages/tensorflow/contrib/fused_conv/python/ops/_fused_conv2d_bias_activation_op.so: undefined symbol: _ZN10tensorflow6Logger18singleton_factory_E
>>>
```
I follow Anaconda compiled tensorflow conda package method(conda-build and some referenced recipes) to compile tensorflow==1.14 with tensorRT and nccl features. 
```
[root@skyaxe-computing-1 work-dir]# curl -L -O  https://repo.anaconda.com/pkgs/main/linux-64/tensorflow-base-1.14.0-gpu_py36he45bfe2_0.tar.bz2
[root@skyaxe-computing-1 work-dir]# tar jxf tensorflow-base-1.14.0-gpu_py36he45bfe2_0.tar.bz2
# get recipes in info/recipe
[root@skyaxe-computing-1 work-dir]# ls -d  info/recipe/
info/recipe/
```

My `build.sh`, `meta.yaml`:
<details>

```
#!/bin/bash

tensorflow_version=$(echo $TENSORFLOW_VERSION_NUMBER | tr -d ""."")

# Compile tensorflow from source
export PYTHON_BIN_PATH=${PYTHON}
export PYTHON_LIB_PATH=${SP_DIR}
export CC_OPT_FLAGS=""-march=native""

# disable jemmloc (needs MADV_HUGEPAGE macro which is not in glib <= 2.12)
export TF_NEED_JEMALLOC=1
export TF_NEED_GCP=0
export TF_NEED_HDFS=0
export TF_NEED_S3=0
export TF_ENABLE_XLA=0
export TF_NEED_OPENCL=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_GDR=0
export TF_NEED_KAFKA=1
export TF_DOWNLOAD_CLANG=0
export TF_NEED_IGNITE=1
export TF_ENABLE_XLA=1
export TF_NEED_ROCM=0

if [[ ""$CUDNN_VERSION"" -le 6 ]]; then
    export TF_NEED_TENSORRT=0
else
    export TF_NEED_TENSORRT=1
    if [[ ""$CUDA_VERSION"" = ""9.2"" ]]; then
        export TENSORRT_INSTALL_PATH=/opt/compute/TensorRT-4.0.1.6
    elif [[ ""$CUDA_VERSION"" = ""10.1"" ]]; then
        export TENSORRT_INSTALL_PATH=/opt/compute/TensorRT-5.1.2.2
    else
        export TENSORRT_INSTALL_PATH=/opt/compute/TensorRT-3.0.4    # Modify it according to your requirement.
    fi
fi
export TF_NCCL_VERSION=2.4  # Modify it according to your requirement, defalut value is 1.3(from GitHub)
if [[ ""$CUDA_VERSION"" = ""8.0"" ]]; then
    export NCCL_INSTALL_PATH=/opt/compute/nccl_2.2.13-1+cuda8.0_x86_64  # Modify it according to your requirement.
elif [[ ""$CUDA_VERSION"" = ""9.0"" ]]; then
    export NCCL_INSTALL_PATH=/opt/compute/nccl_2.2.13-1+cuda9.0_x86_64  # Modify it according to your requirement.
elif [[ ""$CUDA_VERSION"" = ""9.2"" ]]; then
    export NCCL_INSTALL_PATH=/opt/compute/nccl_2.3.7-1+cuda9.2_x86_64  # Modify it according to your requirement.
elif [[ ""$CUDA_VERSION"" = ""10.1"" ]]; then
    export NCCL_INSTALL_PATH=/opt/compute/nccl_2.4.8-1+cuda10.1_x86_64
fi

# CUDA details, these should be customized depending on the system details
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=""${CUDA_VERSION}""
export TF_CUDNN_VERSION=""${CUDNN_VERSION}""
export GCC_HOST_COMPILER_PATH=""$(which gcc)""
export CUDA_TOOLKIT_PATH=""/usr/local/cuda""
export CUDNN_INSTALL_PATH=""/usr/local/cuda""
export TF_CUDA_CLANG=0

export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2""
if [[ ${CUDA_VERSION} = ""8.0"" ]]; then
    export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1""
elif [[ ""$CUDA_VERSION"" = ""9.0"" ]]; then
    export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
elif [[ ""$CUDA_VERSION"" = ""9.2"" ]]; then
    export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
elif [[ ""$CUDA_VERSION"" = ""10.1"" ]]; then
    export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0,7.5""
fi

export TF_NEED_MPI=0
# enable Intel MKL
if [[ ""$tensorflow_version"" -ge 120 ]]; then
    export TF_NEED_MKL=1
    export TF_DOWNLOAD_MKL=1
fi

export TF_NEED_VERBS=1
export TF_SET_ANDROID_WORKSPACE=0
./configure

bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 --copt=-mfpmath=both --config=cuda --jobs=$(nproc) \
    --config=mkl --verbose_failures --color=yes //tensorflow/tools/pip_package:build_pip_package

# build a whl file
mkdir -p $SRC_DIR/tensorflow_pkg
bazel-bin/tensorflow/tools/pip_package/build_pip_package $SRC_DIR/tensorflow_pkg

# install using pip from the whl file
pip install --no-deps $SRC_DIR/tensorflow_pkg/*.whl

rm -f ${PREFIX}/bin/tensorboard
```
```
package:
    name: tensorflow-gpu
    version: {{ version }}

source:
  git_url: https://github.com/tensorflow/tensorflow.git
  git_rev: v{{ version }}


build:
    entry_points:
        - tensorboard = tensorflow.tensorboard.tensorboard:main
    script_env:
        - CUDA_VERSION
        - CUDNN_VERSION
        - TENSORFLOW_VERSION_NUMBER
    string: py{{ CONDA_PY }}_{{ string }}

requirements:
    build:
        - werkzeug
        - bleach
        - numpy 1.15.*
        - mkl
        - six
        - protobuf 3.6.*
        - python x.x
        - backports.weakref
        - html5lib
        - markdown
        - mock
        - keras-applications
        - keras-preprocessing
        - enum34 # [py27]
    run:
        - python
        - werkzeug
        - six
        - protobuf
        - numpy
        - markdown
        - html5lib
        - bleach
        - backports.weakref
```
</details>

**Any other info / logs**
* use `nvidia-docker` and `nvidia/cuda:10.1-cudnn7-devel-centos7` docker image
* import tensorflow is ok and `sess = tensorflow.Session(config=tensorflow.ConfigProto(log_device_placement=True))` will display visible gpu devices and others info
* similar behaviour can refer to: https://github.com/ContinuumIO/anaconda-issues/issues/11239 and https://github.com/tensorflow/tensorflow/issues?q=tensorflow.contrib.fused_conv.python.ops.fused_conv2d_bias_activation_op 
* pre-build wheel package will produce same error, error see https://user-images.githubusercontent.com/19144683/64264093-54077e80-cf63-11e9-9b92-83fc11a4be79.png
:
```
[root@skyaxe-computing-1 log-2019-07]# docker run -ti --rm --network host nvidia/cuda:10.1-cudnn7-devel-centos7 /bin/bash
[root@skyaxe-computing-1 /]# curl https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-4.3.30-Linux-x86_64.sh -L -O
... ...

[root@skyaxe-computing-1 /]# /root/miniconda3/bin/pip install -i   https://mirrors.aliyun.com/pypi/simple/   tensorflow-gpu==1.14.0
```
"
32211,[TF2] Nightly-20190904 broken import,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tf-nightly-2.0-preview (maybe others)
- Python version: 3.6 (probably others)

**Describe the current behavior**
```
!pip install tf-nightly-2.0-preview
import tensorflow as tf
```
Errors with:
```
AttributeError                            Traceback (most recent call last)
<ipython-input-2-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py in <module>()
     96 
     97 # We still need all the names that are toplevel on tensorflow_core
---> 98 from tensorflow_core import *
     99 
    100 # These should not be visible in the main tf module.

AttributeError: module 'tensorflow_core' has no attribute 'compiler'
```"
32210,[TF 2.0.0-rc0] Keras layer dependency tracking behaviour changed in 2.0.0-rc0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS, Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-rc0
- Python version: Python 3.6/3.7

We are developing an open source library for training binarized and other extremely quantized networks at http://larq.dev that heavily relies on `tf.keras`. For this we are adding custom layers that modify (i.e. quantize) the layers kernel (and activations) prior to the computation.

## Expected Behaviour

A simplified version of what we are doing is here:
```python
from tensorflow import keras

# For our use case this would be a quantizer with a custom gradient
def projection(x):
    return 2 * x

class CustomDense(keras.layers.Dense):
    def call(self, inputs):
        original_kernel = self.kernel
        self.kernel = projection(self.kernel)
        outputs = super().call(inputs)
        self.kernel = original_kernel  # reset the original kernel to make this work in eager mode
        return outputs

model = keras.models.Sequential([CustomDense(32, input_shape=(32,))])

assert model.layers[0].kernel in model.layers[0].trainable_weights
```

This worked well for TensorFlow 1.13, 1.14 in graph and eager as well as for `2.0.0-beta0` and `2.0.0-beta1`.

## Current Behaviour

It looks like TensorFlow `2.0.0-rc0` changed the automatic dependency tracking which makes the above code fail on the last line. The same is true for `2.0.0.dev20190904`:
```python-traceback
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-e416b6b97b33> in <module>
     40 model = keras.models.Sequential([CustomDense(32, input_shape=(32,))])
     41 
---> 42 assert model.layers[0].kernel in model.layers[0].trainable_weights

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in __bool__(self)
    874 
    875   def __bool__(self):
--> 876     return bool(self._numpy())
    877 
    878   __nonzero__ = __bool__

ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
```

Ignoring the automatic dependency tracking by using the following makes the above code work.
```python
from tensorflow import keras
from tensorflow.python.training.tracking.base import (
         no_automatic_dependency_tracking_scope,
     )

def projection(x):
    return 2 * x

class CustomDense(keras.layers.Dense):
    def call(self, inputs):
        original_kernel = self.kernel
        with no_automatic_dependency_tracking_scope(self):
            self.kernel = projection(self.kernel)
        outputs = super().call(inputs)
        with no_automatic_dependency_tracking_scope(self):
            self.kernel = original_kernel
        return outputs

model = keras.models.Sequential([CustomDense(32, input_shape=(32,))])

assert model.layers[0].kernel in model.layers[0].trainable_weights
```

## Questions

Is this behaviour change expected or is it a bug in `2.0.0-rc0`?

Is there a better way to achieve this, without needing to temporarily overwrite `layer.kernel`?

If this change is expected, is it save to use the private `no_automatic_dependency_tracking_scope` to workaround this issue?

@dynamicwebpaige Is this issue better posted at the TensorFlow 2.0 testing mailing list?"
32209,TensorFlow Lite softmax error if dims >4  (num_dims >= 1 && num_dims <= 4),"**System information**
- OS Platform and Distribution (Linux Ubuntu 16.04):
- TensorFlow installed from ( binary):
- TensorFlow version (1.13.1):
- Python version (3.7)

my model graph contain reshape op as follows:
self. pixel_link_logits = tf.reshape(self.pixel_link_logits,[shape[0], shape[1], shape[2], 16, 2])
self.pixel_link_scores = tf.nn.softmax(self.pixel_link_logits)

**this is my  tflite_convert code**

```
  6 graph_def_file = ""./model_v4.pb""
  7 input_arrays = [""evaluation_768x768/Placeholder""]
  8 output_arrays = [""evaluation_768x768/Squeeze"",""evaluation_768x768/Squeeze_1""]
  9 converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
 10
 11 converter.post_training_quantize=True
 12
 13 tflite_model = converter.convert()
 14 with open(""converted_model.tflite"", ""wb"") as f:
 15     f.write(tflite_model)
 16 interpreter = tf.lite.Interpreter(model_path=""converted_model.tflite"")
 17 interpreter.allocate_tensors()
 18 input_details = interpreter.get_input_details()
 19 output_details = interpreter.get_output_details()
 20 input_shape = input_details[0]['shape']
 21
 22 input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
 23 interpreter.set_tensor(input_details[0]['index'], input_data)
 24 interpreter.invoke()
 25 output_data = interpreter.get_tensor(output_details[0]['index'])
 26 print(output_data)
```

I counter the error on softmax about num_dims >= 1 && num_dims <= 4, how can I resolver it @ry ,
**this is the error  logs**

when I interpreter the tflite model, counter the dimentions error, the infor:
Traceback (most recent call last):
  File ""interpreter.py"", line 17, in <module>
    interpreter.allocate_tensors()
  File ""/home/xxx/miniconda3/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py"", line 73, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""/home/dengjie/miniconda3/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 106, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/activations.cc:216 num_dims >= 1 && num_dims <= 4 was not true.Node number 146 (SOFTMAX) failed to prepare

I found the tensorflow source code about softmax  only surpport 4 dimensions:
the code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/activations.cc
![image](https://user-images.githubusercontent.com/12806623/64256590-5ca58800-cf56-11e9-8541-15446005d58a.png)
how can i solve it， who help me？"
32208,make_adv_reg_config not found,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

While trying to run the code given on https://www.tensorflow.org/neural_structured_learning/ I ran into the following error:
AttributeError: module 'neural_structured_learning.lib' has no attribute 'make_adv_reg_config'
I have already installed neural_structured_learning using !pip install neural_structured_learning and I am working on TF 2.0.0-rc0 on Colab (also tried it on 1.14)
I have also check the code on github and couldn't find make_adv_reg_config in neural_structured_learning/lib/*

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32207,Unsupported ops,"**System information**
- OS Platform and Distribution: Ubuntu 18.04.3:
- TensorFlow installed from: binary
- TensorFlow version: 2.0.0-rc0

_Here's my code:_

converter = tf.lite.TFLiteConverter.from_saved_model(model_dir)
tflite_model = converter.convert()
open(os.path.join(model_dir, 'model.tflite'), ""wb"").write(tflite_model)

_Here's the error:_

Here is a list of operators for which you will need custom implementations: **Complex, Imag, Real, TensorListReserve, TensorListStack, While**

Hello Tensorflow team, I would love to see the ops above supported in tflite. Any chance you can 
share with me your plans regarding this request?"
32206,"tflite: RandomStandardNormal not convertible with pure tensorflow, but works with keras","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): docker
- TensorFlow version (use command below): 1.15.0-dev20190821
- Python version: 3.6.8

I have an variational autoencoder model utilizing `tf.random.normal` function. I want to convert it into a tflite model.

I have written code in keras that achieves that. The conversion works and I get reasonable results. However, if I re-write the model with tensorflow-only operations (no keras so to speak), the converter claims that RandomStandardNormal is not available for tflite:
```
2019-09-04 12:04:12.858787: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: RandomStandardNormal
2019-09-04 12:04:12.859104: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 21 operators, 32 arrays (0 quantized)
2019-09-04 12:04:12.859336: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 21 operators, 32 arrays (0 quantized)
2019-09-04 12:04:12.859712: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 9 operators, 21 arrays (0 quantized)
2019-09-04 12:04:12.859823: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 9 operators, 21 arrays (0 quantized)
2019-09-04 12:04:12.859876: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 9 operators, 21 arrays (0 quantized)
2019-09-04 12:04:12.859972: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 320 bytes, theoretical optimal value: 256 bytes.
2019-09-04 12:04:12.860009: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 8592
2019-09-04 12:04:12.860269: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, FULLY_CONNECTED, LOGISTIC, MUL. Here is a list of operators for which you will need custom implementations: RandomStandardNormal.
Traceback (most recent call last):
  File ""/usr/local/bin/toco_from_protos"", line 10, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, FULLY_CONNECTED, LOGISTIC, MUL. Here is a list of operators for which you will need custom implementations: RandomStandardNormal.
```
This is very confusing, as the keras model used the very same function and it worked. Does keras do something under the hood?

**Code to reproduce the issue**
I have created a repo for this [HERE](https://github.com/DocDriven/tflite_tests). Steps to reproduce this issue:
1. Execute `python keras_model.py` to train and save the keras model
2. Execute `python keras_conversion.py` to create the tflite model without errors
3. Execute `python tf_model.py` to train, save and freeze the pure tf model
4. Execute `python tf_conversion.py` to try the conversion and get the error
"
32205,KeyError: u'BatchMatMulV2',"Trying to generate a .pb file from .meta file but encountered this error when running in tensorflow 1.13.1. 
Works after changing tensorflow version to 1.14.0
But I need to generate .pb file using tensorflow 1.13.1 or below versions. 

Here's the code: 

```
import tensorflow as tf

meta_path = 'fns.ckpt.meta' # Your .meta file
output_node_names = ['add_37']    # Output nodes

with tf.Session() as sess:

    # Restore the graph
    saver = tf.train.import_meta_graph(meta_path)

    # Load weights
    saver.restore(sess,""checkpoints/fns.ckpt"")

    # Freeze the graph
    frozen_graph_def = tf.graph_util.convert_variables_to_constants(
        sess,
        sess.graph_def,
        output_node_names)

    # Save the frozen graph
    with open('output_graph.pb', 'wb') as f:
      f.write(frozen_graph_def.SerializeToString())
```"
32204,"Coral Board Demo example fails RuntimeError: Model provided has model identifier 'CTYP', should be 'TFL3'","Traceback (most recent call last):
  File ""/usr/bin/edgetpu_classify"", line 11, in <module>
    load_entry_point('edgetpuvision==1.0', 'console_scripts', 'edgetpu_classify')()
  File ""/usr/lib/python3/dist-packages/edgetpuvision/classify.py"", line 162, in main
    run_app(add_render_gen_args, render_gen)
  File ""/usr/lib/python3/dist-packages/edgetpuvision/apps.py"", line 70, in run_app
    display=args.displaymode):
  File ""/usr/lib/python3/dist-packages/edgetpuvision/gstreamer.py"", line 240, in run_gen
    inference_size = render_overlay_gen.send(None)  # Initialize.
  File ""/usr/lib/python3/dist-packages/edgetpuvision/classify.py"", line 112, in render_gen
    engines, titles = utils.make_engines(args.model, ClassificationEngine)
  File ""/usr/lib/python3/dist-packages/edgetpuvision/utils.py"", line 53, in make_engines
    engine = engine_class(model_path)
  File ""/usr/lib/python3/dist-packages/edgetpu/classification/engine.py"", line 38, in __init__
    super().__init__(model_path)
  File ""/usr/lib/python3/dist-packages/edgetpu/swig/edgetpu_cpp_wrapper.py"", line 300, in __init__
    this = _edgetpu_cpp_wrapper.new_BasicEngine(*args)
RuntimeError: Model provided has model identifier 'CTYP', should be 'TFL3'

This is the output of Coral Board with the Coral Camera example. The given tflite model produces the above output. I couldn't figure out why.

This output is from the ssh window connected to the google coral board."
32203,why some OPs are not shown on timeline,"why some OPs are not shown on timeline, such as FlatMapDataset, PrefetchDataset and ParallelMapDataset."
32201,Bug in the Tensorflow .pb to .tflite conversion using TFLiteConverter,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13 (CPU version)
- Python version: 3.6.9


**Describe the current behavior**
TFLiteConverter converts the first convolutional layer of tensorflow model into depthwise convolutional layer if the input to the model has single channel (In my case grey scale images with channel =1).

**Describe the expected behavior**

Convolutional layer in the tensorflow model should remain the same even after TFLiteConverter.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

link to tensorflow model: https://drive.google.com/open?id=14R84txljdYdTIZRmHAD1Rkfl-_n9fCGb

link to the generated tflite model: https://drive.google.com/open?id=1WlmWhJYjxfLIjwZcUJ9vLq8ZB5oBmaUD

Code used to generate tflite file from tensorflow model:

```
import tensorflow as tf
graph_def_file = ""single_layer.pb""
input_arrays =['s']
output_arrays =['conv/Relu']
output_file = ""single_layer.tflite""

converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
tflite_model = converter.convert()
open(output_file, ""wb"").write(tflite_model)

```

I used netron to visualise the .pb and .tflite graph, link: https://github.com/lutzroeder/netron

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Tensorflow graph:
![1](https://user-images.githubusercontent.com/32209750/64239177-0e659a00-cf00-11e9-9d99-67809d639104.PNG)

Tflite graph after TFLiteConverter:
![image](https://user-images.githubusercontent.com/32209750/64239288-45d44680-cf00-11e9-9fb4-18eddf063b4f.png)




"
32200,"[tflite] how to build tflite project to library for ios  bazel, swift, pod, make?","I want to build tflite project library to shared with other projects on android & ios.
It is easy to build android project library via bazel (cc_binary) and test it via adb. 
How about ios, use bazel, swift, pod, make,c++/objectc? how to test it? is there any example?"
32199,_num_gpus_per_worker,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
32196,ImportError: cannot import name 'Event' from 'tensorflow_core.python' (unknown location),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): with ""pip install tensorflow-gpu==2.0.0-rc0""
- TensorFlow version:tensorflow-gpu==2.0.0-rc0
- Python version:3.7.3
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):0.29.0
- CUDA/cuDNN version: CUDA 10.0 and 10.1/cuDNN 10.0 and 10.1
- GPU model and memory: NVIDIA GeForce GTX 1070/ 8 GB GDDR5

I'm using Jupyther notebook. I've installed tensorflow with 'pip install tensorflow-gpu==2.0.0-rc0' in my environment terminal. 
Now in Jupyter notebook trying to import with:
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization

And I've got this error:
`ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3296, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-167-72e06926552d>"", line 11, in <module>
    from tensorflow.keras.models import Sequential
  File ""C:\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 45, in <module>
    from tensorflow._api.v2 import compat
  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\_api\v2\compat\__init__.py"", line 23, in <module>
    from tensorflow._api.v2.compat import v1
  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\_api\v2\compat\v1\__init__.py"", line 72, in <module>
    from tensorflow._api.v2.compat.v1 import summary
  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\_api\v2\compat\v1\summary\__init__.py"", line 14, in <module>
    from tensorflow.python import Event
ImportError: cannot import name 'Event' from 'tensorflow_core.python' (unknown location)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2033, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1095, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""C:\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 313, in wrapped
    return f(*args, **kwargs)
  File ""C:\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 347, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""C:\Anaconda3\lib\inspect.py"", line 1502, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""C:\Anaconda3\lib\inspect.py"", line 1460, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""C:\Anaconda3\lib\inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""C:\Anaconda3\lib\inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""C:\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 45, in <module>
    from tensorflow._api.v2 import compat
  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\_api\v2\compat\__init__.py"", line 23, in <module>
    from tensorflow._api.v2.compat import v1
  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\_api\v2\compat\v1\__init__.py"", line 72, in <module>
    from tensorflow._api.v2.compat.v1 import summary
  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\_api\v2\compat\v1\summary\__init__.py"", line 14, in <module>
    from tensorflow.python import Event
ImportError: cannot import name 'Event' from 'tensorflow_core.python' (unknown location)

When I was using same code with Sublime Text it was working ok.
`"
32194,micro_speech preprocessing,"**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No): yes

tensorflow/examples/speech_commands project defines 3 different preprocessing modes. It is possible to train a ML model using all of them. 

Then there is an experimental project which will use the ML model from the 1st project on a micro-controller: tensorflow/lite/experimental/micro/examples/micro_speech/
This project ignores the preprocessing modes from the first project. By looking at the code in the micro_speech project it assumes the preprocessing mode always to be 'micro'.  The preprocessing includes some scaling operations and it looks like those operations are done inside 'micro_features_generator.cc' 
tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_features_generator.cc
There is an comment related to this: 'These scaling values are derived from those used in input_data.py in the training pipeline.'

The problems I see at the moment:
1 .It is very unclear how those magic scaling numbers in micro_features_generator.cc are derived from training pipeline. What if my pipeline parameters are not exactly the same as they were when those magic numbers where defined.   
2. It is impossible to use another preprocessing mode as 'micro' because it is not implemented on micro_speech project.
3. There is suspicious operation where pointer is always moved 160 steps forward. Why hardcoded 160? From where does this number come and why buffer pointer is always moved 160.  'frontend_input = input + 160; ' 
4. Documentation of the all preprocessing modes is lacking.

So at the moment if somebody use 'average' or 'mfcc' preprocessing mode then the input to the model is not correctly prepared in micro_speech project and it will fail to detect sounds. One should to implement all preprocessing modes in micro_features_generator.cc.

I am also not sure if 'micro' works always correctly because there is that hardcoded 160 which is obviously depend on some ML model input parameters."
32193,Inconsistent behavior of .shape when using sparse.placeholder,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
1.15.0-dev20190821
- Python version:
3.7.4

**Describe the current behavior**

sparse.placeholder does loses the shape entirely when batch is not provided. This is inconstent with how tf.placeholder behaves.
**Describe the expected behavior**
tf.placeholder and tf.sparse.placeholder have the same behavior.

**Code to reproduce the issue**

```
import tensorflow as tf

print(tf.__version__)

print(""Printing shape of sparse placeholder when using (None, 1024)"")
print(tf.compat.v1.sparse.placeholder(dtype=tf.float32, shape=(None, 1024)).shape)

print(""Printing shape of dense placeholder when using (None, 1024)"")
print(tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, 1024)).shape)

print(""Printing shape of sparse placeholder when using (-1, 1024)"")
print(tf.compat.v1.sparse.placeholder(dtype=tf.float32, shape=(-1, 1024)).shape)

print(""Printing shape of dense placeholder when using (-1, 1024)"")
print(tf.compat.v1.placeholder(dtype=tf.float32, shape=(-1, 1024)).shape)
```

**Other info / logs**

```
1.15.0-dev20190821
Printing shape of sparse placeholder when using (None, 1024)
(?, ?)
Printing shape of dense placeholder when using (None, 1024)
(?, 1024)
Printing shape of sparse placeholder when using (-1, 1024)
(?, 1024)
Printing shape of dense placeholder when using (-1, 1024)
Traceback (most recent call last):
  File ""/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py"", line 206, in make_shape
    shape = tensor_shape.as_shape(v)
  File ""/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 1216, in as_shape
    return TensorShape(shape)
  File ""/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 776, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 776, in <listcomp>
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 718, in as_dimension
    return Dimension(value)
  File ""/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py"", line 198, in __init__
    raise ValueError(""Dimension %d must be >= 0"" % self._value)
ValueError: Dimension -1 must be >= 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/tmp/sparse_shape.py"", line 15, in <module>
    print(tf.compat.v1.placeholder(dtype=tf.float32, shape=(-1, 1024)).shape)
  File ""/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py"", line 2619, in placeholder
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File ""/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py"", line 6667, in placeholder
    shape = _execute.make_shape(shape, ""shape"")
  File ""/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py"", line 211, in make_shape
    e))
ValueError: Error converting shape to a TensorShape: Dimension -1 must be >= 0.

```"
32192,JSON serializable issue at RemoteMonitor,"The Tensorflow implementation of RemoteMonitor callback raises the error
`Object of type float32 is not JSON serializable.`

Keras own implementation works fine with the same code.

I think the relevant code difference is 
    
```
for k, v in logs.items():
      send[k] = v

```
in the Tensorflow implementation and

```
for k, v in logs.items():
            if isinstance(v, (np.ndarray, np.generic)):
                send[k] = v.item()
            else:
                send[k] = v
```

in the Keras implementation.
"
32188,Efficient band matrix multiplication,"Tensorflow appears to lack an efficient way to do a multiplication by band matrix.

By this I mean calculating 

C[i,j] = sum_k A[i,j+k] B[i, k] 

For example, if B is

[ b00 b01 b02 .. b07]
[ b10 b11 b12 .. b17]
[ b20 b21 b22 .. b27]

this is equivalent to multiplying A by

[ b00  0   0 ...
[ b10 b01 0 ...
[ b20 b11 b02 ...
[ 0     b21 b12 ... ]
[ 0     0     b22 ... ]
...
[0   0 ...    b27]

This arises as part of the implementation of keras.layers.LocallyConnected1D(), which provides two possible implementations for it; both implementations are suboptimal. One creates a tensorflow op for every column, which means a lot of framework overhead, and the other expands B to the full form, which is inefficient and also seems to incur some overhead. (If B is 512x16, it should take one tensorflow op with 512x16 FMAs; implementation 1 would do 512 ops with 16 FMAs each;  implementation 2 would perform two ops, one with 512x529 float multiplications, and one with 512x529 FMAs less however much is saved internally by sparse_matmul().)

I tried to replace some dense layers with LocallyConnected1D() layers in @tensor2tensor transformer. It would not load at all with implementation 1 (it was still trying to build the graph after 10 minutes) and it ran significantly slower than with dense() (despite doing fewer FMAs) with implementation 2. 

The convolution would be easy to implement if one had direct access to tensor strides. (You simply set the second-to-last stride of A to stride[-2]+stride[-1] and do a regular matrix multiplication call.) And there is some internal support for it in the form of methods called Stream::ThenBlasGbmv() in stream_executor/stream.cc. But I don't see how to change strides from python, and ThenBlasGbmv() is never used in 1.x master."
32187,XLA compilation fails in replica context of distribution strategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9.9 (stretch)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-0-g87989f6 1.14.0
- Python version: Python 3.5.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA Version 10.0.130
- GPU model and memory: 2 x Nvidia Tesla K80

**Describe the current behavior**
Invoking `xla.compile` in the function passed to `strategy.experimental_run_v2` as in the code below raises the following exception (full traceback below):
```
ValueError: XLA compiled computations cannot be nested, (operator name: replica_1/input_0)
```
**Describe the expected behavior**
The expectation was to get one compiled XLA cluster for each replica.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tensorflow as tf
from tensorflow.python.compiler.xla import xla

strategy = tf.distribute.experimental.CentralStorageStrategy(
    compute_devices=[""/device:GPU:0"", ""/device:GPU:1""],
    parameter_device=""/device:CPU:0""
)

with strategy.scope():
    def train_step(inputs):
        def step_fn(x):
            w = tf.get_variable(name=""w"", initializer=1.0)
            loss = w * x
            optimizer = tf.train.GradientDescentOptimizer(0.1)
            train_op = optimizer.minimize(loss)
            with tf.control_dependencies([train_op]):
                return tf.identity(loss)
        
        def compiled_step_fn(x):
            [out] = xla.compile(step_fn, inputs=[x])
            return out

        per_replica_losses = strategy.experimental_run_v2(
            compiled_step_fn, args=[inputs])
        sum_loss = strategy.reduce(
            tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)
        return sum_loss
    
    loss = train_step(1.0)
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for _ in range(10):
            print(sess.run(loss))
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
Traceback (most recent call last):
  File ""multi_gpu_strategy_xla.py"", line 29, in <module>
    loss = train_step(1.0)
  File ""multi_gpu_strategy_xla.py"", line 24, in train_step
    compiled_step_fn, args=[inputs])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 511, in experimental_run_v2
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 1555, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/distribute/parameter_server_strategy.py"", line 407, in _call_for_each_replica
    self._container_strategy(), self._device_map, fn, args, kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 195, in _call_for_each_replica
    coord.join(threads)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/lib/python3.5/dist-packages/six.py"", line 693, in reraise
    raise value
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 911, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""multi_gpu_strategy_xla.py"", line 20, in compiled_step_fn
    [out] = xla.compile(step_fn, inputs=[x])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py"", line 110, in compile
    return _compile_internal(computation, inputs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py"", line 338, in _compile_internal
    for i, x in enumerate(flat_inputs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py"", line 338, in <listcomp>
    for i, x in enumerate(flat_inputs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/dispatch.py"", line 180, in wrapper
    return target(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py"", line 86, in identity
    ret = gen_array_ops.identity(input, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 4253, in identity
    ""Identity"", input=input, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2043, in __init__
    self._control_flow_post_processing()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2054, in _control_flow_post_processing
    self._control_flow_context.AddOp(self)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py"", line 254, in AddOp
    self._outer_context.AddInnerOp(op)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py"", line 274, in AddInnerOp
    self.AddOp(op)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/compiler/xla/xla.py"", line 202, in AddOp
    'name: %s)' % op.name)
ValueError: XLA compiled computations cannot be nested, (operator name: replica_1/input_0)
```"
32183,Build tflite-with-select-ops.aar for running on emulator (tf v1.14),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14.0
- Python version: 3.6
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 7.4.0


**Describe the problem**


First of all I tried to build the `tflite-with-select-ops.aar` lib using this command : 
```
bazel build --cxxopt='--std=c++11' \
            -c opt \
            --config=android_arm \
            --config=monolithic \
            --jobs=1 \
            //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops  
```
Build was successful , but then inside the emulator i was encountering this **ERROR**:
`java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)`

I guess that this is due to the not **arm** architecture of emulator (x86, x86_64).
So I tried this builld command : 
```
bazel build --cxxopt='--std=c++11' \
            -c opt \
            --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \
            --config=monolithic \
            --jobs=1 \
            //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops
```
**(also tried without monolithic argument, both cases failed to build)**

This was the error i got (**with and without monolithic arg**) :

```
ERROR: /home/dav/Desktop/tensorflow-1.14.0/tensorflow/core/BUILD:1856:1: C++ compilation of rule '//tensorflow/core:android_tensorflow_lib_lite' failed (Exit 1)
In file included from tensorflow/core/lib/core/threadpool.cc:26:
In file included from ./tensorflow/core/platform/setround.h:19:
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:68:9: error: no member named 'feclearexcept' in the global namespace
using ::feclearexcept;
      ~~^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:69:9: error: no member named 'fegetexceptflag' in the global namespace
using ::fegetexceptflag;
      ~~^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:70:9: error: no member named 'feraiseexcept' in the global namespace
using ::feraiseexcept;
      ~~^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:71:9: error: no member named 'fesetexceptflag' in the global namespace
using ::fesetexceptflag;
      ~~^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:72:9: error: no member named 'fetestexcept' in the global namespace
using ::fetestexcept;
      ~~^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:73:9: error: no member named 'fegetround' in the global namespace
using ::fegetround;
      ~~^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:74:9: error: no member named 'fesetround' in the global namespace
using ::fesetround;
      ~~^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:75:9: error: no member named 'fegetenv' in the global namespace; did you mean 'getenv'?
using ::fegetenv;
      ~~^
external/androidndk/ndk/sysroot/usr/include/stdlib.h:61:7: note: 'getenv' declared here
char* getenv(const char* __name);
      ^
In file included from tensorflow/core/lib/core/threadpool.cc:26:
In file included from ./tensorflow/core/platform/setround.h:19:
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:76:9: error: no member named 'feholdexcept' in the global namespace
using ::feholdexcept;
      ~~^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:77:9: error: no member named 'fesetenv' in the global namespace
using ::fesetenv;
      ~~^
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cfenv:78:9: error: no member named 'feupdateenv' in the global namespace
using ::feupdateenv;
      ~~^
11 errors generated.
```


**Additional Info**

**WORKSPACE**:
```
build --action_env ANDROID_NDK_HOME=""/home/dav/Android/Sdk/ndk/18""
build --action_env ANDROID_NDK_API_LEVEL=""18""
build --action_env ANDROID_BUILD_TOOLS_VERSION=""29.0.2""
build --action_env ANDROID_SDK_API_LEVEL=""23""
build --action_env ANDROID_SDK_HOME=""/home/dav/Android/Sdk""
```

Any help would be really appreciated!! "
32180,Keras to tflite. Multiple outputs all given name Identity,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):2.0-rc0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0/7.6
- GPU model and memory: RTX 2070

**Describe the current behavior**
When I convert a keras model to a tflite model all outputs are named Identity. When you have a bunch of outputs that all have the same output size it becomes impossible to tell which one is which.

**Describe the expected behavior**
I would expect the names to match the layer names that the output comes from. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf

inputs = []
outputs = []

x = tf.keras.layers.Input(shape=(10), batch_size=1, name='input1')
inputs.append(x)
x = tf.keras.layers.Dense(10, name='dense1')(x)
outputs.append(x)
x = tf.keras.layers.Dense(10, name='dense2')(x)
outputs.append(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs)

model.summary()
```
```
Model: ""model_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input1 (InputLayer)          [(1, 10)]                 0         
_________________________________________________________________
dense1 (Dense)               (1, 10)                   110       
_________________________________________________________________
dense2 (Dense)               (1, 10)                   110       
=================================================================
Total params: 220
Trainable params: 220
Non-trainable params: 0
```
```
model.outputs
```
```
[<tf.Tensor 'dense1/Identity:0' shape=(1, 10) dtype=float32>,
 <tf.Tensor 'dense2/Identity:0' shape=(1, 10) dtype=float32>]
```
```
# Convert and save the model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
open(""./save/simple.tflite"", ""wb"").write(tflite_model)

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""./save/simple.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

output_details
```
```
[{'name': 'Identity',
  'index': 0,
  'shape': array([ 1, 10], dtype=int32),
  'dtype': numpy.float32,
  'quantization': (0.0, 0)},
 {'name': 'Identity_1',
  'index': 1,
  'shape': array([ 1, 10], dtype=int32),
  'dtype': numpy.float32,
  'quantization': (0.0, 0)}]
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32176,Gradient Accumulation feature for Distribution Strategy,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Gradient Accumulation(GA) is a workaround to enable big batches on limited memory GPUs which has been supported in Caffe and PyTorch. Instead of back-propagating for every batch feed-forward; gradients across multiple batches are accumulated. After multiple feed forwards, the accumulated gradient is back-propagated through the network layer.  It boosts performance with a couple of percentages on our several workload (e.g., XLNet, Transformer) with Distribution Strategy. Unfortunately, I am not aware of any official documentation to use such feature in Tensorflow. `tf.contrib.opt.AGNOptimizer` has a similar GA implementation but is not common for general distribution job on DistributionStrategy. Besides it will introduces OOM for large embeddings(#31637).
**Will this change the current api? How?**
Yes. Just need to add one new parameter `iter_size` to current strategy API, which  for example:
```
 # For MirroredStrategy, gradient accumulation is supported using iter_size parameter.
 distribution = tf.distribute.MirroredStrategy(num_gpus=2, iter_size=4)

 # For MultiWorkerMirroredStrategy:
 tf.distribute.experimental.MultiWorkerMirroredStrategy(..., iter_size=4)
```

**Who will benefit with this feature?**
Anyone who use distribution strategy and want to enable big batches on limited memory GPUs 
**Any Other info.**
None.

We already have a concise implementation of GA on DistributionStrategy and are willing to contribute. Thanks in advance for any feedback."
32175,(Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.),"2019-09-02 15:57:27.737 14560-14560/org.tensorflow.demo E/tensorflow: CameraActivity: Exception!
    java.lang.RuntimeException: Failed to load model from 'file:///android_asset/ssd.pb'
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:113)
        at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.create(TensorFlowObjectDetectionAPIModel.java:91)
        at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:157)
        at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:120)
        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1209)
        at android.os.Handler.dispatchMessage(Handler.java:106)
        at android.os.Looper.loop(Looper.java:193)
        at android.app.ActivityThread.main(ActivityThread.java:6669)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:858)
     Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef mentions attr 'half_pixel_centers' not in Op<name=ResizeBilinear; signature=images:T, size:int32 -> resized_images:float; attr=T:type,allowed=[DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=align_corners:bool,default=false>; NodeDef: {{node Preprocessor/map/while/ResizeImage/resize/ResizeBilinear}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:561)
        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)
        at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.create(TensorFlowObjectDetectionAPIModel.java:91) 
        at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:157) 
        at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:120) 
        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1209) 
        at android.os.Handler.dispatchMessage(Handler.java:106) 
        at android.os.Looper.loop(Looper.java:193) 
        at android.app.ActivityThread.main(ActivityThread.java:6669) 
        at java.lang.reflect.Method.invoke(Native Method) 
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493) 
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:858) 
"
32173,Mirror Strategy slow down by adding GPUs,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32172,Mirror Strategy slow down by adding GPUs,"I am using the custom estimator with TFrecord. I am training VGG16. By increasing the number of GPUs the training time increase. All GPUs are in a single machine. GPUs utilization is about 100%. So it seems the input function can feed the data to GPUs well. I am using tf.layers.conv2d, tf.contrib.layers.flatten, tf.layers.dense to create the VGG Model. However, I have a large number of groups_Deps nodes in the computation graph. since all GPUs are in a single machine I am wondering why increasing number of GPUs lead to decreasing the global_Steps/Sec and also increasing the training time. 

![image](https://user-images.githubusercontent.com/17527773/64166782-195cf380-ce48-11e9-91d8-15ceade1db54.png)

"
32171,import tensorflow problem ,"
- OS Platform and Distribution (windows 64):
- 
- TensorFlow version (use command below): 1.14.0
- Python version:3.6.8



Hi 
When i import tensorflow , it just quit the code without any error .

appreciate any help in advance   "
32170,request of SparseApplyFtrlOp running on gpu,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source):  1.13.1


Currently, I notice that `SparseApplyFtrlOp` only supports running on cpu platform. Seeing from here:
https://github.com/tensorflow/tensorflow/blob/d5c6687d9919c562bea2a01a6e1be1756bfaab33/tensorflow/core/kernels/training_ops.cc#L2448

However, in some circumstances, there's great advantage of supporting this  `SparseApplyFtrlOp` in time performance. So is there any plan to support this op on gpu, and if not, what's the reason? Thanks"
32167,transformer implementation error,"https://github.com/tensorflow/models/blob/master/official/transformer/model/transformer.py
The transfomer implementation seems to be incorrect. I cannot find the residual component, and the batchnorm is missed after every feed forward layer."
32164,Tensorflow lite build failed : ‘atomic’ in namespace ‘std’ does not name a type,"This issue happened while building tensorflow lite into AWS Lambda docker

**System information**
- OS platform and distribution : [docker image lambci/lambda:build-python3.6](https://hub.docker.com/r/lambci/lambda/tags)
- TensorFlowLite pip package built from source
- Tensorflow version : master (currently on 496acff)
- Python version: 3.6
- Compiler version : 
  * clang version 3.6.2 (tags/RELEASE_362/final)
  * gcc version 4.8.5 20150623 (Red Hat 4.8.5-28) (GCC) 
  * ldd (GNU libc) 2.17

**Describe the problem**

Using this Dockerfile

```
FROM lambci/lambda:build-python3.6

WORKDIR /var/task

RUN yum update -y && yum -y install swig libjpeg-devel zlib-devel wget

RUN git clone https://code.googlesource.com/re2
WORKDIR /var/task/re2
RUN make && make test && make install && make testinstall

WORKDIR /var/task/

RUN wget https://github.com/google/googletest/archive/release-1.8.0.tar.gz &&\
    tar xf release-1.8.0.tar.gz
		
WORKDIR /var/task/googletest-release-1.8.0

RUN cmake -DBUILD_SHARED_LIBS=ON . &&\
    make &&\
		make install
		
WORKDIR /var/task
RUN pip install numpy
RUN git clone https://github.com/tensorflow/tensorflow
WORKDIR /var/task/tensorflow

RUN git checkout 496acff
RUN bash ./tensorflow/lite/tools/pip_package/build_pip_package.sh
```

It fails with : 

```
./tensorflow/lite/kernels/acceleration_test_util_internal.h: In function ‘absl::optional<T> tflite::GetAccelerationTestParam(std::string)’:
./tensorflow/lite/kernels/acceleration_test_util_internal.h:69:10: error: ‘atomic’ in namespace ‘std’ does not name a type
   static std::atomic<std::vector<ConfigurationEntry<T>>*> test_config_ptr;
```
"
32163,[1.14] Cannot use a custom op with `TPUStrategy` that works with `MirroredStrategy`,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: Python 3.6.8
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10
- GPU model and memory: n/a

**Describe the current behavior**

Copied from https://github.com/google/sentencepiece/issues/390 since this might be a `TPUStrategy` issue, since things work with `MirroredStrategy`.

[SentencePiece](https://github.com/google/sentencepiece) is a library that provides extremely fast and efficient utilities for text encoding during training. It accomplishes this by providing custom ops written in C++, instead of userland code. The custom ops are detected and work with regular TensorFlow and in distributed GPU mode with `MirroredStrategy`, but are not getting detected when using `TPUStrategy`. The custom op makes training ~3x faster, compared to using something like [SubwordTextEncoder](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder).

**Describe the expected behavior**

The custom ops themselves should work with TPUs so I can train on TPUs without compromising the performance of my data pipeline.

**Code to reproduce the issue**

https://colab.research.google.com/gist/suyash/286ba54a1cd6d51fba5c2f091b4900ce
"
32162,[lite doc] broken link in`TensorFlow Lite and TensorFlow operator compatibility`,"The link for `tf.transpose` on page https://www.tensorflow.org/lite/guide/ops_compatibility#compatible_operations
is broken."
32161,tfcompile with negative one shape,"I am trying to compile a tensorflow model, using the following [tutorial](https://barkeywolf.consulting/posts/tf-aot-rust/#prepare-the-tensorflow-graph-for-compilation), which is based on [tf documentation](https://www.tensorflow.org/xla/tfcompile).
<br>
I am doing this in the latest `tensorflow/tensorflow` docker.
<br>
All the tutorials I have seen deal with image processing, but my models receive one-dimensional vector. The config file looks like this:

```
feed {
  id {
    node_name: ""dense_1_input""
  }
  shape {
    dim {
      size: -1
    }
    dim {
      size: 5
    }
  }
}
fetch {
  id {
    node_name: ""k2tfout_0""
  }
}
```

If I use it, the compilation fails. If I change `-1` for `1`, the model compiles, but whenever I try to use it, I get a core dump.
<br>
Could this be an issue with this negative one dimension? How do I compile such models?
<br>
I tried invoking my model using Python and flipping dimension numbers. (1,5) – (5,1) – (5,)...
<br>E.g.:

```
import numpy as np
import pandas as pd

libmodel = np.ctypeslib.load_library('libmodel', '/folder/org_tensorflow/')

libmodel.run.argtypes = [np.ctypeslib.ndpointer(np.float32, ndim=2, shape=(1,5), flags=('c', 'a')),
np.ctypeslib.ndpointer(np.float32, ndim=2, shape=(1,5), flags=('c', 'a', 'w')),
np.ctypeslib.ctypes.c_float,
np.ctypeslib.ctypes.c_float]

df = pd.read_csv('/folder/trial.csv', header = 0, index_col = None)
feats = pd.read_table('/folder/feature_list.txt', header=None, index_col = None)
feats = feats.iloc[:,0].tolist()
df = df.reindex(columns=feats, fill_value=0)

x = np.require(df.iloc[0,:], np.float32, ('c', 'a'))
y = np.require(np.zeros((1,1)), np.float32, ('c', 'a', 'w'))

x = x.reshape((1,5))
libmodel.run(x, y, x.size, y.size)
```

Is it possible to compile such models with negative one dimension? If so, it should be in documentation."
32159,TF 2.0 regression: cloudpickle cannot serialize tf.keras.Sequential.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (code included below in the issue)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip 
- TensorFlow version (use command below): v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0
- Python version: Python 3.6.7 :: Anaconda, Inc.
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

Using cloudpickle to serialize a Python function that uses `tf.keras.Sequential` fails with a recursion error.

**Note** that this works with `tensorflow==1.14.0`.

I imagine it also fails with other things, not just `tf.keras.Sequential`.

```python
import cloudpickle  # cloudpickle.__version__ == '1.2.1'
import tensorflow as tf  # tf.__version__ == '2.0.0-rc0'

def f():
    tf.keras.Sequential

cloudpickle.loads(cloudpickle.dumps(f))  # This fails.
```

The last line fails with

```
---------------------------------------------------------------------------
RecursionError                            Traceback (most recent call last)
<ipython-input-23-25cc307e6227> in <module>
----> 1 cloudpickle.loads(cloudpickle.dumps(f))

~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in __getattr__(self, item)
     48 
     49   def __getattr__(self, item):
---> 50     module = self._load()
     51     return getattr(module, item)
     52 

~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in _load(self)
     42   def _load(self):
     43     """"""Import the target module and insert it into the parent's namespace.""""""
---> 44     module = _importlib.import_module(self.__name__)
     45     self._parent_module_globals[self._local_name] = module
     46     self.__dict__.update(module.__dict__)

... last 2 frames repeated, from the frame below ...

~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in __getattr__(self, item)
     48 
     49   def __getattr__(self, item):
---> 50     module = self._load()
     51     return getattr(module, item)
     52 

RecursionError: maximum recursion depth exceeded while calling a Python object
```

See https://stackoverflow.com/questions/57750920/ray-tensorflow-gpu-2-0-recursionerror/57761034#57761034"
32158,tf.gather should support batch_dims == rank(indices),"**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Yes, if it's just overzealous checks.

**Describe the feature and the current behavior/state.**

Currently, `tf.gather` requires that `batch_dim` is less than `rank(indices)`, but it seems like `batch_dim == rank(indices)` should logically be supported as well. This would be equivalent to a scalar lookup in the last axis of `params`.

Supporting this should come pretty natural and I think that, for the CPU code at least, it is already supported; the checks just need to be changed from `<` to `<=`.

**Will this change the current api? How?**

No."
32157,[TF 2.0] non-tensor argument to target of tf.function logging slows down training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Modified the stock DCGAN TensorFlow example**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Google Colab and Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **2.0.0-rc0**
- Python version: **3.7**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **Colab: unknown, Windows 10: CUDA=10.0, cuDNN=7.4.2**
- GPU model and memory: **Colab: unknown, Windows 10: 1080 Ti 11 GB**

**Describe the current behavior**
Logging summary scalars (for review in TensorBoard) leads to significantly increased training times. 

**Code to reproduce the issue**
Using the DCGAN tutorial (https://www.tensorflow.org/beta/tutorials/generative/dcgan), and modifying it to include a summary_writer :

```
# Load the TensorBoard notebook extension
%load_ext tensorboard
logs_base_dir = ""./logs""
os.makedirs(logs_base_dir, exist_ok=True)

summary_writer = tf.summary.create_file_writer(logs_base_dir)

%tensorboard --logdir {logs_base_dir}
```

and adding scalar logging to the training step function:

```
@tf.function
def train_step(images, step):
    noise = tf.random.normal([BATCH_SIZE, noise_dim])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator(noise, training=True)

        real_output = discriminator(images, training=True)
        fake_output = discriminator(generated_images, training=True)

        gen_loss = generator_loss(fake_output)
        disc_real_loss, disc_fake_loss = discriminator_loss(real_output, fake_output)
        disc_loss = disc_real_loss + disc_fake_loss
    
    # new code here only:
    tf.summary.scalar('gen_loss', gen_loss, step=step)
    tf.summary.scalar('disc_loss', disc_loss, step=step) 
    tf.summary.scalar('total_loss', gen_loss+disc_loss, step=step)
    summary_writer.flush()

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))
```

The average time per epoch [running on Google Colab, and on a Windows 10 PC with 1080 Ti] is a lot longer when logging:

Original configuration:  **24.6 secs** (Colab),  **8.5 secs** (Windows)
With logging summary scalars:  **220 secs** (Colab),  **44.7 secs** (Windows)

**Describe the expected behavior**
I would have expected logging to place a minimal overhead cost on training performance. Perhaps related to the `tf.function` compilation?
"
32156,Distribute strategy extended broadcast_to broadcasts different tensors when using tf.random,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):1.14
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
strategy.extended.broadcast_to does not broadcast the same value when using tf.random.uniform.According to the description of the method, it is supposed to mirror a tensor on one device to all worker devices

**Describe the expected behavior**
Should broadcast the same value to all devices?

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32153,How to run the XLA GPU UTs? it seems filtered.,"bazel test --config=opt --config=cuda --test_verbose_timeout_warnings --verbose_failures --color=yes //tensorflow/compiler/xla/tests:multioutput_fusion_test

WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: All specified test targets were excluded by filters
INFO: Analyzed target //tensorflow/compiler/xla/tests:multioutput_fusion_test_gpu (0 packages loaded, 0 targets configured).
INFO: Found 1 target and 0 test targets...
Target //tensorflow/compiler/xla/tests:multioutput_fusion_test_gpu up-to-date:
  bazel-bin/tensorflow/compiler/xla/tests/multioutput_fusion_test_gpu
INFO: Elapsed time: 6.680s, Critical Path: 6.21s
INFO: 2 processes: 2 local.
INFO: Build completed successfully, 7 total actions
INFO: Build completed successfully, 7 total actions"
32152,Failed to build tensorflow pip package (Python 2.7.15+) inside of the docker image tensorflow/tensorflow:devel,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none
- TensorFlow installed from (source or binary): docker image tensorflow/tensorflow:devel
- TensorFlow version: 1.14
- Python version: 2.7.15+
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: none
- GPU model and memory: none



**Describe the problem**

Unable to build tensorflow pip package inside of the provided docker image **tensorflow/tensorflow:devel** for the tensorflow development using Python 2.7+.

The errors are:
```
Traceback (most recent call last):
File ""/usr/lib/python2.7/site.py"", line 554, in <module>
main()
File ""/usr/lib/python2.7/site.py"", line 536, in main
known_paths = addusersitepackages(known_paths)
File ""/usr/lib/python2.7/site.py"", line 272, in addusersitepackages
user_site = getusersitepackages()
File ""/usr/lib/python2.7/site.py"", line 247, in getusersitepackages
user_base = getuserbase() # this will also set USER_BASE
File ""/usr/lib/python2.7/site.py"", line 237, in getuserbase
USER_BASE = get_config_var('userbase')
File ""/usr/lib/python2.7/sysconfig.py"", line 587, in get_config_var
return get_config_vars().get(name)
File ""/usr/lib/python2.7/sysconfig.py"", line 538, in get_config_vars
_CONFIG_VARS['userbase'] = _getuserbase()
File ""/usr/lib/python2.7/sysconfig.py"", line 215, in _getuserbase
return env_base if env_base else joinuser(""~"", "".local"")
File ""/usr/lib/python2.7/sysconfig.py"", line 201, in joinuser
return os.path.expanduser(os.path.join(*args))
File ""/usr/lib/python2.7/posixpath.py"", line 262, in expanduser
userhome = pwd.getpwuid(os.getuid()).pw_dir
KeyError: 'getpwuid(): uid not found: 2308'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
docker run  -u $(id -u):$(id -g) -v $PWD:$PWD -i --env USER=$USER -w $PWD/repo --env HOME=$PWD  tensorflow/tensorflow:devel bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
```

When I run it without   `-u $(id -u):$(id -g)`, it works fine, but this is not the recommended way.

**Any other info / logs**
If I run the image tensor/tensorflow:devel like this:

```
docker run -it -u $(id -u):$(id -g) tensorflow/tensorflow:devel bash
```

and then I call this command inside of the docker image:
```
whoami
```
The result is ""cannot find name for user ID""

The possible solution is put this line inside of the docker file: devel-cpu.Dockerfiles
RUN chmod g+w /etc/passwd /etc/group

Then this file /etc/passwd will be writable inside of the docker and it will be possible to 
- add passwd file entry for $(id -u)
- add passwd file entry for $(id -g)

This is an issue only for some packages of Python 2.7, but not for Python 3.0
"
32147,ModuleNotFoundError: No module named 'tensorflow' ,"After 
conda install tensorflow 
and
conda activate tensorflow_env

while running code
import tensorflow as tf
**Error**
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-7-64156d691fe5> in <module>
----> 1 import tensorflow as tf

ModuleNotFoundError: No module named 'tensorflow'"
32146,vs2015 build tensorflow failed on windows,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win7 64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary):  source
- TensorFlow version: 1.12.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: compile
- Bazel version (if compiling from source): cmake 3.15.0
- GCC/Compiler version (if compiling from source):  vs2015 update3
- CUDA/cuDNN version:  no
- GPU model and memory: no



**Describe the problem**
When using vs 2015 update3 to build tensorflow, got following error(see detail in log). 
operator system：windows7 x64
tensorflow versiono: 1.12.0 - cpu

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. cmake 
2. vs 2015 update3 (build icu and abseil-cpp by myself, and linked to tensorflow project)
3. build

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


85>tf_session_helper.obj : error LNK2019: unresolved external symbol ""public: __cdecl tensorflow::SessionRef::SessionRef(class tensorflow::Session *)"" (??0SessionRef@tensorflow@@QEAA@PEAVSession@1@@Z) referenced in function ""struct TF_Session * __cdecl tensorflow::TF_NewSessionRef(struct TF_Graph *,struct TF_SessionOptions const *,struct TF_Status *)"" (?TF_NewSessionRef@tensorflow@@YAPEAUTF_Session@@PEAUTF_Graph@@PEBUTF_SessionOptions@@PEAUTF_Status@@@Z)
85>c_api.cc.obj : error LNK2019: unresolved external symbol ""void __cdecl tensorflow::NewRemoteDevices(class tensorflow::Env *,class tensorflow::WorkerCacheInterface *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::function<void __cdecl(class tensorflow::Status const &,class std::vector<class tensorflow::Device *,class std::allocator<class tensorflow::Device *> > *)>)"" (?NewRemoteDevices@tensorflow@@YAXPEAVEnv@1@PEAVWorkerCacheInterface@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$function@$$A6AXAEBVStatus@tensorflow@@PEAV?$vector@PEAVDevice@tensorflow@@V?$allocator@PEAVDevice@tensorflow@@@std@@@std@@@Z@5@@Z) referenced in function ""class tensorflow::Status __cdecl `anonymous namespace'::GetAllRemoteDevices(class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > const &,class tensorflow::WorkerCacheInterface *,class std::unique_ptr<class tensorflow::DeviceMgr,struct std::default_delete<class tensorflow::DeviceMgr> > *)"" (?GetAllRemoteDevices@?A0xdd98b8cf@@YA?AVStatus@tensorflow@@AEBV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@PEAVWorkerCacheInterface@3@PEAV?$unique_ptr@VDeviceMgr@tensorflow@@U?$default_delete@VDeviceMgr@tensorflow@@@std@@@5@@Z)
85>c_api.cc.obj : error LNK2019: unresolved external symbol ""public: class tensorflow::Status __cdecl tensorflow::SessionMgr::CreateSession(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class tensorflow::ServerDef const &,bool)"" (?CreateSession@SessionMgr@tensorflow@@QEAA?AVStatus@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVServerDef@2@_N@Z) referenced in function ""class tensorflow::Status __cdecl `anonymous namespace'::UpdateTFE_ContextWithServerDef(int,class tensorflow::ServerDef const &,struct TFE_Context *)"" (?UpdateTFE_ContextWithServerDef@?A0xdd98b8cf@@YA?AVStatus@tensorflow@@HAEBVServerDef@3@PEAUTFE_Context@@@Z)
85>c_api.cc.obj : error LNK2019: unresolved external symbol ""public: class tensorflow::Status __cdecl tensorflow::SessionMgr::WorkerSessionForSession(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::shared_ptr<struct tensorflow::WorkerSession> *)"" (?WorkerSessionForSession@SessionMgr@tensorflow@@QEAA?AVStatus@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$shared_ptr@UWorkerSession@tensorflow@@@5@@Z) referenced in function ""class tensorflow::Status __cdecl `anonymous namespace'::UpdateTFE_ContextWithServerDef(int,class tensorflow::ServerDef const &,struct TFE_Context *)"" (?UpdateTFE_ContextWithServerDef@?A0xdd98b8cf@@YA?AVStatus@tensorflow@@HAEBVServerDef@3@PEAUTFE_Context@@@Z)
85>c_api.cc.obj : error LNK2019: unresolved external symbol ""class tensorflow::eager::EagerClientCache * __cdecl tensorflow::eager::NewGrpcEagerClientCache(class std::shared_ptr<class tensorflow::GrpcChannelCache>)"" (?NewGrpcEagerClientCache@eager@tensorflow@@YAPEAVEagerClientCache@12@V?$shared_ptr@VGrpcChannelCache@tensorflow@@@std@@@Z) referenced in function ""class tensorflow::Status __cdecl `anonymous namespace'::UpdateTFE_ContextWithServerDef(int,class tensorflow::ServerDef const &,struct TFE_Context *)"" (?UpdateTFE_ContextWithServerDef@?A0xdd98b8cf@@YA?AVStatus@tensorflow@@HAEBVServerDef@3@PEAUTFE_Context@@@Z)
"
32145,The function tf.graph_util.remove_training_nodes doesn't work as expected,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: Python 3.6.8
- Bazel version (if compiling from source): NO
- GCC/Compiler version (if compiling from source): NO
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
When I trying to remove training node with the function tf.graph_util.remove_training_nodes, a Identity node named ""FeatureExtractor/MobilenetV2/expanded_conv_13/expansion_output
"" is not removed.

![image](https://user-images.githubusercontent.com/4157827/64088048-358e6100-cd72-11e9-93c3-8dfbdab3fc14.png)

**Describe the expected behavior**
I tried this in Python2 and there is no such issue. Could you look into it?
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
You can get the model from http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz
After untar, you'll see ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb.
```python
import tensorflow as tf
input_graph_def = tf.GraphDef()
with open(""ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb"", ""rb"") as f:
    input_graph_def.ParseFromString(f.read())
    out_graph_def = tf..graph_util.remove_training_nodes(input_graph_def)
    tf.io.write_graph(out_graph_def, ""./"", ""fused.pb"", as_text=False)
```
 Then you can review it in https://lutzroeder.github.io/netron/

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
NA"
32144,TF 2.0: metrics get mixed up when adding metrics in multiple layers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6, Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1 and v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0
- Python version: 3.6.6
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Creating a Layer and Model, calling `add_metric` in both results in the metric values getting mixed up.

**Describe the expected behavior**
Metric values should not get mixed up

**Code to reproduce the issue**
```python
#!/usr/bin/env python

import numpy as np
import tensorflow as tf
from tensorflow.python.keras import Model
from tensorflow.python.keras.layers import Layer


class MyLayer(Layer):
    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)

    def call(self, input, training=None, mask=None):
        self.add_metric(tf.ones([32]) * 2.0, name='two', aggregation='mean')
        return input


class MyModel(Model):
    def __init__(self, **kwargs):
        super(MyModel, self).__init__(**kwargs)
        self._sampler = MyLayer(name='sampler')

    def call(self, input, training=None, mask=None):
        z = self._sampler(input)
        self.add_metric(tf.ones([32]) * 1.0, name='one', aggregation='mean')
        self.add_metric(tf.ones([32]) * 3.0, name='three', aggregation='mean')
        return z


def train(dataset_train, epochs):
    tf.config.experimental_run_functions_eagerly(True)

    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)

    loss = tf.losses.mean_squared_error

    model = MyModel()
    model.compile(optimizer=optimizer, loss=loss, run_eagerly=True)

    print('Training...')
    history = model.fit(dataset_train, epochs=epochs, verbose=1, callbacks=[])


def main():
    print('Preparing data...')
    batch_size = 32
    num_examples = 32
    xdata = np.random.uniform(size=[num_examples, 16]).astype(np.float32)
    dataset_train = tf.data.Dataset.from_tensor_slices((xdata, xdata))
    dataset_train = dataset_train.batch(batch_size, drop_remainder=True)

    train(dataset_train, epochs=3)


if __name__ == '__main__':
    main()
```

Output:
```
1/1 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - two: 1.0000 - one: 3.0000 - three: 2.0000
```
Note how the values got mixed up. 'two' should assume a value of 2.0, 'one' a value of 1.0, 'three' a value of 3.0.

**Other info / logs**
The problem appeared in a more complex example (training on GPU, graph mode), so although the minimal example above is using eager mode, it also happens when running non-eagerly.
"
32142,Custom loss with extra argument in TF 2.0,"The traditional method of creating a custom loss function with an additional input for tf.keras no longer functions in tensorflow 2.0. It seems like the only way to do it now is with a custom training loop, which means you lose a lot of the convenience of keras (callbacks etc).

In the following case, the extra argument is the input data into the model, which is contained in a `Dataset`. In 1.14 case, I'd run `.make_one_shot_iterator().get_next()` on the dataset and then pass the tensor I get into the loss function. This isn't possible with eager execution, so instead I've tried to just pass the dataset.

```python
import numpy as np
import tensorflow as tf


class WeightedSDRLoss(tf.keras.losses.Loss):

    def __init__(self, noisy_signal, reduction=tf.keras.losses.Reduction.AUTO, name='WeightedSDRLoss'):
        super().__init__(reduction=reduction, name=name)
        self.noisy_signal = noisy_signal

    def sdr_loss(self, sig_true, sig_pred):
        return (-tf.reduce_mean(sig_true * sig_pred) /
                (tf.norm(tensor=sig_pred) * tf.norm(tensor=sig_true)))

    def call(self, y_true, y_pred):
        noise_true = self.noisy_signal - y_true
        noise_pred = self.noisy_signal - y_pred
        alpha = (tf.reduce_mean(tf.square(y_true)) /
                 tf.reduce_mean(tf.square(y_true) + tf.square(self.noisy_signal - y_pred)))
        return alpha * self.sdr_loss(y_true, y_pred) + (1 - alpha) * self.sdr_loss(noise_true, noise_pre$

data_x = np.random.rand(5, 4, 1)
data_y = np.random.rand(5, 4, 1)

x = tf.keras.layers.Input(shape=[4, 1])
y = tf.keras.layers.Activation('tanh')(x)
model = tf.keras.models.Model(inputs=x, outputs=y)

train_dataset = tf.data.Dataset.from_tensor_slices((data_x, data_y)).batch(1)

model.compile(loss=WeightedSDRLoss(x))
model.fit(train_dataset)
```

 I get the error:

````python
op_name = '__inference_distributed_function_169', num_outputs = 2
inputs = [<tf.Tensor: id=82, shape=(), dtype=resource, numpy=<unprintable>>, <tf.Tensor: id=83, shape=(), dtype=variant, numpy=<unprintable>>, <tf.Tensor 'input_1:0' shape=(None, 4, 1) dtype=float32>]
attrs = ('executor_type', '', 'config_proto', b'\n\x07\n\x03GPU\x10\x00\n\x07\n\x03CPU\x10\x012\x02J\x008\x01')
ctx = <tensorflow.python.eager.context.Context object at 0x11785f4e0>
name = None

    def quick_execute(op_name, num_outputs, inputs, attrs, ctx, name=None):
      """"""Execute a TensorFlow operation.
    
      Args:
        op_name: Name of the TensorFlow operation (see REGISTER_OP in C++ code) to
          execute.
        num_outputs: The number of outputs of the operation to fetch.
                     (Explicitly provided instead of being inferred for performance
                     reasons).
        inputs: A list of inputs to the operation. Each entry should be a Tensor, or
          a value which can be passed to the Tensor constructor to create one.
        attrs: A tuple with alternating string attr names and attr values for this
          operation.
        ctx: The value of context.context().
        name: Customized name for the operation.
    
      Returns:
        List of output Tensor objects. The list is empty if there are no outputs
    
      Raises:
        An exception on error.
      """"""
      device_name = ctx.device_name
      # pylint: disable=protected-access
      try:
        ctx.ensure_initialized()
        tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,
                                                   op_name, inputs, attrs,
>                                                  num_outputs)
E                                                  TypeError: An op outside of the function building code is being passed
E                                                  a ""Graph"" tensor. It is possible to have Graph tensors
E                                                  leak out of the function building context by including a
E                                                  tf.init_scope in your function building code.
E                                                  For example, the following function will fail:
E                                                    @tf.function
E                                                    def has_init_scope():
E                                                      my_constant = tf.constant(1.)
E                                                      with tf.init_scope():
E                                                        added = my_constant * 2
E                                                  The graph tensor has name: input_1:0

../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py:61: TypeError

During handling of the above exception, another exception occurred:

    def test_loss():
    
        data_x = np.random.rand(5, 4, 1)
        data_y = np.random.rand(5, 4, 1)
    
        x = keras.layers.Input(shape=[4, 1])
        y = keras.layers.Activation('tanh')(x)
        model = keras.models.Model(inputs=x, outputs=y)
    
        train_dataset = tf.data.Dataset.from_tensor_slices((data_x, data_y)).batch(1)
        print(train_dataset)
    
        model.compile(loss=WeightedSDRLoss(x))
>       model.fit(train_dataset)

test_preprocess.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py:734: in fit
    use_multiprocessing=use_multiprocessing)
../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py:324: in fit
    total_epochs=epochs)
../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py:123: in run_one_epoch
    batch_outs = execution_function(iterator)
../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py:86: in execution_function
    distributed_function(input_fn))
../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py:445: in __call__
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py:1141: in _filtered_call
    self.captured_inputs)
../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py:1224: in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py:511: in call
    ctx=ctx)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

op_name = '__inference_distributed_function_169', num_outputs = 2
inputs = [<tf.Tensor: id=82, shape=(), dtype=resource, numpy=<unprintable>>, <tf.Tensor: id=83, shape=(), dtype=variant, numpy=<unprintable>>, <tf.Tensor 'input_1:0' shape=(None, 4, 1) dtype=float32>]
attrs = ('executor_type', '', 'config_proto', b'\n\x07\n\x03GPU\x10\x00\n\x07\n\x03CPU\x10\x012\x02J\x008\x01')
ctx = <tensorflow.python.eager.context.Context object at 0x11785f4e0>
name = None

    def quick_execute(op_name, num_outputs, inputs, attrs, ctx, name=None):
      """"""Execute a TensorFlow operation.
    
      Args:
        op_name: Name of the TensorFlow operation (see REGISTER_OP in C++ code) to
          execute.
        num_outputs: The number of outputs of the operation to fetch.
                     (Explicitly provided instead of being inferred for performance
                     reasons).
        inputs: A list of inputs to the operation. Each entry should be a Tensor, or
          a value which can be passed to the Tensor constructor to create one.
        attrs: A tuple with alternating string attr names and attr values for this
          operation.
        ctx: The value of context.context().
        name: Customized name for the operation.
    
      Returns:
        List of output Tensor objects. The list is empty if there are no outputs
    
      Raises:
        An exception on error.
      """"""
      device_name = ctx.device_name
      # pylint: disable=protected-access
      try:
        ctx.ensure_initialized()
        tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,
                                                   op_name, inputs, attrs,
                                                   num_outputs)
      except core._NotOkStatusException as e:
        if name is not None:
          message = e.message + "" name: "" + name
        else:
          message = e.message
        six.raise_from(core._status_to_exception(e.code, message), None)
      except TypeError as e:
        keras_symbolic_tensors = [
            x for x in inputs if ops._is_keras_symbolic_tensor(x)
        ]
        if keras_symbolic_tensors:
          raise core._SymbolicException(
              ""Inputs to eager execution function cannot be Keras symbolic ""
>             ""tensors, but found {}"".format(keras_symbolic_tensors))
E         tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'input_1:0' shape=(None, 4, 1) dtype=float32>]
````

Basically seems like the previous way of passing extra arguments into a loss function in keras is broken.

I'm running tensorflow 2.0-rc0 and this happens regardless of the platform.
"
32139,"TF 2.0.0-rc0 + TFP 0.7 broken combo: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key","Error occurs:
tf-gpu 2.0.0-rc0 with tfp 0.7

Code to reproduce:

```
import tensorflow_probability as tfp
tfp.distributions.MultivariateNormalDiag([0.], [1.]).sample()
```

Error returned:

> 
> Traceback (most recent call last):
>   File ""/home/pycharm_project/VAE/save_issue_reproduction.py"", line 3, in <module>
>     tfp.distributions.MultivariateNormalDiag([0.], [1.]).sample()
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/distributions/distribution.py"", line 840, in sample
>     return self._call_sample_n(sample_shape, seed, name, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/distributions/transformed_distribution.py"", line 391, in _call_sample_n
>     y = self.bijector.forward(x, **bijector_kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py"", line 933, in forward
>     return self._call_forward(x, name, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py"", line 904, in _call_forward
>     mapping = self._lookup(x=x, kwargs=kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py"", line 1343, in _lookup
>     mapping = self._from_x[x].get(subkey, mapping).merge(x=x)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py"", line 151, in __getitem__
>     return super(WeakKeyDefaultDict, self).__getitem__(weak_key)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py"", line 181, in __hash__
>     return hash(x)
>   File ""/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py"", line 713, in __hash__
>     raise TypeError(""Tensor is unhashable if Tensor equality is enabled. ""
> TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.

"
32138,TF 2.0: tf.function+dataset do not use GPU by default,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow-gpu==2.0.0rc0 
- Python version: 3.7.4
- CUDA/cuDNN version: CUDA 10.0/ cuDNN 7.6
- GPU model and memory: NVIDIA GeForce GT 630, 2048Mb

**Describe the current behavior**
Wrapping iteration through the dataset with tf.function move computations from GPU to CPU.

**Describe the expected behavior**
I read in [tutorial](https://www.tensorflow.org/beta/guide/effective_tf2) that wrapping iteration through the dataset with tf.function should increase performance, but instead of it computations move from GPU to CPU and as result slow down. Is it correct behavior? 

I can fix this behavior by manual device placement, by I think that computations should be done on GPU by default.

**Code to reproduce the issue**
```python
import tensorflow as tf
import time

data = tf.Variable(initial_value=tf.zeros((1, 1000, 1000)))
dataset = tf.data.Dataset.from_tensor_slices(data).repeat(1000)

weights = tf.Variable(initial_value=tf.zeros((1000, 1000)))

result = tf.Variable(initial_value=tf.zeros((1000, 1000)))

def py_process():
    for sample in dataset:
        result.assign_add(tf.matmul(sample, weights))
    return result

@tf.function
def tf_process():
    for sample in dataset:
        result.assign_add(tf.matmul(sample, weights))
    return result

@tf.function
def tf_gpu_process():
    for sample in dataset:
        with tf.device('/device:GPU:0'):
            result.assign_add(tf.matmul(sample, weights))
    return result

t = time.time()
py_process()
print(""py time"", time.time() - t)

t = time.time()
tf_process()
print(""tf time"", time.time() - t)

t = time.time()
tf_gpu_process()
print(""tf gpu time"", time.time() - t)
```

Output:
```
py time 14.078931093215942
tf time 47.290977478027344
tf gpu time 13.702632665634155
```

**Other info / logs**
I am tracking GPU usage with GPU-Z program, here is screenshot:
![gpu-z](https://user-images.githubusercontent.com/39711437/64075084-56947a80-ccbc-11e9-9853-66555d2c5d06.gif)

"
32137,tensorflow import issue: DLL load failed with error code -1073741795,"ImportError: Traceback (most recent call last):
File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""C:\Users\user\Anaconda3\lib\imp.py"", line 242, in load_module
return load_dynamic(name, filename, file)
File ""C:\Users\user\Anaconda3\lib\imp.py"", line 342, in load_dynamic
return _load(spec)
ImportError: ImportError: Traceback (most recent call last):
File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""C:\Users\user\Anaconda3\lib\imp.py"", line 242, in load_module
return load_dynamic(name, filename, file)
File ""C:\Users\user\Anaconda3\lib\imp.py"", line 342, in load_dynamic
return _load(spec)
ImportError: DLL load failed with error code -1073741795

Failed to load the native TensorFlow runtime.

Failed to load the native TensorFlow runtime."
32136,g3doc,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
32135,Bug in tf.math.lbeta?,"Hi there,

I noticed that there's a discrepancy between the implementations of the log-beta function in tensorflow and scipy.

Here's a simple test (pytest) to reproduce this:

```python
import numpy as np
import tensorflow as tf


def test_lbeta():
    import scipy.special

    ab = [10, 20]
    x = scipy.special.betaln(*ab).astype('float32')

    # compare
    ab = tf.constant(ab, dtype='float32')
    with tf.Session() as sess:
        y = sess.run(tf.math.lbeta(ab))

    np.testing.assert_almost_equal(x, y)  # error: -19.115328 != -19.115334

```
The relative error is about 3e-7, which is not negligible. It gets worse with larger `a` and `b`.

This can be problematic in situations that rely on cancellation of large numbers, e.g. in computations involving the log-pdf of a Beta distribution.

Is this expected or is it a bug?
"
32134,Issue in the example on .../keras/basic_classification page,"The code below and explanation  pick up from your page https://www.tensorflow.org/tutorials/keras/basic_classification its no actually working. In your case is working for the image in the position 0, but no for the rest due to always is picking up/checking test_labels[i] where i in this case is 0, so always is comparing with test_labels[0] which is '9' (anckle boot).
If you try with test_image[1] , or test_image[2] , ..... you can check how the chart is no show the correct color (predicted/true label)

```
img = test_images[0]

predictions_single = model.predict(img)

plot_value_array(0, predictions_single, test_labels)
plt.xticks(range(10), class_names, rotation=45)
plt.show()
```

I only register this issue in order you can improve your documentation. 
Thanks for you software.

"
32133,TF 2.0 - Gradient of 'tf.keras.layers.Dense with bias' produces non-deterministic result,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tf-nightly-gpu-2.0-preview==2.0.0.dev20190826
- TensorFlow version (use command below): v1.12.1-9705-g0fbc138 2.0.0-dev20190826
- Python version: 3.6.9
- CUDA/cuDNN version: 10.0.0/7.3.1
- GPU model and memory: Titan Xp 11Gb

**Describe the current behavior**
 (1) The following code produces the same 'numpy_data0.pkl', 'initial_params0.pkl', 'loss0.pkl' all the times (which means same data, same parameter, same loss), but 'grad0.pkl' changes. I checked it with 'diff' command between generated files. 
 (2) It seems only with tensorflow 2.0 GPU version, this happens. I checked the code with tf-nightly-2.0-preview==2.0.0.dev20190830 (CPU version), it was ok. (= shows deterministic result)
 (3) Using custom dense layer + tf.keras.layers.ReLU() was ok also. (= shows deterministic result) Custom dense layer was
```
class MyDenseLayer(tf.keras.layers.Layer):
    def __init__(self, num_outputs):
        super(MyDenseLayer, self).__init__()
        self.num_outputs = num_outputs
    def build(self, input_shape):
        self.kernel = self.add_variable(""kernel"", initializer=tf.keras.initializers.GlorotUniform(),
                                        shape=[int(input_shape[-1]),
                                               self.num_outputs])
        self.bias = self.add_variable(""bias"", initializer=tf.zeros_initializer,
                                        shape=[self.num_outputs])
    def call(self, input):
        return tf.matmul(input, self.kernel) + self.bias
```
And net with
```
net = tf.keras.Sequential()
net.add(MyDenseLayer(100))
net.add(tf.keras.layers.ReLU())
net.add(MyDenseLayer(100))
net.add(tf.keras.layers.ReLU())
net.add(MyDenseLayer(1))
net.build((None, input_dim))
```
 (+) When 'use_bias=False' option applied on hidden layers, is was ok. (= shows deterministic result)


**Describe the expected behavior**
Since CUDNN force to behave determinisically (os.environ['TF_CUDNN_DETERMINISTIC'] = 'true'), and all the data/parameter/loss are the same, grad is expected to be same.


**Code to reproduce the issue**
```
import os
import pickle
import random
import numpy as np
import tensorflow as tf

os.environ['TF_CUDNN_DETERMINISTIC'] = 'true'

seed = 1234
np.random.seed(seed)
tf.random.set_seed(seed)
random.seed(seed)

# NN Model
input_dim = 5
net = tf.keras.Sequential()
net.add(tf.keras.layers.Dense(100, activation=tf.nn.relu, kernel_initializer=None))
net.add(tf.keras.layers.Dense(100, activation=tf.nn.relu, kernel_initializer=None))
net.add(tf.keras.layers.Dense(1, activation=None, kernel_initializer=None))
net.build((None, input_dim))

# Initial v_params
initial_v_params = net.variables

# Update NN Model one-step
x = np.random.normal(loc=0, scale=1., size=[1000, input_dim])
y = np.random.normal(loc=0, scale=1., size=[1000])

with tf.GradientTape() as tape:
    loss = tf.reduce_mean(tf.square(y - net(x)))
grad = tape.gradient(loss, net.trainable_variables)

# Tag for comparing files
tag = 1

with open('./numpy_data{}.pkl'.format(tag), 'wb') as f:
    pickle.dump([x, y], f)

with open('./initial_params{}.pkl'.format(tag), 'wb') as f:
    pickle.dump(initial_v_params, f)

with open('./loss{}.pkl'.format(tag), 'wb') as f:
    pickle.dump(loss, f)

with open('./grad{}.pkl'.format(tag), 'wb') as f:
    pickle.dump(grad, f)
```
"
32131,Tensorflow tutorial code error,"For tensorflow MNIST Fashion tutorial:

`def` plot_image(i, predictions_array, true_label, img):
  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])
  
  plt.imshow(img, cmap=plt.cm.binary)
  
  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'blue'
  else:
    color = 'red'
  
  plt.xlabel(""{} {:2.0f}% ({})"".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)


the above code does not show the predicted labels and true labels for the predicted images.
The issue seems to be in plt.xlabel"
32130,Can not store plugin name  in logs ( which is then used by  tensorboard to read data for relavant plugin),"**System information**
- OS Platform and Distribution -  Linux Ubuntu 18.04:
- TensorFlow version (or github SHA if from source):1.14.0

In my existing model, i am using CNN model with tf.layers 


def mnist_cnn(inputs):

    


`
def mnist_cnn(inputs):

    input_layer = tf.reshape(inputs, [-1, 28, 28, 3])

    # Convolutional Layer #1
    conv1 = tf.layers.conv2d(
          inputs=input_layer,
          filters=32,
          kernel_size=[5, 5],
          padding=""same"",
          activation=tf.nn.relu,
          name=""conv1"",
    )
	  


    # Pooling Layer #1
    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)

    # Convolutional Layer #2 and Pooling Layer #2
    conv2 = tf.layers.conv2d(
          inputs=pool1,
          filters=64,
          kernel_size=[5, 5],
          padding=""same"",
          activation=tf.nn.relu)

    # weights_1_array = np.zeros((786, 28, 28, 32))
    # conv_kernel_1 = tf.nn.conv2d(conv1, weights_1_array, [1, 4, 4, 1], padding='SAME')


    # kernel = tf.get_collection(tf.GraphKeys.VARIABLES, 'conv1/kernel')[0]



    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)

    # Dense Layer
    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])
    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)
    dropout = tf.layers.dropout(inputs=dense, rate=0.4)


    # Logits Layer
    outputs = tf.layers.dense(inputs=dropout, units=10)
`
And i used to write logs using summary 

` tensor_summaries = list(map(lambda graph_node:
                                tf.summary.tensor_summary(graph_node.name,
                                                          graph.get_tensor_by_name(graph_node.name + "":0""),
                                                          summary_metadata=tf.SummaryMetadata(
                                                              display_name=graph_node.name,
                                                              summary_description=description,
                                                              plugin_data=tf.SummaryMetadata.PluginData(
                                                                  plugin_name=PLUGIN_NAME)),
                                                          collections=collections),
                                tensor_ops))`

Within this summary, there was a feature to write plugin name , this plugin name is then used by my custom tensor board to identify data from my plugin.

With tf keras , we have callbacks like save.model to store weight but that don't give an option to write plugin name this makes it impossible for tensorboard to identify  data for my plugin , 

Is there any way to write logs with plugin name using call back ?
"
32129,[TF.2.0 API Docs] tf.ones_like,"# URL(s) with the issue:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/ones_like

# Description of the issue (what needs changing):
change ""Creates a tensor with all elements set to zero."" to ""Creates a tensor with all elements set to one."""
32128,"Sessions that are closed and reset and all inputs and outputs are out of scope, do not release GPU memory.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS 10.13
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source 1.12.0
- TensorFlow version (use command below):
- Python version:NA
- Bazel version (if compiling from source):0.19.2
- GCC/Compiler version (if compiling from source):Apple LLVM version 9.1.0 (clang-902.0.39.2)
- CUDA/cuDNN version:10.0.130
- GPU model and memory:GTX 1060

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Memory from tensorflow sessions is not freed when a session is closed and no variables are in scope, memory is only freed on program exit
**Describe the expected behavior**
Memory should be freed from the GPU on calling close and reset in C++, or some other way to release GPU resources without forking a process.
**Code to reproduce the issue**

```
#include <stdlib.h>     /* getenv */
#include <iostream>		
#include <tensorflow/core/platform/init_main.h>
#include <tensorflow/core/public/session.h>
#include <tensorflow/core/framework/tensor_shape.h>
#include <string>
#include ""cuda_runtime_api.h""
#include <cuda.h>


int freeCudaMem(){
		int gpuCount, i;
		CUresult res;
		CUdevice dev;
		CUcontext ctx;
		int result;
		cuInit(0);
		cuDeviceGetCount(&gpuCount);
		size_t curClockRate = 0;
		size_t curMaxMem = 0;

		for (int i = 0; i < gpuCount; i++) {
			cudaSetDevice(i);
			cudaDeviceProp curDeviceProp;
			cudaError_t err = cudaGetDeviceProperties(&curDeviceProp, i);
			if (err != cudaSuccess) {
				continue;
			}
			size_t free_mem, total_mem;
			cuDeviceGet(&dev, i);
			cuCtxCreate(&ctx, 0, dev);
			res = cuMemGetInfo(&free_mem, &total_mem);
			result= (int)free_mem;
		}
		return result;
}


int main (int argc, char *argv[]) { 
	setenv(""TF_CPP_MIN_VLOG_LEVEL"", ""3"", 1);
	setenv(""TF_CPP_MIN_LOG_LEVEL"", ""3"", 1);

	{
		std::cout << ""Start of program:  "" << freeCudaMem() << std::endl;
		tensorflow::SessionOptions options;
		tensorflow::ConfigProto &config = options.config;
		config.mutable_gpu_options()->set_allow_growth(true);
		auto* device_count = options.config.mutable_device_count();
		device_count->insert({ ""GPU"", 1 });
		device_count->insert({ ""CPU"", 1 });

		std::unique_ptr<tensorflow::Session> session = std::unique_ptr<tensorflow::Session>(tensorflow::NewSession(options));	

		tensorflow::GraphDef graph_def;
		std::string graphFile(""/path/to/large/frozen_model.pb"");
		tensorflow::Status graphLoadedStatus = ReadBinaryProto(tensorflow::Env::Default(),graphFile.c_str(),&graph_def);
		auto inputTensor2 = tensorflow::Tensor(tensorflow::DT_UINT8, { 1, 2048, 2048, 3 });
		std::vector<tensorflow::Tensor> outputs;
		tensorflow::Status session_create_status = session->Create(graph_def);                                                        
		tensorflow::Status run_status = session->Run({ { ""InputTensor"", inputTensor2 } }, { { ""OutputTensor"" } },{},&outputs);
		outputs.clear();	
		int number3;
		std::cout << ""After session:     "" << freeCudaMem() << std::endl;
		tensorflow::Status closeStatus3  = session	->Close();
		std::cout << ""Close Status: "" << closeStatus3 << std::endl;  
		std::cout << ""After close:       "" << freeCudaMem() << std::endl;                                                                                                        
		session.reset();
		std::cout << ""After reset:       "" << freeCudaMem() << std::endl;
	}
	{
		std::cout << ""Start of program2: "" << freeCudaMem() << std::endl;
		tensorflow::SessionOptions options2;
		tensorflow::ConfigProto &config2 = options2.config;
		config2.mutable_gpu_options()->set_allow_growth(true);
		auto* device_count2 = options2.config.mutable_device_count();
		device_count2->insert({ ""GPU"", 1 });
		device_count2->insert({ ""CPU"", 1 });

		std::unique_ptr<tensorflow::Session> session2 = std::unique_ptr<tensorflow::Session>(tensorflow::NewSession(options2));	


		tensorflow::GraphDef graph_def2;
		std::string graphFile2(""/path/to/large/frozen_model.pb"");
		tensorflow::Status graphLoadedStatus2 = ReadBinaryProto(tensorflow::Env::Default(),graphFile2.c_str(),&graph_def2);
		auto inputTensor22 = tensorflow::Tensor(tensorflow::DT_UINT8, { 1, 2048, 2048, 3 });
		std::vector<tensorflow::Tensor> outputs2;
		tensorflow::Status session_create_status2 = session2->Create(graph_def2);                                                        
		tensorflow::Status run_status2 = session2->Run({ { ""InputTensor"", inputTensor22 } }, { { ""OutputTensor"" } },{},&outputs2);
		outputs2.clear();	
	
		int number2;
		std::cout << ""After session2:    "" << freeCudaMem() << std::endl;
		tensorflow::Status closeStatus2  = session2->Close();
		std::cout << ""Close Status2: "" << closeStatus2 << std::endl;  
		std::cout << ""After close2:      "" << freeCudaMem() << std::endl;                                                                                                        
		session2.reset();
		std::cout << ""After reset2:      "" << freeCudaMem() << std::endl;
	}
	{
		std::cout << ""Start of program3: "" << freeCudaMem() << std::endl;
		tensorflow::SessionOptions options3;
		tensorflow::ConfigProto &config3 = options3.config;
		config3.mutable_gpu_options()->set_allow_growth(true);
		auto* device_count3 = options3.config.mutable_device_count();
		device_count3->insert({ ""GPU"", 1 });
		device_count3->insert({ ""CPU"", 1 });
		std::unique_ptr<tensorflow::Session> session3 = std::unique_ptr<tensorflow::Session>(tensorflow::NewSession(options3));	
		tensorflow::GraphDef graph_def3;
		std::string graphFile3(""/path/to/large/frozen_model.pb"");
		tensorflow::Status graphLoadedStatus3 = ReadBinaryProto(tensorflow::Env::Default(),graphFile3.c_str(),&graph_def3);
		auto inputTensor32 = tensorflow::Tensor(tensorflow::DT_UINT8, { 1, 2048, 2048, 3 });
		std::vector<tensorflow::Tensor> outputs3;
		tensorflow::Status session_create_status3 = session3->Create(graph_def3);                                                        
		tensorflow::Status run_status3 = session3->Run({ { ""InputTensor"", inputTensor32 } }, { { ""OutputTensor"" } },{},&outputs3);
		outputs3.clear();	
		int number3;
		std::cout << ""After session3:    "" << freeCudaMem() << std::endl;
		tensorflow::Status closeStatus3  = session3->Close();
		std::cout << ""Close Status3: "" << closeStatus3 << std::endl;  
		std::cout << ""After close3:      "" << freeCudaMem() << std::endl;                                                                                                        
		session3.reset();
		std::cout << ""After reset3:      "" << freeCudaMem() << std::endl;
	}

}
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here is the output from the program above:
```
Start of program:  1031380992
After session:     700043264
Close Status: OK
After close:       635219968
After reset:       570400768
Start of program2: 505581568
After session2:    440766464
Close Status2: OK
After close2:      375943168
After reset2:      311111680
Start of program3: 246292480
After session3:    181473280
Close Status3: OK
After close3:      116654080
After reset3:      1013313536
```

Note the GPU memory available keeps going down even though the session has been closed and reset and all variables are out of scope."
32127,Tensorflow 2.0 : Combining model.add_loss and keras losses function in training doesn't work,"I read about TensorFlow 2.0 [tutorial](https://www.tensorflow.org/beta/guide/keras/custom_layers_and_models) in VAE section. I follow the tutorial but the model doesn't work as expected despite running the notebook directly from given [Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/guide/keras/custom_layers_and_models.ipynb). The result actually is the same as in the tutorial (i.e. loss value is very similar) but if you look at the output you'll see that the model can't reconstruct the input at all (i.e. output the same image for all inputs). This seems to be a mistake from the tutorial itself when combining `model.add_loss()` and `keras.losses`.

![Original code](https://imgur.com/3rTAOrZ.png)

I changed MSE loss to BinaryCrossentropy but the result is still the same.

Later I tried compute the BinaryCrossentropy loss explicitly in my forward pass then use `model.add_loss()` in addition with the KL-divergence loss

![Use only model.add_loss() to calculate the loss](https://imgur.com/VVOROCI.png)

This way the model can actually learn the data and the output seems good enough.

So I have a question about `model.add_loss()` and losses as a function that takes `(y_true, y_pred)` (i.e. `keras.losses`). The updated code works only if it can calculate losses in forward pass (e.g. kl-divergence or reconstruction loss), how can I combine `model.add_loss()` and `keras.losses` correctly in the case where the model need ground truth of the output (e.g. denoise VAE)."
32125,LARSOptimizer - Layer-wise decomposition,"LARSOptimizer provides the learning rate control used in LARS, but we need layer-wise decomposed parameters of a neural network. how do we do the layer-wise decomposition for a pre-defined architecture?

LARS: https://arxiv.org/abs/1708.03888 | [Release 1.14 code](https://github.com/tensorflow/tensorflow/blob/release_1.14.0/tensorflow/contrib/opt/python/training/lars_optimizer.py)
LAMB: https://arxiv.org/abs/1904.00962 | [Unofficial code](https://github.com/ymcui/LAMB_Optimizer_TF)
NovoGrad: https://arxiv.org/abs/1905.11286 | [OpenSeq2Seq code](https://github.com/NVIDIA/OpenSeq2Seq/blob/master/open_seq2seq/optimizers/novograd.py)

e.g. [Caffe's LARC](https://docs.nvidia.com/deeplearning/frameworks/caffe-user-guide/index.html#larc)

A possible TensorFlow implementation for conv2d:
```python
import numpy as np
import tensorflow as tf
from tensorflow.python.framework import ops

def lars_conv2d():
    input = tf.placeholder(tf.float32, shape=[16,224,224,3])
    with tf.name_scope('conv1_1') as scope:
        kernel_size = 7*7*3*32
        bias_size = 32
        total_size = kernel_size + bias_size
        params = tf.Variable(tf.constant(0.0, shape=[total_size], dtype=tf.float32),
                             trainable=True, name='weights_and_biases')
        kernel = tf.reshape(
                    tf.slice(params, [0], [kernel_size], name='weights'),
                    [7, 7, 3, 32])
        conv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.slice(params, [kernel_size], [bias_size], name='biases')
        bias = tf.nn.bias_add(conv, biases)
        conv1 = tf.nn.relu(bias, name=scope)

    init_op = tf.variables_initializer(var_list=[params])
    with tf.Session() as sess:
        sess.run(init_op)
        print(np.sum(sess.run(conv1, {input: np.zeros([16,224,224,3])})))

lars_conv2d()
```"
32122,Cannot seek on write only tf.gfile.GFile,"**System information**

-  Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0
- Python version: 3.6

**Describe the current behavior**
Calling `seek()` on a `tf.gfile.GFile` opened in write only mode raises `tensorflow.python.framework.errors_impl.PermissionDeniedError`.

**Describe the expected behavior**
GFile should support the Python IO semantics that supports seeking on a write only file.

More generally it would be preferable if GFile followed the API of Python's [`io.IOBase`](https://docs.python.org/3/library/io.html#io.IOBase).


**Code to reproduce the issue**

```
import tensorflow as tf

with tf.io.gfile.GFile('test.txt', 'w') as f:
    f.seek(0)
```

**Other info / logs**
```
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/VENV/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/VENV/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 146, in seek
    self._preread_check()
  File ""/VENV/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 82, in _preread_check
    ""File isn't open for reading"")
tensorflow.python.framework.errors_impl.PermissionDeniedError: File isn't open for reading
```"
32119,"Cannot use ""sample_weight"" in tf.data.Dataset along with a tf.keras model with customized loss","
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **unknown 1.14.0**
- Python version: **3.7.3**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
If a tf.keras.models.Model uses customized loss added to the model by model.add_loss(loss), and  a tf.data.Dataset dataset which yields tuples (x, y, sample_weight) is used in model.fit(dataset), a TypeError will be raised.

**Describe the expected behavior**
The model should be trained normally.

**Code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

inputs = Input(shape = [1], dtype = tf.float32)
labels = Input(shape = [1], dtype = tf.float32)
outputs = Dense(1, use_bias = True, activation = None)(inputs)

model = Model([inputs, labels], outputs)
loss = tf.square(labels - outputs)
model.add_loss(loss)

model.compile(Adam(0.1))

rg = tf.data.Dataset.range(128)
rg = rg.map(lambda x: tf.cast(x, tf.float32))

'''
# This snippet works fine
ds2 = rg.map(lambda x: ((x, 2*x+1), 0))
ds2 = ds2.batch(16)
model.fit(ds2, epochs = 50)
print(model.predict(([-32], [0])))
'''

# This snippet causes exception
ds3 = rg.map(lambda x: ((x, 2*x+1), 0, 1))
ds3 = ds3.batch(16)
model.fit(ds3, epochs = 50)
```

**Other info / logs**
I encounter the problem in a real-world model. I omit the original model and give a simple model here. It is awkward; but it reproduces the bug in a few lines.

Here is the traceback:
```
C:\fakepath\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) o
r '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,
)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
C:\fakepath\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) o
r '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,
)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
C:\fakepath\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) o
r '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,
)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
C:\fakepath\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) o
r '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,
)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
C:\fakepath\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) o
r '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,
)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
C:\fakepath\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) o
r '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,
)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
C:\fakepath\Python37\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (typ
e, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,))
 / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
C:\fakepath\Python37\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (typ
e, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,))
 / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
C:\fakepath\Python37\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (typ
e, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,))
 / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
C:\fakepath\Python37\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (typ
e, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,))
 / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
C:\fakepath\Python37\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (typ
e, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,))
 / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
C:\fakepath\Python37\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (typ
e, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,))
 / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
WARNING:tensorflow:From C:\fakepath\Python37\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling Varia
nceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future versi
on.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Output dense missing from loss dictionary. We assume this was done on purpose. The fit and evaluate A
PIs will not be expecting any data to be passed to dense.
WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input
 dataset.
2019-08-30 23:01:30.110552: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: AVX2
Traceback (most recent call last):
  File ""main.py"", line 31, in <module>
    model.fit(ds3, epochs = 50)
  File ""C:\fakepath\Python37\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 780, in fit
    steps_name='steps_per_epoch')
  File ""C:\fakepath\Python37\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 174, in mode
l_iteration
    ins = _prepare_feed_values(model, inputs, targets, sample_weights, mode)
  File ""C:\fakepath\Python37\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py"", line 501, in _pre
pare_feed_values
    extract_tensors_from_dataset=True)
  File ""C:\fakepath\Python37\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 2678, in _standardi
ze_user_data
    sample_weight, feed_output_names)
  File ""C:\fakepath\Python37\lib\site-packages\tensorflow\python\keras\engine\training_utils.py"", line 441, in stand
ardize_sample_weights
    'sample_weight')
  File ""C:\fakepath\Python37\lib\site-packages\tensorflow\python\keras\engine\training_utils.py"", line 431, in stand
ardize_sample_or_class_weights
    str(x_weight))
TypeError: The model has multiple outputs, so `sample_weight` should be either a list or a dict. Provided `sample_weight
` type not understood: Tensor(""IteratorGetNext:3"", shape=(?,), dtype=int32)
```

"
32118,[TF2] Parallel optimizers in eager mode,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.10
- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0beta1
- Python version: 3.7

**Describe the current behavior**

I would like to optimize a large number of subproblems in parallel using TF 2.0. (For example, to compute the bootstrapped s.e. of the MLE.) It seems like this is not currently possible.

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf

class opt:
    def __init__(self):
        self.x = None
    @tf.function
    def __call__(self, data):
        if self.x is None:
            self.x = tf.Variable(0., dtype=tf.float64)
            self.opt = tf.keras.optimizers.SGD(1.)
        x = self.x
        opt = self.opt
        for _ in range(10):
            with tf.GradientTape() as tape:
                obj = tf.reduce_mean((data - x) ** 2)
            g = tape.gradient(obj, x)
            opt.apply_gradients([(g, x)])
        return x

data = np.random.normal(size=10000)
K = 10
replicates = np.random.choice(data, size=(K, 10000), replace=True)

@tf.function
def g():
    return tf.map_fn(lambda r: opt()(r), replicates, dtype=tf.float64)

g()
```

**Other info / logs**
The above code throws the error:
```
tensorflow.python.framework.errors_impl.FailedPreconditionError:  Error while reading resource variable SGD/learning_rate_130 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/SGD/learning_rate_130/N10tensorflow3VarE does not exist.
         [[{{node map/while/body/_1/StatefulPartitionedCall/SGD/SGD/update/Cast/ReadVariableOp}}]] [Op:__inference_g_1191]

Function call stack:
g
```
I believe this is because the optimizer creates variables as part of its initialization. These variables are not preserved between calls to `opt()`. 

If I try to define a new optimizer at each call to `opt()`, then `tf.function` complains that variables are being created on the non-first call.

The code works if I do not decorate `g()` with `@tf.function`.

Thus far the only workaround I have found is to manually implement the gradient update rule, forgoing the use of `tf.keras.optimizers` completely. But I'm hoping there's a better way."
32117,tf.Keras.fit() runs forever with 0 samples,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): `v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0`
- Python version: 3.7.4
- CUDA/cuDNN version: n/a (CPU)

**Describe the current behavior**
Code runs forever although nothing is to be trained (no parameters, no data).
```
2019-08-30 15:50:26.413745: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
Epoch 1/2
WARNING:tensorflow:The list of trainable weights is empty. Make sure that you are not setting model.trainable to False before compiling the model.
```

**Describe the expected behavior**
Code stops pretty soon.

**Code to reproduce the issue**
```
import numpy as np
from tensorflow.keras import layers, models

shape = (10, 10, 1)
layer = layers.Input(shape)
model = models.Model(layer, layer)
model.compile(loss='mse')
data = np.zeros((0, *shape))
model.fit(x=data, y=data, epochs=2, verbose=2)
```

**Other info / logs**
It's somewhat annoying if you set the number of training samples to 0 by mistake before starting to train - you might not notice it for a while."
32115,[TF2.0.0rc0] TPU update,"When will be updated the nightly build for TPUs in TF2.0.0rc0 ?

Thank you"
32114,Why is the inference for the first time longer than later ones?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution : Windows 10 and Android 9.0
- Mobile device : Xiaomi 8
- TensorFlow version : org.tensorflow:tensorflow-lite:0.0.0-nightly (using 1.11.0 has the same problem)
- Model: ResNet152, ResNet18  convert from Pytorch and MobileNet-V1 from [tensorflow-lite demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo)

if you wanna know how I convert  ResNet152 and ResNet18, you can get the information from  [#issue 27807](https://github.com/tensorflow/tensorflow/issues/27807)

**logs from Android 9.0**
Load ResNet-152 (222 MB)
D/TimeCost: Timecost to load model file: **0 ms**
D/TimeCost: Created a Tensorflow Lite Image Classifier.
Load MobileNet-V1 (16.1 MB)
D/TimeCost: Timecost to load model file: **0 ms** 
D/TimeCost: Created a Tensorflow Lite Image Classifier.
Load ResNet-18 (42.8MB)
D/TimeCost: Timecost to load model file: **0 ms** 
D/TimeCost: Created a Tensorflow Lite Image Classifier.

Inference ResNet-152 for the first time 
D/TimeCost: Timecost to put values into ByteBuffer: **29 ms**
D/TimeCost: Timecost to run model inference: **1681 ms** 
Inference ResNet-152 for the second time 
D/TimeCost: Timecost to put values into ByteBuffer: **3 ms** 
D/TimeCost: Timecost to run model inference: **589 ms** 
Inference ResNet-152 for the third time 
D/TimeCost: Timecost to put values into ByteBuffer: **3 ms** 
D/TimeCost: Timecost to run model inference: **591 ms** 

Inference MobileNet-V1 for the first time
D/TimeCost: Timecost to put values into ByteBuffer: **34 ms**
D/TimeCost: Timecost to run model inference: **134 ms** 
Inference MobileNet-V1 for the second time
D/TimeCost: Timecost to put values into ByteBuffer: **3 ms** 
D/TimeCost: Timecost to run model inference: **56 ms** 
Inference MobileNet-V1 for the third time 
D/TimeCost: Timecost to put values into ByteBuffer: **4 ms**
D/TimeCost: Timecost to run model inference: **58 ms** 

Inference ResNet-18 for the first time 
D/TimeCost: Timecost to put values into ByteBuffer: **10 ms** 
D/TimeCost: Timecost to run model inference: **396 ms**
Inference ResNet-18 for the second time
D/TimeCost: Timecost to put values into ByteBuffer: **2 ms**
D/TimeCost: Timecost to run model inference: **108 ms**
Inference ResNet-18 for the third time
D/TimeCost: Timecost to put values into ByteBuffer: **2 ms**
D/TimeCost: Timecost to run model inference: **85 ms**

"
32113,model.summary() isn't works when dropout layer is in the model.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6
- TensorFlow installed from (source or binary): binary (`pip install tensorflow==2.0.0-rc0`)
- TensorFlow version (use command below): 2.0.0rc0
- Python version: 3.7

**Describe the current behavior**

When I call the model.summary() after define and the build model that is defined by `tensorflow.keras.Model`, the error occurs.
After some trial, I found the reason would be the dropout layer.

The output is:
```
2019-08-30 21:50:30.618187: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-30 21:50:30.630137: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8a50c20bc0 executing computations on platform Host. Devices:
2019-08-30 21:50:30.630161: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
Model: ""my_model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense (Dense)                multiple                  44
_________________________________________________________________
dense_1 (Dense)              multiple                  25
_________________________________________________________________
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-c4b356e43733> in <module>
     16 model = MyModel()
     17 model.build((None, 10))
---> 18 model.summary()

~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in summary(self, line_length, positions, print_fn)
   1457                               line_length=line_length,
   1458                               positions=positions,
-> 1459                               print_fn=print_fn)
   1460
   1461   def _validate_graph_inputs_and_outputs(self):

~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/layer_utils.py in print_summary(model, line_length, positions, print_fn)
    224   for i in range(len(layers)):
    225     if sequential_like:
--> 226       print_layer_summary(layers[i])
    227     else:
    228       print_layer_summary_with_connections(layers[i])

~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/layer_utils.py in print_layer_summary(layer)
    182     name = layer.name
    183     cls_name = layer.__class__.__name__
--> 184     fields = [name + ' (' + cls_name + ')', output_shape, layer.count_params()]
    185     print_row(fields, positions)
    186

~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in count_params(self)
   1585                          ', but the layer isn\'t built. '
   1586                          'You can build it manually via: `' + self.name +
-> 1587                          '.build(batch_input_shape)`.')
   1588     return int(sum(np.prod(w.shape.as_list()) for w in self.weights))
   1589

ValueError: You tried to call `count_params` on dropout, but the layer isn't built. You can build it manually via: `dropout.build(batch_input_shape)`.
```

**Describe the expected behavior**

The model information should be shown.

**Code to reproduce the issue**

```
import tensorflow as tf

class MyModel(tf.keras.Model):
    def __init__(self, training=False):
        super(MyModel, self).__init__()
        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
        self.dropout = tf.keras.layers.Dropout(0.5)
        self.training = training
    def call(self, inputs):
        x = self.dense1(inputs)
        if self.training:
            x = self.dropout(x, training=training)
        return self.dense2(x)

model = MyModel()
model.build((None, 10))
model.summary()
```

**Other info / logs**

If I remove the dropout layer, it works.

```
import tensorflow as tf

class MyModel(tf.keras.Model):
    def __init__(self):
        super(MyModel, self).__init__()
        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    def call(self, inputs):
        x = self.dense1(inputs)
        return self.dense2(x)

model = MyModel()
model.build((None, 10))
model.summary()
```
Output:
```
Model: ""my_model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_6 (Dense)              multiple                  44
_________________________________________________________________
dense_7 (Dense)              multiple                  25
=================================================================
Total params: 69
Trainable params: 69
Non-trainable params: 0
_________________________________________________________________
```
"
32112,can't use tf.nn.moments in tflite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): 
- TensorFlow version (or github SHA if from source):Tensorflow nightly

Actually, error doesn't happened when I use the function of tf.nn.moments in code:
`mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)`
but when i use the tflite model, `aborted (core dumped)` happened, then I realize tflite doesn't support it, but it really important for me. So could you help me to add the function into tflite or just recommend a similar function to me? Thanks a lot"
32111,ImportError: cannot import name 'dense_features' from 'tensorflow.python.feature_column',"```
from tensorflow.python.feature_column import dense_features
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name 'dense_features' from 'tensorflow.python.feature_column'
```

tensorflow==1.14.0
Python==3.7.3

What is the issue in this case?"
32110,Unexpected failure when preparing tensor allocations, Caused by: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/conv.cc:235 input->dims->size != 4 (1 != 4)Node number 0 (CONV_2D) failed to prepare.
32109,Failed to import tensorflow ImportError: DLL load failed with error code 3221225501,"Traceback (most recent call last):
  File ""C:\Users\IEMCSE\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\IEMCSE\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\IEMCSE\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\IEMCSE\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\IEMCSE\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code 3221225501

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/IEMCSE/Documents/Life_In_IEM/Projects/Code/Data_Management/Segment_data.py"", line 2, in <module>
    import tensorflow
  File ""C:\Users\IEMCSE\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\IEMCSE\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\IEMCSE\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\IEMCSE\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\IEMCSE\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\IEMCSE\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\IEMCSE\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\IEMCSE\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code 3221225501


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Process finished with exit code 1
"
32108,How to break tf.while_loop inside body function?,"The issues like this, i write py code for segment chinese sentence, it's easy. But when i use tf do this thing, it' s unlucky ! Specific issues are as follows: 

- py code
 
``` python
x = list(""我是算法工程师"")
y = [3, 3, 0, 2, 0, 1, 2]
def func(x, y):
    res = []
    for i in range(len(x)):
        if y[i] == 3:
            res.append(x[i])
        elif y[i] == 0:
            str_ = """"
            while i < len(x):
                str_ += x[i]
                if y[i] == 2:
                    break
                i += 1
            res.append(str_)
    return res
```

##### call func
func(x, y)
['我', '是', '算法', '工程师']

- tf code

``` python 
x = tf.constant(list(""我是算法工程师""))
y = tf.constant([3, 3, 0, 2, 0, 1, 2, 3, 3])

i = tf.constant(0)
label = tf.constant(0)
out = tf.constant([], dtype=tf.string)

def cond(i, label, out):
    return tf.not_equal(i, x.shape[0])

def body(i, label, out):
    
    str_ = tf.constant("""")
    
    def cond(i, label, str_):
        return tf.not_equal(i, x.shape[0])
    
    def body(i, label, str_):
        
        str_ = str_ + x[i]
            
        def _continue(str_):
            def func_1():
                return str_
            return func_1
        
        def _break():
            def func_2():
                return str_
            return func_2
        
        str_ = tf.cond(
            tf.equal(y[i], 2),
            _break(),
            _continue(str_)
        )
        return [tf.add(i, 1), y[i], str_]
    
    _, _, str_ = tf.while_loop(cond, body, [i, label, str_], shape_invariants=[i.get_shape(), label.get_shape(), tf.TensorShape(None)])
        
    return [tf.add(i, 1), y[i], tf.cond(tf.equal(y[i], 3), lambda: tf.concat((out, x[i:i+1]), axis=0), lambda: tf.cond(tf.equal(y[i], 0), lambda: tf.concat((out, [str_]), axis=0), lambda: out))]

i, label, out = tf.while_loop(cond, body, [i, label, out], shape_invariants=[i.get_shape(), label.get_shape(), tf.TensorShape([None])])
```

##### session run
``` python
with tf.Session() as sess:
    o_i, o_label, o_out = sess.run([i, label, out])
```


i want to same result from py to tf,  and i know what 's problem cause this result . But i can't break tf.while_loop inside body function, i have no idea for this. Could you give me some advice？

- My env
  - tensorflow: 1.13.1
  - os:  win10
  - python: 3.5
"
32106,"""Retval[0] does not have value"" error when using conditions and control dependencies","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6.8
- CUDA/cuDNN version: 9.0.252
- GPU model and memory: GeForce GTX 1050 4031MB

**Describe the current behavior**
 Error that gives us no information what is actually wrong:
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value
**Describe the expected behavior**
 no error and the result should be 4

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Note: you might have to run the code multiple times. The error appeared in 3 out of 5 runs.
```python
import tensorflow as tf

x= tf.constant(2.0)
bool_var = tf.get_variable(""bool_var"", initializer=tf.constant(True), trainable=False,
                                             dtype=tf.bool)
x_copy_var = tf.get_variable(""x_copy_var"", x.shape, initializer=tf.zeros_initializer(),trainable=False)

def true_func():
    assign_op = x_copy_var.assign(x)
    set_bool_var_false = bool_var.assign(False)
    with tf.control_dependencies([assign_op,set_bool_var_false]):
        return x*x_copy_var

cond = tf.cond(bool_var,true_func,lambda:x)
cond= tf.Print(cond,[cond])

sess= tf.Session()
sess.run(tf.global_variables_initializer())
sess.run([cond])
```

**Other info / logs**

Full error log:

/***/***/***/py36Env/bin/python3.6 /snap/pycharm-community/147/helpers/pydev/pydevd.py --multiproc --qt-support=auto --client 127.0.0.1 --port 40811 --file /home/***/PycharmProjects/***/test/cond_test.py
pydev debugger: process 7200 is connecting

Connected to pydev debugger (build 192.6262.63)
WARNING:tensorflow:From /home/***/***/***/test/cond_test.py:16: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.
Instructions for updating:
Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:
```python
    sess = tf.Session()
    with sess.as_default():
        tensor = tf.range(10)
        print_op = tf.print(tensor)
        with tf.control_dependencies([print_op]):
          out = tf.add(tensor, tensor)
        sess.run(out)
    ```
Additionally, to use tf.print in python 2.7, users must make sure to import
the following:

  `from __future__ import print_function`

2019-08-30 10:08:02.921500: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-30 10:08:02.928495: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2019-08-30 10:08:02.928527: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: u-172-c053
2019-08-30 10:08:02.928533: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: u-172-c053
2019-08-30 10:08:02.928562: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 390.87.0
2019-08-30 10:08:02.928584: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 390.87.0
2019-08-30 10:08:02.928589: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 390.87.0
Traceback (most recent call last):
  File ""/localhome/***/virtualEnvironments/py36Env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/localhome/***/virtualEnvironments/py36Env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/localhome/***/virtualEnvironments/py36Env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/snap/pycharm-community/147/helpers/pydev/pydevd.py"", line 2060, in <module>
    main()
  File ""/snap/pycharm-community/147/helpers/pydev/pydevd.py"", line 2054, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/snap/pycharm-community/147/helpers/pydev/pydevd.py"", line 1405, in run
    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)
  File ""/snap/pycharm-community/147/helpers/pydev/pydevd.py"", line 1412, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/snap/pycharm-community/147/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/***/***/***/test/cond_test.py"", line 21, in <module>
    sess.run([cond])
  File ""/localhome/***/virtualEnvironments/py36Env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/localhome/***/virtualEnvironments/py36Env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/localhome/***/virtualEnvironments/py36Env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/localhome/***/virtualEnvironments/py36Env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value
"
32105,FATAL error when using TF_CUDNN_USE_AUTOTUNE=0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.1-10080-g6e0893c79c 1.14.0
- Python version: 3.7
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 7.4
- CUDA/cuDNN version: 10.0 / 7.6.2
- GPU model and memory: V100 16G

**Describe the current behavior**
I have a model which runs fine. However, after I use
```
export TF_CUDNN_USE_AUTOTUNE=0
```
in order to save some autotuning time, it starts to crash with this error in the first iteration:
```
019-08-30 00:05:05.705707: F tensorflow/stream_executor/cuda/cuda_dnn.cc:262] Unsupported Cudnn convolution backward algorithm for filter: 6
zsh: abort (core dumped)  python3 xx.py
```

**Code to reproduce the issue**
Can try to provide one but I think the information is probably already enough to pin down the problem.

**Other info / logs**
The algorithm 6, mentioned in the error message, corresponds to `CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING`, which is explictly disabled in tensorflow: 
https://github.com/tensorflow/tensorflow/blob/a4bfabe12869a00138173afe739733065b39f4a0/tensorflow/stream_executor/cuda/cuda_dnn.cc#L258-L265

When I set AUTOTUNE=0, cudnn will choose the best algorithm with its own heuristics, and I suspect that TensorFlow crashes because cudnn chooses this algorithm that TF has disabled.

cc @chsigg "
32104,Training with GPU on TF 2.0 is much slower than on TF 1.14 if set a large number to `input_dim` of `tf.keras.layers.Embedding`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux`-3.10.0-957.21.3.el7.x86_64 `CentOS-7.3.1611`-Core
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): binary, pip install tensorflow-gpu
- TensorFlow version (use command below): `2.0.0-rc0`(v2.0.0-beta1-5101-gc75bb66), `1.14.0`(v1.14.0-rc1-22-gaf24dc91b5)
- Python version: 3.6.8
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: CUDA 10.0.130, cuDNN 7.6.3.30
- GPU model and memory: RTX 2070 Super, 8GB

**Describe the current behavior**  
I converted the `Keras` implementation of [`Neural Matrix Factorization (NeuMF)`](https://github.com/hexiangnan/neural_collaborative_filtering/blob/master/NeuMF.py) to `tf.keras` and it works well on TF 1.14.  
But when I run it on TF 2.0.0-rc0, the training is much slower than on TF 1.14.  
I use the profiling tools to check the time, and I found `ReadVariableOp` takes too much time if I set a large number to the `input_dim` of `tf.keras.layers.Embedding`.  

``` bash
Tensorflow version:  2.0.0-rc0
Epoch 1/3
10000/10000 [==============================] - 5s 532us/sample - loss: 0.6935
Epoch 2/3
10000/10000 [==============================] - 4s 436us/sample - loss: 0.6903
Epoch 3/3
10000/10000 [==============================] - 4s 431us/sample - loss: 0.6851
```

```bash
Tensorflow version:  1.14.0
Epoch 1/3
10000/10000 [==============================] - 2s 212us/sample - loss: 0.7035
Epoch 2/3
10000/10000 [==============================] - 0s 28us/sample - loss: 0.6981
Epoch 3/3
10000/10000 [==============================] - 0s 29us/sample - loss: 0.6909
```

**Describe the expected behavior**  
The speed of training on TF 2.0 with large `input_dim` of `Embedding` should be the same as TF 1.14 or faster.  

**Code to reproduce the issue**  
I have shared the codes on [Colab](https://drive.google.com/open?id=1wu3fhjtEYtFVWsby5OO4IQ7tRHf5xHWt)
or check the codes below.  
``` Python
# -*- coding:utf-8 -*-

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.regularizers import l1, l2
from tensorflow.keras.layers import Embedding, Input, Dense, Lambda, Flatten

def get_model(num_users, num_items, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0, alpha=0.5):
  assert len(layers) == len(reg_layers)
  num_layer = len(layers) #Number of layers in the MLP
  
  # Input variables
  user_input = Input(shape=(1,), dtype='int32', name = 'user_input')
  item_input = Input(shape=(1,), dtype='int32', name = 'item_input')
  
  # Embedding layer
  MF_Embedding_User = Embedding(input_dim = num_users, output_dim = mf_dim, name = 'mf_embedding_user', 
                                embeddings_initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None), embeddings_regularizer = l2(reg_mf), 
                                input_length=1)
  MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = mf_dim, name = 'mf_embedding_item', 
                                embeddings_initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None), embeddings_regularizer = l2(reg_mf), 
                                input_length=1)

  MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = int(layers[0]/2), name = ""mlp_embedding_user"", 
                                  embeddings_initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None), embeddings_regularizer = l2(reg_layers[0]), 
                                  input_length=1)
  MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = int(layers[0]/2), name = 'mlp_embedding_item', 
                                  embeddings_initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None), embeddings_regularizer = l2(reg_layers[0]), 
                                  input_length=1)

  # MF part
  mf_user_latent = Flatten()(MF_Embedding_User(user_input))
  mf_item_latent = Flatten()(MF_Embedding_Item(item_input))
  mf_vector = keras.layers.Multiply()([mf_user_latent, mf_item_latent])

  # MLP part
  mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))
  mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))
  mlp_vector = keras.layers.Concatenate(axis=-1)([mlp_user_latent, mlp_item_latent])

  for idx in range(1, num_layer):
    mlp_vector = Dense(layers[idx], 
                      activation='relu', 
                      kernel_regularizer = l2(reg_layers[idx]), 
                      bias_regularizer = l2(reg_layers[idx]), 
                      name=""layer%d"" %idx)(mlp_vector)

  # Concatenate MF and MLP parts
  mf_vector = Lambda(lambda x: x * alpha)(mf_vector)
  mlp_vector = Lambda(lambda x : x * (1 - alpha))(mlp_vector)
  predict_vector = keras.layers.Concatenate(axis=-1)([mf_vector, mlp_vector])

  # Final prediction layer
  prediction = Dense(1, 
                    activation='sigmoid', 
                    kernel_initializer='lecun_uniform', 
                    bias_initializer ='lecun_uniform', 
                    name = ""prediction"")(predict_vector)

  model = keras.Model(inputs=[user_input, item_input], outputs=[prediction])
  return model

def generate_data(num_user, num_item, count=100):
    user_input = []
    item_input = []
    labels = []
    for _ in range(count):
        user = np.random.randint(0,num_user)
        item = np.random.randint(0,num_item)
        label = np.random.randint(0,2)
        user_input.append(user)
        item_input.append(item)
        labels.append(label)
    return np.asarray(user_input), np.asarray(item_input), np.asarray(labels)

def test_model():
    num_user = 1000000
    num_item = 100000
    count = 10000
    user_input, item_input, labels = generate_data(num_user, num_item, count)

    model = get_model(num_user, num_item)
    model.compile(
        optimizer=tf.keras.optimizers.Adam(),
        loss=tf.keras.losses.BinaryCrossentropy()
    )

    # Callbacks
    callbacks = [ tf.keras.callbacks.TensorBoard(log_dir='tb-logs') ]
    model.fit([user_input, item_input], labels, batch_size=256, epochs=3, callbacks=callbacks)

if __name__ == ""__main__"":
    print(""Tensorflow version: "", tf.__version__)
    test_model()
```

**Other info / logs**  
The attachment '[tb-logs.zip](https://github.com/tensorflow/tensorflow/files/3558500/tb-logs.zip)' is the tensorboard logs.   

The profiling screenshot of the training on TF 2.0.0-rc0.  
![tf2-profile](https://user-images.githubusercontent.com/4043644/63997461-2b315480-cb31-11e9-9983-43baaeb3b002.png)  

The profiling screenshot of the training on TF 1.14.  
![tf114-profile](https://user-images.githubusercontent.com/4043644/63997474-33898f80-cb31-11e9-8f7d-e02cc6496e69.png)
"
32103,TF-TRT slower than optimized saved model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, I have a network that does 2D convultions + batch normalization on an image.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): TF 1.14
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): 7.4
- CUDA/cuDNN version: 10.0
- GPU model and memory: T4, 12GB

**Describe the current behavior**

I'm trying to optimize a custom model comprised of 2D convolutions and batch normalizations done on an image.  The entire network has fixed dimensions.  I'm using the nightly docker TF image to perform TF-TRT.

I've tried to create a TRT model using both of the following functions:

```
def create_trt_saved_model(saved_model_dir, output_saved_model_dir, precision, batch_size=1):
	''' convert saved model to TRT saved model'''

	converter = trt.TrtGraphConverter(
		input_saved_model_dir = str(saved_model_dir),
		max_batch_size = batch_size,
		precision_mode = precision )
	converter.convert()
	converter.save(output_saved_model_dir = str(output_saved_model_dir))
```

and

```
def create_trt_frozen_graph(graph_def, output_nodes, precision, 
	output_graph_path = None, workspace_size=2<<10, batch_size=1):
	''' convert frozen_graph to a TRT frozen graph'''
	
	converter = trt.TrtGraphConverter(
		input_graph_def = graph_def,
		nodes_blacklist = output_nodes,
		max_batch_size = batch_size,
		max_workspace_size_bytes = workspace_size<<20,
		precision_mode = precision)

	trt_graph_def = converter.convert()

	if not (output_graph_path is None):
		write_graph_to_file(trt_graph_def, output_graph_path)

	return trt_graph_def
```

In both cases, the TF-TRT model is about 35X slower (20ms vs 700ms inference).  The results are the same regardless if I use the graph_def from memory, or load the TF-TRT saved model.

Here is the respective TRT output:

```
2019-08-30 04:48:35.865582: I tensorflow/compiler/tf2tensorrt/segment/segment.cc:460] There are 4 ops of 3 different types in the graph that are not converted to TensorRT: Identity, NoOp, Placeholder, (For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops).
2019-08-30 04:48:35.953878: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:633] Number of TensorRT candidate segments: 1
2019-08-30 04:48:35.969432: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.5
2019-08-30 04:48:35.969838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.5
2019-08-30 04:55:04.225714: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:734] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 944 nodes succeeded.
2019-08-30 04:55:04.352053: W tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:183] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
2019-08-30 04:55:04.402986: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: tf_graph
2019-08-30 04:55:04.403043: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 818 nodes (-817), 914 edges (-949), time = 62.326ms.
2019-08-30 04:55:04.403049: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   layout: Graph size after: 950 nodes (132), 1046 edges (132), time = 50.486ms.
2019-08-30 04:55:04.403054: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 946 nodes (-4), 1042 edges (-4), time = 45.175ms.
2019-08-30 04:55:04.403059: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   TensorRTOptimizer: Graph size after: 3 nodes (-943), 2 edges (-1040), time = 388396.844ms.
2019-08-30 04:55:04.403063: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 3 nodes (0), 2 edges (0), time = 2.443ms.
2019-08-30 04:55:04.403067: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: TRTEngineOp_0_native_segment
2019-08-30 04:55:04.403072: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 946 nodes (0), 1042 edges (0), time = 26.303ms.
2019-08-30 04:55:04.403076: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   layout: Graph size after: 946 nodes (0), 1042 edges (0), time = 30.005ms.
2019-08-30 04:55:04.403092: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 946 nodes (0), 1042 edges (0), time = 26.387ms.
2019-08-30 04:55:04.403097: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   TensorRTOptimizer: Graph size after: 946 nodes (0), 1042 edges (0), time = 3.387ms.
2019-08-30 04:55:04.403103: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 946 nodes (0), 1042 edges (0), time = 26.708ms.
2019-08-30 04:55:04.727419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at 
```

Since the TRT model only has 3 nodes, one of which is the TRT Engine node, does it make sense to convert this via UFF?  Would that get me a speed improvement?

Or, is there a bug in the latest version of TF docker?

Thanks!

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32101,Unsupported Operation (MEAN) while trying to apply GpuDelegate to tflite,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: Nop, just some assembled statements
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung S9
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nightly 1.15.0.dev20190812
- Python version: 3.6.5

**Describe the current behavior**
I'm trying to convert the Keras's MobileNet model with float16 precision for Gpu Inference. But when running tasks, I encountered the following Error:


 Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:
    MEAN: Operation is not supported.
    First 88 operations will run on the GPU, and the remaining 5 on the CPU.tensorflow/lite/kernels/conv.cc:259 bias->type != input_type (10 != 1)Node number 90 (CONV_2D) failed to prepare.
    tensorflow/lite/kernels/conv.cc:259 bias->type != input_type (10 != 1)Node number 3 (CONV_2D) failed to prepare.

**Describe the expected behavior**

**Code to reproduce the issue**
the Python script for conversion is here:
```python
import tensorflow as tf
import tensorflow.keras as keras

model = keras.applications.mobilenet.MobileNet(input_shape=None, alpha=1.0, depth_multiplier=1, dropout=1e-3, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000)
converter = tf.lite.TFLiteConverter.from_keras_model_file(""mobilenet.h5"")
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.lite.constants.FLOAT16]
tflite_model = converter.convert()
open(""mobilenet.tflite"", ""wb"").write(tflite_model)
```

and in Android code, I called `tfliteOptions.setAllowFp16PrecisionForFp32(true);`
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32100,TF2.0-rc0 ValueError on tensor comparison with None using ==,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.0-rc0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Previously when running the code from the CVAE tutorial https://www.tensorflow.org/beta/tutorials/generative/cvae, I used in the sample function
`if eps == None:`
Which worked fine, however with rc0 it now throws an error from comparing a tensor to None and only works if the following is used
`if eps is None`

**Describe the expected behavior**
Comparing if something == None and is None should have the same behavior

**Code to reproduce the issue**
See https://www.tensorflow.org/beta/tutorials/generative/cvae and change the sample function as described.
"
32099,tf2.0.0-rc0: tensor shape check failed when using tf.function in tf.distribute.Strategy scope.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7
- TensorFlow installed from (source or binary): binary(docker image: 2.0.0rc0-gpu-py3-jupyter)
- TensorFlow version (use command below): 2.0.0-rc0
- Python version:3.6.8
- CUDA/cuDNN version: V10.0.130
- GPU model and memory: P40

**Describe the current behavior**
I followed the document for writing custom training loop([ref](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/distribute/Strategy#in_short)), and caught an ValueError exception:
> ValueError: Input tensor 'Const_1:0' enters the loop with shape (), but has shape (None, 110, 110, 1) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.

If I removed 'tf.function' decorator, the code worked fine with a warning:
> WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.

**Describe the expected behavior**
It should work, and run faster than eager mode.

**Code to reproduce the issue**
```python
import tensorflow as tf
mirrored_strategy = tf.distribute.MirroredStrategy()
def get_net():
    net = tf.keras.Sequential()
    net.add(tf.keras.layers.Conv2D(filters=10,
                                   kernel_size=(3, 3)))
    net.add(tf.keras.layers.Dense(1))
    return net

data = tf.random.normal(shape=(1280, 112, 112, 3))
label = tf.random.normal(shape=(1280, ))
multi_db = tf.data.Dataset.from_tensor_slices((data, label)).batch(80)
dist_dataset = mirrored_strategy.experimental_distribute_dataset(multi_db)

with mirrored_strategy.scope():
    net = get_net()
    @tf.function
    def replica_fn(input):
        d, l = input
        return net(d)

    @tf.function
    def distribute_train_epoch(dataset):
        total_result = 0
        for x in dataset:
            per_replica_result = mirrored_strategy.experimental_run_v2(replica_fn, args=(x,))
            total_result = mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_result, axis=None)
        return total_result

    for _ in range(100):
        f = distribute_train_epoch(dist_dataset)
```"
32098,Unable to create TF-TRT model using docker image tensorflow/tensorflow:nightly-gpu-py3,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes, my model is custom.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 1.14
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1 Cuda Drivers
- GPU model and memory: T4, 12GB

**Describe the current behavior**

When I run the following code from the latest python3 gpu build, nothing happens.  When I run it from the nightly build, I'm able to get a model with 1 TRT Engine node.

```
	converter = trt.TrtGraphConverter(
		input_saved_model_dir = str(saved_model_dir),
		max_batch_size = batch_size,
		precision_mode = precision )
	converter.convert()
	converter.save(output_saved_model_dir = str(output_saved_model_dir))
```

Is there an issue with the latest gpu build?  

Thanks,

**Describe the expected behavior**

Should be able to create a TRT model using the latest docker build.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32097,model.save in TF2 forces me to use saved_model.save() but then fails,"Calling model.save(""path/model"") from within my training loop which is inside strategy scope. I get this error:

> AssertionError: tf.saved_model.save is not supported inside a traced @tf.function. Move the call to the outer eagerly-executed context.

But I'm in TF2, I'm NOT calling it inside a @tf.function. Even running tf.executing_eagerly() returns True when I stop the execution just before the save() call. 

How do I work around this?
I can't use .h5 saving (it says nested classes not supported)
I also noticed a warning earlier in the script that appeared around the same time as this error, it says:

> AssertionError: Nesting violated for default stack of <class 'tensorflow.python.framework.func_graph.FuncGraph'> objects

probably related to my attempt trying to manually set_input_shape for my model (following the instructions of an error message that said I cannot otherwise save my model).

The code I am using is just the tensorflow 2 tutorial for VAE plus distribute strategy. The model class is about the same used by the tutorial: https://www.tensorflow.org/beta/tutorials/generative/cvae"
32093,Feature request: linspace can accept tensors for start and stop,"**System information**
- TensorFlow version (you are using): 1.14.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

Currently, [`tf.linspace`](https://www.tensorflow.org/api_docs/python/tf/linspace) accepts only single numbers (0-D tensor) for start and stop. Numpy recently added support for ndarrays for the `start` and `stop` parameters (since 1.16, see [np.linspace](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html) ). It would be very useful (especially when writing vectorized implementations) to have the possibility to call this function for multi dimensional tensors with behavior matching the one in Numpy.

**Will this change the current api? How?** 

It will be a backward-compatible change. Only the behaviour of the function will be changed (for certain sets of inputs) and the dimension checking at the start of the function.

**Who will benefit with this feature?**

Mostly when vectorizing different algorithms. One possible application is when doing a body pose estimation postprocessing pairwise connections per limb, we have all possible combinations of points in a multi dimensional tensor and we want to be able to call `tf.linspace` for the Monte Carlo sampling directly on the tensors to avoid slow iterative solutions.

**Any Other info.**
I have already implemented a function locally, that behaves in the same way. I have not checked if there would be problems in integration though."
32091,[lite] how to build example label_image for ios (library+binary)?,"[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/label_image.md
](url)
this project provide an example for ubuntu+macos+android(32+64), only lack ios(library+binary)."
32090,Initial read on nonexistent tf.gfile.GFile in w+ mode crashes,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0
- Python version: 3.7

**Describe the current behavior**
Python raises `tensorflow.python.framework.errors_impl.NotFoundError` when doing a first read (no writes before it) on a nonexistent `tf.gfile.GFile` in `w+` mode.

**Describe the expected behavior**
Read on an empty `w+` file should return an empty string.
One problem with the current behaviour is that numpy.savez() crashes when writing to a GFile.

**Code to reproduce the issue**
```
import tensorflow as tf

with tf.io.gfile.GFile('test.txt', 'w+') as f:
    f.read()
```

**Other info / logs**
```
Traceback (most recent call last):
  File ""test_gfile.py"", line 5, in <module>
    f.read()
  File ""/VENV/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 122, in read
    self._preread_check()
  File ""/VENV/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 84, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512)
tensorflow.python.framework.errors_impl.NotFoundError: test.txt; No such file or directory
```"
32089,TF2.0 RC tf.keras.model.Evaluate has display bug,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No. This is the code from https://www.tensorflow.org/beta/tutorials/quickstart/beginner
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab and Ubuntu 18.04
- TensorFlow version (use command below): pip install tensorflow==2.0.0-rc0
- Python version: 3.6


**Describe the current behavior**

When I run model.evaluate with verbose=True, I get too many ""======="" signs. I noticed this on Ubuntu 18.04 when I upgraded from TF2.0-beta to TF2.0.0-rc0. This is a mild annoyance when I am running models with soft-warp enabled on the command line.

**Describe the expected behavior**

There should only be this many equal signs:
[==============================]
Not
[===========================............................................

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
``` from __future__ import absolute_import, division, print_function, unicode_literals

# Install TensorFlow
try:
  # %tensorflow_version only exists in Colab.
  %tensorflow_version 2.x
except Exception:
  pass

import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(x_train, y_train, epochs=1)

model.evaluate(x_test, y_test)
```

**Other info / logs**
This issue may be due to the denominator of the number of evaluated samples completed. It should be 10000/10000, not 10000/1.
Instead of 10000/1 [====
"
32087,[2.0.0-rc0] Using dataset makes layers.GRU not use cuDNN backend in graph mode,"I'm experiencing some weird behaviour when compiling a graph containing a GRU cuDNN operation and a dataset using `2.0.0-rc0`.

In eager execution, both executing the GRU and executing the GRU in a dataset loop performs similarly, doing 10 iterations in ~300ms.

When compiling to graph, using `tf.function` the time jumps to 3s for repeatedly exeuting the GRU, and to 30s when wrapped in a dataset iterator. Even when not using any data provided by the dataset (see example).

Looking at the trace it seems like the second case (graph + tf.data iterator: 100x slowdown) is due to it no longer using the cuDNN implementation, as the fused_matmul, sigmoid, etc operations are visible in the trace. In the first case (graph, no tf.data iterator: 10x slowdown) I'm not sure what causes the performance regression.

Attached is the profile traces generated by the script below: 

[gru_cudnn_dataset_graph_profiles.zip](https://github.com/tensorflow/tensorflow/files/3556137/gru_cudnn_dataset_graph_profiles.zip)


```
import tensorflow as tf
from tensorflow.python.eager import profiler

print(tf.version.VERSION, tf.version.GIT_VERSION)

ds = tf.data.experimental.Counter()

rnn = tf.keras.layers.GRU(1024)
inp = tf.random.uniform((128,100,512))

@tf.function
def f():
    start = tf.timestamp()
    for i in tf.range(10):
        rnn(inp)
        tf.print(tf.timestamp() - start)
        start = tf.timestamp()

with profiler.Profiler('test/graph'):
    print('graph')
    f()

def f():
    start = tf.timestamp()
    for i in tf.range(10):
        rnn(inp)
        tf.print(tf.timestamp() - start)
        start = tf.timestamp()

with profiler.Profiler('test/eager'):
    print('eager')
    f()

def f():
    start = tf.timestamp()
    for i in ds.take(10):
        rnn(inp)
        tf.print(tf.timestamp() - start)
        start = tf.timestamp()

with profiler.Profiler('test/dataset_eager'):
    print('dataset eager')
    f()

@tf.function
def f():
    start = tf.timestamp()
    for i in ds.take(10):
        rnn(inp)
        tf.print(tf.timestamp() - start)
        start = tf.timestamp()

with profiler.Profiler('test/dataset_graph'):
    print('dataset graph')
    f()
```
```
2.0.0-rc0 v2.0.0-beta1-5101-gc75bb66
graph
0.0048191547393798828
0.35352301597595215
0.36730408668518066
0.27707195281982422
0.36916518211364746
0.370150089263916
0.31262898445129395
0.28979992866516113
0.300137996673584
0.33440613746643066
eager
0.010959863662719727
0.011440038681030273
0.010359048843383789
0.011663913726806641
0.010751008987426758
0.00952005386352539
0.011648893356323242
0.010584115982055664
0.010535001754760742
0.010977983474731445
dataset eager
0.016921043395996094
0.01053309440612793
0.0098860263824462891
0.010875940322875977
0.0098860263824462891
0.0096909999847412109
0.0097639560699462891
0.00999903678894043
0.0095009803771972656
0.0097169876098632812
dataset graph
0.014286041259765625
3.5808620452880859
3.4670181274414062
3.5227558612823486
3.4991629123687744
3.4808690547943115
3.4846518039703369
3.5459158420562744
3.4938459396362305
3.4940009117126465
```"
32086,Ragged tensor with start and end indices,"**System information**
- TensorFlow version (you are using):
nightly

**Describe the feature and the current behavior/state.**
Ragged tensors are currently defined by specifying the boundaries between subtensors in the ragged dimension. If the elements at the end/beginning of consecutive subtensors in the ragged dimension are shared, defining both the start and end of each such subtensor could reduce the total size of the ragged tensor dramatically.

To make the implementation more straightforward, both starts and ends could be required to increase monotonically .

**Will this change the current api? How?**
No. This would only require adding a construction function to the API.

**Who will benefit with this feature?**
This feature could be used to compress overlapping tensors with overlapping sequences by a large margin. This is useful for training data containing words, tokens, transactions etc.
Furthermore, this would allow slicing the underlying data tensor while using gaps, potentially reducing the number of necessary copies."
32085,Documentation for ./configure environment variables,"I'm looking for documentation with all of the `TF_*` environment variables that can be set to make `./configure` unattended. All I can find are various github issues or random articles using them, but is there a definitive official list somewhere?"
32082,"[TF 2.0] tf.assert_equal([], [1.0]) doesn't raise error","
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Darwin Kernel Version 18.6.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
2.0.0-dev20190827
- Python version:
Python 3.6.8 :: Anaconda, Inc.
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
`tf.assert_equal([], [1.0])` doesn't raise any error. 

**Code to reproduce the issue**
```
import tensorflow as tf

tf.assert_equal([], [1.0])
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32081,Distributed training not working properly: Waiting for model to be ready. Ready_for_local_init_op: Variables not initialized: global_step,"Hello,
I am using tfhub elmo embedding for NER model training.
Training works fine for a single machine.

When I am training to do training on GCP ML engine with 1 master node, 2 parameter server, 3 worker node.

Training started on master node,
But on all three worker node, I am getting this logs continuously for 1 hours,
looks like training is not getting started on worker node.

I am using these versions which currently supported by Google cloud ML Engine:
Python: 3.5
Tensorflow: 1.13.1

**ML engine resource usage:**

```
trainingInput:
    scaleTier: CUSTOM
    masterType: large_model
    workerType: standard
    parameterServerType: standard
    workerCount: 3
    parameterServerCount: 2
```

`Waiting for model to be ready. Ready_for_local_init_op: Variables not initialized: global_step, module/bilm/char_embed, module/bilm/CNN/W_cnn_0, module/bilm/CNN/b_cnn_0, module/bilm/CNN/W_cnn_1, module/bilm/CNN/b_cnn_1, module/bilm/CNN/W_cnn_2, module/bilm/CNN/b_cnn_2, module/bilm/CNN/W_cnn_3, module/bilm/CNN/b_cnn_3, module/bilm/CNN/W_cnn_4, module/bilm/CNN/b_cnn_4, module/bilm/CNN/W_cnn_5, module/bilm/CNN/b_cnn_5, module/bilm/CNN/W_cnn_6, module/bilm/CNN/b_cnn_6, module/bilm/CNN_high_0/W_carry, module/bilm/CNN_high_0/b_carry, module/bilm/CNN_high_0/W_transform, module/bilm/CNN_high_0/b_transform, module/bilm/CNN_high_1/W_carry, module/bilm/CNN_high_1/b_carry, module/bilm/CNN_high_1/W_transform, module/bilm/CNN_high_1/b_transform, module/bilm/CNN_proj/W_proj, module/bilm/CNN_proj/b_proj, module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel, module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias, module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel, module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel, module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias, module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel, module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel, module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias, module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel, module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel, module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias, module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel, module/aggregation/weights, module/aggregation/scaling, lstm_fused_cell/kernel, lstm_fused_cell/bias, lstm_fused_cell_1/kernel, lstm_fused_cell_1/bias, dense/kernel, dense/bias, crf, beta1_power, beta2_power, lstm_fused_cell/kernel/Adam, lstm_fused_cell/kernel/Adam_1, lstm_fused_cell/bias/Adam, lstm_fused_cell/bias/Adam_1, lstm_fused_cell_1/kernel/Adam, lstm_fused_cell_1/kernel/Adam_1, lstm_fused_cell_1/bias/Adam, lstm_fused_cell_1/bias/Adam_1, dense/kernel/Adam, dense/kernel/Adam_1, dense/bias/Adam, dense/bias/Adam_1, crf/Adam, crf/Adam_1, ready: None`"
32079,why the strideconv don't have same value with standconv?,"Examples:
conv1 = slim.conv2d(inputs, 16, [5, 5], stride=1, padding='SAME',scope='conv1')
conv2 = slim.conv2d(inputs, 16, [5,5], stride=2,padding='SAME', scope='conv2')

the size of inputs is [1, 320, 320, 8]
the weight and bias of the conv1 and conv2 are the same.
but the outputs of the conv1 and conv2 are every different. 
can anyone give me a explaination?

I think the result may be like:
conv1_out = [1 2 3 4 5 6 7 8 9 10
                      11 12 13 14 15 16 17 18 19 20
                      21 22 23 24 25 26 27 28 29 30
                       31 32 33 34 35 36 37 38 39 40]
the conv2_out should be:
[1 3 5 7 9
21 23 25 27 29]

actually, the result are very different.






"
32078,feature_column_v2,"HI Support Team 

There is a bug. 


File ""/home/anaconda3/envs/zml/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 86, in <module>
    from tensorflow_estimator.python.estimator.tpu import _tpu_estimator_embedding
  File ""/home/anaconda3/envs/zml/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/_tpu_estimator_embedding.py"", line 33, in <module>
    from tensorflow.python.tpu import feature_column_v2 as tpu_fc_v2
ImportError: cannot import name 'feature_column_v2'

while in the folder only feature_column is available. 

The installed version is 1.14 via PIP. 

The file is available in your git, but doesnot gets installed via PIP. 

Please let me know when the error / issue is resolved. 

Greetings. 



<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32077,The batch_size argument must not be specified when using dataset as an input & Batch size: 32 is not divisible by num_workers: 3,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14.0
- Python version :3.7.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: TITAN Xp, 3 x 12G

---

i'm using `MirroredStrategy()` to train on 3 GPUs, and use a tf `Dataset` as the model inputs. when calling `model.fit()`, if i specify `batch_size`, it shows `The batch_size argument must not be specified when using dataset as an input.` and if i don't specify `batch_size`, it shows `Batch size: 32 is not divisible by num_workers: 3 [Op:ExperimentalRebatchDataset]`.

i know that by calling `fit()`, `batch_size` is 32 by default, but when using dataset, i did call `batch()` and i can't change `batch_size` in `fit()`.

i thought i could use 1, 2, 4, etc. GPUs that can devide 32, but why 3 GPUs is not working.

```python
strategy = MirroredStrategy()
    with strategy.scope():
        model = LSTMModel(embs=que_embs, num_ans=len(ans_vocab.word_index) + 1)
        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        model(tf.zeros(shape=(config.batch_size, config.pad_max_len), dtype=tf.int32))
        model.summary()

dataset = Dataset.from_tensor_slices((inputs, labels))
dataset = dataset.shuffle(buffer_size=4).repeat(3).batch(8)
history = model.fit(dataset, epochs=config.num_epochs,
                               # batch_size=config.batch_size,
                               validation_data=(dataset_val['questions'], dataset_val['answers']),
                               callbacks=create_callbacks())
```
"
32076,TF2.0rc0: experimental_distribute_datasets_from_function() does not respect end of iteration,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: v2.0.0-beta1-5101-gc75bb66 2.0.0-rc0
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0 CUDA, 7.6.3.30-1+cuda10.0 cuDNN
- GPU model and memory: 3x TitanX Pascal 12Gb

**Describe the current behavior**
Note: code worked fine with Tensorflow 2.0 Beta1. Changes introduced in RC0 caused this.

I've created a function, that reads large file and return batches of needed size, considering the number of replicas. At the end of the file it return None (end of iteration). This function is then submitted to `tf.data.Dataset.from_generator()`:
```
def dataset_fn(input_context):
  bs = input_context.get_per_replica_batch_size(self._config.train.batch_size)
  ds = tf.data.Dataset.from_generator(lambda: parallel_map_and_batch(data_file, bs))
  ds = ds.shard(input_context.num_input_pipelines, input_context.input_pipeline_id)
  return ds
```

`tf.distribute.MirroredStrategy.experimental_distribute_datasets_from_function(dataset_fn)` is used to get the dataset and create iterator from it.

`tf.distribute.MirroredStrategy.experimental_run_v2(..., next(iterator))` is used to call a model on each replica.

I've printed a batch size for each call of the model and I see something like:
```
Replica0: batch size 128
Replica1: batch size 128
Replica2: batch size 128
...
Replica0: batch size 107  (this was a last batch)
Replica1: batch size 0  (generator returned None)
Replica2: batch size 0
```

Which causes `Check failed: work_element_count > 0 (0 vs. 0)` inside the model call because of the empty batch.

**Describe the expected behavior**

Replicas shouldn't peek empty batches from dried out iterator. Beta1 used to throw an exception in this case.

P.S. Or `experimental_distribute_datasets_from_function` expects generator to always return number of batches dividable by number of replicas?"
32075,[TF2.0rc]TFRecord file size bigger than original csv file consisting text data.,"I am trying transform a csv file consisting two columns: short_description, label. Description consists sentences from length 1 to 4 and label is just an integer number. The csv file has a size of 740 MBs however on converting it to TFRecord the size of the new file increased to 1.2GB, which I think is not correct since TFRecord files are supposed to have lesser size.
For `short_description` (column 1), I am using BytesList and for `label` (coulmn2) I am using Int64List as was shown here in this [tutorial](https://www.tensorflow.org/beta/tutorials/load_data/tf_records). Just to be clear, the data read from the new TFRecord file is fine i.e. no problem. It is just the size of the file. Now I do not understand what the problem is. Please help. Thank you!"
32074,TFLite build for rpi broken,"**System information**
- OS Platform and Distribution (e.g., Linux Debian):
- TensorFlow installed from (source or binary): github
- TensorFlow version: head
- Installed using virtualenv? pip? conda?: docker tensorflow/tensorflow:nightly-devel
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 7.3.0

Followed https://www.tensorflow.org/lite/guide/build_rpi#cross-compile_for_raspberry_pi

```
$ git clone https://github.com/tensorflow/tensorflow
$ cd tensorflow/
$ ./tensorflow/lite/tools/make/download_dependencies.sh
$ ./tensorflow/lite/tools/make/build_rpi_lib.sh

```

Eventually this fails with:

```
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/root/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/roo
t/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/root/tensorflow/tensorflow/lit
e/tools/make/downloads/absl -I/root/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/root/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/root/tensorflow/tensorflow/lite/tools/make/downl
oads/farmhash/src -I/root/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include \                                                                                            
-o /root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/bin/benchmark_model \                                                                                                                                
 /root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/benchmark-lib.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lstdc++ -lpthread -lm -ldl                       
/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/benchmark-lib.a(benchmark_tflite_model.o): In function `tflite::benchmark::BenchmarkTfLiteModel::LogParams()':                                      
benchmark_tflite_model.cc:(.text+0x1164): undefined reference to `tflite::nnapi::GetStringDeviceNamesList[abi:cxx11]()'                                                                                            
collect2: error: ld returned 1 exit status                                                                                                                                                                         
tensorflow/lite/tools/make/Makefile:292: recipe for target '/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/bin/benchmark_model' failed                                                                 
make: *** [/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/bin/benchmark_model] Error 1 
```

"
32073,build error tensorflow lite for ARM64,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04
- TensorFlow version (or github SHA if from source):master
- GCC/Compiler version (if compiling from source): 7.4.0

**problem:**
i want to build the label_image,and i change the makefile,add label_image like minimal:
```
MINIMAL_SRCS := \
	tensorflow/lite/examples/minimal/minimal.cc
LABEL_IMAGE_SRCS := \
	tensorflow/lite/examples/label_image/label_image.cc \
	tensorflow/lite/examples/label_image/bitmap_helpers.cc
```

```
$(MINIMAL_SRCS) \
$(LABEL_IMAGE_SRCS)
```

```
ALL_SRCS := \
	$(MINIMAL_SRCS) \
        $(LABEL_IMAGE_SRCS) \
```

```
MINIMAL_BINARY := $(BINDIR)minimal
LABEL_IMAGE_BINARY := $(BINDIR)label_image
```

```
MINIMAL_OBJS := $(addprefix $(OBJDIR), \
$(patsubst %.cc,%.o,$(patsubst %.c,%.o,$(MINIMAL_SRCS))))
LABEL_IMAGE_OBJS := $(addprefix $(OBJDIR), \
$(patsubst %.cc,%.o,$(patsubst %.c,%.o,$(LABEL_IMAGE_SRCS))))
```

`all: $(LIB_PATH)  $(MINIMAL_BINARY) $(BENCHMARK_BINARY) $(LABEL_IMAGE_BINARY)`

```
$(MINIMAL_BINARY): $(MINIMAL_OBJS) $(LIB_PATH)
	@mkdir -p $(dir $@)
	$(CXX) $(CXXFLAGS) $(INCLUDES) \
	-o $(MINIMAL_BINARY) $(MINIMAL_OBJS) \
	$(LIBFLAGS) $(LIB_PATH) $(LDFLAGS) $(LIBS)

minimal: $(MINIMAL_BINARY)

$(LABEL_IMAGE_BINARY): $(LABEL_IMAGE_OBJS) $(LIB_PATH)
	@mkdir -p $(dir $@)
	$(CXX) $(CXXFLAGS) $(INCLUDES) \
	-o $(LABEL_IMAGE_BINARY) $(LABEL_IMAGE_OBJS) \
	$(LIBFLAGS) $(LIB_PATH) $(LDFLAGS) $(LIBS)

label_image: $(LABEL_IMAGE_BINARY)
```
i got a build error：label_image.cc
undefined reference：""tflite::evaluation::CreateGPUDelegate(tflite::FlateBufferModel)""
undefined reference：""tflite::evaluation::CreateNNAPIDelegate()"""
32072,iOS GestureClassification (Type 'Interpreter' has no member 'Options'),"When I build this project, I got the error
Type 'Interpreter' has no member 'Options'
Maybe changed class name recently, but the change is not complete
The file name is ModelDataHandler.swift and the line is 99

This is error message
![螢幕快照 2019-08-29 下午1 53 17](https://user-images.githubusercontent.com/32124047/63915118-bf36e980-ca67-11e9-8c32-6562128c73f5.png)

And I try this to fix
![螢幕快照 2019-08-29 下午1 57 12](https://user-images.githubusercontent.com/32124047/63915166-dd044e80-ca67-11e9-9381-7360a7685c5a.png)

"
32070,OP_REQUIRES failed at save_restore_v2_ops.cc:109 : Not found: Failed to create a NewWriteableFile,"my os is win10:
when i use tf.estimator.BestExporter, i find a bug.

in saver.py 
_SHARDED_SUFFIX = ""_temp_%s/part"" % uuid.uuid4().hex
path ='aaa\bbb\_temp_xxx/part....' 

i think to change code:
    _SHARDED_SUFFIX_temp = ""_temp_%s"" % uuid.uuid4().hex
    _SHARDED_SUFFIX = os.path.join(_SHARDED_SUFFIX_temp, 'part')


"
32069,Export Inference TensorFlow model Option(DEFAULT/API_MODEL),"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (1.8/2.0):
- Are you willing to contribute it (Yes):



**Describe the feature and the current behavior/state.**

When exporting(freeze graph) tensorflow model(graph) only for inferencing on mobile/Embedded/IoT using [Tflite](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite)/[MNN](https://github.com/alibaba/MNN) etc lite inference framework, it is difficult to convert/implement the freezed graph efficiently sometimes. For example, when using RNN/GRU/LSTM(static or dynamic) or control flow api(operations), tensorflow will unroll the api to lower operations that make converting/implementing/optimizing on other inference framework difficult, and need much efforts for optimizing the unrolled graph.
I wonder whether the following recommendation is acceptable:
> Tensorflow provide two options when exporting(freeze graph), one is the *DEFAULT* which like the original process, the other is *API_MODEL* which do not lower the api, keep the api-operations in the freezed graph(pb). 
 This method like [convert-RNN-tflite](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/convert/rnn.md), but I think the above method is better.

**Will this change the current api? How?**
Yes, add one option(DEFAULT/API_MODEL)
**Who will benefit with this feature?**
Those who using tensorflow framework to train model, then deploying the model on mobile/Embedded/IoT using Tflite/MNN etc lite inference framework.
**Any Other info.**
"
32068,"I have trained a .pb model which is in tf1.2 and I want to use it in the environment of tf2.0, but I fail to use it successfully. Anyone knows how to solve the problem","from __future__ import division
from __future__ import print_function

try:
    import tensorflow.compat.v1 as tf
    tf.disable_v2_behavior()
except (ImportError, AttributeError):
    import tensorflow as tf

import tensorflow as tf
import numpy as np
import os

        with tf.Graph().as_default():

            output_graph_def = tf.GraphDef()

            with open('./myModel.pb', ""rb"") as f:
                output_graph_def.ParseFromString(f.read())
                tf.import_graph_def(output_graph_def, name="""")

            gpu_options = tf.GPUOptions(
                per_process_gpu_memory_fraction=self.gpu_memory_fraction,
                visible_device_list=self.visible_device_list,
                allow_growth=self.allow_growth)
            tfconfig = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=False)
            tfconfig.gpu_options.allow_growth = True

            with tf.Session(config=tfconfig) as sess:
                sess.run(tf.global_variables_initializer())
                input_photo = sess.graph.get_tensor_by_name('placeholder/photo:0')
                output_photo = sess.graph.get_tensor_by_name('decoder/Sigmoid:0')


################################## erros are as follows:
Traceback (most recent call last):
File ""/Users/govan/Project/test.py"", line 31, in __call__
    output_graph_def = tf.GraphDef()
AttributeError: module 'tensorflow' has no attribute 'GraphDef'"
32067,Used tensorflow arithmetic in the C# of visual studio ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: window 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:1.14
- **Python version**:3.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:10.1
- **GPU model and memory**: 
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

How did i use tensorflow arithmetic in the C# of visual studio 2019 when I had finished my tensorflow arithmetic . please tell you first step second step and so on .  please give me a code example. Thank you!


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
32066,Difference in graphdef binary between training and serving for tf.nn.embedding_lookup,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
NAME=""Linux Mint""
VERSION=""18 (Sarah)""
ID=linuxmint
ID_LIKE=ubuntu
PRETTY_NAME=""Linux Mint 18""
VERSION_ID=""18""
HOME_URL=""http://www.linuxmint.com/""
SUPPORT_URL=""http://forums.linuxmint.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/linuxmint/""
UBUNTU_CODENAME=xenial

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): ('v1.14.0-rc1-22-gaf24dc91b5', '1.14.0')
- Python version: 2.7.12
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: V10.0.130/ v7.5.0
- GPU model and memory: GeForce GTX 1060 (x2) each with 6070MiB.

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When trying to use tf.nn.embedding_lookup during a typical training workflow, everything seems to work fine. However when using the same code with the same tensorflow version/ tensorflow api version, etc. while trying to serve the model an error is thrown. The error seems to point to a mismatch in the way embedding_lookup was defined in tensorflow serving. One is expecting a batch_dims argument for gather_v2 while the other believes this is an invalid argument.

**Describe the expected behavior**
It's expected that, assuming the pasted code is correct that the predict() function should result in same output as the train() output. However, running predict() throws the error above. I have tested this on both tensorflow==1.14.0 and tensorflow-gpu==1.14.0 both with tensorflow-serving-api==1.14.0
**Code to reproduce the issue**

Running this works as expected. Prints out the result of sess.run and exports the 'model'
```python
import numpy as np
import tensorflow as tf
import os
import subprocess as sp
import grpc
from tensorflow_serving.apis import prediction_service_pb2_grpc
from tensorflow_serving.apis import predict_pb2

np.random.seed(0)


def model(indices):
  embeddings = tf.constant(np.random.rand(4, 5))
  embedding_vectors = tf.nn.embedding_lookup(
    embeddings,
    indices
  )
  weights = tf.constant(np.random.rand(5, 2))
  preds = tf.nn.softmax(tf.matmul(embedding_vectors, weights))
  return preds


def train():
  graph = tf.Graph()
  with graph.as_default():

    preds = model(tf.constant([0, 2, 1], dtype=tf.int32))

  with tf.Session(graph=graph) as sess:
    print sess.run(preds)


def export():
  graph = tf.Graph()
  with graph.as_default():

    ph = tf.placeholder(shape=[None], dtype=tf.int32)
    preds = model(ph)

  with tf.Session(graph=graph) as sess:
    inputs = {'input': tf.saved_model.utils.build_tensor_info(ph)}
    outputs = {'output': tf.saved_model.utils.build_tensor_info(preds)}
    signature = (
        tf.saved_model.signature_def_utils.build_signature_def(
            inputs=inputs,
            outputs=outputs,
            method_name=""tensorflow/serving/classify""
        )
    )

    export_file_path = os.path.join('temp_export', '1')
    builder = tf.saved_model.builder.SavedModelBuilder(export_file_path)
    builder.add_meta_graph_and_variables(
        sess,
        [tf.saved_model.tag_constants.SERVING],
        signature_def_map={tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature}
    )

    builder.save()

if __name__ == ""__main__"":
  train()
  export()
```

This is the complete output:
```
WARNING:tensorflow:From temp.py:29: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-08-28 21:27:01.534749: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100070000 Hz
2019-08-28 21:27:01.539551: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3d285e0 executing computations on platform Host. Devices:
2019-08-28 21:27:01.539602: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-08-28 21:27:01.550924: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[[0.30251842 0.69748158]
 [0.2578716  0.7421284 ]
 [0.22793843 0.77206157]]
WARNING:tensorflow:From temp.py:37: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From temp.py:41: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.
WARNING:tensorflow:From temp.py:44: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.

WARNING:tensorflow:From temp.py:52: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.

WARNING:tensorflow:From temp.py:55: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.

WARNING:tensorflow:From temp.py:56: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.
```

The model is then served using docker via the command:
```
docker run -p 8505:8500 --mount type=bind,source=$(pwd)/temp_export,target=/models/temp_model -e MODEL_NAME=temp_model -t tensorflow/serving
```
This is the output of running the docker command:
```
2019-08-29 01:27:32.994250: I tensorflow_serving/model_servers/server.cc:82] Building single TensorFlow model file config:  model_name: temp_model model_base_path: /models/temp_model
2019-08-29 01:27:32.994514: I tensorflow_serving/model_servers/server_core.cc:461] Adding/updating models.
2019-08-29 01:27:32.994538: I tensorflow_serving/model_servers/server_core.cc:558]  (Re-)adding model: temp_model
2019-08-29 01:27:33.094892: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: temp_model version: 1}
2019-08-29 01:27:33.094934: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: temp_model version: 1}
2019-08-29 01:27:33.094951: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: temp_model version: 1}
2019-08-29 01:27:33.094976: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /models/temp_model/1
2019-08-29 01:27:33.094991: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /models/temp_model/1
2019-08-29 01:27:33.095225: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
2019-08-29 01:27:33.117391: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:182] Restoring SavedModel bundle.
2019-08-29 01:27:33.117456: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:192] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: /models/temp_model/1/variables/variables.index
2019-08-29 01:27:33.117481: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:285] SavedModel load for tags { serve }; Status: success. Took 22482 microseconds.
2019-08-29 01:27:33.117517: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:101] No warmup data file found at /models/temp_model/1/assets.extra/tf_serving_warmup_requests
2019-08-29 01:27:33.117615: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: temp_model version: 1}
2019-08-29 01:27:33.130660: I tensorflow_serving/model_servers/server.cc:313] Running gRPC ModelServer at 0.0.0.0:8500 ...
[warn] getaddrinfo: address family for nodename not supported
2019-08-29 01:27:33.135024: I tensorflow_serving/model_servers/server.cc:333] Exporting HTTP/REST API at:localhost:8501 ...
[evhttp_server.cc : 237] RAW: Entering the event loop ...
```

However when this is run:
```python
import numpy as np
import tensorflow as tf
import os
import subprocess as sp
import grpc
from tensorflow_serving.apis import prediction_service_pb2_grpc
from tensorflow_serving.apis import predict_pb2

def predict():
  channel = grpc.insecure_channel('localhost:8505')
  stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)

  request = predict_pb2.PredictRequest()
  request.model_spec.name = 'temp_model'
  request.model_spec.signature_name = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY

  request.inputs['input'].CopyFrom(
    tf.make_tensor_proto([0, 2, 1])
  )

  response = stub.Predict(request)

  print response.outputs


if __name__ == ""__main__"":
  predict()
```

This error is thrown:
```
WARNING:tensorflow:From temp.py:68: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.

WARNING:tensorflow:From temp.py:71: The name tf.make_tensor_proto is deprecated. Please use tf.compat.v1.make_tensor_proto instead.

2019-08-29 01:28:35.360196: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:624] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'batch_dims' not in Op<name=GatherV2; signature=params:Tparams, indices:Tindices, axis:Taxis -> output:Tparams; attr=Tparams:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Taxis:type,allowed=[DT_INT32, DT_INT64]>; NodeDef: {{node embedding_lookup}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[{{node embedding_lookup}}]]
Traceback (most recent call last):
  File ""temp.py"", line 82, in <module>
    predict()
  File ""temp.py"", line 74, in predict
    response = stub.Predict(request)
  File ""/home/crsilkworth/temp/venv/local/lib/python2.7/site-packages/grpc/_channel.py"", line 565, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File ""/home/crsilkworth/temp/venv/local/lib/python2.7/site-packages/grpc/_channel.py"", line 467, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with:
	status = StatusCode.INVALID_ARGUMENT
	details = ""NodeDef mentions attr 'batch_dims' not in Op<name=GatherV2; signature=params:Tparams, indices:Tindices, axis:Taxis -> output:Tparams; attr=Tparams:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Taxis:type,allowed=[DT_INT32, DT_INT64]>; NodeDef: {{node embedding_lookup}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
	 [[{{node embedding_lookup}}]]""
	debug_error_string = ""{""created"":""@1567042115.360651073"",""description"":""Error received from peer ipv4:127.0.0.1:8505"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1052,""grpc_message"":""NodeDef mentions attr 'batch_dims' not in Op<name=GatherV2; signature=params:Tparams, indices:Tindices, axis:Taxis -> output:Tparams; attr=Tparams:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Taxis:type,allowed=[DT_INT32, DT_INT64]>; NodeDef: {{node embedding_lookup}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n\t [[{{node embedding_lookup}}]]"",""grpc_status"":3}""
```


**Other info / logs**
result of pip freeze -l:
```
absl-py==0.8.0
astor==0.8.0
backports.weakref==1.0.post1
enum34==1.1.6
funcsigs==1.0.2
futures==3.3.0
gast==0.2.2
google-pasta==0.1.7
grpcio==1.23.0
h5py==2.9.0
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
Markdown==3.1.1
mock==3.0.5
numpy==1.16.5
protobuf==3.9.1
six==1.12.0
tensorboard==1.14.0
tensorflow==1.14.0
tensorflow-estimator==1.14.0
tensorflow-serving-api==1.14.0
termcolor==1.1.0
Werkzeug==0.15.5
wrapt==1.11.2
```"
32064,Installers for C-API do not include copyright/license information for Tensorflow,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows, Linux and OSX

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A

- TensorFlow installed from (source or binary):
Binary, using the instructions from C-API page at:
(https://www.tensorflow.org/install/lang_c)

- TensorFlow version: 
1.14.0

- Python version:
N/A

- Installed using virtualenv? pip? conda?:
Installed by following instructions at C-API page (https://www.tensorflow.org/install/lang_c)
, basically extracting from tarball or zip file.

- Bazel version (if compiling from source):
N/A

- GCC/Compiler version (if compiling from source):
N/A

- CUDA/cuDNN version:
N/A 

- GPU model and memory:
N/A


**Describe the problem**

The problem I see is that there is no COPYRIGHT or LICENSE information for the TENSORFLOW library itself. There is a LICENSE file under \include\tensorflow\c\LICENSE, but that gives all of the LICENSE information for the 3rd party libraries that tensorflow uses, but there is nothing for tensorflow itself.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
Windows:
Unzip the zip file

LINUX/Darwin
Detar the compressed tarballs


**Any other info / logs**
None"
32060,"ValueError: Dimensions must be equal, but are 19 and 39 for 'metrics/accuracy/Equal' (op: 'Equal') with input shapes: [?,19], [?,39].","**System information**
- OS Platform and Distribution : Win10
- TensorFlow installed from (source or binary): pip3 install tensorflow==2.0.0-rc0 
- Python version: 3.7

**Describe the current behavior**
I have try to make a chatbot based on a transformer model.
But when I try to train the model, i have an error :
`ValueError: Dimensions must be equal, but are 19 and 39 for 'metrics/accuracy/Equal' (op: 'Equal') with input shapes: [?,19], [?,39].`

The entire error code: 

> PS C:\Users\Neicureuil\Workspace\Python\ChatBot Prototype> py .\train.py
Epoch 1/10
      1/Unknown - 3s 3s/stepTraceback (most recent call last):
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1610, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 19 and 39 for 'metrics/accuracy/Equal' (op: 'Equal') with input shapes: [?,19], [?,39].
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "".\train.py"", line 38, in <module>
    chatbot_model.fit(dataset_var, epochs=EPOCHS)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 734, in fit
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 324, in fit
    total_epochs=epochs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 86, in execution_function
    distributed_function(input_fn))
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 427, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 370, in _initialize
    *args, **kwds))
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1847, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2147, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2038, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 320, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 73, in distributed_function
    per_replica_function, args=(model, x, y, sample_weights))
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py"", line 760, in experimental_run_v2
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py"", line 1787, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py"", line 2132, in _call_for_each_replica
    return fn(*args, **kwargs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\autograph\impl\api.py"", line 292, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 264, in train_on_batch
    output_loss_metrics=model._output_loss_metrics)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_eager.py"", line 315, in train_on_batch
    model, outs, targets, sample_weights=sample_weights, masks=masks)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_eager.py"", line 74, in _eager_metrics_fn
    skip_target_masks=model._prepare_skip_target_masks())
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2027, in _handle_metrics
    target, output, output_mask))
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 1978, in _handle_per_output_metrics
    metric_fn, y_true, y_pred, weights=weights, mask=mask)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_utils.py"", line 1066, in call_metric_function
    return metric_fn(y_true, y_pred, sample_weight=weights)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\metrics.py"", line 194, in __call__
    replica_local_fn, *args, **kwargs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\distribute\distributed_training_utils.py"", line 1135, in call_replica_local_fn
    return fn(*args, **kwargs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\metrics.py"", line 177, in replica_local_fn
    update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\utils\metrics_utils.py"", line 75, in decorated
    update_op = update_state_fn(*args, **kwargs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\metrics.py"", line 582, in update_state
    matches = self._fn(y_true, y_pred, **self._fn_kwargs)
  File ""C:\Users\Neicureuil\Workspace\Python\ChatBot Prototype\model.py"", line 195, in accuracy
    accuracy_ = tf.metrics.SparseCategoricalAccuracy()(y_true, y_pred)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\metrics.py"", line 194, in __call__
    replica_local_fn, *args, **kwargs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\distribute\distributed_training_utils.py"", line 1135, in call_replica_local_fn
    return fn(*args, **kwargs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\metrics.py"", line 177, in replica_local_fn
    update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\utils\metrics_utils.py"", line 75, in decorated
    update_op = update_state_fn(*args, **kwargs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\metrics.py"", line 582, in update_state
    matches = self._fn(y_true, y_pred, **self._fn_kwargs)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\metrics.py"", line 2787, in sparse_categorical_accuracy
    return math_ops.cast(math_ops.equal(y_true, y_pred), K.floatx())
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\ops\gen_math_ops.py"", line 3628, in equal
    ""Equal"", x=x, y=y, name=name)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\op_def_library.py"", line 793, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\func_graph.py"", line 548, in create_op
    compute_device)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 3429, in _create_op_internal
    op_def=op_def)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1773, in __init__
    control_input_ops)
  File ""C:\Users\Neicureuil\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1613, in _create_c_op
    raise ValueError(str(e))
ValueError: Dimensions must be equal, but are 19 and 39 for 'metrics/accuracy/Equal' (op: 'Equal') with input shapes: [?,19], [?,39].


**Code to reproduce the issue**
dataset.py: https://pastebin.com/pTVqZaiG
model.py: https://pastebin.com/pgnWRhEt
train.py: https://pastebin.com/FwJK9yDb
"
32058,user can't add_loss w/ input dependence in custom layers in eager mode? (or docs unclear),"**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

Thanks for making tensorflow. 

I want to make custom layers which handle their own losses to write less code 
(ex, layerwise reconstruction + kl divergence losses in stacked autoencoders)
The [docs](https://github.com/tensorflow/tensorflow/blob/a2e398f299e62559118f3e59bd8ef11925cdc449/tensorflow/python/keras/engine/base_layer.py#L1083) indicate this isn't possible in eager mode, which sucks because Eager is default in 2.0... 

I like tf.function / graph mode but frankly it's a different dialect of TF which forces users to waste time translating code into ""graph dialect"" ... so i want to use Eager, even if performance is worse, it's better than debugging tf.function. 

without modular custom losses, users must write complicated/annoying training loops to apply correct loss function to correct combination of inputs and outputs from a model (and models return arrays so ordering gets complicated... can we name model outputs?)

would it be possible to permit TF users to define module-specific input-dependent losses for custom layers in Eager mode? 

**Will this change the current api? How?**
users could add custom input-dependent losses to custom layers. 
thus, users can dramatically simplify training loops

**Who will benefit with this feature?**
2.0 keras users with custom layers that use custom losses

**Any Other info.**
I want to make these ""coder"" bricks into Layers which handle reconstruction + KL divergences in a modular way
```
def get_sensor_and_actuator(agent, in_spec):
    if in_spec.rank is 3:
        sensor = use_image_sensor(agent)
        actuator = use_image_actuator(agent)
    elif in_spec.rank is 2 and in_spec.shape[1] is None:
        sensor = use_ragged_sensor(agent, in_spec)
        actuator = use_ragged_actuator(agent, in_spec)
    else:
        sensor = use_resizer(agent.code_spec.shape)
        actuator = use_resizer(in_spec.shape)
    return sensor, actuator


def use_coder(agent, in_spec):
    log('use_coder', in_spec, color=""blue"")
    normalizer = norm = use_norm()

    if in_spec.rank is 3:
        h, w = get_hw(in_spec.shape)
        hw = [h, w]

        def resize_then_norm(x):
            x = tf.image.resize(x, hw)
            return norm(x)
        normalizer = resize_then_norm

    coordinator = L.Lambda(concat_coords)
    sensor, actuator = get_sensor_and_actuator(agent, in_spec)

    def call(x):
        normie = normalizer(x)
        normie_w_coords = coordinator(normie)
        code = sensor(normie_w_coords)
        reconstruction = actuator(code)
        return normie, code, reconstruction
    return call
```
alas, since i can't use input-dependent modular losses, I need to
1. design my model to return a bunch of extra outputs
2. keep track of the order of those outputs. which one is a code? which one is a reconstruction?
3. unpack the outputs of the model to organize codes, reconstructions, normalized inputs, and ""actual"" desired outputs
4. organize pairs of these normalized inputs and reconstructions
5. loop over the pairs and apply a loss function
6. append error terms to a list of losses.

Is there some reason we cannot add a loss function to the layers themselves and avoid all this BS?

this would REALLY simplify modular agents... please advise!"
32056,Trying to install Tensorflow on a locked-down terminal server with no internet access,"We have a terminal server that is locked down and would not have any internet connection.  We have researchers that go to this system and some of them have expressed interest in using TensorFlow when doing their analysis.

I have only seen a few areas where people have tried this.  So I would need to get definitive answeres to the following:
1.  Can this be installed in this fashion?
2.  If so, what is the best way to go about this?
3.  If not, is there just a particular URL/website that the product needs to go to all the time that could be whitelisted withing our system?"
32055,TFLite build for rpi armv6 broken,"Cross compiling as per https://www.tensorflow.org/lite/guide/build_rpi#cross-compile_for_raspberry_pi

**System information**
- OS Platform and Distribution (e.g., Linux Debain):
- TensorFlow installed from (source or binary): github
- TensorFlow version: v1.13.2
- Installed using virtualenv? pip? conda?: docker tensorflow/tensorflow:nightly-devel
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 7.3.0

```
$ git clone https://github.com/tensorflow/tensorflow
$ cd tensorflow/
$ git checkout v1.13.2
$ sed -i 's/armv7l/armv6/g' ./tensorflow/lite/tools/make/build_rpi_lib.sh
$ ./tensorflow/lite/tools/make/download_dependencies.sh
$ ./tensorflow/lite/tools/make/build_rpi_lib.sh
+ set -e                                                                                                                                                                                                          
+++ dirname ./tensorflow/lite/tools/make/build_rpi_lib.sh
++ cd ./tensorflow/lite/tools/make
++ pwd                                                                                                                                                                                                            
+ SCRIPT_DIR=/root/tensorflow/tensorflow/lite/tools/make                                                                                                                                                          
+ cd /root/tensorflow/tensorflow/lite/tools/make/../../../..                                                                                                                                                      
+ CC_PREFIX=arm-linux-gnueabihf-                                                                                                                                                                                  
+ make -j 3 -f tensorflow/lite/tools/make/Makefile TARGET=rpi TARGET_ARCH=armv6
/bin/sh: 1: [[: not found
arm-linux-gnueabihf-g++ -O3 -DNDEBUG  --std=c++11 -march=armv6 -mfpu=vfp -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/root/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/root/tensorflow/te
nsorflow/lite/tools/make/../../../../../../ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/root/tensorflow/tensorflow/lite/tools/make/do
wnloads/absl -I/root/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/root/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/root/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/s
rc -I/root/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/allocation.cc -o /root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow
/lite/allocation.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG  --std=c++11 -march=armv6 -mfpu=vfp -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/root/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/root/tensorflow/te
nsorflow/lite/tools/make/../../../../../../ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/root/tensorflow/tensorflow/lite/tools/make/do
wnloads/absl -I/root/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/root/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/root/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/s
rc -I/root/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/arena_planner.cc -o /root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorf
low/lite/arena_planner.o
arm-linux-gnueabihf-gcc -O3 -DNDEBUG  -march=armv6 -mfpu=vfp -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/root/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/root/tensorflow/tensorflow/lit
e/tools/make/../../../../../../ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/root/tensorflow/tensorflow/lite/tools/make/downloads/absl
 -I/root/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/root/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/root/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/root/t
ensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/c/c_api_internal.c -o /root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/c
/c_api_internal.o
In file included from tensorflow/lite/c/c_api_internal.c:16:0:
./tensorflow/lite/c/c_api_internal.h:60:34: warning: 'struct TfLiteContext' declared inside parameter list will not be visible outside of this definition or declaration
   TfLiteStatus (*Refresh)(struct TfLiteContext* context);
                                  ^~~~~~~~~~~~~
In file included from /usr/arm-linux-gnueabihf/include/stdio.h:859:0,
                 from tensorflow/lite/c/c_api_internal.c:18:
/usr/arm-linux-gnueabihf/include/bits/stdio.h: In function 'getchar':
/usr/arm-linux-gnueabihf/include/bits/stdio.h:45:1: sorry, unimplemented: Thumb-1 hard-float VFP ABI
 {
 ^
tensorflow/lite/tools/make/Makefile:179: recipe for target '/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/c/c_api_internal.o' failed
make: *** [/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/c/c_api_internal.o] Error 1
make: *** Waiting for unfinished jobs....
In file included from /usr/arm-linux-gnueabihf/include/stdio.h:859:0,
                 from /usr/arm-linux-gnueabihf/include/c++/7/cstdio:42,
                 from ./tensorflow/lite/allocation.h:20,
                 from tensorflow/lite/allocation.cc:16:
/usr/arm-linux-gnueabihf/include/bits/stdio.h: In function 'int getchar()':
/usr/arm-linux-gnueabihf/include/bits/stdio.h:44:14: sorry, unimplemented: Thumb-1 hard-float VFP ABI
 getchar (void)
              ^
In file included from /usr/arm-linux-gnueabihf/include/c++/7/bits/stl_algobase.h:62:0,
                 from /usr/arm-linux-gnueabihf/include/c++/7/memory:62,
                 from ./tensorflow/lite/arena_planner.h:18,
                 from tensorflow/lite/arena_planner.cc:15:
/usr/arm-linux-gnueabihf/include/c++/7/ext/type_traits.h: In function 'bool __gnu_cxx::__is_null_pointer(std::nullptr_t)':                                                                                        
/usr/arm-linux-gnueabihf/include/c++/7/ext/type_traits.h:162:35: sorry, unimplemented: Thumb-1 hard-float VFP ABI                                                                                                 
   __is_null_pointer(std::nullptr_t)
                                   ^
tensorflow/lite/tools/make/Makefile:175: recipe for target '/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/allocation.o' failed                                                    
make: *** [/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/allocation.o] Error 1                                                                                                    
tensorflow/lite/tools/make/Makefile:175: recipe for target '/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/arena_planner.o' failed                                                 
make: *** [/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/arena_planner.o] Error 1        
```
"
32054,TFLite v1.14.0 download_dependencies.sh is broken,"- OS Platform and Distribution (e.g., Linux Debian):
- TensorFlow installed from (source or binary): github
- TensorFlow version: v1.14.0
- Installed using virtualenv? pip? conda?: docker tensorflow/tensorflow:nightly-devel

```
$ git clone https://github.com/tensorflow/tensorflow
Cloning into 'tensorflow'...
remote: Enumerating objects: 38, done.
remote: Counting objects: 100% (38/38), done.
remote: Compressing objects: 100% (37/37), done.
remote: Total 668742 (delta 8), reused 29 (delta 1), pack-reused 668704
Receiving objects: 100% (668742/668742), 378.17 MiB | 28.86 MiB/s, done.
Resolving deltas: 100% (542405/542405), done.
Checking out files: 100% (20047/20047), done.
$ cd tensorflow/ 
$ git checkout v1.14.0
Checking out files: 100% (8943/8943), done.
Note: checking out 'v1.14.0'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by performing another checkout.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -b with the checkout command again. Example:

  git checkout -b <new-branch-name>

HEAD is now at 87989f6959 Add Sergii Khomenko to contributor list
$ ./tensorflow/lite/tools/make/download_dependencies.sh
downloading http://mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/a0d250e79c79.tar.gz

gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error is not recoverable: exiting now
```"
32053,"Tensorflow, Windows Server 2019 and WSL","**System information**
- Windows Server 2019 Essentials and Ubuntu 18.04 LTS in WSL
- TensorFlow installed with pip3 install
- TensorFlow version: All versions above 1.6.0 throw an error
- Python version: 3.6.8
- Installed using pip
- Only CPU

I am trying to use this GitHub repository: https://github.com/begeekmyfriend/tacotron. And all versions above 1.6.0 don't work. I get an error: core dumped. If I install older versions I get different errors like this: `AttributeError: module 'tensorflow.python.ops.rnn_cell_impl' has no attribute 'assert_like_rnncell'` -> On version 1.5.0
Google search didn't helped.

I ran this command:  `sudo python3 train.py`

Log after I run command above:
```
user@SERVER:/mnt/d/tts/tacotron-master$ sudo python3 train.py
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
Checkpoint path: ./logs-tacotron/model.ckpt
Loading training data from: ./training/train.txt
Using model: tacotron
Hyperparameters:
  adam_beta1: 0.9
  adam_beta2: 0.999
  attention_depth: 128
  batch_size: 32
  cleaners: english_cleaners
  decay_learning_rate: True
  decoder_depth: 1024
  embed_depth: 512
  encoder_depth: 256
  fmax: 7600
  fmin: 125
  frame_length_ms: 50
  frame_shift_ms: 12.5
  griffin_lim_iters: 60
  initial_learning_rate: 0.001
  max_abs_value: 4
  max_frame_num: 1000
  max_iters: 300
  min_level_db: -100
  num_freq: 1025
  num_mels: 160
  outputs_per_step: 5
  postnet_depth: 512
  power: 1.2
  preemphasis: 0.97
  prenet_depths: [256, 256]
  ref_level_db: 20
  reg_weight: 1e-06
  sample_rate: 24000
  use_cmudict: False
Loaded metadata for 32 examples (0.09 hours)
Traceback (most recent call last):
  File ""train.py"", line 157, in <module>
    main()
  File ""train.py"", line 153, in main
    train(log_dir, args)
  File ""train.py"", line 66, in train
    model.initialize(feeder.inputs, feeder.input_lengths, feeder.mel_targets, feeder.linear_targets, feeder.stop_token_targets, global_step)
  File ""/mnt/d/tts/tacotron-master/models/tacotron.py"", line 77, in initialize
    CustomDecoder(decoder_cell, helper, decoder_init_state),
  File ""/mnt/d/tts/tacotron-master/models/custom_decoder.py"", line 46, in __init__
    rnn_cell_impl.assert_like_rnncell(type(cell), cell)
AttributeError: module 'tensorflow.python.ops.rnn_cell_impl' has no attribute 'assert_like_rnncell'
```
"
32052,Linear RAM memory increase with Dataset and Estimator with epoch loops,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below):  tensorflow-gpu==2.0.0-rc0
- Python version: 3.7.1
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: Cuda 10.1 / cuDNN 7.6
- GPU model and memory: GeForce GTX 1060 Mobile 6GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When using Dataset with Estimator, the memory foot print of RAM keeps raising when estimator's train and evaluate APIs are called in loop.

**Describe the expected behavior**
RAM usage should not increase with epochs.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Please find the source code @ https://gist.github.com/Mageswaran1989/facc3fc2a003807d029a914c721629db

[Update] My latest test case @ https://github.com/dhiraa/tf_issue_32052

StackOverflow Rereference :  https://stackoverflow.com/questions/55211315/memory-leak-with-tf-data

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[Updated the graph]
![tf_memory_test](https://user-images.githubusercontent.com/3304549/63917881-30bb6b80-ca59-11e9-8598-ebdca798dff5.png)
"
32050,RMSprop can‘t convergence.,"When I use a Adam optimizer, it can convergence. But when I use the RMSprop, it can't convergence. Maybe there is a bug in the keras.optimizers.RMSprop.
"
32049,Creating a boolean constant prints a deprecation warning,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: 2.0.0rc0
- Python version: 3.6

**Describe the current behavior**

Creating a boolean constant prints a deprecation warning:

> W0828 15:45:36.142576 139852094695168 deprecation.py:323] From /lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py:253: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.

**Describe the expected behavior**

No deprecation warning.

**Code to reproduce the issue**

```python
import tensorflow as tf
tf.zeros([10], dtype=tf.bool)
```"
32048,[lite] only armv8.so carsh,"[lite] resloved, unity problem"
32045,static int64 GetDirectConvCost - Integer overflow,"/tensorflow/core/kernels/deep_conv2d.cc: ln 74,
static int64 GetDirectConvCost(int filter_rows, int filter_cols, int in_depth,
int out_depth, int out_rows, int out_cols) {
return filter_rows * filter_cols * in_depth * out_depth * out_rows * out_cols;
}

Can lead to integer overflow and weird results
I think, it should be smth like that
return (int64)filter_rows * (int64)filter_cols * (int64)in_depth * (int64)out_depth * (int64)out_rows * (int64)out_cols;"
32043,[TF 2.0.0rc0] Cannot connect to TPU device,"Created VM and v3-8 TPU with ctpu up command and updated TF version to TF2.0.0rc0 via pip3. 
When i try to connect to tpu device returns error:
```
InvalidArgumentError: Unable to find a context_id matching the specified one (5613663074031560004). Perhaps the worker was restarted, or the context was GC'd?
Additional GRPC error information:
{""created"":""@1566994715.938381293"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Unable to find a context_id matching the specified one (5613663074031560004). Perhaps the worker was restarted, or the context was GC'd?"",""grpc_status"":3}
2019-08-28 12:18:36.196440: E tensorflow/core/distributed_runtime/rpc/eager/grpc_eager_client.cc:72] Remote EagerContext with id 5613663074031560004 does not seem to exist.
```
I also tried the same in Colab, with rc0 version and i have the same error. The code i used is the one given in documentation:

```
tpu='test'
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu)
tf.config.experimental_connect_to_host(resolver.master())
tf.tpu.experimental.initialize_tpu_system(resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)
```
"
32042,Distributed Tensorflow eval values are coming as 0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.1
- Python version: 3.7
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I'm running asynchronous distributed training in Tensorflow using Parameter server strategy. Multi worker on multiple CPUs with evaluator as a separate node.

**Describe the expected behavior**
Training and evaluation should happen fine.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Sample tf_config of Parameter server: Index and type varies for chief, worker and evaluator on other TF_CONFIGs.
TF_CONFIG={
""task"": {
    ""type"": ""ps"",
    ""index"": 0
},
""cluster"": {
    ""chief"": [""machine2:2222""],
    ""worker"": [""machine3:2223"",""machine4:2224""],
    ""evaluator"": [""machine5:2225""],
    ""ps"": [""machine1:2218""]
}
}

**model_main.py**
predict_input_fn = train_and_eval_dict['predict_input_fn']
train_steps = train_and_eval_dict['train_steps']

if FLAGS.checkpoint_dir:
    if FLAGS.eval_training_data:
        name = 'training_data'
        input_fn = eval_on_train_input_fn
    else:
        name = 'validation_data'

    # The first eval input will be evaluated.
    input_fn = eval_input_fns[0]

    if FLAGS.run_once:
        estimator.evaluate(input_fn,
            num_eval_steps=None,
            checkpoint_path=tf.train.latest_checkpoint(
                FLAGS.checkpoint_dir))
    else:
        model_lib.continuous_eval(estimator, FLAGS.checkpoint_dir, input_fn, train_steps, name)
else:
    train_spec, eval_specs = model_lib.create_train_and_eval_specs(
        train_input_fn,
        eval_input_fns,
        eval_on_train_input_fn,
        predict_input_fn,
        train_steps,
        eval_on_train_data=False)

  # Currently only a single Eval Spec is allowed.
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])


if __name__ == '__main__':
    tf.app.run()

**Other info / logs**
Warnings:

    W0828 00:03:55.229441 140490069309248 estimator.py:1924] Estimator's model_fn (.model_fn at 0x7fc5da9b5268>) includes params argument, but params are not passed to Estimator.

    ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/ResizeToRange/stack_2) axis attribute is out of bounds: 0

**But the training runs fine and evaluation happens. But my evaluation results are 0 all the time.**

creating index... 
index created! 
creating index... 
index created! 
Running per image evaluation... 
Evaluate annotation type bbox 
DONE (t=1.66s). 
Accumulating evaluation results... 
DONE (t=0.52s).

    Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.000

    Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.000

    Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.000

    Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000

    Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000

    Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000

    Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.000

    Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.000

    Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.000

    Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000

    Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000

    Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000

**Any help would be much appreciated. Thanks in advance.**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32041,micro: riscv32_mcu build failed with undefined references,"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: source
- TensorFlow version: 298534b745db43b2ad18256ed781bd2f142e5bc7
- Python version: 2.7.15+/3.6.8
- Installed using virtualenv? pip? conda?: pip3

**Describe the problem**
TF Lite for micro, riscv32 build fails with undefined references.

make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=riscv32_mcu TARGET_ARCH=riscv32 hello_world_bin

/home/ehirdoy/src/tensorflow/tensorflow/lite/experimental/micro/riscv32_mcu/debug_log.cc:18: undefined reference to `__wrap_puts'
exit.c:(.text.exit+0x2e): undefined reference to `__wrap__exit'
sbrkr.c:(.text._sbrk_r+0x12): undefined reference to `__wrap__sbrk'
writer.c:(.text._write_r+0x16): undefined reference to `__wrap__write'
closer.c:(.text._close_r+0x12): undefined reference to `__wrap__close'
lseekr.c:(.text._lseek_r+0x16): undefined reference to `__wrap__lseek'
readr.c:(.text._read_r+0x16): undefined reference to `__wrap__read'
fstatr.c:(.text._fstat_r+0x14): undefined reference to `__wrap__fstat'
isattyr.c:(.text._isatty_r+0x12): undefined reference to `__wrap__isatty'



**Provide the exact sequence of commands / steps that you executed before running into the problem**

export PATH=""$PATH:./tensorflow/lite/experimental/micro/tools/make/downloads/riscv_toolchain/bin""
make -f tensorflow/lite/experimental/micro/tools/make/Makefile clean
make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=riscv32_mcu TARGET_ARCH=riscv32 hello_world_bin


**Any other info / logs**
[make.log](https://github.com/tensorflow/tensorflow/files/3550454/make.log)
"
32039,[2.0] Wrong result in tf.keras.losses.BinaryCrossentropy?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.0 beta
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1 / 7.6.2
- GPU model and memory: rtx 2080 ti (11GB)

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Trying to translate a model from PyTorch to TF, i've encountered a strange behaviour which I don't understand. The value of the bceloss seems to be different from the value obtained in PyTorch, which instead coincides with the ""correct"" one.

**Describe the expected behavior**
The expected behaviour is to have the same value.

**Code to reproduce the issue**
out = [0.8800, 0.9271, 0.8596, 0.8748]
target = [1, 1, 1, 1]

expected value: -ln(0.88) - ln(0.9271) - ln(0.8596) - ln(0.8748) = 0.4886/4 = 0.1221

TF: 
tf.keras.losses.BinaryCrossentropy()(out, target)
<tf.Tensor: id=1452660, shape=(), dtype=float32, numpy=1.7575724>

PyTorch:
nn.BCELoss()(out, target)
tensor(0.1221, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)

The PyTorch one is the expected value, I don't understand what the TF value is.

**Other info / logs**
Solved temporarily by (re)defining the BCELoss simply copying the source from https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/backend.py

def real_bce_loss(output, target):
    epsilon = 1e-7
    output = tf.clip_by_value(output, epsilon, 1. - epsilon)
    bce = target * tf.math.log(output + epsilon)
    bce += (1 - target) * tf.math.log(1 - output + epsilon)
    return -tf.reduce_mean(bce)"
32038,Failed to build TFLite OpenCL delegate,"**System information**
- OS Platform and Distribution: [Official dockerfile for Android CI](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/Dockerfile.android)
- TensorFlow installed from (source or binary): source
- TensorFlow version: latest
- Python version:3.5
- Bazel version (if compiling from source):0.26.1
- GCC/Compiler version (if compiling from source):5.4.0
- CUDA/cuDNN version:not used

**Describe the problem**

I couldn't build TFLite OpenCL delegate.
Would like to tell me how to build the added feature?

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
bazel build -c opt --config=opt --config android_arm64 --cxxopt=--std=c++14 --color=yes //tensorflow/lite/delegates/gpu/cl:gpu_api_delegate
```

**Any other info / logs**

[Here](https://dev.azure.com/mlops/tflite/_build/results?buildId=332&view=logs&j=6040bc1a-72ff-56a7-5fe9-eeb7e9979f36&t=5d8150e6-3798-5a08-0300-038b15469693&l=55) is my complete CI logs.

```
[__w/1/s/tensorflow/tensorflow/lite/delegates/gpu/cl/BUILD:141:1: C++ compilation of rule '//tensorflow/lite/delegates/gpu/cl:gl_interop' failed (Exit 1): clang failed: error executing command 
  (cd /__w/1/b/execroot/org_tensorflow && \
  exec env - \
    ANDROID_BUILD_TOOLS_VERSION=28.0.0 \
    ANDROID_NDK_API_LEVEL=18 \
    ANDROID_NDK_HOME=/android/ndk \
    ANDROID_SDK_API_LEVEL=23 \
    ANDROID_SDK_HOME=/android/sdk \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/android/sdk/tools:/android/sdk/platform-tools:/android/ndk \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages \
    TF_CONFIGURE_IOS=0 \
  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64 -target aarch64-none-linux-android -ffunction-sections -funwind-tables -fstack-protector-strong -fpic -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -isystemexternal/androidndk/ndk/sysroot/usr/include/aarch64-linux-android '-D__ANDROID_API__=18' -O2 -g -DNDEBUG -MD -MF bazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/gpu/cl/_objs/gl_interop/gl_interop.pic.d '-frandom-seed=bazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/gpu/cl/_objs/gl_interop/gl_interop.pic.o' -fPIC -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEGL_EGLEXT_PROTOTYPES -iquote . -iquote bazel-out/arm64-v8a-opt/bin -iquote external/com_google_absl -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_absl -iquote external/opencl_headers -iquote bazel-out/arm64-v8a-opt/bin/external/opencl_headers -iquote external/FP16 -iquote bazel-out/arm64-v8a-opt/bin/external/FP16 -iquote external/flatbuffers -iquote bazel-out/arm64-v8a-opt/bin/external/flatbuffers -iquote external/farmhash_archive -iquote bazel-out/arm64-v8a-opt/bin/external/farmhash_archive -Ibazel-out/arm64-v8a-opt/bin/external/FP16/_virtual_includes/FP16 -isystem external/opencl_headers -isystem bazel-out/arm64-v8a-opt/bin/external/opencl_headers -isystem external/FP16/include -isystem bazel-out/arm64-v8a-opt/bin/external/FP16/include -isystem tensorflow/lite/delegates/gpu/cl -isystem bazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/gpu/cl -isystem external/flatbuffers/include -isystem bazel-out/arm64-v8a-opt/bin/external/flatbuffers/include -isystem external/farmhash_archive/src -isystem bazel-out/arm64-v8a-opt/bin/external/farmhash_archive/src -w '-std=c++14' '--std=c++14' '--sysroot=external/androidndk/ndk/platforms/android-28/arch-arm64' -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/lite/delegates/gpu/cl/gl_interop.cc -o bazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/gpu/cl/_objs/gl_interop/gl_interop.pic.o)
Execution platform: @bazel_tools//platforms:host_platform
tensorflow/lite/delegates/gpu/cl/gl_interop.cc:30:30: error: unknown type name 'EGLSync'
using PFNEGLCREATESYNCPROC = EGLSync(EGLAPIENTRYP)(
                             ^
tensorflow/lite/delegates/gpu/cl/gl_interop.cc:33:1: error: unknown type name 'PFNEGLCREATESYNCPROC'; did you mean 'PFNEGLCREATESYNCKHRPROC'?
PFNEGLCREATESYNCPROC g_eglCreateSync = nullptr;
^~~~~~~~~~~~~~~~~~~~
PFNEGLCREATESYNCKHRPROC
external/androidndk/ndk/sysroot/usr/include/EGL/eglext.h:152:34: note: 'PFNEGLCREATESYNCKHRPROC' declared here
typedef EGLSyncKHR (EGLAPIENTRYP PFNEGLCREATESYNCKHRPROC) (EGLDisplay dpy, EGLenum type, const EGLint *attrib_list);
                                 ^
tensorflow/lite/delegates/gpu/cl/gl_interop.cc:42:3: error: unknown type name 'EGLSync'
  EGLSync egl_sync;
  ^
tensorflow/lite/delegates/gpu/cl/gl_interop.cc:43:35: error: use of undeclared identifier 'EGL_CL_EVENT_HANDLE'
  const EGLAttrib attributes[] = {EGL_CL_EVENT_HANDLE,
                                  ^
tensorflow/lite/delegates/gpu/cl/gl_interop.cc:46:39: error: use of undeclared identifier 'EGL_SYNC_CL_EVENT'
                                      EGL_SYNC_CL_EVENT, attributes));
                                      ^
tensorflow/lite/delegates/gpu/cl/gl_interop.cc:47:19: error: use of undeclared identifier 'EGL_NO_SYNC'
  if (egl_sync == EGL_NO_SYNC) {
                  ^
tensorflow/lite/delegates/gpu/cl/gl_interop.cc:58:40: error: unknown type name 'PFNEGLCREATESYNCPROC'; did you mean 'PFNEGLCREATESYNCKHRPROC'?
    g_eglCreateSync = reinterpret_cast<PFNEGLCREATESYNCPROC>(
                                       ^~~~~~~~~~~~~~~~~~~~
                                       PFNEGLCREATESYNCKHRPROC
external/androidndk/ndk/sysroot/usr/include/EGL/eglext.h:152:34: note: 'PFNEGLCREATESYNCKHRPROC' declared here
typedef EGLSyncKHR (EGLAPIENTRYP PFNEGLCREATESYNCKHRPROC) (EGLDisplay dpy, EGLenum type, const EGLint *attrib_list);
                                 ^
7 errors generated.
Target //tensorflow/lite/delegates/gpu/cl:gpu_api_delegate failed to build
```"
32037,"when i follow the guide to create a mbed folder,i get this error,this is the first time i use TFlite ,can any one help me please","when i follow the [guide] to  demonstrate the absolute basics of using TensorFlow Lite for Microcontrollers.i create a folder for mbed,but i get this error:
make:*** no rule to make target ""tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/tensorflow/lite/experimental/micro/tools/make/downloads/CMSIS_ext/arm_cmplx_mag_squared_q10p6.c"",needed by ""generate_hello_world_mbed_project""
(https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/hello_world)
- OS Platform and Distribution (Linux Ubuntu 18.04):
"
32036,custom layers,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
32035,TF-TRT not producing any Tensor Engine Nodes,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.  Converting custom saved model.  The model is able to run.  However, ""trt.TrtGraphConverter"" does not appear to work.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04, Docker 19.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Installed from docker (tensorflow/tensorflow:nightly-gpu-py3)
- TensorFlow version (use command below): 1.14
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: 10.0
- GPU model and memory: T4

**Describe the current behavior**

This might be a documentation issue, but I'm confused as how to properly use TF-TRT with the nightly build.  Do I need to also use docker image `nvcr.io/nvidia/tensorrt:19.07-py3` or does image `tensorflow/tensorflow:nightly-gpu-py3` contain the necessary TensorRT binaries?  

When I used the image 'tensorflow/tensorflow:latest-gpu-py3', I received errors of an unknown TensorRT version (0,0,0).  I no longer see this error with the nightly version.

I have verified nvidia-docker is installed and working.  Am I missing a step, or is there an issue with TensorRT in the docker image?

My conversion code is:

```
from tensorflow.python.compiler.tensorrt import trt_convert as trt

converter = trt.TrtGraphConverter(
		input_saved_model_dir = str(saved_model_dir),
		max_batch_size = batch_size,
		precision_mode = precision )
	converter.convert()
	converter.save(output_saved_model_dir = str(output_saved_model_dir))
```

Am I linking to the correct trt import?  Thanks in advance.
"
32034,Customize Keras Tensorboard callback,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): install by pip in anaconda environment
- TensorFlow version (use command below):  tensorflow-gpu==2.0.0-rc0; Tensorboard 1.14
- Python version: 3.6.9
- CUDA/cuDNN version:  cudatoolkit( 10.0.130); cudnn( 7.6.0)     
- GPU model and memory: GeForce GTX 1080 - 8117MiB 


**Describe the current behavior**
In Tensorflow 1.13.1, I create my tensorboard callback to add image while training by customizing the `keras.callbacks.TensorBoard.`

**Ex**: from this :https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L1114-L1122
I add the code:
```
                if hasattr(layer, 'output'):
                    if isinstance(layer.output, list):
                        for i, output in enumerate(layer.output):
                            tf.summary.histogram('{}_out_{}'.format(layer.name, i),
                                                 output)
                    else:
                        tf.summary.histogram('{}_out'.format(layer.name),
                                             layer.output)

                # My code
                input1 = self.model.get_layer('input1').input
                tf.summary.image('input1', input1 , max_outputs=MAX_OUT)
                # End my code

        self.merged = tf.summary.merge_all()
```
Then I can see the image while training.

**But now in** https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/callbacks.py#L1491. 
I can't find `if self.histogram_freq and self.merged is None:`  or 
`self.merged = tf.summary.merge_all()` in the `def set_model(self, model):`

**Describe the expected behavior**
Where I can add the 
```
input1 = self.model.get_layer('input1').input
tf.summary.image('input1', input1 , max_outputs=MAX_OUT)
```
to see the image while training ?

"
32033,TFLite conversion change model weights,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution : Windows Server 2016
- TensorFlow installed from source
- TensorFlow version: 1.14
- Python version:3.5

**Describe the current behavior**
I convert .pb file to .tflite file .Then I found the model weights changed in .tflite file.
My CNN model just some separable convolution layers.In fact, I just replaced the ordinary convolution layer of VGG16 with the separable convolution layer. 
All the pointwise convolution  weights change in .tflite file ,while all the depthwise convolution weights remain the same in two files.
As you can see below as an example.

### **pointwise convolution weights in .pb:**
![1566965436(1)](https://user-images.githubusercontent.com/4417111/63825376-2cc21780-c98d-11e9-8802-27b67e615c19.png)

### **pointwise convolution weights in .tflite**
![1566965467(1)](https://user-images.githubusercontent.com/4417111/63825409-46fbf580-c98d-11e9-9efa-993f1833237c.png)

**Code to reproduce the issue**
### **I used this code below to freeze model weights and saved as .pb file.**
![1566965770(1)](https://user-images.githubusercontent.com/4417111/63825655-2ed8a600-c98e-11e9-8e01-62420e930fac.png)

### **I used this code below to convert .pb file to .tflite file.**
![1566965842(1)](https://user-images.githubusercontent.com/4417111/63825687-49128400-c98e-11e9-8853-c7b376b11c92.png)

### **I used this code below to print weights in .pb file and .tflite file.**
![1566965966(1)](https://user-images.githubusercontent.com/4417111/63825722-69dad980-c98e-11e9-8847-fae9f5e50884.png)


I don't konw why this problem occur.
"
32032,Is there a way to use keras flow_from_directory with distributed training (mirrored strategy)?,"I cannot find any documentation or example code on how to do dynamic file loading with distribute strategies in tensorflow 2.0 when doing distributed training. 

I see many examples using keras flow_from_directory which seems to be very nice because it relies on generator and does scaling etc on the fly so doesn't require loading hundreds of thousands of images at once. But the distribute strategy examples (mirrored strategy etc) all show pre-loading the entire dataset and then using dataset.from_tensor_slices() and loading that to strategy.experimental_distribute_dataset(). This is not feasable with large data that exceeds the memory. I've tried to combine the above dynamic loading method from keras with distributed batching but it looks like there is no trivial way to convert some flow_from_directory output to be compatible with what strategy.experimental_distribute_dataset() expects so these two features are probably not compatible with one another.

Is there some other way to do this with distributed training?

I could probably hand-code dynamic file loading during training but it would be pretty slow and basic compared to what generators and flow offer. I would be very surprised if TF2 didn't include this functionality for distributed training yet.

However, in lack of examples or documentation, assuming this is not yet implemented, I am posting it as feature request. Hopefully I'm wrong and there is a way to do it. Then I would recommend to add that to the distribute strategies documentation as example code.
"
32031,when I quantized mobilenetv3 got an error,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.13.1
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**
got error log:
2019-08-28 02:01:30.680587: F tensorflow/lite/toco/graph_transformations/quantize.cc:491] Unimplemented: this graph contains an operator of type Div for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
tflite_convert  --graph_def_file ./mobilenetstpu/v3small.pb   --inference_type QUANTIZED_UINT8  --input_arrays truediv --input_shapes 1,224,224,3  --output_arrays softmax_tensor  --std_dev_values 1  --mean_values 0  --default_ranges_min 0 --default_ranges_max 6 --output_file mobilenetv3.tflite 
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32029,tensorflow.keras.Model.compute_output_shape gives wrong results,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
linux Ubuntu 18.04

- TensorFlow installed from (source or binary):
conda
- TensorFlow version (use command below):
tried with 1.12.0 and 1.14.0
- Python version:
3.6

**Describe the current behavior**

using a keras model (stored in a variable mm) in tensorflow.keras I would like to calculate the output_shape for a given input. This works correctly only the first time I call `mm.compute_output_shape()`, the subsequent results for calling the same function with different shapes are inconsistent.

Using standard keras methods I get different and consistent results. 
An example for the problem is implemented in the tf_bug.py script that you find in the zip
if you call it without parameters it loads a fully convolutional model 
from a json file (provided in the zip) and does 

```
import json
import tensorflow.keras as keras
with open(""model_tf_bug.json"", ""r"") as fi:  
    kk=json.load(fi)  
    mm=keras.models.model_from_json(json.dumps(kk)) 

for n in range(999, 1020):  
     ss=[(1,n,1,1)]
     print(ss,mm.compute_output_shape(input_shape=ss)) 
```
the result displaying the input and corresponding output shape on each line is
```
[(1, 999, 1, 1)] (1, 481, 1, 1)
[(1, 1000, 1, 1)] (1, 481, 1, 1)
[(1, 1001, 1, 1)] (1, 482, 1, 1)
[(1, 1002, 1, 1)] (1, 482, 1, 1)
[(1, 1003, 1, 1)] (1, 483, 1, 1)
[(1, 1004, 1, 1)] (1, 483, 1, 1)
[(1, 1005, 1, 1)] (1, 484, 1, 1)
[(1, 1006, 1, 1)] (1, 484, 1, 1)
[(1, 1007, 1, 1)] (1, 482, 1, 1)
[(1, 1008, 1, 1)] (1, 485, 1, 1)
...
```
I kept only the relevant lines. You see that after the first lines that are correct 
starting with input shape 1007 the output shape decreases and starts to produce erratic behavior, while for the fully convolutional model it should increase monotonously with the input size.

**Describe the expected behavior**

Running the same script with argument keras uses the vanilla keras version 2.2.4
and in this case the output shape increases  as expected
```
[(1, 999, 1, 1)] (1, 481, 1, 1)
[(1, 1000, 1, 1)] (1, 481, 1, 1)
[(1, 1001, 1, 1)] (1, 482, 1, 1)
[(1, 1002, 1, 1)] (1, 482, 1, 1)
[(1, 1003, 1, 1)] (1, 483, 1, 1)
[(1, 1004, 1, 1)] (1, 483, 1, 1)
[(1, 1005, 1, 1)] (1, 484, 1, 1)
[(1, 1006, 1, 1)] (1, 484, 1, 1)
[(1, 1007, 1, 1)] (1, 485, 1, 1)
[(1, 1008, 1, 1)] (1, 485, 1, 1)
...
```

Note that I can get a correct result with tf.keras as well if I clear the model._output_shape_cache before I compute the output_shape.
Running the script with argument clear uses a modified loop as follows
```
for n in range(999, 1020):  
     ss=[(1,n,1,1)]
     if len(sys.argv) > 1 and sys.argv[1] == ""clear"":
         mm._output_shape_cache.clear()
     print(ss,mm.compute_output_shape(input_shape=ss)) 
```
The results are correct as expected. 

Looking into the function `mm.compute_output_shape`
 I found that compared to keras you changed the cache_key generation

where keras does
```   
cache_key = ', '.join([str(x) for x in input_shapes])
```
tf.keras does
```
    cache_key = generic_utils.object_list_uid(input_shape)
```
It appears that the cache_key in tf.keras confuses different input shapes as the same and returns wrong results from the cache.

**Code to reproduce the issue**

You find the script, model and output files in the zip

[tf_compute_output_shape_bug.zip](https://github.com/tensorflow/tensorflow/files/3548512/tf_compute_output_shape_bug.zip)
"
32028,Can't import tensorflow.config.slim in TensorFlow 1.15 nightly build ,"## System Information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux  Ubuntu 18.04 LTS 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.12.1-7396-g12481e7e74 1.15.0-dev20190730
- **Python version**: Python 3.6.8
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: Don't need it in this case, but cuda_10.1.243_418.87.00
- **GPU model and memory**: Don't need it in this case, but GeForce GTX 960 with 2 GB/
- **Exact command to reproduce**: `python -c ""import tensorflow.contrib.slim""`

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
I am trying to quantize a model for use on Coral USB Accelerator.  According to https://coral.withgoogle.com/docs/edgetpu/models-intro/, ""you must use the TensorFlow 1.15 ""nightly"" build and set both the input and output type to uint8"".  The model I am working with uses tensorflow.contrib.slim.  If I try to import tensorflow.contrib.slim with 1.15 installed, I get the following traceback:

### Source code / logs
```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/jbrownkramer/Desktop/YOLOV3 on Coral USB/yoloOnCoralEnv/lib/python3.6/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/home/jbrownkramer/Desktop/YOLOV3 on Coral USB/yoloOnCoralEnv/lib/python3.6/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/home/jbrownkramer/Desktop/YOLOV3 on Coral USB/yoloOnCoralEnv/lib/python3.6/site-packages/tensorflow_core/contrib/__init__.py"", line 39, in <module>
    from tensorflow.contrib import compiler
  File ""/home/jbrownkramer/Desktop/YOLOV3 on Coral USB/yoloOnCoralEnv/lib/python3.6/site-packages/tensorflow_core/contrib/compiler/__init__.py"", line 21, in <module>
    from tensorflow.contrib.compiler import jit
  File ""/home/jbrownkramer/Desktop/YOLOV3 on Coral USB/yoloOnCoralEnv/lib/python3.6/site-packages/tensorflow_core/contrib/compiler/__init__.py"", line 22, in <module>
    from tensorflow.contrib.compiler import xla
  File ""/home/jbrownkramer/Desktop/YOLOV3 on Coral USB/yoloOnCoralEnv/lib/python3.6/site-packages/tensorflow_core/contrib/compiler/xla.py"", line 22, in <module>
    from tensorflow.python.estimator import model_fn as model_fn_lib
  File ""/home/jbrownkramer/Desktop/YOLOV3 on Coral USB/yoloOnCoralEnv/lib/python3.6/site-packages/tensorflow_core/python/estimator/model_fn.py"", line 26, in <module>
    from tensorflow_estimator.python.estimator import model_fn
  File ""/home/jbrownkramer/Desktop/YOLOV3 on Coral USB/yoloOnCoralEnv/lib/python3.6/site-packages/tensorflow_estimator/__init__.py"", line 10, in <module>
    from tensorflow_estimator._api.v1 import estimator
  File ""/home/jbrownkramer/Desktop/YOLOV3 on Coral USB/yoloOnCoralEnv/lib/python3.6/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 10, in <module>
    from tensorflow_estimator._api.v1.estimator import experimental
  File ""/home/jbrownkramer/Desktop/YOLOV3 on Coral USB/yoloOnCoralEnv/lib/python3.6/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 10, in <module>
    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder
  File ""/home/jbrownkramer/Desktop/YOLOV3 on Coral USB/yoloOnCoralEnv/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 23, in <module>
    from tensorflow.python.feature_column import dense_features
ImportError: cannot import name 'dense_features'
```

"
32026,Tensorflow r2.0 will not build successfully on skylake machines,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): not installed (attempting to build from source)
- TensorFlow version: attempting to build from branch r2.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: Anaconda conda virtual environment 
- Bazel version (if compiling from source): 0.26.0
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: CUDA 10, cuDNN 7.6.2
- GPU model and memory: NVidia RTX 2080 Ti (MSI Sea Hawk X, 11 GB)

**Describe the problem**
bazel build is unsuccessful yielding error in tensorflow/lite/experimental/ruy/kernel_avx512.cc
Please see [Issue 31187](https://github.com/tensorflow/tensorflow/issues/31187) for background leading to the same bug being resolved on the master branch.  I don't know how they fixed it on master.  The fix needs to be ported from master to r2.0 branch.

Please also note that you can bypass using AVX-512 and get a successful build by changing:
~/tensorflow/tensorflow/lite/experimental/ruy/platform.h
Comment out:
// TODO(b/138433137) Select AVX-512 at runtime rather than via compile options.
// #if defined(__AVX512F__) && defined(__AVX512DQ__) && defined(__AVX512CD__) && \
    defined(__AVX512BW__) && defined(__AVX512VL__)
// #define RUY_DONOTUSEDIRECTLY_AVX512 1
// #else
#define RUY_DONOTUSEDIRECTLY_AVX512 0
// #endif

**Provide the exact sequence of commands / steps that you executed before running into the problem**

git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout r2.0
./configure
Accept defaults initially
When asked for CUDA support, type ""y""
When asked for TensorRT support, type ""y""
When asked for compute capabilities, type 7.0,7.5
Accept defaults thereafter
bazel build --explain=verbose_explanations.txt --verbose_explanations --verbose_failures --subcommands=pretty_print --config=opt --config=cuda --config=v2 --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
This error does not occur on machines that do not have Skylake support for AVX-512 (I know because I can build it fine on my laptop and other users report this bug is particular to Skylake).

I have attached a dump of the terminal output of the bazel build command (cleaned-capture.txt) and verbose_explanations.txt:

[cleaned-capture.txt](https://github.com/tensorflow/tensorflow/files/3548002/cleaned-capture.txt)
[verbose_explanations.txt](https://github.com/tensorflow/tensorflow/files/3548006/verbose_explanations.txt)

Full error message:
ERROR: /home/daniel/tensorflow/tensorflow/lite/experimental/ruy/BUILD:271:1: C++ compilation of rule '//tensorflow/lite/experimental/ruy:kernel' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/daniel/.cache/bazel/_bazel_daniel/79db702fc9f94af7d11e11c5d64854d0/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/extras/CUPTI/lib64 \
    PATH=/home/daniel/anaconda3/envs/tfgpu/bin:/home/daniel/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/lite/experimental/ruy/_objs/kernel/kernel_avx512.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/lite/experimental/ruy/_objs/kernel/kernel_avx512.pic.o' -iquote . -iquote bazel-out/host/bin -iquote external/gemmlowp -iquote bazel-out/host/bin/external/gemmlowp '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -c tensorflow/lite/experimental/ruy/kernel_avx512.cc -o bazel-out/host/bin/tensorflow/lite/experimental/ruy/_objs/kernel/kernel_avx512.pic.o)
Execution platform: @bazel_tools//platforms:host_platform
In file included from tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:0:
./tensorflow/lite/experimental/ruy/kernel.h:613:9: warning: multi-line comment [-Wcomment]
 #endif  // (RUY_PLATFORM(NEON_64) || RUY_PLATFORM(NEON_32) || \
         ^
In file included from external/gemmlowp/fixedpoint/fixedpoint.h:895:0,
                 from ./tensorflow/lite/experimental/ruy/kernel.h:22,
                 from tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:
external/gemmlowp/fixedpoint/./fixedpoint_sse.h:43:39: warning: ignoring attributes on template argument ‘__m128i {aka __vector(2) long long int}’ [-Wignored-attributes]
 struct FixedPointRawTypeTraits<__m128i> {
                                       ^
In file included from tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:0:
./tensorflow/lite/experimental/ruy/kernel.h: In function ‘void ruy::MakeKernelParamsFloat(const ruy::PackedMatrix<float>&, const ruy::PackedMatrix<float>&, const ruy::BasicSpec<float, float>&, int, int, int, int, ruy::Matrix<float>*, ruy::KernelParamsFloat<LhsCols, RhsCols>*)’:
./tensorflow/lite/experimental/ruy/kernel.h:456:53: warning: typedef ‘using Params = struct ruy::KernelParamsFloat<LhsCols, RhsCols>’ locally defined but not used [-Wunused-local-typedefs]
   using Params = KernelParamsFloat<LhsCols, RhsCols>;
                                                     ^
tensorflow/lite/experimental/ruy/kernel_avx512.cc: In function ‘void ruy::Kernel8bitAvx512(const ruy::KernelParams8bit<16, 16>&)’:
tensorflow/lite/experimental/ruy/kernel_avx512.cc:111:34: error: ‘_mm512_loadu_epi8’ was not declared in this scope
         const __m512i lhs_data = _mm512_loadu_epi8(lhs_ptr);
                                  ^~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:111:34: note: suggested alternative: ‘_mm512_add_epi8’
         const __m512i lhs_data = _mm512_loadu_epi8(lhs_ptr);
                                  ^~~~~~~~~~~~~~~~~
                                  _mm512_add_epi8
tensorflow/lite/experimental/ruy/kernel_avx512.cc:161:32: error: ‘_mm512_loadu_epi32’ was not declared in this scope
                                _mm512_loadu_epi32(&params.lhs_sums[row]));
                                ^~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:161:32: note: suggested alternative: ‘_mm512_load_epi32’
                                _mm512_loadu_epi32(&params.lhs_sums[row]));
                                ^~~~~~~~~~~~~~~~~~
                                _mm512_load_epi32
tensorflow/lite/experimental/ruy/kernel_avx512.cc:170:32: error: ‘_mm512_loadu_epi32’ was not declared in this scope
                                _mm512_loadu_epi32(&params.rhs_sums[col]));
                                ^~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:170:32: note: suggested alternative: ‘_mm512_load_epi32’
                                _mm512_loadu_epi32(&params.rhs_sums[col]));
                                ^~~~~~~~~~~~~~~~~~
                                _mm512_load_epi32
tensorflow/lite/experimental/ruy/kernel_avx512.cc:277:13: error: ‘_mm_storeu_epi8’ was not declared in this scope
             _mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));
             ^~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:277:13: note: suggested alternative: ‘_mm_store_epi64’
             _mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));
             ^~~~~~~~~~~~~~~
             _mm_store_epi64
tensorflow/lite/experimental/ruy/kernel_avx512.cc:293:13: error: ‘_mm_storeu_epi8’ was not declared in this scope
             _mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));
             ^~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:293:13: note: suggested alternative: ‘_mm_store_epi64’
             _mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));
             ^~~~~~~~~~~~~~~
             _mm_store_epi64
tensorflow/lite/experimental/ruy/kernel_avx512.cc:309:13: error: ‘_mm256_storeu_epi16’ was not declared in this scope
             _mm256_storeu_epi16(tmp_ptr,
             ^~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:309:13: note: suggested alternative: ‘_mm256_store_epi64’
             _mm256_storeu_epi16(tmp_ptr,
             ^~~~~~~~~~~~~~~~~~~
             _mm256_store_epi64
tensorflow/lite/experimental/ruy/kernel_avx512.cc:326:13: error: ‘_mm512_storeu_epi32’ was not declared in this scope
             _mm512_storeu_epi32(tmp_ptr, accum_data_v[j]);
             ^~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:326:13: note: suggested alternative: ‘_mm512_store_epi32’
             _mm512_storeu_epi32(tmp_ptr, accum_data_v[j]);
             ^~~~~~~~~~~~~~~~~~~
             _mm512_store_epi32
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 35.925s, Critical Path: 10.04s
INFO: 282 processes: 282 local.
FAILED: Build did NOT complete successfully"
32025,I need more accuracy,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
 1.13 and 2.0
- Are you willing to contribute it (Yes/No): 
Yes

**Describe the feature and the current behavior/state.**
Tensorflow has been used to compute images but I want to use Tensorflow to compute Biological Models. However, the biological model requires big division and this causes numerical instability. I want to have tensorflow that supports more numerically stablility.  
**Will this change the current api? How?**
I don't know
**Who will benefit with this feature?**
A biologist who wants to use tensorflow as a means to compute gene expression models with high degree of accuracy.
**Any Other info.**
This is based on my recent work in 
https://www.biorxiv.org/content/10.1101/655639v1"
32024,TPU nightly,"I'm training a custom resnet on a single TPU device using tf.keras, and saving the model using ModelCheckpoint callback on the VM. The time takes to save the model during training is very much. The model is about 200mb and it takes ~1 hour to save it to the VM"
32023,tf.keras.layers.Concatenate layer has unexpected behavior since 1.13,"`tf.keras.layers.Concatenate` used to operate in a very straightforward way with the Keras functional API for building DenseNet-esque feedforward networks.

The code below works in TF 1.13 but fails in 1.14, 2.0.0a0 and beyond.
 
I also tested replacing the `Concatenate` layer with its functional alternative, `concatenate`, and produced the same error.
```python
from tensorflow.keras.layers import Dense, Input, Concatenate
from tensorflow.keras.models import Model

inputs = Input(shape=(10,))

all_layers = []

x1 = Dense(512)(inputs)
all_layers.append(x1)

# all layers: [x1]
x2 = Dense(256, activation='relu')(x1)
all_layers.append(x2)

# all layers: [x1, x2]
conc = Concatenate()(all_layers)
x3 = Dense(128, activation='relu')(conc)
all_layers.append(x3)

# all layers: [x1, x2, x3]
conc = Concatenate()(all_layers)
prediction = Dense(1)(conc)
model = Model(inputs=inputs, outputs=prediction)
```
The error output is 
```
ValueError: Graph disconnected: cannot obtain value for tensor Tensor(""dense_2/Identity:0"", shape=(None, 128), dtype=float32) at layer ""concatenate"". The following previous layers were accessed without issue: ['input_1', 'dense', 'dense_1']
```
Visualization of model
![](https://i.imgur.com/17UNrSk.png)
"
32019,hi  i want to freeze east text a  pretrained model    the ckpt.meta data file  exist in the model folder but i get this error can any one help me please !!!!                                              ,"re_fusion/Conv_7/Sigmoid,model_0/feature_fusion/concat_3
Traceback (most recent call last):
  File ""freez2.py"", line 66, in <module>
    freeze_graph(args.model_dir, args.output_node_names)
  File ""freez2.py"", line 41, in freeze_graph
    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices
=clear_devices)
  File ""C:\Users\DeLl\Anaconda3\lib\site-packages\tensorflow\python\training\sav
er.py"", line 1435, in import_meta_graph
    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]
  File ""C:\Users\DeLl\Anaconda3\lib\site-packages\tensorflow\python\training\sav
er.py"", line 1447, in _import_meta_graph_with_return_elements
    meta_graph_def = meta_graph.read_meta_graph_file(meta_graph_or_file)
  File ""C:\Users\DeLl\Anaconda3\lib\site-packages\tensorflow\python\framework\me
ta_graph.py"", line 633, in read_meta_graph_file
    raise IOError(""File %s does not exist."" % filename)
OSError: File /tmp/east_icdar2015_resnet_v1_50_rbox/model.ckpt-49491.meta does n
ot exist.

(base) C:\Users\DeLl\Documents\east>"
32018,[TF 2.0] Unsupported op node error messages in latest tf-nightly (8-27-19),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview==2.0.0.dev20190827
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0/7.6.2
- GPU model and memory: Titan Xp 12 gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When running a model that previously yielded no errors, I'm getting errors of the form 
```
2019-08-27 11:35:15.004095: E tensorflow/compiler/jit/compilability_check_util.cc:346] unsupported op node : {name:'policy/fn__call__/build_one_time_effects_or_recurrence/get_effect
s/get_phiC/add_collisions/while/enter/_15' id:223 op device:{/job:localhost/replica:0/task:0/device:GPU:0} def:{{{node policy/fn__call__/build_one_time_effects_or_recurrence/get_eff
ects/get_phiC/add_collisions/while/enter/_15}} = Enter[T=DT_INT32, frame_name=""policy/fn_...ions/while"", is_constant=false, parallel_iterations=10, _device=""/job:localhost/replica:0
/task:0/device:GPU:0""](policy/apply_gauss_force/ExpandDims/dim)}}
2019-08-27 11:35:15.004669: E tensorflow/compiler/jit/compilability_check_util.cc:346] unsupported op node : {name:'policy/fn__call__/build_one_time_effects_or_recurrence/get_effect
s/get_phiC/add_collisions/while/enter/_9' id:224 op device:{/job:localhost/replica:0/task:0/device:GPU:0} def:{{{node policy/fn__call__/build_one_time_effects_or_recurrence/get_effe
cts/get_phiC/add_collisions/while/enter/_9}} = Enter[T=DT_INT32, frame_name=""policy/fn_...ions/while"", is_constant=false, parallel_iterations=10, _device=""/job:localhost/replica:0/t
ask:0/device:GPU:0""](policy/postprocess_policy_inputs/Sum/reduction_indices)}}
2019-08-27 11:35:15.005043: E tensorflow/compiler/jit/compilability_check_util.cc:346] unsupported op node : {name:'policy/fn__call__/build_one_time_effects_or_recurrence/get_effect
s/get_phiC/add_collisions/while/enter/_4' id:227 op device:{/job:localhost/replica:0/task:0/device:GPU:0} def:{{{node policy/fn__call__/build_one_time_effects_or_recurrence/get_effe
cts/get_phiC/add_collisions/while/enter/_4}} = Enter[T=DT_INT32, frame_name=""policy/fn_...ions/while"", is_constant=false, parallel_iterations=10, _device=""/job:localhost/replica:0/t
ask:0/device:GPU:0""](policy/postprocess_policy_inputs/get_collision/strided_slice_3/stack_1/0)}}
2019-08-27 11:35:15.005378: E tensorflow/compiler/jit/compilability_check_util.cc:346] unsupported op node : {name:'policy/fn__call__/build_one_time_effects_or_recurrence/get_effect
s/get_phiC/add_collisions/while/enter/_2' id:228 op device:{/job:localhost/replica:0/task:0/device:GPU:0} def:{{{node policy/fn__call__/build_one_time_effects_or_recurrence/get_effe
cts/get_phiC/add_collisions/while/enter/_2}} = Enter[T=DT_INT32, frame_name=""policy/fn_...ions/while"", is_constant=false, parallel_iterations=10, _device=""/job:localhost/replica:0/t
ask:0/device:GPU:0""](policy/postprocess_policy_inputs/get_collision/strided_slice_3/stack_1/0)}}
2019-08-27 11:35:15.005796: E tensorflow/compiler/jit/compilability_check_util.cc:346] unsupported op node : {name:'policy/fn__call__/build_one_time_effects_or_recurrence/get_effect
s/get_phiC/add_collisions/while/enter/_11' id:242 op device:{/job:localhost/replica:0/task:0/device:GPU:0} def:{{{node policy/fn__call__/build_one_time_effects_or_recurrence/get_eff
ects/get_phiC/add_collisions/while/enter/_11}} = Enter[T=DT_BOOL, frame_name=""policy/fn_...ions/while"", is_constant=false, parallel_iterations=10, _device=""/job:localhost/replica:0/
task:0/device:GPU:0""](policy/postprocess_policy_inputs/Tile_7)}}
2019-08-27 11:35:15.006245: E tensorflow/compiler/jit/compilability_check_util.cc:346] unsupported op node : {name:'policy/fn__call__/build_one_time_effects_or_recurrence/get_effect
s/get_phiC/add_collisions/while/merge/_29' id:255 op device:{/job:localhost/replica:0/task:0/device:GPU:0} def:{{{node policy/fn__call__/build_one_time_effects_or_recurrence/get_eff
ects/get_phiC/add_collisions/while/merge/_29}} = Merge[N=2, T=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](policy/fn__call__/build_one_time_effects_or_recurrenc
e/get_effects/get_phiC/add_collisions/while/enter/_15, policy/fn__call__/build_one_time_effects_or_recurrence/get_effects/get_phiC/add_collisions/while/next_iteration/_74)}}
2019-08-27 11:35:15.006623: E tensorflow/compiler/jit/compilability_check_util.cc:346] unsupported op node : {name:'policy/fn__call__/build_one_time_effects_or_recurrence/get_effect
s/get_phiC/add_collisions/while/merge/_23' id:256 op device:{/job:localhost/replica:0/task:0/device:GPU:0} def:{{{node policy/fn__call__/build_one_time_effects_or_recurrence/get_eff
ects/get_phiC/add_collisions/while/merge/_23}} = Merge[N=2, T=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](policy/fn__call__/build_one_time_effects_or_recurrenc
e/get_effects/get_phiC/add_collisions/while/enter/_9, policy/fn__call__/build_one_time_effects_or_recurrence/get_effects/get_phiC/add_collisions/while/next_iteration/_68)}}
2019-08-27 11:35:15.007007: E tensorflow/compiler/jit/compilability_check_util.cc:346] unsupported op node : {name:'policy/fn__call__/build_one_time_effects_or_recurrence/get_effect
s/get_phiC/add_collisions/while/merge/_18' id:259 op device:{/job:localhost/replica:0/task:0/device:GPU:0} def:{{{node policy/fn__call__/build_one_time_effects_or_recurrence/get_eff
ects/get_phiC/add_collisions/while/merge/_18}} = Merge[N=2, T=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](policy/fn__call__/build_one_time_effects_or_recurrenc
e/get_effects/get_phiC/add_collisions/while/enter/_4, policy/fn__call__/build_one_time_effects_or_recurrence/get_effects/get_phiC/add_collisions/while/next_iteration/_63)}}
2019-08-27 11:35:15.007369: E tensorflow/compiler/jit/compilability_check_util.cc:346] unsupported op node : {name:'policy/fn__call__/build_one_time_effects_or_recurrence/get_effect
s/get_phiC/add_collisions/while/merge/_16' id:260 op device:{/job:localhost/replica:0/task:0/device:GPU:0} def:{{{node policy/fn__call__/build_one_time_effects_or_recurrence/get_eff
ects/get_phiC/add_collisions/while/merge/_16}} = Merge[N=2, T=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](policy/fn__call__/build_one_time_effects_or_recurrenc
e/get_effects/get_phiC/add_collisions/while/enter/_2, policy/fn__call__/build_one_time_effects_or_recurrence/get_effects/get_phiC/add_collisions/while/next_iteration/_61)}}
```
when using autograph with the tf.function decorator.

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
I'm just curious how we are supposed to interpret these logs; so far it's non blocking, but I'm wondering if this is indicating autograph is failing to convert parts of my model.
"
32017,Training stalls after saving checkpoint 0,"Hello. 

I'm trying to run the LibriSpeech problem using tensor2tensor on Google Colab's GPU runtime, but the training stalls after saving checkpoint 0 and opening dynamic library libcublas.so.10.0. There is no error message, it just stops there forever. 
I'm posting it here because the stalling point happens on Tensorflow's packages.

Python's version : 3.6.8
Tensorflow's version : 1.14.0
tensor2tensor's version : 1.14.0
CUDA's version : 10.1
OS : Ubuntu 18.04

This is the code
```
from tensor2tensor import models
from tensor2tensor.utils import registry

!t2t-trainer \
    --tmp_dir='/content/gdrive/My Drive/TCC/T2T LibriSpeech/tmp/' \
    --problem='librispeech_clean_small' \
    --model='transformer' \
    --train_steps=10 \
    --hparams_set='transformer_librispeech' \
    --data_dir='/content/gdrive/My Drive/TCC/T2T LibriSpeech/data/' \
    --output_dir='/content/gdrive/My Drive/TCC/T2T LibriSpeech/output/' \
    --worker-gpu=0
```

And here's the output : 

```
WARNING: Logging before flag parsing goes to stderr.
W0827 17:43:33.747592 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/expert_utils.py:68: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0827 17:43:35.111425 139969908836224 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0827 17:43:36.899833 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/adafactor.py:27: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

W0827 17:43:36.900365 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/multistep_optimizer.py:32: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0827 17:43:36.911696 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/mesh_tensorflow/ops.py:4237: The name tf.train.CheckpointSaverListener is deprecated. Please use tf.estimator.CheckpointSaverListener instead.

W0827 17:43:36.911862 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/mesh_tensorflow/ops.py:4260: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

W0827 17:43:36.928164 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/models/research/neural_stack.py:38: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

W0827 17:43:36.975095 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/rl/gym_utils.py:235: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

W0827 17:43:36.993708 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/trainer_lib.py:111: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

W0827 17:43:37.006869 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorflow_gan/python/contrib_utils.py:305: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.

W0827 17:43:37.007008 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorflow_gan/python/contrib_utils.py:310: The name tf.estimator.tpu.TPUEstimatorSpec is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimatorSpec instead.

W0827 17:43:38.449517 139969908836224 deprecation_wrapper.py:119] From /usr/local/bin/t2t-trainer:32: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

W0827 17:43:38.449705 139969908836224 deprecation_wrapper.py:119] From /usr/local/bin/t2t-trainer:32: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

W0827 17:43:38.449807 139969908836224 deprecation_wrapper.py:119] From /usr/local/bin/t2t-trainer:33: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

I0827 17:43:38.450179 139969908836224 t2t_trainer.py:155] Found unparsed command-line arguments. Checking if any start with --hp_ and interpreting those as hparams settings.
W0827 17:43:38.450768 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/bin/t2t_trainer.py:165: The name tf.logging.warn is deprecated. Please use tf.compat.v1.logging.warn instead.

W0827 17:43:38.450837 139969908836224 t2t_trainer.py:165] Found unknown flag: --worker-gpu=0
W0827 17:43:38.451183 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/hparams_lib.py:49: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.

W0827 17:43:38.451832 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/trainer_lib.py:839: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

W0827 17:43:38.452693 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/trainer_lib.py:123: The name tf.GraphOptions is deprecated. Please use tf.compat.v1.GraphOptions instead.

W0827 17:43:38.452859 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/trainer_lib.py:129: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

W0827 17:43:38.453019 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/trainer_lib.py:242: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.
Instructions for updating:
When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.
I0827 17:43:38.453181 139969908836224 trainer_lib.py:265] Configuring DataParallelism to replicate the model.
I0827 17:43:38.453252 139969908836224 devices.py:76] schedule=continuous_train_and_eval
I0827 17:43:38.453314 139969908836224 devices.py:77] worker_gpu=1
I0827 17:43:38.453381 139969908836224 devices.py:78] sync=False
W0827 17:43:38.453437 139969908836224 devices.py:141] Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.
I0827 17:43:38.453504 139969908836224 devices.py:170] datashard_devices: ['gpu:0']
I0827 17:43:38.453559 139969908836224 devices.py:171] caching_devices: None
I0827 17:43:38.454001 139969908836224 devices.py:172] ps_devices: ['gpu:0']
I0827 17:43:38.454567 139969908836224 estimator.py:209] Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4cda9f8438>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
, '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_protocol': None, '_session_config': gpu_options {
  per_process_gpu_memory_fraction: 0.95
}
allow_soft_placement: true
graph_options {
  optimizer_options {
    global_jit_level: OFF
  }
}
isolate_session_state: true
, '_save_checkpoints_steps': 1000, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/content/gdrive/My Drive/TCC/T2T LibriSpeech/output/', 'use_tpu': False, 't2t_device_info': {'num_async_replicas': 1}, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7f4cda9f84a8>}
W0827 17:43:38.454751 139969908836224 model_fn.py:630] Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7f4cda9e7ae8>) includes params argument, but params are not passed to Estimator.
W0827 17:43:38.454877 139969908836224 trainer_lib.py:783] ValidationMonitor only works with --schedule=train_and_evaluate
W0827 17:43:38.455530 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/bin/t2t_trainer.py:328: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.

W0827 17:43:38.458196 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/bin/t2t_trainer.py:344: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.

I0827 17:43:38.487565 139969908836224 estimator_training.py:186] Not using Distribute Coordinator.
I0827 17:43:38.487942 139969908836224 training.py:612] Running training and evaluation locally (non-distributed).
I0827 17:43:38.488237 139969908836224 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.
W0827 17:43:38.493283 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0827 17:43:38.502703 139969908836224 problem.py:644] Reading data files from /content/gdrive/My Drive/TCC/T2T LibriSpeech/data/librispeech_clean_small-train*
I0827 17:43:38.543926 139969908836224 problem.py:670] partition: 0 num_data_files: 100
W0827 17:43:38.545797 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/data_generators/problem.py:680: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0827 17:43:38.581830 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/layers/common_audio.py:92: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0827 17:43:38.823341 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/layers/common_audio.py:115: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0827 17:43:38.987241 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/data_reader.py:275: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.
Instructions for updating:
Use eager execution and: 
`tf.data.TFRecordDataset(path)`
W0827 17:43:40.327878 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/data_reader.py:395: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(dataset)`.
W0827 17:43:40.328149 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/data_reader.py:398: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.

W0827 17:43:40.328256 139969908836224 data_reader.py:399] Shapes are not fully defined. Assuming batch_size means tokens.
W0827 17:43:40.374079 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/grouping.py:193: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0827 17:43:40.414666 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/data_reader.py:231: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

I0827 17:43:40.470206 139969908836224 estimator.py:1145] Calling model_fn.
I0827 17:43:40.481091 139969908836224 t2t_model.py:2248] Setting T2TModel mode to 'train'
W0827 17:43:40.552857 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/t2t_model.py:244: The name tf.summary.text is deprecated. Please use tf.compat.v1.summary.text instead.

I0827 17:43:41.160171 139969908836224 api.py:255] Using variable initializer: uniform_unit_scaling
I0827 17:43:41.531091 139969908836224 t2t_model.py:2248] Transforming feature 'inputs' with speech_recognition_modality.bottom
W0827 17:43:41.532868 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/layers/modalities.py:439: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0827 17:43:41.922302 139969908836224 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_256_384.targets_bottom
I0827 17:43:42.037450 139969908836224 t2t_model.py:2248] Building model body
W0827 17:43:42.094394 139969908836224 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/models/transformer.py:96: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
W0827 17:43:42.130389 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/layers/common_layers.py:3077: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.

W0827 17:43:42.473380 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/layers/common_attention.py:1249: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

I0827 17:43:49.011597 139969908836224 t2t_model.py:2248] Transforming body output with symbol_modality_256_384.top
W0827 17:43:49.118912 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/learning_rate.py:120: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

I0827 17:43:49.120072 139969908836224 learning_rate.py:29] Base learning rate: 2.000000
I0827 17:43:49.131614 139969908836224 optimize.py:338] Trainable Variables Total size: 70343552
I0827 17:43:49.131888 139969908836224 optimize.py:338] Non-trainable variables Total size: 5
I0827 17:43:49.132170 139969908836224 optimize.py:193] Using optimizer adam
I0827 17:43:59.596418 139969908836224 estimator.py:1147] Done calling model_fn.
I0827 17:43:59.597772 139969908836224 basic_session_run_hooks.py:541] Create CheckpointSaverHook.
I0827 17:44:03.685569 139969908836224 monitored_session.py:240] Graph was finalized.
2019-08-27 17:44:03.685968: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-27 17:44:03.708726: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-08-27 17:44:03.898700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-27 17:44:03.899340: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x207fb80 executing computations on platform CUDA. Devices:
2019-08-27 17:44:03.899389: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5
2019-08-27 17:44:03.901408: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2019-08-27 17:44:03.901570: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x207ea00 executing computations on platform Host. Devices:
2019-08-27 17:44:03.901594: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-08-27 17:44:03.901797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-27 17:44:03.902276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59
pciBusID: 0000:00:04.0
2019-08-27 17:44:03.902614: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-27 17:44:03.907500: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-27 17:44:03.908556: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-27 17:44:03.911851: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-27 17:44:03.916549: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-27 17:44:03.917606: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-27 17:44:03.925044: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-27 17:44:03.925147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-27 17:44:03.925681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-27 17:44:03.926137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-08-27 17:44:03.926182: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-27 17:44:03.927269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-27 17:44:03.927290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-08-27 17:44:03.927300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-08-27 17:44:03.927408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-27 17:44:03.927907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-27 17:44:03.928376: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2019-08-27 17:44:03.928411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14325 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)
2019-08-27 17:44:07.049904: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
I0827 17:44:09.037412 139969908836224 session_manager.py:500] Running local_init_op.
I0827 17:44:09.280463 139969908836224 session_manager.py:502] Done running local_init_op.
I0827 17:44:18.882892 139969908836224 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /content/gdrive/My Drive/TCC/T2T LibriSpeech/output/model.ckpt.
2019-08-27 17:44:39.361151: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
```"
32015,"""File already exists in database: google/protobuf/descriptor.proto"" after TensorFlow import","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.14
- Python version: 3.6
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): gcc 5.4.0
- CUDA/cuDNN version: 10.1, 7.5.1
- GPU model and memory: N/A

**Describe the problem**

I built TensorFlow 1.14.0 from source. I have a patch to ensure that we build against protobuf 3.6.1 since that is the version that is used across the rest of my firm:
```
diff --git a/tensorflow/workspace.bzl b/tensorflow/workspace.bzl
index 55d7eb9371..9a3693818e 100755
--- a/tensorflow/workspace.bzl
+++ b/tensorflow/workspace.bzl
@@ -373,16 +373,17 @@ def tf_workspace(path_prefix = """", tf_repo_name = """"):
 
     # 5902e759108d14ee8e6b0b07653dac2f4e70ac73 is based on 3.7.1 with a fix for BUILD file.
     PROTOBUF_URLS = [
-        ""http://mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/5902e759108d14ee8e6b0b07653dac2f4e70ac73.tar.gz"",
-        ""https://github.com/protocolbuffers/protobuf/archive/5902e759108d14ee8e6b0b07653dac2f4e70ac73.tar.gz"",
+        ""http://mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/48cb18e5c419ddd23d9badcfe4e9df7bde1979b2.tar.gz"",
+        ""https://github.com/protocolbuffers/protobuf/archive/48cb18e5c419ddd23d9badcfe4e9df7bde1979b2.tar.gz"",
     ]
-    PROTOBUF_SHA256 = ""1c020fafc84acd235ec81c6aac22d73f23e85a700871466052ff231d69c1b17a""
-    PROTOBUF_STRIP_PREFIX = ""protobuf-5902e759108d14ee8e6b0b07653dac2f4e70ac73""
+    PROTOBUF_SHA256 = ""f5a35e17fb07f3b13517264cd17a089636fcbb2912f9df7bef7414058969a8d2""
+    PROTOBUF_STRIP_PREFIX = ""protobuf-48cb18e5c419ddd23d9badcfe4e9df7bde1979b2""
 
     tf_http_archive(
         name = ""protobuf_archive"",
         sha256 = PROTOBUF_SHA256,
         strip_prefix = PROTOBUF_STRIP_PREFIX,
+        patch_file = clean_dep(""//third_party/protobuf:expose-protoc-path.patch""),
         system_build_file = clean_dep(""//third_party/systemlibs:protobuf.BUILD""),
         system_link_files = {
             ""//third_party/systemlibs:protobuf.bzl"": ""protobuf.bzl"",
diff --git a/third_party/protobuf/expose-protoc-path.patch b/third_party/protobuf/expose-protoc-path.patch
new file mode 100644
index 0000000000..8f3f0caff7
--- /dev/null
+++ b/third_party/protobuf/expose-protoc-path.patch
@@ -0,0 +1,24 @@
+diff --git a/protobuf.bzl b/protobuf.bzl
+index 78f19c62..325a952d 100644
+--- a/protobuf.bzl
++++ b/protobuf.bzl
+@@ -109,7 +109,7 @@ def _proto_gen_impl(ctx):
+         inputs=inputs,
+         outputs=ctx.outputs.outs,
+         arguments=args + import_flags + [s.path for s in srcs],
+         mnemonic=""ProtoCompile"",
+         use_default_shell_env=True,
+     )
+@@ -266,8 +266,8 @@ def internal_gen_well_known_protos_java(srcs):
+   Args:
+     srcs: the well known protos
+   """"""
+-  root = Label(""%s//protobuf_java"" % (REPOSITORY_NAME)).workspace_root
+-  pkg = PACKAGE_NAME + ""/"" if PACKAGE_NAME else """"
++  root = Label(""%s//protobuf_java"" % (native.repository_name())).workspace_root
++  pkg = PACKAGE_NAME + ""/"" if native.package_name() else """"
+   if root == """":
+     include = "" -I%ssrc "" % pkg
+   else:
```

We have another library that is dynamically linked against libprotobuf.so. When you load the other library and then TensorFlow, everything works as expected:
```python
Python 3.6.8 (default, Aug  9 2019, 04:47:37) 
[GCC 4.7.2] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import celfs
>>> import tensorflow as tf
2019-08-27 16:12:09.162189: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
WARNING: Logging before flag parsing goes to stderr.
W0827 16:12:11.169919 140361581864704 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.
>>> 
```

However, if I load TensorFlow and then the other library, I get the following error:
```python
Python 3.6.8 (default, Aug  9 2019, 04:47:37) 
[GCC 4.7.2] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2019-08-27 16:07:30.536598: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1
WARNING: Logging before flag parsing goes to stderr.
W0827 16:07:32.523208 139698008938240 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.
>>> import celfs
[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/descriptor_database.cc:58] File already exists in database: google/protobuf/descriptor.proto
[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/descriptor.cc:1358] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
Aborted
```

Should I be linking protobuf dynamically for TensorFlow as well?"
32014,TF 1.14 : assgin Variable in loss function can't update value ? ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from : binary
- TensorFlow version :  tensorflow-gpu 1.14.0
- Python version: python 3.7.1
- CUDA/cuDNN version: CUDA 10.1 cuDNN 7.5
- GPU model and memory: GTX 2060 6G

**Question**

I customize the loss function in tf. keras and use a variable to output part of the loss to metric. What puzzles me is that in normal custom metrics, direct assgin variables give the correct output, but in loss functions, I have to call the assgin operator to get the normal output.

**Describe the current behavior**
now code :
```python
import tensorflow as tf
import tensorflow.keras as k
import tensorflow.keras.layers as kl
import tensorflow.keras.metrics as km
from tensorflow.keras.datasets import fashion_mnist

tfcfg = tf.ConfigProto()
tfcfg.gpu_options.allow_growth = True
sess = tf.Session(config=tfcfg)
k.backend.set_session(sess)

(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
x_train = x_train.reshape((-1, 28, 28, 1))
x_test = x_test.reshape((-1, 28, 28, 1))
y_train = k.utils.to_categorical(y_train, 10)
y_test = k.utils.to_categorical(y_test, 10)


model = k.Sequential([
    kl.Conv2D(32, 3, 1, input_shape=[28, 28, 1]),
    kl.BatchNormalization(),
    kl.LeakyReLU(),
    kl.Conv2D(64, 3, 1),
    kl.BatchNormalization(),
    kl.LeakyReLU(),
    kl.Conv2D(128, 3, 1),
    kl.BatchNormalization(),
    kl.LeakyReLU(),
    kl.LeakyReLU(),
    kl.Flatten(),
    kl.Dense(512),
    kl.BatchNormalization(),
    kl.LeakyReLU(),
    kl.Dense(10)
])


class Metric_HIGH_COST(km.Metric):
    def __init__(self, name=None, dtype=None, **kwargs):
        super().__init__(name=name, dtype=dtype, **kwargs)
        self.ce = self.add_weight('ce', initializer=tf.zeros_initializer)

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.ce.assign(tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)))

    def result(self):
        return self.ce


class Metric_LOW_COST(km.Metric):
    def __init__(self, cross_entropy: tf.Variable, name='CE', dtype=None):
        """""" yolo landmark error metric

        Parameters
        ----------
        MeanMetricWrapper : [type]

        landmark_error : ResourceVariable
            a variable from yoloalign loss
        name : str, optional
            by default 'LE'
        dtype : [type], optional
            by default None
        """"""
        super().__init__(name=name)
        self.ce = cross_entropy

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.ce

    def result(self):
        return self.ce.read_value()


class Myloss(k.losses.Loss):
    def __init__(self, name=None):
        super().__init__(name=name)
        self.ce = tf.get_variable('ce', (), tf.float32, tf.zeros_initializer)  # type:tf.RefVariable

    def call(self, y_true, y_pred):
        ce_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred))

        # ! method 1 got zero output :
        self.ce.assign(ce_loss)
        return ce_loss

        # ! method 2 get correct output :
        # return ce_loss + 0 * self.ce.assign(ce_loss)


myloss = Myloss()
high_cost_metric = Metric_HIGH_COST('high_cost_ce')
low_cost_metric = Metric_LOW_COST(myloss.ce, 'low_cost_ce')

sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])

model.compile(k.optimizers.Adam(), [myloss], [high_cost_metric, low_cost_metric])

model.fit(x_train, y_train, 100, 10)

```

**output `low_cost_ce`  always `0`**: 

```sh
11700/60000 [====>.........................] - ETA: 24s - loss: 175.2779 - high_cost_ce: 67.9055 - low_cost_ce: 0.0000e+00
```

**Describe the expected behavior**

new code:
```python
import tensorflow as tf
import tensorflow.keras as k
import tensorflow.keras.layers as kl
import tensorflow.keras.metrics as km
from tensorflow.keras.datasets import fashion_mnist

tfcfg = tf.ConfigProto()
tfcfg.gpu_options.allow_growth = True
sess = tf.Session(config=tfcfg)
k.backend.set_session(sess)

(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
x_train = x_train.reshape((-1, 28, 28, 1))
x_test = x_test.reshape((-1, 28, 28, 1))
y_train = k.utils.to_categorical(y_train, 10)
y_test = k.utils.to_categorical(y_test, 10)


model = k.Sequential([
    kl.Conv2D(32, 3, 1, input_shape=[28, 28, 1]),
    kl.BatchNormalization(),
    kl.LeakyReLU(),
    kl.Conv2D(64, 3, 1),
    kl.BatchNormalization(),
    kl.LeakyReLU(),
    kl.Conv2D(128, 3, 1),
    kl.BatchNormalization(),
    kl.LeakyReLU(),
    kl.LeakyReLU(),
    kl.Flatten(),
    kl.Dense(512),
    kl.BatchNormalization(),
    kl.LeakyReLU(),
    kl.Dense(10)
])


class Metric_HIGH_COST(km.Metric):
    def __init__(self, name=None, dtype=None, **kwargs):
        super().__init__(name=name, dtype=dtype, **kwargs)
        self.ce = self.add_weight('ce', initializer=tf.zeros_initializer)

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.ce.assign(tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)))

    def result(self):
        return self.ce


class Metric_LOW_COST(km.Metric):
    def __init__(self, cross_entropy: tf.Variable, name='CE', dtype=None):
        """""" yolo landmark error metric

        Parameters
        ----------
        MeanMetricWrapper : [type]

        landmark_error : ResourceVariable
            a variable from yoloalign loss
        name : str, optional
            by default 'LE'
        dtype : [type], optional
            by default None
        """"""
        super().__init__(name=name)
        self.ce = cross_entropy

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.ce

    def result(self):
        return self.ce.read_value()


class Myloss(k.losses.Loss):
    def __init__(self, name=None):
        super().__init__(name=name)
        self.ce = tf.get_variable('ce', (), tf.float32, tf.zeros_initializer)  # type:tf.RefVariable

    def call(self, y_true, y_pred):
        ce_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred))

        # ! method 1 got zero output :
        # self.ce.assign(ce_loss)
        # return ce_loss

        # ! method 2 get correct output :
        return ce_loss + 0 * self.ce.assign(ce_loss)


myloss = Myloss()
high_cost_metric = Metric_HIGH_COST('high_cost_ce')
low_cost_metric = Metric_LOW_COST(myloss.ce, 'low_cost_ce')

sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])

model.compile(k.optimizers.Adam(), [myloss], [high_cost_metric, low_cost_metric])

model.fit(x_train, y_train, 100, 10)
```

```sh
11500/60000 [====>.........................] - ETA: 24s - loss: 172.6385 - high_cost_ce: 85.5574 - low_cost_ce: 171.8945     
```

#### Why do you have to call the assgin operator in the loss function to update variables?Is this a bug? 


"
32013,CustomOp segfaults using work sharder on MacOS with TF1.13.1,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): from pip
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.8
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
When running a simple custom-op on CPU adapted from the tutorial with multiple threads using work sharder it can find only 1 thread and segfaults.

**Describe the expected behavior**
Should find 12 threads and runs.

**Code to reproduce the issue**
minimal.cc
```c++
 #include <stdio.h>
#include <cfloat>

#include ""third_party/eigen3/unsupported/Eigen/CXX11/Tensor""
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/op_kernel.h""
#include ""tensorflow/core/framework/tensor_shape.h""

#include ""./work_sharder.h""

using namespace tensorflow;
typedef Eigen::ThreadPoolDevice CPUDevice;

REGISTER_OP(""Minimal"")
    .Input(""input: float"")
    .Output(""shared_arr: float"")
;

class MinimalOp : public OpKernel {
 public:
  explicit MinimalOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {

    const Tensor& input= context->input(0);
    auto input_flat = input.flat<float>();
    const int N = input_flat.size();

    // Create an output tensor of the right shape
    Tensor* shared_arr = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input.shape(),
                                                     &shared_arr));
    // This tensor is going to be shared among threads
    auto shared_arr_flat = shared_arr->flat<float>();

    // Shard function on ranges
    auto shard = [&input_flat, &shared_arr_flat]
                  (int64 start, int64 limit) {
        for (int i = 0; start < limit; i++) {
            if ((input_flat(i))<0.){
                shared_arr_flat(i) = 0.;
            }}};

    std::cout<<""Shard definition was okay\n"";
    const DeviceBase::CpuWorkerThreads& worker_threads = *(context->device()->tensorflow_cpu_worker_threads());
    std::cout<<""Number of workers = ""<<worker_threads.num_threads<<""\n"";
    const int64 shard_cost = N;
    Shard(worker_threads.num_threads, worker_threads.workers,
            N, shard_cost, shard);

  }};

REGISTER_KERNEL_BUILDER(Name(""Minimal"").Device(DEVICE_CPU), MinimalOp);
```
compiling commands:
```
TF_CFLAGS=( $(python3 -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') )
TF_LFLAGS=( $(python3 -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') )
g++ -std=c++11 -shared -undefined dynamic_lookup minimal.cc -o minimal.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2
```
Python script:
```python
import tensorflow as tf
import numpy as np
import time


minimal_module = tf.load_op_library(""./minimal.so"")
tf_minimal = minimal_module.minimal

input_tensor = tf.constant(np.random.normal(size=(100, 100)).astype(""float32""))
returned_tensor = tf_minimal(input_tensor)
sess = tf.Session()
sess.run(returned_tensor)
```
**Other info / logs**
https://stackoverflow.com/questions/57427277/tensorflow-customop-multiprocessing-not-working-for-cpu
Maybe related issue: 
https://github.com/tensorflow/tensorflow/issues/13308
gcc is clang: 
g++ --version
```
Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
Apple LLVM version 10.0.1 (clang-1001.0.46.3)
Target: x86_64-apple-darwin18.2.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin
```
"
32010,NotImplementedError in Distributed tensorflow with parameter_server,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0rc
- Python version: 3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:  cuda 10.0/ cudnn 7.6
- GPU model and memory: Tesla, 24gb per gpg

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Hi, i am using this script resnet_cifar_main.py in https://github.com/tensorflow/models/tree/master/official/vision/image_classification to test parameter distributed strategy. I have added some codes to configure cluster in this file as follows
```
import os
 import json
 tf.compat.v1.disable_eager_execution() # if not, it will raise error
os.environ[""TF_CONFIG""] = json.dumps({
      'cluster': {
        'ps' : [""100.102.32.179:8080""],
        'worker':  [""100.102.33.40:8080""]
                  },
         'task': # {'type':'ps', 'index':0} # for the ps sever
            {'type': 'worker', 'index': 0}
          })
```
for the ps server, i have done the similar things. 
i use the following command to run .
`python resnet_cifar_main.py --data_dir cifar-10-batches-bin/ --distribution_strategy parameter_server`.
It works when i set only one gpu avaiable. that is,
```
export CUDA_VISIBLE_DEVICES=0
```
However, when i set all gpu avaiable, it fails with the following error log.
```
/data1/wayneweixu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/data1/wayneweixu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/data1/wayneweixu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/data1/wayneweixu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/data1/wayneweixu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/data1/wayneweixu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2019-08-27 21:20:21.718509: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-08-27 21:20:21.771457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:04:00.0
2019-08-27 21:20:21.772876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:05:00.0
2019-08-27 21:20:21.774307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:08:00.0
2019-08-27 21:20:21.775831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:09:00.0
2019-08-27 21:20:21.777206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 4 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:84:00.0
2019-08-27 21:20:21.778613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 5 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:85:00.0
2019-08-27 21:20:21.780023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 6 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:88:00.0
2019-08-27 21:20:21.781431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 7 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:89:00.0
2019-08-27 21:20:21.781638: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-08-27 21:20:21.783058: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-08-27 21:20:21.784468: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-08-27 21:20:21.784759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-08-27 21:20:21.786481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-08-27 21:20:21.787803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-08-27 21:20:21.791692: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-08-27 21:20:21.813851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-08-27 21:20:21.814258: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-27 21:20:21.823038: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2399910000 Hz
2019-08-27 21:20:21.824902: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1b04056a20 executing computations on platform Host. Devices:
2019-08-27 21:20:21.824934: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-08-27 21:20:23.267705: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1b03884990 executing computations on platform CUDA. Devices:
2019-08-27 21:20:23.267737: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla M40 24GB, Compute Capability 5.2
2019-08-27 21:20:23.267744: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla M40 24GB, Compute Capability 5.2
2019-08-27 21:20:23.267749: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): Tesla M40 24GB, Compute Capability 5.2
2019-08-27 21:20:23.267755: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): Tesla M40 24GB, Compute Capability 5.2
2019-08-27 21:20:23.267760: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (4): Tesla M40 24GB, Compute Capability 5.2
2019-08-27 21:20:23.267766: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (5): Tesla M40 24GB, Compute Capability 5.2
2019-08-27 21:20:23.267771: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (6): Tesla M40 24GB, Compute Capability 5.2
2019-08-27 21:20:23.267776: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (7): Tesla M40 24GB, Compute Capability 5.2
2019-08-27 21:20:23.277272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:04:00.0
2019-08-27 21:20:23.278704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:05:00.0
2019-08-27 21:20:23.280111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:08:00.0
2019-08-27 21:20:23.281511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:09:00.0
2019-08-27 21:20:23.282918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 4 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:84:00.0
2019-08-27 21:20:23.284400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 5 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:85:00.0
2019-08-27 21:20:23.285821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 6 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:88:00.0
2019-08-27 21:20:23.287255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 7 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:89:00.0
2019-08-27 21:20:23.287327: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-08-27 21:20:23.287349: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-08-27 21:20:23.287374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-08-27 21:20:23.287398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-08-27 21:20:23.287416: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-08-27 21:20:23.287434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-08-27 21:20:23.287452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-08-27 21:20:23.309337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-08-27 21:20:23.309395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-08-27 21:20:23.321783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-27 21:20:23.321864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 2 3 4 5 6 7 
2019-08-27 21:20:23.321876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y Y Y N N N N 
2019-08-27 21:20:23.321885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N Y Y N N N N 
2019-08-27 21:20:23.321900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 2:   Y Y N Y N N N N 
2019-08-27 21:20:23.321908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 3:   Y Y Y N N N N N 
2019-08-27 21:20:23.321916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 4:   N N N N N Y Y Y 
2019-08-27 21:20:23.321930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 5:   N N N N Y N Y Y 
2019-08-27 21:20:23.321944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 6:   N N N N Y Y N Y 
2019-08-27 21:20:23.321959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 7:   N N N N Y Y Y N 
2019-08-27 21:20:23.337402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13843 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)
2019-08-27 21:20:23.339205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 16016 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)
2019-08-27 21:20:23.340928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 16166 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)
2019-08-27 21:20:23.342575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 15535 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)
2019-08-27 21:20:23.344296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 13420 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)
2019-08-27 21:20:23.345968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 16228 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)
2019-08-27 21:20:23.347619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 15985 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)
2019-08-27 21:20:23.349287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 16314 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)
I0827 21:20:23.353662 139753917540160 parameter_server_strategy.py:250] Multi-worker ParameterServerStrategy with cluster_spec = {'ps': ['100.102.32.179:8080'], 'worker': ['100.102.33.40:8080']}, task_type = 'worker', task_id = 0, num_ps_replicas = 1, is_chief = True, device_map = ReplicaDeviceMap(['/job:worker/replica:0/task:0/device:GPU:0', '/job:worker/replica:0/task:0/device:GPU:1', '/job:worker/replica:0/task:0/device:GPU:2', '/job:worker/replica:0/task:0/device:GPU:3', '/job:worker/replica:0/task:0/device:GPU:4', '/job:worker/replica:0/task:0/device:GPU:5', '/job:worker/replica:0/task:0/device:GPU:6', '/job:worker/replica:0/task:0/device:GPU:7']), variable_device = <bound method _ReplicaDeviceChooser.device_function of <tensorflow.python.training.device_setter._ReplicaDeviceChooser object at 0x7f1a966e84e0>>
Contrib missing: Skip remove monkey patch tf.contrib.distribute.*
W0827 21:20:23.734989 139753917540160 deprecation.py:323] From /data1/wayneweixu/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
W0827 21:20:23.736282 139753917540160 deprecation.py:323] From /data1/wayneweixu/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py:332: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
W0827 21:20:23.800738 139753917540160 deprecation.py:506] From /data1/wayneweixu/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Traceback (most recent call last):
  File ""resnet_cifar_main_dist_ps_remote_0.py"", line 260, in <module>
    absl_app.run(main)
  File ""/data1/wayneweixu/.local/lib/python3.6/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/data1/wayneweixu/.local/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""resnet_cifar_main_dist_ps_remote_0.py"", line 254, in main
    return run(flags.FLAGS)
  File ""resnet_cifar_main_dist_ps_remote_0.py"", line 173, in run
    model = resnet_cifar_model.resnet56(classes=cifar_preprocessing.NUM_CLASSES)
  File ""/data1/wayneweixu/models/official/vision/image_classification/resnet_cifar_model.py"", line 227, in resnet
    name='conv1')(x)
  File ""/data1/wayneweixu/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 777, in __call__
    self._maybe_build(inputs)
  File ""/data1/wayneweixu/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2099, in _maybe_build
    self.build(input_shapes)
  File ""/data1/wayneweixu/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/convolutional.py"", line 165, in build
    dtype=self.dtype)
  File ""/data1/wayneweixu/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2269, in __setattr__
    if val.trainable:
  File ""/data1/wayneweixu/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py"", line 476, in trainable
    raise NotImplementedError
NotImplementedError
```
how can i fix this error? thanks a lot.
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
32009,[C++] The SessionOptions doesn't control the usage of CPU ?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, only use C++ API
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): GCC 5.5
- CUDA/cuDNN version: only CPU
- GPU model and memory: only CPU

**Describe the current behavior**
I created a session option with inter=1 and intra=1. Then I used this option to create a session and load a model.
When I run the model, the program used almost all the CPUs in my server(the CPU usage is more than 5000%).
And No matter what value I set to ""inter"" and ""intra"", the CPU usage is more than 5000%.

**Describe the expected behavior**
When I set inter=1 and intra=1, I expect the usage of CPU is limited to <= 100%.

**Code to reproduce the issue**

    std::unique_ptr<tensorflow::Session> sess_;
    tensorflow::SessionOptions sess_options_;
    sess_options_.config.set_use_per_session_threads(false);
    sess_options_.config.set_intra_op_parallelism_threads(1);
    sess_options_.config.set_inter_op_parallelism_threads(1);
    sess_.reset(tensorflow::NewSession(sess_options_));
    tensorflow::GraphDef graph_def;
    auto default_env = tensorflow::Env::Default();
    tensorflow::ReadBinaryProto(default_env, model_path, &graph_def);
    sess_->Create(graph_def);
    run_model();

**Other info / logs**
When I use the python API of TensorFlow 1.14 do the same thing, it seems like the CPU usage can be controlled by the inter and intra parameter.

Why did this happen? What's the right way to control the computing resources used by tensorflow?
"
32007,.a library file is not working on android platform while using tf lite with c++ api.,"I am trying to make an android project using tensorflow lite, I want to use lite api in c++ level, however the examples in the repository are all using java api.

As the guidance tf lite can be applied in both java level and c++ level, but the .a library is not working. 

I built the libtensorflow-lite.a following the https://tensorflow.google.cn/lite/guide/build_arm64, but it seems this .a file is not working on android platform. Anyone can give me some advices about how to build a available .so library file for me to use c++ api in a android project?

Thanks a lot!
"
32006,Very unhelpful error msg building keras models with while loops,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution: Linux Ubuntu 16.04:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v1.12.1-9365-gff401a6 1.15.0-dev20190821
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0 / ??
- GPU model and memory: Quadro 2gb

**Describe the current behavior**
`_create_keras_history_helper` is making building networks much more pleasant in general by forgoing the need to wrap everything in `Lambda` layers. It's failing for while loops without `Lambda` wrapping, giving a very unhelpful error message. `tensorflow.python.framework.errors_impl.InvalidArgumentError: A cross-device loop must have a pivot predicate: while/while_context`

**Describe the expected behavior**
Indicate the source of the problem/possible resolution.

**Code to reproduce the issue**
```python
import tensorflow as tf

def cond(i, x):
    return tf.reduce_all(x < 10)

def body(i, x):
    return i + 1, x + i

x = tf.keras.layers.Input(shape=(), dtype=tf.float32)
inc = tf.while_loop(cond, body, [tf.constant(0, dtype=tf.float32), x])
# the following fixes things
# inc = tf.keras.layers.Lambda(lambda x: tf.while_loop(
#     cond, body, [tf.constant(0, dtype=tf.float32), x]))(x)

model = tf.keras.Model(inputs=x, outputs=inc)  # <- error occurs here
```

**Other info / logs**
Traceback:
```
Traceback (most recent call last):
  File ""loop.py"", line 23, in <module>
    model = tf.keras.Model(inputs=x, outputs=inc)  # <- error occurs here
  File "".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 147, in __init__
    super(Model, self).__init__(*args, **kwargs)
  File "".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 164, in __init__
    self._init_graph_network(*args, **kwargs)
  File "".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File "".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 267, in _init_graph_network
    base_layer_utils.create_keras_history(self._nested_outputs)
  File "".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py"", line 184, in create_keras_history
    _, created_layers = _create_keras_history_helper(tensors, set(), [])
  File "".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py"", line 231, in _create_keras_history_helper
    layer_inputs, processed_ops, created_layers)
  File "".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py"", line 231, in _create_keras_history_helper
    layer_inputs, processed_ops, created_layers)
  File "".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py"", line 229, in _create_keras_history_helper
    constants[i] = backend.function([], op_input)([])
  File "".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 3473, in __call__
    self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)
  File "".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 3410, in _make_callable
    callable_fn = session._make_callable_from_options(callable_opts)
  File "".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1505, in _make_callable_from_options
    return BaseSession._Callable(self, callable_options)
  File "".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1460, in __init__
    session._session, options_ptr)
tensorflow.python.framework.errors_impl.InvalidArgumentError: A cross-device loop must have a pivot predicate: while/while_context
```"
32004,Python/C++ API interpreter example for hybrid models,"I have a hybrid tflite model, e.g. was converted with the option

`converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]`

So it contains both tflite ops and normal ops. I test a lot, so this is quicker than implementing the missing parts myself. When trying to load the model with an interpreter, either with the Python or the C++ API, I get errors:

Python
```
RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you invoke the Flex delegate before inference.Node number 4 (Flex) failed to prepare.
```
C++
```
INFO: Initialized TensorFlow Lite runtime.
ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you invoke the Flex delegate before inference.
ERROR: Node number 4 (FlexSoftplus) failed to prepare.
```

It doesn't seem there are docs that cover how to treat this error and load such hybrid models correctly. If I have missed any docs by any chance, please share the link!"
32003,TF Serving version 1.10.0,"Hi all,

Can some one give me steps to build TF Serving version 1.10.0  on centos7 image"
32002,Tensorflow Lite compute library for hardware acceleration ,"Does Tensorflow lite has their own compute library for hardware(arm cpu, gpu, fpga, etc.) acceleration? If they have it, where is it located at? which directory? Thanks!"
32001,The title is the shared_embeddings module，but the document introduces shared_embedding_columns module,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/feature_column/shared_embeddings


## Description of issue (what needs changing):

The description in the document is a way to use the shared_embedding_columns module, but the title is the shared_embeddings module, and the shared_embedding_columns module has been removed in tensorflow2.0.
"
31999,SparseTensor stopped working on tf.keras when moving from 2.0.0-beta1 to 2.0.0-rc0,"I just moved from `2.0.0-beta1` to `2.0.0-rc0` and some code for handling sparse categorical variable stopped working for me.

Here is some minimal code to reproduce the issue.

```
import tensorflow as tf
import numpy as np

class SparseSlice(tf.keras.layers.Layer):
    def __init__(self, feature_column):
        super(SparseSlice, self).__init__()
        self.fc = feature_column

    def build(self, input_shape):

        self.kernel = self.add_weight('{}_kernel'.format(self.fc.name), shape=(self.fc.num_buckets, ), dtype=tf.float32)

    def call(self, input):
        ids = self.fc._transform_input_tensor(input)
        return tf.expand_dims(tf.gather(self.kernel, ids.values), axis=1)


batch_size = 10
c = 'smth'
col = tf.feature_column.categorical_column_with_hash_bucket(c, 10000, dtype=tf.int64)
example_spec = tf.feature_column.make_parse_example_spec([col])

inputs = tf.keras.layers.Input(name=c, shape=(None, ), batch_size=batch_size, sparse=True, dtype=tf.int64)
sparse_out = SparseSlice(col)(inputs)
output = tf.keras.layers.Dense(1, activation='sigmoid')(sparse_out)

model = tf.keras.Model(inputs, output)

model.compile(optimizer='adam',
              loss='mse')


features = {c: tf.sparse.SparseTensor(indices=[[i, 0] for i in range(batch_size)], values=np.random.randint(0, 1000, (batch_size, )).tolist(), dense_shape=(batch_size, 1))}
ys = tf.constant(np.random.rand(batch_size).tolist(), dtype=tf.float32)

dataset = tf.data.Dataset.from_tensor_slices((features, ys)).batch(batch_size)

model.fit(x=dataset,
          epochs=1
          )
```
on `2.0.0-rc0` I am getting the following error

```
ValueError: The two structures don't have the same nested structure.
First structure: type=SparseTensorSpec str=SparseTensorSpec(TensorShape([None, 1]), tf.int32)
Second structure: type=SparseTensor str=SparseTensor(indices=Tensor(""smth/indices:0"", shape=(None, 2), dtype=int64), values=Tensor(""smth/values:0"", shape=(None,), dtype=int64), dense_shape=Tensor(""smth/shape:0"", shape=(2,), dtype=int64))
More specifically: Incompatible CompositeTensor TypeSpecs: type=SparseTensorSpec str=SparseTensorSpec(TensorShape([None, 1]), tf.int32) vs. type=SparseTensorSpec str=SparseTensorSpec(TensorShape([None, None]), tf.int64)
Entire first structure:
.
Entire second structure:
.
```
Whereas everything runs fine in `2.0.0-beta1`"
31998,[TF 2.0] GRU layer doesn't work when called from tf.function,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Darwin Kernel Version 18.6.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
2.0.0-dev20190826
- Python version:
Python 3.6.8 :: Anaconda, Inc.
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A


**Describe the current behavior**

The code below raises error:

> Tried to convert 'tensor' to a tensor and failed. Error: None values not supported.

As noted in comments, the bug disappears if we don't use `tf.function`, or set `persistent=False` in gradient tape.

**Code to reproduce the issue**

```
import numpy as np
import tensorflow as tf

x = tf.cast(np.random.randn(1, 100), tf.float32)
y = tf.cast(np.random.randn(1, 100, 100), tf.float32)
z = tf.cast(np.random.randn(1, 100), tf.float32)

class Model(tf.keras.Model):
    def __init__(self):
        super(Model, self).__init__()
        self.layer = tf.keras.layers.GRU(100)

    @tf.function # remove this and it works fine
    def call(self, x, y):
        z = self.layer(y, initial_state=x)
        return z

model = Model()

with tf.GradientTape(persistent=True) as tape: # if persistent=False it works fine
    loss = tf.norm(model(x, y) - z)
grads = tape.gradient(loss, model.trainable_variables)
```"
31996,import tensorflow as tf issue ,"I tried many times, but still the same issue. 

```
ImportError                               Traceback (most recent call last)
~\Miniconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\Miniconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\Miniconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\Miniconda3\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

~\Miniconda3\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-2-64156d691fe5> in <module>
----> 1 import tensorflow as tf

~\Miniconda3\lib\site-packages\tensorflow\__init__.py in <module>
     26 
     27 # pylint: disable=g-bad-import-order
---> 28 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     29 from tensorflow.python.tools import module_util as _module_util
     30 

~\Miniconda3\lib\site-packages\tensorflow\python\__init__.py in <module>
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

~\Miniconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\Khalil\Miniconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Khalil\Miniconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Khalil\Miniconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Khalil\Miniconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Khalil\Miniconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
​"
31995,NotImplementedError: tf.GradientTape.gradients() does not support graph control flow operations like tf.cond or tf.while at this time. Use tf.gradients() instead.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
31994,Autotune seems not improve performance,"How autotune is implemented? it seems not improve performance, the dataset is still the bottleneck."
31991,Iterate on Unknown Batch Size with Custom Layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **OS X 10.14.6**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version (use command below): **1.14.0**
- Python version: **3.6.6**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

**Describe the current behavior**
I am attempting to build a custom TensorFlow Layer to perform K-Means clustering across channels of a given image. I am having difficulty creating this new layer to add to the model, as it seems that fundamentally, I don't have the ability to iterate over the batch size, which is unknown until runtime. I have tried a few alternatives such as the `@tf.function` function decorator and the `tf.scan` function, which have both been unsuccessful. 

**Describe the expected behavior**
I was expecting that since the batch size is unknown until runtime, that TensorFlow would be able to handle this error, similar to how TensorFlow can accept an unknown dimension and generate a matrix/tensor with the unknown shape.

**Code to reproduce the issue**

```
import numpy as np
import tensorflow as tf
from sklearn.cluster import KMeans


class KMeansLayer(tf.keras.layers.Layer):
    def __init__(self, num_clusters=8, n_init=5, trainable=False):
        super(KMeansLayer, self).__init__()
        self.clusters = num_clusters
        self.n_init = n_init
        self.trainable = trainable

    def build(self, input_shape):
        print('Input shape:', input_shape)
        self.output_s = (input_shape[0], input_shape[1], input_shape[2], 1)
        self.built = True

    def call(self, input):

        @tf.function
        def KMeansBase(input_mat, clusters, n_init):
            base_mat = tf.zeros((input_mat.shape[0], input_mat.shape[1] * input_mat.shape[2]))
            for frame in range(input_mat.shape[0]):
                init_mat = np.zeros((input_mat.shape[1] * input_mat.shape[2]))
                reshape_mat = tf.reshape(input_mat[frame], shape=(input_mat.shape[1] * input_mat.shape[2], input_mat.shape[3]))
                kmeans_init = KMeans(n_clusters=clusters, n_init=n_init)
                class_pred = kmeans_init.fit_predict(reshape_mat.numpy())

                for clust in range(clusters):
                    init_mat[class_pred == clust] = tf.keras.backend.mean(tf.boolean_mask(reshape_mat, class_pred == clust), axis=1).numpy()
                    init_mat[class_pred == clust] = np.mean(init_mat[class_pred == clust], axis=None)
                base_mat = tf.compat.v1.scatter_update(base_mat, frame, tf.convert_to_tensor(init_mat))
            base_mat = tf.reshape(base_mat, (input_mat.shape[0], input_mat.shape[1], input_mat.shape[2]))

            return tf.expand_dims(base_mat, axis=-1)

        return KMeansBase(input, clusters=self.clusters, n_init=self.n_init)



input_1 = tf.keras.Input(shape=(28, 28, 1), name='input_1', dtype='float32')
conv_1 = tf.keras.layers.Conv2D(filters=3, kernel_size=3, strides=1, padding='same', data_format='channels_last', activation='elu', kernel_initializer='glorot_uniform')(input_1)
kmeans_out = KMeansLayer(num_clusters=8, n_init=5)(conv_1)


model = tf.keras.Model(inputs=[input_1], outputs=kmeans_out)
tf.keras.utils.plot_model(model, show_shapes=True)
model.compile(optimizer='adam', loss='mse', metrics=['mse'])
```



The error that I get from running the above code is as follows:

```
Traceback (most recent call last):
  File ""example_error_file.py"", line 44, in <module>
    kmeans_out = KMeansLayer(num_clusters=8, n_init=5)(conv_1)
  File ""~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 634, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 149, in wrapper
    raise e.ag_error_metadata.to_exception(type(e))
TypeError: in converted code:

    example_error_file.py:38 call *
        return KMeansBase(input, clusters=self.clusters, n_init=self.n_init)
    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:414 __call__
        self._initialize(args, kwds, add_initializers_to=initializer_map)
    /var/folders/6p/0r05_kf55273nh_nftm5q9tw0000gn/T/tmp0zjq0l_w.py:14 KMeansBase *
        base_mat = ag__.converted_call('zeros', tf, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), ((input_mat.shape[0], input_mat.shape[1] * input_mat.shape[2]),), None)
    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1880 zeros
        shape = ops.convert_to_tensor(shape, dtype=dtypes.int32)
    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1087 convert_to_tensor
        return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1145 convert_to_tensor_v2
        as_ref=False)
    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1224 internal_convert_to_tensor
        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:305 _constant_tensor_conversion_function
        return constant(v, dtype=dtype, name=name)
    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:246 constant
        allow_broadcast=True)
    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:284 _constant_impl
        allow_broadcast=allow_broadcast))
    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py:467 make_tensor_proto
        nparray = np.array(values, dtype=np_dt)

    TypeError: __int__ returned non-int (type NoneType)
```


My main question is: can TensorFlow not handle iterating over an unknown batch size, or am I missing some functionality?

Thank you for the help!"
31987,[TF 2.0.0rc0] Installation on Travis CI,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Xenial 16.04 (xenial distribution on Travis CI)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): `pip`
- TensorFlow version: `2.0.0rc0`
- Python version: 3.6, 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: No
- GPU model and memory: No

Tox on Travis CI does not seem to be able to fetch Tensorflow 2.0.0rc0 using pip. Corresponding build is here : https://travis-ci.org/sicara/tf-explain/jobs/576955077
Everything is running fine locally. Any idea what is wrong with the build?"
31986,Tensorflow Golang Gpu Support,"Seems like golang port of tensorflow does not use my Gpu, however, strangely, they recognize my Gpu.

This is only message I've got.
```
StreamExecutor device(0): undefibed, undefined 
```

 I've installed all cuda and cudnn libraries and it works just fine in Python. What would be the problem?"
31985,Mixed precision training with Keras in r1.13,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7 (Core)
- TensorFlow version (use command below): 1.13.0
- Python version: 3.6.8

There seems to be no obvious way to have mixed precision training using keras layers
in r1.13. 

1. The approach based on setting a custom getter doesn't work, e.g.,

`with tf.variable_scope(custom_getter=custom_getter):
    layer = tf.keras.layers.Dense(...)`

does not invoke the `custom_getter`.

2. The mixed precision policy is available only starting r1.14, i.e.,

`self.policy = tf.keras.mixed_precision.experimental.Policy('infer_float32_vars')` 

results in 

`AttributeError: module 'tensorflow.keras' has no attribute 'mixed_precision'`

Is there a way to have mixed precision for keras layers in r1.13?



"
31984,[TF 2.0.0rc0] Error when using cuDNN GRU,"Upgrading from pip package `tensorflow-gpu==2.0.0-beta1` to `tensorflow-gpu=2.0.0-rc0` the cuDNN GRU op no longer seems to work:

```
import tensorflow as tf

print(tf.__version__)

print(tf.keras.layers.GRU(10)(tf.random.uniform((1,1,1))))
```
Gives expected output for `beta1`:
```
2.0.0-beta1
tf.Tensor(
[[ 0.14719923 -0.10625355 -0.01931523 -0.01283051 -0.11508528 -0.05038062
  -0.04895313 -0.03125525  0.12742375 -0.06588683]], shape=(1, 10), dtype=float32)
```
versus this for `rc0`
```
2.0.0-rc0
---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
<ipython-input-1-5455006264d0> in <module>
      3 print(tf.__version__)
      4 
----> 5 print(tf.keras.layers.GRU(10)(tf.random.uniform((1,1,1))))

~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)
    621 
    622     if initial_state is None and constants is None:
--> 623       return super(RNN, self).__call__(inputs, **kwargs)
    624 
    625     # If any of `initial_state` or `constants` are specified and are Keras

~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    849           with base_layer_utils.autocast_context_manager(
    850               self._compute_dtype):
--> 851             outputs = self.call(cast_inputs, *args, **kwargs)
    852           self._handle_activity_regularization(inputs, outputs)
    853           self._set_mask_metadata(inputs, outputs, input_masks)

~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py in call(self, inputs, mask, training, initial_state)
    342     else:
    343       last_output, outputs, runtime, states = self._defun_gru_call(
--> 344           inputs, initial_state, training, mask)
    345 
    346     if self.stateful:

~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py in _defun_gru_call(self, inputs, initial_state, training, mask)
    396       # Under eager context, check the device placement and prefer the
    397       if can_use_gpu:
--> 398         last_output, outputs, new_h, runtime = cudnn_gru(**cudnn_gru_kwargs)
    399       else:
    400         last_output, outputs, new_h, runtime = standard_gru(**normal_gru_kwargs)

~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py in cudnn_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major, go_backwards)
    536     outputs, h, _, _ = gen_cudnn_rnn_ops.cudnn_rnn(
    537         inputs, input_h=init_h, input_c=0, params=params, is_training=True,
--> 538         rnn_mode='gru')
    539 
    540   last_output = outputs[-1]

~/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py in cudnn_rnn(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)
    107             input_mode=input_mode, direction=direction, dropout=dropout,
    108             seed=seed, seed2=seed2, is_training=is_training, name=name,
--> 109             ctx=_ctx)
    110       except _core._SymbolicException:
    111         pass  # Add nodes to the TensorFlow graph.

~/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py in cudnn_rnn_eager_fallback(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name, ctx)
    196   ""is_training"", is_training)
    197   _result = _execute.execute(b""CudnnRNN"", 4, inputs=_inputs_flat,
--> 198                              attrs=_attrs, ctx=_ctx, name=name)
    199   _execute.record_gradient(
    200       ""CudnnRNN"", _inputs_flat, _attrs, _result, name)

~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

~/.local/lib/python3.6/site-packages/six.py in raise_from(value, from_value)

UnknownError: Fail to find the dnn implementation. [Op:CudnnRNN]
```"
31983,[TF 2.0.0rc0] dependency on functools32 prevents python3.7 installation,"Somewhy poetry thinks, that `tensorflow-gpu = ""2.0.0-rc0""` depends on functools32 on python3.7 environment, because of what `poetry install / update` fails.

Found [this](https://github.com/tensorflow/tensorflow/issues/31767) issue, but it did not help because `tensorflow-gpu = ""2.0.0-beta1""` works greate with poetry.
"
31982,Can't build 1.14.0 on Nvidia Xavier AGX,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04, 
- Device : Nvidia Xavier AGX
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0.326-1
- GPU model and memory: GV10B, 16GB shared

The build process can't start out of the box. Problem is missing instructions for the toolchain.
The following two patches are needed to be able to build:
```
--- third_party/toolchains/cpus/arm/BUILD~      2019-06-19 00:48:23.000000000 +0200
+++ third_party/toolchains/cpus/arm/BUILD       2019-08-26 16:49:33.597480129 +0200
@@ -13,6 +13,7 @@
         ""k8"": "":cc-compiler-local"",
         ""piii"": "":cc-compiler-local"",
         ""arm"": "":cc-compiler-local"",
+        ""aarch64"": "":cc-compiler-local"",
         ""s390x"": "":cc-compiler-local"",
     },
 )
```
and
```
--- third_party/gpus/crosstool/BUILD.tpl~       2019-06-19 00:48:23.000000000 +0200
+++ third_party/gpus/crosstool/BUILD.tpl        2019-08-26 16:50:25.987292816 +0200
@@ -29,6 +29,7 @@
         ""x64_windows|msvc-cl"": "":cc-compiler-windows"",
         ""x64_windows"": "":cc-compiler-windows"",
         ""arm"": "":cc-compiler-local"",
+        ""aarch64"": "":cc-compiler-local"",
         ""k8"": "":cc-compiler-local"",
         ""piii"": "":cc-compiler-local"",
         ""ppc"": "":cc-compiler-local"",
```
It would be nice if these get incorporated into the mainline. Probably related to #22629"
31977,"what's the mean of ""? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------"" in tflite","When I using tensorflow-lite in Android, after change the model officially supported, an error happened but I can't solve it because I didn't find some guide on Internet about tensorflow-lite with details.

The input of my model has two inputs, 32*128*128*1 respectively, and I mimic the way to feed the inputs but failed

Here is some code in my project:
```
@Override
  protected void processImage() {
    try {
      float[][][][] stft=readTxt(""file:///android_asset/stft.txt"");
      float[][][][] mfcc=readTxt(""file:///android_asset/mfcc.txt"");
      final List<Classifier.Recognition> results = detector.recognizeImage(stft, mfcc);

    }
    catch(Exception e)
    {
      e.printStackTrace();
      System.out.print(""can't get Bitmap"");
    }


  }

  @Override
  public List<Recognition> recognizeImage(final float[][][][] stft,final float[][][][] mfcc) {
    // Log this method so that it can be analyzed with systrace.
    Trace.beginSection(""recognizeImage"");

    Trace.beginSection(""preprocessBitmap"");
    // Preprocess the image data from 0-255 int to normalized float based
    // on the provided parameters.
    LOGGER.i(""begin run model"");
    Trace.endSection(); // preprocessBitmap

    // Copy the input data into TensorFlow.
    Trace.beginSection(""feed"");
    imgDatam.rewind();
    imgDatas.rewind();
    for(int i=0; i<32; i++){
      for(int j=0;j<128;j++){
        for(int k =0;k<128;k++){
          imgDatam.putFloat(mfcc[i][j][k][0]);
          imgDatas.putFloat(stft[i][j][k][0]);
        }

      }

    }

    outputClasses = new float[32][NUM_DETECTIONS];

    Object[] inputArray = {imgDatas,imgDatam};

    Map<Integer, Object> outputMap = new HashMap<>();

    outputMap.put(0, outputClasses);

    Trace.endSection();

    // Run the inference call.
    Trace.beginSection(""run"");
    try{
      tfLite.runForMultipleInputsOutputs(inputArray, outputMap);
    }
    catch(Exception e)
    {
      e.printStackTrace();
    }
    LOGGER.i(""finished"");
    Trace.endSection();

    // Show the best detections.
    // after scaling them back to the input size.
    final ArrayList<Recognition> recognitions = new ArrayList<>(NUM_DETECTIONS);

    Trace.endSection(); // ""recognizeImage""
    return recognitions;
  }
```
And the error shows:
```
2019-08-27 15:53:41.636 13951-13951/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------
```
Thanks for your help anyway"
31976,[TF2.0] (rc0) Mocking tf.summary.image,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): `pip install tensorflow==2.0.0rc0` 
- TensorFlow version (use command below):
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CPU version
- GPU model and memory: CPU version

**Problem**

I have been using successfully `unittest.mock.patch` on `tf.summary.image` with version `2.0.0-beta1`. When upgrading to `2.0.0rc0`, the mock doesn't appear anymore and the full function is used. I couldn't find the change that breaks this. Any idea how to perform the mock?

**Sample code**

The following code is a sample of `tf.summary.image` call inside a callback, and running a simple `.fit`. It runs perfectly on `2.0.0-beta1`, but fails on `2.0.0rc0`.

To reproduce, have those 2 snippets (`tf_write_summary.py` and `test_tf_write_summary.py`) in a folder and run `python -m pytest test_tf_write_summary.py`.

**`tf_write_summary.py`**
```python
from datetime import datetime
from pathlib import Path

import tensorflow as tf
from tensorflow.keras.callbacks import Callback

class TestCallback(Callback):
    def __init__(
        self,
        images,
        output_dir=Path(""./logs/activations_visualizations""),
    ):
        super(TestCallback, self).__init__()
        self.images = images
        self.output_dir = Path(output_dir) / datetime.now().strftime(""%Y%m%d-%H%M%S.%f"")
        Path.mkdir(Path(self.output_dir), parents=True, exist_ok=True)

        self.file_writer = tf.summary.create_file_writer(str(self.output_dir))

    def on_epoch_end(self, epoch, logs=None):
        with self.file_writer.as_default():
            # tf.summary.image should be a MagicMock object
            tf.summary.image(
                ""Test Callback"",
                self.images,
                step=epoch,
            )
```

**`test_tf_write_summary.py`**
```python
import numpy as np
import tensorflow as tf

import tf_write_summary


def test_callback(mocker):
    mock_image_summary = mocker.patch('tf_write_summary.tf.summary.image')
    fake_image_values = [0]
    callback = tf_write_summary.TestCallback(fake_image_values)

    model = tf.keras.Sequential(
        [
            tf.keras.layers.Conv2D(
                16,
                (3, 3),
                activation=None,
                name=""conv_1"",
                input_shape=(28, 28, 3),
            ),
            tf.keras.layers.ReLU(name=""activation_1""),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(2, activation=""softmax""),
        ]
    )

    model.compile(optimizer=""adam"", loss=""categorical_crossentropy"")

    y = np.array([1, 0] * 4).reshape((4, 2))
    model.fit(np.random.random((4, 28, 28, 3)), y, epochs=1, batch_size=2, callbacks=[callback])

    mock_image_summary.assert_called_once_with(""Test Callback"", [0], step=0)
```


Thanks for your time!"
31975,Prediction speed worse than community keras due to training related code running during inference,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04.2 (but we believe this to be irrelevant)
- TensorFlow installed from (source or binary): pip (inside a docker image)
- TensorFlow version (use command below): 1.14 (likely affects 1.12 - current 2.0 beta)
- Python version: 3.6.8 (system python, we believe irrelevant)
- CUDA/cuDNN version: 10.0.130 (but we believe irrelevant)
- GPU model and memory: 1080 Ti

**Describe the current behavior**
Calling model.predict() includes calls to various non-prediction related things, in order of decreasing severity:
* reset_metrics
* get_progbar
* standardize_user_data
* validate_or_infer_batch_size

In our project, this results in a 2X prediction speed regression:
35 ms per call for community keras
70 ms per call for tensorflow keras

A Snakeviz flamegraph for prediction of our network in Tensorflow Keras, showing unnecessary overhead:

![Tensorflow Keras](https://user-images.githubusercontent.com/190617/63684638-ab884a80-c7fd-11e9-9dbe-5404eda2d70c.png)

**Describe the expected behavior**
Only do things necessary for prediction while doing prediction.

A Snakeviz flamegraph for prediction of our network in Community Keras, showing no unnecessary overhead:

![Community Keras](https://user-images.githubusercontent.com/190617/63684637-ab884a80-c7fd-11e9-810d-6341eba99070.png)

**Code to reproduce the issue**
This code is active for all calls to predict().

This is for a commercial project we are trying to migrate from community keras to tensorflow keras. It is a speed critical real-time robotics application where the 30ms is enough to miss our hard deadlines. If predict() is intended for production inference, we feel this should be resolved in tensorflow keras. If it is only intended for use during training, this should be noted in the mainline documentation, along with a suggestion for what ~is intended for production inference.

As a temporary workaround, we disabled some of these function calls in a local tensorflow fork, and the performance regression went away as expected. We took care to make sure this is not a profiling glitch. We excluded warmup time from the profile, and average over a number of samples.
"
31974,LD_LIBRARY_PATH is set but library files can not be found,"I have installed tensorflow via `pip install tensorflow-gpu` and I also have copied the label image on my local directory. When I run it, I see some shared library error related to libcu* but such files actually exist and LD_LIBRARY_PATH confirms that.
```
$ ls
BUILD  data  hot.nvvp  label_image.py  main.cc  README.md
$ python label_image.py
WARNING: Logging before flag parsing goes to stderr.
W0826 05:27:49.600733 140369765574464 deprecation_wrapper.py:119] From label_image.py:28: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.

W0826 05:27:50.118398 140369765574464 deprecation_wrapper.py:119] From label_image.py:45: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

W0826 05:27:50.122489 140369765574464 deprecation_wrapper.py:119] From label_image.py:59: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

2019-08-26 05:27:50.126417: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-08-26 05:27:50.130951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-26 05:27:50.131830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:03:00.0
2019-08-26 05:27:50.131924: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:
2019-08-26 05:27:50.131985: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:
2019-08-26 05:27:50.132043: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:
2019-08-26 05:27:50.132100: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:
2019-08-26 05:27:50.132157: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:
2019-08-26 05:27:50.132214: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:
2019-08-26 05:27:50.135607: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-26 05:27:50.135639: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
2019-08-26 05:27:50.228821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-26 05:27:50.229782: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55625c9d47c0 executing computations on platform CUDA. Devices:
2019-08-26 05:27:50.229803: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-08-26 05:27:50.249987: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3591645000 Hz
2019-08-26 05:27:50.250225: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55625c8a63f0 executing computations on platform Host. Devices:
2019-08-26 05:27:50.250242: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-08-26 05:27:50.250308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-26 05:27:50.250320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]
2019-08-26 05:27:50.256827: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-08-26 05:27:50.264294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-26 05:27:50.264335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]
W0826 05:27:52.117340 140369765574464 deprecation_wrapper.py:119] From label_image.py:69: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

military uniform 0.8343058
mortarboard 0.021869581
academic gown 0.010358071
pickelhaube 0.008008132
bulletproof vest 0.005350866
```


Please note messages like 

```
2019-08-26 05:27:50.131924: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] 
Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot 
open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:
```

library path is set correctly

```
$ ls -l /usr/local/cuda-10.1/lib64/libcurand*
lrwxrwxrwx 1 root root       15 Aug 22 02:49 /usr/local/cuda-10.1/lib64/libcurand.so -> libcurand.so.10
lrwxrwxrwx 1 root root       21 Aug 22 02:49 /usr/local/cuda-10.1/lib64/libcurand.so.10 -> libcurand.so.10.1.168
-rwxr-xr-x 1 root root 59812280 Aug 22 02:49 /usr/local/cuda-10.1/lib64/libcurand.so.10.1.168
-rw-r--r-- 1 root root 59842274 Aug 22 02:49 /usr/local/cuda-10.1/lib64/libcurand_static.a
$ echo $LD_LIBRARY_PATH
/usr/local/cuda-10.1/lib64:
```

Any more steps should I take?"
31973,[TF 2.0.0-rc0] Cannot find any 2.0.0 RC0 API references.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 and macOS
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0-rc0 (both GPU and CPU versions)
- Python version: 3.6.8

**Describe the current behavior**
Not a single reference to members of tensorflow can be found in IDE(PyCharm). 
However, programs run as expected.

**Describe the expected behavior**
References to members of tensorflow can be found.

<img width=""602"" alt=""Screen Shot 2019-08-26 at 17 13 30"" src=""https://user-images.githubusercontent.com/6904036/63680285-e1412980-c825-11e9-89c5-8f8abc376275.png"">

"
31972,`tf.load_op_library` contains no custom ops,"I face the same problem with that in https://github.com/tensorflow/tensorflow/issues/27307. However, the author did not give more details how he solved this problem. Is there anyone who has successfully solved this problem?"
31971,vs2015: c++ dependencies,"I already build a tensorflow1.13r c++ lib with bazel0.21.0 .I found a problem when I try to create the VS project.
Severity	Code	Description	Project	File	Line	Suppression State
Error	LNK2019	unresolved external symbol ""char const * __cdecl tensorflow::core::GetVarint32PtrFallback(char const *,char const *,unsigned int *)"" (?GetVarint32PtrFallback@core@tensorflow@@YAPEBDPEBD0PEAI@Z) referenced in function ""char const * __cdecl tensorflow::core::GetVarint32Ptr(char const *,char const *,unsigned int *)"" (?GetVarint32Ptr@core@tensorflow@@YAPEBDPEBD0PEAI@Z)	tensorflowc++test	E:\c++\tensorflowc++test\tensorflowc++test\t1.obj	1	

I know it meanings I haven't finishied the dependencies.But I don't know which .lib file should be includ. There are too many .lib files in bazel-out floder.
![cut](https://user-images.githubusercontent.com/23164484/63672466-979c1300-c814-11e9-9237-6ac7246f6814.png)
"
31970,ERROR: NotFoundError - File under path not being found.,"I am trying to train a whole model based on the COCO dataset using this scripts provided but reducing the number of classes to only 6. 

I run the `download_and_preprocess_coco.sh` script which downloads the dataset and calls the `create_coco_tf_record.py` script which creates the TFRecords from the dataset previously downloaded. After that steps (successfully achieved) I try to run the `retrain_detection_model.sh` as it is described in the tutorial, but modifying the labels .pdtxt file in order to take into account only 6 clases and modifying the `pipeline.config` file in order to achieve the same (with a v2 net and training the whole model option).

The first error that came out was:

`RuntimeError: Did not find any input files matching the glob pattern [u'/tensorflow/models/research/tmp/mscoco/coco_train.record-00001-of-00010']`

When I do have a file under: `/tensorflow/models/research/tmp/mscoco/` which contains files of the following format:
```
coco_testdev.record-00000-of-00100
coco_train.record-00024-of-00100
coco_val.record-00001-of-00010
```
Being the first set of 5 numbers after the record part numbers that go from 00000 to 00099.

So I do have those files that the error reports I do not have, and I have the PATH specified in the pipeline.config file.

I managed to move on a bit by skipping the use of the `glob` library in the `dataset_builder.py` script under the route `research/object_detection/builders/`. It is not working as it should, so by just removing the use of it the script runs a bit ahead, but it still throws and error:

```
NotFoundError (see above for traceback): /tensorflow/models/research/tmp/mscoco/coco_train.record-00001-of-00010; No such file or directory
         [[node IteratorGetNext (defined at object_detection/model_main.py:105)  = IteratorGetNext[output_shapes=[[128], [128,300,300,3], [128,2], [128,3], [128,100], [128,100,4], [128,100,2], [128,100,2], [128,100], [128,100], [128,100], [128]],
 output_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_BOOL, DT_FLOAT, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](IteratorV2)]]
```

I have not figured out how to move on from here.


I paste my `pipeline.config` file:

```
model {
  ssd {
    num_classes: 2
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
    }
    feature_extractor {
      type: ""ssd_mobilenet_v2""
      depth_multiplier: 1.0
      min_depth: 16
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 3.99999989895e-05
          }
        }
        initializer {
          random_normal_initializer {
            mean: 0.0
            stddev: 0.00999999977648
          }
        }
        activation: RELU_6
        batch_norm {
          decay: 0.97000002861
          center: true
          scale: true
          epsilon: 0.0010000000475
        }
      }
      override_base_feature_extractor_hyperparams: true
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 3.99999989895e-05
            }
          }
          initializer {
            random_normal_initializer {
              mean: 0.0
              stddev: 0.00999999977648
            }
          }
          activation: RELU_6
          batch_norm {
            decay: 0.97000002861
            center: true
            scale: true
            epsilon: 0.0010000000475
          }
        }
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.800000011921
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
        class_prediction_bias_init: -4.59999990463
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.20000000298
        max_scale: 0.949999988079
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.333299994469
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 0.300000011921
        iou_threshold: 0.600000023842
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid_focal {
          gamma: 2.0
          alpha: 0.75
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    encode_background_as_zeros: true
    normalize_loc_loss_by_codesize: true
    inplace_batchnorm_update: true
    freeze_batchnorm: false
  }
}
train_config {
  batch_size: 128
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
  sync_replicas: true
  optimizer {
    momentum_optimizer {
      learning_rate {
        cosine_decay_learning_rate {
          learning_rate_base: 0.20000000298
          total_steps: 50000
          warmup_learning_rate: 0.0599999986589
          warmup_steps: 2000
        }
      }
      momentum_optimizer_value: 0.899999976158
    }
    use_moving_average: false
  }
  fine_tune_checkpoint: ""/tensorflow/models/research/learn_human_car/ckpt/model.ckpt""
  from_detection_checkpoint: true
  load_all_detection_checkpoint_vars: true
  num_steps: 50000
  startup_delay_steps: 0.0
  replicas_to_aggregate: 8
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
}
train_input_reader {
  label_map_path: ""/tensorflow/models/research/object_detection/data/mscoco_label_map.pbtxt""
  tf_record_input_reader {
    input_path: ""/tensorflow/models/research/tmp/mscoco/coco_train.record-00001-of-00010""
  }
}
eval_config {
  num_examples: 8000
  metrics_set: ""coco_detection_metrics""
  use_moving_averages: false
}
eval_input_reader {
  label_map_path: ""/tensorflow/models/research/object_detection/data/mscoco_label_map.pbtxt""
  shuffle: false
  num_readers: 1
  tf_record_input_reader {
    input_path: ""/tensorflow/models/research/tmp/mscoco/coco_val.record-?????-of-00010""
  }
}
graph_rewriter {
  quantization {
    delay: 48000
    weight_bits: 8
    activation_bits: 8
  }
}
```
"
31969,Distributed Tensorflow,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 

example script provided by Tensorflow https://github.com/tensorflow/models/tree/master/official/vision/image_classification

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): tensorflow-gpu 1.14.0
- Python version: python 3.6.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 10.0
- GPU model and memory: Tesla M40 , each with 24GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Hi, i am using this script `resnet_cifar_main.py` in https://github.com/tensorflow/models/tree/master/official/vision/image_classification to test multi worker distributed strategy. I have added some codes to configure cluster in this file as follows
```
import os
import json
os.environ[""TF_CONFIG""] = json.dumps({
      'cluster': {
        'worker': # [""username@100.102.33.44:8080"", ""username@100.102.32.179:8080""]
                   [""username@100.102.32.179:8080"", ""username@100.102.33.40:8080""]
                  },
          'task': {'type': 'worker', 'index': 0}
          })
```
both machines can login from each other without authentication using ssh. I use `python resnet_cifar_main.py --data_dir cifar-10-batches-bin/ --distribution_strategy multi_worker_mirrored --num_gpus 8` to run it.
However,  the code stalls. Here is the information.
```
/data1/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/data1/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/data1/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/data1/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/data1/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/data1/username/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/data1/username/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/data1/username/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/data1/username/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/data1/username/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/data1/username/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/data1/username/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
W0826 11:51:16.547584 140208701863744 deprecation_wrapper.py:119] From /data1/username/models/official/utils/misc/keras_utils.py:154: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-08-26 11:51:16.563363: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-08-26 11:51:16.616212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:04:00.0
2019-08-26 11:51:16.617656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:05:00.0
2019-08-26 11:51:16.619078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:08:00.0
2019-08-26 11:51:16.620503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:09:00.0
2019-08-26 11:51:16.621998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:84:00.0
2019-08-26 11:51:16.623482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:85:00.0
2019-08-26 11:51:16.624936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:88:00.0
2019-08-26 11:51:16.626392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:89:00.0
2019-08-26 11:51:16.626574: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-26 11:51:16.627793: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-26 11:51:16.629061: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-26 11:51:16.629329: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-26 11:51:16.630910: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-26 11:51:16.632135: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-26 11:51:16.635787: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-26 11:51:16.658649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-08-26 11:51:16.659053: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-26 11:51:18.127150: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f84e4413620 executing computations on platform CUDA. Devices:
2019-08-26 11:51:18.127186: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla M40 24GB, Compute Capability 5.2
2019-08-26 11:51:18.127194: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla M40 24GB, Compute Capability 5.2
2019-08-26 11:51:18.127200: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): Tesla M40 24GB, Compute Capability 5.2
2019-08-26 11:51:18.127205: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): Tesla M40 24GB, Compute Capability 5.2
2019-08-26 11:51:18.127210: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (4): Tesla M40 24GB, Compute Capability 5.2
2019-08-26 11:51:18.127215: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (5): Tesla M40 24GB, Compute Capability 5.2
2019-08-26 11:51:18.127221: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (6): Tesla M40 24GB, Compute Capability 5.2
2019-08-26 11:51:18.127227: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (7): Tesla M40 24GB, Compute Capability 5.2
2019-08-26 11:51:18.133237: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400090000 Hz
2019-08-26 11:51:18.135546: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f84e6c0fe90 executing computations on platform Host. Devices:
2019-08-26 11:51:18.135571: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-08-26 11:51:18.142054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:04:00.0
2019-08-26 11:51:18.143568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:05:00.0
2019-08-26 11:51:18.145001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:08:00.0
2019-08-26 11:51:18.146484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:09:00.0
2019-08-26 11:51:18.147938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:84:00.0
2019-08-26 11:51:18.149383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:85:00.0
2019-08-26 11:51:18.150933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:88:00.0
2019-08-26 11:51:18.152413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:89:00.0
2019-08-26 11:51:18.152461: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-26 11:51:18.152480: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-26 11:51:18.152498: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-26 11:51:18.152515: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-26 11:51:18.152532: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-26 11:51:18.152548: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-26 11:51:18.152566: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-26 11:51:18.175302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-08-26 11:51:18.175343: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-26 11:51:18.188053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-26 11:51:18.188075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 
2019-08-26 11:51:18.188101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N 
2019-08-26 11:51:18.188112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N 
2019-08-26 11:51:18.188119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N 
2019-08-26 11:51:18.188126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N 
2019-08-26 11:51:18.188140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y 
2019-08-26 11:51:18.188147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y 
2019-08-26 11:51:18.188154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y 
2019-08-26 11:51:18.188161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N 
2019-08-26 11:51:18.203869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)
2019-08-26 11:51:18.205583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)
2019-08-26 11:51:18.207222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)
2019-08-26 11:51:18.208873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)
2019-08-26 11:51:18.210517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)
2019-08-26 11:51:18.212232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)
2019-08-26 11:51:18.213911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)
2019-08-26 11:51:18.215580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)
W0826 11:51:18.218294 140208701863744 deprecation_wrapper.py:119] From /data1/username/models/official/utils/misc/keras_utils.py:155: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.

2019-08-26 11:51:18.221324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:04:00.0
2019-08-26 11:51:18.222757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:05:00.0
2019-08-26 11:51:18.224187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:08:00.0
2019-08-26 11:51:18.225622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:09:00.0
2019-08-26 11:51:18.227063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:84:00.0
2019-08-26 11:51:18.228503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:85:00.0
2019-08-26 11:51:18.229937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:88:00.0
2019-08-26 11:51:18.231370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:89:00.0
2019-08-26 11:51:18.231399: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-26 11:51:18.231418: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-26 11:51:18.231434: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-26 11:51:18.231450: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-26 11:51:18.231466: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-26 11:51:18.231483: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-26 11:51:18.231499: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-26 11:51:18.254277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-08-26 11:51:18.254618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-26 11:51:18.254634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 
2019-08-26 11:51:18.254652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N 
2019-08-26 11:51:18.254666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N 
2019-08-26 11:51:18.254673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N 
2019-08-26 11:51:18.254680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N 
2019-08-26 11:51:18.254693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y 
2019-08-26 11:51:18.254700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y 
2019-08-26 11:51:18.254707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y 
2019-08-26 11:51:18.254721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N 
2019-08-26 11:51:18.270274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)
2019-08-26 11:51:18.271763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)
2019-08-26 11:51:18.273259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)
2019-08-26 11:51:18.274711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)
2019-08-26 11:51:18.276196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)
2019-08-26 11:51:18.277656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)
2019-08-26 11:51:18.279127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)
2019-08-26 11:51:18.280830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)
2019-08-26 11:51:18.292016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:04:00.0
2019-08-26 11:51:18.293441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:05:00.0
2019-08-26 11:51:18.294855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:08:00.0
2019-08-26 11:51:18.296320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:09:00.0
2019-08-26 11:51:18.297760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:84:00.0
2019-08-26 11:51:18.299191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:85:00.0
2019-08-26 11:51:18.300627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:88:00.0
2019-08-26 11:51:18.302090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:89:00.0
2019-08-26 11:51:18.302118: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-26 11:51:18.302137: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-26 11:51:18.302153: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-26 11:51:18.302169: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-26 11:51:18.302185: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-26 11:51:18.302201: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-26 11:51:18.302217: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-26 11:51:18.324811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-08-26 11:51:18.325169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-26 11:51:18.325185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 
2019-08-26 11:51:18.325203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N 
2019-08-26 11:51:18.325211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N 
2019-08-26 11:51:18.325224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N 
2019-08-26 11:51:18.325232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N 
2019-08-26 11:51:18.325238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y 
2019-08-26 11:51:18.325252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y 
2019-08-26 11:51:18.325259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y 
2019-08-26 11:51:18.325266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N 
2019-08-26 11:51:18.341278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)
2019-08-26 11:51:18.342725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)
2019-08-26 11:51:18.344171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)
2019-08-26 11:51:18.345625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)
2019-08-26 11:51:18.347083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)
2019-08-26 11:51:18.348544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)
2019-08-26 11:51:18.349997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)
2019-08-26 11:51:18.351556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)
I0826 11:51:18.352102 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0
I0826 11:51:18.352772 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:0
I0826 11:51:18.352954 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:1
I0826 11:51:18.353148 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:2
I0826 11:51:18.353321 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:3
I0826 11:51:18.353487 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:4
I0826 11:51:18.353651 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:5
I0826 11:51:18.353815 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:6
I0826 11:51:18.353976 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:7
I0826 11:51:18.354135 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0
I0826 11:51:18.354294 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:0
I0826 11:51:18.354458 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:1
I0826 11:51:18.354618 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:2
I0826 11:51:18.354776 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:3
I0826 11:51:18.354935 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:4
I0826 11:51:18.355093 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:5
I0826 11:51:18.355252 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:6
I0826 11:51:18.355409 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:7
W0826 11:51:18.355474 140208701863744 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
I0826 11:51:18.356196 140208701863744 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['username@100.102.32.179:8080', 'username@100.102.33.40:8080']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:GPU:0', '/job:worker/task:0/device:GPU:1', '/job:worker/task:0/device:GPU:2', '/job:worker/task:0/device:GPU:3', '/job:worker/task:0/device:GPU:4', '/job:worker/task:0/device:GPU:5', '/job:worker/task:0/device:GPU:6', '/job:worker/task:0/device:GPU:7'), communication = CollectiveCommunication.AUTO
W0826 11:51:19.012401 140208701863744 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0826 11:51:19.093180 140208701863744 deprecation.py:323] From /data1/username/.local/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
W0826 11:51:19.094039 140208701863744 deprecation.py:323] From /data1/username/models/official/vision/image_classification/cifar_preprocessing.py:80: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
I0826 11:51:19.311534 140208701863744 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2
I0826 11:51:19.311678 140208701863744 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2
I0826 11:51:19.332920 140208701863744 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2
I0826 11:51:19.333035 140208701863744 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2
I0826 11:51:19.354012 140208701863744 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2
I0826 11:51:19.354128 140208701863744 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2
I0826 11:51:19.406786 140208701863744 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2
I0826 11:51:19.406906 140208701863744 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2
I0826 11:51:19.427956 140208701863744 cross_device_ops.py:1032] Collective batch_all_reduce: 1 all-reduces, num_workers = 2
I0826 11:51:19.428072 140208701863744 cross_device_ops.py:1053] Collective batch_all_reduce: 1 all-reduces, num_workers = 2
W0826 11:51:50.855791 140208701863744 deprecation.py:506] From /data1/username/.local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0826 11:51:50.961928 140208701863744 distribute_coordinator.py:776] Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'worker': ['username@100.102.32.179:8080', 'username@100.102.33.40:8080']}, task_type = 'worker', task_id = 0, environment = None, rpc_layer = 'grpc'
W0826 11:51:50.962039 140208701863744 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an ""evaluator"" task exists in the cluster.
W0826 11:51:50.962105 140208701863744 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
2019-08-26 11:51:50.967967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:04:00.0
2019-08-26 11:51:50.969414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:05:00.0
2019-08-26 11:51:50.970952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:08:00.0
2019-08-26 11:51:50.972416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:09:00.0
2019-08-26 11:51:50.973948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:84:00.0
2019-08-26 11:51:50.975438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:85:00.0
2019-08-26 11:51:50.976896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:88:00.0
2019-08-26 11:51:50.978343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:89:00.0
2019-08-26 11:51:50.978393: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-26 11:51:50.978413: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-26 11:51:50.978430: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-26 11:51:50.978447: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-26 11:51:50.978463: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-26 11:51:50.978479: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-26 11:51:50.978496: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-26 11:51:51.002211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-08-26 11:51:51.003014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-26 11:51:51.003031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 
2019-08-26 11:51:51.003054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N 
2019-08-26 11:51:51.003068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N 
2019-08-26 11:51:51.003075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N 
2019-08-26 11:51:51.003089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N 
2019-08-26 11:51:51.003096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y 
2019-08-26 11:51:51.003103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y 
2019-08-26 11:51:51.003116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y 
2019-08-26 11:51:51.003124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N 
2019-08-26 11:51:51.021461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)
2019-08-26 11:51:51.022911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)
2019-08-26 11:51:51.024357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)
2019-08-26 11:51:51.026395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)
2019-08-26 11:51:51.028537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)
2019-08-26 11:51:51.030097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)
2019-08-26 11:51:51.031552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)
2019-08-26 11:51:51.033061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)
I0826 11:51:51.033764 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0
I0826 11:51:51.033987 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:0
I0826 11:51:51.034153 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:1
I0826 11:51:51.034312 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:2
I0826 11:51:51.034467 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:3
I0826 11:51:51.034620 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:4
I0826 11:51:51.034772 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:5
I0826 11:51:51.034923 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:6
I0826 11:51:51.035072 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:7
I0826 11:51:51.035221 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0
I0826 11:51:51.035371 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:0
I0826 11:51:51.035520 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:1
I0826 11:51:51.035670 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:2
I0826 11:51:51.035823 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:3
I0826 11:51:51.035972 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:4
I0826 11:51:51.036118 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:5
I0826 11:51:51.036265 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:6
I0826 11:51:51.036412 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:7
W0826 11:51:51.036476 140208701863744 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
I0826 11:51:51.037198 140208701863744 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['username@100.102.32.179:8080', 'username@100.102.33.40:8080']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:GPU:0', '/job:worker/task:0/device:GPU:1', '/job:worker/task:0/device:GPU:2', '/job:worker/task:0/device:GPU:3', '/job:worker/task:0/device:GPU:4', '/job:worker/task:0/device:GPU:5', '/job:worker/task:0/device:GPU:6', '/job:worker/task:0/device:GPU:7'), communication = CollectiveCommunication.AUTO
I0826 11:51:51.037485 140208701863744 distribute_coordinator.py:438] Starting standard TensorFlow server, target = 'grpc://username@100.102.32.179:8080', session_config= allow_soft_placement: true
graph_options {
  rewrite_options {
    scoped_allocator_optimization: ON
    scoped_allocator_opts {
      enable_op: ""CollectiveReduce""
    }
  }
}
experimental {
  collective_group_leader: ""/job:worker/replica:0/task:0""
}

2019-08-26 11:51:51.039570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:04:00.0
2019-08-26 11:51:51.041442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:05:00.0
2019-08-26 11:51:51.043607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:08:00.0
2019-08-26 11:51:51.045599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:09:00.0
2019-08-26 11:51:51.047044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:84:00.0
2019-08-26 11:51:51.048483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:85:00.0
2019-08-26 11:51:51.049915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:88:00.0
2019-08-26 11:51:51.052019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:89:00.0
2019-08-26 11:51:51.052058: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-26 11:51:51.052077: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-26 11:51:51.052094: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-26 11:51:51.052110: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-26 11:51:51.052126: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-26 11:51:51.052142: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-26 11:51:51.052158: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-26 11:51:51.086456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-08-26 11:51:51.097618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-26 11:51:51.097635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 
2019-08-26 11:51:51.097659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N 
2019-08-26 11:51:51.097666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N 
2019-08-26 11:51:51.097680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N 
2019-08-26 11:51:51.097687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N 
2019-08-26 11:51:51.097701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y 
2019-08-26 11:51:51.097708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y 
2019-08-26 11:51:51.097722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y 
2019-08-26 11:51:51.097735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N 
2019-08-26 11:51:51.135823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)
2019-08-26 11:51:51.138364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)
2019-08-26 11:51:51.141177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)
2019-08-26 11:51:51.146457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)
2019-08-26 11:51:51.151085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)
2019-08-26 11:51:51.156594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)
2019-08-26 11:51:51.162514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)
2019-08-26 11:51:51.164078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)
2019-08-26 11:51:51.165756: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:8080, 1 -> username@100.102.33.40:8080}
2019-08-26 11:51:51.169005: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:8080
2019-08-26 11:51:51.169031: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:369] Server already started (target: grpc://localhost:8080)
2019-08-26 11:51:51.180244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:04:00.0
2019-08-26 11:51:51.181868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:05:00.0
2019-08-26 11:51:51.183521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:08:00.0
2019-08-26 11:51:51.185127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:09:00.0
2019-08-26 11:51:51.186747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:84:00.0
2019-08-26 11:51:51.188347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:85:00.0
2019-08-26 11:51:51.189975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:88:00.0
2019-08-26 11:51:51.191620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:89:00.0
2019-08-26 11:51:51.191660: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-26 11:51:51.191688: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-26 11:51:51.191708: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-26 11:51:51.191726: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-26 11:51:51.191749: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-26 11:51:51.191768: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-26 11:51:51.191801: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-26 11:51:51.217200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-08-26 11:51:51.217612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-26 11:51:51.217632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 
2019-08-26 11:51:51.217643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N 
2019-08-26 11:51:51.217668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N 
2019-08-26 11:51:51.217689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N 
2019-08-26 11:51:51.217705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N 
2019-08-26 11:51:51.217720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y 
2019-08-26 11:51:51.217739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y 
2019-08-26 11:51:51.217747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y 
2019-08-26 11:51:51.217758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N 
2019-08-26 11:51:51.235327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)
2019-08-26 11:51:51.236996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)
2019-08-26 11:51:51.238641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)
2019-08-26 11:51:51.240324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)
2019-08-26 11:51:51.242048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)
2019-08-26 11:51:51.243783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)
2019-08-26 11:51:51.245461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)
2019-08-26 11:51:51.247184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)
I0826 11:51:51.248429 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0
I0826 11:51:51.248695 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:0
I0826 11:51:51.248878 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:1
I0826 11:51:51.249054 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:2
I0826 11:51:51.249228 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:3
I0826 11:51:51.249400 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:4
I0826 11:51:51.249571 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:5
I0826 11:51:51.249751 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:6
I0826 11:51:51.249931 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:7
I0826 11:51:51.250112 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0
I0826 11:51:51.250284 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:0
I0826 11:51:51.250463 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:1
I0826 11:51:51.250636 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:2
I0826 11:51:51.250803 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:3
I0826 11:51:51.250995 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:4
I0826 11:51:51.251197 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:5
I0826 11:51:51.251389 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:6
I0826 11:51:51.251568 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:7
W0826 11:51:51.251645 140208701863744 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
I0826 11:51:51.252460 140208701863744 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['username@100.102.32.179:8080', 'username@100.102.33.40:8080']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:GPU:0', '/job:worker/task:0/device:GPU:1', '/job:worker/task:0/device:GPU:2', '/job:worker/task:0/device:GPU:3', '/job:worker/task:0/device:GPU:4', '/job:worker/task:0/device:GPU:5', '/job:worker/task:0/device:GPU:6', '/job:worker/task:0/device:GPU:7'), communication = CollectiveCommunication.AUTO
I0826 11:51:55.961434 140208701863744 distribute_coordinator.py:776] Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'worker': ['username@100.102.32.179:8080', 'username@100.102.33.40:8080']}, task_type = 'worker', task_id = 0, environment = None, rpc_layer = 'grpc'
W0826 11:51:55.961624 140208701863744 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an ""evaluator"" task exists in the cluster.
W0826 11:51:55.961693 140208701863744 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
2019-08-26 11:51:55.967643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:04:00.0
2019-08-26 11:51:55.969124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:05:00.0
2019-08-26 11:51:55.970583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:08:00.0
2019-08-26 11:51:55.972132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:09:00.0
2019-08-26 11:51:55.973567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:84:00.0
2019-08-26 11:51:55.974998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:85:00.0
2019-08-26 11:51:55.976440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:88:00.0
2019-08-26 11:51:55.977881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:89:00.0
2019-08-26 11:51:55.977933: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-26 11:51:55.977954: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-26 11:51:55.977972: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-26 11:51:55.978010: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-26 11:51:55.978030: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-26 11:51:55.978050: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-26 11:51:55.978068: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-26 11:51:56.003848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-08-26 11:51:56.004245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-26 11:51:56.004263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 
2019-08-26 11:51:56.004274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N 
2019-08-26 11:51:56.004295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N 
2019-08-26 11:51:56.004313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N 
2019-08-26 11:51:56.004322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N 
2019-08-26 11:51:56.004337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y 
2019-08-26 11:51:56.004346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y 
2019-08-26 11:51:56.004361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y 
2019-08-26 11:51:56.004372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N 
2019-08-26 11:51:56.022906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)
2019-08-26 11:51:56.024706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)
2019-08-26 11:51:56.027009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)
2019-08-26 11:51:56.028847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)
2019-08-26 11:51:56.030286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)
2019-08-26 11:51:56.031725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)
2019-08-26 11:51:56.033410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)
2019-08-26 11:51:56.035645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)
I0826 11:51:56.036307 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0
I0826 11:51:56.036504 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:0
I0826 11:51:56.036674 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:1
I0826 11:51:56.036839 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:2
I0826 11:51:56.036999 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:3
I0826 11:51:56.037156 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:4
I0826 11:51:56.037314 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:5
I0826 11:51:56.037470 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:6
I0826 11:51:56.037625 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:7
I0826 11:51:56.037780 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0
I0826 11:51:56.037934 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:0
I0826 11:51:56.038089 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:1
I0826 11:51:56.038243 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:2
I0826 11:51:56.038396 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:3
I0826 11:51:56.038550 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:4
I0826 11:51:56.038703 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:5
I0826 11:51:56.038856 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:6
I0826 11:51:56.039009 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:7
W0826 11:51:56.039075 140208701863744 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
I0826 11:51:56.039798 140208701863744 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['username@100.102.32.179:8080', 'username@100.102.33.40:8080']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:GPU:0', '/job:worker/task:0/device:GPU:1', '/job:worker/task:0/device:GPU:2', '/job:worker/task:0/device:GPU:3', '/job:worker/task:0/device:GPU:4', '/job:worker/task:0/device:GPU:5', '/job:worker/task:0/device:GPU:6', '/job:worker/task:0/device:GPU:7'), communication = CollectiveCommunication.AUTO
2019-08-26 11:51:56.045441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:04:00.0
2019-08-26 11:51:56.046901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:05:00.0
2019-08-26 11:51:56.048481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:08:00.0
2019-08-26 11:51:56.050782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:09:00.0
2019-08-26 11:51:56.053113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:84:00.0
2019-08-26 11:51:56.054669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:85:00.0
2019-08-26 11:51:56.056100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:88:00.0
2019-08-26 11:51:56.057525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 7 with properties: 
name: Tesla M40 24GB major: 5 minor: 2 memoryClockRate(GHz): 1.112
pciBusID: 0000:89:00.0
2019-08-26 11:51:56.057556: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-26 11:51:56.057575: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-26 11:51:56.057599: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-26 11:51:56.057619: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-26 11:51:56.057639: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-26 11:51:56.057658: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-26 11:51:56.057678: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-26 11:51:56.114764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2019-08-26 11:51:56.125872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-26 11:51:56.125889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6 7 
2019-08-26 11:51:56.125916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y N N N N 
2019-08-26 11:51:56.125924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y N N N N 
2019-08-26 11:51:56.125941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y N N N N 
2019-08-26 11:51:56.125951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N N N N N 
2019-08-26 11:51:56.125967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   N N N N N Y Y Y 
2019-08-26 11:51:56.125981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   N N N N Y N Y Y 
2019-08-26 11:51:56.125988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   N N N N Y Y N Y 
2019-08-26 11:51:56.125996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 7:   N N N N Y Y Y N 
2019-08-26 11:51:56.160059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 20545 MB memory) -> physical GPU (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0, compute capability: 5.2)
2019-08-26 11:51:56.161525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 20545 MB memory) -> physical GPU (device: 1, name: Tesla M40 24GB, pci bus id: 0000:05:00.0, compute capability: 5.2)
2019-08-26 11:51:56.163024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 20545 MB memory) -> physical GPU (device: 2, name: Tesla M40 24GB, pci bus id: 0000:08:00.0, compute capability: 5.2)
2019-08-26 11:51:56.164528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 20545 MB memory) -> physical GPU (device: 3, name: Tesla M40 24GB, pci bus id: 0000:09:00.0, compute capability: 5.2)
2019-08-26 11:51:56.165979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 20545 MB memory) -> physical GPU (device: 4, name: Tesla M40 24GB, pci bus id: 0000:84:00.0, compute capability: 5.2)
2019-08-26 11:51:56.167440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 20545 MB memory) -> physical GPU (device: 5, name: Tesla M40 24GB, pci bus id: 0000:85:00.0, compute capability: 5.2)
2019-08-26 11:51:56.168887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 20545 MB memory) -> physical GPU (device: 6, name: Tesla M40 24GB, pci bus id: 0000:88:00.0, compute capability: 5.2)
2019-08-26 11:51:56.170380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:7 with 20545 MB memory) -> physical GPU (device: 7, name: Tesla M40 24GB, pci bus id: 0000:89:00.0, compute capability: 5.2)
I0826 11:51:56.170995 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:CPU:0
I0826 11:51:56.171188 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:0
I0826 11:51:56.171359 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:1
I0826 11:51:56.171522 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:2
I0826 11:51:56.171682 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:3
I0826 11:51:56.171840 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:4
I0826 11:51:56.171997 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:5
I0826 11:51:56.172154 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:6
I0826 11:51:56.172314 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_GPU:7
I0826 11:51:56.172470 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:XLA_CPU:0
I0826 11:51:56.172623 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:0
I0826 11:51:56.172777 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:1
I0826 11:51:56.172931 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:2
I0826 11:51:56.173083 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:3
I0826 11:51:56.173235 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:4
I0826 11:51:56.173387 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:5
I0826 11:51:56.173537 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:6
I0826 11:51:56.173689 140208701863744 cross_device_ops.py:1174] Device is available but not used by distribute strategy: /device:GPU:7
W0826 11:51:56.173753 140208701863744 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
I0826 11:51:56.174468 140208701863744 collective_all_reduce_strategy.py:226] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['username@100.102.32.179:8080', 'username@100.102.33.40:8080']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:GPU:0', '/job:worker/task:0/device:GPU:1', '/job:worker/task:0/device:GPU:2', '/job:worker/task:0/device:GPU:3', '/job:worker/task:0/device:GPU:4', '/job:worker/task:0/device:GPU:5', '/job:worker/task:0/device:GPU:6', '/job:worker/task:0/device:GPU:7'), communication = CollectiveCommunication.AUTO
W0826 11:51:56.174733 140208701863744 distributed_training_utils.py:1082] ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
```
`nvidia-smi` show that only a tiny part of gpu memory is used in node 0. and the other machine, i.e., the worker node 1, does not run any program. The worker node has the same python and tensorflow version.
Here are my questions.
1. should I set up some configuration in worker node 1? what and how?
2. Is there anything wrong using the resnet_cifar_main.py for distributed training
3. i found the port on node 0 was listening
 COMMAND     PID       USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME
resnet_ci 51888 username 93u  IPv4 8730954      0t0  TCP *:webcache (LISTEN)
however,  the port on node 1 was not opening.

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31968,Why HLO Importer doesn't support U32 type?,"Hi:
I am learning the internal data structure of Tensorflow and MLIR.  
I first used ``tfcompile --graph=graph.pb  --config=graph.config.pbtxt   --xla_dump_to=""/tmp/"" --cpp_class=""mynamespace::MyComputation"" --xla_dump_hlo_as_text=true`` and then extracted ``module_0000.after_optimizations.txt`` which if I understood this correctly is the lowered HLO IR from input TF GraphDef.

However when I pass this file into ``tf-mlir-translate --hlo-text-to-mlir-hlo /tmp/module_0000.after_optimizations.txt`` I got greeted with an internal error says 'Unsupported Type: U32'.

Is there any reason U32 isn't supported in HLO or am I missing something obvious here?"
31967,Select compiler for building,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
1.14

- Are you willing to contribute it (Yes/No):

Yes


**Describe the feature and the current behavior/state.**
Select the compiler to use for building in configure.py

**Will this change the current api? How?**

I don't think so.

**Who will benefit with this feature?**

Everyone who needs it

**Any Other info.**


Is it possible in the python configure script to choose the compiler when building on a non-linux? On windows, I have several compilers, but the script always selects the wrong compiler. On linux, it asks for the compiler to use, but doesn't on other systems. I will try to see how the script works so it can detect the compilers and let the user select it so it won't choose the wrong compiler or version."
31966,NotFoundError: _ZN10tensorflow31MaybeForwardRefInputToRefOutputEPNS_15OpKernelContextEii ,"Python 3.6.5
compiled tensorflow-1.13.1-cp36-cp36m-linux_x86_64.whl

The compilation process of my newly created optimizer libgftrl_op.so can be compiled without error. However, when it is loaded by `resource_loader.get_path_to_datafile('libgftrl_op.so'))`, then comes the error message
`
tensorflow.python.framework.errors_impl.NotFoundError: /opt/ml/job/python/jarvis/tensorflow/libgftrl_op.so: undefined symbol: _ZN10tensorflow31MaybeForwardRefInputToRefOutputEPNS_15OpKernelContextEii
`

I've searched throughout google but with the key word of `_ZN10tensorflow31MaybeForwardRefInputToRefOutputEPNS_15OpKernelContextEii` I found nothing. 


`
nm libgftrl_op.so | grep _ZN10tensorflow31MaybeForwardRefInputToRefOutputEPNS_15OpKernelContextEii`
command is also executed, with the output of 
`                  U _ZN10tensorflow31MaybeForwardRefInputToRefOutputEPNS_15OpKernelContextEii`

However, there's no same record of `                  U _ZN10tensorflow31MaybeForwardRefInputToRefOutputEPNS_15OpKernelContextEii` in the `libtensorflow_framework.so`

So what on earth the meaning of `_ZN10tensorflow31MaybeForwardRefInputToRefOutputEPNS_15OpKernelContextEii`  is actually confuses me a lot"
31965,Crash-course issue,"#31958  URL(s) with the issue:
https://developers.google.cn/machine-learning/crash-course/reducing-loss/video-lecture

## Description of issue (what needs changing):
On 1:50,it prompts me to do the gradient-descent practice，when i click the button,then redirect to the wrong page.

### Correct links

https://developers.google.cn/machine-learning/crash-course/reducing-loss/gradient-descent
"
31964,ERROR: opt-einsum requires Python '>=3.5' but the running Python is 2.7.15,"I want upgrade tensorflow-gpu==2.0.0-beta to  tensorflow-gpu==2.0.0-rc0. I upgrade with the following command: 
`pip install tensorflow-gpu==2.0.0-rc0 -U`

I got an error like this:

```
  Downloading http://pypi.sys.srv/root/pypi/%2Bf/8ab/a07af4cf80e86/opt_einsum-3.0.1.tar.gz (66kB)
     |████████████████████████████████| 71kB 13.6MB/s 
ERROR: opt-einsum requires Python '>=3.5' but the running Python is 2.7.15
...
```
But it is ok for last version. How to solve the problem?"
31963,Broken link in XLA page,"All the link on the following page is forwarding to a 404 page.

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/operation_semantics.md"
31962,Tensorflow 2.0 tf.function internal error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab, Ubuntu, Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0-beta, 2.0-rc, 2.0-nightly (v1.12.1-9694-g006e2933 2.0.0-dev20190825)
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10, also without CUDA
- GPU model and memory:

**Describe the current behavior**
Decorating the training loop that consumes a `tf.Data.Dataset` with `@tf.function` causes an internal error in tensorflow (an object is returned to Python with an error set and I'm unable to understand the actual origin of the error). Not using `@tf.function`, the code works alright.

**Describe the expected behavior**
The model gradients should be calculated well regardless of being within a `@tf.function` trace or not.

**Code to reproduce the issue**
Code is provided in the [colab notebook available here](https://colab.research.google.com/drive/1LCZqyGa8mPjBS6KXOnEH_T7VOWoFpnnB)

**Other info / logs**
While working with a rather complex module that operates on irregular data (Graphs), using `@tf.function` worsens performance in TF 2.0 due to bug #29075. While following the workaround described in that issue, I stumbled upon this issue.

Because of these issues, TF 2.0 is not a good fit for DL on irregularly shaped data.

Relevant traceback:
```
    <ipython-input-8-e24395a4965a>:137 train_step  *
        gradients = tape.gradient(loss, model.trainable_variables)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py:1015 gradient
        unconnected_gradients=unconnected_gradients)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/imperative_grad.py:76 imperative_grad
        compat.as_str(unconnected_gradients.value))
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py:599 _aggregate_grads
        if len(gradients) == 1:

    SystemError: <built-in function len> returned a result with an error set
```"
31960,No description for / option to select non default cuda location when building TF2.0 from source,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>
commit hash:
553a3b826a55acb78de18ca6dbca8c965c7cf78e

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0rc0
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: none
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: 9.1 + 10.1
- GPU model and memory: various

Option/Description how to select a non-default cuda path missing.

I am working on a shared server with cuda 9.1 in /usr/local/cuda/
I want to build tensorflow 2.0 rc0 from source using a different cuda version in a different location.

Executing ./configure it automatically selects the default one and i do not have an option to select a different one.

Documentation
https://www.tensorflow.org/install/source#tensorflow_2
only says: ```If your system has multiple versions of CUDA or cuDNN installed, explicitly set the version instead of relying on the default```


**Provide the exact sequence of commands / steps that you executed before running into the problem**
`./configure`

"
31957,Eager mode: Accessing contents of scalars in a tf.function,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

I am trying to parallel download some images. I am using a `tf.data.Dataset` with the image urls as content. I want to store them in a GCS, so I am using functions from the `tf.io.gfile` package inside a `tf.function`. This function will be called through `tf.data.Dataset.map`.

When the different `tf.io.gfile` functions are called inside the `tf.function`, like `makedirs`, it raises an Error indicating that it requires a binary or unicode string as input:

```TypeError: Expected binary or unicode string, got <tf.Tensor 'StringJoin_1:0' shape=() dtype=string>```

If I try to use `.numpy()`, it is not available, as expected. The result is that I cannot download the images in the GCS.

**Describe the expected behavior**

As a tensorflow package, I would expect the `tf.io.gfile` functions to allow the use of scalar string tensors, or I would expect tensorflow to provide a solution similar to the `.numpy()` function inside `tf.function` for these cases. If not, at least it should be a warning in the documentation that these functions cannot be used inside `tf.function`.

**Code to reproduce the issue**

```python
@tf.function
def test_string(value): 
    return tf.io.gfile.exists(value)

test_string(tf.constant('test'))
```

The result in this case is:

```
TypeError: Expected binary or unicode string, got <tf.Tensor 'value:0' shape=() dtype=string>
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31956,LossScaleOptimizer does not work,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0
- GPU model and memory: GTX 1080Ti

**Describe the current behavior**
I am trying to run the sample code from [https://www.tensorflow.org/api_docs/python/tf/contrib/mixed_precision/LossScaleOptimizer](https://www.tensorflow.org/api_docs/python/tf/contrib/mixed_precision/LossScaleOptimizer) and get the following error when no gradient can be computed for some variables:

```
ValueError                                Traceback (most recent call last)
C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    526                 as_ref=input_arg.is_ref,
--> 527                 preferred_dtype=default_dtype)
    528           except TypeError as err:

C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)
   1223     if ret is None:
-> 1224       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1225 

C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    304   _ = as_ref
--> 305   return constant(v, dtype=dtype, name=name)
    306 

C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name)
    245   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 246                         allow_broadcast=True)
    247 

C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    283           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
--> 284           allow_broadcast=allow_broadcast))
    285   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)

C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    453     if values is None:
--> 454       raise ValueError(""None values not supported."")
    455     # if dtype is provided, forces numpy array to be the type

ValueError: None values not supported.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    540               observed = ops.internal_convert_to_tensor(
--> 541                   values, as_ref=input_arg.is_ref).dtype.name
    542             except ValueError as err:

C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)
   1223     if ret is None:
-> 1224       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1225 

C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    304   _ = as_ref
--> 305   return constant(v, dtype=dtype, name=name)
    306 

C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name)
    245   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 246                         allow_broadcast=True)
    247 

C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    283           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
--> 284           allow_broadcast=allow_broadcast))
    285   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)

C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    453     if values is None:
--> 454       raise ValueError(""None values not supported."")
    455     # if dtype is provided, forces numpy array to be the type

ValueError: None values not supported.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-3-5d2950c170d2> in <module>
     13 
     14 # Call minimize() on the loss scale optimizer.
---> 15 train_op = loss_scale_optimizer.minimize(loss)

C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\training\optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)
    411 
    412     return self.apply_gradients(grads_and_vars, global_step=global_step,
--> 413                                 name=name)
    414 
    415   def compute_gradients(self, loss, var_list=None,

C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\contrib\mixed_precision\python\loss_scale_optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)
    148     is_finite_grad = []
    149     for g in grads:
--> 150       is_finite_grad.append(math_ops.reduce_all(gen_math_ops.is_finite(g)))
    151     is_overall_finite = math_ops.reduce_all(is_finite_grad)
    152 

C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_math_ops.py in is_finite(x, name)
   4919   try:
   4920     _, _, _op = _op_def_lib._apply_op_helper(
-> 4921         ""IsFinite"", x=x, name=name)
   4922   except (TypeError, ValueError):
   4923     result = _dispatch.dispatch(

C:\Users\admin\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    543               raise ValueError(
    544                   ""Tried to convert '%s' to a tensor and failed. Error: %s"" %
--> 545                   (input_name, err))
    546             prefix = (""Input '%s' of '%s' Op has type %s that does not match"" %
    547                       (input_name, op_type_name, observed))

ValueError: Tried to convert 'x' to a tensor and failed. Error: None values not supported.

```
**Describe the expected behavior**
No error would occur for some other optimizers such as AdamOptimizer and MovingAverageOptimizer, even if no gradient can be computed for some variables.
**Code to reproduce the issue**
```
import tensorflow as tf
a1=tf.Variable(1., name='a1')
a2=tf.Variable(2., name='a2')

model_params = [var for var in tf.global_variables() if 'a' in var.name]
loss = a1**2
opt = tf.train.AdamOptimizer(learning_rate=.1, beta1=0., beta2=0.9)

# Choose a loss scale manager which decides how to pick the right loss scale
# throughout the training process.
loss_scale_manager = tf.contrib.mixed_precision.FixedLossScaleManager(5000)

# Wraps the original optimizer in a LossScaleOptimizer.
loss_scale_optimizer =tf.contrib.mixed_precision.LossScaleOptimizer(opt, loss_scale_manager)

# Call minimize() on the loss scale optimizer.
train_op = loss_scale_optimizer.minimize(loss, var_list=model_params)
```
"
31954,TPUStrategy with Keras in TF2.0.0rc0,"Hi,

Although the description of TF 2.0.0rc0 contains this
 _""Distribution Strategy: TF 2.0 users will be able to use the tf.distribute.Strategy API to distribute training with minimal code changes, yielding great out-of-the-box performance. It supports distributed training with Keras model.fit, as well as with custom training loops. Multi-GPU support is available, along with experimental support for multi worker and Cloud TPUs. Check out the guide for more details.""_

When I try to implemented it with Keras and multiple TPU units, crashes with _""NotImplementedError: Using multiple TPUs in a single session is not yet implemented""_

So, is supported TPUStrategy with keras in this release or not?

Thank you
"
31953,TF2.0 from source crashes with optimizer specified as instance,"**System information**
- Have I written custom code: **yes**
- OS Platform and Distribution: **OpenSUSE Tumbleweed**
- TensorFlow installed from: **source**
- TensorFlow version: **v2.0.0-rc0-0-gc75bb66a99 2.0.0-rc0**
- Python version: **3.7.3**
- Bazel version: **0.26**
- GCC/Compiler version: **gcc (SUSE Linux) 9.1.1 20190805 [gcc-9-branch revision 274114]**
- CUDA/cuDNN version: **-**
- GPU model and memory: **gfx803 - Ellesmere - AMD Radeon RX 580 - 8192MB**

**Describe the current behavior**
When the optimizer is specified in the form of a string (e.g. `adam`) in `model.compile(...)`, everything works as expected. When the optimizer is specified with non-default parameters, like:
```python
model.compile(loss='mse', optimizer=optimizers.Adam(lr=0.01337))
```
the application crashes with the following error:
```
Traceback (most recent call last):
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 527, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1296, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 286, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 227, in constant
    allow_broadcast=True)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 265, in _constant_impl
    allow_broadcast=allow_broadcast))
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py"", line 437, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 541, in _apply_op_helper
    values, as_ref=input_arg.is_ref).dtype.name
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1296, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 286, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 227, in constant
    allow_broadcast=True)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 265, in _constant_impl
    allow_broadcast=allow_broadcast))
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py"", line 437, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/seiji/.vscode/extensions/ms-python.python-2019.8.30787/pythonFiles/ptvsd_launcher.py"", line 43, in <module>
    main(ptvsdArgs)
  File ""/home/seiji/.vscode/extensions/ms-python.python-2019.8.30787/pythonFiles/lib/python/ptvsd/__main__.py"", line 432, in main
    run()
  File ""/home/seiji/.vscode/extensions/ms-python.python-2019.8.30787/pythonFiles/lib/python/ptvsd/__main__.py"", line 316, in run_file
    runpy.run_path(target, run_name='__main__')
  File ""/usr/lib64/python3.7/runpy.py"", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File ""/usr/lib64/python3.7/runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""/usr/lib64/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/mnt/data/Documents/seiji.li/FHWS/Master/SS2019/Masterarbeit/workspace/Autoencoder_Example/autoencoder_conv3.py"", line 28, in <module>
    autoencoder.fit(train_features, train_features, verbose=1, shuffle=True)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 734, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 674, in fit
    steps_name='steps_per_epoch')
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 189, in model_iteration
    f = _make_execution_function(model, mode)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 565, in _make_execution_function
    return model._make_execution_function(mode)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 2149, in _make_execution_function
    self._make_train_function()
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 2081, in _make_train_function
    params=self._collected_trainable_weights, loss=self.total_loss)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizers.py"", line 476, in get_updates
    grads = self.get_gradients(loss, params)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizers.py"", line 92, in get_gradients
    if None in grads:
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py"", line 1283, in tensor_equals
    return gen_math_ops.equal(self, other)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py"", line 3629, in equal
    ""Equal"", x=x, y=y, name=name)
  File ""/home/seiji/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 545, in _apply_op_helper
    (input_name, err))
ValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.
Terminated
```
This **only** happens, when using the tensorflow compiled from source. The version `2.0.0b1` as distributed through pip works as expected.
I have tested Tensorflow compiled from source **with** and **without** ROCm enabled. Both crash.

**Describe the expected behavior**
Specifying an optimizer as variable instead of with a string and default parameters should work.

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow
from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras import backend as K
from tensorflow.python.keras import optimizers

original_dim = 160
intermediate_dim = 32
# load data
train_features = np.random.rand(100000, original_dim).astype(np.float32)
test_features = np.random.rand(10000, original_dim).astype(np.float32)

############# BUILD MODEL #############
autoencoder = Sequential()
autoencoder.add( Input(shape=(original_dim,)) )
autoencoder.add( Dense(intermediate_dim) )
autoencoder.add( Dense(original_dim) )
autoencoder.build()
autoencoder.summary()

# crashes:
autoencoder.compile(loss='mse', optimizer=optimizers.Adam(lr=0.01337))
#works:
#autoencoder.compile(loss='mse', optimizer='adam')

# Train model
autoencoder.fit(train_features, train_features, verbose=1, shuffle=True)
```"
31952,[TF 2.0] tf.gather doesn't work alongside @tf.function,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Darwin Kernel Version 18.6.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
2.0.0-dev20190730
- Python version:
Python 3.6.8 :: Anaconda, Inc.
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A


**Describe the current behavior**
It seems that when `tf.gather()` is called after a `tf.function`, the gradient cannot be calculated. The example code blow shows the bug. The code itself raises the following error message:

> AssertionError: Expected all args to be Tensors or Variables; but got CompositeTensor

The code will work if we remove the `tf.function` decorator, or  put the `tf.gather` line inside the `tf.funtion` graph.

**Code to reproduce the issue**
```
import numpy as np
import tensorflow as tf

x = tf.cast(np.random.randn(100, 100), tf.float32)
z = tf.cast(np.random.randn(1, 100), tf.float32)

layer = tf.keras.layers.Dense(100)

@tf.function  # <- removing this and the code works fine
def fun(x, layer):
    y = layer(x)
    return y

with tf.GradientTape() as tape:
    y = fun(x, layer)
    y = tf.gather(y, [0])  # if we put this line inside the function it works fine
    loss = tf.norm(y - z)

grads = tape.gradient(loss, layer.trainable_variables)
```

"
31951,No Brier Score Loss Function,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.14.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
https://en.wikipedia.org/wiki/Brier_score

**Will this change the current api? How?**
Add support for calculating Brier Score

**Who will benefit with this feature?**
Anyone who wants to utilize Brier Score

**Any Other info.**
"
31949,[TensorFlow Lite] GPU experimental does not work with converted SSD model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no (for the demo APPs);
yes (for the object detection APP), but only added GPU delegates and used a self-converted SSD float model
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy): Google Pixel 2 XL (Android 8.0.0 and Android 9)
- TensorFlow Lite version: GPU experimental 0.0.1 (I tested version 0.0.0 as well)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 64 bit
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0 / 7.6.0
- GPU model and memory: GeForce GTX 970, AMD64

**Describe the current behavior**
I only got to run the demo APP on GPU with GPU experimental but not the object detection APP. For the object detection APP I had to add the GPU delegates, change some code, and use a self-converted SSD model. 

The error occurs because of the converted SSD model. `GPU delegate does not support TFLite_Detection_PostProcess`. But this option is necessary to convert an SSD model successfully (with 4 outputs). This is the way I convert the ssdlite mobilenet model from model zoo: https://github.com/tensorflow/tensorflow/issues/31015#issuecomment-517165233. This is also the way which was recommended to me, because the conversion did not work with the frozen model provided by model zoo (or by getting the frozen model with the inference_graph script).

**Describe the expected behavior**
I want to run the object detection APP on GPU by using GPU experimental with a self-converted SSD model.

**Code to reproduce the issue**
I changed the current demo APP to GPU experimental and I had to do some minor changes in the code to get the APP running successfully on GPU. (To know which changes to make, I looked into the demo APP version r1.13.)

For the object detection APP I added the GPU delegates, changed isModelQuantized to false (and don't use isQuantized anymore), and use a self-converted SSD mobilenet model (because the APP is currently written for quantized models using CPU and doesn’t have GPU delegates in the original code). I wrote the delegates as described in https://www.tensorflow.org/lite/performance/gpu_advanced#android_java and I converted the model this way: #31015 (comment). Then, I applied the same changes to the object detection APP as I did to the current demo APP before (by using the knowledge of demo APP version r1.13).

- Current demo APP: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo
- Demo APP version r1.13: https://github.com/tensorflow/tensorflow/tree/r1.13/tensorflow/lite/java/demo
- Object Detection APP: https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android


**Performance results tested on Google Pixel 2 XL (with Android 8.0.0 and Android 9):**

Demo APP version r1.13
- Quantized Model + CPU -> 80ms
- Float Model + CPU -> 90ms
- Float Model + GPU -> **38ms**

Current demo APP
- Quantized Model + CPU -> 80ms
- Float Model + CPU -> 150ms
- Float Model + GPU -> **28ms**

Object Detection APP
- (self-converted) Float Model + GPU -> **ERROR**


**Full error**
When running the object detection APP with GPU experimental 0.0.1 and a converted SSDlite mobilenet model.

```
E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: org.tensorflow.lite.examples.detection, PID: 4159
    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Next operations are not supported by GPU delegate:
    CUSTOM TFLite_Detection_PostProcess: Operation is not supported.
    First 114 operations will run on the GPU, and the remaining 1 on the CPU.TfLiteGlDelegate Invoke: Delegate should run on the same thread where it was initialized.Node number 115 (TfLiteGlDelegate) failed to invoke.

        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:149)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)
        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:214)
        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:181)
        at android.os.Handler.handleCallback(Handler.java:789)
        at android.os.Handler.dispatchMessage(Handler.java:98)
        at android.os.Looper.loop(Looper.java:164)
        at android.os.HandlerThread.run(HandlerThread.java:65)
```

(Note: With GPU experimental 0.0.0 it neither works but the camera keeps running, the model is not used, and therefore, nothing will be detected. No error occurs and only by debugging the APP, you can find out when the APP stops working properly. It’s in the same line as before with 0.0.1 but the APP doesn't display any error. Therefore, 0.0.1 should be used in order to see what's going wrong.)


**Questions**
- Is there another way to convert an SSD model successfully (with 4 outputs) which then works with GPU delegate?
- Will GPU delegate support TFLite_Detection_PostProcess in the near future? When would this be approximately?
- Is there another way to solve this problem? - The goal is using the object detection APP on GPU with a self-converted SSD model (preferred with GPU experimental). (I couldn't get GPU nightly to work properly neither, the APP seems to run on CPU: https://github.com/tensorflow/tensorflow/issues/31948)

Thanks in advance!"
31948,[TensorFlow Lite] GPU delegates on Android with GPU nightly fail to run on GPU (it seems to run on CPU),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
no (for the demo APP & classification APP); 
yes (for the object detection APP), but only added GPU delegate and used a self-converted SSD float model
- Mobile device: Google Pixel 2 XL (Android 8.0.0 and Android 9)
- TensorFlow Lite version: nightly 0.0.0 + GPU nightly 0.0.0
- OS Platform and Distribution: Windows 10 64 bit
- TensorFlow installed from: binary
- TensorFlow version: 1.13.1
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0 / 7.6.0
- GPU model and memory: GeForce GTX 970, AMD64

**Describe the current behavior**
I cannot get any APP to run on GPU using GPU nightly. All the APPs seem to run on CPU even though the GPU delegates are used by the interpreter and no error occurs.

**Describe the expected behavior**
The APPs should run on GPU. The models running on GPU should be faster than the models running on CPU.

**Code to reproduce the issue**
I tried the demo APP and the classification APP without changes to the code. For the object detection APP I added the GPU delegate, changed isQuantized to false, and I use a self-converted SSD mobilenet model. I wrote the delegate as described in https://www.tensorflow.org/lite/performance/gpu_advanced#android_java and I converted the model this way: https://github.com/tensorflow/tensorflow/issues/31015#issuecomment-517165233.

- Demo APP: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo
- Classification APP: https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android
- Object Detection APP: https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android

**Other info / logs**
Performance results testing the APPs on Google Pixel 2 XL (with Android 8.0.0 and Android 9):

_GPU nightly:_
Current demo APP
- Quantized Model + CPU -> 75ms
- Float Model + CPU -> 135ms
- Float Model + GPU -> **135ms**

Object Detection APP 
- (self-converted) Float Model + GPU -> **100 ms**

_CPU only_:
Objekt Detection APP (I only changed targedSdkVersion to 28 to get the original code working):
- Quantized Model + CPU -> 45-50ms

Thanks in advance!"
31945,tf.custom_gradient does not handle variables correctly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf 1.13.1 and 1.13.2 tested
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0/7.3.1 and 7.4.2
- GPU model and memory: Nvidia Quadro P2000

When applying the custom gradient to a function which uses/creates variables, then it recognizes it through the gradient tape, however it does not find the `variables=None` argument in the gradient function, which results in an error.
`TypeError: If using @custom_gradient with a function that uses variables, then grad_fn must accept a keyword argument 'variables'.`

The expected behavior would be of course, that the found variables are passed correctly to the underlying gradient function. In the following code snippet I expect the `print` statement to display `[<tf.Variable 'outside:0' shape=() dtype=float32>]`. In fact, I achieved this already by changing the custom_gradient functor at one line though I am not sure if this is wanted.

```python
import tensorflow as tf
# from custom_gradient import custom_gradient  # my corrected version
from tensorflow import custom_gradient


def layer(t, name):
    var = tf.Variable(1.0, dtype=tf.float32, use_resource=True, name=name)
    return t * var


@custom_gradient
def custom_gradient_layer(t):
    result = layer(t, name='outside')

    def grad(*grad_ys, variables=None):
        assert variables is not None
        print(variables)
        grads = tf.gradients(
            layer(t, name='inside'),
            [t, *variables],
            grad_ys=grad_ys,
        )
        grads = (grads[:1], grads[1:])
        return grads

    return result, grad


var = tf.Variable(0.0, dtype=tf.float32)
result = custom_gradient_layer(var)
grads = tf.gradients(result, var)[0]
```

The line I changed is https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/ops/custom_gradient.py#L197-L198
to
```python
  variables_in_signature = (""variables"" in grad_argspec.args or
                            grad_argspec.varkw or ""variables"" in grad_argspec.kwonlyargs)
```
"
31944,failed to bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windiws 2012 r2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 1.13.1
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): tried so many version
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: none 
- GPU model and memory: none

**Describe the problem**
Hi, I followed the instruction on [Tensorflow](https://www.tensorflow.org/install/source_windows)
I had a hard time when doing this command
`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`
(I also found others said: change to -c opt, but not working either)
most of the error said 
**FAILED: Build did NOT complete successfully**

I have tried many different versions of Bazel, from 0.18.0 to 0.28.1

**Provide the exact sequence of commands / steps that you executed before running into the problem**
follow the instruction on the website

1. install python and tensorflow: which I already have
2. install bazel and add to %path%
3. install msys2 and add to %path%
4. install visual c++ build tools
5. python ./configure.py
6. bazel build error

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

thanks a lot!!"
31943,[TF 2.0.0-rc0] Run model.evaluate() let  notebook crash (Chrome).,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Mac
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): Tensorflow 2.0.0-rc0
- Python version: 3.6.5

print(tf.version.GIT_VERSION, tf.version.VERSION)
v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0

**Describe the current behavior**
When I run `result = model.evaluate(x_train, y_train), my jupyter notebook crashed.

**Describe the expected behavior**
I will be run another cell in jupyter notebook, but I can't.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
 
I use sample code in Tensorflow 2.0.0 RC: Classify images

**Other info / logs**

"
31942,win10 Creating a Virtual Environment,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):window10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):do not install 
- TensorFlow version:do not install 
- Python version:3.6
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1/7.6.3 
- GPU model and memory: Geforce GTX 1060



**Describe the problem**
the command do not run.
C:\>virtualenv --system-site-packages -p python3 ./venv
The path python3 (from --python=python3) does not exist

**Provide the exact sequence of commands / steps that you executed before running into the problem**
According to tenserflow website install tenserflow with pip in windows10 system. Python3.6 was installed in my computer.

C:\>pip3 --version
pip 19.2.2 from c:\program files\python36\lib\site-packages\pip (python 3.6)

C:\>virtualenv --version
16.7.4

C:\>virtualenv --system-site-packages -p python3 ./venv
The path python3 (from --python=python3) does not exist

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31940,Where is stop_if_no_increase_hook gone?,"I use tensorflow.contrib.estimator.stop_if_no_increase_hook in my code to perform an early stopping, which works smoothly before. But today it throws out an error:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:

https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
https://github.com/tensorflow/addons
https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.
Traceback (most recent call last):
File "".\main.py"", line 215, in 
hook = tf.contrib.estimator.stop_if_no_increase_hook(
AttributeError: module 'tensorflow.contrib.estimator' has no attribute 'stop_if_no_increase_hook'

I noticed that estimator has been moved to tensorflow/estimator. But it doesn't support this method now. Would you tell me where to find it or other workaround to perform early stoppoing?"
31939,Pip install can't find tensorflow2.0.0rc0,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nope
- TensorFlow installed from (source or binary): wheel file via pip
- TensorFlow version: tensorflow==2.0.0rc0
- Python version: python3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): not source
- GCC/Compiler version (if compiling from source): not source
- CUDA/cuDNN version: not GPU
- GPU model and memory: not GPU



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

When you pip install tensorflow==2.0.0rc0, you get an error message saying that pip can't find a matching version of the distribution that't given.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
Step 5/11 : RUN /usr/local/bin/algorithmia-build
 ---> Running in 229b8273366c
Collecting algorithmia<2.0,>=1.0.0 (from -r requirements.txt (line 1))
  Downloading https://files.pythonhosted.org/packages/12/ae/38a82aae155a42261621eaba0a878959a0af93bf2d70d0ba98048ddaa3d2/algorithmia-1.2.0-py2.py3-none-any.whl
Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.12.0)
Collecting pip==19.2.2 (from -r requirements.txt (line 3))
  Downloading https://files.pythonhosted.org/packages/8d/07/f7d7ced2f97ca3098c16565efbe6b15fafcba53e8d9bdb431e09140514b0/pip-19.2.2-py2.py3-none-any.whl (1.4MB)
Collecting tensorflow-gpu==2.0.0rc0 (from -r requirements.txt (line 4))
[91m  Could not find a version that satisfies the requirement tensorflow-gpu==2.0.0rc0 (from -r requirements.txt (line 4)) (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)
[0m[91mNo matching distribution found for tensorflow-gpu==2.0.0rc0 (from -r requirements.txt (line 4))
[0m[91mYou are using pip version 18.1, however version 19.2.2 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
[0mRemoving intermediate container 229b8273366c
The command '/bin/sh -c /usr/local/bin/algorithmia-build' returned a non-zero code: 1
```"
31938,tf.function decorator with GradientTape >10x slower than tf.keras.model.Models.fit(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, reused code from https://www.tensorflow.org/beta/guide/effective_tf2 with minor modifications 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (originally found on Ubuntu 18.04)
- TensorFlow version (use command below): 2
- Python version: 3.6


**Describe the current behavior**
When using @tf.function decorator with  tf.GradientTape to compute updates on a model, I find that it takes 110 seconds per epoch. When I used model.fit(dataset), it only takes ~7 seconds. I'm not sure why it takes so much longer using the custom train function. This is code copied over from the Effective TensorFlow 2 documentation on tensorflow.org. To make sure this not specific to my personal machine, I used google colab with GPU enabled under Notebook settings. 

**Describe the expected behavior**
I would expect that using @tf.function decorator for the train function would compute the gradient update steps in a similar amount of time as model.fit(). I am wondering if the model is not being put onto GPU and is kept on CPU when trained this way.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
try:
  # %tensorflow_version only exists in Colab.
  %tensorflow_version 2.x
except Exception:
  pass

import tensorflow as tf
import time


class MyModel(tf.keras.models.Model):

    def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')
        self.flatten = tf.keras.layers.Flatten()
        self.d1 = tf.keras.layers.Dense(128, activation='relu')
        self.d2 = tf.keras.layers.Dense(10, activation='softmax')
        self.times_called = tf.Variable(0.0, trainable=False)

    def call(self, x):
        self.times_called.assign_add(1)
        x = self.conv1(x)
        x = self.flatten(x)
        x = self.d1(x)
        return self.d2(x)


optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
@tf.function
def train_step(model, dataset):
    for images, labels in dataset:
        with tf.GradientTape() as tape:
            preds = model(images)
            loss = loss_object(labels, preds)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))


if __name__ == ""__main__"":

    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)

    model = MyModel()
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
    model.fit(train_ds)

    model2 = MyModel()
    start = time.time()
    train_step(model2, train_ds)
    print(model.times_called)
    print(time.time() - start)
```

**Other info / logs**


This is the output I get when running this code on google colab:
```
1563/1563 [==============================] - 7s 4ms/step - loss: 1.4598
```
Time it takes to run with @tf.function 
```
<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1563.0>
110.49453139305115
```

"
31935,Simple way to manage and release GPU memory in colab,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab Ubuntu 18.04.2 LTS (Bionic Beaver)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Google Colab has tensorflow preinstalled
- TensorFlow version (use command below): tensorflow-gpu 1.14.0
- Python version: 3
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: Cuda compilation tools, release 10.0, V10.0.130
- GPU model and memory: Google Colab GPU Tesla T4, Memory: 15079MiB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
For my research work I need to build lots of different convolutional GAN models and train them.
For this I build temporary models inside functions and test them. Once the function is done executing the models are no longer needed.
A simple example:
```python
def test_model():
   model = build_model()
   inputs = ...
   outputs = model(inputs)
   with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      results = sess.run(outputs)
   # evaluate results

test_model() # call the test function, once this is over we never use the model again.
```
However currently the models persist (can be seen in %tensorboard) and continue to fill up
GPU memory. Eventually I start getting warning about GPU memory usage and start getting OOM errors. At this point I can't build new models or train any existing ones.

I have already tried lots of different suggestions on how to release GPU memory
https://github.com/tensorflow/tensorflow/issues/1578
https://github.com/tensorflow/tensorflow/issues/19731
https://github.com/tensorflow/tensorflow/issues/17048
and several stackoverflow suggestions to no effect.

This problem is specific to a Jupter notebook based workflow (such as on Google Colab).
A workflow that uses python files will not encounter this issue since all the GPU memory is released automatically once the python interpreter finishes.

**Describe the expected behavior**
- Please implement or suggest a way to release GPU memory being used by unneeded models in Google Colab/Jupter notebooks.
- It would be nice to be able to release memory being used by specific models (that are no longer necessary) rather than resetting the runtime every time I run out of memory (which is often).
- Some way to build models so that the GPU memory they occupy gets automatically released when they go out of scope would also be appreciated.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31932,How to create Python extension module that uses TensorFlow C API?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.14.0
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**



I've written a Python extension module that uses TensorFlow through the C API. I installed the API as described in https://www.tensorflow.org/install/lang_c. On its own, my module works correctly. But if I import my extension module and then also `import tensorflow`, Python crashes with this error.

```
[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/descriptor_database.cc:58] File already exists in database: tensorflow/core/protobuf/master.proto
[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/descriptor.cc:1370] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
libc++abi.dylib: terminating with uncaught exception of type google::protobuf::FatalException: CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
Abort trap: 6
```

I believe this happens because the C API includes its own complete copy of TensorFlow, so now I get two different copies loaded into the same process at the same time.

What is the solution to this? How can I have Python code that uses TensorFlow, and also invokes C code that uses TensorFlow?


**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31931,2019-08-23 nightlies missing `tf.summary` symbols,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

#### System information

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux (like Debian)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): `tf-nightly-2.0-preview==2.0.0.dev20190823`
- TensorFlow version (use command below): `2.0.0-dev20190823`
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

#### Describe the current behavior

```
$ virtualenv -q -p python3.6 ./ve
$ . ./ve/bin/activate
(ve) $ pip install -q tf-nightly-2.0-preview==2.0.0.dev20190823
(ve) $ python
Python 3.6.7 (default, Oct 21 2018, 08:08:16) 
[GCC 8.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> tf.__version__
'2.0.0-dev20190823'
>>> tf.summary.create_file_writer
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorboard.summary._tf.summary' has no attribute 'create_file_writer'
>>> tf.summary.write
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'tensorboard.summary._tf.summary' has no attribute 'write'
```

#### Describe the expected behavior

These symbols should exist (they’re documented, for one).

#### Code to reproduce the issue

See above.

#### Other info / logs

Not due to a regression in TensorBoard; our nightlies did not ship today
because our smoke test caught this.
"
31930,[TF 2.0] Different results of binary cross entropy loss of the same architecture ,"**System information**
- Tensorflow 2.0

**Describe the current behavior**
I have created model from tf.keras.models for binary classification problem. It is dense with sigmoid activation in the last layer and binary cross entropy loss. I got different loss values whether my last layer was:
```last_layer = Dense(1, activation='sigmoid')(previous_layer)```
or
```
last_layer = Dense(1)(previous_layer)
last_layer = sigmoid(last_layer)
```

If `loss=BinaryCrossentropy(from_logits=False)`:
-In the first case BCE do apply sigmoid before computing loss.
-In the second case BCE do not apply sigmoid before computing loss.

**Describe the expected behavior**
I believe that from user point of view it would consistent if both examples gives exactly the same output- there is no difference in architecture.

It would be better if sigmoid is applied, no matter what are the initialization arguments of BinaryCrossentropy.

**Code to reproduce the issue**
```
from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.keras.activations import sigmoid
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.losses import BinaryCrossentropy
import numpy as np


x_train = np.array([[1000]])
y_train = [0]

inp = Input(x_train.shape[1])
out = Dense(1, trainable=False, use_bias=False)(inp)
model = Model(inp, out)
model.get_layer('dense').set_weights([np.array([[1]])])
model.compile(optimizer='adam', loss=BinaryCrossentropy(from_logits=True))

print('\nFrom logits=True, no activation')
model.fit(x_train, y_train, epochs=1)
print('Prediction:')
print(model.predict(x_train))


inp = Input(x_train.shape[1])
out = Dense(1, trainable=False, use_bias=False)(inp)
out = sigmoid(out)
model = Model(inp, out)
model.get_layer('dense_1').set_weights([np.array([[1]])])
model.compile(optimizer='adam', loss=BinaryCrossentropy(from_logits=False))

print('\nFrom logits=False, separate sigmoid')
model.fit(x_train, y_train, epochs=1)
print('Prediction:')
print(model.predict(x_train))


inp = Input(x_train.shape[1])
out = Dense(1, trainable=False, use_bias=False, activation='sigmoid')(inp)
model = Model(inp, out)
model.get_layer('dense_2').set_weights([np.array([[1]])])
model.compile(optimizer='adam', loss=BinaryCrossentropy(from_logits=False))

print('\nFrom logits=False, sigmoid in dense')
model.fit(x_train, y_train, epochs=1)
print('Prediction:')
print(model.predict(x_train))
```


Output tf 2.0:
```
From logits=True, no activation
1/1 [==============================] - 0s 17ms/sample - loss: 1000.0000
Prediction:
[[1000.]]

From logits=False, separate sigmoid
Train on 1 samples
1/1 [==============================] - 0s 15ms/sample - loss: 1000.0000
Prediction:
[[1.]]

From logits=False, sigmoid in dense
Train on 1 samples
1/1 [==============================] - 0s 17ms/sample - loss: 15.3332
Prediction:
[[1.]]
```

output tf 1.14

```
From logits=True, no activation
1/1 [==============================] - 0s 39ms/sample - loss: 1000.0000
Prediction:
[[1000.]]

From logits=False, separate sigmoid
1/1 [==============================] - 0s 44ms/sample - loss: 1000.0000
Prediction:
[[1.]]

From logits=False, sigmoid in dense
1/1 [==============================] - 0s 57ms/sample - loss: 1000.0000
Prediction:
[[1.]]
```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
31928,[tf.estimator] Training with tf.estimator + tf.keras and tf.keras only yields inconsistent results,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `macOS 10.14.6 (18G87)`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/a**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): `v1.14.0-rc1-22-gaf24dc91b5 1.14.0`
- Python version: `3.7.3`
- Bazel version (if compiling from source): **n/a**
- GCC/Compiler version (if compiling from source): **n/a**
- CUDA/cuDNN version: **n/a**
- GPU model and memory: **none**

**Describe the current behavior**

Following the official guides ([[1]](https://www.tensorflow.org/guide/estimators#creating_estimators_from_keras_models), [[2]](https://www.tensorflow.org/beta/guide/migration_guide#using_a_custom_model_fn)), I was training [PSENet](https://github.com/sdll/psenet/tree/master/psenet) for text detection. Even though train metrics did improve to almost perfect levels and the loss remained stable and low after a while, the inference I got was gibberish.

PSENet works as follows: running the image through the feature pyramid network to obtain segmentation maps, it then applies a custom algorithm to extract bboxes. The images below show the segmentation maps, with yellow regions corresponding to the predicted text, and purple to everything else.

After 186 attempts to make it work on the AI Platform and $400 of GSoC credits, I realized that the problem was deeper than the implementation details, and decided to overfit on a single sample, using the `tf.keras` implementation of FPN from [`segmentation_models`](https://github.com/qubvel/segmentation_models) by @qubvel. I have tweaked his implementation for PSENet and ran into the same problems with `tf.estimator`, so it seems that `tf.estimator` is indeed the culprit.

For this sample image

![sample image](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/input.png)
 
and one of the labels

![sample label](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/label.png)

after 300 epochs, the same loss and optimizer, the predicted labels

- with a pure `tf.keras` implementation:

![](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/training-with-pure-keras.png)

- with the `tf.keras` model converted using `tf.keras.estimator.model_to_estimator`:

![](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/training-with-estimator-and-conversion.png)

- with the `tf.keras` model used in the `tf.estimator` model function:

![](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/training-with-estimator-and-no-conversion.png)

I have tried the `tf.keras`-in-`model_fn` setup on 10 000 images for 30-50 epochs, and the results are much worse than this, which is itself not perfect.

**Describe the expected behavior**

1. TensorFlow documentation should state clearly the preferred way to use `tf.keras` models inside `tf.estimator`, given the knowledge that `tf.estimator` is built on `tf.keras.layers` and thus the expectation that interops is seamless.

2. The discrepancy between training with `tf.keras` and `tf.estimator` + `tf.keras` should be minimal or non-existent

**Code to reproduce the issue**

The minimal failing example with the code and data is [here](https://github.com/sdll/tf.estimator-failing-example).

**Other info / logs**

https://github.com/tensorflow/tensorflow/issues/25670 is a related issue. Similar findings are documented [here](https://stackoverflow.com/questions/54910215/tensorflow-estimator-fails-to-converge-on-model-converted-from-keras-when-using).

I can also confirm that training with `tf.estimator` took longer than with pure `tf.keras`, in alignment with [other reports](https://stackoverflow.com/questions/56930892/tf-estimator-vs-tf-keras-speed-disparity) of this behavior.  


The model itself is sensible, and the following example shows that it does generalize well. For this input (not in the original data):

![IMAGE 2019-08-23 17:13:30](https://user-images.githubusercontent.com/17913919/63598985-549a2f80-c5c9-11e9-9cfe-2db8e72694f9.jpg)

- the output from [another implementation](https://github.com/liuheng92/tensorflow_PSENet) written in tf.slim is as follows:

![IMAGE 2019-08-23 17:14:20](https://user-images.githubusercontent.com/17913919/63599060-72679480-c5c9-11e9-84e1-0b95a9f12edf.jpg)

- the output from the [Pytorch implementation](https://github.com/whai362/PSENet) by the original authors is this:

![IMAGE 2019-08-23 17:16:54](https://user-images.githubusercontent.com/17913919/63599211-cecab400-c5c9-11e9-95e7-e8b48a5cc2e3.jpg)"
31927,tf2 load model issue,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nightly-2.0-preview==2.0.0.dev20190818
- Python version: 3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
When I try to load a model I obtain the following error:
```
ValueError: Unable to save the object ListWrapper([ListWrapper([]), ListWrapper([])]) (a list wrapper constructed to track trackable TensorFlow objects). A list element was replaced (__setitem__, __setslice__), deleted (__delitem__, __delslice__), or moved (sort). In order to support restoration on object creation, tracking is exclusively for append-only data structures.
```

**Describe the expected behavior**
I would like load the saved model.

**Code to reproduce the issue**
Google Colab link: [https://colab.research.google.com/drive/1e1TPBzhSipaAGI38gZF00kZMSi5Pg8fp](https://colab.research.google.com/drive/1e1TPBzhSipaAGI38gZF00kZMSi5Pg8fp)

Here a portion of source code:

```
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow import feature_column
from tensorflow.python.keras import Input, Model
from tensorflow.python.keras.layers import Dense, Dropout
from tensorflow.python.keras.optimizers import Adam
from tensorflow.python.keras.models import load_model
n = 200
df = pd.DataFrame(data={'a': [x for x in range(n)], 'b': [x for x in range(n+10,n+n+10)], 'labels': [int(x%2==0) for x in range(n)]})
df = df.astype({'b': str})
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  
  labels = dataframe.pop('labels')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds
train, test = train_test_split(df, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
train_ds = df_to_dataset(train)
val_ds = df_to_dataset(val, shuffle=False)
test_ds = df_to_dataset(test, shuffle=False)
feature_columns = []
feature_layer_inputs = {}
for c in df.columns:
  if c == 'labels':
    continue
  elif c == 'b':
    el = feature_column.categorical_column_with_vocabulary_list(c, df[c].unique(), default_value=-10)
    el_one_hot = feature_column.indicator_column(el)
    feature_columns.append(el_one_hot)
    feature_layer_inputs[c] = tf.keras.Input(shape=(1,), name=c, dtype=tf.string)
  elif c == 'a':
    feature_columns.append(feature_column.numeric_column(c, default_value=-10))
    feature_layer_inputs[c] = Input(shape=(1,), name=c)
feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
f_layer = feature_layer(feature_layer_inputs)
input = [v for v in feature_layer_inputs.values()]
x = Dense(2048, activation='relu')(f_layer)
x = Dropout(0.5)(x)
out = Dense(1, activation='sigmoid')(x)
model = Model(inputs=[input], outputs=out)
model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['binary_accuracy', 'AUC'])
model.fit(train_ds, validation_data=val_ds, epochs=10, verbose=0)
model.save('aaa.model')
new_model = load_model('aaa.model')
```

"
31926,Proper way to install TensorFlow Docker image with GPU support on Debian 10 (Debian Buster),"**System information**
- OS: Debian GNU/Linux 10 (buster) x86_64 
- Kernel: 4.19.0-5-amd64 
- CPU: Intel i7-6700 (8) @ 3.400GHz 
- GPU: Intel HD Graphics 530 
- GPU: NVIDIA GeForce RTX 2070

**Describe the problem**
I followed the [Tensorflow documentation](https://www.tensorflow.org/install/docker) in order to install a TensorFlow Docker image with GPU support on Debian Buster. For verification, I execute
```
docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi
```
as stated in the documentation. However, all I get is the following error message:
```
svdhero@ml-box:~$ docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi
docker: Error response from daemon: Unknown runtime specified nvidia.
```
Alternatively, I also tried
```
svdhero@ml-box:~$ docker run --rm nvidia/cuda nvidia-smi
docker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""exec: \""nvidia-smi\"": executable file not found in $PATH"": unknown.
```
without any luck, as one can see.

Previously, I installed my NVIDIA drivers successfully via
```
sudo apt install nvidia-driver
```
as one can see here:
```
svdhero@ml-box:~$ nvidia-smi 
Fri Aug 23 13:01:51 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.74       Driver Version: 418.74       CUDA Version: N/A      |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 2070    On   | 00000000:01:00.0 Off |                  N/A |
|  0%   39C    P8     3W / 175W |      0MiB /  7952MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```

I also installed docker successfully, as one can see here:
```
svdhero@ml-box:~$ docker --version
Docker version 19.03.1, build 74b1e89

svdhero@ml-box:~$ docker run --rm hello-world

Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the ""hello-world"" image from the Docker Hub.
    (amd64)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker ID:
 https://hub.docker.com/

For more examples and ideas, visit:
 https://docs.docker.com/get-started/

```
I have **not** installed any `nvidia-docker` or `nvidia-container-toolkit`, because the Tensorflow documentation clearly says:

> Note: The latest version of Docker includes native support for GPUs and nvidia-docker is not necessary.

This is a brand-new Debian install with no legacy packages installed.

So what am I doing wrong? I do realize that this is not a Tensorflow problem, but is there anything missing in the Tensorflow documentation? I followed the documentation exactly.

"
31925,[TF1.x][TPU] How to perform preprocessing steps for text classification for training on TPU?,"I have been trying since 5 days to train a simple text classification model on TPUs. But because of lack of documentation it is very difficult. I just can not perform tokenization, encoding, padding without `tf.py_func`. Please add some examples for doing these steps for TPU devices so that dumb people like me can understand TF. Will be greatly thankful to everyone at Google.
I am following this [tutorial](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches)."
31923, module 'tensorflow' has no attribute 'matvec',"I saw the file tensorflow/python/ops/math_ops.py  include matvec excample:

```
`# 2-D tensor `a`
  # [[1, 2, 3],
  #  [4, 5, 6]]
  a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

  # 1-D tensor `b`
  # [7, 9, 11]
  b = tf.constant([7, 9, 11], shape=[3])

  # `a` * `b`
  # [ 58,  64]
  c = tf.matvec(a, b)`
```

when I code that.

that error heppend:
`AttributeError: module 'tensorflow' has no attribute 'matvec'`"
31922,Running MobileNet SSD Float Detector,"Hello. 

I'm trying to run the model ""mobile_ssd_v2_float_coco.tflite"" in the [object detection app](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android). I have gotten the model to run on the GPU following the [tutorial](https://www.tensorflow.org/lite/performance/gpu) and adjusting image resolution and output accordingly. The output seems to follow the [old format](https://github.com/tensorflow/tensorflow/commit/f3785197b4de9466b48462f4f93b455c88dd622b#diff-c748d758eb53ac58d70a13e07e02c6db), where only outputLocations and outputClasses are produced. In the previous code it was also necessary to rescale the bounding boxes and there was a file ""box_priors"". I found the old file, it has 1917 entries, since this was the number of results produced. The new model on the other hand produces 2034 results. Is there a compatible ""box_priors"" file? Is this file even necessary?"
31921,Failed importing _pywrap_tensorflow,"Hi all, I have a problem.

After I installed tensorflow on my windows 10 machine with python 3.7.4
using this pip command: `pip install --user --upgrade tensorflow`

I want the CPU only version on a 64-bit version!

I was trying to verify the installation by running this command:
`python -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))""`

But that gives me the following error:

> Traceback (most recent call last):
>   File ""C:\Users\Alexander\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
>     fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
>   File ""C:\Users\Alexander\AppData\Local\Programs\Python\Python37-32\lib\imp.py"", line 296, in find_module
>     raise ImportError(_ERR_MSG.format(name), name=name)
> ImportError: No module named '_pywrap_tensorflow'
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""C:\Users\Alexander\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
>     from tensorflow.python import pywrap_tensorflow
>   File ""C:\Users\Alexander\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
>     _pywrap_tensorflow = swig_import_helper()
>   File ""C:\Users\Alexander\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
>     import _pywrap_tensorflow
> ModuleNotFoundError: No module named '_pywrap_tensorflow'
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""<string>"", line 1, in <module>
>   File ""C:\Users\Alexander\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
>     from tensorflow.python import *
>   File ""C:\Users\Alexander\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\__init__.py"", line 72, in <module>
>     raise ImportError(msg)
> ImportError: Traceback (most recent call last):
>   File ""C:\Users\Alexander\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
>     fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
>   File ""C:\Users\Alexander\AppData\Local\Programs\Python\Python37-32\lib\imp.py"", line 296, in find_module
>     raise ImportError(_ERR_MSG.format(name), name=name)
> ImportError: No module named '_pywrap_tensorflow'
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""C:\Users\Alexander\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\__init__.py"", line 66, in <module>
>     from tensorflow.python import pywrap_tensorflow
>   File ""C:\Users\Alexander\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
>     _pywrap_tensorflow = swig_import_helper()
>   File ""C:\Users\Alexander\AppData\Local\Programs\Python\Python37-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
>     import _pywrap_tensorflow
> ModuleNotFoundError: No module named '_pywrap_tensorflow'
> 
> 
> Failed to load the native TensorFlow runtime.
> 
> See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error
> 
> for some common reasons and solutions.  Include the entire stack trace
> above this error message when asking for help.
 
I have already tried reinstalling: Microsoft Visual C++ 2015 Redistributable Update 3, according to this issue here: [https://github.com/tensorflow/tensorflow/issues/7529](https://github.com/tensorflow/tensorflow/issues/7529)
But that didn't work....

I am not missing a the dll file called: MSVCP140.DLL!

How can I fix this?"
31919,tf.GradientTape().tape() is returning None on the generator loss,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: I am using Colab
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Not applicable
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: `2.0.0-beta1`
- **Python version**: 3.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: [This Colab notebook](https://colab.research.google.com/drive/1iIGWaktKZfDmxpjl4XARsBinTHcgd2Xf)

### Describe the problem

I am trying to reproduce [this simplistic GANs example](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f) using TensorFlow 2.0. Everything is working as _expected except for the generator network_. Here's one forward and backward pass of the generator network:

```python
with tf.GradientTape() as tape:
  g_fake_data = G(gen_input)
  dg_fake_decision = D(get_moments(g_fake_data.numpy().T).reshape((1,4)))
  g_error = criterion(dg_fake_decision, np.ones((1,1)))
tape.gradient(g_error, G.trainable_weights)
```
The gradients are all coming as `None`. I have tried to do `tape.watch(G.trainable_weights)` as well but it does not help. My suspect is that the `GradientTape` context manager is unable to keep track of the appropriate dependencies to compute the gradients. I have tried to move the `dg_fake_decision = D(get_moments(g_fake_data.numpy().T).reshape((1,4)))` step to `with tape.stop_recording()` but it still does not help. 

Any suggestions on this would be very helpful. I hope this would be helpful for the community specifically for them who are willing to dig deep with automatic differentiation. 
"
31917,Intermittent crash with CUDA_ERROR_LAUNCH_FAILED failure,"### System information
- **OS Platform and Distribution**:
on (Linux-based) ML-engine runtime version 1.12, which means it is using TensorFlow 1.12, with Python 3.6.
- **CUDA/cuDNN version**: 
CUDA Version: 10.1, with cuDNN: libcudnn.so.7.4.2
- **GPU model and memory**:
Tesla K80
- **Exact command to reproduce**:
running custom code on object segmentation task.

### The problem
The problem is the training crash after reaching thousands of steps (few hours). The crash is intermittent. The same code and dataset when rerun is OK on another occasion. This problems happened quite frequently (>20x) in my testings. One observation is that this does not happen for earlier TensorFlow 1.8. Current workaround for me is to downgrade to TF 1.8. This could be a bug on the library or CUDA driver.  The same problem happened in workstation offline .

### The Logs
```
Instructions for updating:
Use standard file APIs to delete files with this prefix.
INFO:tensorflow:Recording summary at step 1239.
INFO:tensorflow:global step 1240: loss = 0.2247 (1.349 sec/step)
INFO:tensorflow:global step 1260: loss = 0.2513 (1.189 sec/step)
INFO:tensorflow:global step 1280: loss = 0.3016 (1.181 sec/step)
INFO:tensorflow:Recording summary at step 1291.
INFO:tensorflow:global step 1300: loss = 0.2227 (1.151 sec/step)
INFO:tensorflow:global step 1320: loss = 0.2227 (1.139 sec/step)
INFO:tensorflow:global step 1340: loss = 0.2224 (1.273 sec/step)
INFO:tensorflow:Recording summary at step 1342.
INFO:tensorflow:global step 1360: loss = 0.2220 (1.186 sec/step)
INFO:tensorflow:global step 1380: loss = 0.2297 (1.157 sec/step)
INFO:tensorflow:Recording summary at step 1393.
INFO:tensorflow:global step 1400: loss = 0.2217 (1.160 sec/step)
INFO:tensorflow:global step 1420: loss = 0.2213 (1.169 sec/step)
INFO:tensorflow:global step 1440: loss = 0.2209 (1.134 sec/step)
INFO:tensorflow:Recording summary at step 1443.
2019-08-01 01:43:15.655519: E tensorflow/stream_executor/cuda/cuda_driver.cc:1131] failed to enqueue async memcpy from host to device: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure; GPU dst: 0x71133f800; host src: 0x7ff986bfcc80; size: 131072=0x20000
2019-08-01 01:43:15.655586: E tensorflow/stream_executor/cuda/cuda_driver.cc:1000] could not wait stream on event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-08-01 01:43:15.655630: E tensorflow/stream_executor/cuda/cuda_driver.cc:1000] could not wait stream on event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-08-01 01:43:15.655630: E tensorflow/stream_executor/cuda/cuda_driver.cc:1000] could not wait stream on event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-08-01 01:43:15.655644: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-08-01 01:43:15.655731: I tensorflow/stream_executor/stream.cc:5027] [stream=0x5c23080,impl=0x5c23120] did not memcpy host-to-device; source: 0x7ff97e0fd7c0
2019-08-01 01:43:15.655731: I tensorflow/stream_executor/stream.cc:5027] [stream=0x5c23080,impl=0x5c23120] did not memcpy host-to-device; source: 0x7ff97e6241c0
2019-08-01 01:43:15.655717: I tensorflow/stream_executor/stream.cc:5027] [stream=0x5c23080,impl=0x5c23120] did not memcpy host-to-device; source: 0x7ff97e62e9c0
2019-08-01 01:43:15.655755: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1
```

### Additional information logs:
The ML team support kindly provides machine debug information as followings:
```
""The VM logs of failed jobs all have the same error, like
 I 2019-07-25T17:00:20.282471416Z [37582.281240] NVRM: Xid (PCI:0000:00:04): 13, Graphics Exception:  MISSING_INLINE_DATA\r\n 
I 2019-07-25T17:00:20.282477291Z [37582.289153] NVRM: Xid (PCI:0000:00:04): 13, Graphics Exception: ESR 0x404600=0x80000002\r\n 
I 2019-07-25T17:00:20.282557397Z [37582.297688] NVRM: Xid (PCI:0000:00:04): 13, Graphics Exception: ChID 0017, Class 0000a1c0, Offset 000001b4, Data 00002000\r\n ""
```
FYI, I did not succeed to reproduce the error with  cuda-memcheck or cuda-gdb or CUDA_DEVICE_WAITS_ON_EXCEPTION=1.
This issue is probably similar to ##20356

"
31915,tf logging error ,"**System information**
```
== check python ===================================================
python version: 3.6.0
python branch: 
python build version: ('default', 'Dec 23 2016 12:22:00')
python compiler version: GCC 4.4.7 20120313 (Red Hat 4.4.7-1)
python implementation: CPython


== check os platform ===============================================
os: Linux
os kernel version: #1 SMP Wed Apr 12 15:04:24 UTC 2017
os release version: 3.10.0-514.16.1.el7.x86_64
os platform: Linux-3.10.0-514.16.1.el7.x86_64-x86_64-with-centos-7.3.1611-Core
linux distribution: ('CentOS Linux', '7.3.1611', 'Core')
linux os distribution: ('centos', '7.3.1611', 'Core')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='dfdd64ab2304', release='3.10.0-514.16.1.el7.x86_64', version='#1 SMP Wed Apr 12 15:04:24 UTC 2017', machine='x86_64', processor='x86_64')
architecture: ('64bit', 'ELF')
machine: x86_64


== are we in docker =============================================
Yes

== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy                         1.16.4   
protobuf                      3.9.0    
tensorflow-estimator          1.14.0   
tensorflow-gpu                1.14.0   

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 1.14.0
tf.version.GIT_VERSION = v1.14.0-rc1-22-gaf24dc91b5
tf.version.COMPILER_VERSION = 4.8.5
Sanity check: array([1], dtype=int32)
     21634:     find library=libpython3.6m.so.1.0 [0]; searching
     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/tls/x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/tls:/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib              (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/bin/python3)
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/tls/x86_64/libpython3.6m.so.1.0
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/tls/libpython3.6m.so.1.0
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/x86_64/libpython3.6m.so.1.0
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/libpython3.6m.so.1.0
     21634:
     21634:     find library=libpthread.so.0 [0]; searching
     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib                (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/bin/python3)
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/libpthread.so.0
     21634:      search path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64/tls/x86_64:/usr/local/cuda/lib64/tls:/usr/local/cuda/lib64/x86_64:/usr/local/cuda/lib64:tls/x86_64:tls:x86_64:           (LD_LIBRARY_PATH)
     21634:       trying file=/usr/local/nvidia/lib/tls/x86_64/libpthread.so.0
     21634:       trying file=/usr/local/nvidia/lib/tls/libpthread.so.0
     21634:       trying file=/usr/local/nvidia/lib/x86_64/libpthread.so.0
     21634:       trying file=/usr/local/nvidia/lib/libpthread.so.0
/../../x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../..          (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so)
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../tls/x86_64/liblzma.so.5
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../tls/liblzma.so.5
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../x86_64/liblzma.so.5
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../liblzma.so.5
     21634:
     21634:
     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../liblzma.so.5
     21634:
     21634:
     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so
     21634:
     21634:
     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so
     21634:
     21634:
     21634:     calling init: /home/luban/.local/lib/python3.6/site-packages/numpy/fft/fftpack_lite.cpython-36m-x86_64-linux-gnu.so
     21634:
     21634:
     21634:     calling init: /home/luban/.local/lib/python3.6/site-packages/numpy/random/mtrand.cpython-36m-x86_64-linux-gnu.so
     21634:
     21634:     find library=libssl.so.1.0.0 [0]; searching
     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../..           (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so)
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../libssl.so.1.0.0
     21634:
     21634:     find library=libcrypto.so.1.0.0 [0]; searching
     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../..           (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so)
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../libcrypto.so.1.0.0
     21634:
     21634:
     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../libcrypto.so.1.0.0
     21634:
     21634:
     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../libssl.so.1.0.0
     21634:
     21634:
     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so
     21634:
     21634:
     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_blake2.cpython-36m-x86_64-linux-gnu.so
     21634:
     21634:
     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_sha3.cpython-36m-x86_64-linux-gnu.so
     21634:
     21634:
     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_bisect.cpython-36m-x86_64-linux-gnu.so
     21634:
     21634:
     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_random.cpython-36m-x86_64-linux-gnu.so
     21634:
     21634:     find library=libtensorflow_framework.so.1 [0]; searching
     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/tls/x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/tls:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../tls/x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../tls:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/..             (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64/libtensorflow_framework.so.1
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/libtensorflow_framework.so.1
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64/libtensorflow_framework.so.1
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so.1
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/tls/x86_64/libtensorflow_framework.so.1
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/tls/libtensorflow_framework.so.1
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/x86_64/libtensorflow_framework.so.1
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/libtensorflow_framework.so.1
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../tls/x86_64/libtensorflow_framework.so.1
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../tls/libtensorflow_framework.so.1
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../x86_64/libtensorflow_framework.so.1
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so.1
     21634:
     21634:     find library=libstdc++.so.6 [0]; searching
     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/..           (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/libstdc++.so.6
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../libstdc++.so.6
     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib                (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/bin/python3)
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/libstdc++.so.6
     21634:      search path=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:tls/x86_64:tls:x86_64:         (LD_LIBRARY_PATH)
     21634:       trying file=/usr/local/nvidia/lib/libstdc++.so.6
     21634:       trying file=/usr/local/nvidia/lib64/libstdc++.so.6
     21634:       trying file=/usr/local/cuda/lib64/libstdc++.so.6
     21634:       trying file=tls/x86_64/libstdc++.so.6
     21634:       trying file=tls/libstdc++.so.6
     21634:       trying file=x86_64/libstdc++.so.6
     21634:       trying file=libstdc++.so.6
     21634:      search cache=/etc/ld.so.cache
     21634:       trying file=/lib64/libstdc++.so.6
     21634:
     21634:     find library=libgcc_s.so.1 [0]; searching
     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/..           (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/libgcc_s.so.1
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../libgcc_s.so.1
     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib                (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/bin/python3)
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/libgcc_s.so.1
     21634:
     21634:
     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/libgcc_s.so.1
     21634:
     21634:
     21634:     calling init: /lib64/libstdc++.so.6
     21634:
     21634:
     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so.1
     21634:
     21634:     find library=libhdfs.so [0]; searching
     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/..          (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../libhdfs.so
     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/..           (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/libhdfs.so
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../libhdfs.so
     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib                (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/bin/python3)
     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/libhdfs.so
     21634:      search path=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:tls/x86_64:tls:x86_64:         (LD_LIBRARY_PATH)
     21634:       trying file=/usr/local/nvidia/lib/libhdfs.so
     21634:       trying file=/usr/local/nvidia/lib64/libhdfs.so
     21634:       trying file=/usr/local/cuda/lib64/libhdfs.so
     21634:       trying file=tls/x86_64/libhdfs.so
     21634:       trying file=tls/libhdfs.so
     21634:       trying file=x86_64/libhdfs.so
     21634:       trying file=libhdfs.so
     21634:      search cache=/etc/ld.so.cache
     21634:      search path=/lib64/tls/x86_64:/lib64/tls:/lib64/x86_64:/lib64:/usr/lib64/tls/x86_64:/usr/lib64/tls:/usr/lib64/x86_64:/usr/lib64                (system search path)
     21634:       trying file=/lib64/tls/x86_64/libhdfs.so
     21634:       trying file=/lib64/tls/libhdfs.so
     21634:       trying file=/lib64/x86_64/libhdfs.so
     21634:     calling fini: /lib64/libutil.so.1 [0]
     21634:
     21634:
     21634:     calling fini: /lib64/libdl.so.2 [0]
     21634:
     21634:
     21634:     calling fini: /lib64/libpthread.so.0 [0]
     21634:

== env ==========================================================
LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Aug 23 12:51:21 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.72       Driver Version: 410.72       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P40           Off  | 00000000:02:00.0 Off |                  N/A |
| N/A   23C    P0    53W / 250W |     10MiB / 22919MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P40           Off  | 00000000:03:00.0 Off |                  N/A |
| N/A   20C    P8    10W / 250W |     10MiB / 22919MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla P40           Off  | 00000000:83:00.0 Off |                  N/A |
| N/A   46C    P0    73W / 250W |  22041MiB / 22919MiB |     34%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla P40           Off  | 00000000:84:00.0 Off |                  N/A |
| N/A   44C    P0   114W / 250W |  21935MiB / 22919MiB |     97%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-10.0/lib64/libcudart_static.a
/usr/local/cuda-10.0/lib64/libcudart.so.10.0.130
/usr/local/cuda-10.0/doc/man/man7/libcudart.7
/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7

== tensorflow installed from info ==================

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 6, 0, 'final', 0)

== bazel version  ===============================================
Build label: 0.16.1
Build time: Mon Aug 13 13:43:36 2018 (1534167816)
Build timestamp: 1534167816
Build timestamp as int: 1534167816
```

**Describe the current behavior**
using `evaluate_generateor` do evaluation, tf logging has error on flush info to stream.

**Describe the expected behavior**

**Code to reproduce the issue**


**Other info / logs**
```
--- Logging error ---
Traceback (most recent call last):
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 989, in emit
    stream.write(msg)
ValueError: I/O operation on closed file.
Call stack:
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py"", line 244, in __del__
    .format(pretty_printer.node_names[node_id]))
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py"", line 166, in warning
    get_logger().warning(msg, *args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1313, in warning
    self._log(WARNING, msg, args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1437, in _log
    self.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1447, in handle
    self.callHandlers(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1509, in callHandlers
    hdlr.handle(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 891, in handle
    return self._current_handler.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 858, in handle
    self.emit(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 829, in emit
    super(PythonHandler, self).emit(record)
Message: ""Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense1.layer.kernel""
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 989, in emit
    stream.write(msg)
ValueError: I/O operation on closed file.
Call stack:
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py"", line 244, in __del__
    .format(pretty_printer.node_names[node_id]))
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py"", line 166, in warning
    get_logger().warning(msg, *args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1313, in warning
    self._log(WARNING, msg, args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1437, in _log
    self.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1447, in handle
    self.callHandlers(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1509, in callHandlers
    hdlr.handle(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 891, in handle
    return self._current_handler.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 858, in handle
    self.emit(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 829, in emit
    super(PythonHandler, self).emit(record)
Message: ""Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense1.layer.bias""
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 989, in emit
    stream.write(msg)
ValueError: I/O operation on closed file.
Call stack:
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py"", line 244, in __del__
    .format(pretty_printer.node_names[node_id]))
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py"", line 166, in warning
    get_logger().warning(msg, *args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1313, in warning
    self._log(WARNING, msg, args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1437, in _log
    self.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1447, in handle
    self.callHandlers(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1509, in callHandlers
    hdlr.handle(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 891, in handle
    return self._current_handler.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 858, in handle
    self.emit(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 829, in emit
    super(PythonHandler, self).emit(record)
Message: ""Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).lstm1.forward_layer.cell.kernel""
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 989, in emit
    stream.write(msg)
ValueError: I/O operation on closed file.
Call stack:
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py"", line 244, in __del__
    .format(pretty_printer.node_names[node_id]))
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py"", line 166, in warning
    get_logger().warning(msg, *args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1313, in warning
    self._log(WARNING, msg, args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1437, in _log
    self.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1447, in handle
    self.callHandlers(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1509, in callHandlers
    hdlr.handle(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 891, in handle
    return self._current_handler.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 858, in handle
    self.emit(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 829, in emit
    super(PythonHandler, self).emit(record)
Message: ""Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).lstm1.forward_layer.cell.recurrent_kernel""
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 989, in emit
    stream.write(msg)
ValueError: I/O operation on closed file.
Call stack:
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py"", line 244, in __del__
    get_logger().warning(msg, *args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1313, in warning
    self._log(WARNING, msg, args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1437, in _log
    self.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1447, in handle
    self.callHandlers(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1509, in callHandlers
    hdlr.handle(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 891, in handle
    return self._current_handler.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 858, in handle
    self.emit(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 829, in emit
    super(PythonHandler, self).emit(record)
Message: ""Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).lstm1.forward_layer.cell.recurrent_kernel""
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 989, in emit
    stream.write(msg)
ValueError: I/O operation on closed file.
Call stack:
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py"", line 244, in __del__
    .format(pretty_printer.node_names[node_id]))
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py"", line 166, in warning
    get_logger().warning(msg, *args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1313, in warning
    self._log(WARNING, msg, args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1437, in _log
    self.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1447, in handle
    self.callHandlers(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1509, in callHandlers
    hdlr.handle(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 891, in handle
    return self._current_handler.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 858, in handle
    self.emit(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 829, in emit
    super(PythonHandler, self).emit(record)
Message: ""Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).lstm1.forward_layer.cell.bias""
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 989, in emit
    stream.write(msg)
ValueError: I/O operation on closed file.
Call stack:
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py"", line 244, in __del__
    .format(pretty_printer.node_names[node_id]))
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py"", line 166, in warning
    get_logger().warning(msg, *args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1313, in warning
    self._log(WARNING, msg, args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1437, in _log
    self.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1447, in handle
    self.callHandlers(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1509, in callHandlers
    hdlr.handle(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 891, in handle
    return self._current_handler.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 858, in handle
    self.emit(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 829, in emit
    super(PythonHandler, self).emit(record)
Message: ""Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).lstm1.backward_layer.cell.kernel""
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 989, in emit
    stream.write(msg)
ValueError: I/O operation on closed file.
Call stack:
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py"", line 244, in __del__
    .format(pretty_printer.node_names[node_id]))
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py"", line 166, in warning
    get_logger().warning(msg, *args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1313, in warning
    self._log(WARNING, msg, args, **kwargs)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1437, in _log
    self.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1447, in handle
    self.callHandlers(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 1509, in callHandlers
    hdlr.handle(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 891, in handle
    return self._current_handler.handle(record)
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 858, in handle
    self.emit(record)
  File ""/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py"", line 829, in emit
    super(PythonHandler, self).emit(record)
Message: ""Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).lstm1.backward_layer.cell.recurrent_kernel""
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File ""/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py"", line 989, in emit
    stream.write(msg)
ValueError: I/O operation on closed file.
```
"
31913,All_reduce of collective_ops hangs in a distributed environment,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.6.1810
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): No
- TensorFlow version (use command below): v1.13.1-0-g6612da8951
- Python version: 3.6.8
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**

The monitored session hangs in there fetching the `reduced_weight`.

**Describe the expected behavior**

The all_reduce tensor `reduced_weight` gives proper answer on all workers.

**Code to reproduce the issue**
```python
""""""Illustrate AllReduce""""""

import multiprocessing as mp

MP_METHOD = 'fork'  # 'fork' (UNIX), 'spawn' (WINDOWS);
NUM_PROCESSES = 2

def process_fn(worker_hosts, task_index):
    """"""allreduce process""""""
    import time
    import tensorflow as tf
    from tensorflow.python.ops import collective_ops

    num_workers = len(worker_hosts)

    cluster_spec = tf.train.ClusterSpec({'worker': worker_hosts})

    server = tf.train.Server(cluster_spec,
                             job_name='worker', task_index=task_index)
    group_key = 0
    instance_key = 0
    with tf.Graph().as_default():
        weights = list()
        reduced_weight = list()
        for worker_index in range(num_workers):
            with tf.variable_scope('worker{}'.format(worker_index)), \
                    tf.device('job:worker/task:{}/device:CPU:0'.format(
                            worker_index)):
                weight = tf.get_variable('weight', shape=[])
                weights.append(weight)
                if worker_index == task_index:
                    reduced_weight = collective_ops.all_reduce(
                        weight, num_workers, group_key, instance_key,
                        'Add', 'Div')

        session_creator = tf.train.ChiefSessionCreator(master=server.target)
        with tf.train.MonitoredSession(session_creator=session_creator) \
                as mon_sess:
            print('task {} have {}'.format(task_index, mon_sess.run(weights)))
            result = mon_sess.run(reduced_weight)
        print('task {} reduce {}'.format(task_index, result))
        time.sleep(1)

def start_process():
    """"""start process""""""
    port = 60000
    host_fmt = 'localhost:{}'
    worker_hosts = list()
    for process_index in range(NUM_PROCESSES):
        worker_hosts.append(host_fmt.format(port + process_index))
    mp_ctx = mp.get_context(MP_METHOD)
    processes = list()
    for process_index in range(NUM_PROCESSES):
        process = mp_ctx.Process(target=process_fn,
                                 args=(worker_hosts, process_index,))
        processes.append(process)
        process.start()
    for process in processes:
        process.join()

if __name__ == '__main__':
    start_process()
```

**Other info / logs**

```console
(tf-1.13-py3) [huwh1@huwh1-centos worksync]$ python ./tf_distribute_collective_ops.py
2019-08-23 10:55:20.150797: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-23 10:55:20.152951: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-23 10:55:20.163464: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
2019-08-23 10:55:20.163852: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4364100 executing computations on platform Host. Devices:
2019-08-23 10:55:20.163883: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-08-23 10:55:20.165614: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:60000, 1 -> localhost:60001}
2019-08-23 10:55:20.165828: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
2019-08-23 10:55:20.166148: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4363fe0 executing computations on platform Host. Devices:
2019-08-23 10:55:20.166174: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-08-23 10:55:20.166519: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:60001
2019-08-23 10:55:20.167632: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:60000, 1 -> localhost:60001}
2019-08-23 10:55:20.168829: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:60000
WARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-08-23 10:55:20.262106: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session c45b1693e334d401 with config: 
2019-08-23 10:55:20.269965: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session a7c551a16b557bd8 with config: 
task 1 have [-0.82924074, -0.72853804]
task 0 have [-0.82924074, -0.72853804]
```
The `collective_ops.all_reduce` seems to be referenced only once in [build_collective_reduce](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/distribute/cross_device_utils.py#L360-L362), where the instruction suggests ""input_tensors: tensors within a single worker graph that are to be reduced together; must be one per device."" Is that mean the `all_reduce` is only applicable to [In-graph replication](https://github.com/tensorflow/docs/blob/r1.9/site/en/deploy/distributed.md#replicated-training)?"
31912,Not able to build GPU custom op example ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.5
- CUDA/cuDNN version: 10.0
- GPU model and memory: GeForce GTX 980M

**Describe the current behavior**
Following the [instructions here](https://www.tensorflow.org/guide/extend/op#gpu_support) I navigate to ```tensorflow/tensorflow/examples/adding_an_op``` and run
```
nvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc  ${TF_CFLAGS[@]} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC
```

and I get the error

```
In file included from cuda_op_kernel.cu.cc:19:0:
/home/alex/.local/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/util/gpu_kernel_helper.h:22:53: fatal error: third_party/gpus/cuda/include/cuda_fp16.h: No such file or directory
compilation terminated.
```
"
31911,Data Augmentation from PREPROCESSING_FUNCTION_MAP?,How do we utilize Data Augmentation features in the Tensorflow API as indicated from the PREPROCESSING_FUNCTION_MAP in preprocessor_builder.py? Is there a file we have to modify in the /object_detection folder or is there additional command lines we need to be aware of?
