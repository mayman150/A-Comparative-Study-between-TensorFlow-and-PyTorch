Issue Number,Issue Title,Issue Body
62229,Loading MoViNet saved_model in tensorflow 2.7 throws FileNotFoundError: Op type not registered 'DisableCopyOnRead' in binary running,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.7

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Jetson Nano Maxwell Architecture GPU - 4GB

### Current behavior?

I am trying to load an exported MoViNet saved model in tensorflow version 2.7. The model loads fine in tensorflow version 2.10. However, I need to run the model on Nvidia's Jetson nano which does not support tensorflow version greater than 2.7. Loading the model in tf 2.7 gives me the error `Op type not registered 'DisableCopyOnRead'`. This issue might be due to version compatibility  since I exported the model using tensorflow 2.10 and tf-models-official. For exporting model I used :
[this notebook's code](https://github.com/tensorflow/models/blob/master/official/projects/movinet/movinet_streaming_model_training_and_inference.ipynb).
I tried to export the model using tf 2.7 but tf-models-official does not work with tf<2.10. 

Are there any workarounds by converting the model in some other format so that I can run it on my Jetson Nano's tensorflow version 2.7.
Any help would be greatly appreciated. Much thanks.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

model = tf.saved_model.load(""path/to/saved_model"")
```


### Relevant log output

```shell
KeyError                                  Traceback (most recent call last)
[c:\Users\Hammad\anaconda3\envs\temp\lib\site-packages\tensorflow\python\framework\ops.py](file:///C:/Users/Hammad/anaconda3/envs/temp/lib/site-packages/tensorflow/python/framework/ops.py) in _get_op_def(self, type)
   4097     try:
-> 4098       return self._op_def_cache[type]
   4099     except KeyError:

KeyError: 'DisableCopyOnRead'

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
[c:\Users\Hammad\anaconda3\envs\temp\lib\site-packages\tensorflow\python\saved_model\load.py](file:///C:/Users/Hammad/anaconda3/envs/temp/lib/site-packages/tensorflow/python/saved_model/load.py) in load_internal(export_dir, tags, options, loader_cls, filters)
    938         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,
--> 939                             ckpt_options, options, filters)
    940       except errors.NotFoundError as err:

[c:\Users\Hammad\anaconda3\envs\temp\lib\site-packages\tensorflow\python\saved_model\load.py](file:///C:/Users/Hammad/anaconda3/envs/temp/lib/site-packages/tensorflow/python/saved_model/load.py) in __init__(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)
    138         function_deserialization.load_function_def_library(
--> 139             meta_graph.graph_def.library, wrapper_function=_WrapperFunction))
    140     self._checkpoint_options = ckpt_options

[c:\Users\Hammad\anaconda3\envs\temp\lib\site-packages\tensorflow\python\saved_model\function_deserialization.py](file:///C:/Users/Hammad/anaconda3/envs/temp/lib/site-packages/tensorflow/python/saved_model/function_deserialization.py) in load_function_def_library(library, load_shared_name_suffix, wrapper_function)
    387     with graph.as_default():
--> 388       func_graph = function_def_lib.function_def_to_graph(copy)
...
    943             ""from the computational device. Consider setting the ""
    944             ""`experimental_io_device` option in `tf.saved_model.LoadOptions` ""

FileNotFoundError: Op type not registered 'DisableCopyOnRead' in binary running on LAPTOP-V4DO1PQN. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
 You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.
```
"
62228,TFLITE does not compile with CMake in Visual Studio 2019,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-master

### Custom code

Yes

### OS platform and distribution

Windows 7 SP1 x64

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

NVIDIA GeForce 1660 SUPER 6 GB

### Current behavior?

Cannot find #include <sys/mman.h> in ""tensorflow/lite/kernels/internal/optimized/fully_connected_4bit.h""
Expected successful build.

P.S. There's TFLITE_MMAP_DISABLED condition in this file, but I didn't find it in CMakeLists.txt or anywhere else (except Bazel config files that are not used while building with CMake).

### Standalone code to reproduce the issue

```shell
1. Open latest VS 2019 on 64-bit Windows 7 (maybe newer versions too).
2. Download tensorflow-master as zip, unpack.
3. Open tensoflow/lite as CMake project.
4. Right click on root CMakeLists.txt and choose ""Build"".
```


### Relevant log output

```shell
Cannot find #include <sys/mman.h> in ""tensorflow/lite/kernels/internal/optimized/fully_connected_4bit.h""
```
"
62227,TensorflowLite C API not linking with TensorflowLite_Flex delegate (Automatically or manually),"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution**: Windows 11
-   **Mobile device**: Additionally happens on Android 14 Pixel 7a
-   **TensorFlowLite installed from (source or binary)**: C API build with CMake and Tensorflowlite_flex built with bazel
-   **TensorFlow version (use command below)**: 2.14
-   **Bazel version (if compiling from source)**: Bazelisk version: v1.18.0
-   **GCC/Compiler version (if compiling from source)**: Visual Studio 17.7.4
-   **CUDA/cuDNN version**: N/A
-   **GPU model and memory**: N/A
-   **Exact command to reproduce**: N/A

Using Cmake To build and link the TensorflowLite C API Works perfectly with any tflite model signature not using Flex operators. However, after linking the C API with tensorflowlite_flex, using the following as part of a CMakeLists.txt:
```cmake
if(${CMAKE_SYSTEM_NAME} STREQUAL ""Windows"")
  set_target_properties(tensorflowlite_c PROPERTIES 
    IMPORTED_LOCATION_DEBUG ""${TFLITE_ROOT}/lib/windows/Debug/tensorflowlite_c.dll""
    IMPORTED_IMPLIB_DEBUG ""${TFLITE_ROOT}/lib/windows/Debug/tensorflowlite_c.lib""

    IMPORTED_LOCATION_RELEASE ""${TFLITE_ROOT}/lib/windows/Release/tensorflowlite_c.dll""
    IMPORTED_IMPLIB_RELEASE ""${TFLITE_ROOT}/lib/windows/Release/tensorflowlite_c.lib""
    )

  set_target_properties(tensorflowlite_flex PROPERTIES 
    IMPORTED_LOCATION ""${TFLITE_ROOT}/lib/windows/tensorflowlite_flex.dll""
    IMPORTED_IMPLIB ""${TFLITE_ROOT}/lib/windows/tensorflowlite_flex.dll.if.lib""
    )
  target_link_libraries(tensorflowlite_c INTERFACE tensorflowlite_flex)
endif()
```  
, the running process outputs: ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.

My final approach was to use (what I guessed) was the auto linking code that exists in the C++ code (namely:
```cpp
auto hdll = LoadLibrary(""tensorflowlite_flex.dll"");

auto flex_fp = reinterpret_cast<TfLiteDelegatePtr(*)()>(
    GetProcAddress(hdll, ""TF_AcquireFlexDelegate""));
if (flex_fp == NULL) {
  throw std::runtime_error(""flex_fp couldn't be run"");
}

flex_ = flex_fp(); // flex_ is a member (not shown here) // This line gets the message: INFO: Created TensorFlow Lite delegate for select TF ops.

TfLiteInterpreterOptionsAddDelegate(options_.get(), flex_.get()); // *THIS* is the important line

```
). If the final line is commented out we get the nice INFO message that the delegate has been created but end up with the same warning message. Alternatively, if the last line is included we get an invalid memory access exception. 

I am going to switch to the C++ API for tensorflow lite and see if that works as I am under a tight deadline but any help would be much appreciated as the stable ABI for the C API is far better for my PhD.


Additionally, I built ""tensorflow/lite/delegates/flex:delegate"" with bazelisk (bazel v1.18.0) but (after waiting an extremely long time for it to compile) I cannot track down what it built and did not change anything.


(Note, some small changes to CMakeFiles were required to build tensorflowlite_c.dll and the exact binary I have used can be obtained from [here](https://github.com/SagaraBattousai/tflite-clib-builder/releases/download/v1.0.1/tensorflowlite_c-winx64-Android.zip) )



"
62226,crop bounding boxes,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behaviour?

I think that TF may be missing a **utility to crop several bounding boxes**. It should be **vectorized for better performance**, as cropping one-by-one is known to be slow. I have not found it in API (but maybe is out there?).

The use case is clear â€“ extracting patches of annotated images for object detection.

The util function should take a tensor of boxes `(n_boxes,4)` and a window/patch `(4,)` and crop all the boxes to a given window. Note that we want to **crop boxes, not an image**.

So, starting from this:
![image](https://github.com/tensorflow/tensorflow/assets/31315784/1224a91d-325a-4f9f-bd45-f248be8715ce)

We want to achieve this:
![image](https://github.com/tensorflow/tensorflow/assets/31315784/94e3596c-2f94-45ee-bddf-1c84725a1da3)


In terms of other implementations, this one from MatLab: https://www.mathworks.com/help/vision/ref/bboxcrop.html

### Standalone code to reproduce the issue

```shell
Here is a proposal, essentially used to generate the image above, what is important that the code is vectorized. 


def crop_bounding_boxes(boxes,window):
    """"""Crop bounding boxes to the speficied window. 

    Args:
        boxes: A tensor of shape `[n_boxes,4]` describing bounding boxes. Each box is in pixel units and in the format `x_min,y_min,x_max,y_max`
        window A tensor of shape `[4]` describing the window. The window is in pixel units and in the format `x_min,y_min,x_max,y_max`

    Returns:
        _type_: _description_
    """"""    
    """"""
    Args:
        boxes 
    The annotation boxes are assumed to be in pixels and in the format `x_min,y_min,x_max,y_max`.

    """"""
    # assume boxes and patch are given as (x1,y1,x2,y2)

    # compute intersections of rectangles
    tf_ops = [tf.maximum,tf.maximum,tf.minimum,tf.minimum]
    cropped_boxes = [op(window[pos],boxes[:,pos]) for (pos,op) in enumerate(tf_ops)]
    cropped_boxes = tf.stack(cropped_boxes,axis=-1)
    mask = tf.logical_and( tf.less(cropped_boxes[:,0],cropped_boxes[:,2]), tf.less(cropped_boxes[:,1],cropped_boxes[:,3]) )
    cropped_boxes = tf.boolean_mask(cropped_boxes,mask)
    # move the coordinates origin to (x1,y1)
    corner = tf.concat([window[:2],window[:2]],axis=0)
    corner = tf.broadcast_to(corner, cropped_boxes.shape)
    cropped_boxes = cropped_boxes - corner
    return cropped_boxes


See also this [SO discussion](https://stackoverflow.com/questions/59722712/crop-multiple-bounding-boxes-from-image-with-list-of-bounding-boxes/77358447#77358447) and this [Kaggle notebook](https://www.kaggle.com/code/mskorski/efficient-bounding-boxes-cropping/)
```


### Relevant log output

_No response_"
62225,tensorflow new(sz=18446744073709551615) got std::bad_alloc,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 1.14.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.2

### Bazel version

unknown

### GCC/compiler version

gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609

### CUDA/cuDNN version

no

### GPU model and memory

_No response_

### Current behavior?

throw std::bad_malloc

### Standalone code to reproduce the issue

```shell
no std::bad_malloc, no new(sz=18446744073709551615)
```


### Relevant log output

```shell
#0  __lll_lock_wait () at ../sysdeps/unix/sysv/linux/x86_64/lowlevellock.S:135
#1  0x00007f7f6df7be42 in __GI___pthread_mutex_lock (mutex=0x7f7f6eb43970 <_rtld_global+2352>) at ../nptl/pthread_mutex_lock.c:115
#2  0x00007f7f6dceb0df in __GI___dl_iterate_phdr (callback=0x7f7f45dc4df0, data=0x7f7e28ff92a0) at dl-iteratephdr.c:41
#3  0x00007f7f45dc616e in _Unwind_Find_FDE () from /lib/x86_64-linux-gnu/libgcc_s.so.1
#4  0x00007f7f45dc2b63 in ?? () from /lib/x86_64-linux-gnu/libgcc_s.so.1
#5  0x00007f7f45dc3d80 in ?? () from /lib/x86_64-linux-gnu/libgcc_s.so.1
#6  0x00007f7f45dc422e in _Unwind_RaiseException () from /lib/x86_64-linux-gnu/libgcc_s.so.1
#7  0x00007f7f4605833c in __cxxabiv1::__cxa_throw (obj=0x7f7e258670a0, tinfo=0x7f7f4633c770 <typeinfo for std::bad_alloc>, dest=0x7f7f46056550 <std::bad_alloc::~bad_alloc()>)
    at ../../../../libstdc++-v3/libsupc++/eh_throw.cc:82
#8  0x00007f7f4605886c in operator new (sz=18446744073709551615) at ../../../../libstdc++-v3/libsupc++/new_op.cc:54
#9  0x00007f7f5170f2b9 in Eigen::internal::aligned_malloc(unsigned long) () from /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007f7f526fa18e in void* Eigen::internal::TensorContractionBlockMemAllocator<float, float>::allocateSlices<Eigen::ThreadPoolDevice const>(Eigen::ThreadPoolDevice const&, long, long, long, long, long, long, std::vector<float*, std::allocator<float*> >*, std::vector<float*, std::allocator<float*> >*) () from /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007f7f52911ec5 in void* Eigen::internal::TensorContractionKernel<float, float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer> >::allocateSlices<Eigen::ThreadPoolDevice const>(Eigen::ThreadPoolDevice const&, int, int, int, std::vector<Eigen::internal::ColMajorBlock<float, long>, std::allocator<Eigen::internal::ColMajorBlock<float, long> > >*, std::vector<Eigen::internal::ColMajorBlock<float, long>, std::allocator<Eigen::internal::ColMajorBlock<float, long> > >*) () from /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007f7f5293cf30 in void Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::evalProduct<0>(float*) const ()
   from /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007f7f5293f1cb in Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const> const, Eigen::ThreadPoolDevice, true, false>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const> const&, Eigen::ThreadPoolDevice const&) () from /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007f7f529a2c7e in tensorflow::MatMulOp<Eigen::ThreadPoolDevice, float, false>::Compute(tensorflow::OpKernelContext*) ()
   from /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#15 0x00007f7f4dee0146 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()
   from /usr/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.1
#16 0x00007f7f4ded05a5 in std::_Function_handler<void (), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> (tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_invoke(std::_Any_data const&) ()
   from /usr/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.1
#17 0x00007f7f4df76cfe in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) () from /usr/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.1
#18 0x00007f7f4df73b98 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /usr/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.1
#19 0x00007f7f460828b3 in std::execute_native_thread_routine_compat (__p=<optimized out>) at ../../../../../libstdc++-v3/src/c++11/thread.cc:110
#20 0x00007f7f6df796ba in start_thread (arg=0x7f7e28ffb700) at pthread_create.c:333
#21 0x00007f7f6dcaf4dd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
```
"
62224,how to self-hosting docs of old version,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

1.12

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

need to host old version doc as a website

### Standalone code to reproduce the issue

```shell
no code
```


### Relevant log output

_No response_"
62223,Different Behavior of tf.raw_ops.Cos+tf.raw_ops.LeakyRelu with jit_compile=True,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070

### Current behavior?

When the tf.raw_ops.Cos+tf.raw_ops.LeakyRelu operation is invoked within a tf.function with JIT compilation enabled (jit_compile=True), it produces different results compared to the same operation called without JIT compilation. This inconsistency is observed when the code is executed on a GPU device.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import traceback

class Network(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(jit_compile=True)
    def __call__(self, x):
      
      x = tf.raw_ops.Cos(x=x, )        
      x = tf.raw_ops.LeakyRelu(features=x, alpha=9.456766920329814)        
      return x

m = Network()
inp = {
    ""x"": tf.random.normal([10,9,8,1,8,3,2], dtype=tf.bfloat16),
}

with tf.device('/GPU:0'):
    tf.config.run_functions_eagerly(True)
    no_op_res = m(**inp)
    tf.config.run_functions_eagerly(False)
    with tf.device('/GPU:0'):
        op_res = m(**inp)

    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
```


### Relevant log output

```shell
File ""/home/guihuan/LLM/results/tf-2/2023-10-22-20-21/test.py"", line 29, in <module>
    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_assert.py"", line 102, in Assert
    raise errors.InvalidArgumentError(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''
b'x and y not equal to tolerance rtol = tf.Tensor(0.001, shape=(), dtype=float64), atol = tf.Tensor(0.001, shape=(), dtype=float64)'
b'x (shape=(10, 9, 8, 1, 8, 3, 2) dtype=float64) = '
0.83984375, 0.96484375, -2.6875, ...
b'y (shape=(10, 9, 8, 1, 8, 3, 2) dtype=float64) = '
0.83984375, 0.96484375, -2.6875, ...
```
"
62222,Different Behavior of tf.raw_ops.Cos+tf.raw_ops.Asin with jit_compile=True,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070

### Current behavior?

When the tf.raw_ops.Cos+tf.raw_ops.Asin operation is invoked within a tf.function with JIT compilation enabled (jit_compile=True), it produces different results compared to the same operation called without JIT compilation. This inconsistency is observed when the code is executed on a GPU device.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import traceback

class Network(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(jit_compile=True)
    def __call__(self, x):
      
      x = tf.raw_ops.Cos(x=x, )        
      x = tf.raw_ops.Asin(x=x, )        
      return x

m = Network()
inp = {
    ""x"": tf.random.uniform([10,9], dtype=tf.bfloat16),
}

with tf.device('/GPU:0'):
    tf.config.run_functions_eagerly(True)
    no_op_res = m(**inp)
    tf.config.run_functions_eagerly(False)
    with tf.device('/GPU:0'):
        op_res = m(**inp)

    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
```


### Relevant log output

```shell
File ""/home/guihuan/LLM/results/tf-2/2023-10-22-20-21/test.py"", line 27, in <module>
    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_assert.py"", line 102, in Assert
    raise errors.InvalidArgumentError(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''
b'x and y not equal to tolerance rtol = tf.Tensor(0.001, shape=(), dtype=float64), atol = tf.Tensor(0.001, shape=(), dtype=float64)'
b'x (shape=(10, 9) dtype=float64) = '
1.484375, 0.93359375, 0.7734375, ...
b'y (shape=(10, 9) dtype=float64) = '
1.484375, 0.9375, 0.7734375, ...
```
"
62221,Different Behavior of tf.raw_ops.Sin+tf.raw_ops.Acos with jit_compile=True,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070

### Current behavior?

When the tf.raw_ops.Sin+raw_ops.Acos operation is invoked within a tf.function with JIT compilation enabled (jit_compile=True), it produces different results compared to the same operation called without JIT compilation. This inconsistency is observed when the code is executed on a GPU device.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import traceback

class Network(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(jit_compile=True)
    def __call__(self, x):
      
      x = tf.raw_ops.Sin(x=x, )        
      x = tf.raw_ops.Acos(x=x, )        
      return x

m = Network()
inp = {
    ""x"": tf.random.uniform([10,9], dtype=tf.bfloat16),
}

with tf.device('/GPU:0'):
    tf.config.run_functions_eagerly(True)
    no_op_res = m(**inp)
    tf.config.run_functions_eagerly(False)
    with tf.device('/GPU:0'):
        op_res = m(**inp)

    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
```


### Relevant log output

```shell
File ""/home/guihuan/LLM/results/tf-2/2023-10-22-20-21/test.py"", line 27, in <module>
    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_assert.py"", line 102, in Assert
    raise errors.InvalidArgumentError(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''
b'x and y not equal to tolerance rtol = tf.Tensor(0.001, shape=(), dtype=float64), atol = tf.Tensor(0.001, shape=(), dtype=float64)'
b'x (shape=(10, 9) dtype=float64) = '
1.4453125, 0.88671875, 1.03125, ...
b'y (shape=(10, 9) dtype=float64) = '
1.4453125, 0.8828125, 1.03125, ...
```
"
62220,Different Behavior of tf.raw_ops.Asinh with jit_compile=True,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070

### Current behavior?

When the tf.raw_ops.Asinh operation is invoked within a tf.function with JIT compilation enabled (jit_compile=True), it produces different results compared to the same operation called without JIT compilation. This inconsistency is observed when the code is executed on a GPU device.

### Standalone code to reproduce the issue

```shell
I can reproduce this issue on the colab: https://colab.research.google.com/drive/1q6CDDRX-F0wCLvitMgfXTY92k72MFb2Z?usp=sharing
```


### Relevant log output

```shell
File ""/home/guihuan/LLM/results/tf-2/2023-10-22-20-21/test.py"", line 46, in <module>
    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_assert.py"", line 102, in Assert
    raise errors.InvalidArgumentError(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''
b'x and y not equal to tolerance rtol = tf.Tensor(0.001, shape=(), dtype=float64), atol = tf.Tensor(0.001, shape=(), dtype=float64)'
b'x (shape=(10, 9, 8) dtype=float64) = '
14.555534362792969, -12.858231544494629, -13.878985404968262, ...
b'y (shape=(10, 9, 8) dtype=float64) = '
14.555534362792969, -3.812309503555298, -4.158883094787598, ...
```
"
62219,Different Behavior of tf.raw_ops.Ndtri with jit_compile=True,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070

### Current behavior?

When the tf.raw_ops.Ndtri operation is invoked within a tf.function with JIT compilation enabled (jit_compile=True), it produces different results compared to the same operation called without JIT compilation. This inconsistency is observed when the code is executed on a GPU device.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import traceback

class Network(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(jit_compile=True)
    def __call__(self, x):
      
      x = tf.raw_ops.Ndtri(x=x, )        
      return x

m = Network()
inp = {
    ""x"": tf.random.normal(shape=[10, 9, 8], dtype=tf.float32)
}

with tf.device('/GPU:0'):
    tf.config.run_functions_eagerly(True)
    no_op_res = m(**inp)
    tf.config.run_functions_eagerly(False)
    with tf.device('/GPU:0'):
        op_res = m(**inp)

    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
```


### Relevant log output

```shell
File ""/home/guihuan/LLM/results/tf-2/2023-10-22-20-21/test.py"", line 77, in <module>
    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_assert.py"", line 102, in Assert
    raise errors.InvalidArgumentError(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''
b'x and y not equal to tolerance rtol = tf.Tensor(0.001, shape=(), dtype=float64), atol = tf.Tensor(0.001, shape=(), dtype=float64)'
b'x (shape=(10, 9, 8) dtype=float64) = '
inf, -inf, inf, ...
b'y (shape=(10, 9, 8) dtype=float64) = '
nan, nan, nan, ...
```
"
62218,TypeError: tensor_equals() missing 1 required positional argument: 'other' #47390,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.3

### Custom code

Yes
```

class CoordinateChannel(Layer):
    """""" Adds Coordinate Channels to the input tensor.
    # Arguments
        rank: An integer, the rank of the input data-uniform,
            e.g. ""2"" for 2D convolution.
        use_radius: Boolean flag to determine whether the
            radius coordinate should be added for 2D rank
            inputs or not.
        data_format: A string,
            one of `""channels_last""` or `""channels_first""`.
            The ordering of the dimensions in the inputs.
            `""channels_last""` corresponds to inputs with shape
            `(batch, ..., channels)` while `""channels_first""` corresponds to
            inputs with shape `(batch, channels, ...)`.
            It defaults to the `image_data_format` value found in your
            Keras config file at `~/.keras/keras.json`.
            If you never set it, then it will be ""channels_last"".
    # Input shape
        ND tensor with shape:
        `(samples, channels, *)`
        if `data_format` is `""channels_first""`
        or ND tensor with shape:
        `(samples, *, channels)`
        if `data_format` is `""channels_last""`.
    # Output shape
        ND tensor with shape:
        `(samples, channels + 2, *)`
        if `data_format` is `""channels_first""`
        or 5D tensor with shape:
        `(samples, *, channels + 2)`
        if `data_format` is `""channels_last""`.
    # References:
        - [An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution](https://arxiv.org/abs/1807.03247)
    """"""

    def __init__(self, rank,
                 use_radius=False,
                 data_format=None,
                 **kwargs):
        super(CoordinateChannel, self).__init__(**kwargs)

        if data_format not in [None, 'channels_first', 'channels_last']:
            raise ValueError('`data_format` must be either ""channels_last"", ""channels_first"" '
                             'or None.')

        self.rank = rank
        self.use_radius = use_radius
        self.data_format = K.image_data_format() if data_format is None else data_format
        self.axis = 1 if K.image_data_format() == 'channels_first' else -1

        self.input_spec = InputSpec(min_ndim=2)
        self.supports_masking = True

    def build(self, input_shape):
        assert len(input_shape) >= 2
        input_dim = input_shape[self.axis]

        self.input_spec = InputSpec(min_ndim=self.rank + 2,
                                    axes={self.axis: input_dim})
        self.built = True

    def call(self, inputs, training=None, mask=None):
        input_shape = K.shape(inputs)

        if self.rank == 1:
            input_shape = [input_shape[i] for i in range(3)]
            batch_shape, dim, channels = input_shape

            xx_range = K.tile(K.expand_dims(K.arange(0, dim, dtype='float32'), axis=0),
                              K.stack([batch_shape, 1]))
            xx_range = K.expand_dims(xx_range, axis=-1)

            xx_channels = K.cast(xx_range, K.floatx())
            xx_channels = xx_channels / K.cast(dim - 1, K.floatx())
            xx_channels = (xx_channels * 2) - 1.

            outputs = K.concatenate([inputs, xx_channels], axis=-1)

        if self.rank == 2:
            if self.data_format == 'channels_first':
                inputs = K.permute_dimensions(inputs, [0, 2, 3, 1])
                input_shape = K.shape(inputs)

            input_shape = [input_shape[i] for i in range(4)]
            batch_shape, dim1, dim2, channels = input_shape

            xx_ones = tf.ones(K.stack([batch_shape, dim2]), dtype='float32')
            xx_ones = K.expand_dims(xx_ones, axis=-1)

            xx_range = K.tile(K.expand_dims(K.arange(0, dim1, dtype='float32'), axis=0),
                              K.stack([batch_shape, 1]))
            xx_range = K.expand_dims(xx_range, axis=1)
            xx_channels = K.batch_dot(xx_ones, xx_range, axes=[2, 1])
            xx_channels = K.expand_dims(xx_channels, axis=-1)
            xx_channels = K.permute_dimensions(xx_channels, [0, 2, 1, 3])

            yy_ones = tf.ones(K.stack([batch_shape, dim1]), dtype='float32')
            yy_ones = K.expand_dims(yy_ones, axis=1)

            yy_range = K.tile(K.expand_dims(K.arange(0, dim2, dtype='float32'), axis=0),
                              K.stack([batch_shape, 1]))
            yy_range = K.expand_dims(yy_range, axis=-1)

            yy_channels = K.batch_dot(yy_range, yy_ones, axes=[2, 1])
            yy_channels = K.expand_dims(yy_channels, axis=-1)
            yy_channels = K.permute_dimensions(yy_channels, [0, 2, 1, 3])

            xx_channels = K.cast(xx_channels, K.floatx())
            xx_channels = xx_channels / K.cast(dim1 - 1, K.floatx())
            xx_channels = (xx_channels * 2) - 1.

            yy_channels = K.cast(yy_channels, K.floatx())
            yy_channels = yy_channels / K.cast(dim2 - 1, K.floatx())
            yy_channels = (yy_channels * 2) - 1.

            # import pdb;pdb.set_trace()
            outputs = K.concatenate([inputs, xx_channels, yy_channels], axis=-1)
            # outputs = K.concatenate([inputs, tf.cast(xx_channels, dtype=tf.float16), tf.cast(yy_channels, dtype=tf.float16)], axis=-1)

            if self.use_radius:
                rr = K.sqrt(K.square(xx_channels - 0.5) +
                            K.square(yy_channels - 0.5))
                outputs = K.concatenate([outputs, rr], axis=-1)

            if self.data_format == 'channels_first':
                outputs = K.permute_dimensions(outputs, [0, 3, 1, 2])

        if self.rank == 3:
            if self.data_format == 'channels_first':
                inputs = K.permute_dimensions(inputs, [0, 2, 3, 4, 1])
                input_shape = K.shape(inputs)

            input_shape = [input_shape[i] for i in range(5)]
            batch_shape, dim1, dim2, dim3, channels = input_shape

            xx_ones = tf.ones(K.stack([batch_shape, dim3]), dtype='float32')
            xx_ones = K.expand_dims(xx_ones, axis=-1)

            xx_range = K.tile(K.expand_dims(K.arange(0, dim2, dtype='float32'), axis=0),
                              K.stack([batch_shape, 1]))
            xx_range = K.expand_dims(xx_range, axis=1)

            xx_channels = K.batch_dot(xx_ones, xx_range, axes=[2, 1])
            xx_channels = K.expand_dims(xx_channels, axis=-1)
            xx_channels = K.permute_dimensions(xx_channels, [0, 2, 1, 3])

            xx_channels = K.expand_dims(xx_channels, axis=1)
            xx_channels = K.tile(xx_channels,
                                 [1, dim1, 1, 1, 1])

            yy_ones = tf.ones(K.stack([batch_shape, dim2]), dtype='float32')
            yy_ones = K.expand_dims(yy_ones, axis=1)

            yy_range = K.tile(K.expand_dims(K.arange(0, dim3, dtype='float32'), axis=0),
                              K.stack([batch_shape, 1]))
            yy_range = K.expand_dims(yy_range, axis=-1)

            yy_channels = K.batch_dot(yy_range, yy_ones, axes=[2, 1])
            yy_channels = K.expand_dims(yy_channels, axis=-1)
            yy_channels = K.permute_dimensions(yy_channels, [0, 2, 1, 3])

            yy_channels = K.expand_dims(yy_channels, axis=1)
            yy_channels = K.tile(yy_channels,
                                 [1, dim1, 1, 1, 1])

            zz_range = K.tile(K.expand_dims(K.arange(0, dim1, dtype='float32'), axis=0),
                              K.stack([batch_shape, 1]))
            zz_range = K.expand_dims(zz_range, axis=-1)
            zz_range = K.expand_dims(zz_range, axis=-1)

            zz_channels = K.tile(zz_range,
                                 [1, 1, dim2, dim3])
            zz_channels = K.expand_dims(zz_channels, axis=-1)

            xx_channels = K.cast(xx_channels, K.floatx())
            xx_channels = xx_channels / K.cast(dim2 - 1, K.floatx())
            xx_channels = xx_channels * 2 - 1.

            yy_channels = K.cast(yy_channels, K.floatx())
            yy_channels = yy_channels / K.cast(dim3 - 1, K.floatx())
            yy_channels = yy_channels * 2 - 1.

            zz_channels = K.cast(zz_channels, K.floatx())
            zz_channels = zz_channels / K.cast(dim1 - 1, K.floatx())
            zz_channels = zz_channels * 2 - 1.

            outputs = K.concatenate([inputs, zz_channels, xx_channels, yy_channels],
                                    axis=-1)

            if self.data_format == 'channels_first':
                outputs = K.permute_dimensions(outputs, [0, 4, 1, 2, 3])

        return outputs

    def compute_output_shape(self, input_shape):
        assert input_shape and len(input_shape) >= 2
        assert input_shape[self.axis]

        if self.use_radius and self.rank == 2:
            channel_count = 3
        else:
            channel_count = self.rank

        output_shape = list(input_shape)
        output_shape[self.axis] = input_shape[self.axis] + channel_count
        return tuple(output_shape)

    def get_config(self):
        config = {
            'rank': self.rank,
            'use_radius': self.use_radius,
            'data_format': self.data_format
        }
        base_config = super(CoordinateChannel, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))



class TransformerDecoderLayer(layers.Layer):
    def __init__(self, d_model, num_heads, dim_feedforward, regularizer_rate=0, dropout=0.1, vocab_size=2):
        super(TransformerDecoderLayer, self).__init__()
        self.last_attn_scores = None
        self.regularizer_rate = regularizer_rate
        self.kernel_regularizer = l2(regularizer_rate)
        self.self_attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout)
        self.multihead_attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout)

        self.linear1 = layers.Dense(dim_feedforward, activation='relu', kernel_regularizer=self.kernel_regularizer)
        self.dropout = layers.Dropout(dropout)
        self.linear2 = layers.Dense(d_model, kernel_regularizer=self.kernel_regularizer)

        self.norm1 = layers.LayerNormalization(epsilon=1e-6)
        self.norm2 = layers.LayerNormalization(epsilon=1e-6)
        self.norm3 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(dropout)
        self.dropout2 = layers.Dropout(dropout)
        self.dropout3 = layers.Dropout(dropout)
        self.d_model = d_model
        self.vocab_size = vocab_size
        self.drop_out_rate = dropout

    def get_config(self):
        config = {
            'd_model': self.d_model,
            'num_heads': self.d_model//32,
            'dim_feedforward': self.d_model*4,
            'dropout': self.drop_out_rate,
            'vocab_size': self.vocab_size,
            'regularizer_rate': self.regularizer_rate,
        }
        return config

    def call(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None,
             memory_key_padding_mask=None):
        tgt = PositionalEmbedding(self.vocab_size, self.d_model, tgt.shape[1]).call(tgt)
        look_ahead_mask = self.create_look_ahead_mask(tgt.shape[1])
        look_ahead_mask = look_ahead_mask[tf.newaxis, :, :]

        tgt2 = self.self_attn(query=tgt, value=tgt, key=tgt, attention_mask=look_ahead_mask)[0]
        tgt = tgt + self.dropout1(tgt2)
        tgt = self.norm1(tgt)

        multihead_attn_output = self.multihead_attn(query=tgt, value=memory, key=memory, return_attention_scores=True)
        tgt2 = multihead_attn_output[0][0]
        attn_scores = multihead_attn_output[1][0]
        tgt = tgt + self.dropout2(tgt2)
        tgt = self.norm2(tgt)

        self.last_attn_scores = attn_scores

        tgt2 = self.linear2(self.dropout(self.linear1(tgt)))
        tgt = tgt + self.dropout3(tgt2)
        tgt = self.norm3(tgt)
        return tgt

    def create_look_ahead_mask(self, size):
        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
        return mask  # (seq_len, seq_len)
        
    
def add_regularizers(model, regularizer, custom_objects=None):
    '''
    :param regularizer: The regularizer you want to add
    :param model: The model you want to add regularizer.
    Make sure you freeze layer before pass in otherwise it will add regularizer to all layers
    :return: The model after regularizers added.
    '''
    from tensorflow.keras.models import model_from_json
    import os, shutil, uuid
    random_number = uuid.uuid4()
    folder = './tmp-{random_number}'.format(random_number=random_number)
    tmp_filename = 'tmp.ckpt'
    tmp_file_path = os.path.join(folder, tmp_filename)

    os.makedirs(folder, exist_ok=True)
    model.save_weights(tmp_file_path)

    for layer in model.layers:
        for attr in ['kernel_regularizer', 'bias_regularizer', 'depthwise_regularizer', 'pointwise_regularizer']:
            if hasattr(layer, attr) and layer.trainable:
                if attr == 'bias_regularizer' and not layer.use_bias:
                    continue
                setattr(layer, attr, regularizer)

    out = model_from_json(model.to_json(), custom_objects=custom_objects)
    out.load_weights(tmp_file_path)
    shutil.rmtree(folder)
    return out


    ```
Trying to use  add_regularizers to model with self designed layer, but failed.
        

### Standalone code to reproduce the issue

```shell
x1 = Input(shape=[64, 7,7, 256])
x2 = Input(shape=[64, 159])
x  = layers.GlobalAveragePooling2D()(x1)
x  = layers.Dense(256, use_bias=False)(x)
x  = tf.expand_dims(x , axis=1)
x  = TransformerDecoderLayer(128, 128 // 32, 128 * 4, 0.2)(x2, x)
model = Model(inputs=[x1, x2], outputs=x)

model = add_regularizers(model, l2(l2_rate), custom_objects={
            ""TransformerDecoderLayer"": TransformerDecoderLayer
        })

or 

from tensorflow.keras.models import model_from_json

with open(""/data/sbot_shared_2/qiwang/cv/barcode_deblur/model.json"", ""r"") as json_file:
    json_string = json_file.read()
model = model_from_json(json_string, custom_objects={""TransformerDecoderLayer"":TransformerDecoderLayer,""CoordinateChannel"": CoordinateChannel,}))
```


### Relevant log output

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/model_config.py"", line 131, in model_from_json
    return deserialize(config, custom_objects=custom_objects)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py"", line 177, in deserialize
    printable_module_name='layer')
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py"", line 358, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py"", line 669, in from_config
    config, custom_objects)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py"", line 1285, in reconstruct_from_config
    process_node(layer, node_data)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py"", line 1233, in process_node
    output_tensors = layer(input_tensors, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 952, in __call__
    input_list)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 1091, in _functional_construction_call
    inputs, input_masks, args, kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 822, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 863, in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py"", line 1327, in _call_wrapper
    return self._call_wrapper(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py"", line 1359, in _call_wrapper
    result = self.function(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
TypeError: tensor_not_equals() missing 1 required positional argument: 'other'


## **Json file use to load model** 

```json

{""class_name"": ""Functional"", ""config"": {""name"": ""model"", ""layers"": [{""class_name"": ""InputLayer"", ""config"": {""batch_input_shape"": [null, 224, 224, 1], ""dtype"": ""float32"", ""sparse"": false, ""ragged"": false, ""name"": ""input_1""}, ""name"": ""input_1"", ""inbound_nodes"": []}, {""class_name"": ""CoordinateChannel"", ""config"": {""name"": ""coordinate_channel"", ""trainable"": true, ""dtype"": ""float32"", ""rank"": 2, ""use_radius"": false, ""data_format"": ""channels_last""}, ""name"": ""coordinate_channel"", ""inbound_nodes"": [[[""input_1"", 0, 0, {}]]]}, {""class_name"": ""Sequential"", ""config"": {""name"": ""sequential"", ""layers"": [{""class_name"": ""InputLayer"", ""config"": {""batch_input_shape"": [null, 224, 224, 3], ""dtype"": ""float32"", ""sparse"": false, ""ragged"": false, ""name"": ""conv2d_input""}}, {""class_name"": ""Conv2D"", ""config"": {""name"": ""conv2d"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 48, ""kernel_size"": [3, 3], ""strides"": [2, 2], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1], ""groups"": 1, ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""BatchNormalization"", ""config"": {""name"": ""batch_normalization"", ""trainable"": true, ""dtype"": ""float32"", ""axis"": [3], ""momentum"": 0.05, ""epsilon"": 1e-05, ""center"": true, ""scale"": true, ""beta_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""gamma_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""moving_mean_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""moving_variance_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""beta_regularizer"": null, ""gamma_regularizer"": null, ""beta_constraint"": null, ""gamma_constraint"": null}}, {""class_name"": ""ReLU"", ""config"": {""name"": ""re_lu"", ""trainable"": true, ""dtype"": ""float32"", ""max_value"": null, ""negative_slope"": 0.0, ""threshold"": 0.0}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}, {""class_name"": ""Conv2D"", ""config"": {""name"": ""conv2d_1"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 96, ""kernel_size"": [3, 3], ""strides"": [2, 2], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1], ""groups"": 1, ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""BatchNormalization"", ""config"": {""name"": ""batch_normalization_1"", ""trainable"": true, ""dtype"": ""float32"", ""axis"": [3], ""momentum"": 0.05, ""epsilon"": 1e-05, ""center"": true, ""scale"": true, ""beta_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""gamma_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""moving_mean_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""moving_variance_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""beta_regularizer"": null, ""gamma_regularizer"": null, ""beta_constraint"": null, ""gamma_constraint"": null}}, {""class_name"": ""ReLU"", ""config"": {""name"": ""re_lu_1"", ""trainable"": true, ""dtype"": ""float32"", ""max_value"": null, ""negative_slope"": 0.0, ""threshold"": 0.0}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout_1"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}, {""class_name"": ""Conv2D"", ""config"": {""name"": ""conv2d_2"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 96, ""kernel_size"": [3, 3], ""strides"": [1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1], ""groups"": 1, ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""BatchNormalization"", ""config"": {""name"": ""batch_normalization_2"", ""trainable"": true, ""dtype"": ""float32"", ""axis"": [3], ""momentum"": 0.05, ""epsilon"": 1e-05, ""center"": true, ""scale"": true, ""beta_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""gamma_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""moving_mean_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""moving_variance_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""beta_regularizer"": null, ""gamma_regularizer"": null, ""beta_constraint"": null, ""gamma_constraint"": null}}, {""class_name"": ""ReLU"", ""config"": {""name"": ""re_lu_2"", ""trainable"": true, ""dtype"": ""float32"", ""max_value"": null, ""negative_slope"": 0.0, ""threshold"": 0.0}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout_2"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}, {""class_name"": ""Conv2D"", ""config"": {""name"": ""conv2d_3"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 96, ""kernel_size"": [3, 3], ""strides"": [2, 2], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1], ""groups"": 1, ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""BatchNormalization"", ""config"": {""name"": ""batch_normalization_3"", ""trainable"": true, ""dtype"": ""float32"", ""axis"": [3], ""momentum"": 0.05, ""epsilon"": 1e-05, ""center"": true, ""scale"": true, ""beta_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""gamma_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""moving_mean_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""moving_variance_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""beta_regularizer"": null, ""gamma_regularizer"": null, ""beta_constraint"": null, ""gamma_constraint"": null}}, {""class_name"": ""ReLU"", ""config"": {""name"": ""re_lu_3"", ""trainable"": true, ""dtype"": ""float32"", ""max_value"": null, ""negative_slope"": 0.0, ""threshold"": 0.0}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout_3"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}, {""class_name"": ""Conv2D"", ""config"": {""name"": ""conv2d_4"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 96, ""kernel_size"": [3, 3], ""strides"": [1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1], ""groups"": 1, ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""BatchNormalization"", ""config"": {""name"": ""batch_normalization_4"", ""trainable"": true, ""dtype"": ""float32"", ""axis"": [3], ""momentum"": 0.05, ""epsilon"": 1e-05, ""center"": true, ""scale"": true, ""beta_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""gamma_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""moving_mean_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""moving_variance_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""beta_regularizer"": null, ""gamma_regularizer"": null, ""beta_constraint"": null, ""gamma_constraint"": null}}, {""class_name"": ""ReLU"", ""config"": {""name"": ""re_lu_4"", ""trainable"": true, ""dtype"": ""float32"", ""max_value"": null, ""negative_slope"": 0.0, ""threshold"": 0.0}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout_4"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}, {""class_name"": ""Conv2D"", ""config"": {""name"": ""conv2d_5"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 96, ""kernel_size"": [3, 3], ""strides"": [1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1], ""groups"": 1, ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""BatchNormalization"", ""config"": {""name"": ""batch_normalization_5"", ""trainable"": true, ""dtype"": ""float32"", ""axis"": [3], ""momentum"": 0.05, ""epsilon"": 1e-05, ""center"": true, ""scale"": true, ""beta_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""gamma_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""moving_mean_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""moving_variance_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""beta_regularizer"": null, ""gamma_regularizer"": null, ""beta_constraint"": null, ""gamma_constraint"": null}}, {""class_name"": ""ReLU"", ""config"": {""name"": ""re_lu_5"", ""trainable"": true, ""dtype"": ""float32"", ""max_value"": null, ""negative_slope"": 0.0, ""threshold"": 0.0}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout_5"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}, {""class_name"": ""MaxPooling2D"", ""config"": {""name"": ""max_pooling2d"", ""trainable"": true, ""dtype"": ""float32"", ""pool_size"": [2, 2], ""padding"": ""valid"", ""strides"": [2, 2], ""data_format"": ""channels_last""}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout_6"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}, {""class_name"": ""Conv2D"", ""config"": {""name"": ""conv2d_6"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 192, ""kernel_size"": [3, 3], ""strides"": [1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1], ""groups"": 1, ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""BatchNormalization"", ""config"": {""name"": ""batch_normalization_6"", ""trainable"": true, ""dtype"": ""float32"", ""axis"": [3], ""momentum"": 0.05, ""epsilon"": 1e-05, ""center"": true, ""scale"": true, ""beta_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""gamma_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""moving_mean_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""moving_variance_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""beta_regularizer"": null, ""gamma_regularizer"": null, ""beta_constraint"": null, ""gamma_constraint"": null}}, {""class_name"": ""ReLU"", ""config"": {""name"": ""re_lu_6"", ""trainable"": true, ""dtype"": ""float32"", ""max_value"": null, ""negative_slope"": 0.0, ""threshold"": 0.0}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout_7"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}, {""class_name"": ""Conv2D"", ""config"": {""name"": ""conv2d_7"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 192, ""kernel_size"": [3, 3], ""strides"": [1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1], ""groups"": 1, ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""BatchNormalization"", ""config"": {""name"": ""batch_normalization_7"", ""trainable"": true, ""dtype"": ""float32"", ""axis"": [3], ""momentum"": 0.05, ""epsilon"": 1e-05, ""center"": true, ""scale"": true, ""beta_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""gamma_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""moving_mean_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""moving_variance_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""beta_regularizer"": null, ""gamma_regularizer"": null, ""beta_constraint"": null, ""gamma_constraint"": null}}, {""class_name"": ""ReLU"", ""config"": {""name"": ""re_lu_7"", ""trainable"": true, ""dtype"": ""float32"", ""max_value"": null, ""negative_slope"": 0.0, ""threshold"": 0.0}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout_8"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}, {""class_name"": ""Conv2D"", ""config"": {""name"": ""conv2d_8"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 192, ""kernel_size"": [3, 3], ""strides"": [1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1], ""groups"": 1, ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""BatchNormalization"", ""config"": {""name"": ""batch_normalization_8"", ""trainable"": true, ""dtype"": ""float32"", ""axis"": [3], ""momentum"": 0.05, ""epsilon"": 1e-05, ""center"": true, ""scale"": true, ""beta_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""gamma_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""moving_mean_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""moving_variance_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""beta_regularizer"": null, ""gamma_regularizer"": null, ""beta_constraint"": null, ""gamma_constraint"": null}}, {""class_name"": ""ReLU"", ""config"": {""name"": ""re_lu_8"", ""trainable"": true, ""dtype"": ""float32"", ""max_value"": null, ""negative_slope"": 0.0, ""threshold"": 0.0}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout_9"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}, {""class_name"": ""Conv2D"", ""config"": {""name"": ""conv2d_9"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 384, ""kernel_size"": [3, 3], ""strides"": [1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1], ""groups"": 1, ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""BatchNormalization"", ""config"": {""name"": ""batch_normalization_9"", ""trainable"": true, ""dtype"": ""float32"", ""axis"": [3], ""momentum"": 0.05, ""epsilon"": 1e-05, ""center"": true, ""scale"": true, ""beta_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""gamma_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""moving_mean_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""moving_variance_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""beta_regularizer"": null, ""gamma_regularizer"": null, ""beta_constraint"": null, ""gamma_constraint"": null}}, {""class_name"": ""ReLU"", ""config"": {""name"": ""re_lu_9"", ""trainable"": true, ""dtype"": ""float32"", ""max_value"": null, ""negative_slope"": 0.0, ""threshold"": 0.0}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout_10"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}, {""class_name"": ""MaxPooling2D"", ""config"": {""name"": ""max_pooling2d_1"", ""trainable"": true, ""dtype"": ""float32"", ""pool_size"": [2, 2], ""padding"": ""valid"", ""strides"": [2, 2], ""data_format"": ""channels_last""}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout_11"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}, {""class_name"": ""Conv2D"", ""config"": {""name"": ""conv2d_10"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 1536, ""kernel_size"": [1, 1], ""strides"": [1, 1], ""padding"": ""valid"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1], ""groups"": 1, ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""BatchNormalization"", ""config"": {""name"": ""batch_normalization_10"", ""trainable"": true, ""dtype"": ""float32"", ""axis"": [3], ""momentum"": 0.05, ""epsilon"": 1e-05, ""center"": true, ""scale"": true, ""beta_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""gamma_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""moving_mean_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""moving_variance_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""beta_regularizer"": null, ""gamma_regularizer"": null, ""beta_constraint"": null, ""gamma_constraint"": null}}, {""class_name"": ""ReLU"", ""config"": {""name"": ""re_lu_10"", ""trainable"": true, ""dtype"": ""float32"", ""max_value"": null, ""negative_slope"": 0.0, ""threshold"": 0.0}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout_12"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}, {""class_name"": ""Conv2D"", ""config"": {""name"": ""conv2d_11"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 192, ""kernel_size"": [1, 1], ""strides"": [1, 1], ""padding"": ""valid"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1], ""groups"": 1, ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""BatchNormalization"", ""config"": {""name"": ""batch_normalization_11"", ""trainable"": true, ""dtype"": ""float32"", ""axis"": [3], ""momentum"": 0.05, ""epsilon"": 1e-05, ""center"": true, ""scale"": true, ""beta_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""gamma_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""moving_mean_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""moving_variance_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""beta_regularizer"": null, ""gamma_regularizer"": null, ""beta_constraint"": null, ""gamma_constraint"": null}}, {""class_name"": ""ReLU"", ""config"": {""name"": ""re_lu_11"", ""trainable"": true, ""dtype"": ""float32"", ""max_value"": null, ""negative_slope"": 0.0, ""threshold"": 0.0}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout_13"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}, {""class_name"": ""Conv2D"", ""config"": {""name"": ""conv2d_12"", ""trainable"": true, ""dtype"": ""float32"", ""filters"": 192, ""kernel_size"": [3, 3], ""strides"": [1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1], ""groups"": 1, ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}}, {""class_name"": ""BatchNormalization"", ""config"": {""name"": ""batch_normalization_12"", ""trainable"": true, ""dtype"": ""float32"", ""axis"": [3], ""momentum"": 0.05, ""epsilon"": 1e-05, ""center"": true, ""scale"": true, ""beta_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""gamma_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""moving_mean_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""moving_variance_initializer"": {""class_name"": ""Ones"", ""config"": {}}, ""beta_regularizer"": null, ""gamma_regularizer"": null, ""beta_constraint"": null, ""gamma_constraint"": null}}, {""class_name"": ""ReLU"", ""config"": {""name"": ""re_lu_12"", ""trainable"": true, ""dtype"": ""float32"", ""max_value"": null, ""negative_slope"": 0.0, ""threshold"": 0.0}}, {""class_name"": ""Dropout"", ""config"": {""name"": ""dropout_14"", ""trainable"": true, ""dtype"": ""float32"", ""rate"": 0.0, ""noise_shape"": null, ""seed"": null}}]}, ""name"": ""sequential"", ""inbound_nodes"": [[[""coordinate_channel"", 0, 0, {}]]]}, {""class_name"": ""GlobalAveragePooling2D"", ""config"": {""name"": ""global_average_pooling2d_1"", ""trainable"": true, ""dtype"": ""float32"", ""data_format"": ""channels_last""}, ""name"": ""global_average_pooling2d_1"", ""inbound_nodes"": [[[""sequential"", 1, 0, {}]]]}, {""class_name"": ""InputLayer"", ""config"": {""batch_input_shape"": [null, 159], ""dtype"": ""float32"", ""sparse"": false, ""ragged"": false, ""name"": ""input_2""}, ""name"": ""input_2"", ""inbound_nodes"": []}, {""class_name"": ""Dense"", ""config"": {""name"": ""dense_4"", ""trainable"": true, ""dtype"": ""float32"", ""units"": 450, ""activation"": ""linear"", ""use_bias"": false, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""name"": ""dense_4"", ""inbound_nodes"": [[[""global_average_pooling2d_1"", 0, 0, {}]]]}, {""class_name"": ""GlobalAveragePooling2D"", ""config"": {""name"": ""global_average_pooling2d"", ""trainable"": true, ""dtype"": ""float32"", ""data_format"": ""channels_last""}, ""name"": ""global_average_pooling2d"", ""inbound_nodes"": [[[""sequential"", 1, 0, {}]]]}, {""class_name"": ""Dense"", ""config"": {""name"": ""dense"", ""trainable"": true, ""dtype"": ""float32"", ""units"": 256, ""activation"": ""linear"", ""use_bias"": false, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""name"": ""dense"", ""inbound_nodes"": [[[""global_average_pooling2d"", 0, 0, {}]]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.expand_dims"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""expand_dims""}, ""name"": ""tf.expand_dims"", ""inbound_nodes"": [[""dense"", 0, 0, {""axis"": 1}]]}, {""class_name"": ""TransformerDecoderLayer"", ""config"": {""d_model"": 256, ""num_heads"": 8, ""dim_feedforward"": 1024, ""dropout"": 0.0, ""vocab_size"": 2, ""regularizer_rate"": 3e-06}, ""name"": ""transformer_decoder_layer"", ""inbound_nodes"": [[[""input_2"", 0, 0, {""memory"": [""tf.expand_dims"", 0, 0]}]]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.cast"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""cast""}, ""name"": ""tf.cast"", ""inbound_nodes"": [[""input_2"", 0, 0, {""dtype"": ""float32""}]]}, {""class_name"": ""Dense"", ""config"": {""name"": ""dense_3"", ""trainable"": true, ""dtype"": ""float32"", ""units"": 2, ""activation"": ""linear"", ""use_bias"": false, ""kernel_initializer"": {""class_name"": ""GlorotUniform"", ""config"": {""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": {""class_name"": ""L2"", ""config"": {""l2"": 3.000000106112566e-06}}, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""name"": ""dense_3"", ""inbound_nodes"": [[[""transformer_decoder_layer"", 0, 0, {}]]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.convert_to_tensor_1"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""convert_to_tensor""}, ""name"": ""tf.convert_to_tensor_1"", ""inbound_nodes"": [[""tf.cast"", 0, 0, {}]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.convert_to_tensor"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""convert_to_tensor""}, ""name"": ""tf.convert_to_tensor"", ""inbound_nodes"": [[""dense_3"", 0, 0, {}]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.cast_1"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""cast""}, ""name"": ""tf.cast_1"", ""inbound_nodes"": [[""tf.convert_to_tensor_1"", 0, 0, {""dtype"": ""int64""}]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.convert_to_tensor_2"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""convert_to_tensor""}, ""name"": ""tf.convert_to_tensor_2"", ""inbound_nodes"": [[""tf.convert_to_tensor"", 0, 0, {}]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.nn.sparse_softmax_cross_entropy_with_logits"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""nn.sparse_softmax_cross_entropy_with_logits""}, ""name"": ""tf.nn.sparse_softmax_cross_entropy_with_logits"", ""inbound_nodes"": [[""tf.cast_1"", 0, 0, {""logits"": [""tf.convert_to_tensor_2"", 0, 0]}]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.cast_2"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""cast""}, ""name"": ""tf.cast_2"", ""inbound_nodes"": [[""tf.nn.sparse_softmax_cross_entropy_with_logits"", 0, 0, {""dtype"": ""float32""}]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.math.multiply"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""math.multiply""}, ""name"": ""tf.math.multiply"", ""inbound_nodes"": [[""tf.cast_2"", 0, 0, {""y"": 1.0}]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.__operators__.ne"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""__operators__.ne""}, ""name"": ""tf.__operators__.ne"", ""inbound_nodes"": [[""input_2"", 0, 0, {}]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.cast_3"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""cast""}, ""name"": ""tf.cast_3"", ""inbound_nodes"": [[""tf.math.multiply"", 0, 0, {""dtype"": ""float32""}]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.cast_4"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""cast""}, ""name"": ""tf.cast_4"", ""inbound_nodes"": [[""tf.__operators__.ne"", 0, 0, {""dtype"": ""float32""}]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.math.multiply_1"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""math.multiply""}, ""name"": ""tf.math.multiply_1"", ""inbound_nodes"": [[""tf.cast_3"", 0, 0, {""y"": [""tf.cast_4"", 0, 0], ""name"": null}]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.math.reduce_sum"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""math.reduce_sum""}, ""name"": ""tf.math.reduce_sum"", ""inbound_nodes"": [[""tf.math.multiply_1"", 0, 0, {}]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.math.reduce_sum_1"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""math.reduce_sum""}, ""name"": ""tf.math.reduce_sum_1"", ""inbound_nodes"": [[""tf.cast_4"", 0, 0, {}]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.math.truediv"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""math.truediv""}, ""name"": ""tf.math.truediv"", ""inbound_nodes"": [[""tf.math.reduce_sum"", 0, 0, {""y"": [""tf.math.reduce_sum_1"", 0, 0], ""name"": null}]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.math.multiply_2"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""math.multiply""}, ""name"": ""tf.math.multiply_2"", ""inbound_nodes"": [[""tf.math.truediv"", 0, 0, {""y"": 0.5, ""name"": null}]]}, {""class_name"": ""AddLoss"", ""config"": {""name"": ""add_loss"", ""trainable"": true, ""dtype"": ""float32"", ""unconditional"": false}, ""name"": ""add_loss"", ""inbound_nodes"": [[[""tf.math.multiply_2"", 0, 0, {}]]]}, {""class_name"": ""TFOpLambda"", ""config"": {""name"": ""tf.math.multiply_3"", ""trainable"": true, ""dtype"": ""float32"", ""function"": ""math.multiply""}, ""name"": ""tf.math.multiply_3"", ""inbound_nodes"": [[""tf.math.truediv"", 0, 0, {""y"": 0.5, ""name"": null}]]}, {""class_name"": ""AddMetric"", ""config"": {""name"": ""add_metric"", ""trainable"": true, ""dtype"": ""float32"", ""aggregation"": ""mean"", ""metric_name"": ""transformer_decoder_loss""}, ""name"": ""add_metric"", ""inbound_nodes"": [[[""tf.math.multiply_3"", 0, 0, {}]]]}], ""input_layers"": [[""input_1"", 0, 0], [""input_2"", 0, 0]], ""output_layers"": [[""dense_4"", 0, 0]]}, ""keras_version"": ""2.4.0"", ""backend"": ""tensorflow""}

```"
62217,TypeError: this __dict__ descriptor does not support '_DictWrapper' objects,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

Colab

### Python version

3.10

### Current behavior?

- Install a clean tensorflow environment with the latest `typing-extensions` pip package.
- Assign a dictionary to a `tf.Module` (will wrap it in a `_DictWrapper` for tracking).
- Output that dictionary from a `tf.function`.
- `TypeError: this __dict__ descriptor does not support '_DictWrapper' objects`.

This is somewhat of a zombie bug, see https://github.com/tensorflow/tensorflow/issues/60687.

This is important because it breaks all `keras` functional models with dictionary output, but is not a `keras` bug. This can be reproduced simply with low-level tensorflow.

We either need to continue pinning an older version of typing extensions, or fix tensorflow to work with the latest version of typing extension. The latter seems less likely to keep breaking.

### Standalone code to reproduce the issue

https://colab.research.google.com/gist/mattdangerw/6904dc4ab29ff936ad3c3b966848f463/dict-output-bug.ipynb


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-4-4a26ae41286e> in <cell line: 12>()
     10 x = tf.constant([2, 3])
     11 y = tf.constant([3, -2])
---> 12 f(x, y)

4 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/trace_type/trace_type_builder.py in from_value(value, context)
    142   if context.is_legacy_signature and isinstance(value, trace.TraceType):
    143     return value
--> 144   elif isinstance(value, trace.SupportsTracingProtocol):
    145     generated_type = value.__tf_tracing_type__(context)
    146     if not isinstance(generated_type, trace.TraceType):

/usr/local/lib/python3.10/dist-packages/typing_extensions.py in __instancecheck__(cls, instance)
    601             for attr in cls.__protocol_attrs__:
    602                 try:
--> 603                     val = inspect.getattr_static(instance, attr)
    604                 except AttributeError:
    605                     break

/usr/lib/python3.10/inspect.py in getattr_static(obj, attr, default)
   1741         if (dict_attr is _sentinel or
   1742             type(dict_attr) is types.MemberDescriptorType):
-> 1743             instance_result = _check_instance(obj, attr)
   1744     else:
   1745         klass = obj

/usr/lib/python3.10/inspect.py in _check_instance(obj, attr)
   1688     instance_dict = {}
   1689     try:
-> 1690         instance_dict = object.__getattribute__(obj, ""__dict__"")
   1691     except AttributeError:
   1692         pass

TypeError: this __dict__ descriptor does not support '_DictWrapper' objects
```
"
62215,Tensorflow Partitioned Variable checkpoint Inconsistency,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14

### Custom code

No

### OS platform and distribution

Mac/Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

For tf.optimizers.experimental vs tf.optimizers.legacy, the former saves variables in checkpoint partitioned while latter does not. Model variables are saved not partitioned. This leads to save_weights/load_weights incompatibility when changing partition count. For example if you train model with 10 parameter servers using partitioner, save weights, and then load it outside PS with no partitioning the model fails to load.

There is even a test case focused on supporting saving -> loading model with different partitioning [here](https://github.com/tensorflow/tensorflow/blob/4dacf3f368eb7965e9b5c3bbdd5193986081c3b2/tensorflow/python/training/saver_test.py#L1070). I would not expect tf.optimizers.experimental.Optimizer variables to require partitioning to match while model variables/legacy optimizer do not.

I tried to use tf-nightly but it uses keras-nightly which is already keras 3 and has various breaking changes. model.save_weights does not supports non h5 format currently with nightly nor did tf.train.Checkpoint(model=model).write() work with the example code.

When you print checkpoint weights you can see model variable shapes are [100, 32], but optimizer variable shapes are [50, 32], [50, 32]. I've simplified code to make partitioned variables directly, but in realistic example they would be made by PS strategy.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.distribute.sharded_variable import ShardedVariable
from typing import Any
import tempfile
import os

# More realistic example this would be done by Parameter Server Strategy.
def shard_variables_creator(partitioner) -> Any:
    def _creator(next_creator, **kwargs):
        var = next_creator(**kwargs)
        if var.shape.rank == 0:
            return var

        num_shards = partitioner(var.shape, var.dtype, axis=0)
        if num_shards[0] == 1:
            return var
        
        shard_count = num_shards[0]
        shards = []
        start = 0
        for index in range(shard_count):
            shard_name = var.name.removesuffix("":0"") + f""/part_{index}""
            size = var.shape[0] // shard_count + (1 if var.shape[0] % shard_count > index else 0)
            shards.append(tf.Variable(var[start:start+size], name=shard_name))
            start += size
        
        return ShardedVariable(shards)
    
    return _creator

partitioner = tf.distribute.experimental.partitioners.MaxSizePartitioner(max_shard_bytes=100*16*4)

with tf.variable_creator_scope(shard_variables_creator(partitioner)):
    toy_model = tf.keras.Sequential([tf.keras.layers.Embedding(100, 32), tf.keras.layers.Dense(1, activation=""sigmoid"")])
    toy_model.compile(loss=""binary_crossentropy"", optimizer=tf.optimizers.experimental.Adam())
    toy_model.build(input_shape=(None, 1))
    toy_model.optimizer.build(toy_model.trainable_variables) # type: ignore

temp_dir = tempfile.gettempdir()
weights_path = os.path.join(temp_dir, ""model_weights"")
toy_model.save_weights(weights_path)

reader = tf.train.load_checkpoint(weights_path)
print(reader.get_variable_to_shape_map())
```


### Relevant log output

```shell
{'optimizer/_velocities/3/.ATTRIBUTES/VARIABLE_VALUE': [1],
 'optimizer/_velocities/2/.ATTRIBUTES/VARIABLE_VALUE': [32, 1],
 'optimizer/_velocities/1/.ATTRIBUTES/VARIABLE_VALUE': [50, 32],
 'optimizer/_momentums/3/.ATTRIBUTES/VARIABLE_VALUE': [1],
 'optimizer/_learning_rate/.ATTRIBUTES/VARIABLE_VALUE': [],
 'layer_with_weights-1/kernel/.ATTRIBUTES/VARIABLE_VALUE': [32, 1],
 'layer_with_weights-1/bias/.ATTRIBUTES/VARIABLE_VALUE': [1],
 'optimizer/_momentums/1/.ATTRIBUTES/VARIABLE_VALUE': [50, 32],
 'layer_with_weights-0/embeddings/.ATTRIBUTES/VARIABLE_VALUE': [100, 32],
 'optimizer/_velocities/0/.ATTRIBUTES/VARIABLE_VALUE': [50, 32],
 'optimizer/_momentums/2/.ATTRIBUTES/VARIABLE_VALUE': [32, 1],
 '_CHECKPOINTABLE_OBJECT_GRAPH': [],
 'optimizer/_momentums/0/.ATTRIBUTES/VARIABLE_VALUE': [50, 32],
 'optimizer/_iterations/.ATTRIBUTES/VARIABLE_VALUE': []}
```
"
62214,Get XNNPACK Profiling Info in TfLite Example,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

master branch

### Custom code

No

### OS platform and distribution

Ubuntu 222.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When I run the example label_image in tensorflow lite with profiling enabled (see [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/label_image) for the set-up) I get the following output:

```
ERROR: failed to get XNNPACK profile information.
ERROR: failed to get XNNPACK profile information.
ERROR: failed to get XNNPACK profile information.
ERROR: failed to get XNNPACK profile information.
ERROR: failed to get XNNPACK profile information.
ERROR: failed to get XNNPACK profile information.
INFO: invoked
INFO: average time: 22.357 ms
INFO:     23.639, Subgraph   0, Node   0, OpCode   3, CONV_2D
INFO:      0.002, Subgraph   0, Node  29, OpCode  43, SQUEEZE
INFO:     22.513, Subgraph   0, Node   0, OpCode   3, CONV_2D
INFO:      0.001, Subgraph   0, Node  29, OpCode  43, SQUEEZE
INFO:     22.354, Subgraph   0, Node   0, OpCode   3, CONV_2D
INFO:      0.001, Subgraph   0, Node  29, OpCode  43, SQUEEZE
```
I would like to get profiling info from XNNPACK. 

I see that the reason is because the program bails out in [xnn_get_runtime_profiling_info](https://github.com/google/XNNPACK/blob/master/src/runtime.c#L742)  because the profiling flags is set to false.  The stack trace is:

```
#0  tflite::xnnpack::(anonymous namespace)::Subgraph::AddEventsToProfiler (
    profiler=0x5555563f54a0, runtime=0x55555631fc90)
    at tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:1051
#1  0x0000555555b284c1 in tflite::xnnpack::(anonymous namespace)::Subgraph::Invoke (
    this=0x5555563da950, context=0x55555632bd08)
    at tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:1037
#2  0x0000555555b3a618 in tflite::xnnpack::(anonymous namespace)::SubgraphInvoke (
    context=0x55555632bd08, node=0x5555563325e0)
    at tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:6192
#3  0x0000555555a5f3f5 in tflite::Subgraph::OpInvoke (this=0x55555632bce0, op_reg=..., 
    node=0x5555563325e0) at tensorflow/lite/core/subgraph.cc:1288
#4  0x0000555555a6046c in tflite::Subgraph::InvokeImpl (this=0x55555632bce0)
    at tensorflow/lite/core/subgraph.cc:1601
#5  0x0000555555a5feea in tflite::Subgraph::Invoke (this=0x55555632bce0)
    at tensorflow/lite/core/subgraph.cc:1494
#6  0x0000555555a2e124 in tflite::impl::Interpreter::Invoke (this=0x55555631d230)
    at tensorflow/lite/core/interpreter.cc:237
#7  0x000055555557393a in tflite::label_image::RunInference (settings=0x7fffffffd8f0, 
    delegate_providers=...) at tensorflow/lite/examples/label_image/label_image.cc:335
#8  0x0000555555574dca in tflite::label_image::Main (argc=11, argv=0x7fffffffdad8)
    at tensorflow/lite/examples/label_image/label_image.cc:558
#9  0x0000555555574e68 in main (argc=11, argv=0x7fffffffdad8)
    at tensorflow/lite/examples/label_image/label_image.cc:566
```
I would like to set the flags to enable profiling, but I am not sure how. Can somebody kindly explain me how to set the flag? 

### Standalone code to reproduce the issue

```shell
See here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/label_image#build-the-example
```


### Relevant log output

```shell
ERROR: failed to get XNNPACK profile information.
ERROR: failed to get XNNPACK profile information.
ERROR: failed to get XNNPACK profile information.
ERROR: failed to get XNNPACK profile information.
ERROR: failed to get XNNPACK profile information.
ERROR: failed to get XNNPACK profile information.
```
"
62212,Different behaviors of raw_ops.Sigmoid can be observed when jitcompiled=true.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070

### Current behavior?

In TensorFlow, enabling jitcompiled results in inconsistent behavior of raw_ops.Sigmoid.



### Standalone code to reproduce the issue

```shell
I can reproduce this issue on colab: https://colab.research.google.com/drive/16gZRKnhRtqUBJiUQEobXCrjt3B4-w8hS?usp=sharing
```


### Relevant log output

```shell
InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''
b'x and y not equal to tolerance rtol = tf.Tensor(0.001, shape=(), dtype=float32), atol = tf.Tensor(0.001, shape=(), dtype=float32)'
b'x (shape=(10, 9) dtype=complex64) = '
(nan+nanj), (1+0j), (nan+nanj), ...
b'y (shape=(10, 9) dtype=complex64) = '
(1+0j), (-0-0j), (1+0j), ...
```
"
62211,Different behaviors of raw_ops.Expm1 can be observed when jitcompiled=true. ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070

### Current behavior?

In TensorFlow, enabling jitcompiled results in inconsistent behavior of raw_ops.Expm1.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1k9OjhFTmZNsj94JJ0lmJ-iiBL29-au00?usp=sharing
```


### Relevant log output

```shell
File ""/home/guihuan/LLM/results/tf-2/2023-10-22-20-21/test.py"", line 42, in <module>
    tf.debugging.assert_near(no_op_res, op_res, atol=0.001, rtol=0.001)
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_assert.py"", line 102, in Assert
    raise errors.InvalidArgumentError(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''
b'x and y not equal to tolerance rtol = tf.Tensor(0.001, shape=(), dtype=float64), atol = tf.Tensor(0.001, shape=(), dtype=float64)'
b'x (shape=(10, 9, 8) dtype=complex128) = '
(-1+0j), (-1+0j), (nan+infj), ...
b'y (shape=(10, 9, 8) dtype=complex128) = '
(-1+0j), (-1+0j), (inf+infj), ...
```
"
62210,Missing `'tensorflow.python.training.tracking'` in version 2.14.0; cannot load pickled model,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current behavior?

We used to load some pickled TF models using:

```
import pickle
pickle.load(open(""/content/cosmopower/cosmopower/trained_models/CP_paper/CMB/cmb_TT_NN.pkl"", 'rb'))
```

but as of version 2.14.0 we get:

```
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
[<ipython-input-3-776b938106f2>](https://localhost:8080/#) in <cell line: 1>()
----> 1 pickle.load(open(""/content/cosmopower/cosmopower/trained_models/CP_paper/CMB/cmb_TT_NN.pkl"", 'rb'))

ModuleNotFoundError: No module named 'tensorflow.python.training.tracking'
```

We checked that there are no errors with TF `2.13.0`, and everything works as expected. The `tensorflow.python.training.tracking` module seems to have been removed since `2.14.0`, but we are unable to find it in the release notes. This error is observed on Colab as well as on other platforms and OSs.

### Standalone code to reproduce the issue

```shell
! git clone https://github.com/alessiospuriomancini/cosmopower.git # download folder with TF model

# this is intended for colab; change path if necessary
import pickle
pickle.load(open(""/content/cosmopower/cosmopower/trained_models/CP_paper/CMB/cmb_TT_NN.pkl"", 'rb'))
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-3-776b938106f2> in <cell line: 1>()
----> 1 pickle.load(open(""/content/cosmopower/cosmopower/trained_models/CP_paper/CMB/cmb_TT_NN.pkl"", 'rb'))

ModuleNotFoundError: No module named 'tensorflow.python.training.tracking'

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
""Open Examples"" button below.
---------------------------------------------------------------------------
```
"
62209,Deserialization issues with non-default dtypes,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf v2.14.0-rc1-21-g4dacf3f368e

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The following model cannot be deserialized:

```python
x = keras.Input(batch_shape=(4, 16), dtype=tf.float16)
output = 1.0 + x
keras_model = keras.Model(x, output)
```
It appears as if Keras fails to recognize the `dtype` of constants and always saves them as default type (`float32`/`int32`).
Setting `dtype` explicitly does not help:

```python
output = tf.convert_to_tensor(1, dtype=tf.float16) + x
```
The problem persists across various non-default data types (`float16`, `float64`, `int16`, `int64`), operations (`add`, `sub`, `mul`, `pow`) and model formats (`.h5`, `SavedModel`). Oddly enough, it goes away when the arguments are swapped:

```python
output = x + 1.0  # Works just fine!
```
In `v2.16-nightly`, which resolves to Keras 3, everything's alright.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow import keras


x = keras.Input(batch_shape=(4, 16), dtype=tf.float16)
output = tf.convert_to_tensor(1, dtype=tf.float16) + x
keras_model = keras.Model(x, output)

model_path = 'model.h5'
keras_model.save(model_path)
print('Model saved')

keras_model_restored = keras.models.load_model(model_path)
print('Model loaded')
```


### Relevant log output

```shell
TypeError: Exception encountered when calling layer ""tf.__operators__.add"" (type TFOpLambda).

Input 'y' of 'AddV2' Op has type float16 that does not match type float32 of argument 'x'.

Call arguments received by layer ""tf.__operators__.add"" (type TFOpLambda):
  â€¢ x=tf.Tensor(shape=(), dtype=float32)
  â€¢ y=tf.Tensor(shape=(4, 16), dtype=float16)
  â€¢ name=None
```
"
62208,//tensorflow/python/eager:forwardprop_test test fails on AARCH64,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

17.0.0

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

Since Eigen was updated, the unit test //tensorflow/python/eager:forwardprop_test now fails.
Eigen was updated by https://github.com/tensorflow/tensorflow/commit/57e6377cf9879e33f3612f1ffd3619b6513e5296
It looks like this commit in Eigen is the problem.
https://gitlab.com/libeigen/eigen/-/commit/81b48065ea673cd352d11ef9b6a3d86778ac962d

This seems to only affect AARCH64 and not x86.

### Standalone code to reproduce the issue

```shell
bazel test --cache_test_results=no --build_tests_only --config=mkl_aarch64_threadpool --test_env=TF_ENABLE_ONEDNN_OPTS=1 --copt=-flax-vector-conversions --test_env=TF2_BEHAVIOR=1 --test_env=PORTSERVER_ADDRESS=@unittest-portserver --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --test_tag_filters=--oss_serial,-no_oss,-oss_excluded,-v1only,-benchmark-test,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 --jobs=75 -- //tensorflow/python/eager:forwardprop_test
```


### Relevant log output

```shell
======================================================================
FAIL: testNumericHigherOrder (__main__.ForwardpropTest)
ForwardpropTest.testNumericHigherOrder
Warms up, gets object counts, runs the test, checks for new objects.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 713, in decorator
    f(self, *args, **kwargs)
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/forwardprop_test.py"", line 455, in testNumericHigherOrder
    _test_gradients(
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/forwardprop_test.py"", line 215, in _test_gradients
    _test_gradients(
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/forwardprop_test.py"", line 215, in _test_gradients
    _test_gradients(
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/forwardprop_test.py"", line 231, in _test_gradients
    testcase.assertAllClose(sym_jac_back, sym_jac_fwd, rtol=srtol, atol=satol)
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1657, in decorated
    return f(*args, **kwds)
           ^^^^^^^^^^^^^^^^
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 3293, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 3229, in _assertAllCloseRecursive
    self._assertArrayLikeAllClose(
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 3186, in _assertArrayLikeAllClose
    np.testing.assert_allclose(
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/pypi_numpy/site-packages/numpy/testing/_private/utils.py"", line 1527, in assert_allclose
    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/pypi_numpy/site-packages/numpy/testing/_private/utils.py"", line 844, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=1e-06, atol=1e-06
Mismatched value: a is different from b. 
not close where = (array([0]), array([0]), array([2]))
not close lhs = [-155.46957]
not close rhs = [-155.46982]
not close dif = [0.00024414]
not close tol = [0.00015647]
dtype = float32, shape = (1, 4, 4)
Mismatched elements: 1 / 16 (6.25%)
Max absolute difference: 0.00024414
Max relative difference: 1.5703409e-06
 x: array([[[-4755.0654  ,  -139.43396 ,  -155.46957 ,   158.52019 ],
        [ -139.43398 ,  -119.946   ,   -26.157635,    20.20992 ],
        [ -155.4697  ,   -26.15764 ,   428.02136 ,  -365.0485  ],...
 y: array([[[-4755.0654  ,  -139.43398 ,  -155.46982 ,   158.52022 ],
        [ -139.43399 ,  -119.946014,   -26.157648,    20.209923],
        [ -155.46967 ,   -26.157635,   428.02112 ,  -365.04846 ],...

----------------------------------------------------------------------
Ran 13 tests in 95.972s

FAILED (failures=1)
================================================================================
```
"
62207,Different behaviors of raw_ops.Cos + raw_ops.Asinh on jitcompiled=true.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070

### Current behavior?

In TensorFlow, enabling jitcompiled results in inconsistent behavior of raw_ops.Cos and raw_ops.Asinh.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import traceback

class Network(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(jit_compile=True)
    def __call__(self, x):
      
      x = tf.raw_ops.Cos(x=x, )        
      x = tf.raw_ops.Asinh(x=x, )        
      return x

m = Network()
inp = {
    ""x"": tf.random.uniform([10,9,8,6], -100, 100, dtype=tf.bfloat16),
}
        
with tf.device('/GPU:0'):
    tf.config.run_functions_eagerly(True)
    no_op_res = m(**inp)
    tf.config.run_functions_eagerly(False)
    with tf.device('/GPU:0'):
        op_res = m(**inp)

    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
```


### Relevant log output

```shell
File ""/home/guihuan/LLM/results/tf-2/2023-10-22-20-21/test.py"", line 27, in <module>
    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_assert.py"", line 102, in Assert
    raise errors.InvalidArgumentError(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''
b'x and y not equal to tolerance rtol = tf.Tensor(0.001, shape=(), dtype=float64), atol = tf.Tensor(0.001, shape=(), dtype=float64)'
b'x (shape=(10, 9, 8, 6) dtype=float64) = '
-0.86328125, 0.87890625, 0.8828125, ...
b'y (shape=(10, 9, 8, 6) dtype=float64) = '
-0.8671875, 0.87890625, 0.8828125, ...
```
"
62206,Different behaviors of raw_ops.Sigmoid can be observed when jitcompiled=true.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070

### Current behavior?

In TensorFlow, enabling jitcompiled results in inconsistent behavior of raw_ops.Sigmoid.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import traceback
class Network(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(jit_compile=True)
    def __call__(self, x):       
      x = tf.raw_ops.Sigmoid(x=x, )        
      return x

m = Network()
inp = {
    ""x"": tf.random.uniform([10,9,8], dtype=tf.bfloat16),
}

with tf.device('/GPU:0'):
    tf.config.run_functions_eagerly(True)
    no_op_res = m(**inp)
    tf.config.run_functions_eagerly(False)
    with tf.device('/GPU:0'):
        op_res = m(**inp)

    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/guihuan/LLM/results/tf-2/2023-10-22-20-21/test.py"", line 27, in <module>
    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_assert.py"", line 102, in Assert
    raise errors.InvalidArgumentError(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''
b'x and y not equal to tolerance rtol = tf.Tensor(0.001, shape=(), dtype=float64), atol = tf.Tensor(0.001, shape=(), dtype=float64)'
b'x (shape=(10, 9, 8) dtype=float64) = '
0.65234375, 0.5078125, 0.56640625, ...
b'y (shape=(10, 9, 8) dtype=float64) = '
0.59765625, 0.03125, 0.26953125, ...
```
"
62205,Bug/Problem when ploting the result of `tf.image.resize`. Solution is given to the problem,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

```python
import tensorflow as tf
import matplotlib.pyplot as plt

image_string = tf.io.read_file(""/content/_1574258b-b76a-401c-8c1d-f9cf632d7631.jpeg"")
tensor = tf.io.decode_jpeg(image_string, channels=3)
print(tensor.shape)
plt.imshow(tensor.numpy())
plt.show()
```
![download](https://github.com/tensorflow/tensorflow/assets/109357590/091608bc-25e9-4165-b911-2787366e5eb1)

```python
plt.imshow((tf.image.resize(tensor, [100, 100])).numpy())
plt.show() # NO IMAGE, THERE IS PROBLEM
print(tf.image.resize(tensor, [100, 100]))
```
> WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).

![download](https://github.com/tensorflow/tensorflow/assets/109357590/0fcaa65a-88cd-46d7-ac9e-acd4ca0b9fde)
```python
<tf.Tensor: shape=(100, 100, 3), dtype=float32, numpy=
array([[[ 12.7644  ,  18.7644  ,  32.7644  ],
        [ 14.      ,  20.      ,  34.      ],
        [ 14.      ,  20.      ,  34.      ],
        ...,
        [ 16.619999,  25.619999,  40.62    ],
        [ 16.039814,  28.039814,  44.039814],
        [ 17.995617,  29.995617,  45.995617]],

       [[ 14.      ,  20.      ,  34.      ],
        [ 15.      ,  21.      ,  35.      ],
        [ 16.      ,  22.      ,  36.      ],
        ...,
        [ 18.899963,  27.899963,  42.899963],
        [ 15.279907,  27.279907,  43.279907],
        [ 16.14    ,  28.14    ,  44.14    ]],

       [[ 14.      ,  22.      ,  35.      ],
        [ 16.099998,  24.099998,  37.1     ],
        [ 16.      ,  22.      ,  36.      ],
        ...,
        [ 15.619899,  31.619898,  46.6199  ],
        [ 15.907904,  32.9079  ,  48.9079  ],
        [ 17.240013,  34.240013,  50.240013]],

       ...,

       [[ 26.533968,  24.533968,  35.533966],
        [ 25.967827,  23.967827,  34.967827],
        [ 33.67996 ,  30.67996 ,  39.67996 ],
        ...,
        [114.779915,  31.979984,  41.879948],
        [119.47677 ,  33.44877 ,  44.46277 ],
        [103.15075 ,  21.950676,  31.050713]],

       [[ 27.532555,  25.532555,  36.532555],
        [ 27.5823  ,  25.5823  ,  36.5823  ],
        [ 23.576063,  21.576063,  32.576065],
        ...,
        [104.3003  ,  30.880442,  40.020397],
        [100.94113 ,  24.181866,  32.0615  ],
        [112.63941 ,  38.57296 ,  45.57296 ]],

       [[ 34.041607,  32.04161 ,  43.041607],
        [ 20.49961 ,  18.49961 ,  29.49961 ],
        [ 24.038023,  22.038023,  33.03802 ],
        ...,
        [ 83.27584 ,  32.956017,  39.75404 ],
        [ 95.79817 ,  19.357985,  27.21803 ],
        [100.01961 ,  31.019604,  36.019604]]], dtype=float32)>
```
```python
# SOLUTION
plt.imshow((tf.image.resize(tensor, [100, 100])/255.).numpy())
plt.show() # BUT ON NORMALIZING WE GET THE PROPER IMAGE
# OR
# plt.imshow((tf.image.resize(tf.cast(tensor, tf.float32)/255., [100, 100])).numpy())
# plt.show()
```
![download](https://github.com/tensorflow/tensorflow/assets/109357590/94eb3abb-b7d9-4ddc-8c80-eeca0bcec3ba)

The code is self-explanatory. There is a problem in `tf.mage.resize` when the input image is not normalized. Normalizing the input image or the output of `tf.mage.resize` will solve the problem.
Add support to `tf.mage.resize` when the input image is not normalized i.e when values are int between 0 and 255

### Standalone code to reproduce the issue


[Colab Link](https://colab.research.google.com/drive/1y41vpFs6Cd4NHmSY6IRmU53PXgl_SAFE#scrollTo=trFnmSDqtNxz&line=1&uniqifier=1)



### Relevant log output

_No response_"
62204,Float32 precision loss causing cross_entropy loss to return incorrect results,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.8

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

```
import tensorflow.keras.backend as K
K.binary_crossentropy([[0.0],[1.0]],[[1.0],[0.0]], from_logits=False)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
array([[15.333239],
       [15.424949]], dtype=float32)>
```
In theory, crossentropy isn't symmetric but the non-trivial discrepancy between crossentropy(0,1) and crossentropy(1,0) here is caused by precision loss. Specifically, 
In https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/backend.py#L4825C67-L4825C67
```
epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)
output = clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)

bce = target * math_ops.log(output + epsilon())
bce += (1 - target) * math_ops.log(1 - output + epsilon())
return -bce
```
The problem is, when output==1, 1 - output + epsilon() = 1 - (1. - epsilon_) + epsilon() = 2* epsilon_
the expected result should be 2e-7, but the actual result is 2.1920928e-07 due to precision loss. After taking the natural log, we get 15.333239 and 15.424949. A 0.092 difference in a single sample loss is pretty significant.

### Standalone code to reproduce the issue

```shell
import tensorflow.keras.backend as K
K.binary_crossentropy([[0.0],[1.0]],[[1.0],[0.0]], from_logits=False)
```


### Relevant log output

```shell
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=array([[15.333239],  [15.424949]], dtype=float32)>
```
"
62203,tf.random.normal() causes RAM usage to keep growing,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

macOS Venture 13.1 (22C65)

### Mobile device

_No response_

### Python version

3.8.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

**Issue** 
I'm getting what looks like a memory leak from running tf.random.normal as below, in eager mode. 
I have not encountered this issue with others versions, e.g. 2.12.1

**Context** 
I'm running RL algorithms using a TF-addon [NoisyDense](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/NoisyDense) layer, which exposes a function that does the below random sampling. Since it's being executed millions of times, it causes exploding RAM.

**Notes**
Wrapping the random sampling inside a function decorated with tf.function seems to avoid the issue.

### Standalone code to reproduce the issue

```shell
import os, psutil
process = psutil.Process()
print(process.memory_info().rss / 1000000)  # Ram usage in MB

import tensorflow as tf
print(tf.__version__)

for _ in range(10):
    print(process.memory_info().rss / 1000000)
    for _ in range(5000):
        x = tf.random.normal(shape=(10000, 1))  # Causing growing RAM
```


### Relevant log output

```shell
382.963712
2.13.0
382.963712
385.220608
387.21536
388.927488
390.926336
392.392704
394.338304
396.374016
398.323712
400.273408
```
"
62200,Error when building from source: AddKernelActivityEvent<TF_CUPTI_HAS_CHANNEL_ID> â€˜TF_CUPTI_HAS_CHANNEL_IDâ€™ was not declared in this scope,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

6.1.0

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

Cuda=11.4.152, cuDNN=8.9.5

### GPU model and memory

GeForce RTX 2070 Rev. A

### Current behavior?

I am trying to install tensorflow from source. I am following this [tutorial](https://www.tensorflow.org/install/source). I have the cuDNN and the Cuda Toolkit installed, and the following configuration: 
```
You have bazel 6.1.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 

Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.8/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: N
No TensorRT support will be enabled for TensorFlow.

Found CUDA 11.4 in:
    /usr/local/cuda-11.4/targets/x86_64-linux/lib
    /usr/local/cuda-11.4/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/local/cuda-11.4/targets/x86_64-linux/lib
    /usr/local/cuda-11.4/targets/x86_64-linux/include

Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.5]: 

Do you want to use clang as CUDA compiler? [Y/n]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.
```

Then doing: `bazel build --config=opt --action_env=PATH -c opt //tensorflow/tools/pip_package:build_pip_package`

```
ERROR: /home/master/.cache/bazel/_bazel_master/7d15ddcbf9badca106464f95f49bc497/external/local_xla/xla/backends/profiler/gpu/BUILD:172:16: Compiling xla/backends/profiler/gpu/cupti_tracer.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command (from target @local_xla//xla/backends/profiler/gpu:cupti_tracer) external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/local_xla/xla/backends/profiler/gpu/_objs/cupti_tracer/cupti_tracer.pic.d ... (remaining 180 arguments skipped)
external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc: In member function â€˜tsl::Status xla::profiler::CuptiTracer::ProcessActivityBuffer(CUcontext, uint32_t, uint8_t*, size_t)â€™:
external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:2177:34: error: â€˜TF_CUPTI_HAS_CHANNEL_IDâ€™ was not declared in this scope
 2177 |           AddKernelActivityEvent<TF_CUPTI_HAS_CHANNEL_ID>(
      |                                  ^~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:2178:76: error: no matching function for call to â€˜AddKernelActivityEvent<<expression error> >(xla::profiler::CuptiTraceCollector*&, xla::profiler::{anonymous}::CuptiActivityKernelTy*)â€™
 2178 |               collector_, reinterpret_cast<CuptiActivityKernelTy *>(record));
      |                                                                            ^
external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:747:6: note: candidate: â€˜template<bool cupti_has_channel_id, class CuptiActivityKernel> void xla::profiler::{anonymous}::AddKernelActivityEvent(xla::profiler::CuptiTraceCollector*, const CuptiActivityKernel*)â€™
  747 | void AddKernelActivityEvent(CuptiTraceCollector *collector,
      |      ^~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:747:6: note:   template argument deduction/substitution failed:
external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:2178:76: error: template argument 1 is invalid
 2178 |               collector_, reinterpret_cast<CuptiActivityKernelTy *>(record));
      |                                                                            ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2354.699s, Critical Path: 306.56s
INFO: 11360 processes: 84 internal, 11276 local.
FAILED: Build did NOT complete successfully

```

### Standalone code to reproduce the issue

```shell
bazel build --config=opt --action_env=PATH -c opt //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /home/master/Downloads/tensorflow/tensorflow/core/kernels/image/BUILD:325:18: Compiling tensorflow/core/kernels/image/image_ops_gpu.cu.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command (from target //tensorflow/core/kernels/image:image_ops_gpu) 
  (cd /home/master/.cache/bazel/_bazel_master/7d15ddcbf9badca106464f95f49bc497/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.4 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 \
    LD_LIBRARY_PATH=/usr/local/cuda-11.4/lib64:/home/master/TensorRT-8.6.1.6/lib:/usr/local/cuda-11.4/lib64:/home/master/TensorRT-8.6.1.6/lib \
    PATH=/home/master/.cache/bazelisk/downloads/sha256/6c25a6d716545d6b672ec46f770521cd9ebb63d73617b8f4e6747825d1db1839/bin:/usr/local/cuda-11.4/bin:/home/master/.local/bin:/usr/local/cuda-11.4/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=7.5 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/image_ops_gpu/image_ops_gpu.cu.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/image_ops_gpu/image_ops_gpu.cu.pic.o' '-DEIGEN_MAX_ALIGN_BYTES=64' -DEIGEN_ALLOW_UNALIGNED_SCALARS '-DEIGEN_USE_AVX512_GEMM_KERNELS=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL '-DEIGEN_ALTIVEC_USE_CUSTOM_PACK=0' '-DEIGEN_NEON_GEBP_NR=4' '-DBAZEL_CURRENT_REPOSITORY=""""' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/local_tsl -iquote bazel-out/k8-opt/bin/external/local_tsl -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/ml_dtypes -iquote bazel-out/k8-opt/bin/external/ml_dtypes -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/snappy -iquote bazel-out/k8-opt/bin/external/snappy -iquote external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/nccl_archive -iquote bazel-out/k8-opt/bin/external/nccl_archive -iquote external/local_config_rocm -iquote bazel-out/k8-opt/bin/external/local_config_rocm -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -iquote external/png -iquote bazel-out/k8-opt/bin/external/png -iquote external/onednn -iquote bazel-out/k8-opt/bin/external/onednn -iquote external/local_xla -iquote bazel-out/k8-opt/bin/external/local_xla -Ibazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8 -Ibazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/int4 -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/nccl_config -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/eigen_archive/mkl_include -isystem bazel-out/k8-opt/bin/external/eigen_archive/mkl_include -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/ml_dtypes -isystem bazel-out/k8-opt/bin/external/ml_dtypes -isystem external/ml_dtypes/ml_dtypes -isystem bazel-out/k8-opt/bin/external/ml_dtypes/ml_dtypes -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/png -isystem bazel-out/k8-opt/bin/external/png -isystem external/onednn/include -isystem bazel-out/k8-opt/bin/external/onednn/include -isystem external/onednn/src -isystem bazel-out/k8-opt/bin/external/onednn/src -isystem external/onednn/src/common -isystem bazel-out/k8-opt/bin/external/onednn/src/common -isystem external/onednn/src/common/ittnotify -isystem bazel-out/k8-opt/bin/external/onednn/src/common/ittnotify -isystem external/onednn/src/cpu -isystem bazel-out/k8-opt/bin/external/onednn/src/cpu -isystem external/onednn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/onednn/src/cpu/gemm -isystem external/onednn/src/cpu/x64/xbyak -isystem bazel-out/k8-opt/bin/external/onednn/src/cpu/x64/xbyak -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-sign-compare '-std=c++17' -x cuda '-DGOOGLE_CUDA=1' '--cuda-include-ptx=sm_75' '--cuda-gpu-arch=sm_75' '-Xcuda-fatbinary=--compress-all' '-nvcc_options=expt-relaxed-constexpr' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -DENABLE_ONEDNN_V3 -DAMD_ZENDNN -msse3 -pthread '-nvcc_options=relaxed-constexpr' '-nvcc_options=ftz=true' -c tensorflow/core/kernels/image/image_ops_gpu.cu.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/image_ops_gpu/image_ops_gpu.cu.pic.o)
# Configuration: a0763b48cf05909e46ed8ba9df7ecc2eca8eed6449f35b44f6b17dbe52c77dbe
# Execution platform: @local_execution_config_platform//:platform
bazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8/ml_dtypes/include/float8.h(1164): warning: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To ml_dtypes::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=ml_dtypes::float8_internal::float8_e4m3b11fnuz, To=ml_dtypes::float8_internal::float8_e4m3fn, kSaturate=false, kTruncate=false]"" 
(1255): here
            instantiation of ""Derived ml_dtypes::float8_internal::float8_base<Derived>::ConvertFrom(const From &) [with Derived=ml_dtypes::float8_internal::float8_e4m3fn, kSaturate=false, kTruncate=false, From=ml_dtypes::float8_internal::float8_e4m3b11fnuz]"" 
(258): here

bazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8/ml_dtypes/include/float8.h(1164): warning: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To ml_dtypes::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=ml_dtypes::float8_internal::float8_e4m3b11fnuz, To=float, kSaturate=false, kTruncate=false]"" 
(1261): here
            instantiation of ""To ml_dtypes::float8_internal::float8_base<Derived>::ConvertTo<To,kSaturate,kTruncate>(const Derived &) [with Derived=ml_dtypes::float8_internal::float8_e4m3b11fnuz, To=float, kSaturate=false, kTruncate=false]"" 
(87): here
            instantiation of ""ml_dtypes::float8_internal::float8_base<Derived>::operator float() const [with Derived=ml_dtypes::float8_internal::float8_e4m3b11fnuz]"" 
(128): here
            instantiation of ""Derived ml_dtypes::float8_internal::float8_base<Derived>::operator-(const Derived &) const [with Derived=ml_dtypes::float8_internal::float8_e4m3b11fnuz]"" 
(287): here

bazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8/ml_dtypes/include/float8.h(1164): warning: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To ml_dtypes::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=ml_dtypes::float8_internal::float8_e4m3fnuz, To=float, kSaturate=false, kTruncate=false]"" 
(1261): here
            instantiation of ""To ml_dtypes::float8_internal::float8_base<Derived>::ConvertTo<To,kSaturate,kTruncate>(const Derived &) [with Derived=ml_dtypes::float8_internal::float8_e4m3fnuz, To=float, kSaturate=false, kTruncate=false]"" 
(87): here
            instantiation of ""ml_dtypes::float8_internal::float8_base<Derived>::operator float() const [with Derived=ml_dtypes::float8_internal::float8_e4m3fnuz]"" 
(128): here
            instantiation of ""Derived ml_dtypes::float8_internal::float8_base<Derived>::operator-(const Derived &) const [with Derived=ml_dtypes::float8_internal::float8_e4m3fnuz]"" 
(335): here

bazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8/ml_dtypes/include/float8.h(1164): warning: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To ml_dtypes::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=ml_dtypes::float8_internal::float8_e4m3b11fnuz, To=ml_dtypes::float8_internal::float8_e4m3fnuz, kSaturate=false, kTruncate=false]"" 
(1255): here
            instantiation of ""Derived ml_dtypes::float8_internal::float8_base<Derived>::ConvertFrom(const From &) [with Derived=ml_dtypes::float8_internal::float8_e4m3fnuz, kSaturate=false, kTruncate=false, From=ml_dtypes::float8_internal::float8_e4m3b11fnuz]"" 
(323): here

bazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8/ml_dtypes/include/float8.h(1164): warning: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To ml_dtypes::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=ml_dtypes::float8_internal::float8_e5m2fnuz, To=ml_dtypes::float8_internal::float8_e5m2, kSaturate=false, kTruncate=false]"" 
(1255): here
            instantiation of ""Derived ml_dtypes::float8_internal::float8_base<Derived>::ConvertFrom(const From &) [with Derived=ml_dtypes::float8_internal::float8_e5m2, kSaturate=false, kTruncate=false, From=ml_dtypes::float8_internal::float8_e5m2fnuz]"" 
(357): here

bazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8/ml_dtypes/include/float8.h(1164): warning: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To ml_dtypes::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=ml_dtypes::float8_internal::float8_e5m2fnuz, To=float, kSaturate=false, kTruncate=false]"" 
(1261): here
            instantiation of ""To ml_dtypes::float8_internal::float8_base<Derived>::ConvertTo<To,kSaturate,kTruncate>(const Derived &) [with Derived=ml_dtypes::float8_internal::float8_e5m2fnuz, To=float, kSaturate=false, kTruncate=false]"" 
(87): here
            instantiation of ""ml_dtypes::float8_internal::float8_base<Derived>::operator float() const [with Derived=ml_dtypes::float8_internal::float8_e5m2fnuz]"" 
(128): here
            instantiation of ""Derived ml_dtypes::float8_internal::float8_base<Derived>::operator-(const Derived &) const [with Derived=ml_dtypes::float8_internal::float8_e5m2fnuz]"" 
(399): here

bazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8/ml_dtypes/include/float8.h(1164): warning: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To ml_dtypes::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=ml_dtypes::float8_internal::float8_e5m2, To=ml_dtypes::float8_internal::float8_e5m2fnuz, kSaturate=false, kTruncate=false]"" 
(1255): here
            instantiation of ""Derived ml_dtypes::float8_internal::float8_base<Derived>::ConvertFrom(const From &) [with Derived=ml_dtypes::float8_internal::float8_e5m2fnuz, kSaturate=false, kTruncate=false, From=ml_dtypes::float8_internal::float8_e5m2]"" 
(383): here

external/com_google_absl/absl/status/internal/statusor_internal.h(252): warning: ignoring return value type with ""nodiscard"" attribute

external/com_google_absl/absl/status/internal/statusor_internal.h(259): warning: ignoring return value type with ""nodiscard"" attribute

external/com_google_absl/absl/strings/internal/str_format/bind.h: In constructor â€˜absl::lts_20230125::str_format_internal::FormatSpecTemplate<Args>::FormatSpecTemplate(const absl::lts_20230125::str_format_internal::ExtendedParsedFormat<absl::lts_20230125::FormatConversionCharSet(C)...>&)â€™:
external/com_google_absl/absl/strings/internal/str_format/bind.h:171:1: error: parse error in template argument list
  171 |     CheckArity<sizeof...(C), sizeof...(Args)>();
      | ^   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~              
external/com_google_absl/absl/strings/internal/str_format/bind.h:171:63: error: expected â€˜;â€™ before â€˜)â€™ token
  171 |     CheckArity<sizeof...(C), sizeof...(Args)>();
      |                                                               ^
      |                                                               ;
external/com_google_absl/absl/strings/internal/str_format/bind.h:172:147: error: template argument 1 is invalid
  172 |     CheckMatches<C...>(absl::make_index_sequence<sizeof...(C)>{});
      |                                                                                                                                                   ^
external/com_google_absl/absl/strings/internal/str_format/bind.h:172:151: error: expected primary-expression before â€˜{â€™ token
  172 |     CheckMatches<C...>(absl::make_index_sequence<sizeof...(C)>{});
      |                                                                                                                                                       ^
external/com_google_absl/absl/strings/internal/str_format/bind.h:172:150: error: expected â€˜;â€™ before â€˜{â€™ token
  172 |     CheckMatches<C...>(absl::make_index_sequence<sizeof...(C)>{});
      |                                                                                                                                                      ^ 
      |                                                                                                                                                      ;
external/com_google_absl/absl/strings/internal/str_format/bind.h:172:153: error: expected primary-expression before â€˜)â€™ token
  172 |     CheckMatches<C...>(absl::make_index_sequence<sizeof...(C)>{});
      |                                                                                                                                                         ^
external/com_google_absl/absl/strings/internal/str_format/arg.h: In instantiation of â€˜constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ArgumentToConv() [with Arg = long int]â€™:
external/com_google_absl/absl/strings/str_format.h:268:156:   required by substitution of â€˜template<class ... Args> using FormatSpec = absl::lts_20230125::str_format_internal::FormatSpecTemplate<absl::lts_20230125::FormatConversionCharSet((ArgumentToConv<Args>)())...> [with Args = {long int, const tensorflow::ResourceBase*}]â€™
external/com_google_absl/absl/strings/str_format.h:351:1:   required by substitution of â€˜template<class ... Args> std::string absl::lts_20230125::StrFormat(absl::lts_20230125::FormatSpec<Args ...>&, const Args& ...) [with Args = {long int, const tensorflow::ResourceBase*}]â€™
./tensorflow/core/framework/resource_base.h:44:62:   required from here
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: error: no matching function for call to â€˜ExtractCharSet(ConvResult)â€™
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:196:1: note: candidate: â€˜template<absl::lts_20230125::FormatConversionCharSet C> constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ExtractCharSet(absl::lts_20230125::FormatConvertResult<(absl::lts_20230125::FormatConversionCharSet)(C)>)â€™
  196 | constexpr FormatConversionCharSet ExtractCharSet(FormatConvertResult<C>) {
      | ^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:196:1: note:   template argument deduction/substitution failed:
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: note:   couldnâ€™t deduce template parameter â€˜Câ€™
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:201:1: note: candidate: â€˜template<absl::lts_20230125::FormatConversionCharSet C> constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ExtractCharSet(absl::lts_20230125::str_format_internal::ArgConvertResult<(absl::lts_20230125::FormatConversionCharSet)(C)>)â€™
  201 | constexpr FormatConversionCharSet ExtractCharSet(ArgConvertResult<C>) {
      | ^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:201:1: note:   template argument deduction/substitution failed:
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: note:   couldnâ€™t deduce template parameter â€˜Câ€™
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h: In instantiation of â€˜constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ArgumentToConv() [with Arg = const tensorflow::ResourceBase*]â€™:
external/com_google_absl/absl/strings/str_format.h:268:156:   required by substitution of â€˜template<class ... Args> using FormatSpec = absl::lts_20230125::str_format_internal::FormatSpecTemplate<absl::lts_20230125::FormatConversionCharSet((ArgumentToConv<Args>)())...> [with Args = {long int, const tensorflow::ResourceBase*}]â€™
external/com_google_absl/absl/strings/str_format.h:351:1:   required by substitution of â€˜template<class ... Args> std::string absl::lts_20230125::StrFormat(absl::lts_20230125::FormatSpec<Args ...>&, const Args& ...) [with Args = {long int, const tensorflow::ResourceBase*}]â€™
./tensorflow/core/framework/resource_base.h:44:62:   required from here
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: error: no matching function for call to â€˜ExtractCharSet(ConvResult)â€™
external/com_google_absl/absl/strings/internal/str_format/arg.h:196:1: note: candidate: â€˜template<absl::lts_20230125::FormatConversionCharSet C> constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ExtractCharSet(absl::lts_20230125::FormatConvertResult<(absl::lts_20230125::FormatConversionCharSet)(C)>)â€™
  196 | constexpr FormatConversionCharSet ExtractCharSet(FormatConvertResult<C>) {
      | ^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:196:1: note:   template argument deduction/substitution failed:
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: note:   couldnâ€™t deduce template parameter â€˜Câ€™
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:201:1: note: candidate: â€˜template<absl::lts_20230125::FormatConversionCharSet C> constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ExtractCharSet(absl::lts_20230125::str_format_internal::ArgConvertResult<(absl::lts_20230125::FormatConversionCharSet)(C)>)â€™
  201 | constexpr FormatConversionCharSet ExtractCharSet(ArgConvertResult<C>) {
      | ^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:201:1: note:   template argument deduction/substitution failed:
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: note:   couldnâ€™t deduce template parameter â€˜Câ€™
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
./tensorflow/core/framework/resource_base.h: In member function â€˜virtual std::string tensorflow::ResourceBase::MakeRefCountingHandleName(int64_t) constâ€™:
./tensorflow/core/framework/resource_base.h:44:62: error: no matching function for call to â€˜StrFormat(const char [18], int64_t&, const tensorflow::ResourceBase*)â€™
   44 |     return absl::StrFormat(""Resource-%d-at-%p"", resource_id, this);
      |                                                              ^
external/com_google_absl/absl/strings/str_format.h:351:1: note: candidate: â€˜template<class ... Args> std::string absl::lts_20230125::StrFormat(absl::lts_20230125::FormatSpec<Args ...>&, const Args& ...)â€™
  351 | ABSL_MUST_USE_RESULT std::string StrFormat(const FormatSpec<Args...>& format,
      | ^~~~~~~~~
external/com_google_absl/absl/strings/str_format.h:351:1: note:   substitution of deduced template arguments resulted in errors seen above
external/com_google_absl/absl/strings/internal/str_format/arg.h: In instantiation of â€˜constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ArgumentToConv() [with Arg = int]â€™:
external/com_google_absl/absl/strings/internal/str_format/arg.h:570:61:   required from â€˜static bool absl::lts_20230125::str_format_internal::FormatArgImpl::Dispatch(absl::lts_20230125::str_format_internal::FormatArgImpl::Data, absl::lts_20230125::str_format_internal::FormatConversionSpecImpl, void*) [with T = int]â€™
external/com_google_absl/absl/strings/internal/str_format/arg.h:524:15:   required from â€˜void absl::lts_20230125::str_format_internal::FormatArgImpl::Init(const T&) [with T = int]â€™
external/com_google_absl/absl/strings/internal/str_format/arg.h:474:1:   required from â€˜absl::lts_20230125::str_format_internal::FormatArgImpl::FormatArgImpl(const T&) [with T = int]â€™
external/com_google_absl/absl/strings/internal/str_format/bind.h:202:49:   required from here
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: error: no matching function for call to â€˜ExtractCharSet(ConvResult)â€™
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:196:1: note: candidate: â€˜template<absl::lts_20230125::FormatConversionCharSet C> constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ExtractCharSet(absl::lts_20230125::FormatConvertResult<(absl::lts_20230125::FormatConversionCharSet)(C)>)â€™
  196 | constexpr FormatConversionCharSet ExtractCharSet(FormatConvertResult<C>) {
      | ^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:196:1: note:   template argument deduction/substitution failed:
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: note:   couldnâ€™t deduce template parameter â€˜Câ€™
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:201:1: note: candidate: â€˜template<absl::lts_20230125::FormatConversionCharSet C> constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ExtractCharSet(absl::lts_20230125::str_format_internal::ArgConvertResult<(absl::lts_20230125::FormatConversionCharSet)(C)>)â€™
  201 | constexpr FormatConversionCharSet ExtractCharSet(ArgConvertResult<C>) {
      | ^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:201:1: note:   template argument deduction/substitution failed:
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: note:   couldnâ€™t deduce template parameter â€˜Câ€™
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 35.257s, Critical Path: 32.01s
INFO: 238 processes: 30 internal, 208 local.
FAILED: Build did NOT complete successfully
```
"
62198,[XLA] using auto-clustering with tf_xla_auto_jit causes multiple compilation with variable seq. len.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have a somewhat complex model that deals with variable length tensors as input. The feature dimension remains the same, only one of the dimension changes (time). If I fix this by giving always the same batch of features, I can use tf_xla_auto_jit and see that the training speed improves quite a lot. However, as soon as I throw the for `batch in train_dataset` in the mix, everything becomes infinitely slow and, looking at the warning, it feels like XLA keeps recompiling ... 

I don't really know what to expect here? 

### Standalone code to reproduce the issue

```shell
Impossible to share unfortunately.
```


### Relevant log output

_No response_"
62197,How to use tf.repeat and another buil-in highlevel funcs on Dataset?,Can you help me in [stackoverflow](https://stackoverflow.com/questions/77332989/how-to-use-tf-repeat-and-another-buil-in-highlevel-funcs-on-dataset)? 
62196,"Quantization produces large scale coffiecient, which pervents the model from being loaded ","### 1. System information
Colab , as of 2023-10-23


### 2. Code
Please see the attached colab notebook  here
https://colab.research.google.com/drive/1yUD0nDu8oeeDtQBa7xCbQWx_w8PxS4UC?usp=sharing
to  reproduce the issue. It loads a pre-trained resnet18 from pytorch, converts  it to onnx, converts it to tensorflow, and then exports it to tf-lite.  ( The process is a bit convoluted, but I need a pretrained resnet18, and didn't find it in the tensorflow orbit so I used torchvision, hope that's ok.)

If you download the generated model (model_int8.tflite) and open it in netron.app and click on the first MaxPool2D op, you can see that the quantization scale is ` 1.3344405750530544e+36 `. See the attached image. 

![image](https://github.com/tensorflow/tensorflow/assets/10357496/60968bdd-ee50-48fd-9802-9b498ac619e4)

This scale parameter itself is of course implausible (impossible), but loading the model also produces an error here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.cc#L117 


Does anybody know why the quantization parameter is that high, and what can be done to fix it? Furthermore, can I let the quantization fails explicitly when it generates such high values? 
"
62195,New,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
62194,Loading cudNN in docker image where tensorflow is installed via pip returns `Error: libnvrtc.so: cannot open shared object file: No such file or directory`,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8/8.7

### GPU model and memory

NVIDIA L40, 46BG

### Current behavior?

We are building a Docker image that is running an Ubuntu 22.04. The host machine is also Ubuntu 22.04. We have chosen to not use the prebuilt tensorflow Docker image. We are trying to install tensorflow via pip in the docker image, and at the first glance it seems to work, as `import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))` works perfectly fine and prints something along the lines of `tf.Tensor(-163.40398, shape=(), dtype=float32)`.

However, when we try to load cudNN by running the keras `Model.train_on_batch` method we get the error `Could not load library libcudnn_cnn_infer.so.8. Error: libnvrtc.so: cannot open shared object file: No such file or directory`. Full stack trace is in the ""Relevant log ouput"" cell.

If we install cuda in the docker image by running the following commands it works though. But we would prefer not installing cuda directly and only through pip.

```
apt-get install wget \
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin \
mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600 \
wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb \
dpkg -i cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb \
cp /var/cuda-repo-ubuntu2204-11-8-local/cuda-*-keyring.gpg /usr/share/keyrings/ \
apt-get update \
DEBIAN_FRONTEND=noninteractive apt-get -y install cuda \
echo ""export PATH=${PATH}:/usr/local/cuda/bin"" >> ~/.bashrc \
```

`nvidia-smi` prints

```
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA L40                     Off | 00000000:0B:00.0 Off |                    0 |
| N/A   37C    P0              78W / 300W |      4MiB / 46068MiB |      3%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```

`pip freeze` gives us these dependencies (some dependencies stripped):

```
keras==2.14.0
keras-core==0.1.7
keras-cv==0.6.4
...
numpy==1.26.1
nvidia-cublas-cu11==11.11.3.6
nvidia-cuda-cupti-cu11==11.8.87
nvidia-cuda-nvcc-cu11==11.8.89
nvidia-cuda-runtime-cu11==11.8.89
nvidia-cudnn-cu11==8.7.0.84
nvidia-cufft-cu11==10.9.0.58
nvidia-curand-cu11==10.3.0.86
nvidia-cusolver-cu11==11.4.1.48
nvidia-cusparse-cu11==11.7.5.86
nvidia-nccl-cu11==2.16.5
...
tensorboard==2.14.1
tensorboard-data-server==0.7.1
tensorflow==2.14.0
tensorflow-datasets==4.9.3
tensorflow-estimator==2.14.0
tensorflow-io-gcs-filesystem==0.34.0
tensorflow-metadata==1.14.0
tensorrt==8.5.3.1
```

To me it seems like installing tensorflow via `pip install tensorflow[and-cuda]==2.14.0` doesn't include the libnvrtc.so file? Or maybe it's some error related to keras?


### Standalone code to reproduce the issue

```shell
Dockerfile:


FROM ghcr.io/osgeo/gdal:ubuntu-small-3.7.2

ARG USE_GPU

RUN apt-get update && apt-get install ca-certificates -y

# Install python dependencies
RUN apt install python3-pip -y
RUN pip install pillow==10.1.0 transformers==4.33.3 imageio==2.31.1 scipy==1.11.1 geopandas==0.14.0 dtale==3.7.0 rasterio==1.3.9 rasterstats==0.19.0  psycopg2-binary==2.9.9 sqlalchemy==2.0.22 keras-cv==0.6.4 focal-loss==0.0.7 azure-storage-blob==12.18.3

RUN if [ ""$USE_GPU"" = ""true"" ]; then \
    pip install tensorflow[and-cuda]==2.14.0; \
  else \
    pip install tensorflow==2.14.0; \
  fi
```
```


### Relevant log output

```shell
2023-10-23 05:58:21.251995: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-23 05:58:21.252097: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-23 05:58:21.252135: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-23 05:58:21.259174: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Using TensorFlow backend
2023-10-23 05:58:27.246915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-23 05:58:27.284909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-23 05:58:27.286183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-23 05:58:27.298554: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b5e53fd570 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2023-10-23 05:58:27.298575: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2023-10-23 05:58:27.298922: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.
2023-10-23 05:58:27.299033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-23 05:58:27.300290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-23 05:58:27.301475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-23 05:58:27.447455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-23 05:58:27.448893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-23 05:58:27.450160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-23 05:58:27.451404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43412 MB memory:  -> device: 0, name: NVIDIA L40, pci bus id: 0000:0b:00.0, compute capability: 8.9
2023-10-23 05:58:27.453893: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b5e5f86f60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-10-23 05:58:27.453916: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA L40, Compute Capability 8.9
learning_rate 0.001
2023-10-23 05:59:11.072927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700
Could not load library libcudnn_cnn_infer.so.8. Error: libnvrtc.so: cannot open shared object file: No such file or directory
```
"
62193,EfficientNet transfer learning - init with imagenet weights changes model,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.14

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current behavior?

My current pipeline of:
1. Train a model using EfficientNet backbone with `weights=""imagenet""`
2. Transfer a trained model **weights** to the prod environment 
3. Initialize the production model with `weights=None` and load the previously learned weights
4. Compare the model's output between training and productions environment - **they are different**

Obviously, I expected the outputs to be the same. After LOTS of investigation, I found the culprit which is NOT mentioned anywhere in the documentation: [github link](https://github.com/keras-team/keras/blob/68f9af408a1734704746f7e6fa9cfede0d6879d8/keras/applications/efficientnet.py#L359).
Turns out, when the model is initialized with `weights=""imagenet""` an additional rescaling layer is added. I understand the reason behind it (reproducing the Imagenet results) but the above situation proves how misleading the implementation is.

_Why didn't I just save the whole model instead of transferring only the weights?_
Well, because the model is trained with mixed precision on a GPU and transferring it this way to a production env which uses a CPU would make it unusable (nans all around).

_What I would expect ideally:_
The same model structure no matter which weights are loaded.

_What I would expect to minimize the impact of this issue:_
When the weights are loaded to production model, the mismatch between number of layers is detected and a warning is raised.

See test collab notebook with the code from below: [link](https://colab.research.google.com/drive/1M_VbMiH79elsPex08awkjDwIuvFBqOpC?usp=sharing )

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from keras.applications.efficientnet import EfficientNetB3, preprocess_input

img_shape = (300,300,3)
x = np.ones((1,) + img_shape)*255

trained_model = EfficientNetB3(weights=""imagenet"", include_top=False, input_shape=img_shape)
trained_model.save_weights(""weights.h5"")

prod_model = EfficientNetB3(weights=None, include_top=False, input_shape=img_shape)
prod_model.load_weights(""weights.h5"")

y_trained = trained_model.predict(preprocess_input(x), verbose=0)[0,:2,:2,:2]
y_prod = prod_model.predict(preprocess_input(x), verbose=0)[0,:2,:2,:2]

print(f""Trained model output (initialized with imagenet weights):\n{y_trained}"")
print(f""Production model output (initialized with 'None' weights):\n{y_prod}"")
```


### Relevant log output

```shell
Trained model output (initialized with imagenet weights):
[[[-0.27523986 -0.25020567]
  [-0.26998693 -0.27772853]]

 [[-0.2728562  -0.27376127]
  [-0.24821427 -0.23993279]]]
Production model output (initialized with 'None' weights):
[[[-0.2746129   0.89038897]
  [-0.25032952  0.1611695 ]]

 [[-0.1986168   0.0976367 ]
  [-0.17414406 -0.26978934]]]
```
"
62192,tf.keras.Model with nested dictionary inputs fails to serialize/deserialize,"Info:
- Issue type: **Bug**
- Have you reproduced the bug with TensorFlow Nightly? No
- Source: binary
- TensorFlow version: 2.14.0
- Custom code: No
- OS platform and distribution: macOS 13.6
- Python version: 3.11
- CUDA/cuDNN version: none

### Current behavior?

When trying to serialize/deserialize a `tf.keras.Model` nested input shapes cause an error.

Note that this has been observed many years back in #37061, which was closed because their MVP included a `tf.keras.Sequential` which was deemed as not supported. However, the issue has nothing to do with `tf.keras.Sequential` at all, and instead lies purely in the deserialisation code of keras.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

@tf.keras.saving.register_keras_serializable(package=""MyPackage"")
class DummyModel(tf.keras.Model):
    def __init__(self, name=None):
        super().__init__(name=name)
        self.sublayer = tf.keras.layers.Dense(16)
    def call(self, x, **kw):
        a = x[""a""]
        nested = x[""nested""]
        b = nested[""b""]
        c = nested[""c""]
        return self.sublayer(tf.concat([a,b,c], axis=-1))

model = DummyModel()
out = model(dict(
    a = tf.keras.Input(3,dtype=tf.float32),
    nested = dict(
        b = tf.keras.Input(4,dtype=tf.float32),
        c = tf.keras.Input(5,dtype=tf.float32),
    ),
))
model.summary()

model.save(""temp.keras"")

tf.keras.saving.load_model(""temp.keras"")
```


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/tensor_shape.py:851, in TensorShape.__init__(self, dims)
    850 try:
--> 851   self._dims.append(as_dimension(d).value)
    852 except TypeError as e:

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/tensor_shape.py:741, in as_dimension(value)
    740 else:
--> 741   return Dimension(value)

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/tensor_shape.py:217, in Dimension.__init__(self, value)
    216 except AttributeError:
--> 217   raise TypeError(
    218       ""Dimension value must be integer or None or have ""
    219       ""an __index__ method, got value '{0!r}' with type '{1!r}'"".format(
    220           value, type(value))) from None
    221 if self._value < 0:

TypeError: Dimension value must be integer or None or have an __index__ method, got value ''b'' with type '<class 'str'>'

The above exception was the direct cause of the following exception:

TypeError                                 Traceback (most recent call last)
File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:204, in make_shape(v, arg_name)
    203 try:
--> 204   shape = tensor_shape.as_shape(v)
    205 except TypeError as e:

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/tensor_shape.py:1526, in as_shape(shape)
   1525 else:
-> 1526   return TensorShape(shape)

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/tensor_shape.py:853, in TensorShape.__init__(self, dims)
    852   except TypeError as e:
--> 853     raise TypeError(
    854         ""Failed to convert '{0!r}' to a shape: '{1!r}'""
    855         ""could not be converted to a dimension. A shape should ""
    856         ""either be single dimension (e.g. 10), or an iterable of ""
    857         ""dimensions (e.g. [1, 10, None])."".format(dims, d)) from e
    858 self._dims = tuple(self._dims)

TypeError: Failed to convert '{'b': [None, 4], 'c': [None, 5]}' to a shape: ''b''could not be converted to a dimension. A shape should either be single dimension (e.g. 10), or an iterable of dimensions (e.g. [1, 10, None]).

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
Cell In[6], line 1
----> 1 tf.keras.saving.load_model(""temp.keras"")

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/saving/saving_api.py:254, in load_model(filepath, custom_objects, compile, safe_mode, **kwargs)
    249     if kwargs:
    250         raise ValueError(
    251             ""The following argument(s) are not supported ""
    252             f""with the native Keras format: {list(kwargs.keys())}""
    253         )
--> 254     return saving_lib.load_model(
    255         filepath,
    256         custom_objects=custom_objects,
    257         compile=compile,
    258         safe_mode=safe_mode,
    259     )
    261 # Legacy case.
    262 return legacy_sm_saving_lib.load_model(
    263     filepath, custom_objects=custom_objects, compile=compile, **kwargs
    264 )

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:281, in load_model(filepath, custom_objects, compile, safe_mode)
    278             asset_store.close()
    280 except Exception as e:
--> 281     raise e
    282 else:
    283     return model

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:246, in load_model(filepath, custom_objects, compile, safe_mode)
    244 # Construct the model from the configuration file in the archive.
    245 with ObjectSharingScope():
--> 246     model = deserialize_keras_object(
    247         config_dict, custom_objects, safe_mode=safe_mode
    248     )
    250 all_filenames = zf.namelist()
    251 if _VARS_FNAME + "".h5"" in all_filenames:

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/saving/serialization_lib.py:731, in deserialize_keras_object(config, custom_objects, safe_mode, **kwargs)
    729 build_config = config.get(""build_config"", None)
    730 if build_config:
--> 731     instance.build_from_config(build_config)
    732 compile_config = config.get(""compile_config"", None)
    733 if compile_config:

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/base_layer.py:2331, in Layer.build_from_config(self, config)
   2329 input_shape = config[""input_shape""]
   2330 if input_shape is not None:
-> 2331     self.build(input_shape)

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:494, in Model.build(self, input_shape)
    489     x = [
    490         base_layer_utils.generate_placeholders_from_shape(shape)
    491         for shape in input_shape
    492     ]
    493 elif isinstance(input_shape, dict):
--> 494     x = {
    495         k: base_layer_utils.generate_placeholders_from_shape(
    496             shape
    497         )
    498         for k, shape in input_shape.items()
    499     }
    500 else:
    501     x = base_layer_utils.generate_placeholders_from_shape(
    502         input_shape
    503     )

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:495, in <dictcomp>(.0)
    489     x = [
    490         base_layer_utils.generate_placeholders_from_shape(shape)
    491         for shape in input_shape
    492     ]
    493 elif isinstance(input_shape, dict):
    494     x = {
--> 495         k: base_layer_utils.generate_placeholders_from_shape(
    496             shape
    497         )
    498         for k, shape in input_shape.items()
    499     }
    500 else:
    501     x = base_layer_utils.generate_placeholders_from_shape(
    502         input_shape
    503     )

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/base_layer_utils.py:189, in generate_placeholders_from_shape(shape)
    188 def generate_placeholders_from_shape(shape):
--> 189     return tf1.placeholder(shape=shape, dtype=backend.floatx())

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py:3283, in placeholder(dtype, shape, name)
   3279 if context.executing_eagerly():
   3280   raise RuntimeError(""tf.placeholder() is not compatible with ""
   3281                      ""eager execution."")
-> 3283 return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/ops/gen_array_ops.py:7071, in placeholder(dtype, shape, name)
   7069 if shape is None:
   7070   shape = None
-> 7071 shape = _execute.make_shape(shape, ""shape"")
   7072 _, _, _op, _outputs = _op_def_library._apply_op_helper(
   7073       ""Placeholder"", dtype=dtype, shape=shape, name=name)
   7074 _result = _outputs[:]

File ~/.pyenv/versions/3.11.3/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:206, in make_shape(v, arg_name)
    204   shape = tensor_shape.as_shape(v)
    205 except TypeError as e:
--> 206   raise TypeError(""Error converting %s to a TensorShape: %s."" % (arg_name, e))
    207 except ValueError as e:
    208   raise ValueError(""Error converting %s to a TensorShape: %s."" %
    209                    (arg_name, e))

TypeError: Error converting shape to a TensorShape: Failed to convert '{'b': [None, 4], 'c': [None, 5]}' to a shape: ''b''could not be converted to a dimension. A shape should either be single dimension (e.g. 10), or an iterable of dimensions (e.g. [1, 10, None])..
```
"
62191,"tf.strings.to_number cannot convert positive integers prefixed with ""+"" when out_type is tf.int32 or tf.int64","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.11.0, 2.13.0, 2.14.0

### Custom code

Yes

### OS platform and distribution

MacOS 13.1

### Mobile device

Macbook Pro

### Python version

3.10.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Converting string to numbers with ""+""  throws errors in TF 2.11.0, 2.13.0, 2.14.0 when `out_type=tf.int64` or `out_type=tf.in32`. Not expecting error to be thrown and strings can be correctly converted into integers. For example, I would like to parse timezone information from a string (using substring)

```python 
t = tf.constant([
    ""2023-05-07 17:32:25-08:00"", # utc: next day
    ""2023-05-07 05:32:25+11:00"", # utc: previous day
    ""2023-05-07 05:32:25-08:00"", # utc: same date
    ""2023-02-29 23:32:15-04:00"", # leap year
    ]
)

```

### Standalone code to reproduce the issue

```shell
Code to reproduce


import tensorflow

# these are all okay
tf.string.to_number(tf.constant(""-11""), out_type=tf.int64)
tf.string.to_number(tf.constant(""11""), out_type=tf.int64)
tf.string.to_number(tf.constant(""+11""), out_type=tf.float32)

# this throws the error below
tf.strings.to_number(tf.constant(""+11""), out_type=tf.int64)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/edward/opt/miniforge3/envs/testtf214/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/Users/edward/opt/miniforge3/envs/testtf214/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 5888, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StringToNumber_device_/job:localhost/replica:0/task:0/device:CPU:0}} StringToNumberOp could not correctly convert string: +11 [Op:StringToNumber] name:
```
"
62189,Could not find a version that satisfies the requirement tf-nightly (from versions: none) when install with pip,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**: tf-nightly
-   **Python version**: 3.8.10
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 12.0/8.8.1.3-1+cuda12.0
-   **GPU model and memory**: H100
-   **Exact command to reproduce**:

pip install tf-nightly
python -m pip install tf-nightly

But none works

```
Defaulting to user installation because normal site-packages is not writeable
ERROR: Could not find a version that satisfies the requirement tf-nightly (from versions: none)
ERROR: No matching distribution found for tf-nightly
WARNING: There was an error checking the latest version of pip.
```

You can collect some of this information using our environment capture script:



https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
62188,can't install @tensorflow/tf-node on npm,"please help. i can't install @tensorflow/tf-node on my project node.js

i have python 3.12, node.js 18 and npm ver. 10.2.1

but i can install @tensorflow/tfjs

this is the error:

npm WARN cleanup Failed to remove some directories [
npm WARN cleanup   [
npm WARN cleanup     'D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@mapbox',
npm WARN cleanup     [Error: EPERM: operation not permitted, rmdir 'D:\xampp8\htdocs\testnodejs\node_modules\@mapbox\node-pre-gyp\node_modules\agent-base'] {
npm WARN cleanup       errno: -4048,
npm WARN cleanup       code: 'EPERM',
npm WARN cleanup       syscall: 'rmdir',
npm WARN cleanup       path: 'D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@mapbox\\node-pre-gyp\\node_modules\\agent-base'
npm WARN cleanup     }
npm WARN cleanup   ]
npm WARN cleanup ]
npm ERR! code 1
npm ERR! path D:\xampp8\htdocs\testnodejs\node_modules\@tensorflow\tfjs-node
npm ERR! command failed
npm ERR! command C:\WINDOWS\system32\cmd.exe /d /s /c node scripts/install.js
npm ERR! CPU-windows-4.12.0.zip
npm ERR! https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.9.1.zip
npm ERR! node-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-build
npm ERR! node-pre-gyp info it worked if it ends with ok
npm ERR! node-pre-gyp info using node-pre-gyp@1.0.9
npm ERR! node-pre-gyp info using node@18.18.2 | win32 | x64
npm ERR! node-pre-gyp info check checked for ""D:\xampp8\htdocs\testnodejs\node_modules\@tensorflow\tfjs-node\lib\napi-v8\tfjs_binding.node"" (not found)
npm ERR! node-pre-gyp http GET https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/4.12.0/CPU-windows-4.12.0.zip
npm ERR! node-pre-gyp ERR! install response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/4.12.0/CPU-windows-4.12.0.zip
npm ERR! node-pre-gyp WARN Pre-built binaries not installable for @tensorflow/tfjs-node@4.12.0 and node@18.18.2 (node-v108 ABI, unknown) (falling back to source compile with node-gyp)
npm ERR! node-pre-gyp WARN Hit error response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/4.12.0/CPU-windows-4.12.0.zip
npm ERR! gyp info it worked if it ends with ok
npm ERR! gyp info using node-gyp@9.4.0
npm ERR! gyp info using node@18.18.2 | win32 | x64
npm ERR! gyp info ok
npm ERR! gyp info it worked if it ends with ok
npm ERR! gyp info using node-gyp@9.4.0
npm ERR! gyp info using node@18.18.2 | win32 | x64
npm ERR! gyp info find Python using Python version 3.12.0 found at ""C:\Program Files\Python312\python.exe""
npm ERR! gyp http GET https://nodejs.org/download/release/v18.18.2/node-v18.18.2-headers.tar.gz
npm ERR! gyp http 200 https://nodejs.org/download/release/v18.18.2/node-v18.18.2-headers.tar.gz
npm ERR! gyp http GET https://nodejs.org/download/release/v18.18.2/SHASUMS256.txt
npm ERR! gyp http GET https://nodejs.org/download/release/v18.18.2/win-x64/node.lib
npm ERR! gyp http 200 https://nodejs.org/download/release/v18.18.2/SHASUMS256.txt
npm ERR! gyp http 200 https://nodejs.org/download/release/v18.18.2/win-x64/node.lib
npm ERR! gyp info find VS using VS2022 (17.7.34202.233) found at:
npm ERR! gyp info find VS ""C:\Program Files\Microsoft Visual Studio\2022\Enterprise""
npm ERR! gyp info find VS run with --verbose for detailed information
npm ERR! gyp info spawn C:\Program Files\Python312\python.exe
npm ERR! gyp info spawn args [
npm ERR! gyp info spawn args   'C:\\Users\\User\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\node-gyp\\gyp\\gyp_main.py',
npm ERR! gyp info spawn args   'binding.gyp',
npm ERR! gyp info spawn args   '-f',
npm ERR! gyp info spawn args   'msvs',
npm ERR! gyp info spawn args   '-I',
npm ERR! gyp info spawn args   'D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@tensorflow\\tfjs-node\\build\\config.gypi',
npm ERR! gyp info spawn args   '-I',
npm ERR! gyp info spawn args   'C:\\Users\\User\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\node-gyp\\addon.gypi',
npm ERR! gyp info spawn args   '-I',
npm ERR! gyp info spawn args   'C:\\Users\\User\\AppData\\Local\\node-gyp\\Cache\\18.18.2\\include\\node\\common.gypi',
npm ERR! gyp info spawn args   '-Dlibrary=shared_library',
npm ERR! gyp info spawn args   '-Dvisibility=default',
npm ERR! gyp info spawn args   '-Dnode_root_dir=C:\\Users\\User\\AppData\\Local\\node-gyp\\Cache\\18.18.2',
npm ERR! gyp info spawn args   '-Dnode_gyp_dir=C:\\Users\\User\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\node-gyp',
npm ERR! gyp info spawn args   '-Dnode_lib_file=C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\node-gyp\\\\Cache\\\\18.18.2\\\\<(target_arch)\\\\node.lib',
npm ERR! gyp info spawn args   '-Dmodule_root_dir=D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@tensorflow\\tfjs-node',npm ERR! gyp info spawn args   '-Dnode_engine=v8',
npm ERR! gyp info spawn args   '--depth=.',
npm ERR! gyp info spawn args   '--no-parallel',
npm ERR! gyp info spawn args   '--generator-output',
npm ERR! gyp info spawn args   'D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@tensorflow\\tfjs-node\\build',
npm ERR! gyp info spawn args   '-Goutput_dir=.'
npm ERR! gyp info spawn args ]
npm ERR! Traceback (most recent call last):
npm ERR!   File ""C:\Users\User\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\gyp\gyp_main.py"", line 42, in <module>
npm ERR!     import gyp  # noqa: E402
npm ERR!     ^^^^^^^^^^
npm ERR!   File ""C:\Users\User\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\gyp\pylib\gyp\__init__.py"", line 9, in <module>
npm ERR!     import gyp.input
npm ERR!   File ""C:\Users\User\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\gyp\pylib\gyp\input.py"", line 19, in <module>
npm ERR!     from distutils.version import StrictVersion
npm ERR! ModuleNotFoundError: No module named 'distutils'
npm ERR! gyp ERR! configure error
npm ERR! gyp ERR! stack Error: `gyp` failed with exit code: 1
npm ERR! gyp ERR! stack     at ChildProcess.onCpExit (C:\Users\User\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\configure.js:325:16)
npm ERR! gyp ERR! stack     at ChildProcess.emit (node:events:517:28)
npm ERR! gyp ERR! stack     at ChildProcess._handle.onexit (node:internal/child_process:292:12)
npm ERR! gyp ERR! System Windows_NT 10.0.19045
npm ERR! gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Users\\User\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8\\tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8"" ""--napi_version=9"" ""--node_abi_napi=napi"" ""--napi_build_version=8"" ""--node_napi_label=napi-v8""
npm ERR! gyp ERR! cwd D:\xampp8\htdocs\testnodejs\node_modules\@tensorflow\tfjs-node
npm ERR! gyp ERR! node -v v18.18.2
npm ERR! gyp ERR! node-gyp -v v9.4.0
npm ERR! gyp ERR! not ok
npm ERR! node-pre-gyp ERR! build error
npm ERR! node-pre-gyp ERR! stack Error: Failed to execute 'C:\Program Files\nodejs\node.exe C:\Users\User\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\bin\node-gyp.js configure --fallback-to-build --module=D:\xampp8\htdocs\testnodejs\node_modules\@tensorflow\tfjs-node\lib\napi-v8\tfjs_binding.node --module_name=tfjs_binding --module_path=D:\xampp8\htdocs\testnodejs\node_modules\@tensorflow\tfjs-node\lib\napi-v8 --napi_version=9 --node_abi_napi=napi --napi_build_version=8 --node_napi_label=napi-v8' (1)
npm ERR! node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (D:\xampp8\htdocs\testnodejs\node_modules\@mapbox\node-pre-gyp\lib\util\compile.js:89:23)
npm ERR! node-pre-gyp ERR! stack     at ChildProcess.emit (node:events:517:28)
npm ERR! node-pre-gyp ERR! stack     at maybeClose (node:internal/child_process:1098:16)
npm ERR! node-pre-gyp ERR! stack     at ChildProcess._handle.onexit (node:internal/child_process:303:5)
npm ERR! node-pre-gyp ERR! System Windows_NT 10.0.19045
npm ERR! node-pre-gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@mapbox\\node-pre-gyp\\bin\\node-pre-gyp"" ""install"" ""--fallback-to-build""
npm ERR! node-pre-gyp ERR! cwd D:\xampp8\htdocs\testnodejs\node_modules\@tensorflow\tfjs-node
npm ERR! node-pre-gyp ERR! node -v v18.18.2
npm ERR! node-pre-gyp ERR! node-pre-gyp -v v1.0.9
npm ERR! node-pre-gyp ERR! not ok
npm ERR! * Downloading libtensorflow
npm ERR!
npm ERR! * Building TensorFlow Node.js bindings

![Screenshot_1](https://github.com/tensorflow/tensorflow/assets/85969639/8f834004-73e9-415c-9436-8a006693074e)
![Screenshot_2](https://github.com/tensorflow/tensorflow/assets/85969639/bea6901a-3557-4f09-b6d0-368ae0852487)
![Screenshot_3](https://github.com/tensorflow/tensorflow/assets/85969639/d4caa528-fbdb-4be4-a0d1-c7c9e03662f4)
![Screenshot_4](https://github.com/tensorflow/tensorflow/assets/85969639/42b7d62e-47af-4571-8d8c-18ab4c256157)
![Screenshot_5](https://github.com/tensorflow/tensorflow/assets/85969639/018b3162-a010-4939-9853-b4735f2fc360)
![Screenshot_6](https://github.com/tensorflow/tensorflow/assets/85969639/0f5fc8a0-57fd-4f6e-9699-573f82b7de04)


what I missed?"
62187,"AddN cannot handle shapes [x,y] + [1,y] or [x,1]","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Windows 11, CentOs8, Ubuntu

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I found this behaviour when calculating hessians of my custom TensorFlow function. For performance, I manipulate some tensors with unique and gather. This pops up for hessians only so I can't think of how to manipulate my code to avoid AddN. 

Surely, AddN should support the same operations as Add?


Output:
`2.13.0 **Also repeated in 2.16.0-dev20231020**
Shapes:
X=  [9 10] 
Y[o,:] =  [1 10]
X + Y  [9 10]
 tf.add_n([x,y]) fails!

    ValueError: Dimension 0 in both shapes must be equal, but are 9 and 1. Shapes are [9,10] and [1,10].
    	From merging shape 0 with other shapes. for '{{node AddN}} = AddN[N=2, T=DT_FLOAT](x, y)' with input shapes: [9,10], [1,10].`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
o=tf.newaxis

nel=tf.range(1,10,1,tf.float32)
nph=tf.range(5,15,1,tf.float32)
nel_2D=tf.repeat(nel[:,o],tf.shape(nph)[0],axis=1)

@tf.function
def add_N(x,y):
 return tf.add_n([x,y])
@tf.function
def add_v2(x,y):
 return x+y

tf.print('Shapes:\nX= ',tf.shape(nel_2D), '\nY[o,:] = ',tf.shape(nph[o,:]))


tf.print('X + Y ',tf.shape(add_v2(nel_2D,nph[o,:])))

tf.print(' tf.add_n([x,y]) fails!')

add_N(nel_2D,nph[o,:])
```


### Relevant log output

_No response_"
62184,core dumped Error with tf.raw_ops.TensorScatterUpdate,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20231005

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

while using the tf.raw_ops.TensorScatterUpdate, I encountered a comre dumped error with below parameters.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf 
print(tf.__version__)
args = {'tensor': tf.random.uniform([4]), 
        'indices': tf.random.uniform([4, 4, 4], 0, 256, dtype=tf.int32), 
        'updates': tf.random.uniform([4])}
res = tf.raw_ops.TensorScatterUpdate(**args)
print(res)
```


### Relevant log output

```shell
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-20 21:24:05.155348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2.15.0-dev20231005
2023-10-20 21:24:07.182885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1924] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3350 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:02:00.0, compute capability: 7.5
2023-10-20 21:24:07.183565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1924] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6826 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:04:00.0, compute capability: 7.5
2023-10-20 21:24:07.184101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1924] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6826 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:83:00.0, compute capability: 7.5
2023-10-20 21:24:07.184626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1924] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 6826 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:84:00.0, compute capability: 7.5
2023-10-20 21:24:07.292391: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```
"
62183,"TensorShape is (None, None, None) on images","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

An I/O pipeline for images using Dataset reads images into tensor but the shape is (None, None, None), preventing some applications that need the tensor shape further down in preprocessing (for example, resizing the image by a factor).

In the example below I can get by by recovering the image dimensions with a trick but it feels very *very* hacky. Is there a reason why the shape should be None?

### Standalone code to reproduce the issue

```shell
def preprocess(file_name):
    x = tf.io.read_file(file_name)
    x = tf.io.decode_jpeg(x)
    
    nrows, ncols,_ = x.shape
    
    x = tf.image.resize(x, (nrows//2, ncols//2))
    
    x = tf.image.random_crop(x, (32,32,3))
    
    return x, tf.zeros_like(x)

def preprocess_workaround(file_name):
    x = tf.io.read_file(file_name)
    x = tf.io.decode_jpeg(x)
    
    nrows = tf.math.reduce_sum(tf.ones_like(x[:,0], dtype=tf.int32))
    ncols = tf.math.reduce_sum(tf.ones_like(x[0,:], dtype=tf.int32))
    
    x = tf.image.resize(x, (nrows//2, ncols//2))
    
    x = tf.image.random_crop(x, (32,32,3))
    
    return x, tf.zeros_like(x)

model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(32,32,3)))
model.add(tf.keras.layers.Conv2D(3, (3,3), padding='same'))
model.summary()

files = glob.glob(""/mnt/ng/ncl/acquisition/stitches/20231019/row1/??/1/crop.jpg"")
print(len(files))

## crashes with the output below
ds = tf.data.Dataset.from_tensor_slices(files).map(preprocess).batch(1)


## the following works:
## ds = tf.data.Dataset.from_tensor_slices(files).map(preprocess_workaround).batch(1)

model.fit(ds)
```
```


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/tmp/ipykernel_583425/761276435.py in <module>
      1 files = glob.glob(""/mnt/ng/ncl/acquisition/stitches/20231019/row1/??/1/crop.jpg"")
      2 print(len(files))
----> 3 ds = tf.data.Dataset.from_tensor_slices(files).map(preprocess).batch(1)
      4 model.fit(ds)

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic, name)
   2292         warnings.warn(""The `deterministic` argument has no effect unless the ""
   2293                       ""`num_parallel_calls` argument is specified."")
-> 2294       return MapDataset(self, map_func, preserve_cardinality=True, name=name)
   2295     else:
   2296       return ParallelMapDataset(

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)
   5497     self._use_inter_op_parallelism = use_inter_op_parallelism
   5498     self._preserve_cardinality = preserve_cardinality
-> 5499     self._map_func = structured_function.StructuredFunctionWrapper(
   5500         map_func,
   5501         self._transformation_name(),

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
    261         fn_factory = trace_tf_function(defun_kwargs)
    262 
--> 263     self._function = fn_factory()
    264     # There is no graph to add in eager mode.
    265     add_to_graph &= not context.executing_eagerly()

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py in get_concrete_function(self, *args, **kwargs)
    224         `tf.Tensor` or `tf.TensorSpec`.
    225     """"""
--> 226     concrete_function = self._get_concrete_function_garbage_collected(
    227         *args, **kwargs)
    228     concrete_function._garbage_collector.release()  # pylint: disable=protected-access

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
    190 
    191     with self._lock:
--> 192       concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)
    193       seen_names = set()
    194       captured = object_identity.ObjectIdentitySet(

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py in _maybe_define_concrete_function(self, args, kwargs)
    155       kwargs = {}
    156 
--> 157     return self._maybe_define_function(args, kwargs)
    158 
    159   def _get_concrete_function_internal_garbage_collected(self, *args, **kwargs):

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py in _maybe_define_function(self, args, kwargs)
    358             args, kwargs = generalized_func_key._placeholder_value()  # pylint: disable=protected-access
    359 
--> 360           concrete_function = self._create_concrete_function(args, kwargs)
    361 
    362           graph_capture_container = concrete_function.graph._capture_func_lib  # pylint: disable=protected-access

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py in _create_concrete_function(self, args, kwargs)
    282     arg_names = base_arg_names + missing_arg_names
    283     concrete_function = monomorphic_function.ConcreteFunction(
--> 284         func_graph_module.func_graph_from_py_func(
    285             self._name,
    286             self._python_function,

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)
   1281         _, original_func = tf_decorator.unwrap(python_func)
   1282 
-> 1283       func_outputs = python_func(*func_args, **func_kwargs)
   1284 
   1285       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py in wrapped_fn(*args)
    238           attributes=defun_kwargs)
    239       def wrapped_fn(*args):  # pylint: disable=missing-docstring
--> 240         ret = wrapper_helper(*args)
    241         ret = structure.to_tensor_list(self._output_structure, ret)
    242         return [ops.convert_to_tensor(t) for t in ret]

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py in wrapper_helper(*args)
    169       if not _should_unpack(nested_args):
    170         nested_args = (nested_args,)
--> 171       ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
    172       ret = variable_utils.convert_variables_to_tensors(ret)
    173       if _should_pack(ret):

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    690       except Exception as e:  # pylint:disable=broad-except
    691         if hasattr(e, 'ag_error_metadata'):
--> 692           raise e.ag_error_metadata.to_exception(e)
    693         else:
    694           raise

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    687       try:
    688         with conversion_ctx:
--> 689           return converted_call(f, args, kwargs, options=options)
    690       except Exception as e:  # pylint:disable=broad-except
    691         if hasattr(e, 'ag_error_metadata'):

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options)
    437     try:
    438       if kwargs is not None:
--> 439         result = converted_f(*effective_args, **kwargs)
    440       else:
    441         result = converted_f(*effective_args)

/tmp/__autograph_generated_filecu6im70c.py in tf__preprocess(file_name)
     11                 x = ag__.converted_call(ag__.ld(tf).io.decode_jpeg, (ag__.ld(x),), None, fscope)
     12                 (nrows, ncols, _) = ag__.ld(x).shape
---> 13                 x = ag__.converted_call(ag__.ld(tf).image.resize, (ag__.ld(x), (ag__.ld(nrows) // 2, ag__.ld(ncols) // 2)), None, fscope)
     14                 x = ag__.converted_call(ag__.ld(tf).image.random_crop, (ag__.ld(x), (32, 32)), None, fscope)
     15                 try:

TypeError: in user code:

    File ""/tmp/ipykernel_583425/4207213119.py"", line 7, in preprocess  *
        x = tf.image.resize(x, (nrows//2, ncols//2))

    TypeError: unsupported operand type(s) for //: 'NoneType' and 'int'
```
"
62182,import keras_cv results in import error for keras_utils,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.14

### Custom code

No

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Running StableDiffusion Finetuning colab example throws the error 

https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/finetune_stable_diffusion.ipynb



### Standalone code to reproduce the issue

```shell
import keras_cv
from keras_cv.models.stable_diffusion.clip_tokenizer import SimpleTokenizer
```


### Relevant log output

```shell
ImportError                               Traceback (most recent call last)
[<ipython-input-2-1a02c2e511a0>](https://localhost:8080/#) in <cell line: 4>()
      2 import os
      3 
----> 4 import keras_cv
      5 import matplotlib.pyplot as plt
      6 import numpy as np

3 frames
[/usr/local/lib/python3.10/dist-packages/keras_cv/models/weights.py](https://localhost:8080/#) in <module>
     12 # See the License for the specific language governing permissions and
     13 import tensorflow as tf
---> 14 from keras.utils import data_utils
     15 
     16 

ImportError: cannot import name 'data_utils' from 'keras.utils' (/usr/local/lib/python3.10/dist-packages/keras/utils/__init__.py)

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
""Open Examples"" button below.
```
"
62181,Group Deconvolution,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  On Windows 10 / WSL2, but targeting but Android
- TensorFlow installed from (source or binary): (using PyTorch and converting to tflite from there)
- TensorFlow version (or github SHA if from source):

Using TFLite 2.8.0, but there's not been significant changes to relevant code in latest (and I'd be happy to upgrade if there were...).

**Provide the text output from tflite_convert**
I'm using TinyNN to convert to direct from PyTorch. it says:
> UserWarning: Group transposed conv is not supported if official tflite interpreter is used.
and
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc#L6213
has groups=1 hard-coded

```
Converting this network:
https://github.com/Picsart-AI-Research/MI-GAN/
Specifically, the function creating the Transpose Convolutions / Deconvolutions is at:
https://github.com/Picsart-AI-Research/MI-GAN/blob/main/lib/model_zoo/migan_inference.py#L79



"
62180,TypeError (Missing required positional argument) when using py_function,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Simply running the example from the py_function docs (https://www.tensorflow.org/api_docs/python/tf/py_function) results in a TypeError.

I am seeing this on my local install but also in a fresh colab notebook, so I don't think it's anything to do with my installation.

I am seeing similar behaviour with `numpy_function`, and when trying to use `py_function` in the other ways described in the docs

I would expect the snippet to run without exceptions.

I can't find any way of using `py_function`, or any workarounds.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1o-VtzDgEm6kNm9mOqojHSd03FlIMg9AL?usp=sharing

@tf.py_function(Tout=tf.float32)
def py_log_huber(x, m):
  print('Running with eager execution.')
  if tf.abs(x) <= m:
    return x**2
  else:
    return m**2 * (1 - 2 * tf.math.log(m) + tf.math.log(x**2))
```


### Relevant log output

```shell
Traceback (most recent call last)
[<ipython-input-2-aea281a2ba70>](https://localhost:8080/#) in <cell line: 2>()
      1 import tensorflow as tf
----> 2 @tf.py_function(Tout=tf.float32)
      3 def py_log_huber(x, m):
      4   print('Running with eager execution.')
      5   if tf.abs(x) <= m:

1 frames
[/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py](https://localhost:8080/#) in op_dispatch_handler(*args, **kwargs)
   1168         if iterable_params is not None:
   1169           args, kwargs = replace_iterable_params(args, kwargs, iterable_params)
-> 1170         result = api_dispatcher.Dispatch(args, kwargs)
   1171         if result is not NotImplemented:
   1172           return result

TypeError: Missing required positional argument
```
"
62175,Output tensor inconsistencies across different TF versions,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12.1 / 2.14.0

### Custom code

No

### OS platform and distribution

Win 10 22H2

### Mobile device

_No response_

### Python version

3.8.0 / 3.11.5

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

0) Save some simple model (does not really matter which one)
1) Activate Py 3.8 + TF 2.12.1 environment
2) Run inference on some random tensor with predetermined seed
3) Save the output tensor to a pickle file
4) Activate Py 3.11.5 + TF 2.14 environment
5) Run inference on the same random tensor with predetermined seed
6) Load previous output tensor which we pickled earlier
7) Compare two outputs with `np.testing.assert_allclose` with default `rtol` of `1e-07`
8) Get an error that the tensors mismatch
9) ???
10) PROFIT

For your convenience, I will attach the model and the pickled tensors. The code I used to produce it all will be down below.
https://drive.google.com/drive/folders/1LWeotexKYGlzu2TtqbwN7ErgW_X_4C_4

### Standalone code to reproduce the issue

#### On Py 3.8 + TF 2.12.1 environment:

```python
out_dir = Path(r'')

image_input = tf.keras.Input(shape=[224, 224, 3])

conv1 = tf.keras.layers.SeparableConv2D(32, (3, 3))(image_input)
conv1 = tf.keras.layers.BatchNormalization()(conv1)
conv1 = tf.keras.layers.ReLU()(conv1)
pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)

conv2 = tf.keras.layers.SeparableConv2D(64, (3, 3))(pool1)
conv2 = tf.keras.layers.BatchNormalization()(conv2)
conv2 = tf.keras.layers.ReLU()(conv2)
pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)

out = tf.keras.layers.Dense(4, activation='softmax')(pool2)

model = tf.keras.Model(image_input, out)

model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(),
              metrics=['categorical_accuracy'])

model.save(out_dir / 'model_tf_12_1')
```

Then load the saved_model, and save the prediction
```python
model_1 = tf.saved_model.load(out_dir / 'model_tf_12_1')

np.random.seed(123) # predetermined seed
random_tensor = np.random.random([1, 224, 224, 3]).astype('float32')

pred_1 = model_1(random_tensor)

with open(out_dir / 'pred_1', mode='wb') as f:
    pickle.dump(pred_1, f)
```

#### On Py 3.11.5 + TF 2.14.0 environment:

```python
model_1 = tf.saved_model.load(out_dir / 'model_tf_12_1')

np.random.seed(123) # predetermined seed
random_tensor = np.random.random([1, 224, 224, 3]).astype('float32')

pred_2 = model_1(random_tensor)

with open(out_dir / 'pred_1', mode='rb') as f:
    pred_1 = pickle.load(f)

np.testing.assert_allclose(pred_1, pred_2)
```

See the output of `np.testing.assert_allclose` below


### Relevant log output

```shell
AssertionError: 
Not equal to tolerance rtol=1e-07, atol=0

Mismatched elements: 753 / 11664 (6.46%)
Max absolute difference: 5.9604645e-08
Max relative difference: 2.3390216e-07
 x: array([[[[0.259732, 0.241168, 0.242284, 0.256816],
         [0.25886 , 0.242881, 0.243037, 0.255222],
         [0.256769, 0.243074, 0.243309, 0.256848],...
 y: array([[[[0.259732, 0.241168, 0.242284, 0.256816],
         [0.25886 , 0.242881, 0.243037, 0.255222],
         [0.256769, 0.243074, 0.243309, 0.256848],...
```
"
62174,tf.map_fn and TensorArray do not seem to support backpropagation,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf2.14

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

****
I provide three different snippet implementation of a function. I am calculating multiple losses with respect to different networks. 
The first and third implementation gives `gradients=None` in both graph and eager mode.

The second implementation works in eager mode but not in graph mode. 

It seems that tf.map_fn does not support backpropagation #19897 and this issue could be related to that.

### Standalone code to reproduce the issue

```shell
First snippet ( Gradients are None in both graph and eager mode )


    def _compute_unadjusted_ce(self, label_batch, logits):
        ce = tf.map_fn(
            lambda x: tf.reduce_mean(
                tf.keras.losses.categorical_crossentropy(
                    from_logits=True,
                    y_pred=logits[x, ...],
                    y_true=label_batch
                )
            ),
            elems=tf.range(self.num_learners),
            fn_output_signature=tf.float32,
            parallel_iterations=1,
            back_prop=True
        )
        return ce



# The following function does the work but fails in graph mode (`tf.function`) ( similar to #37512   


 def _compute_unadjusted_ce(self, label_batch, logits):
       ce = list()
         for i in range(self.num_learners):
            ce.append(
                 tf.reduce_mean(
                     tf.keras.losses.categorical_crossentropy(
                         from_logits=True,
                         y_pred=logits[i, ...],
                         y_true=label_batch
                     )
                 )
             )
    
         return ce


# The following function causes gradients as None ( in both eager and graph mode )


 def _compute_unadjusted_ce(self, label_batch, logits):
         ce = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True, clear_after_read=True)
         for i in range(self.num_learners):
             ce = ce.write(
                 ce.size(),
                 tf.reduce_mean(
                     tf.keras.losses.categorical_crossentropy(
                         from_logits=True,
                         y_pred=logits[i, ...],
                         y_true=label_batch
                     )
                 )
             )
    
         return ce.stack()
```


### Relevant log output

_No response_"
62173,UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods,"### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

- TensorFlow Lite initialisation crashes when using versions 2.12 and above on any Android Emulator with API level between 21 and 25 included.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Steps to reproduce:
1. Create a new Android project.
2. Add TensorFlow Lite dependencies:
```
    implementation(""org.tensorflow:tensorflow-lite:2.14.0"")
    implementation(""org.tensorflow:tensorflow-lite-support:0.4.3"")
```
3. [Download](https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/latest/blaze_face_short_range.tflite) BlazeFace tflite model and add to src/main/assets folder
4. Initialise Tensorflow Lite Interpreter in `onCreate()` of MainActivity:
```
        val model = FileUtil.loadMappedFile(this, ""blaze_face_short_range.tflite"")
        val options = Interpreter.Options()
        options.numThreads = 4
        val interpreter = Interpreter(model, options)
```
5. Run the app on any Android Emulator using API Level between 21 and 25 included.
6. App crashes with the following error:
```
 E  FATAL EXCEPTION: main
    Process: com.example.myapplication, PID: 8645
    java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():
      java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""strtod_l"" referenced by ""/data/app/com.example.myapplication-2/base.apk!/lib/arm64-v8a/libtensorflowlite_jni.so""...
    	at org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:137)
    	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:62)
    	at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:36)
    	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:232)
    	at com.example.myapplication.MainActivity.onCreate(MainActivity.kt:35)
    	at android.app.Activity.performCreate(Activity.java:6679)
    	at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1118)
    	at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2618)
    	at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2726)
    	at android.app.ActivityThread.-wrap12(ActivityThread.java)
    	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1477)
    	at android.os.Handler.dispatchMessage(Handler.java:102)
    	at android.os.Looper.loop(Looper.java:154)
    	at android.app.ActivityThread.main(ActivityThread.java:6119)
    	at java.lang.reflect.Method.invoke(Native Method)
    	at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:886)
    	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:776)
    Caused by: java.lang.UnsatisfiedLinkError: No implementation found for void org.tensorflow.lite.TensorFlowLite.nativeDoNothing() (tried Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing and Java_org_tensorflow_lite_TensorFlowLite_nativeDoNothing__)
    	at org.tensorflow.lite.TensorFlowLite.nativeDoNothing(Native Method)
    	at org.tensorflow.lite.TensorFlowLite.init(TensorFlowLite.java:132)
    	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:62)Â 
    	at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:36)Â 
    	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:232)Â 
    	at com.example.myapplication.MainActivity.onCreate(MainActivity.kt:35)Â 
    	at android.app.Activity.performCreate(Activity.java:6679)Â 
    	at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1118)Â 
    	at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2618)Â 
    	at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2726)Â 
    	at android.app.ActivityThread.-wrap12(ActivityThread.java)Â 
    	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1477)Â 
    	at android.os.Handler.dispatchMessage(Handler.java:102)Â 
    	at android.os.Looper.loop(Looper.java:154)Â 
    	at android.app.ActivityThread.main(ActivityThread.java:6119)Â 
    	at java.lang.reflect.Method.invoke(Native Method)Â 
    	at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:886)Â 
    	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:776)Â 
``` "
62172,Set reshuffle_each_iteration option of shuffle as True by default instead of None,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When shuffling a dataset, the option reshuffle_each_iteration is None by default. But, when we look closer at the documentation, the default behavior is to reshuffle each iteration.
It would be better to set the default value of reshuffle_each_iteration to 'True' instead of None which is ambiguous.
Please find attached the code of the function at : https://github.com/tensorflow/tensorflow/blame/8c25f8252175c6a4f9614797e953fcbfb93c3aed/tensorflow/python/data/ops/shuffle_op.py#L49C12-L49C12

### Standalone code to reproduce the issue

```shell
# Code in tensorflow/tensorflow/python/data/ops/shuffle_op.py
class _ShuffleDataset(dataset_ops.UnaryUnchangedStructureDataset):
  """"""A `Dataset` that randomly shuffles the elements of its input.""""""

  def __init__(self,
               input_dataset,
               buffer_size,
               seed=None,
               reshuffle_each_iteration=None,
               name=None):
    """"""See `Dataset.shuffle()` for details.""""""
    self._input_dataset = input_dataset
    self._buffer_size = ops.convert_to_tensor(
        buffer_size, dtype=dtypes.int64, name=""buffer_size"")
    self._seed, self._seed2 = random_seed.get_seed(seed)
    if reshuffle_each_iteration is None:
      reshuffle_each_iteration = True
    self._reshuffle_each_iteration = reshuffle_each_iteration
    self._name = name
```


### Relevant log output

_No response_"
62171,TFLite CNN model quantization error,"### 1. System information

- OS Platform and Distribution : Ubuntu 22.04.3 LTS
- TensorFlow installation: pip install tensorflow (virtual env: venv)
- TensorFlow library: pip package -> tensorflow 2.14.0

### 2. Code

Colab to build the models and reproduce the issue:

[Reproduce the issue](https://colab.research.google.com/drive/1sx7qmXfP5RA1ituF5Fmh8X-cy0-M7Xol?usp=sharing)


### 3. Failure after conversion
Hi, I'm having an issue when trying to use signatures of quantized tflite CNN model.

The conversion and quantization go well, but when I try to use infer or fine_tune signatures, I get the following error which seems to be related to the quantization process:

RuntimeError: tensorflow/lite/kernels/conv.cc:374 affine_quantization->zero_point->data[i] != 0 (-24 != 0)Node number 21 (CONV_2D) failed to prepare.tensorflow/lite/kernels/conv.cc:374 affine_quantization->zero_point->data[i] != 0 (-24 != 0)Node number 41 (CONV_2D) failed to prepare.


**Note**: I don't get this error with the exact same code using linear model instead of CNN.

Thanks for your help"
62170,"Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10175 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5)","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-gpu 2.4.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04

### Mobile device

Ubuntu 18.04

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.0 8.0

### GPU model and memory

T4 15G

### Current behavior?

I deploment an u-net like model and use it to matting, why it take so much memory

### Standalone code to reproduce the issue

```shell
As the above description
```


### Relevant log output

```shell
python'''
2023-10-19 17:12:05,653 - modelscope - INFO - PyTorch version 2.0.1 Found.
2023-10-19 17:12:05,654 - modelscope - INFO - TensorFlow version 2.4.0 Found.
2023-10-19 17:12:05,654 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer
2023-10-19 17:12:05,764 - modelscope - INFO - Updating the files for the changes of local files, first time updating will take longer time! Please wait till updating done!
2023-10-19 17:12:05,765 - modelscope - INFO - AST-Scanning the path ""/root/miniconda/envs/modelscope/lib/python3.8/site-packages/modelscope"" with the following sub folders ['models', 'metrics', 'pipelines', 'preprocessors', 'trainers', 'msdatasets', 'exporters']
2023-10-19 17:12:05,769 - modelscope - INFO - Scanning done! A number of 2 components indexed or updated! Time consumed 0.0038497447967529297s
2023-10-19 17:12:05,809 - modelscope - INFO - Loading done! Current index file version is 1.9.2, with md5 e5b1c859add8b0dd7419f7819a63c9a0 and a total number of 941 components indexed
2023-10-19 17:12:06,946 - modelscope - INFO - Model revision not specified, use revision: v1.0.0
2023-10-19 17:12:07.433075: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-10-19 17:12:08,529 - modelscope - INFO - initiate model from /root/.cache/modelscope/hub/damo/cv_unet_universal-matting
2023-10-19 17:12:08,529 - modelscope - INFO - initiate model from location /root/.cache/modelscope/hub/damo/cv_unet_universal-matting.
2023-10-19 17:12:08,532 - modelscope - WARNING - No preprocessor field found in cfg.
2023-10-19 17:12:08,532 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.
2023-10-19 17:12:08,532 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/root/.cache/modelscope/hub/damo/cv_unet_universal-matting'}. trying to build by task and model information.
2023-10-19 17:12:08,532 - modelscope - WARNING - Find task: universal-matting, model type: None. Insufficient information to build preprocessor, skip building preprocessor
WARNING:tensorflow:From /root/miniconda/envs/modelscope/lib/python3.8/site-packages/modelscope/utils/device.py:60: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2023-10-19 17:12:08.533865: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-19 17:12:08.534257: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-10-19 17:12:08.534330: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2023-10-19 17:12:08.545027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:08.545636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:07.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2023-10-19 17:12:08.545678: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-10-19 17:12:08.545736: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-10-19 17:12:08.545821: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-10-19 17:12:08.545859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-10-19 17:12:08.545897: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-10-19 17:12:08.549506: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-10-19 17:12:08.549600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-10-19 17:12:08.549652: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-10-19 17:12:08.549761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:08.550454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:08.551001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2023-10-19 17:12:10.620467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-10-19 17:12:10.620510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2023-10-19 17:12:10.620518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2023-10-19 17:12:10.620749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.621411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.622007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.622561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 10175 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5)
2023-10-19 17:12:10.623148: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-10-19 17:12:10.623261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.623834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:07.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2023-10-19 17:12:10.623921: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-10-19 17:12:10.623944: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-10-19 17:12:10.623960: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-10-19 17:12:10.624018: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-10-19 17:12:10.624068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-10-19 17:12:10.624169: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-10-19 17:12:10.624189: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-10-19 17:12:10.624208: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-10-19 17:12:10.624284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.624888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.625400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2023-10-19 17:12:10.625769: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-10-19 17:12:10.625880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.626610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:07.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2023-10-19 17:12:10.626686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-10-19 17:12:10.626737: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-10-19 17:12:10.626769: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-10-19 17:12:10.626816: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-10-19 17:12:10.626863: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-10-19 17:12:10.626962: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-10-19 17:12:10.626986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-10-19 17:12:10.627003: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-10-19 17:12:10.627074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.627931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.628514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2023-10-19 17:12:10.628550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-10-19 17:12:10.628561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2023-10-19 17:12:10.628569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2023-10-19 17:12:10.628681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.629268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.629781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10175 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5)
2023-10-19 17:12:10.630834: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-10-19 17:12:10.630927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.631480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:00:07.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2023-10-19 17:12:10.631536: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-10-19 17:12:10.631553: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-10-19 17:12:10.631570: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-10-19 17:12:10.631586: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-10-19 17:12:10.631604: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-10-19 17:12:10.631645: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-10-19 17:12:10.631709: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-10-19 17:12:10.631755: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-10-19 17:12:10.631851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.632539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.633141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2023-10-19 17:12:10.633167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-10-19 17:12:10.633175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2023-10-19 17:12:10.633183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2023-10-19 17:12:10.633288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.633865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-10-19 17:12:10.634366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10175 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5)
2023-10-19 17:12:10,634 - modelscope - INFO - loading model from /root/.cache/modelscope/hub/damo/cv_unet_universal-matting/tf_graph.pb
WARNING:tensorflow:From /root/miniconda/envs/modelscope/lib/python3.8/site-packages/modelscope/pipelines/cv/image_matting_pipeline.py:47: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.gfile.GFile.
2023-10-19 17:12:14,106 - modelscope - INFO - load model done
INFO:     Started server process [26718]
'''
```
"
62169,null pointer dereference in split_v,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A maliciously constructed `split_v` operator model leads to `op_context.input` being empty, causing a null pointer dereference in the `Prepare` function.
```c
// maximum_minimum.cc
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 3);

  OpContext op_context(context, node);

  TF_LITE_ENSURE_EQ(context, NumOutputs(node), op_context.params->num_splits);

  auto input_type = op_context.input->type;   // op_context.input is nullptr
```
[split_v.zip](https://github.com/tensorflow/tensorflow/files/13038752/split_v.zip)


### Standalone code to reproduce the issue

```shell
I use the benchmark tool built according to [this official guide](https://www.tensorflow.org/lite/guide/build_cmake#step_1_install_cmake_tool), as follows:
1. git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
2. mkdir tflite_build && cd tflite_build
3. cmake ../tensorflow_src/tensorflow/lite
4. cmake --build . -j
5. cmake --build . -j -t benchmark_model

The benchmark is in the tools directory


When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).

â¯ ./benchmark_model --graph=../poc/split_v.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/split_v.tflite]
INFO: Loaded model ../poc/split_v.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[1]    13162 segmentation fault (core dumped)  ./benchmark_model --graph=../poc/split_v.tflite
```


### Relevant log output

_No response_"
62168,null pointer dereference in reduce_prod,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A maliciously constructed `reduce_prod` operator model leads to `op_context.axis` being empty, causing a null pointer dereference in the `PrepareSimple` function.
```c
// reduce.cc
TfLiteStatus PrepareSimple(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

  OpContext op_context(context, node);
  TF_LITE_ENSURE_TYPES_EQ(context, op_context.axis->type, kTfLiteInt32);   // op_context.axis is nullptr
```
[reduce_prod.zip](https://github.com/tensorflow/tensorflow/files/13038718/reduce_prod.zip)


### Standalone code to reproduce the issue

```shell
I use the benchmark tool built according to [this official guide](https://www.tensorflow.org/lite/guide/build_cmake#step_1_install_cmake_tool), as follows:
1. git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
2. mkdir tflite_build && cd tflite_build
3. cmake ../tensorflow_src/tensorflow/lite
4. cmake --build . -j
5. cmake --build . -j -t benchmark_model

The benchmark is in the tools directory


When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).

â¯ ./benchmark_model --graph=../poc/reduce_prod.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/reduce_prod.tflite]
INFO: Loaded model ../poc/reduce_prod.tflite
ERROR: Invalid tensor index 10 in inputs. The subgraph has 3 tensors

ERROR: Invalid tensor index 24 in outputs. The subgraph has 3 tensors

INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[1]    9785 segmentation fault (core dumped)  ./benchmark_model --graph=../poc/reduce_prod.tflite
```


### Relevant log output

_No response_"
62167,null pointer dereference in pad,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A maliciously constructed `pad` operator model leads to `op_context.output` being empty, causing a null pointer dereference in the `Prepare` function.
```c
// pad.cc
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE(context, NumInputs(node) == 2 || NumInputs(node) == 3);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

  PadContext op_context(context, node);
  if (IsConstantTensor(op_context.paddings)) {
    TF_LITE_ENSURE_MSG(context, !CheckPaddingOverflow(&op_context),
                       ""INT64 padding overflow. Only support value between ""
                       ""INT32_MIN and INT32_MAX."");
  }
  TF_LITE_ENSURE_TYPES_EQ(context, op_context.input->type,
op_context.output->type);   // op_context.output is nullptr
```
[pad.zip](https://github.com/tensorflow/tensorflow/files/13038679/pad.zip)


### Standalone code to reproduce the issue

```shell
I use the benchmark tool built according to [this official guide](https://www.tensorflow.org/lite/guide/build_cmake#step_1_install_cmake_tool), as follows:
1. git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
2. mkdir tflite_build && cd tflite_build
3. cmake ../tensorflow_src/tensorflow/lite
4. cmake --build . -j
5. cmake --build . -j -t benchmark_model

The benchmark is in the tools directory


When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).

â¯ ./benchmark_model --graph=../poc/pad.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/pad.tflite]
INFO: Loaded model ../poc/pad.tflite
ERROR: Invalid tensor index 4098 in outputs. The subgraph has 3 tensors

INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[1]    7087 segmentation fault (core dumped)  ./benchmark_model --graph=../poc/pad.tflite
```


### Relevant log output

_No response_"
62166,null pointer dereference in onehot,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A maliciously constructed `onehot` operator model leads to `op_context.output` being empty, causing a null pointer dereference in the `Prepare` function.
```c
// one_hot.cc
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 4);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

  OneHotContext op_context{context, node};
  switch (op_context.dtype) {
    // TODO(b/111744875): Support uint8 and quantization.
    case kTfLiteFloat32:
    case kTfLiteInt16:
    case kTfLiteInt32:
    case kTfLiteInt64:
    case kTfLiteInt8:
    case kTfLiteUInt8:
    case kTfLiteBool:
      op_context.output->type = op_context.dtype;  // op_context.output is nullptr
```
[onehot.zip](https://github.com/tensorflow/tensorflow/files/13038648/onehot.zip)


### Standalone code to reproduce the issue

```shell
I use the benchmark tool built according to [this official guide](https://www.tensorflow.org/lite/guide/build_cmake#step_1_install_cmake_tool), as follows:
1. git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
2. mkdir tflite_build && cd tflite_build
3. cmake ../tensorflow_src/tensorflow/lite
4. cmake --build . -j
5. cmake --build . -j -t benchmark_model

The benchmark is in the tools directory


When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).

â¯ ./benchmark_model --graph=../poc/onehot.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/onehot.tflite]
INFO: Loaded model ../poc/onehot.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[1]    3686 segmentation fault (core dumped)  ./benchmark_model --graph=../poc/onehot.tflite
```


### Relevant log output

_No response_"
62165,null pointer dereference in maximum,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A maliciously constructed `maximum` operator model leads to `op_context.input1` being empty, causing a null pointer dereference in the `Prepare` function.
```c
// maximum_minimum.cc
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

  OpContext op_context(context, node);
  TF_LITE_ENSURE_TYPES_EQ(context, op_context.input1->type,   // op_context.input1 is nullptr
```
[maximum.zip](https://github.com/tensorflow/tensorflow/files/13037309/maximum.zip)


### Standalone code to reproduce the issue

```shell
I use the benchmark tool built according to [this official guide](https://www.tensorflow.org/lite/guide/build_cmake#step_1_install_cmake_tool), as follows:
1. git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
2. mkdir tflite_build && cd tflite_build
3. cmake ../tensorflow_src/tensorflow/lite
4. cmake --build . -j
5. cmake --build . -j -t benchmark_model

The benchmark is in the tools directory


When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).

â¯ ./benchmark_model --graph=../poc/maximum.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/maximum.tflite]
INFO: Loaded model ../poc/maximum.tflite
ERROR: Invalid tensor index 65536 in inputs. The subgraph has 3 tensors

INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[1]    16244 segmentation fault (core dumped)  ./benchmark_model --graph=../poc/maximum.tflite
```


### Relevant log output

_No response_"
62164,null pointer dereference in reshape,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A maliciously constructed `reshape` operator model leads to `input` being empty, causing a null pointer dereference in the `IsConstantTensor` function.
```c
// reshape.cc
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  OpData* op_data = reinterpret_cast<OpData*>(node->user_data);
  op_data->output_ptr = nullptr;

  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context,
                    GetOutputSafe(context, node, kOutputTensor, &output));
  if (output->type != kTfLiteString) {
    const TfLiteTensor* input = GetInput(context, node, kInputTensor);
    const TfLiteTensor* shape = GetInput(context, node, kShapeTensor);
    if (NumInputs(node) == 1 || IsConstantOrPersistentTensor(shape)) {
      if (IsConstantOrPersistentTensor(input)) {   // input is nullptr
        SetTensorToPersistentRo(output);
```

```c
// kernel_util.h
inline bool IsConstantOrPersistentTensor(const TfLiteTensor* tensor) {
  return IsConstantTensor(tensor) ||
         (tensor->allocation_type == kTfLitePersistentRo);
}

inline bool IsConstantTensor(const TfLiteTensor* tensor) {
  return tensor->allocation_type == kTfLiteMmapRo;  // tensor is nullptr
}
```

[reshape.zip](https://github.com/tensorflow/tensorflow/files/13037278/reshape.zip)


### Standalone code to reproduce the issue

```shell
I use the benchmark tool built according to [this official guide](https://www.tensorflow.org/lite/guide/build_cmake#step_1_install_cmake_tool), as follows:
1. git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
2. mkdir tflite_build && cd tflite_build
3. cmake ../tensorflow_src/tensorflow/lite
4. cmake --build . -j
5. cmake --build . -j -t benchmark_model

The benchmark is in the tools directory


When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).

â¯ ./benchmark_model --graph=../poc/reshape.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/reshape.tflite]
INFO: Loaded model ../poc/reshape.tflite
ERROR: Invalid tensor index 32 in inputs. The subgraph has 3 tensors

INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[1]    10805 segmentation fault (core dumped)  ./benchmark_model --graph=../poc/reshape.tflite
```


### Relevant log output

_No response_"
62163,null pointer dereference in exp,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A maliciously constructed `exp` operator model leads to `op_context.input` being empty, causing a null pointer dereference in the `Prepare` function.
```c
// exp.cc
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  OpData* data = static_cast<OpData*>(node->user_data);
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

  ExpContext op_context(context, node);
  const TfLiteTensor* input = op_context.input;
  TfLiteTensor* output = op_context.output;

  TfLiteIntArray* output_dims = TfLiteIntArrayCopy(input->dims);
  output->type = input->type;   // input is nullptr
```
[exp.zip](https://github.com/tensorflow/tensorflow/files/13037247/exp.zip)


### Standalone code to reproduce the issue

```shell
I use the benchmark tool built according to [this official guide](https://www.tensorflow.org/lite/guide/build_cmake#step_1_install_cmake_tool), as follows:
1. git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
2. mkdir tflite_build && cd tflite_build
3. cmake ../tensorflow_src/tensorflow/lite
4. cmake --build . -j
5. cmake --build . -j -t benchmark_model

The benchmark is in the tools directory


When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).
â¯ ./benchmark_model --graph=../poc/exp.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/exp.tflite]
INFO: Loaded model ../poc/exp.tflite
ERROR: Invalid tensor index 1025 in inputs. The subgraph has 3 tensors

INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[1]    6687 segmentation fault (core dumped)  ./benchmark_model --graph=../poc/exp.tflite
```


### Relevant log output

_No response_"
62162,null pointer dereference in dequantize,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A maliciously constructed `dequantize` operator model leads to `op_context.input` being empty, causing a null pointer dereference in the `Prepare` function.

```c
// dequantize.cc
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

  OpContext op_context(context, node);

  TF_LITE_ENSURE(context, op_context.input->type == kTfLiteUInt8 ||   // op_context.input is nullptr
```
[dequantize.zip](https://github.com/tensorflow/tensorflow/files/13037224/dequantize.zip)


### Standalone code to reproduce the issue

```shell
I use the benchmark tool built according to [this official guide](https://www.tensorflow.org/lite/guide/build_cmake#step_1_install_cmake_tool), as follows:
1. git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
2. mkdir tflite_build && cd tflite_build
3. cmake ../tensorflow_src/tensorflow/lite
4. cmake --build . -j
5. cmake --build . -j -t benchmark_model

The benchmark is in the tools directory


When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).
â¯ ./benchmark_model --graph=../poc/dequantize.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/dequantize.tflite]
INFO: Loaded model ../poc/dequantize.tflite
ERROR: Invalid tensor index 256 in outputs. The subgraph has 3 tensors

INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[1]    1062 segmentation fault (core dumped)  ./benchmark_model --graph=../poc/dequantize.tflite
```


### Relevant log output

_No response_"
62161,null pointer dereference in densify,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A maliciously constructed densify operator model leads to `op_context.input` being empty, causing a null pointer dereference in the `Prepare` function.
```c
// densify.cc
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

  OpContext op_context(context, node);

  TF_LITE_ENSURE(context, op_context.input->type != kTfLiteString); // op_context.input is nullptr
```
[densify.zip](https://github.com/tensorflow/tensorflow/files/13037207/densify.zip)

### Standalone code to reproduce the issue

```shell
I use the benchmark tool built according to [this official guide](https://www.tensorflow.org/lite/guide/build_cmake#step_1_install_cmake_tool), as follows:
1. git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
2. mkdir tflite_build && cd tflite_build
3. cmake ../tensorflow_src/tensorflow/lite
4. cmake --build . -j
5. cmake --build . -j -t benchmark_model

The benchmark is in the tools directory


When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).

â¯ ./benchmark_model --graph=../poc/densify.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/densify.tflite]
INFO: Loaded model ../poc/densify.tflite
ERROR: Invalid tensor index 8193 in inputs. The subgraph has 3 tensors

INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[1]    30027 segmentation fault (core dumped)  ./benchmark_model --graph=../poc/densify.tflite
```


### Relevant log output

_No response_"
62160,null pointer dereference in squeeze,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A maliciously constructed `squeeze` operator model leads to `op_context.output` being empty, causing a null pointer dereference in the `ResizeTensor` function.
```c
// squeeze.cc
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  SqueezeContext op_context(context, node);
      ...
  return context->ResizeTensor(context, op_context.output, output_dims);  // op_context.output is null
}
```

```c
// subgraph.cc
TfLiteStatus Subgraph::ResizeTensor(TfLiteContext* context,
                                    TfLiteTensor* tensor,
                                    TfLiteIntArray* new_size) {
  if (tensor->data.raw != nullptr &&   // tensor is nullptr
      EqualArrayAndTfLiteIntArray(tensor->dims, new_size->size,
                                  new_size->data)) {
```

[squeeze.zip](https://github.com/tensorflow/tensorflow/files/13037193/squeeze.zip)


### Standalone code to reproduce the issue

```shell
I use the benchmark tool built according to [this official guide](https://www.tensorflow.org/lite/guide/build_cmake#step_1_install_cmake_tool), as follows:
1. git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
2. mkdir tflite_build && cd tflite_build
3. cmake ../tensorflow_src/tensorflow/lite
4. cmake --build . -j
5. cmake --build . -j -t benchmark_model

The benchmark is in the tools directory


When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).

â¯ ./benchmark_model --graph=../poc/squeeze.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/squeeze.tflite]
INFO: Loaded model ../poc/squeeze.tflite
ERROR: Invalid tensor index 1025 in inputs. The subgraph has 3 tensors

INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[1]    25682 segmentation fault (core dumped)  ./benchmark_model --graph=../poc/squeeze.tflite
```


### Relevant log output

_No response_"
62159,OOB write in relu6,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A maliciously constructed `Relu6` operator model may lead to an out-of-bounds write in the `Relu6Eval` function, resulting in denial of service due to accessing an illegal address.
```c
// activations.cc
TfLiteStatus Relu6Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  ReluOpData* data = reinterpret_cast<ReluOpData*>(node->user_data);
  switch (input->type) {
    case kTfLiteFloat32: {
      size_t elements = input->bytes / sizeof(float);
      const float* in = GetTensorData<float>(input);
      const float* in_end = in + elements;
      float* out = GetTensorData<float>(output);
      for (; in < in_end; in++, out++) *out = std::min(std::max(0.f, *in), 6.f);  // this line 
```
[relu6.zip](https://github.com/tensorflow/tensorflow/files/13037007/relu6.zip)


### Standalone code to reproduce the issue

```shell
I use the benchmark tool built according to [this official guide](https://www.tensorflow.org/lite/guide/build_cmake#step_1_install_cmake_tool), as follows:
1. git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
2. mkdir tflite_build
cd tflite_build
3. cmake ../tensorflow_src/tensorflow/lite
4. cmake --build . -j
5. cmake --build . -j -t benchmark_model

The benchmark is in the tools directory


When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).

â¯ ./benchmark_model --graph=../poc/relu6.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/relu6.tflite]
INFO: Loaded model ../poc/relu6.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
ERROR: failed to delegate RELU node #0
ERROR: Node number 2 (TfLiteXNNPackDelegate) failed to prepare.
ERROR: Restored original execution plan after delegate application failure.
INFO: The input model file size (MB): 0.00062
INFO: Initialized session in 181.67ms.
INFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
[1]    12443 segmentation fault (core dumped)  ./benchmark_model --graph=../poc/relu6.tflite
```


### Relevant log output

_No response_"
62151,Building failure of libtensorflowlite_gpu_delegate.so,"System information

OS Platform and Distribution: Ubuntu 20.04
TensorFlow installed from (source or binary): source
TensorFlow version: 2.12.0
Bazel version (if compiling from source): 5.3.0
GCC/Compiler version (if compiling from source): 9.4.0


Describe the problem
Building libtensorflowlite_gpu_delegate.so fails
------------------------

Provide the exact sequence of commands / steps that you executed before running into the problem

bazel build -c opt tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so --copt -DEGL_NO_X11=1

Any other info / logs

ERROR: /home/sstc/tensorflow/tensorflow/lite/delegates/gpu/BUILD:134:10: Linking tensorflow/lite/delegates/gpu/libtensorflowlite_gpu_delegate.so failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/libtensorflowlite_gpu_delegate.so-2.params
/usr/bin/ld: cannot find -lnativewindow
/usr/bin/ld: cannot find -lnativewindow
collect2: error: ld returned 1 exit status
Target //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 123.625s, Critical Path: 46.21s
INFO: 804 processes: 190 internal, 614 local.
FAILED: Build did NOT complete successfully"
62150,Cross compilation TensorFlow Lite with CMake mat error,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf.2.8

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

gcc-arm-10.3

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I want to get a libtensorflow.so for my Arm Linux.

### Standalone code to reproduce the issue

```shell
ARMCC_FLAGS=""-march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -mfp16-format=ieee""
cmake -DCMAKE_C_COMPILER=gcc-arm-10.3-2021.07-x86_64-arm-none-linux-gnueabihf/bin/arm-none-linux-gnueabihf-gcc \
  -DCMAKE_CXX_COMPILER=gcc-arm-10.3-2021.07-x86_64-arm-none-linux-gnueabihf/bin/arm-none-linux-gnueabihf-g++ \
  -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}"" \
  -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}"" \
  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
  -DCMAKE_SYSTEM_NAME=Linux \
  -DCMAKE_SYSTEM_PROCESSOR=armv7 \
  ../tensorflow/lite/

make -j 16

then I get the Error:
w/tensorflow/build_sdk/xnnpack/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-1x16c4-minmax-neondot.c
[ 79%] Building C object _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o
cd /mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/_deps/xnnpack-build && /mnt/fileroot/guanghui.wan/nanoQ/Algorithm/tools/gcc-arm-10.3-2021.07-x86_64-arm-none-linux-gnueabihf/bin/arm-none-linux-gnueabihf-gcc -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ARM_BF16=0 -DXNN_ENABLE_ARM_DOTPROD=1 -DXNN_ENABLE_ARM_FP16_SCALAR=1 -DXNN_ENABLE_ARM_FP16_VECTOR=1 -DXNN_ENABLE_ARM_I8MM=0 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_CPUINFO=1 -DXNN_ENABLE_DWCONV_MULTIPASS=0 -DXNN_ENABLE_GEMM_M_SPECIALIZATION=1 -DXNN_ENABLE_JIT=0 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_RISCV_VECTOR=1 -DXNN_ENABLE_SPARSE=1 -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/third_party/xla/third_party/tsl -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/xnnpack/src -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/pthreadpool-source/include -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/FXdiv-source/include -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/FP16-source/include -march=armv8-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -mfp16-format=ieee -O3 -DNDEBUG -std=c99 -fPIC -Wno-psabi -O2 -pthread  -fno-math-errno  -marm  -march=armv8.2-a+dotprod -mfpu=neon-fp-armv8  -MD -MT _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o -MF CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o.d -o CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o -c /mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/xnnpack/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c
/tmp/ccrZ5z20.s: Assembler messages:
/tmp/ccrZ5z20.s:75: Error: selected processor does not support `vsdot.s8 q14,q12,d7[0]' in ARM mode
/tmp/ccrZ5z20.s:76: Error: selected processor does not support `vsdot.s8 q10,q9,d7[0]' in ARM mode
/tmp/ccrZ5z20.s:77: Error: selected processor does not support `vsdot.s8 q14,q11,d7[1]' in ARM mode
/tmp/ccrZ5z20.s:78: Error: selected processor does not support `vsdot.s8 q10,q8,d7[1]' in ARM mode
/tmp/ccrZ5z20.s:134: Error: selected processor does not support `vsdot.s8 q14,q9,d7[0]' in ARM mode
/tmp/ccrZ5z20.s:135: Error: selected processor does not support `vsdot.s8 q10,q8,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49877: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-1x8c4-minmax-neondot.c.o] Error 1
make[2]: *** Waiting for unfinished jobs....
/tmp/ccWEkUHN.s: Assembler messages:
/tmp/ccWEkUHN.s:81: Error: selected processor does not support `vsdot.s8 q12,q9,d7[0]' in ARM mode
/tmp/ccWEkUHN.s:82: Error: selected processor does not support `vsdot.s8 q13,q2,d7[0]' in ARM mode
/tmp/ccWEkUHN.s:83: Error: selected processor does not support `vsdot.s8 q15,q8,d7[0]' in ARM mode
/tmp/ccWEkUHN.s:85: Error: selected processor does not support `vsdot.s8 q10,q14,d7[0]' in ARM mode
/tmp/ccWEkUHN.s:91: Error: selected processor does not support `vsdot.s8 q12,q8,d7[1]' in ARM mode
/tmp/ccWEkUHN.s:93: Error: selected processor does not support `vsdot.s8 q13,q14,d7[1]' in ARM mode
/tmp/ccWEkUHN.s:99: Error: selected processor does not support `vsdot.s8 q10,q14,d7[1]' in ARM mode
/tmp/ccWEkUHN.s:100: Error: selected processor does not support `vsdot.s8 q15,q8,d7[1]' in ARM mode
/tmp/ccWEkUHN.s:184: Error: selected processor does not support `vsdot.s8 q13,q2,d7[0]' in ARM mode
/tmp/ccWEkUHN.s:185: Error: selected processor does not support `vsdot.s8 q12,q9,d7[0]' in ARM mode
/tmp/ccWEkUHN.s:186: Error: selected processor does not support `vsdot.s8 q10,q14,d7[0]' in ARM mode
/tmp/ccWEkUHN.s:187: Error: selected processor does not support `vsdot.s8 q15,q8,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49905: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-1x16c4-minmax-neondot.c.o] Error 1
/tmp/ccEQXTmv.s: Assembler messages:
/tmp/ccEQXTmv.s:88: Error: selected processor does not support `vsdot.s8 q15,q11,d7[0]' in ARM mode
/tmp/ccEQXTmv.s:89: Error: selected processor does not support `vsdot.s8 q14,q9,d7[0]' in ARM mode
/tmp/ccEQXTmv.s:90: Error: selected processor does not support `vsdot.s8 q15,q10,d7[1]' in ARM mode
/tmp/ccEQXTmv.s:91: Error: selected processor does not support `vsdot.s8 q14,q8,d7[1]' in ARM mode
/tmp/ccEQXTmv.s:93: Error: selected processor does not support `vsdot.s8 q13,q11,d7[0]' in ARM mode
/tmp/ccEQXTmv.s:94: Error: selected processor does not support `vsdot.s8 q12,q9,d7[0]' in ARM mode
/tmp/ccEQXTmv.s:95: Error: selected processor does not support `vsdot.s8 q13,q10,d7[1]' in ARM mode
/tmp/ccEQXTmv.s:96: Error: selected processor does not support `vsdot.s8 q12,q8,d7[1]' in ARM mode
/tmp/ccEQXTmv.s:189: Error: selected processor does not support `vsdot.s8 q15,q9,d6[0]' in ARM mode
/tmp/ccEQXTmv.s:190: Error: selected processor does not support `vsdot.s8 q14,q8,d6[0]' in ARM mode
/tmp/ccEQXTmv.s:191: Error: selected processor does not support `vsdot.s8 q13,q9,d7[0]' in ARM mode
/tmp/ccEQXTmv.s:192: Error: selected processor does not support `vsdot.s8 q12,q8,d7[0]' in ARM mode
/tmp/ccmAlACc.s: make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49933: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-2x8c4-minmax-neondot.c.o] Error 1
Assembler messages:
/tmp/ccmAlACc.s:87: Error: selected processor does not support `vsdot.s8 q1,q10,q8' in ARM mode
/tmp/ccmAlACc.s:90: Error: selected processor does not support `vsdot.s8 q0,q5,q8' in ARM mode
/tmp/ccmAlACc.s:91: Error: selected processor does not support `vsdot.s8 q2,q9,q8' in ARM mode
/tmp/ccmAlACc.s:92: Error: selected processor does not support `vsdot.s8 q14,q4,q8' in ARM mode
/tmp/ccmAlACc.s:101: Error: selected processor does not support `vsdot.s8 q13,q5,q8' in ARM mode
/tmp/ccmAlACc.s:102: Error: selected processor does not support `vsdot.s8 q3,q10,q8' in ARM mode
/tmp/ccmAlACc.s:103: Error: selected processor does not support `vsdot.s8 q12,q4,q8' in ARM mode
/tmp/ccmAlACc.s:104: Error: selected processor does not support `vsdot.s8 q15,q9,q8' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49919: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-1x16c8-minmax-neondot-ld64.c.o] Error 1
/tmp/ccBfIgBU.s: Assembler messages:
/tmp/ccBfIgBU.s:109: Error: selected processor does not support `vsdot.s8 q14,q10,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:110: Error: selected processor does not support `vsdot.s8 q13,q9,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:111: Error: selected processor does not support `vsdot.s8 q12,q8,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:112: Error: selected processor does not support `vsdot.s8 q1,q11,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:114: Error: selected processor does not support `vsdot.s8 q0,q10,d7[0]' in ARM mode
/tmp/ccBfIgBU.s:115: Error: selected processor does not support `vsdot.s8 q4,q9,d7[0]' in ARM mode
/tmp/ccBfIgBU.s:116: Error: selected processor does not support `vsdot.s8 q5,q8,d7[0]' in ARM mode
/tmp/ccBfIgBU.s:117: Error: selected processor does not support `vsdot.s8 q6,q11,d7[0]' in ARM mode
/tmp/ccBfIgBU.s:126: Error: selected processor does not support `vsdot.s8 q14,q11,d6[1]' in ARM mode
/tmp/ccBfIgBU.s:127: Error: selected processor does not support `vsdot.s8 q0,q11,d7[1]' in ARM mode
/tmp/ccBfIgBU.s:128: Error: selected processor does not support `vsdot.s8 q13,q10,d6[1]' in ARM mode
/tmp/ccBfIgBU.s:129: Error: selected processor does not support `vsdot.s8 q4,q10,d7[1]' in ARM mode
/tmp/ccBfIgBU.s:130: Error: selected processor does not support `vsdot.s8 q12,q9,d6[1]' in ARM mode
/tmp/ccBfIgBU.s:131: Error: selected processor does not support `vsdot.s8 q1,q8,d6[1]' in ARM mode
/tmp/ccBfIgBU.s:132: Error: selected processor does not support `vsdot.s8 q5,q9,d7[1]' in ARM mode
/tmp/ccBfIgBU.s:133: Error: selected processor does not support `vsdot.s8 q6,q8,d7[1]' in ARM mode
/tmp/ccBfIgBU.s:255: Error: selected processor does not support `vsdot.s8 q13,q9,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:256: Error: selected processor does not support `vsdot.s8 q4,q9,d7[0]' in ARM mode
/tmp/ccBfIgBU.s:258: Error: selected processor does not support `vsdot.s8 q14,q10,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:260: Error: selected processor does not support `vsdot.s8 q0,q10,d7[0]' in ARM mode
/tmp/ccBfIgBU.s:262: Error: selected processor does not support `vsdot.s8 q12,q9,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:263: Error: selected processor does not support `vsdot.s8 q5,q9,d7[0]' in ARM mode
/tmp/ccBfIgBU.s:264: Error: selected processor does not support `vsdot.s8 q1,q8,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:265: Error: selected processor does not support `vsdot.s8 q6,q8,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49947: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-2x16c4-minmax-neondot.c.o] Error 1
/tmp/ccDJj1kS.s: Assembler messages:
/tmp/ccDJj1kS.s:75: Error: selected processor does not support `vsdot.s8 q15,q12,q10' in ARM mode
/tmp/ccDJj1kS.s:76: Error: selected processor does not support `vsdot.s8 q2,q9,q10' in ARM mode
/tmp/ccDJj1kS.s:77: Error: selected processor does not support `vsdot.s8 q13,q11,q10' in ARM mode
/tmp/ccDJj1kS.s:78: Error: selected processor does not support `vsdot.s8 q3,q8,q10' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49891: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-1x8c8-minmax-neondot-ld64.c.o] Error 1
/tmp/ccRqcaMJ.s: Assembler messages:
/tmp/ccRqcaMJ.s:143: Error: selected processor does not support `vsdot.s8 q1,q14,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:146: Error: selected processor does not support `vsdot.s8 q5,q14,d5[0]' in ARM mode
/tmp/ccRqcaMJ.s:147: Error: selected processor does not support `vsdot.s8 q15,q14,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:149: Error: selected processor does not support `vsdot.s8 q6,q7,d5[0]' in ARM mode
/tmp/ccRqcaMJ.s:151: Error: selected processor does not support `vsdot.s8 q4,q7,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:152: Error: selected processor does not support `vsdot.s8 q0,q7,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:158: Error: selected processor does not support `vsdot.s8 q7,q14,d5[0]' in ARM mode
/tmp/ccRqcaMJ.s:159: Error: selected processor does not support `vsdot.s8 q13,q10,d5[0]' in ARM mode
/tmp/ccRqcaMJ.s:160: Error: selected processor does not support `vsdot.s8 q12,q14,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:162: Error: selected processor does not support `vsdot.s8 q11,q14,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:163: Error: selected processor does not support `vsdot.s8 q10,q1,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:166: Error: selected processor does not support `vsdot.s8 q14,q1,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:170: Error: selected processor does not support `vsdot.s8 q6,q1,d5[1]' in ARM mode
/tmp/ccRqcaMJ.s:171: Error: selected processor does not support `vsdot.s8 q5,q9,d5[1]' in ARM mode
/tmp/ccRqcaMJ.s:172: Error: selected processor does not support `vsdot.s8 q4,q1,d6[1]' in ARM mode
/tmp/ccRqcaMJ.s:173: Error: selected processor does not support `vsdot.s8 q0,q1,d7[1]' in ARM mode
/tmp/ccRqcaMJ.s:174: Error: selected processor does not support `vsdot.s8 q15,q9,d7[1]' in ARM mode
/tmp/ccRqcaMJ.s:177: Error: selected processor does not support `vsdot.s8 q1,q9,d6[1]' in ARM mode
/tmp/ccRqcaMJ.s:186: Error: selected processor does not support `vsdot.s8 q7,q9,d5[1]' in ARM mode
/tmp/ccRqcaMJ.s:187: Error: selected processor does not support `vsdot.s8 q10,q8,d6[1]' in ARM mode
/tmp/ccRqcaMJ.s:188: Error: selected processor does not support `vsdot.s8 q14,q8,d7[1]' in ARM mode
/tmp/ccRqcaMJ.s:191: Error: selected processor does not support `vsdot.s8 q13,q8,d5[1]' in ARM mode
/tmp/ccRqcaMJ.s:192: Error: selected processor does not support `vsdot.s8 q12,q9,d6[1]' in ARM mode
/tmp/ccRqcaMJ.s:193: Error: selected processor does not support `vsdot.s8 q11,q9,d7[1]' in ARM mode
/tmp/ccRqcaMJ.s:453: Error: selected processor does not support `vsdot.s8 q6,q9,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:454: Error: selected processor does not support `vsdot.s8 q5,q8,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:456: Error: selected processor does not support `vsdot.s8 q4,q9,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:457: Error: selected processor does not support `vsdot.s8 q0,q9,d5[0]' in ARM mode
/tmp/ccRqcaMJ.s:458: Error: selected processor does not support `vsdot.s8 q1,q8,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:460: Error: selected processor does not support `vsdot.s8 q15,q8,d5[0]' in ARM mode
/tmp/ccRqcaMJ.s:467: Error: selected processor does not support `vsdot.s8 q10,q9,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:468: Error: selected processor does not support `vsdot.s8 q12,q9,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:469: Error: selected processor does not support `vsdot.s8 q11,q9,d5[0]' in ARM mode
/tmp/ccRqcaMJ.s:472: Error: selected processor does not support `vsdot.s8 q13,q8,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:474: Error: selected processor does not support `vsdot.s8 q9,q8,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:478: Error: selected processor does not support `vsdot.s8 q9,q8,d5[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49961: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-3x16c4-minmax-neondot.c.o] Error 1
/tmp/ccuyJ4D9.s: Assembler messages:
/tmp/ccuyJ4D9.s:64: Error: selected processor does not support `vsdot.s8 q9,q11,d7[0]' in ARM mode
/tmp/ccuyJ4D9.s:66: Error: selected processor does not support `vsdot.s8 q8,q10,d7[0]' in ARM mode
/tmp/ccuyJ4D9.s:69: Error: selected processor does not support `vsdot.s8 q9,q10,d7[1]' in ARM mode
/tmp/ccuyJ4D9.s:73: Error: selected processor does not support `vsdot.s8 q8,q10,d7[1]' in ARM mode
/tmp/ccuyJ4D9.s:130: Error: selected processor does not support `vsdot.s8 q9,q11,d7[0]' in ARM mode
/tmp/ccuyJ4D9.s:132: Error: selected processor does not support `vsdot.s8 q8,q10,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:50031: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-1x8c4-minmax-neondot.c.o] Error 1
/tmp/ccRNmUcC.s: Assembler messages:
/tmp/ccRNmUcC.s:110: Error: selected processor does not support `vsdot.s8 q4,q11,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:111: Error: selected processor does not support `vsdot.s8 q2,q9,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:112: Error: selected processor does not support `vsdot.s8 q4,q10,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:113: Error: selected processor does not support `vsdot.s8 q2,q8,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:116: Error: selected processor does not support `vsdot.s8 q0,q11,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:117: Error: selected processor does not support `vsdot.s8 q15,q9,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:118: Error: selected processor does not support `vsdot.s8 q0,q10,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:119: Error: selected processor does not support `vsdot.s8 q15,q8,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:123: Error: selected processor does not support `vsdot.s8 q1,q11,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:124: Error: selected processor does not support `vsdot.s8 q14,q9,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:125: Error: selected processor does not support `vsdot.s8 q1,q10,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:126: Error: selected processor does not support `vsdot.s8 q14,q8,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:129: Error: selected processor does not support `vsdot.s8 q13,q11,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:130: Error: selected processor does not support `vsdot.s8 q12,q9,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:131: Error: selected processor does not support `vsdot.s8 q13,q10,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:132: Error: selected processor does not support `vsdot.s8 q12,q8,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:300: Error: selected processor does not support `vsdot.s8 q4,q10,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:301: Error: selected processor does not support `vsdot.s8 q2,q9,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:303: Error: selected processor does not support `vsdot.s8 q0,q10,d6[0]' in ARM mode
/tmp/ccRNmUcC.s:304: Error: selected processor does not support `vsdot.s8 q15,q9,d6[0]' in ARM mode
/tmp/ccRNmUcC.s:305: Error: selected processor does not support `vsdot.s8 q1,q10,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:307: Error: selected processor does not support `vsdot.s8 q13,q10,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:309: Error: selected processor does not support `vsdot.s8 q14,q9,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:311: Error: selected processor does not support `vsdot.s8 q12,q9,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49975: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-4x8c4-minmax-neondot.c.o] Error 1
/tmp/ccNllqu0.s: Assembler messages:
/tmp/ccNllqu0.s:176: Error: selected processor does not support `vsdot.s8 q6,q11,d4[0]' in ARM mode
/tmp/ccNllqu0.s:177: Error: selected processor does not support `vsdot.s8 q4,q11,d5[0]' in ARM mode
/tmp/ccNllqu0.s:178: Error: selected processor does not support `vsdot.s8 q15,q11,d6[0]' in ARM mode
/tmp/ccNllqu0.s:179: Error: selected processor does not support `vsdot.s8 q14,q11,d7[0]' in ARM mode
/tmp/ccNllqu0.s:183: Error: selected processor does not support `vsdot.s8 q7,q12,d4[0]' in ARM mode
/tmp/ccNllqu0.s:184: Error: selected processor does not support `vsdot.s8 q5,q12,d5[0]' in ARM mode
/tmp/ccNllqu0.s:186: Error: selected processor does not support `vsdot.s8 q0,q12,d6[0]' in ARM mode
/tmp/ccNllqu0.s:189: Error: selected processor does not support `vsdot.s8 q1,q12,d7[0]' in ARM mode
/tmp/ccNllqu0.s:195: Error: selected processor does not support `vsdot.s8 q9,q11,d5[0]' in ARM mode
/tmp/ccNllqu0.s:196: Error: selected processor does not support `vsdot.s8 q8,q11,d7[0]' in ARM mode
/tmp/ccNllqu0.s:199: Error: selected processor does not support `vsdot.s8 q13,q11,d4[0]' in ARM mode
/tmp/ccNllqu0.s:200: Error: selected processor does not support `vsdot.s8 q12,q10,d4[0]' in ARM mode
/tmp/ccNllqu0.s:203: Error: selected processor does not support `vsdot.s8 q9,q10,d5[0]' in ARM mode
/tmp/ccNllqu0.s:211: Error: selected processor does not support `vsdot.s8 q9,q11,d6[0]' in ARM mode
/tmp/ccNllqu0.s:215: Error: selected processor does not support `vsdot.s8 q9,q10,d7[0]' in ARM mode
/tmp/ccNllqu0.s:221: Error: selected processor does not support `vsdot.s8 q11,q10,d6[0]' in ARM mode
/tmp/ccNllqu0.s:225: Error: selected processor does not support `vsdot.s8 q7,q10,d4[1]' in ARM mode
/tmp/ccNllqu0.s:226: Error: selected processor does not support `vsdot.s8 q5,q10,d5[1]' in ARM mode
/tmp/ccNllqu0.s:227: Error: selected processor does not support `vsdot.s8 q0,q10,d6[1]' in ARM mode
/tmp/ccNllqu0.s:228: Error: selected processor does not support `vsdot.s8 q1,q10,d7[1]' in ARM mode
/tmp/ccNllqu0.s:231: Error: selected processor does not support `vsdot.s8 q6,q9,d4[1]' in ARM mode
/tmp/ccNllqu0.s:232: Error: selected processor does not support `vsdot.s8 q4,q9,d5[1]' in ARM mode
/tmp/ccNllqu0.s:233: Error: selected processor does not support `vsdot.s8 q15,q9,d6[1]' in ARM mode
/tmp/ccNllqu0.s:234: Error: selected processor does not support `vsdot.s8 q14,q9,d7[1]' in ARM mode
/tmp/ccNllqu0.s:240: Error: selected processor does not support `vsdot.s8 q10,q9,d5[1]' in ARM mode
/tmp/ccNllqu0.s:241: Error: selected processor does not support `vsdot.s8 q13,q9,d4[1]' in ARM mode
/tmp/ccNllqu0.s:244: Error: selected processor does not support `vsdot.s8 q12,q8,d4[1]' in ARM mode
/tmp/ccNllqu0.s:245: Error: selected processor does not support `vsdot.s8 q11,q8,d6[1]' in ARM mode
/tmp/ccNllqu0.s:248: Error: selected processor does not support `vsdot.s8 q10,q8,d5[1]' in ARM mode
/tmp/ccNllqu0.s:255: Error: selected processor does not support `vsdot.s8 q10,q9,d6[1]' in ARM mode
/tmp/ccNllqu0.s:260: Error: selected processor does not support `vsdot.s8 q10,q9,d7[1]' in ARM mode
/tmp/ccNllqu0.s:264: Error: selected processor does not support `vsdot.s8 q9,q8,d7[1]' in ARM mode
/tmp/ccNllqu0.s:658: Error: selected processor does not support `vsdot.s8 q7,q9,d7[0]' in ARM mode
/tmp/ccNllqu0.s:659: Error: selected processor does not support `vsdot.s8 q6,q8,d7[0]' in ARM mode
/tmp/ccNllqu0.s:660: Error: selected processor does not support `vsdot.s8 q5,q9,d6[0]' in ARM mode
/tmp/ccNllqu0.s:661: Error: selected processor does not support `vsdot.s8 q4,q8,d6[0]' in ARM mode
/tmp/ccNllqu0.s:665: Error: selected processor does not support `vsdot.s8 q0,q9,d5[0]' in ARM mode
/tmp/ccNllqu0.s:667: Error: selected processor does not support `vsdot.s8 q15,q8,d5[0]' in ARM mode
/tmp/ccNllqu0.s:669: Error: selected processor does not support `vsdot.s8 q1,q9,d4[0]' in ARM mode
/tmp/ccNllqu0.s:670: Error: selected processor does not support `vsdot.s8 q14,q8,d4[0]' in ARM mode
/tmp/ccNllqu0.s:678: Error: selected processor does not support `vsdot.s8 q10,q8,d7[0]' in ARM mode
/tmp/ccNllqu0.s:679: Error: selected processor does not support `vsdot.s8 q13,q9,d7[0]' in ARM mode
/tmp/ccNllqu0.s:684: Error: selected processor does not support `vsdot.s8 q10,q9,d6[0]' in ARM mode
/tmp/ccNllqu0.s:689: Error: selected processor does not support `vsdot.s8 q10,q8,d6[0]' in ARM mode
/tmp/ccNllqu0.s:694: Error: selected processor does not support `vsdot.s8 q10,q9,d5[0]' in ARM mode
/tmp/ccNllqu0.s:699: Error: selected processor does not support `vsdot.s8 q10,q9,d4[0]' in ARM mode
/tmp/ccNllqu0.s:704: Error: selected processor does not support `vsdot.s8 q9,q8,d5[0]' in ARM mode
/tmp/ccNllqu0.s:708: Error: selected processor does not support `vsdot.s8 q9,q8,d4[0]' in ARM mode
/tmp/ccWKfZvp.s: Assembler messages:
/tmp/ccWKfZvp.s:65: Error: selected processor does not support `vsdot.s8 q10,q14,q8' in ARM mode
/tmp/ccWKfZvp.s:68: Error: selected processor does not support `vsdot.s8 q12,q13,q8' in ARM mode
/tmp/ccWKfZvp.s:71: Error: selected processor does not support `vsdot.s8 q9,q13,q8' in ARM mode
/tmp/ccWKfZvp.s:75: Error: selected processor does not support `vsdot.s8 q11,q13,q8' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49989: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-4x16c4-minmax-neondot.c.o] Error 1
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:50045: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-1x8c8-minmax-neondot-ld64.c.o] Error 1
/tmp/ccw5qRTa.s: Assembler messages:
/tmp/ccw5qRTa.s:183: Error: selected processor does not support `vsdot.s8 q6,q7,d2[0]' in ARM mode
/tmp/ccw5qRTa.s:184: Error: selected processor does not support `vsdot.s8 q4,q7,d3[0]' in ARM mode
/tmp/ccw5qRTa.s:185: Error: selected processor does not support `vsdot.s8 q10,q7,d4[0]' in ARM mode
/tmp/ccw5qRTa.s:186: Error: selected processor does not support `vsdot.s8 q15,q7,d5[0]' in ARM mode
/tmp/ccw5qRTa.s:188: Error: selected processor does not support `vsdot.s8 q11,q7,d6[0]' in ARM mode
/tmp/ccw5qRTa.s:189: Error: selected processor does not support `vsdot.s8 q5,q9,d2[0]' in ARM mode
/tmp/ccw5qRTa.s:190: Error: selected processor does not support `vsdot.s8 q0,q9,d3[0]' in ARM mode
/tmp/ccw5qRTa.s:193: Error: selected processor does not support `vsdot.s8 q10,q9,d4[0]' in ARM mode
/tmp/ccw5qRTa.s:196: Error: selected processor does not support `vsdot.s8 q12,q9,d5[0]' in ARM mode
/tmp/ccw5qRTa.s:197: Error: selected processor does not support `vsdot.s8 q14,q9,d6[0]' in ARM mode
/tmp/ccw5qRTa.s:198: Error: selected processor does not support `vsdot.s8 q13,q9,d7[0]' in ARM mode
/tmp/ccw5qRTa.s:201: Error: selected processor does not support `vsdot.s8 q10,q7,d7[0]' in ARM mode
/tmp/ccw5qRTa.s:206: Error: selected processor does not support `vsdot.s8 q6,q9,d2[1]' in ARM mode
/tmp/ccw5qRTa.s:207: Error: selected processor does not support `vsdot.s8 q5,q8,d2[1]' in ARM mode
/tmp/ccw5qRTa.s:208: Error: selected processor does not support `vsdot.s8 q4,q9,d3[1]' in ARM mode
/tmp/ccw5qRTa.s:209: Error: selected processor does not support `vsdot.s8 q0,q8,d3[1]' in ARM mode
/tmp/ccw5qRTa.s:210: Error: selected processor does not support `vsdot.s8 q10,q9,d7[1]' in ARM mode
/tmp/ccw5qRTa.s:212: Error: selected processor does not support `vsdot.s8 q15,q9,d5[1]' in ARM mode
/tmp/ccw5qRTa.s:213: Error: selected processor does not support `vsdot.s8 q1,q9,d4[1]' in ARM mode
/tmp/ccw5qRTa.s:214: Error: selected processor does not support `vsdot.s8 q12,q8,d5[1]' in ARM mode
/tmp/ccw5qRTa.s:216: Error: selected processor does not support `vsdot.s8 q11,q9,d6[1]' in ARM mode
/tmp/ccw5qRTa.s:217: Error: selected processor does not support `vsdot.s8 q14,q8,d6[1]' in ARM mode
/tmp/ccw5qRTa.s:220: Error: selected processor does not support `vsdot.s8 q13,q8,d7[1]' in ARM mode
/tmp/ccw5qRTa.s:221: Error: selected processor does not support `vsdot.s8 q1,q8,d4[1]' in ARM mode
/tmp/ccw5qRTa.s:513: Error: selected processor does not support `vsdot.s8 q6,q9,d7[0]' in ARM mode
/tmp/ccw5qRTa.s:514: Error: selected processor does not support `vsdot.s8 q5,q8,d7[0]' in ARM mode
/tmp/ccw5qRTa.s:515: Error: selected processor does not support `vsdot.s8 q4,q9,d6[0]' in ARM mode
/tmp/ccw5qRTa.s:516: Error: selected processor does not support `vsdot.s8 q0,q8,d6[0]' in ARM mode
/tmp/ccw5qRTa.s:517: Error: selected processor does not support `vsdot.s8 q15,q9,d4[0]' in ARM mode
/tmp/ccw5qRTa.s:519: Error: selected processor does not support `vsdot.s8 q12,q8,d4[0]' in ARM mode
/tmp/ccw5qRTa.s:520: Error: selected processor does not support `vsdot.s8 q3,q9,d5[0]' in ARM mode
/tmp/ccw5qRTa.s:521: Error: selected processor does not support `vsdot.s8 q11,q9,d3[0]' in ARM mode
/tmp/ccw5qRTa.s:523: Error: selected processor does not support `vsdot.s8 q10,q9,d2[0]' in ARM mode
/tmp/ccw5qRTa.s:524: Error: selected processor does not support `vsdot.s8 q14,q8,d3[0]' in ARM mode
/tmp/ccw5qRTa.s:527: Error: selected processor does not support `vsdot.s8 q13,q8,d2[0]' in ARM mode
/tmp/ccw5qRTa.s:528: Error: selected processor does not support `vsdot.s8 q3,q8,d5[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:50003: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-6x8c4-minmax-neondot.c.o] Error 1
/tmp/cczsG4jB.s: Assembler messages:
/tmp/cczsG4jB.s:76: Error: selected processor does not support `vsdot.s8 q12,q2,q8' in ARM mode
/tmp/cczsG4jB.s:79: Error: selected processor does not support `vsdot.s8 q3,q2,q8' in ARM mode
/tmp/cczsG4jB.s:82: Error: selected processor does not support `vsdot.s8 q11,q2,q8' in ARM mode
/tmp/cczsG4jB.s:85: Error: selected processor does not support `vsdot.s8 q15,q2,q8' in ARM mode
/tmp/cczsG4jB.s:88: Error: selected processor does not support `vsdot.s8 q10,q2,q8' in ARM mode
/tmp/cczsG4jB.s:91: Error: selected processor does not support `vsdot.s8 q14,q2,q8' in ARM mode
/tmp/cczsG4jB.s:94: Error: selected processor does not support `vsdot.s8 q9,q2,q8' in ARM mode
/tmp/cczsG4jB.s:98: Error: selected processor does not support `vsdot.s8 q13,q2,q8' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:50073: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-1x16c8-minmax-neondot-ld64.c.o] Error 1
/tmp/cchtznlO.s: Assembler messages:
/tmp/cchtznlO.s:236: Error: selected processor does not support `vsdot.s8 q7,q12,d2[0]' in ARM mode
/tmp/cchtznlO.s:238: Error: selected processor does not support `vsdot.s8 q6,q11,d2[0]' in ARM mode
/tmp/cchtznlO.s:240: Error: selected processor does not support `vsdot.s8 q5,q12,d3[0]' in ARM mode
/tmp/cchtznlO.s:242: Error: selected processor does not support `vsdot.s8 q4,q11,d3[0]' in ARM mode
/tmp/cchtznlO.s:245: Error: selected processor does not support `vsdot.s8 q0,q12,d4[0]' in ARM mode
/tmp/cchtznlO.s:246: Error: selected processor does not support `vsdot.s8 q9,q11,d4[0]' in ARM mode
/tmp/cchtznlO.s:247: Error: selected processor does not support `vsdot.s8 q14,q12,d5[0]' in ARM mode
/tmp/cchtznlO.s:250: Error: selected processor does not support `vsdot.s8 q13,q11,d5[0]' in ARM mode
/tmp/cchtznlO.s:251: Error: selected processor does not support `vsdot.s8 q15,q11,d7[0]' in ARM mode
/tmp/cchtznlO.s:255: Error: selected processor does not support `vsdot.s8 q9,q12,d6[0]' in ARM mode
/tmp/cchtznlO.s:261: Error: selected processor does not support `vsdot.s8 q9,q12,d7[0]' in ARM mode
/tmp/cchtznlO.s:267: Error: selected processor does not support `vsdot.s8 q12,q11,d6[0]' in ARM mode
/tmp/cchtznlO.s:274: Error: selected processor does not support `vsdot.s8 q9,q11,d2[0]' in ARM mode
/tmp/cchtznlO.s:275: Error: selected processor does not support `vsdot.s8 q8,q10,d3[0]' in ARM mode
/tmp/cchtznlO.s:281: Error: selected processor does not support `vsdot.s8 q9,q10,d2[0]' in ARM mode
/tmp/cchtznlO.s:286: Error: selected processor does not support `vsdot.s8 q9,q11,d3[0]' in ARM mode
/tmp/cchtznlO.s:291: Error: selected processor does not support `vsdot.s8 q9,q11,d4[0]' in ARM mode
/tmp/cchtznlO.s:296: Error: selected processor does not support `vsdot.s8 q8,q10,d4[0]' in ARM mode
/tmp/cchtznlO.s:301: Error: selected processor does not support `vsdot.s8 q9,q11,d5[0]' in ARM mode
/tmp/cchtznlO.s:306: Error: selected processor does not support `vsdot.s8 q8,q10,d5[0]' in ARM mode
/tmp/cchtznlO.s:311: Error: selected processor does not support `vsdot.s8 q9,q11,d6[0]' in ARM mode
/tmp/cchtznlO.s:316: Error: selected processor does not support `vsdot.s8 q8,q11,d7[0]' in ARM mode
/tmp/cchtznlO.s:321: Error: selected processor does not support `vsdot.s8 q9,q10,d7[0]' in ARM mode
/tmp/cchtznlO.s:329: Error: selected processor does not support `vsdot.s8 q11,q10,d6[0]' in ARM mode
/tmp/cchtznlO.s:335: Error: selected processor does not support `vsdot.s8 q8,q9,d4[1]' in ARM mode
/tmp/cchtznlO.s:336: Error: selected processor does not support `vsdot.s8 q7,q10,d2[1]' in ARM mode
/tmp/cchtznlO.s:339: Error: selected processor does not support `vsdot.s8 q5,q10,d3[1]' in ARM mode
/tmp/cchtznlO.s:340: Error: selected processor does not support `vsdot.s8 q0,q10,d4[1]' in ARM mode
/tmp/cchtznlO.s:341: Error: selected processor does not support `vsdot.s8 q14,q10,d5[1]' in ARM mode
/tmp/cchtznlO.s:344: Error: selected processor does not support `vsdot.s8 q8,q10,d6[1]' in ARM mode
/tmp/cchtznlO.s:347: Error: selected processor does not support `vsdot.s8 q12,q9,d6[1]' in ARM mode
/tmp/cchtznlO.s:348: Error: selected processor does not support `vsdot.s8 q6,q9,d2[1]' in ARM mode
/tmp/cchtznlO.s:349: Error: selected processor does not support `vsdot.s8 q4,q9,d3[1]' in ARM mode
/tmp/cchtznlO.s:352: Error: selected processor does not support `vsdot.s8 q8,q10,d7[1]' in ARM mode
/tmp/cchtznlO.s:353: Error: selected processor does not support `vsdot.s8 q13,q9,d5[1]' in ARM mode
/tmp/cchtznlO.s:356: Error: selected processor does not support `vsdot.s8 q15,q9,d7[1]' in ARM mode
/tmp/cchtznlO.s:367: Error: selected processor does not support `vsdot.s8 q10,q12,d2[1]' in ARM mode
/tmp/cchtznlO.s:368: Error: selected processor does not support `vsdot.s8 q11,q8,d6[1]' in ARM mode
/tmp/cchtznlO.s:372: Error: selected processor does not support `vsdot.s8 q10,q8,d2[1]' in ARM mode
/tmp/cchtznlO.s:377: Error: selected processor does not support `vsdot.s8 q10,q12,d3[1]' in ARM mode
/tmp/cchtznlO.s:382: Error: selected processor does not support `vsdot.s8 q10,q8,d3[1]' in ARM mode
/tmp/cchtznlO.s:387: Error: selected processor does not support `vsdot.s8 q10,q12,d4[1]' in ARM mode
/tmp/cchtznlO.s:392: Error: selected processor does not support `vsdot.s8 q10,q8,d4[1]' in ARM mode
/tmp/cchtznlO.s:397: Error: selected processor does not support `vsdot.s8 q9,q8,d7[1]' in ARM mode
/tmp/cchtznlO.s:404: Error: selected processor does not support `vsdot.s8 q10,q12,d5[1]' in ARM mode
/tmp/cchtznlO.s:409: Error: selected processor does not support `vsdot.s8 q10,q8,d5[1]' in ARM mode
/tmp/cchtznlO.s:414: Error: selected processor does not support `vsdot.s8 q10,q12,d6[1]' in ARM mode
/tmp/cchtznlO.s:419: Error: selected processor does not support `vsdot.s8 q10,q12,d7[1]' in ARM mode
/tmp/cchtznlO.s:1117: Error: selected processor does not support `vsdot.s8 q7,q9,d7[0]' in ARM mode
/tmp/cchtznlO.s:1118: Error: selected processor does not support `vsdot.s8 q6,q8,d7[0]' in ARM mode
/tmp/cchtznlO.s:1119: Error: selected processor does not support `vsdot.s8 q5,q9,d6[0]' in ARM mode
/tmp/cchtznlO.s:1120: Error: selected processor does not support `vsdot.s8 q4,q8,d6[0]' in ARM mode
/tmp/cchtznlO.s:1124: Error: selected processor does not support `vsdot.s8 q12,q8,d5[0]' in ARM mode
/tmp/cchtznlO.s:1126: Error: selected processor does not support `vsdot.s8 q0,q9,d5[0]' in ARM mode
/tmp/cchtznlO.s:1134: Error: selected processor does not support `vsdot.s8 q14,q9,d4[0]' in ARM mode
/tmp/cchtznlO.s:1136: Error: selected processor does not support `vsdot.s8 q13,q8,d4[0]' in ARM mode
/tmp/cchtznlO.s:1140: Error: selected processor does not support `vsdot.s8 q12,q9,d3[0]' in ARM mode
/tmp/cchtznlO.s:1147: Error: selected processor does not support `vsdot.s8 q12,q9,d2[0]' in ARM mode
/tmp/cchtznlO.s:1148: Error: selected processor does not support `vsdot.s8 q15,q8,d2[0]' in ARM mode
/tmp/cchtznlO.s:1151: Error: selected processor does not support `vsdot.s8 q9,q8,d3[0]' in ARM mode
/tmp/cchtznlO.s:1162: Error: selected processor does not support `vsdot.s8 q10,q9,d7[0]' in ARM mode
/tmp/cchtznlO.s:1166: Error: selected processor does not support `vsdot.s8 q10,q8,d7[0]' in ARM mode
/tmp/cchtznlO.s:1171: Error: selected processor does not support `vsdot.s8 q10,q9,d6[0]' in ARM mode
/tmp/cchtznlO.s:1176: Error: selected processor does not support `vsdot.s8 q10,q8,d6[0]' in ARM mode
/tmp/cchtznlO.s:1181: Error: selected processor does not support `vsdot.s8 q10,q9,d5[0]' in ARM mode
/tmp/cchtznlO.s:1186: Error: selected processor does not support `vsdot.s8 q10,q8,d5[0]' in ARM mode
/tmp/cchtznlO.s:1191: Error: selected processor does not support `vsdot.s8 q10,q9,d4[0]' in ARM mode
/tmp/cchtznlO.s:1196: Error: selected processor does not support `vsdot.s8 q10,q8,d4[0]' in ARM mode
/tmp/cchtznlO.s:1201: Error: selected processor does not support `vsdot.s8 q10,q9,d3[0]' in ARM mode
/tmp/cchtznlO.s:1206: Error: selected processor does not support `vsdot.s8 q10,q9,d2[0]' in ARM mode
/tmp/cchtznlO.s:1211: Error: selected processor does not support `vsdot.s8 q9,q8,d3[0]' in ARM mode
/tmp/cchtznlO.s:1216: Error: selected processor does not support `vsdot.s8 q9,q8,d2[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:50017: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-6x16c4-minmax-neondot.c.o] Error 1
/tmp/ccJrNOda.s: Assembler messages:
/tmp/ccJrNOda.s:72: Error: selected processor does not support `vsdot.s8 q8,q13,d7[0]' in ARM mode
/tmp/ccJrNOda.s:74: Error: selected processor does not support `vsdot.s8 q11,q12,d7[0]' in ARM mode
/tmp/ccJrNOda.s:77: Error: selected processor does not support `vsdot.s8 q10,q12,d7[0]' in ARM mode
/tmp/ccJrNOda.s:80: Error: selected processor does not support `vsdot.s8 q9,q12,d7[0]' in ARM mode
/tmp/ccJrNOda.s:83: Error: selected processor does not support `vsdot.s8 q8,q12,d7[1]' in ARM mode
/tmp/ccJrNOda.s:86: Error: selected processor does not support `vsdot.s8 q11,q12,d7[1]' in ARM mode
/tmp/ccJrNOda.s:89: Error: selected processor does not support `vsdot.s8 q10,q12,d7[1]' in ARM mode
/tmp/ccJrNOda.s:93: Error: selected processor does not support `vsdot.s8 q9,q12,d7[1]' in ARM mode
/tmp/ccJrNOda.s:171: Error: selected processor does not support `vsdot.s8 q8,q13,d7[0]' in ARM mode
/tmp/ccJrNOda.s:173: Error: selected processor does not support `vsdot.s8 q11,q12,d7[0]' in ARM mode
/tmp/ccJrNOda.s:176: Error: selected processor does not support `vsdot.s8 q10,q12,d7[0]' in ARM mode
/tmp/ccJrNOda.s:180: Error: selected processor does not support `vsdot.s8 q9,q12,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:50059: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-1x16c4-minmax-neondot.c.o] Error 1
/tmp/cceW7UTH.s: Assembler messages:
/tmp/cceW7UTH.s:78: Error: selected processor does not support `vsdot.s8 q10,q11,d6[0]' in ARM mode
/tmp/cceW7UTH.s:80: Error: selected processor does not support `vsdot.s8 q13,q12,d6[0]' in ARM mode
/tmp/cceW7UTH.s:84: Error: selected processor does not support `vsdot.s8 q9,q11,d7[0]' in ARM mode
/tmp/cceW7UTH.s:85: Error: selected processor does not support `vsdot.s8 q8,q12,d7[0]' in ARM mode
/tmp/cceW7UTH.s:87: Error: selected processor does not support `vsdot.s8 q13,q11,d6[1]' in ARM mode
/tmp/cceW7UTH.s:89: Error: selected processor does not support `vsdot.s8 q8,q11,d7[1]' in ARM mode
/tmp/cceW7UTH.s:92: Error: selected processor does not support `vsdot.s8 q10,q11,d6[1]' in ARM mode
/tmp/cceW7UTH.s:93: Error: selected processor does not support `vsdot.s8 q9,q11,d7[1]' in ARM mode
/tmp/cceW7UTH.s:168: Error: selected processor does not support `vsdot.s8 q13,q12,d6[0]' in ARM mode
/tmp/cceW7UTH.s:169: Error: selected processor does not support `vsdot.s8 q8,q12,d7[0]' in ARM mode
/tmp/cceW7UTH.s:171: Error: selected processor does not support `vsdot.s8 q10,q11,d6[0]' in ARM mode
/tmp/cceW7UTH.s:172: Error: selected processor does not support `vsdot.s8 q9,q11,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:50087: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o] Error 1
make[2]: Leaving directory '/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk'
make[1]: *** [CMakeFiles/Makefile2:6654: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/all] Error 2
make[1]: Leaving directory '/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk'
make: *** [Makefile:139: all] Error 2
guanghui.wan@rd01:/mnt/fileroot/guanghui.wan/tensorflow/tensorflow$
```


### Relevant log output

```shell
[ 79%] Building C object _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o
cd /mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/_deps/xnnpack-build && /mnt/fileroot/guanghui.wan/nanoQ/Algorithm/tools/gcc-arm-10.3-2021.07-x86_64-arm-none-linux-gnueabihf/bin/arm-none-linux-gnueabihf-gcc -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ARM_BF16=0 -DXNN_ENABLE_ARM_DOTPROD=1 -DXNN_ENABLE_ARM_FP16_SCALAR=1 -DXNN_ENABLE_ARM_FP16_VECTOR=1 -DXNN_ENABLE_ARM_I8MM=0 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_CPUINFO=1 -DXNN_ENABLE_DWCONV_MULTIPASS=0 -DXNN_ENABLE_GEMM_M_SPECIALIZATION=1 -DXNN_ENABLE_JIT=0 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_RISCV_VECTOR=1 -DXNN_ENABLE_SPARSE=1 -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/third_party/xla/third_party/tsl -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/xnnpack/src -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/pthreadpool-source/include -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/FXdiv-source/include -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/FP16-source/include -march=armv8-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -mfp16-format=ieee -O3 -DNDEBUG -std=c99 -fPIC -Wno-psabi -O2 -pthread  -fno-math-errno  -marm  -march=armv8.2-a+dotprod -mfpu=neon-fp-armv8  -MD -MT _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o -MF CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o.d -o CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o -c /mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/xnnpack/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c
/tmp/ccrZ5z20.s: Assembler messages:
/tmp/ccrZ5z20.s:75: Error: selected processor does not support `vsdot.s8 q14,q12,d7[0]' in ARM mode
/tmp/ccrZ5z20.s:76: Error: selected processor does not support `vsdot.s8 q10,q9,d7[0]' in ARM mode
/tmp/ccrZ5z20.s:77: Error: selected processor does not support `vsdot.s8 q14,q11,d7[1]' in ARM mode
/tmp/ccrZ5z20.s:78: Error: selected processor does not support `vsdot.s8 q10,q8,d7[1]' in ARM mode
/tmp/ccrZ5z20.s:134: Error: selected processor does not support `vsdot.s8 q14,q9,d7[0]' in ARM mode
/tmp/ccrZ5z20.s:135: Error: selected processor does not support `vsdot.s8 q10,q8,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49877: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-1x8c4-minmax-neondot.c.o] Error 1
```
"
62147,TensorFlow Lite - CMake build installable package fail,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14

### Custom code

No

### OS platform and distribution

Mac Ventura

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When following https://www.tensorflow.org/lite/guide/build_cmake#build_installable_package to build TensorFlow Lite with CMake, using the flag `-DTFLITE_ENABLE_INSTALL=ON`, a CMake error happens, saying that `the Target ml_dtypes INTERFACE_INCLUDE_DIRECTORIES property contains path: which is prefixed in the build directory.`



### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
mkdir tflite_build
cd tflite_build
cmake ../tensorflow_src/tensorflow/lite -DTFLITE_ENABLE_INSTALL=ON
```


### Relevant log output

```shell
CMake Error in tools/cmake/modules/ml_dtypes/CMakeLists.txt:
  Target ""ml_dtypes"" INTERFACE_INCLUDE_DIRECTORIES property contains path:

    ""/Users/me/tf_build/ml_dtypes""

  which is prefixed in the build directory.


CMake Error in tools/cmake/modules/ml_dtypes/CMakeLists.txt:
  Target ""ml_dtypes"" INTERFACE_INCLUDE_DIRECTORIES property contains path:

    ""/Users/me/tf_build/ml_dtypes/ml_dtypes""

  which is prefixed in the build directory.
```
"
62146,core dumped with tf.raw_ops.BiasAdd,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v1.12.1-100714-gd8e55c05473 2.15.0-dev20231005

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current behavior?

core dumped when running tf.raw_ops.BiasAdd with below code.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf 
args = {'value':  tf.random.uniform([8, 2], 0, 256, dtype=tf.int32), 'bias':  tf.random.uniform([5],0, 256, dtype=tf.int32), 'data_format': 'NCHW'}
res = tf.raw_ops.BiasAdd(**args)
print(res)
```


### Relevant log output

```shell
2023-10-18 13:29:08.644285: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (2 vs. 2)
Aborted (core dumped)
```
"
62145,Trying to process RBG images and use RandomContrast: LookupError: gradient registry has no entry for: AdjustContrastv2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

google collaboration

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

T4

### Current behavior?

I see an error when I add RandomContrast layer into the model. Otherwise, it works fine.


### Standalone code to reproduce the issue

```shell
# Clean session
clear_session()

model_6 = Sequential()

model_6.add(Conv2D(filters = 16, kernel_size = (3, 3)
                   , padding = ""same"", input_shape = (img_height, img_wdith, 3)
                   , activation='relu'))


model_6.add(RandomRotation(factor=0.3))
model_6.add(layers.RandomFlip(""horizontal_and_vertical""))
model_6.add(MaxPooling2D(pool_size = (2, 2)))
model_6.add(Conv2D(filters = 16, kernel_size = (3, 3) , padding = ""same"" , activation='relu'))
model_6.add(MaxPooling2D(pool_size = (2, 2)))

# Add batch normalization
batch_normalization = BatchNormalization()
batch_normalization.trainable = True
model_6.add(batch_normalization)
model_6.add(layers.RandomContrast(factor=0.2, seed=11))

model_6.add(Flatten())

# Adding a fully connected dense layer with 256 neurons
model_6.add(Dense(256))

model_6.add(LeakyReLU(0.1))

model_6.add(Dense(num_classes, activation = 'softmax'))

# Printing the model summary
model_6.summary()
```


### Relevant log output

```shell
Epoch 1/20
---------------------------------------------------------------------------
StagingError                              Traceback (most recent call last)
[<ipython-input-72-d1c53732b3af>](https://39j3jgoo0eu-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20231016-060118_RC00_573771261#) in <cell line: 7>()
      5 
      6 # Fit training dataset to model
----> 7 history_6 = model_6.fit(n_train_ds, validation_data=n_val_ds, epochs=num_epochs, verbose = 1 )

1 frames
[/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py](https://39j3jgoo0eu-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20231016-060118_RC00_573771261#) in autograph_handler(*args, **kwargs)
     50     except Exception as e:  # pylint:disable=broad-except
     51       if hasattr(e, ""ag_error_metadata""):
---> 52         raise e.ag_error_metadata.to_exception(e)
     53       else:
     54         raise

StagingError: in user code:

    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1338, in train_function  *
        return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1322, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1303, in run_step  **
        outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1084, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 543, in minimize
        grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 276, in compute_gradients
        grads = tape.gradient(loss, var_list)

    LookupError: gradient registry has no entry for: AdjustContrastv2
```
"
62142,core dumped with Conv2DBackpropFilter,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v1.12.1-100714-gd8e55c05473 2.15.0-dev20231005

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current behavior?

core dumped error with specific input parameters.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf 
args = {'data_format': 'NHWC', 
 'dilations': [1], 
 'explicit_paddings': [2], 
 'filter_sizes': [3], 
 'input': tf.random.normal([2, 7]), 
 'out_backprop': tf.random.normal([10]), 
 'padding': 'VALID', 'strides': [3], 
 'use_cudnn_on_gpu': True}
res = tf.raw_ops.Conv2DBackpropFilter(**args)
print(res)
```


### Relevant log output

```shell
2023-10-18 01:54:46.384973: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C
Aborted (core dumped)
```
"
62140,core dumped Error with tf.raw_ops.BiasAddGrad,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v1.12.1-100714-gd8e55c05473 2.15.0-dev20231005

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current behavior?

Description:

While using the tf.raw_ops.BiasAddGrad operation, I encountered a core dumped error with specific input parameters.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf 
args = {'out_backprop': tf.random.normal([2, 7]), 'data_format': 'NCHW'}
res = tf.raw_ops.BiasAddGrad(**args)
print(res)
```


### Relevant log output

```shell
2023-10-17 22:20:54.530750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-17 22:20:56.422509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1924] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 733 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:02:00.0, compute capability: 7.5
2023-10-17 22:20:56.423111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1924] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6732 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:04:00.0, compute capability: 7.5
2023-10-17 22:20:56.423635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1924] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6732 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:83:00.0, compute capability: 7.5
2023-10-17 22:20:56.424135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1924] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 6732 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:84:00.0, compute capability: 7.5
2023-10-17 22:20:56.471424: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (2 vs. 2)
Aborted (core dumped)
```
"
62138,Incorrect calculation of cropping indices,"https://github.com/tensorflow/tensorflow/blob/b2a7a25d102fec8fc7e3690218b627738d8a6fc2/tensorflow/compiler/mlir/tosa/transforms/legalize_common.cc#L1234

Should be:

```
int crops_lo = crops_const[i * 2 + 0];
int crops_hi = crops_const[i * 2 + 1];
```

The tests happen to cover only the case in which `crops_dims` is 2, so the bug is hidden."
62137,relu provide wrong / inconsistent result for tensorflow-macos   2.14.0.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.14

### Custom code

No

### OS platform and distribution

macOS 14.0 

### Mobile device

_No response_

### Python version

3.11 / 3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple m1 16GB

### Current behavior?

I was playing with https://www.tensorflow.org/tutorials/generative/autoencoder and I found the results of autoencoder were completely inconsistent between when I run this on **google colab** and my own **m1 macbook**.

Concrete problems are 

1. Result image of first AE ( basic autoencoder) doesn't make sense at all. 
<img width=""892"" alt=""Screenshot 2023-10-17 at 14 37 31"" src=""https://github.com/tensorflow/tensorflow/assets/57752203/49a56138-faf0-4f31-813d-53b97a3fa99f"">


2. Displayed loss value is quite different from when I test after training. (See below sc)

local 
<img width=""1034"" alt=""Screenshot 2023-10-17 at 14 40 41"" src=""https://github.com/tensorflow/tensorflow/assets/57752203/e5c15d6c-83c1-46db-9748-0a1240a550c0"">

colab
<img width=""1398"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/57752203/03cdb403-6b72-4c70-b6ae-280a6f9aaf59"">


3. When I use `elu` instead of `relu`, problems were solved. 
<img width=""1022"" alt=""Screenshot 2023-10-17 at 14 47 36"" src=""https://github.com/tensorflow/tensorflow/assets/57752203/1ae8274d-e302-4488-8a07-c2312df8426d"">
<img width=""971"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/57752203/f828d8a1-706a-4bef-90c3-8dec9dee0194"">

Is `relu` not recommended for tensorflor-macos right now?



### Standalone code to reproduce the issue

```shell
You can directy use notebook from google colab.
```


### Relevant log output

_No response_"
62136,Build broken for AARCH64 by XLA PJRT,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

17.0.0

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

Build fails on AARCH64 since https://github.com/tensorflow/tensorflow/commit/7b81b8c94a3fcf067ce50c31ce031a9ea449a59b

### Standalone code to reproduce the issue

```shell
bazel build --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --define=tf_api_version=2 --verbose_failures --jobs=75 -- //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/external/local_xla/xla/pjrt/BUILD:754:11: Compiling xla/pjrt/transpose.cc failed: (Exit 1): clang failed: error executing command (from target @local_xla//xla/pjrt:transpose) 
  (cd /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \
  exec env - \
    CACHEBUSTER=20220325 \
    CLANG_COMPILER_PATH=/usr/lib/llvm-17/bin/clang \
    PATH=/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-arm64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3.10 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
  /usr/lib/llvm-17/bin/clang -MD -MF bazel-out/aarch64-opt/bin/external/local_xla/xla/pjrt/_objs/transpose/transpose.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/local_xla/xla/pjrt/_objs/transpose/transpose.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY '-DBAZEL_CURRENT_REPOSITORY=""local_xla""' -iquote external/local_xla -iquote bazel-out/aarch64-opt/bin/external/local_xla -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -iquote external/local_tsl -iquote bazel-out/aarch64-opt/bin/external/local_tsl -iquote . -iquote bazel-out/aarch64-opt/bin -iquote external/eigen_archive -iquote bazel-out/aarch64-opt/bin/external/eigen_archive -iquote external/ml_dtypes -iquote bazel-out/aarch64-opt/bin/external/ml_dtypes -iquote external/nsync -iquote bazel-out/aarch64-opt/bin/external/nsync -iquote external/double_conversion -iquote bazel-out/aarch64-opt/bin/external/double_conversion -iquote external/com_google_protobuf -iquote bazel-out/aarch64-opt/bin/external/com_google_protobuf -iquote external/snappy -iquote bazel-out/aarch64-opt/bin/external/snappy -iquote external/com_googlesource_code_re2 -iquote bazel-out/aarch64-opt/bin/external/com_googlesource_code_re2 -Ibazel-out/aarch64-opt/bin/external/ml_dtypes/_virtual_includes/float8 -Ibazel-out/aarch64-opt/bin/external/ml_dtypes/_virtual_includes/int4 -isystem third_party/eigen3/mkl_include -isystem bazel-out/aarch64-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/aarch64-opt/bin/external/eigen_archive -isystem external/ml_dtypes -isystem bazel-out/aarch64-opt/bin/external/ml_dtypes -isystem external/ml_dtypes/ml_dtypes -isystem bazel-out/aarch64-opt/bin/external/ml_dtypes/ml_dtypes -isystem external/nsync/public -isystem bazel-out/aarch64-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/aarch64-opt/bin/external/com_google_protobuf/src -fmerge-all-constants -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-gnu-offsetof-extensions '-mtune=generic' '-march=armv8-a' -O3 -flax-vector-conversions '-std=c++17' '--sysroot=/dt10' -c external/local_xla/xla/pjrt/transpose.cc -o bazel-out/aarch64-opt/bin/external/local_xla/xla/pjrt/_objs/transpose/transpose.pic.o)
# Configuration: 05cc03ffdfad7b565350ad2b01d7497c783d20d25ca4abeda724de457d2afb5e
# Execution platform: @local_execution_config_platform//:platform
In file included from external/local_xla/xla/pjrt/transpose.cc:94:
external/local_xla/xla/pjrt/transpose_kernels.h:710:46: error: use of undeclared identifier '__m128i'
  710 |       if constexpr (sizeof(T) * bs <= sizeof(__m128i)) {
      |                                              ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned short, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned short, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned short, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:459:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned short, xla::TransposePlan::Transformation::kNone>' requested here
  459 |         ExecuteTyped<uint16_t, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '512 <= 256'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:471:17: error: static assertion failed due to requirement '32UL * (0 + 1) <= sizeof(__attribute__((neon_vector_type(2))) unsigned long)'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:551:4: note: in instantiation of function template specialization 'xla::StoreElementFromVec128<32UL, 0>' requested here
  551 |   (StoreElementFromVec128</*bytes=*/bytes, lane>(b + ldb * (i + lane), x), ...);
      |    ^
external/local_xla/xla/pjrt/transpose_kernels.h:594:7: note: in instantiation of function template specialization 'xla::StoreElementsFromVec128<32UL, 0UL>' requested here
  594 |       StoreElementsFromVec128<element_size * bs>(
      |       ^
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned short, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned short, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned short, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:459:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned short, xla::TransposePlan::Transformation::kNone>' requested here
  459 |         ExecuteTyped<uint16_t, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:471:36: note: expression evaluates to '32 <= 16'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:498:21: error: static assertion failed due to requirement '32UL == 0'
  498 |       static_assert(bytes == 0);
      |                     ^~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned int, 8>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned int, 8>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned int, 8, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:463:11: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned int, xla::TransposePlan::Transformation::kNone>' requested here
  463 |           ExecuteTyped<uint32_t, Transformation::kNone>(ac, bc, nodes);
      |           ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '256 <= 128'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned int, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned int, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned int, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:463:11: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned int, xla::TransposePlan::Transformation::kNone>' requested here
  463 |           ExecuteTyped<uint32_t, Transformation::kNone>(ac, bc, nodes);
      |           ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '1024 <= 256'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:471:17: error: static assertion failed due to requirement '64UL * (0 + 1) <= sizeof(__attribute__((neon_vector_type(2))) unsigned long)'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:551:4: note: in instantiation of function template specialization 'xla::StoreElementFromVec128<64UL, 0>' requested here
  551 |   (StoreElementFromVec128</*bytes=*/bytes, lane>(b + ldb * (i + lane), x), ...);
      |    ^
external/local_xla/xla/pjrt/transpose_kernels.h:594:7: note: in instantiation of function template specialization 'xla::StoreElementsFromVec128<64UL, 0UL>' requested here
  594 |       StoreElementsFromVec128<element_size * bs>(
      |       ^
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned int, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned int, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned int, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:463:11: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned int, xla::TransposePlan::Transformation::kNone>' requested here
  463 |           ExecuteTyped<uint32_t, Transformation::kNone>(ac, bc, nodes);
      |           ^
external/local_xla/xla/pjrt/transpose_kernels.h:471:36: note: expression evaluates to '64 <= 16'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:498:21: error: static assertion failed due to requirement '64UL == 0'
  498 |       static_assert(bytes == 0);
      |                     ^~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned long, 4>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned long, 4>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned long, 4, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:470:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned long, xla::TransposePlan::Transformation::kNone>' requested here
  470 |         ExecuteTyped<uint64_t, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '128 <= 64'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned long, 8>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned long, 8>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned long, 8, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:470:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned long, xla::TransposePlan::Transformation::kNone>' requested here
  470 |         ExecuteTyped<uint64_t, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '512 <= 128'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned long, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned long, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned long, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:470:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned long, xla::TransposePlan::Transformation::kNone>' requested here
  470 |         ExecuteTyped<uint64_t, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '2048 <= 256'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:471:17: error: static assertion failed due to requirement '128UL * (0 + 1) <= sizeof(__attribute__((neon_vector_type(2))) unsigned long)'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:551:4: note: in instantiation of function template specialization 'xla::StoreElementFromVec128<128UL, 0>' requested here
  551 |   (StoreElementFromVec128</*bytes=*/bytes, lane>(b + ldb * (i + lane), x), ...);
      |    ^
external/local_xla/xla/pjrt/transpose_kernels.h:594:7: note: in instantiation of function template specialization 'xla::StoreElementsFromVec128<128UL, 0UL>' requested here
  594 |       StoreElementsFromVec128<element_size * bs>(
      |       ^
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned long, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned long, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned long, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:470:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned long, xla::TransposePlan::Transformation::kNone>' requested here
  470 |         ExecuteTyped<uint64_t, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:471:36: note: expression evaluates to '128 <= 16'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:498:21: error: static assertion failed due to requirement '128UL == 0'
  498 |       static_assert(bytes == 0);
      |                     ^~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<xla::uint128, 2>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<xla::uint128, 2>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<xla::uint128, 2, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:473:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<xla::uint128, xla::TransposePlan::Transformation::kNone>' requested here
  473 |         ExecuteTyped<uint128, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '64 <= 32'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<xla::uint128, 4>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<xla::uint128, 4>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<xla::uint128, 4, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:473:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<xla::uint128, xla::TransposePlan::Transformation::kNone>' requested here
  473 |         ExecuteTyped<uint128, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '256 <= 64'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<xla::uint128, 8>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<xla::uint128, 8>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<xla::uint128, 8, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:473:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<xla::uint128, xla::TransposePlan::Transformation::kNone>' requested here
  473 |         ExecuteTyped<uint128, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '1024 <= 128'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<xla::uint128, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<xla::uint128, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<xla::uint128, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:473:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<xla::uint128, xla::TransposePlan::Transformation::kNone>' requested here
  473 |         ExecuteTyped<uint128, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '4096 <= 256'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:471:17: error: static assertion failed due to requirement '256UL * (0 + 1) <= sizeof(__attribute__((neon_vector_type(2))) unsigned long)'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:551:4: note: in instantiation of function template specialization 'xla::StoreElementFromVec128<256UL, 0>' requested here
  551 |   (StoreElementFromVec128</*bytes=*/bytes, lane>(b + ldb * (i + lane), x), ...);
      |    ^
external/local_xla/xla/pjrt/transpose_kernels.h:594:7: note: in instantiation of function template specialization 'xla::StoreElementsFromVec128<256UL, 0UL>' requested here
  594 |       StoreElementsFromVec128<element_size * bs>(
      |       ^
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<xla::uint128, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<xla::uint128, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<xla::uint128, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:473:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<xla::uint128, xla::TransposePlan::Transformation::kNone>' requested here
  473 |         ExecuteTyped<uint128, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:471:36: note: expression evaluates to '256 <= 16'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:498:21: error: static assertion failed due to requirement '256UL == 0'
  498 |       static_assert(bytes == 0);
      |                     ^~~~~~~~~~
19 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
"
62135,Not able to get TFlite Model Maker Object Detector to produce expected output,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WSL Linux Ubuntu 20.04 (although I've also tried on Linux Ubuntu 22.04)
- TensorFlow installation (pip package or built from source): Pip package (t.
- TensorFlow library (version, if pip package or github SHA, if built from source):
- I built it with Conda and Python 3.9. Basically I install the environment in a Jupyter kernel and then use it to install the packages and eventually run the script.

```
# Install Miniconda
!apt install wget
!wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.9.2-Linux-x86_64.sh
!chmod +x Miniconda3-py37_4.9.2-Linux-x86_64.sh
!bash ./Miniconda3-py37_4.9.2-Linux-x86_64.sh -b -f -p /usr/local

# Update Conda
!conda update -n base -c defaults conda -y

# #Create a Python 3.9 environment
!conda create --name py39_environment python=3.9 -y

# # Initialize shell for Conda
!conda init bash

# # Activate the environment and check Python version
!source activate py39_environment && python --version
!source /usr/local/etc/profile.d/conda.sh && conda activate py39_environment && pip install tflite_model_maker
!source /usr/local/etc/profile.d/conda.sh && conda activate py39_environment && pip install -q pycocotools
!source /usr/local/etc/profile.d/conda.sh && conda activate py39_environment && pip install opencv-python-headless
!source /usr/local/etc/profile.d/conda.sh && conda activate py39_environment && pip uninstall -y tensorflow && pip install -q tensorflow==2.8.0
!source /usr/local/etc/profile.d/conda.sh && conda activate py39_environment && pip install ipykernel
!source /usr/local/etc/profile.d/conda.sh && conda activate py39_environment && pip install --upgrade numba llvmlite
```

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

train.py:

```
import numpy as np
import os

np.object = object
np.bool = bool
np.complex = complex

from tflite_model_maker.config import ExportFormat
from tflite_model_maker import model_spec
from tflite_model_maker import object_detector

import tensorflow as tf
assert tf.__version__.startswith('2')

tf.get_logger().setLevel('ERROR')
from absl import logging
logging.set_verbosity(logging.ERROR)

label_map = {1: 'Plastic', 2: 'Trash'} 

train_images_dir = '/trashnet-training/pascal_voc/train/images'
train_annotations_dir = '/trashnet-training/pascal_voc/train/Annotations'
val_images_dir = '/trashnet-training/pascal_voc/valid/images/'
val_annotations_dir = '/trashnet-training/pascal_voc/valid/Annotations'
# test_images_dir =  '/trashnet-training/pascal_voc/train/Annotations'
# test_annotations_dir = '/trashnet-training/pascal_voc/train/Annotations'

train_data = object_detector.DataLoader.from_pascal_voc(
    train_images_dir, train_annotations_dir, label_map=label_map)

validation_data = object_detector.DataLoader.from_pascal_voc(
    val_images_dir, val_annotations_dir, label_map=label_map)

# test_data = object_detector.DataLoader.from_pascal_voc(
#     test_images_dir, test_annotations_dir, label_map=label_map)

spec = model_spec.get('efficientdet_lite0')

model = object_detector.create(train_data=train_data, 
                               model_spec=spec, 
                               validation_data=validation_data, 
                               epochs=50, 
                               batch_size=10, 
                               train_whole_model=True)

model.evaluate(validation_data)

TFLITE_FILENAME = 'trained_model.tflite'
LABELS_FILENAME = 'model-labels.txt'

model.export(export_dir='.', tflite_filename=TFLITE_FILENAME, label_filename=LABELS_FILENAME,
             export_format=[ExportFormat.TFLITE, ExportFormat.LABEL])
```

test_model.py:

```
import tensorflow as tf
import numpy as np


interpreter = tf.lite.Interpreter(model_path=""trained_model.tflite"")
interpreter.allocate_tensors()


input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
input_data = interpreter.get_tensor(input_details[0]['index'])
print(f""Input tensor: {input_data}"")
print(f""Output tensor: {output_data}"")


tf.lite.experimental.Analyzer.analyze(model_path=""trained_model.tflite"")

```

My dataset is in the Pascal VOC format with train and valid subfolders, each with their own images/ and Annotations/ folders. I've been able to train the model several times but whenever I put the resulting .tflite into the testing script, it doesn't product the shape it's supposed to. According to the TF documentation for the TFLite Object Detector export_tflite function [here](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py#L348-L401) the output shape should be:

    Four Outputs:
      detection_boxes: a float32 tensor of shape [1, num_boxes, 4] with box
        locations.
      detection_classes: a float32 tensor of shape [1, num_boxes] with class
        indices.
      detection_scores: a float32 tensor of shape [1, num_boxes] with class
        scores.
      num_boxes: a float32 tensor of size 1 containing the number of detected
        boxes.

### 3. Failure after conversion
 The training seems to go fine, although somewhere along the way it warns me that:
`2023-10-17 04:41:28.736735: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'resample_p7/PartitionedCall' has 1 outputs but the _output_shapes attribute specifies shapes for 3 outputs. Output shapes may be inaccurate.` 
Which is concerning. And I think part of the problem because when I run my testing script on the model the output tensors are all blank:

```
...
d_4;class_net/class-predict/bias11) shape:[1, 3, 3, 18], type:INT8
  T#583(Reshape) shape:[1, 14400, 2], type:INT8
  T#584(Reshape_1) shape:[1, 14400, 4], type:INT8
  T#585(Reshape_2) shape:[1, 3600, 2], type:INT8
  T#586(Reshape_3) shape:[1, 3600, 4], type:INT8
  T#587(Reshape_4) shape:[1, 900, 2], type:INT8
  T#588(Reshape_5) shape:[1, 900, 4], type:INT8
  T#589(Reshape_6) shape:[1, 225, 2], type:INT8
  T#590(Reshape_7) shape:[1, 225, 4], type:INT8
  T#591(Reshape_8) shape:[1, 81, 2], type:INT8
  T#592(concat) shape:[1, 19206, 2], type:INT8
  T#593(Sigmoid) shape:[1, 19206, 2], type:INT8
  T#594(Sigmoid1) shape:[1, 19206, 2], type:FLOAT32
  T#595(Reshape_9) shape:[1, 81, 4], type:INT8
  T#596(concat_1) shape:[1, 19206, 4], type:INT8
  T#597(tfl.dequantize) shape:[1, 19206, 4], type:FLOAT32
  T#598(StatefulPartitionedCall:3) shape:[], type:FLOAT32
  T#599(StatefulPartitionedCall:2) shape:[], type:FLOAT32
  T#600(StatefulPartitionedCall:1) shape:[], type:FLOAT32
  T#601(StatefulPartitionedCall:0) shape:[], type:FLOAT32

---------------------------------------------------------------
Your TFLite model has â€˜1â€™ signature_def(s).

Signature#0 key: 'serving_default'
- Subgraph: Subgraph#0
- Inputs: 
    'images' : T#0
- Outputs: 
    'output_0' : T#601
    'output_1' : T#600
    'output_2' : T#599
    'output_3' : T#598

---------------------------------------------------------------
              Model size:    4444720 bytes
    Non-data buffer size:     799672 bytes (17.99 %)
  Total data buffer size:    3645048 bytes (82.01 %)
    (Zero value buffers):        256 bytes (00.01 %)
```

I've trained it several times with different formats but I just can't get the right output shapes, every time they're blank Could someone help me to understand why this is not working? Any guidance or insight would be greatly appreciated.
"
62130,Make configure.py non-interactive,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

r2.6
### Custom code

Yes

### OS platform and distribution

centos / redhat 7.x

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Right now you have to manually select all of your options for building tensorflow from source.  When you are wanting to optimize across various architectures this becomes manually intensive.  It would be much better if you could pass the various arguments via the command line to setup your environment, instead of being forced to accept or override the default arguments.  


### Standalone code to reproduce the issue

```shell
run `./configure`
```


### Relevant log output

_No response_"
62127,Invalid classifier used for CUDA 12.2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

16.0.6

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

Uploads to PyPi fail with invalid classifier.

Introduced by https://github.com/tensorflow/tensorflow/commit/c3b98ea5a8387eec21e808caa6e999417494e09a

### Standalone code to reproduce the issue

```shell
python3 -m twine upload --verbose /home/ubuntu/actions-runner/_work/tensorflow/tensorflow/whl/* -u ""__token__"" -p ***
```


### Relevant log output

```shell
INFO     Response from https://upload.pypi.org/legacy/:                         
         400 Invalid value for classifiers. Error: Classifier 'Environment ::   
         GPU :: NVIDIA CUDA :: 12.2' is not a valid classifier.                 
INFO     <html>                                                                 
          <head>                                                                
           <title>400 Invalid value for classifiers. Error: Classifier          
         'Environment :: GPU :: NVIDIA CUDA :: 12.2' is not a valid             
         classifier.</title>                                                    
          </head>                                                               
          <body>                                                                
           <h1>400 Invalid value for classifiers. Error: Classifier 'Environment
         :: GPU :: NVIDIA CUDA :: 12.2' is not a valid classifier.</h1>         
           The server could not comply with the request since it is either      
         malformed or otherwise incorrect.<br/><br/>                            
         Invalid value for classifiers. Error: Classifier &#x27;Environment ::  
         GPU :: NVIDIA CUDA :: 12.2&#x27; is not a valid classifier.            
                                                                                
                                                                                
          </body>                                                               
         </html>                                                                
ERROR    HTTPError: 400 Bad Request from https://upload.pypi.org/legacy/        
         Invalid value for classifiers. Error: Classifier 'Environment :: GPU ::
         NVIDIA CUDA :: 12.2' is not a valid classifier.
```
"
62125,static_assert failure with gcc,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

Build failure

### Standalone code to reproduce the issue

```shell
bazel build --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --define=tf_api_version=2 --verbose_failures --jobs=75 -- //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo.cc: In member function 'mlir::LogicalResult mlir::odml::{anonymous}::ConvertReduceOpToTfArgMinMax<TfReduce, TfArgReduce>::matchAndRewrite(mlir::mhlo::ReduceOp, mlir::OpConversionPattern<mlir::mhlo::ReduceOp>::OpAdaptor, mlir::ConversionPatternRewriter&) const':
tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo.cc:2186: error: static assertion failed: Only TF::MaxOp and TF::MinOp are supported.
 2186 |         static_assert(false, ""Only TF::MaxOp and TF::MinOp are supported."");
      | 
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 502.712s, Critical Path: 197.25s
INFO: 1603 processes: 79 internal, 1524 local.
FAILED: Build did NOT complete successfully
```
"
62123,Different results interpreter python and Android,"- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13


I get different results when running my model with the TF Lite interpreter in Python and on Android. I do the same normalization in both of those.


The Python code:

```
def read_image(file_path):
    mean = 255*np.array([0.485, 0.456, 0.406])
    std = 255*np.array([0.229, 0.224, 0.225])
    img = Image.open(file_path).convert('RGB') 
    img = img.resize((224,224), resample=PIL.Image.BILINEAR) 
    img = np.array(img)
    img = (img - mean[None, None, :]) / std[None, None, :]
    img = np.float32(img)
    return img

img = read_image_normal(path)

interpreter = tf.lite.Interpreter(model_path=""./model.tflite"") 
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

interpreter.set_tensor(input_details[0]['index'], processed_image)
interpreter.invoke()
predictions = interpreter.get_tensor(output_details[0]['index'])
```

Snippets of the relevant Android code:
For preprocessing:

```
  private fun preprocessImage(image1: Bitmap) = with(TensorImage(DataType.FLOAT32)) {
      load(image1)
      val imageProcessor: ImageProcessor = ImageProcessor.Builder()
              .add(
              ResizeOp(
                  224,
                  224,
                  ResizeOp.ResizeMethod.BILINEAR
              )
          )
          .add(
              NormalizeOp(
                  floatArrayOf(
                          0.485f*255f,
                          0.456f*255f,
                          0.406f*255f,
                  ),
                  floatArrayOf(
                          0.229f*255f,
                          0.224f*255f,
                          0.225f*255f,
                  ),
              )
          )

          .build()
      imageProcessor.process(this)
  }
```

For inference:

  ```
        val options = Interpreter.Options()
        options.setNumThreads(1)
        interpreter = Interpreter(model, options)
        val timeStart = System.currentTimeMillis()
        val image = preprocessImage(bitmap)
        val probabilityBuffer = TensorBuffer.createFixedSize(intArrayOf(1, 1), DataType.FLOAT32)
        interpreter.run(image.buffer, probabilityBuffer.buffer)
        val score = probabilityBuffer.floatArray[0]
```


Both results are different by quite a lot - around 0.1 score. The images that enter are exactly the same.

Model: https://drive.google.com/file/d/1Pr3mCZ7kocEPw_tAQuokrZBpqqXC0gek/view?usp=sharing
"
62122,Tensorflow Importation Error,"Good day! 
I have a project and i need TensorFlow module (this is the first time using the library)
I have installed the library and try to force-download different lower versions and they got installed successfully but to import now is the issue
Below is the error message it keep bringing up:

ImportError                               Traceback (most recent call last)
~\anaconda3\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     61   try:
---> 62     from tensorflow.python._pywrap_tensorflow_internal import *
     63   # This try catch logic is because there is no bazel equivalent for py_extension.

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>
----> 1 import tensorflow as tf

~\anaconda3\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>
     35 import typing as _typing
     36 
---> 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     39 

~\anaconda3\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>
     34 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
     35 
---> 36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
     37 from tensorflow.python.eager import context
     38 

~\anaconda3\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     76 except ImportError:
     77   raise ImportError(
---> 78       f'{traceback.format_exc()}'
     79       f'\n\nFailed to load the native TensorFlow runtime.\n'
     80       f'See https://www.tensorflow.org/install/errors '

ImportError: Traceback (most recent call last):
  File ""C:\Users\DAMMYICAN\anaconda3\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.

I would be glad if you could be of help
Thanks in advance.
"
62121,Tensorflow Im,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
62120,Error when loading TFLite model on Android with GPU Delegate,"### 1. System information

- OS Platform and Distribution: TFLite conversion on Windows 10 and run models on Android 13
- TensorFlow installation: pip package
- TensorFlow library: 2.14.0

### 2. Code

I understand the importance of providing a reproducible code for better troubleshooting. Given the size and complexity of the model, I'm unable to provide a simplified version immediately. However, I am actively working on preparing one to help diagnose the issue more effectively.

In the meantime, if there are any insights, workarounds, how to debug, or known issues that you can infer from the error message I've shared below, it would be immensely helpful.

### 3. Failure after conversion
Model produces correct results on my PC, but an error occurs when tyring to load it on an Android device.
Any insights or solutions would be greatly appreciated. I'd like to know if there's something I'm missing or if this is a known issue.

### 5.  Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I'm currently trying to run a TFLite model on an Android mobile device using the GPU Delegate. While the converted TFLite model works perfectly on my PC, I encounter an error when trying to load it on the mobile device. Here's the error message I received:
```
Internal error: Failed to apply delegate: Failed to build program executable - Build program failure<source>:69:103: error: expected expression
{half4 second_value = read_imageh(src_tensor_1_image2d, smp_zero, (int2)(((0) * shared_int4_0.w + (())), ((0) * shared_int4_1.x + ((Z)))));
                                                                                                 ^
error: Compiler frontend failed (error code 63)
Falling back to OpenGL

TfLiteGpuDelegate Init: Batch size mismatch, exp
        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:110)
  	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:73)
  	at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:36)
  	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:232)
  	at com.XXX.ModelHelper.loadModelFromStorage(ModelHelper.kt:171)
```

While there's no issue loading with TFLite CPU, I encounter the following error during execution. What's weird is that even though I'm feeding the same size of data on both PC and mobile, this error only appears on the mobile. 
```
Shutting down VM
FATAL EXCEPTION: main
Process: com.XXX, PID: 8445
java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/reshape.cc:92 num_input_elements != num_output_elements (81920 != 1310720)
Node number 995 (RESHAPE) failed to prepare.
	at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)
	at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensorsIfNeeded(NativeInterpreterWrapper.java:308)
	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:248)
	at org.tensorflow.lite.InterpreterImpl.runForMultipleInputsOutputs(InterpreterImpl.java:101)
	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:95)
	at com.XXX.ModelHelper.runModel(ModelHelper.kt:316)
```

I have confirmed that the same error occurs on at least two different Android devices (Pixel 7 Pro and Galaxy S20 Exynos).

Thank you for your understanding and assistance. I truly appreciate any guidance you can provide.
"
62118,How to use Tensorflow or TFLite for Renesas Boards. There are almost no tutorials on official tf website also.,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

Renesas Board

### Python version

3.8

### Bazel version

6.1.0

### GCC/compiler version

9

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

AXM-8-256

### Current behavior?

I want to run a resnet50 model on Renesas v4h board using tensorflow or tflite. Does tflite/tensorflow support Renesas v-series boards . If so , then do we need to cross-compile tflite for renesas board  and how to run the model on board?

### Standalone code to reproduce the issue

```shell
How to run tensorflow/tflite models on renesas v-series boards?
```


### Relevant log output

_No response_"
62117,Command `pip install tensorflow[and-cuda]` need quotation mark,"Running this `pip install tensorflow[and-cuda]` command would be wrong as failed to install, but running `pip install ""tensorflow[and-cuda]""` is okay. It just required the quotation mark. Please fix the documentation. "
62116,Questions about the interpreterâ€™s operational flow for quantized neural networks,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi. TensorFlow
This is not a post related to issues or bugs, but a post related to questions.

I am curious about how the interpreter operates when a quantized neural network is given to it.
I looked for documentation related to this, but couldn't find it.

Let me give you an example of what I am looking for in a document.
Let's say that the data type of the convolution's input activation is float32, and the data type of the weight is int8.

At this time, the interpreter
operates after dequantizing the weight of the convolution. 
OR,
operates after converting the activation of Convolution to int8.

I'm looking for an explanation like this.

Is there a link with an explanation like this?

### Standalone code to reproduce the issue

```shell
Not required.
```


### Relevant log output

```shell
Not required.
```
"
62114,Internal Error for truncatediv when denominator is an integer zero.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tf.truncatediv raises internal error when denominator is an integer zero. If the denominator is a float zero, this function works normally.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant(33, dtype=""int32"")
y = tf.constant(0, dtype=""int32"")
print(x, y)
print(tf.truncatediv(x,y))
```


### Relevant log output

_No response_"
62113,Utilize Hexagon DSP using Python on ARM Devkit,"I am trying to run Tensorflow utilizing the Hexagon DSP on an ARM Devkit running ARM Linux 

I am able to build Tensorflow for DSP. Also the necessary DSP files are available (.so files) , but I am not able to find resources for any code in Python that would be able to run on DSP "
62111,"After compilation, only 'libtensorflow_cc.so' was generated, and 'liblibtensorflow_cc.so.ifso' is missing","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.6.0

### Custom code

Yes

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.7.16

### Bazel version

3.7.2

### GCC/compiler version

2019

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

3090

### Current behavior?


""libtensorflow_cc.so""å’Œ""liblibtensorflow_cc.so.ifso"" were both generated.

### Standalone code to reproduce the issue

```shell
only 'libtensorflow_cc.so' was generated, and 'liblibtensorflow_cc.so.ifso' is missing
```


### Relevant log output

_No response_"
62109,Errors when building v2.14 from source - ubuntu 20.04 / arm64 / GPU + CUDA (capabilities 8.7) / clang 16,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.14.0

### Custom code

No

### OS platform and distribution

Ubuntu 20.04.6

### Mobile device

_No response_

### Python version

3.10.13

### Bazel version

6.1.0

### GCC/compiler version

clang 16.0.6

### CUDA/cuDNN version

11.4 / 8.6.0

### GPU model and memory

jetson orin agx  -> nvidia ampere

### Current behavior?

Attempting to build TF from source
- Jetson Orin w Ampere GPU, arm64
- TF 2.14.0
- bazel 6.1.0
- python 3.10

`./configure`
- no ROCm
- yes CUDA (capabilities = 8.7)
- no TensorRT
- using clang 16 as cuda compiler

`bazel build //tensorflow/tools/pip_package:build_pip_package`

Resultng behavior: build fails, with the following error:
```
.../tensorflow/core/kernels/BUILD:4996:18: Compiling tensorflow/core/kernels/sparse_reorder_op_gpu.cu.cc failed: (Exit 1): clang failed: error executing command (from target
 //tensorflow/core/kernels:sparse_reorder_op_gpu) /usr/lib/llvm-16/bin/clang -MD -MF bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/sparse_reorder_op_gpu/sparse_reorder_op_gpu.cu.pic.d ... (remaining 192 a
rguments skipped)
In file included from tensorflow/core/kernels/sparse_reorder_op_gpu.cu.cc:21:
In file included from ./tensorflow/core/kernels/gpu_prim.h:24:
In file included from bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include/cub/device/device_radix_sort.cuh:40:
In file included from bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include/cub/device/dispatch/dispatch_radix_sort.cuh:40:
In file included from bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include/cub/device/dispatch/../../agent/agent_radix_sort_histogram.cuh:38:
bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include/cub/device/dispatch/../../agent/../block/radix_rank_sort_operations.cuh:124:20: error: explicit qualification required to use member 'ProcessFl
oatMinusZero' from dependent base class
        return BFE(ProcessFloatMinusZero(key), bit_start, num_bits);
                   ^
 ...
 
 bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include/cub/device/device_radix_sort.cuh:168:65: note: in instantiation of member function 'cub::DispatchRadixSort<false, long, long, int>::Dispatch' r
equested here
        return DispatchRadixSort<false, KeyT, ValueT, OffsetT>::Dispatch(
                                                                ^
./tensorflow/core/kernels/gpu_prim_helpers.h:117:37: note: in instantiation of function template specialization 'cub::DeviceRadixSort::SortPairs<long, long>' requested here
    err = gpuprim::DeviceRadixSort::SortPairs(
                                    ^
./tensorflow/core/kernels/gpu_prim_helpers.h:159:18: note: in instantiation of function template specialization 'tensorflow::detail::GpuRadixSortImpl<false, long, long>' requested here
  return detail::GpuRadixSortImpl</*Descending=*/false>(
                 ^
tensorflow/core/kernels/sparse_reorder_op_gpu.cu.cc:104:12: note: in instantiation of function template specialization 'tensorflow::GpuRadixSort<long, long>' requested here
        c, GpuRadixSort(c, num_elems, /*keys_in=*/flat_indices,
           ^
bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include/cub/device/dispatch/../../agent/../block/radix_rank_sort_operations.cuh:98:52: note: member is declared here
    static __device__ __forceinline__ UnsignedBits ProcessFloatMinusZero(UnsignedBits key)
                                                   ^
2 errors generated when compiling for sm_87.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 9646.040s, Critical Path: 487.26s
INFO: 17934 processes: 6479 internal, 11455 local.
FAILED: Build did NOT complete successfully
```

### Standalone code to reproduce the issue

```shell
see below
```


### Relevant log output

```shell
<someuser>:tensorflow âžœ  git status
HEAD detached at v2.14.0
nothing to commit, working tree clean
<someuser>:tensorflow âžœ  ./configure
You have bazel 6.1.0 installed.
Please specify the location of python. [Default is /home/someuser/.pyenv/versions/3.10.13/bin/python3]:


Found possible Python library paths:
  /home/someuser/.pyenv/versions/3.10.13/lib/python3.10/site-packages
Please input the desired Python library path to use.  Default is [/home/someuser/.pyenv/versions/3.10.13/lib/python3.10/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]:
No TensorRT support will be enabled for TensorFlow.

Found CUDA 11.4 in:
    /usr/local/cuda-11.4/targets/aarch64-linux/lib
    /usr/local/cuda-11.4/targets/aarch64-linux/include
Found cuDNN 8 in:
    /usr/lib/aarch64-linux-gnu
    /usr/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.7


Do you want to use clang as CUDA compiler? [Y/n]:
Clang will be used as CUDA compiler.

Please specify clang path that to be used as host compiler. [Default is /usr/lib/llvm-16/bin/clang]:


You have Clang 16.0.6 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished
<someuser>:tensorflow âžœ  bazel build //tensorflow/tools/pip_package:build_pip_package
WARNING: while reading option defaults file '/home/someuser/projects/other/tensorflow/.bazelrc':
  invalid command name 'startup:windows'.
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=214
INFO: Reading rc options for 'build' from /home/someuser/projects/other/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/someuser/projects/other/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/someuser/projects/other/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/someuser/.pyenv/versions/3.10.13/bin/python3 --action_env PYTHON_LIB_PATH=/home/someuser/.pyenv/versions/3.10.13/lib/python3.10/site-packages --python_path=/home/someuser/.pyenv/versions/3.10.13/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.4 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.7 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-16/bin/clang --copt=-Wno-gnu-offsetof-extensions --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /home/someuser/projects/other/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/someuser/projects/other/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /home/someuser/projects/other/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /home/someuser/projects/other/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda_clang in file /home/someuser/projects/other/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /home/someuser/projects/other/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /home/someuser/projects/other/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/someuser/projects/other/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/668e33c6401abe7844691fb7d47a3cf2d2012dbc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/769f5cc9b8732933140b09e8808d13614182b496.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: while reading option defaults file '/home/someuser/projects/other/tensorflow/.bazelrc':
  invalid command name 'startup:windows'.
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Build options --copt and --host_copt have changed, discarding analysis cache.
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/9ae6c373a6e2941ff84a8831bb3724728cb2b49a.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pytorch/cpuinfo/archive/87d8234510367db49a65535021af5e1838a65ac2.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/b9d4073a6913891ce9cbd8965c8d506075d2a45a.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/triton/archive/cl546794996.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded, 47122 targets configured).
INFO: Found 1 target...
[1,765 / 6,027] 8 actions running
[1,802 / 6,027] 8 actions running
    Compiling mlir/lib/Dialect/Arith/IR/ArithOps.cpp [for tool]; 41s local
    Compiling mlir/lib/Dialect/Arith/IR/ArithDialect.cpp [for tool]; 40s local
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: build interrupted
INFO: Elapsed time: 195.807s, Critical Path: 101.10s
INFO: 301 processes: 15 internal, 286 local.
FAILED: Build did NOT complete successfully

<someuser>:tensorflow âžœ
<someuser>:tensorflow âžœ

<someuser>:tensorflow âžœ
<someuser>:tensorflow âžœ  reset

<someuser>:tensorflow âžœ  bazel clean
WARNING: while reading option defaults file '/home/someuser/projects/other/tensorflow/.bazelrc':
  invalid command name 'startup:windows'.
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=214
INFO: Reading rc options for 'clean' from /home/someuser/projects/other/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'clean' from /home/someuser/projects/other/tensorflow/.bazelrc:
  Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'clean' from /home/someuser/projects/other/tensorflow/.tf_configure.bazelrc:
  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/home/someuser/.pyenv/versions/3.10.13/bin/python3 --action_env PYTHON_LIB_PATH=/home/someuser/.pyenv/versions/3.10.13/lib/python3.10/site-packages --python_path=/home/someuser/.pyenv/versions/3.10.13/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.4 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.7 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-16/bin/clang --copt=-Wno-gnu-offsetof-extensions --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /home/someuser/projects/other/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/someuser/projects/other/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /home/someuser/projects/other/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /home/someuser/projects/other/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda_clang in file /home/someuser/projects/other/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /home/someuser/projects/other/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /home/someuser/projects/other/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/someuser/projects/other/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
<someuser>:tensorflow âžœ  reset
'





















<someuser>:tensorflow âžœ  git status
HEAD detached at v2.14.0
nothing to commit, working tree clean
<someuser>:tensorflow âžœ  bazel --version
bazel 6.1.0
<someuser>:tensorflow âžœ  clang-16 --version
Ubuntu clang version 16.0.6 (++20230710042046+7cbf1a259152-1~exp1~20230710162136.105)
Target: aarch64-unknown-linux-gnu
Thread model: posix
InstalledDir: /usr/bin
<someuser>:tensorflow âžœ  ./configure
You have bazel 6.1.0 installed.
Please specify the location of python. [Default is /home/someuser/.pyenv/versions/3.10.13/bin/python3]:


Found possible Python library paths:
  /home/someuser/.pyenv/versions/3.10.13/lib/python3.10/site-packages
Please input the desired Python library path to use.  Default is [/home/someuser/.pyenv/versions/3.10.13/lib/python3.10/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]:
No TensorRT support will be enabled for TensorFlow.

Found CUDA 11.4 in:
    /usr/local/cuda-11.4/targets/aarch64-linux/lib
    /usr/local/cuda-11.4/targets/aarch64-linux/include
Found cuDNN 8 in:
    /usr/lib/aarch64-linux-gnu
    /usr/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.7


Do you want to use clang as CUDA compiler? [Y/n]:
Clang will be used as CUDA compiler.

Please specify clang path that to be used as host compiler. [Default is /usr/lib/llvm-16/bin/clang]:


You have Clang 16.0.6 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl            # Build with MKL support.                                                                                                                                                    [12/7774]
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished
<someuser>:tensorflow âžœ  bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package



/////////////////////////
```
"
62108,[Q] ConvertPrimitiveTypeToMLIRType and mlir::IntegerType::Signless,"I have a question about the code block below https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/translate/hlo_to_mhlo/hlo_utils.cc#L286-L288
hlo_utils.cc - ConvertPrimitiveTypeToMLIRType
```
    default:
      if (primitive_util::IsIntegralType(element_type)) {
        return mlir::IntegerType::get(
            builder.getContext(),
            /*width=*/primitive_util::BitWidth(element_type),
            /*signed=*/
            primitive_util::IsUnsignedIntegralType(element_type)
                ? mlir::IntegerType::Unsigned
                : mlir::IntegerType::Signless);
      }
```
Should we return `mlir::IntegerType::Signed` instead of `mlir::IntegerType::Signless` ?
"
62107,UnicodeDecodeError: 'utf-8' codec can't decode byte 0xce in position XX: invalid continuation byte,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Windows 11 Home 22H2

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I've created a model in google colab: [here](https://colab.research.google.com/drive/1776CZxkKcetQsPkJfMACWnNel3Hj-hbA?usp=sharing). Then downloaded it and loaded few times on my computer. However after these times it stopped to load by the function `tf.keras.saving.load_model(model_name)` and started to drop this error: 

```
---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
Cell In[3], line 9
      5 import time
      6 from random import sample
----> 9 model = tf.keras.saving.load_model('Ð’ÐµÑ€Ñ… ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ')

File C:\Program Files\Python39\lib\site-packages\keras\src\saving\saving_api.py:262, in load_model(filepath, custom_objects, compile, safe_mode, **kwargs)
    254     return saving_lib.load_model(
    255         filepath,
    256         custom_objects=custom_objects,
    257         compile=compile,
    258         safe_mode=safe_mode,
    259     )
    261 # Legacy case.
--> 262 return legacy_sm_saving_lib.load_model(
    263     filepath, custom_objects=custom_objects, compile=compile, **kwargs
    264 )

File C:\Program Files\Python39\lib\site-packages\keras\src\utils\traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File C:\Program Files\Python39\lib\site-packages\h5py\_hl\files.py:562, in File.__init__(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)
    553     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,
    554                      locking, page_buf_size, min_meta_keep, min_raw_keep,
    555                      alignment_threshold=alignment_threshold,
    556                      alignment_interval=alignment_interval,
    557                      meta_block_size=meta_block_size,
    558                      **kwds)
    559     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,
    560                      fs_persist=fs_persist, fs_threshold=fs_threshold,
    561                      fs_page_size=fs_page_size)
--> 562     fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)
    564 if isinstance(libver, tuple):
    565     self._libver = libver

File C:\Program Files\Python39\lib\site-packages\h5py\_hl\files.py:235, in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)
    233     if swmr and swmr_support:
    234         flags |= h5f.ACC_SWMR_READ
--> 235     fid = h5f.open(name, flags, fapl=fapl)
    236 elif mode == 'r+':
    237     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)

File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper()

File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper()

File h5py\h5f.pyx:102, in h5py.h5f.open()

File h5py\h5fd.pyx:155, in h5py.h5fd.H5FD_fileobj_get_eof()

File h5py\h5fd.pyx:155, in h5py.h5fd.H5FD_fileobj_get_eof()

File h5py\h5fd.pyx:155, in h5py.h5fd.H5FD_fileobj_get_eof()

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xce in position 77: invalid continuation byte
```

I haven't changed anything in the code, it just suddenly stopped to open. I've tried to recreate the model in colab and download it again, changed tf version on computer to tf 2.13.0 (colab has it). Also I've tried to open another models on my pc and all of them threw this error

[Model in google drive](https://drive.google.com/drive/folders/1QIYJklw1tW5WCBanlfkIFyKFLwMGzez3?usp=sharing)

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1776CZxkKcetQsPkJfMACWnNel3Hj-hbA?usp=sharing - colab code for the nn

https://gist.github.com/DemO-O-On/65d5f1114d8163ae234a57c7789d7039 - script for loading model
```


### Relevant log output

Output from jupyter when trying to load model:

`2023-10-13 16:33:40.479946: E tensorflow/tsl/platform/windows/windows_file_system.cc:363] ERROR: GetSymbolicLinkTarget cannot open file for \\?\C:\Users\termi\Desktop\stlsegm\â•¨Ð¢â•¨â•¡â•¤Ðâ•¤Ð•_â•¨â–“â•¤Ð‘â•¨â•¡\â•¨Ð¢â•¨â•¡â•¤Ðâ•¤Ð•_â•¨â–’â•¨â•¡â•¨â•–_â•¨â”¤â•¨â•¡â•¤Ð‘â•¨â•œâ•¤Ð› â•¤Ð’â•¨â•›â•¨â•—â•¤Ðœâ•¨â•‘â•¨â•› â•¨â–“â•¨â•¡â•¤Ðâ•¤Ð•â•¨â•œâ•¨â••â•¨â•¡\â•¨Ð¢â•¨â•¡â•¤Ðâ•¤Ð• â•¤Ð‘â•¨â•¡â•¨â”‚â•¨â•â•¨â•¡â•¨â•œâ•¤Ð’â•¨â–‘â•¤Ð–â•¨â••â•¤ÐŸ GetLastError: 5`"
62106,ç¼–è¯‘åŽç¼ºå°‘liblibtensorflow_cc.so.ifsoæ–‡ä»¶ åªæœ‰libtensorflow_cc.so,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.6.0

### Custom code

Yes

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.7.16

### Bazel version

3.7.2

### GCC/compiler version

2019

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

3090

### Current behavior?

1

### Standalone code to reproduce the issue

```shell
INFO: Analyzed target //tensorflow:libtensorflow_cc.so (227 packages loaded, 22492 targets configured).
INFO: Found 1 target...
Target //tensorflow:libtensorflow_cc.so up-to-date:
  bazel-bin/tensorflow/libtensorflow_cc.so
INFO: Elapsed time: 5307.398s, Critical Path: 942.31s
INFO: 14402 processes: 3952 internal, 10450 local.
INFO: Build completed successfully, 14402 total actions
```


### Relevant log output

_No response_"
62105,Error when convert dynamic axes ONNX to dynamic axes TFLite ,"### 1. System information

- Ubuntu20.04
- Tensorflow: 2.11
- Python: 3.10.8
- Pytorch: 1.12.0

### 2. Code
#### The first convert Pytorch model to Onnx with dynamic input:
    network = 'rSfM120k-tl-resnet50-gem-w'
    state = load_url(PRETRAINED[network], model_dir=os.path.join(get_data_root(), 'networks'))
    net_params = {}
    net_params['architecture'] = state['meta']['architecture']
    net_params['pooling'] = state['meta']['pooling']
    net_params['local_whitening'] = state['meta'].get('local_whitening', False)
    net_params['regional'] = state['meta'].get('regional', False)
    net_params['whitening'] = state['meta'].get('whitening', True)
    net_params['mean'] = state['meta']['mean']
    net_params['std'] = state['meta']['std']
    net_params['pretrained'] = False
    
    net = init_network(net_params)
    net.load_state_dict(state['state_dict'])
    
    if useRmac:
        net.pool = RMAC(3)
    net.cuda()
    net.eval()
    dummy_input = torch.randn(1, 3, 400, 900)
    input_names = [ ""actual_input"" ]
    output_names = [ ""output"" ]
    dynamic_axes_dict = { 'actual_input': { 0: 'bs',  2: 'img_x',3: 'img_y'},'Output': { 0: 'bs'}} 
    torch.onnx.export(net,
                     dummy_input,
                     ""resnet50.onnx"",
                     verbose=False,
                     input_names=input_names,
                     output_names=output_names,
                    dynamic_axes=dynamic_axes_dict,
                     opset_version=12, 
                    )
#### The second convert Onnx model to Tf:
    onnx_path = 'resnet50.onnx'
    onnx_model = onnx.load( onnx_path)
    onnx.checker.check_model(onnx_model)
    tf_path = 'cirtorch_tf_0210_resnet50_withoutNor'
    tf_rep = prepare(onnx_model)  #Prepare TF representation
    tf_rep.export_graph(tf_path)  #Export the model
#### The third convert Tf model to TfLite:
    tf_lite_path = '0210_cirtorch_fl16_resnet50_withoutNor.tflite'
    converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)
    tflite_model  = converter.convert()
    with open(tf_lite_path, 'wb') as f:
        f.write(tflite_model)
#### Test model TfLite with dynamic input:
    tflite_model_path = tf_lite_path
    interpreter = tf.lite.Interpreter(model_path=tflite_model_path,experimental_delegates=[])
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    interpreter.resize_tensor_input(interpreter.get_input_details()[0]['index'],[1,3,200,200])
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    interpreter.allocate_tensors()
    x1 = torch.ones(1,3,200, 200)
    input_data = np.array(x1, dtype=np.float32)
    print(input_data.shape)
    interpreter.set_tensor(input_details[0]['index'], input_data)
    interpreter.invoke()
    output_data = interpreter.get_tensor(output_details[0]['index'])
    print(output_data.shape)
### But I got error when invoke model TFLite:
Traceback (most recent call last):
  File ""convertCirtorchFreesizeTFlite.py"", line 143, in <module>
    interpreter.invoke()
  File ""/home/anlab/anaconda3/envs/bopw/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py"", line 917, in invoke
    self._interpreter.Invoke()
RuntimeError: tensorflow/lite/kernels/squeeze.cc:63 current >= 0 && current < input_num_dims && input_dims->data[current] == 1 was not true.Node number 0 (SQUEEZE) failed to prepare.Node number 144 (IF) failed to prepare.
#### Or
  File ""/home/anlab/anaconda3/envs/bopw/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py"", line 917, in invoke
    self._interpreter.Invoke()
RuntimeError: tensorflow/lite/kernels/reshape.cc:85 num_input_elements != num_output_elements (2048 != 4194304)Node number 2 (RESHAPE) failed to invoke.Node number 144 (IF) failed to invoke.
"
62104,"Unable to change validation dataset for ""model.fit()"" function after it raises an exception","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using model.fit() in TensorFlow, the validation data provided as an input parameter is initially used for model evaluation. However, if the initial evaluation fails, subsequent runs of model.fit() still use the previously provided validation data, even if new data is provided.

### Standalone code to reproduce the issue

```shell
The error is quite easy to reproduce. Check the ""compile and train section"" in the below provided colab.

https://colab.research.google.com/drive/1Es8mQd0FOoWea4SrJ2TDz4NRSx0ZxwEt#scrollTo=08WJXiheNxK7

I modified a tensorflow tutorial colab on CNNs to reproduce this error. In the section i first run the model with correct parameters, then with incorrect and then again with correct parameters but this time it fails.
```


### Relevant log output

_No response_"
62103,AuditWheel Failures on Build for TensorFlow 2.14,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

TF2.14

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

Python3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current behavior?

When trying to build TensorFlow 2.14 on the ci/official `wheel.sh` script, we are seeing manylinux failures during the auditwheel check stating that the ""presence of too-recent versioned symbols. You'll need to compile the wheel on an older toolchain.""

Are there any recommendations to fix this issue for TF2.14 builds?

### Standalone code to reproduce the issue

```shell
Env Vars local:
TF_PYTHON_VERSION=3.10
TFCI_INDEX_HTML_ENABLE=0
TFCI_NIGHTLY_UPDATE_VERSION_ENABLE=0
TFCI_DOCKER_ENABLE=1
PWD=/home/ubuntu/TF/tensorflow/build/logs
LANG=C.UTF-8
TFCI_CAPTURE_LOGS_ENABLE=1
TFCI_UPLOAD_LIB_ENABLE=0
TFCI_DOCKER_IMAGE=tensorflow/build:2.14-python3.10
TFCI_NVIDIA_SMI_ENABLE=0
TFCI_GIT_DIR=<TF_HOME>
TFCI_DOCKER_PULL_ENABLE=0
TFCI_COPYBARA_ENABLE=0

Run:
./ci/official/wheel.sh
```


### Relevant log output

```shell
/root/.cache/bazel/_bazel_root/80314ce22acbaa076a5371cd57252cbd/external/python_x86_64-unknown-linux-gnu/lib/python3.10/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.
  warnings.warn(
Thu Oct 12 22:03:56 UTC 2023 : === Output wheel file is in: /home/ubuntu/TF/tensorflow/build
+ tfrun ./ci/official/utilities/rename_and_verify_wheels.sh build
+ docker exec tf ./ci/official/utilities/rename_and_verify_wheels.sh build
+ DIR=build
+ for wheel in $DIR/*.whl
+ echo 'Checking and renaming build/tensorflow-2.14.0-cp310-cp310-linux_x86_64.whl...'
Checking and renaming build/tensorflow-2.14.0-cp310-cp310-linux_x86_64.whl...
+ python3 -m auditwheel repair --plat manylinux2014_x86_64 build/tensorflow-2.14.0-cp310-cp310-linux_x86_64.whl --wheel-dir build
+ tee check.txt
INFO:auditwheel.main_repair:Repairing tensorflow-2.14.0-cp310-cp310-linux_x86_64.whl
usage: __main__.py [-h] [-V] [-v] command ...
__main__.py: error: cannot repair ""build/tensorflow-2.14.0-cp310-cp310-linux_x86_64.whl"" to ""manylinux2014_x86_64"" ABI because of the presence of too-recent versioned symbols. You'll need to compile the wheel on an older toolchain.

real    0m26.251s
user    0m23.852s
sys     0m2.388s
```
"
62101,Problemas com instalacao do tensorflow no docker,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

Atual

### Custom code

Yes

### OS platform and distribution

Torizon-Linux arm64

### Mobile device

Linux arm64

### Python version

3.10.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Estou tentando construir um docker, mas somente a lib do tensorflow nÃ£o funciona. Coisa que nÃ£o acontecia antes, hÃ¡ pouco tempo atrÃ¡s, eu nÃ£o tinha nenhum problema. JÃ¡ tentei diversas coisas como especificar versÃ£o do tensorflow, e do tensorflow-cpu-aws no requirements, atualizo o setuptools, construo a wheel, construo o build.

### Standalone code to reproduce the issue

```shell
Antes usando mesma imagem eu nÃ£o tinha nenhum problema com instalaÃ§Ã£o das libs, agora eu tenho problema somente com a lib do tensorflow.
```


### Relevant log output

```shell
FROM python:3.10

RUN apt-get update && apt-get install -y libgl1-mesa-glx

RUN pip install --upgrade pip

RUN pip install --upgrade setuptools

RUN pip install -r requirements.txt



Meu arquivo requirements.txt:
pytorch
keras
pytesseract
joblib
facenet-pytorch
flask-cors
flask
pandas
plotly
opencv-python
folium
socketio
regex
paramiko
joblib
numpy
psycopg2
tensorflow



ERRO:
ERROR: Could not find a version that satisfies the requirement tensorflow-cpu-aws==2.11.1; platform_system == ""Linux"" and (platform_machine == ""arm64"" or platform_machine == ""aarch64"") (from tensorflow) (from versions: 2.9.1, 2.10.0rc0, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0)
ERROR: No matching distribution found for tensorflow-cpu-aws==2.11.1; platform_system == ""Linux"" and (platform_machine == ""arm64"" or platform_machine == ""aarch64"")
```
"
62100,Deterministic behavior gets extremely slow with CNNs in TF > 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Tesla T4 15 GB

### Current behavior?

When setting up deterministic behavior with `tf.config.experimental.enable_op_determinism()`, a small model with convolution layers gets prohibitively slow.

The same code runs in about 80 seconds without the flag, but over 10 minutes with it. This behavior was not observed in TF 2.8.

The model code is as follows:

```
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Embedding, GlobalMaxPooling1D, Conv1D, BatchNormalization, Activation, concatenate, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.initializers import Constant
import numpy as np

vocab_size = 200_000
vocab_dim = 100
length1 = 50
length2 = 100
length = 100
num_samples = 100_000
conv_dim = 128
window_sizes = [2, 3, 4, 5]
num_classes = 2000

input_features = [Input(shape=(length1,), dtype=""int32""), Input(shape=(length2,), dtype=""int32"")]
hidden_tensors = []
    
for feature in input_features:
    embeddings = Embedding(vocab_size, vocab_dim)(feature)

    for window_size in window_sizes:
        convoluted = Conv1D(conv_dim, window_size)(embeddings)
        normed = BatchNormalization()(convoluted)
        activated = Activation(""relu"")(normed)
        pooled = GlobalMaxPooling1D()(activated)
        hidden_tensors.append(pooled)
    
hidden = Dropout(0.5)(concatenate(hidden_tensors))
outputs = Dense(num_classes, activation='softmax')(hidden)

model = Model(inputs=input_features, outputs=outputs)
model.compile(loss=""sparse_categorical_crossentropy"", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

features1 = np.random.randint(0, vocab_size, size=(num_samples, length1), dtype=np.int32)
features2 = np.random.randint(0, vocab_size, size=(num_samples, length2), dtype=np.int32)
labels = np.random.randint(0, num_classes, size=num_samples)

model.fit([features1, features2], labels, epochs=1, batch_size=256)
```

And I triggered determinism with:
```
tf.keras.utils.set_random_seed(1234)
tf.config.experimental.enable_op_determinism()
```

### Standalone code to reproduce the issue

[Colab notebook](https://colab.research.google.com/drive/16B1zAX24EYLkty6j3FmbecBx-t9jNoFe?usp=sharing)


### Relevant log output

_No response_"
62098,The configuration file of newer tensorflow versions doesn't provide building with sycl/opencl option. Does newer versions of Tensorflow doesn't provide sycl support?,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

9

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

NVIDIA RTX 3090 24 GB

### Current behavior?

I am trying to build Tensorflow version 2.14 from source with sycl/opencl support but when I am running the configuration file it isn't showing any sycl support option. So does the newer versions of tensorflow doesn't provide opencl/sycl support? 
If they do provide support, how to build newer versions of TF with sycl/opencl support?

### Standalone code to reproduce the issue

```shell
download and unzip the TF 2.14 
and run configure file
```


### Relevant log output

_No response_"
62097,"Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter.","
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
ERROR: Node number 13 (FlexTensorArrayV3) failed to prepare.

Problems with calling tesonflowlite in C/C++ã€‚Has anyone encountered this problemï¼Ÿï¼Ÿ"
62096,Unit tests failed on some Intel Sapphire Rapids CPUs due to wrong reference results from numpy 1.23.5,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

This is not an issue with using Tensorflow but an issue of false test failure when running some unit tests (e.g. //tensorflow/python/kernel_tests/math_ops:tensordot_op_test_gpu) due to issues with numpy 1.23.5 on some Intel Sapphire Rapids CPUs, including and not limited to:
Intel(R) Xeon(R) Platinum 8480C
Intel(R) Xeon(R) Platinum 8468

I have filed an issue with numpy, which has more details on the numpy bug (https://github.com/numpy/numpy/issues/24903).

Currently, Tensorflow uses numpy 1.23.5 when running unit tests via bazel (https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/requirements_lock_3_10.txt#L287). For some tests like //tensorflow/python/kernel_tests/math_ops:tensordot_op_test_gpu, it uses the results from numpy as reference. When running on systems with the above described Intel CPUs, the tests would falsely fail.



### Standalone code to reproduce the issue

```shell
## This is modified from tensorflow/tools/ci_build/linux/gpu/run_py3_core.sh 
set -e
set -x

N_JOBS=$(grep -c ^processor /proc/cpuinfo)

echo """"
echo ""Bazel will use ${N_JOBS} concurrent job(s).""
echo """"

# Run configure.
export PYTHON_BIN_PATH=`which python3`
export CC_OPT_FLAGS='-mavx'

export TF_NEED_ROCM=0
export TF_NEED_CUDA=1
export TF_CUDA_COMPUTE_CAPABILITIES=9.0 ## Replace with the compute capability of your GPUs
export TF_CUDA_CLANG=0

yes """" | $PYTHON_BIN_PATH configure.py

# Run bazel test command. Double test timeouts to avoid flakes.
bazel test --config=cuda --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-no_gpu,-benchmark-test -k \
    --test_lang_filters=py --jobs=${N_JOBS} --test_timeout 300,450,1200,3600 \
    --build_tests_only --test_output=errors --local_test_jobs=8 --config=opt \
    --test_size_filters=small,medium \
    --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute -- \
    //tensorflow/python/kernel_tests/math_ops:tensordot_op_test_gpu
```


### Relevant log output

```shell
[ RUN      ] TensordotTest.test_test_tensordot_scalar_axes_float64_5_5_5_False
INFO:tensorflow:Running test_tensordot_scalar_axes in GRAPH mode.
I1012 05:21:54.708423 139914706186240 test_util.py:1521] Running test_tensordot_scalar_axes in GRAPH mode.
2023-10-12 05:21:54.715617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78659 MB memory:  -> device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:5d:00.0, compute capability: 9.0
INFO:tensorflow:time(__main__.TensordotTest.test_test_tensordot_scalar_axes_float64_5_5_5_False): 5.42s
I1012 05:22:00.128815 139914706186240 test_util.py:2574] time(__main__.TensordotTest.test_test_tensordot_scalar_axes_float64_5_5_5_False): 5.42s
INFO:tensorflow:Running test_tensordot_scalar_axes in EAGER mode.
I1012 05:22:00.129978 139914706186240 test_util.py:1540] Running test_tensordot_scalar_axes in EAGER mode.
INFO:tensorflow:time(__main__.TensordotTest.test_test_tensordot_scalar_axes_float64_5_5_5_False): 5.11s
I1012 05:22:05.238928 139914706186240 test_util.py:2574] time(__main__.TensordotTest.test_test_tensordot_scalar_axes_float64_5_5_5_False): 5.11s
No pending test case: __main__.TensordotTest.test_test_tensordot_scalar_axes_float64_5_5_5_False
======================================================================
FAIL: test_test_tensordot_scalar_axes_float64_5_5_5_False (__main__.TensordotTest) [graph_mode]
TensordotTest.test_test_tensordot_scalar_axes_float64_5_5_5_False
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/math_ops/tensordot_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1535, in decorated
    f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/math_ops/tensordot_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2436, in decorated
    f(self, *args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/math_ops/tensordot_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/math_ops/tensordot_op_test.py"", line 226, in test_tensordot_scalar_axes
    self.assertAllClose(tf_ans, np_ans, rtol=tol, atol=tol)
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/math_ops/tensordot_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1657, in decorated
    return f(*args, **kwds)
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/math_ops/tensordot_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 3293, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/math_ops/tensordot_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 3249, in _assertAllCloseRecursive
    self._assertArrayLikeAllClose(
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/math_ops/tensordot_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 3186, in _assertArrayLikeAllClose
    np.testing.assert_allclose(
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/math_ops/tensordot_op_test_gpu.runfiles/pypi_numpy/site-packages/numpy/testing/_private/utils.py"", line 1527, in assert_allclose
    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
  File ""/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/math_ops/tensordot_op_test_gpu.runfiles/pypi_numpy/site-packages/numpy/testing/_private/utils.py"", line 844, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Not equal to tolerance rtol=1e-12, atol=1e-12
Mismatched value: a is different from b.
not close where = (array([0, 0, 0, ..., 4, 4, 4]), array([0, 0, 0, ..., 4, 4, 4]), array([0, 0, 0, ..., 4, 4, 4]), array([0, 0, 0, ..., 4, 4, 4]), array([0, 0, 0, ..., 4, 4, 4]), array([0, 0, 0, ..., 4, 4, 4]), array([2, 2, 2, ..., 2, 2, 3]), array([0, 1, 2, ..., 3, 4, 0]))
not close lhs = [ 1.62769716 -0.26834483  0.47169531 ...  0.99900873  0.74603632
 -1.2264429 ]
not close rhs = [-0.02489534  1.05437826 -1.4554834  ...  0.14947069  1.62145362
 -0.23293471]
not close dif = [1.65259251 1.32272308 1.92717871 ... 0.84953805 0.8754173  0.99350819]
not close tol = [1.02489534e-12 2.05437826e-12 2.45548340e-12 ... 1.14947069e-12
 2.62145362e-12 1.23293471e-12]
dtype = float64, shape = (5, 5, 5, 5, 5, 5, 5, 5)
Mismatched elements: 315000 / 390625 (80.6%)
Max absolute difference: 4.74676846
Max relative difference: 795311.53094219
 x: array([[[[[[[[ 9.517157e-01, -4.421175e-01, -5.290973e-01,
              -9.733468e-01, -9.410919e-02],
             [-3.183883e-01, -1.084177e+00, -6.588139e-01,...
 y: array([[[[[[[[ 9.517157e-01, -4.421175e-01, -5.290973e-01,
              -9.733468e-01, -9.410919e-02],
             [-3.183883e-01, -1.084177e+00, -6.588139e-01,...
```
"
62095,WSL2 tensorflow install not working,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.14.0-rc1-21-g4dacf3f368e 2.14.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.1 on WSL2

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA: 12.2 cudnn: ??

### GPU model and memory

rtx 3090 24gb 

### Current behavior?

Im trying to use tensorflow 2 on wsl2. I did the installation as in the tf docs.
After Installing tensorflow on wsl2 and running a test script as stated in the relevant code.

I get this some errors.

I have tried training a simple model with resnet which, at first, didnt work because cudnn was missing. After manually installing cudnn it worked but it feels so sketchy given that it should work without having to install it manually.

Also when I train my model (i dont know if this is normal behaviour) only my vram is used, the compute power of my gpu seems to not be used at all when i look at my taskmanager.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.test.is_built_with_cuda()
```


### Relevant log output

```shell
2023-10-11 22:19:14.589975: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-11 22:19:14.590010: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-11 22:19:14.590032: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-11 22:19:14.593224: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0
  warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}""
```
"
62094,Is there a way to run predict() for the same batch of data on two different sets of weights (same model architecture),"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

n/a

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi all, sorry If not the write forum to ask such a question, but I am wondering about functionality of TensorFlow.

Basically I want to run the same batch of data, through X different models. These models will have a different set of weights, but are architecturally the same model.

From a technical perspective, I don't see why it is not possible to do in a single set of computations. In practice, it not different than two submodels that share inputs nodes, but the nodes after are just not connected at all, you would have two output nodes (one of each sub-model).

### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_"
62093,OneRemoteGPUStrategy,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.8

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have been able to run a ParameterServer with only one external worker and an external parameter server in a remote machine. Unfortunately, the communication overhead is very big compared to a local run, for trainings with keras model.fit

I wonder if someone has developed a simpler version of PSS, that executes multiple steps before returning, or that is able to cache the variables to avoid overhead.

### Standalone code to reproduce the issue

```shell
#ideally I want to run the worker similarly to PSS, this is, just setting the TCONFIG and running #Server(), and in the main or coordinator side create the code:

with my_one_remote_machine_strategy:
    model.compile(...)

model.fit()
```


### Relevant log output

_No response_"
62092,Internal definition breaks tensorflow-probability,"Tensorflow-probability's [test](https://github.com/tensorflow/probability/issues/1753#issuecomment-1753993699) is broken, and it appears to be due to a [bad definition in tensorflow](https://github.com/tensorflow/probability/issues/1753#issuecomment-1756359940).

This is preventing the tensorflow-probability team from updating `typing_extensions`, which then breaks upgrading to Python 3.12.
"
62091,NNAPI delegate crashes on kTfLiteBuiltinCos in TF Lite 2.14,"### Issue type

Bug (segfault!)

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14

### Custom code

No

### OS platform and distribution

Android 13

### Mobile device

Pixel 4

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In TF Lite 2.14, the NNAPI delegate was changed to support the cos operator. Unfortunately, it does not work as expected and causes a segfault.

The originating change is https://github.com/tensorflow/tensorflow/commit/4aac8c95b7d7827eedca82a76cb71db1525dafc9.

The repro is easy. Use the below network and run with NNAPI.

The error is in `TransformCosIntoSupportedOps`, which makes the assumption that `theta` is non-null. This causes a null-pointer dereference.

But the problem is bigger than this - the NNAPI delegate should _not_ be modifying static data. This is dangerous and will lead to problems if the node is not accepted by the delegate, or if the execution graph is modified again to use a different delegate.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

class MyModel(tf.Module):
    @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])
    def my_operation(self, x):
        return tf.cos(x)

model = MyModel()

concrete_func = model.my_operation.get_concrete_function()
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
tflite_model = converter.convert()

with open(""nnapi_cos_bug.tflite"", ""wb"") as f:
    f.write(tflite_model)
```

Convert to tflite and load using NNAPI. The application will segfault.
```


### Relevant log output

_No response_"
62089,Failed to determine best cudnn convolution algorithm when using mixed_float16 policy,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

'v2.13.0-rc2-7-g1cb1a030a62', '2.13.0'

### Custom code

Yes

### OS platform and distribution

Google Colab

### Mobile device

No

### Python version

Google Colab default

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Google Colab default

### GPU model and memory

Google Colab default

### Current behavior?

When using depthwise conv inside custom attention (https://arxiv.org/pdf/2304.04237.pdf) layer with **mixed_float16** policy got ""Failed to determine best cudnn convolution algorithm"".

There is no such error with full precision.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1vsdCr_Mus9N6xax1vKjDvN093dh5F6rO?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
<ipython-input-3-32ea805343ae> in <cell line: 23>()
     21 # Train
     22 model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', run_eagerly=False, jit_compile=True)
---> 23 model.fit(ds_train, epochs=1, steps_per_epoch=20)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

UnknownError: Graph execution error:

Detected at node 'StatefulPartitionedCall' defined at (most recent call last):
    File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>
      ColabKernelApp.launch_instance()
    File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
      self.io_loop.start()
    File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
      self._run_once()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
      handle._run()
    File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
      lambda f: self._run_callback(functools.partial(callback, future))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
      ret = callback()
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
      self.ctx_run(self.run)
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
      yielded = self.gen.send(value)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
      yield gen.maybe_future(dispatch(*args))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
      yield gen.maybe_future(handler(stream, idents, msg))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
      self.do_execute(
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
      result = self._run_cell(
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
      return runner(coro)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
      coro.send(None)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-3-32ea805343ae>"", line 23, in <cell line: 23>
      model.fit(ds_train, epochs=1, steps_per_epoch=20)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1742, in fit
      tmp_logs = self.train_function(iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1338, in train_function
      return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1322, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
Node: 'StatefulPartitionedCall'
Failed to determine best cudnn convolution algorithm for:
%cudnn-conv.19 = (f16[8,128,128,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[8,128,128,1600]{3,2,1,0} %bitcast.959, f16[64,5,5,32]{3,2,1,0} %pad.15), window={size=5x5 pad=2_2x2_2}, dim_labels=b01f_o01i->b01f, feature_group_count=64, custom_call_target=""__cudnn$convForward"", metadata={op_type=""DepthwiseConv2dNativeBackpropInput"" op_name=""gradient_tape/model_1/slide_attention_1/depthwise/DepthwiseConv2dNativeBackpropInput"" source_file=""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"" source_line=276}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}""

Original error: UNKNOWN: CUDNN_STATUS_BAD_PARAM
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(3999): 'op' CUDNN_BACKEND_OPERATION: cudnnFinalize Failed

To ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.
	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_train_function_4989]
```
"
62088,Shape inference for tf.gather is inconsistent with/without --tf_mlir_enable_mlir_bridge,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Please refer to the ""Relevant log output"" section for detailed description of current behavior.

### Standalone code to reproduce the issue

```shell
# Run the code snippet with the following flags to reproduce the issue:
# TF_XLA_FLAGS=""--tf_mlir_enable_mlir_bridge --tf_xla_enable_xla_devices --tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_min_cluster_size=1""

import tensorflow as tf

params = tf.constant([
    [0, 0, 1, 0, 2],
    [3, 0, 0, 0, 4],
    [0, 5, 0, 6, 0]])
indices = tf.constant([
    [2, 4],
    [0, 4]])

print(tf.gather(params, indices, axis=1, batch_dims=1))
```


### Relevant log output

```shell
When mlir bridge is disabled, the code snippet works without any error. The return value of tf.gather is [[1 2][3 4]]


when mlir bridge is enabled, the code snippet returned with error:

2023-10-11 11:43:24.846292: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:841 : INVALID_ARGUMENT: Shape used to set computation result layout (s32[3,2]{1,0}) is not compatible with result shape (s32[2,2])
Traceback (most recent call last):
  File ""/tf/playground/test_gather.py"", line 37, in <module>
    print(tf.gather(params, indices, validate_indices=True, axis=1, batch_dims=1).numpy())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py"", line 5888, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shape used to set computation result layout (s32[3,2]{1,0}) is not compatible with result shape (s32[2,2])
         [[{{node cluster_1_1/xla_compile}}]] [Op:GatherV2] name:
```
"
62087,"TFRecordWriter stuck (or very slow) while serializing data, depending on feature transformation type","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

ubuntu 22.4

### Mobile device

_No response_

### Python version

3.8.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current behavior?


**tf.io.TFRecordWriter** freeze when preprocessing features with **scikit-learn** (**Dask-ML**) **QuantileTranformer**, whereas its working (writing out few k samples within seconds) when using **StandardScaler**.

How might QuantileTransformer produce output data shaped in a way it breaks TFRecords serialization? Might dtype precision influence serialization performance in such criticality?

### Standalone code to reproduce the issue

```shell
(reduced pseudo code)

# fit scalers
#### NOTE Option 1: using this scaler breaks TF records writer!
feat_standardizer = dask_QuantileTransformer(output_distribution=standardizer_distribution, 
                                                n_quantiles=n_feat_standardizer_quantils, 
                                                subsample=n_fit_samples, copy=False)


# NOTE Option 2: Using this one works for TF records writer
feat_standardizer = dask_StandardScaler(copy=False)


## from here proceed the same way until TF data serialization as TFRecord files

x_train_future = dask_client.scatter(x_train_arr) 
feat_standardizer = dask_client.submit(feat_standardizer.fit, x_train_future).result()
x_train_preproc_future = dask_client.submit(feat_standardizer.transform, x_train_future)
x_train_dask_arr = dask_client.gather(x_train_preproc_future)

....

# Transform training data (in chuncks of subsets of the total training dataset)
X_train_future = dask_client.scatter(x_train_arr_transform_batch)
X_train_future = dask_client.submit(feat_standardizer.transform, X_train_future)
X_train_future = dask_client.submit(feat_normalizer.transform, X_train_future)
x_train_arr_transform_batch  = dask_client.gather(X_train_future)
    
# reshape features: (sample * time, feat) -> (sample, time, feat)
X_train = x_train_arr_transform_batch.reshape((n_train_samples, n_steps, n_feat))

# cast numpy default precision float64 -> TF float32                               
X_train = X_train.astype(np.float32)
y_train = y_train.astype(np.int64)

def array_to_tfrecords(X, y):
        feature_dict = {
            'X': tf.train.Feature(float_list=tf.train.FloatList(value=X.flatten())),
            'y': tf.train.Feature(int64_list=tf.train.Int64List(value=y.flatten()))
        }
        example = tf.train.Example(features=tf.train.Features(feature=feature_dict)) 
        return example.SerializeToString()

tf.io.TFRecordWriter(tfrecords_file_path, options=tf.io.TFRecordOptions(compression_type='ZLIB', compression_level=7)) as writer:
        for x, y in tqdm(zip(X_train, y_train)):         # <---- no progress visible here when using QuantileTransformer!
            serialized = array_to_tfrecords(x, y)
            writer.write(serialized)
```


### Relevant log output

```shell
No output visible. The script just never continues and freeze while pretending to serialize data
```
"
62086,Conda-forge doesn't have tensorflow 1.15 anymore?,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

1.15

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have an old project that uses tensorflow 1.15, but recently I can't install the conda environment for this anymore. The version no longer exists on conda-forge, for some reason?
There are versions 1.14 and lower, and 2.*, but no longer 1.15? Why is that?

### Standalone code to reproduce the issue

```shell
conda create --name tf1 python=3.7
conda activate tf1
conda install -c conda-forge tensorflow=1.15
```


### Relevant log output

```shell
PackagesNotFoundError: The following packages are not available from current channels:                                                                                                                                                                                 
                                                                                                                                                                                                                                                                       
  - tensorflow=1.15   

Current channels:

  - https://conda.anaconda.org/conda-forge/win-64
  - https://conda.anaconda.org/conda-forge/noarch
```
"
62085,"Tensorflow build with sycl and ComputeCPP throwing error ""missing value for mandatory attribute 'toolchain_config' in 'cc_toolchain' rule""","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.3.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

3.1.0

### GCC/compiler version

9

### CUDA/cuDNN version

11.8 / 8.6

### GPU model and memory

RTX 3090- 24 GB

### Current behavior?

I'm trying to build Tensorflow from source with sycl and ComputeCPP backend but when I'm trying to build Tensorflow, I'm getting following errors:
@local_config_sycl//crosstool:cc-compiler-local: missing value for mandatory attribute 'toolchain_config' in 'cc_toolchain' rule
Target '@local_config_sycl//crosstool:empty' contains an error and its package is in error and referenced by '@local_config_sycl//crosstool:cc-compiler-local'
Target '@local_config_sycl//crosstool:cc-compiler-local' contains an error and its package is in error and referenced by '@local_config_sycl//crosstool:toolchain'
/home/vpy2kor/tensorflow-2.3.0/tensorflow/python/BUILD:2998:1: every rule of type cc_binary implicitly depends upon the target '@local_config_sycl//crosstool:toolchain', but this target could not be found because of: Target '@local_config_sycl//crosstool:toolchain' contains an error and its package is in error

### Standalone code to reproduce the issue

```shell
Move to tensorflow folder 
Run ./configure and make the necessary checks for sycl and computecpp
Start building tensorflow : bazel build --verbose_failures --jobs=16 //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_"
62082,Nested ExtensionTypes failing with new __tf_flatten__ / __tf_unflatten__ api,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.14

### Custom code

Yes

### OS platform and distribution

Apple Silicon M1

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Nested ExtensionTypes failing with new __tf_flatten__, __tf_unflatten__ api

### Standalone code to reproduce the issue

```shell

import dataclasses
import tensorflow as tf

@dataclasses.dataclass
class MaskedTensor(tf.experimental.ExtensionType):
    mask: bool
    value: tf.Tensor

    def __tf_flatten__(self):
        metadata = (self.mask,)  # static config.
        components = (self.value,)  # dynamic values.
        return metadata, components

    @classmethod
    def __tf_unflatten__(cls, metadata, components):
        return cls(*metadata, *components)
        

@dataclasses.dataclass
class MaskedTensorComp(tf.experimental.ExtensionType):
    mask: bool
    value: tf.Tensor
    mt: MaskedTensor

    def __tf_flatten__(self):
        metadata = (self.mask)  # static config.
        components = (self.value, self.mt)  # dynamic values.
        return metadata, components

    @classmethod
    def __tf_unflatten__(cls, metadata, components):
        print('Unflattening MaskedTensorComposite', components, metadata)
        return cls(*metadata, *components)

mt0 = MaskedTensor(True, tf.constant(3.))
mt = MaskedTensorComp(False, tf.constant(99.), mt=mt0)

mt_flat = tf.nest.flatten(mt)
mt_recon = tf.nest.pack_sequence_as(mt, mt_flat)
```
```


### Relevant log output

```shell
File ~/mambaforge/envs/_/lib/python3.10/site-packages/tensorflow/python/util/nest.py:538, in pack_sequence_as(structure, flat_sequence, expand_composites)
    424 @tf_export(""nest.pack_sequence_as"")
    425 def pack_sequence_as(structure, flat_sequence, expand_composites=False):
    426   """"""Returns a given flattened sequence packed into a given structure.
    427 
    428   Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)
   (...)
    536     TypeError: `structure` is or contains a dict with non-sortable keys.
    537   """"""
--> 538   return nest_util.pack_sequence_as(
    539       nest_util.Modality.CORE, structure, flat_sequence, expand_composites
    540   )

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:958, in pack_sequence_as(modality, structure, flat_sequence, expand_composites, sequence_fn)
    835 """"""Returns a given flattened sequence packed into a given structure.
    836 
    837 - For Modality.CORE: Refer to
   (...)
    955   non-sortable keys.
    956 """"""
    957 if modality == Modality.CORE:
--> 958   return _tf_core_pack_sequence_as(
    959       structure, flat_sequence, expand_composites, sequence_fn
    960   )
    961 elif modality == Modality.DATA:
    962   return _tf_data_pack_sequence_as(structure, flat_sequence)

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1022, in _tf_core_pack_sequence_as(structure, flat_sequence, expand_composites, sequence_fn)
   1015   if len(flat_structure) != len(flat_sequence):
   1016     # pylint: disable=raise-missing-from
   1017     raise ValueError(
   1018         ""Could not pack sequence. Structure had %d atoms, but ""
   1019         ""flat_sequence had %d items.  Structure: %s, flat_sequence: %s.""
   1020         % (len(flat_structure), len(flat_sequence), structure, flat_sequence)
   1021     )
-> 1022 return sequence_fn(structure, packed)

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:336, in sequence_like(instance, args)
    334   assert len(args) == 1
    335   spec = instance._type_spec  # pylint: disable=protected-access
--> 336   return spec._from_components(args[0])  # pylint: disable=protected-access
    337 elif _is_type_spec(instance):
    338   # Pack a CompositeTensor's components according to a TypeSpec.
    339   assert len(args) == 1

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/framework/extension_type.py:482, in ExtensionTypeSpec._from_components(self, components)
    476 if list(components_iter):
    477   raise ValueError(
    478       'Cannot build an ExtensionType instance from components '
    479       'because more components are provided than the number expected '
    480       'by the type spec.'
    481   )
--> 482 value_tuple = nest.pack_sequence_as(spec_tuple, flat)
    483 fields = dict(zip(self.__dict__.keys(), value_tuple))
    485 # Build the new value.  Bypass the constructor (__init__), in case the user
    486 # who defined the ExtensionType used a custom constructor.

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest.py:538, in pack_sequence_as(structure, flat_sequence, expand_composites)
    424 @tf_export(""nest.pack_sequence_as"")
    425 def pack_sequence_as(structure, flat_sequence, expand_composites=False):
    426   """"""Returns a given flattened sequence packed into a given structure.
    427 
    428   Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)
   (...)
    536     TypeError: `structure` is or contains a dict with non-sortable keys.
    537   """"""
--> 538   return nest_util.pack_sequence_as(
    539       nest_util.Modality.CORE, structure, flat_sequence, expand_composites
    540   )

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:958, in pack_sequence_as(modality, structure, flat_sequence, expand_composites, sequence_fn)
    835 """"""Returns a given flattened sequence packed into a given structure.
    836 
    837 - For Modality.CORE: Refer to
   (...)
    955   non-sortable keys.
    956 """"""
    957 if modality == Modality.CORE:
--> 958   return _tf_core_pack_sequence_as(
    959       structure, flat_sequence, expand_composites, sequence_fn
    960   )
    961 elif modality == Modality.DATA:
    962   return _tf_data_pack_sequence_as(structure, flat_sequence)

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1006, in _tf_core_pack_sequence_as(structure, flat_sequence, expand_composites, sequence_fn)
   1003   return flat_sequence[0]
   1005 try:
-> 1006   final_index, packed = _tf_core_packed_nest_with_indices(
   1007       structure, flat_sequence, 0, is_nested_fn, sequence_fn
   1008   )
   1009   if final_index < len(flat_sequence):
   1010     raise IndexError

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:679, in _tf_core_packed_nest_with_indices(structure, flat, index, is_nested_fn, sequence_fn)
    675 if is_nested_fn(s):
    676   new_index, child = _tf_core_packed_nest_with_indices(
    677       s, flat, index, is_nested_fn, sequence_fn
    678   )
--> 679   packed.append(sequence_fn(s, child))
    680   index = new_index
    681 else:

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:336, in sequence_like(instance, args)
    334   assert len(args) == 1
    335   spec = instance._type_spec  # pylint: disable=protected-access
--> 336   return spec._from_components(args[0])  # pylint: disable=protected-access
    337 elif _is_type_spec(instance):
    338   # Pack a CompositeTensor's components according to a TypeSpec.
    339   assert len(args) == 1

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/framework/extension_type.py:472, in ExtensionTypeSpec._from_components(self, components)
    470 spec_tuple = tuple(self.__dict__.values())
    471 components_iter = iter(components)
--> 472 flat = [
    473     next(components_iter) if isinstance(x, type_spec.TypeSpec) else x
    474     for x in nest.flatten(spec_tuple)
    475 ]
    476 if list(components_iter):
    477   raise ValueError(
    478       'Cannot build an ExtensionType instance from components '
    479       'because more components are provided than the number expected '
    480       'by the type spec.'
    481   )

File ~/mambaforge/envs/
File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest.py:538, in pack_sequence_as(structure, flat_sequence, expand_composites)
    424 @tf_export(""nest.pack_sequence_as"")
    425 def pack_sequence_as(structure, flat_sequence, expand_composites=False):
    426   """"""Returns a given flattened sequence packed into a given structure.
    427 
    428   Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)
   (...)
    536     TypeError: `structure` is or contains a dict with non-sortable keys.
    537   """"""
--> 538   return nest_util.pack_sequence_as(
    539       nest_util.Modality.CORE, structure, flat_sequence, expand_composites
    540   )

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:958, in pack_sequence_as(modality, structure, flat_sequence, expand_composites, sequence_fn)
    835 """"""Returns a given flattened sequence packed into a given structure.
    836 
    837 - For Modality.CORE: Refer to
   (...)
    955   non-sortable keys.
    956 """"""
    957 if modality == Modality.CORE:
--> 958   return _tf_core_pack_sequence_as(
    959       structure, flat_sequence, expand_composites, sequence_fn
    960   )
    961 elif modality == Modality.DATA:
    962   return _tf_data_pack_sequence_as(structure, flat_sequence)

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1022, in _tf_core_pack_sequence_as(structure, flat_sequence, expand_composites, sequence_fn)
   1015   if len(flat_structure) != len(flat_sequence):
   1016     # pylint: disable=raise-missing-from
   1017     raise ValueError(
   1018         ""Could not pack sequence. Structure had %d atoms, but ""
   1019         ""flat_sequence had %d items.  Structure: %s, flat_sequence: %s.""
   1020         % (len(flat_structure), len(flat_sequence), structure, flat_sequence)
   1021     )
-> 1022 return sequence_fn(structure, packed)

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:336, in sequence_like(instance, args)
    334   assert len(args) == 1
    335   spec = instance._type_spec  # pylint: disable=protected-access
--> 336   return spec._from_components(args[0])  # pylint: disable=protected-access
    337 elif _is_type_spec(instance):
    338   # Pack a CompositeTensor's components according to a TypeSpec.
    339   assert len(args) == 1

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/framework/extension_type.py:482, in ExtensionTypeSpec._from_components(self, components)
    476 if list(components_iter):
    477   raise ValueError(
    478       'Cannot build an ExtensionType instance from components '
    479       'because more components are provided than the number expected '
    480       'by the type spec.'
    481   )
--> 482 value_tuple = nest.pack_sequence_as(spec_tuple, flat)
    483 fields = dict(zip(self.__dict__.keys(), value_tuple))
    485 # Build the new value.  Bypass the constructor (__init__), in case the user
    486 # who defined the ExtensionType used a custom constructor.

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest.py:538, in pack_sequence_as(structure, flat_sequence, expand_composites)
    424 @tf_export(""nest.pack_sequence_as"")
    425 def pack_sequence_as(structure, flat_sequence, expand_composites=False):
    426   """"""Returns a given flattened sequence packed into a given structure.
    427 
    428   Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)
   (...)
    536     TypeError: `structure` is or contains a dict with non-sortable keys.
    537   """"""
--> 538   return nest_util.pack_sequence_as(
    539       nest_util.Modality.CORE, structure, flat_sequence, expand_composites
    540   )

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:958, in pack_sequence_as(modality, structure, flat_sequence, expand_composites, sequence_fn)
    835 """"""Returns a given flattened sequence packed into a given structure.
    836 
    837 - For Modality.CORE: Refer to
   (...)
    955   non-sortable keys.
    956 """"""
    957 if modality == Modality.CORE:
--> 958   return _tf_core_pack_sequence_as(
    959       structure, flat_sequence, expand_composites, sequence_fn
    960   )
    961 elif modality == Modality.DATA:
    962   return _tf_data_pack_sequence_as(structure, flat_sequence)

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1006, in _tf_core_pack_sequence_as(structure, flat_sequence, expand_composites, sequence_fn)
   1003   return flat_sequence[0]
   1005 try:
-> 1006   final_index, packed = _tf_core_packed_nest_with_indices(
   1007       structure, flat_sequence, 0, is_nested_fn, sequence_fn
   1008   )
   1009   if final_index < len(flat_sequence):
   1010     raise IndexError

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:679, in _tf_core_packed_nest_with_indices(structure, flat, index, is_nested_fn, sequence_fn)
    675 if is_nested_fn(s):
    676   new_index, child = _tf_core_packed_nest_with_indices(
    677       s, flat, index, is_nested_fn, sequence_fn
    678   )
--> 679   packed.append(sequence_fn(s, child))
    680   index = new_index
    681 else:

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:336, in sequence_like(instance, args)
    334   assert len(args) == 1
    335   spec = instance._type_spec  # pylint: disable=protected-access
--> 336   return spec._from_components(args[0])  # pylint: disable=protected-access
    337 elif _is_type_spec(instance):
    338   # Pack a CompositeTensor's components according to a TypeSpec.
    339   assert len(args) == 1

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/framework/extension_type.py:472, in ExtensionTypeSpec._from_components(self, components)
    470 spec_tuple = tuple(self.__dict__.values())
    471 components_iter = iter(components)
--> 472 flat = [
    473     next(components_iter) if isinstance(x, type_spec.TypeSpec) else x
    474     for x in nest.flatten(spec_tuple)
    475 ]
    476 if list(components_iter):
    477   raise ValueError(
    478       'Cannot build an ExtensionType instance from components '
    479       'because more components are provided than the number expected '
    480       'by the type spec.'
    481   )

File ~/mambaforge/envs//lib/python3.10/site-packages/tensorflow/python/framework/extension_type.py:473, in <listcomp>(.0)
    470 spec_tuple = tuple(self.__dict__.values())
    471 components_iter = iter(components)
    472 flat = [
--> 473     next(components_iter) if isinstance(x, type_spec.TypeSpec) else x
    474     for x in nest.flatten(spec_tuple)
    475 ]
    476 if list(components_iter):
    477   raise ValueError(
    478       'Cannot build an ExtensionType instance from components '
    479       'because more components are provided than the number expected '
    480       'by the type spec.'
    481   )

StopIteration: 
```/lib/python3.10/site-packages/tensorflow/python/framework/extension_type.py:473, in <listcomp>(.0)
    470 spec_tuple = tuple(self.__dict__.values())
    471 components_iter = iter(components)
    472 flat = [
--> 473     next(components_iter) if isinstance(x, type_spec.TypeSpec) else x
    474     for x in nest.flatten(spec_tuple)
    475 ]
    476 if list(components_iter):
    477   raise ValueError(
    478       'Cannot build an ExtensionType instance from components '
    479       'because more components are provided than the number expected '
    480       'by the type spec.'
    481   )

StopIteration:
```
"
62081,NVCC compilation failure with tf-nightly 2.15,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0.dev20231002

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

_No response_

### GCC/compiler version

c++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0

### CUDA/cuDNN version

cuda-11.8, cudnn-8.6

### GPU model and memory

_No response_

### Current behavior?

mvcc fails to compile following code (test.cc).
```
#include ""tensorflow/core/framework/tensor.h""

using namespace tensorflow;

int main() {
	Tensor x;
	return 0;
}
```

```
# nvcc -I/usr/local/cuda/include -expt-relaxed-constexpr $(python -c ""import tensorflow as tf; print(' '.join(tf.sysconfig.get_compile_flags()))"") -x cu -c test.cc -o test.o

/miniconda/envs/venv/lib/python3.10/site-packages/tensorflow/include/absl/strings/internal/str_format/bind.h: In constructor â€˜absl::lts_20230125::str_format_internal::FormatSpecTemplate<Args>::FormatSpecTemplate(const absl::lts_20230125::str_format_internal::ExtendedParsedFormat<absl::lts_20230125::FormatConversionCharSet(C)...>&)â€™:
/miniconda/envs/venv/lib/python3.10/site-packages/tensorflow/include/absl/strings/internal/str_format/bind.h:171:1: error: parse error in template argument list
  171 |     CheckArity<sizeof...(C), sizeof...(Args)>();
      | ^   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~              
/miniconda/envs/venv/lib/python3.10/site-packages/tensorflow/include/absl/strings/internal/str_format/bind.h:171:63: error: expected â€˜;â€™ before â€˜)â€™ token
  171 |     CheckArity<sizeof...(C), sizeof...(Args)>();
      |                                                               ^
/miniconda/envs/venv/lib/python3.10/site-packages/tensorflow/include/absl/strings/internal/str_format/bind.h:172:147: error: template argument 1 is invalid
  172 |     CheckMatches<C...>(absl::make_index_sequence<sizeof...(C)>{});
      |                                                                                                                                                   ^
/miniconda/envs/venv/lib/python3.10/site-packages/tensorflow/include/absl/strings/internal/str_format/bind.h:172:151: error: expected primary-expression before â€˜{â€™ token
  172 |     CheckMatches<C...>(absl::make_index_sequence<sizeof...(C)>{});
      |                                                                                                                                                       ^
/miniconda/envs/venv/lib/python3.10/site-packages/tensorflow/include/absl/strings/internal/str_format/bind.h:172:151: error: expected â€˜;â€™ before â€˜{â€™ token
/miniconda/envs/venv/lib/python3.10/site-packages/tensorflow/include/absl/strings/internal/str_format/bind.h:172:153: error: expected primary-expression before â€˜)â€™ token
  172 |     CheckMatches<C...>(absl::make_index_sequence<sizeof...(C)>{});
      |                                                                                                                                                         ^
```

This is likely due to [`#include ""absl/strings/str_format.h""`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/resource_base.h#L22) introduced from https://github.com/tensorflow/tensorflow/commit/fa87199c133ece4d7e3f5947cec2ec0a74d2cc50.

### Standalone code to reproduce the issue

```shell
See above.
```


### Relevant log output

This works fine with previous versions of TensorFlow (2.1~2.14)."
62080,"GPU not detected even after installing CUDA, cuDNN correct versions","### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.14.0

### Custom code

No

### OS platform and distribution

Windows 10 22H2 19045.3448

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA v11.2, cudnn-11.2-windows-x64-v8.1.0.77

### GPU model and memory

GeForce GTX 960M, 4Gb

### Current behavior?

Unable to detect the GPU. TF says: 

`print(tf.test.is_built_with_cuda())
False`

### Standalone code to reproduce the issue

```shell
It is an installation issue. 

C:\Users\user>python
Python 3.10.10 (tags/v3.10.10:aad5f6a, Feb  7 2023, 17:20:36) [MSC v.1929 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> import tensorflow as tf
>>> print(tf.test.is_built_with_cuda())
False
>>> print(tf.config.list_physical_devices('GPU'))
[]
>>>
>>> print(tf.config.list_physical_devices())
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]
>>>
```


### Relevant log output

```shell
C:\Windows\system32>
python -V 
Python 3.10.10 (tags/v3.10.10:aad5f6a, Feb  7 2023, 17:20:36) [MSC v.1929 64 bit (AMD64)] on win32
---------------------------------------------------------------------------------
pip list
Package                      Version
---------------------------- ------------
absl-py                      2.0.0
anyio                        4.0.0
argon2-cffi                  23.1.0
argon2-cffi-bindings         21.2.0
arrow                        1.3.0
asttokens                    2.4.0
astunparse                   1.6.3
async-lru                    2.0.4
attrs                        23.1.0
Babel                        2.13.0
backcall                     0.2.0
beautifulsoup4               4.12.2
bleach                       6.1.0
cachetools                   5.3.1
certifi                      2023.7.22
cffi                         1.16.0
charset-normalizer           3.3.0
colorama                     0.4.6
comm                         0.1.4
contourpy                    1.1.1
cycler                       0.12.1
debugpy                      1.8.0
decorator                    5.1.1
defusedxml                   0.7.1
exceptiongroup               1.1.3
executing                    2.0.0
fastjsonschema               2.18.1
flatbuffers                  23.5.26
fonttools                    4.43.1
fqdn                         1.5.1
gast                         0.5.4
google-auth                  2.23.3
google-auth-oauthlib         1.0.0
google-pasta                 0.2.0
grpcio                       1.59.0
h5py                         3.10.0
idna                         3.4
ipykernel                    6.25.2
ipython                      8.16.1
ipython-genutils             0.2.0
ipywidgets                   8.1.1
isoduration                  20.11.0
jedi                         0.19.1
Jinja2                       3.1.2
joblib                       1.3.2
json5                        0.9.14
jsonpointer                  2.4
jsonschema                   4.19.1
jsonschema-specifications    2023.7.1
jupyter                      1.0.0
jupyter_client               8.3.1
jupyter-console              6.6.3
jupyter_core                 5.4.0
jupyter-events               0.7.0
jupyter-lsp                  2.2.0
jupyter_server               2.7.3
jupyter_server_terminals     0.4.4
jupyterlab                   4.0.6
jupyterlab-pygments          0.2.2
jupyterlab_server            2.25.0
jupyterlab-widgets           3.0.9
keras                        2.14.0
kiwisolver                   1.4.5
libclang                     16.0.6
Markdown                     3.5
MarkupSafe                   2.1.3
matplotlib                   3.8.0
matplotlib-inline            0.1.6
mistune                      3.0.2
ml-dtypes                    0.2.0
mpmath                       1.3.0
nbclient                     0.8.0
nbconvert                    7.9.2
nbformat                     5.9.2
nest-asyncio                 1.5.8
notebook                     7.0.4
notebook_shim                0.2.3
numpy                        1.26.0
oauthlib                     3.2.2
opt-einsum                   3.3.0
overrides                    7.4.0
packaging                    23.2
pandas                       2.1.1
pandocfilters                1.5.0
parso                        0.8.3
pickleshare                  0.7.5
Pillow                       10.0.1
pip                          23.2.1
platformdirs                 3.11.0
plotly                       5.17.0
prometheus-client            0.17.1
prompt-toolkit               3.0.39
protobuf                     4.24.4
psutil                       5.9.5
pure-eval                    0.2.2
pyasn1                       0.5.0
pyasn1-modules               0.3.0
pycparser                    2.21
Pygments                     2.16.1
pyparsing                    3.1.1
python-dateutil              2.8.2
python-json-logger           2.0.7
pytz                         2023.3.post1
pywin32                      306
pywinpty                     2.0.12
PyYAML                       6.0.1
pyzmq                        25.1.1
qtconsole                    5.4.4
QtPy                         2.4.0
referencing                  0.30.2
requests                     2.31.0
requests-oauthlib            1.3.1
rfc3339-validator            0.1.4
rfc3986-validator            0.1.1
rpds-py                      0.10.4
rsa                          4.9
scikit-learn                 1.3.1
scipy                        1.11.3
Send2Trash                   1.8.2
setuptools                   65.5.0
six                          1.16.0
sniffio                      1.3.0
soupsieve                    2.5
stack-data                   0.6.3
sympy                        1.12
tenacity                     8.2.3
tensorboard                  2.14.1
tensorboard-data-server      0.7.1
tensorflow                   2.14.0
tensorflow-estimator         2.14.0
tensorflow-intel             2.14.0
tensorflow-io-gcs-filesystem 0.31.0
termcolor                    2.3.0
terminado                    0.17.1
threadpoolctl                3.2.0
tinycss2                     1.2.1
tomli                        2.0.1
tornado                      6.3.3
tqdm                         4.66.1
traitlets                    5.11.2
types-python-dateutil        2.8.19.14
typing_extensions            4.8.0
tzdata                       2023.3
uri-template                 1.3.0
urllib3                      2.0.6
wcwidth                      0.2.8
webcolors                    1.13
webencodings                 0.5.1
websocket-client             1.6.4
Werkzeug                     3.0.0
wheel                        0.41.2
widgetNVIDIA snbextension           4.0.9
wrapt                        1.14.1
---------------------------------------------------------------------------------
Hardware: GeForce GTX 960M
https://www.nvidia.com/en-us/geforce/gaming-laptops/geforce-gtx-960m/specifications/
https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html

CUDA Cores
1096 + Boost Base Clock (MHz)
GTX 960M Memory Specs:
2500 MHz Memory Clock
GDDR5 Memory Interface
128-bit Memory Interface Width
80Memory Bandwidth (GB/sec)
GTX 960M Technology Support: Yes 
NVIDIAÂ® Optimusâ„¢ Support 1 Yes 
NVIDIA Battery Boostâ„¢ Support 2
2.0NVIDIA GPU Boostâ„¢ Yes
NVIDIA GameStreamâ„¢-Ready Yes
GeForce ShadowPlayâ„¢ Yes
NVIDIA GameWorksâ„¢
12 API Microsoft DirectX
4.5 OpenGL 
CUDA Yes
PCI Express 3.0 Bus Support
Windows 8 and 8.1
Windows 7OS Certification

Processor	Intel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz   2.60 GHz
Installed RAM	32.0 GB (31.8 GB usable)
System type	64-bit operating system, x64-based processor

---------------------------------------------------------------------------------

>>> import tensorflow as tf
>>> print(tf.__version__)
2.14.0

---------------------------------------------------------------------------------
Used table from here: https://www.tensorflow.org/install/source_windows

tensorflow 2.14.0,	Python 3.7-3.10,	cuDNN 8.1	CUDA 11.2

CUDA v11.2, cudnn-11.2-windows-x64-v8.1.0.77, Python 3.10.10
---------------------------------------------------------------------------------

>>> print(tf.test.is_built_with_cuda())
False
>>> print(tf.config.list_physical_devices('GPU'))
[]
>>>
>>> print(tf.config.list_physical_devices())
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]
>>>

---------------------------------------------------------------------------------
OS

Edition	Windows 10 Home
Version	22H2
Installed on	â€Ž2/â€Ž28/â€Ž2023
OS build	19045.3448
Experience	Windows Feature Experience Pack 1000.19044.1000.0
```
"
62078,An example of  `a batched of RaggedTensor(s) with rank 2` as sparse_ids for tf.nn.safe_/embedding_lookup_sparse?,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Currently, on the document https://tensorflow.google.cn/api_docs/python/tf/nn/safe_embedding_lookup_sparse?hl=en, it got claimed that `use of RaggedTensors can yield higher performance.`, without concrete examples for showing how to do the work.

An example pls?

### Standalone code to reproduce the issue

```shell
I tried several approaches to feed a batch of RaggedTensor to embedding_lookup operator, all failed with `object has no attribute 'dense_shape'` error, we could explicitly call `.to_sparse` method of RaggedTensor, but again, it comes SparseTensor which could not bring any performance gain, right?

Pls show an example, thx!
```


### Relevant log output

_No response_"
62077,Code to make TFQ Demos run in Colab,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tensorflow==2.7.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.9, Also tried current version in Colab

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Does not correctly install tensorflow or tensorflow_quantum with current version of Python in Colab or Python 3.9 for multiple demos. 

### Standalone code to reproduce the issue

```shell
ERROR: Could not find a version that satisfies the requirement tensorflow==2.7.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0)
ERROR: No matching distribution found for tensorflow==2.7.0

ERROR: Could not find a version that satisfies the requirement tensorflow-quantum==0.7.2 (from versions: none)
ERROR: No matching distribution found for tensorflow-quantum==0.7.2

ModuleNotFoundError: No module named 'tensorflow_quantum'
```


### Relevant log output

_No response_"
62076,Bias size verification in `tfl.conv_3d_tranpose` op,"https://github.com/tensorflow/tensorflow/blob/83078cb3cc4da3f2224e2e09825d1f164ccc2da5/tensorflow/compiler/mlir/lite/ir/tfl_ops.td#L5543

should be

`TFL_NumElementsEqualsDim<3, 1, 3>`

instead of

`TFL_NumElementsEqualsDim<3, 1, 4>`

since the number of output channels in the filter is given in dimension 3.
"
62075,"cuDNN, cuFFT, and cuBLAS Errors","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

GIT_VERSION:v2.14.0-rc1-21-g4dacf3f368e VERSION:2.14.0

### Custom code

No

### OS platform and distribution

WSL2 Linux Ubuntu 22

### Mobile device

_No response_

### Python version

3.10, but I can try different versions

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA version: 11.8, cuDNN version: 8.7

### GPU model and memory

NVIDIA Geforce GTX 1660 Ti, 8GB Memory

### Current behavior?

When I run the GPU test from the TensorFlow install instructions, I get several errors and warnings.
I don't care about the NUMA stuff, but the first 3 errors are that TensorFlow was not able to load cuDNN. I would really like to be able to use it to speed up training some RNNs and FFNNs. I do get my GPU in the list of physical devices, so I can still train, but not as fast as with cuDNN.

### Standalone code to reproduce the issue

```shell
python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```


### Relevant log output

```shell
2023-10-09 13:36:23.355516: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-09 13:36:23.355674: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-09 13:36:23.355933: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-09 13:36:23.413225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-09 13:36:25.872586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-09 13:36:25.916952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-09 13:36:25.917025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```
"
62073,ERROR: No matching distribution found for tensorflow-gpu==2.11,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

windows

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11 / 8

### GPU model and memory

3060

### Current behavior?

WARNING: Ignoring invalid distribution -pencv-python-headless (c:\users\rites\appdata\roaming\python\python39\site-packages)
WARNING: Ignoring invalid distribution -pencv-python-headless (c:\users\rites\appdata\roaming\python\python39\site-packages)
ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.11 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.12.0)

### Standalone code to reproduce the issue

```shell
WARNING: Ignoring invalid distribution -pencv-python-headless (c:\users\rites\appdata\roaming\python\python39\site-packages)
WARNING: Ignoring invalid distribution -pencv-python-headless (c:\users\rites\appdata\roaming\python\python39\site-packages)
ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.11 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.12.0)
```


### Relevant log output

_No response_"
62072,tf.math.is_non_decreasing outputs incorrect result when input is an uint tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When giving tf.math.is_non_decreasing a decreasing tensor with dtype=uint32. This API incorrectly output True instead of False.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([10,9], dtype='uint32')
print(x)
out = tf.math.is_non_decreasing(x)
print(out)
```


### Relevant log output

_No response_"
62071,tf.truncatediv does not support float/complex tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Not sure if it is an expected behavior but it conflicts the documentation (https://www.tensorflow.org/api_docs/python/tf/truncatediv). 
tf.truncatediv will directly crash when the input is float-related data type of complex-related datatype.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import warnings
warnings.filterwarnings(""ignore"")
dtype_list = [
    'bfloat16', 'bool', 'complex128', 'complex64',
    'double', 'float16', 'float32',
    'float64', 'half', 'int16', 'int32', 'int64', 'int8',
    'uint16', 'uint32', 'uint64', 'uint8'
]
for dtype in dtype_list:
    x = tf.constant(np.random.rand(0), dtype=dtype)
    y = tf.constant(np.random.randint(0, 100, ()), dtype=dtype)
    try:
        out = tf.truncatediv(x,y)
        print(f""Success on dtype: {dtype}"")
    except:
        print(f""Fail on dtype: {dtype}"")
```


### Relevant log output

```shell
Fail on dtype: bfloat16
Fail on dtype: bool
Fail on dtype: complex128
Fail on dtype: complex64
Fail on dtype: double
Fail on dtype: float16
Fail on dtype: float32
Fail on dtype: float64
Fail on dtype: half
Success on dtype: int16
Success on dtype: int32
Success on dtype: int64
Success on dtype: int8
Success on dtype: uint16
Success on dtype: uint32
Success on dtype: uint64
Success on dtype: uint8
```
"
62070,tf.truncatemod does not support half and bfloat16 data type,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following the documentation (https://www.tensorflow.org/api_docs/python/tf/truncatemod) tf.truncatemod supports the half and bfloat16 data type but in practical it does not.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import warnings
warnings.filterwarnings(""ignore"")
x = tf.constant(np.random.rand(2,2), dtype='half')
y = tf.constant(np.random.randint(0, 100, ()), dtype='half')
out = tf.truncatemod(x,y)  # crash
print(out)


import tensorflow as tf
import numpy as np
import warnings
warnings.filterwarnings(""ignore"")
x = tf.constant(np.random.rand(2,2), dtype='bfloat16')
y = tf.constant(np.random.randint(0, 100, ()), dtype='bfloat16')
out = tf.truncatemod(x,y)  # crash
print(out)
```


### Relevant log output

```shell
NotFoundError: Could not find device for node: {{node TruncateMod}} = TruncateMod[T=DT_BFLOAT16]
All kernels registered for op TruncateMod:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]
  device='DEFAULT'; T in [DT_INT32]
  device='GPU'; T in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
 [Op:TruncateMod] name: 

NotFoundError: Could not find device for node: {{node TruncateMod}} = TruncateMod[T=DT_HALF]
All kernels registered for op TruncateMod:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]
  device='DEFAULT'; T in [DT_INT32]
  device='GPU'; T in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
 [Op:TruncateMod] name:
```
"
62067,"Calculating Gradients for a graph containing tf.image.extract_patches, using TF 2.9.0","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.9.0-rc2-42-g8a20d54a3c1 2.9.0

### Custom code

Yes

### OS platform and distribution

Windows 10-64bit

### Mobile device

_No response_

### Python version

Python 3.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda11.2

### GPU model and memory

_No response_

### Current behavior?

My codes extract patches from a batch of  images as an input and produces a batch of  image patches as an output.

The input (batch_size, rows, cols, 1) is split into patches (patches_num, batch_size, patch_row, patch_col) before being fed to next layer of the network.  This function was called by a custom layer (keras.layers.Layer), and it works well during extracts the patches, but when calculating gradients for a graph with `grads = tape.gradient(loss, model.variables)`, an error came:
```
UnimplementedError: Graph execution error:

Detected at node 'gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul'

Node: 'gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul'
2 root error(s) found.
  (0) UNIMPLEMENTED:  A deterministic GPU implementation of SparseTensorDenseMatmulOp is not currently available.
	 [[{{node gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul}}]]
	 [[gradients/fan_weight_8/StatefulPartitionedCall_grad/PartitionedCall/gradients/ExtractImagePatches_grad/ExtractImagePatches/_63]]
  (1) UNIMPLEMENTED:  A deterministic GPU implementation of SparseTensorDenseMatmulOp is not currently available.
	 [[{{node gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference___backward_call_20302_28692]
```
I found an issue about tf.extract_image_patches() opend on Feb 10, 2017 (https://github.com/tensorflow/tensorflow/issues/7414)
and added `tf.cast(input_batch, dtype=tf.float32, name=""castData"") ` in the function, but it wont work.

Here is my code of the function.

```
def spilt_patches(input_batch, 
                   blocks_num_cur, 
                   block_shape_cur):
   '''
    input_batch = np.tile(np.random.randint(0,2,(4,3,3)), (1,2,2))
    blocks_num_cur = 4,
    block_shape_cur = [6,6]
   '''
    shape = input_batch.shape
    input_batch = tf.reshape(input_batch,(shape[0],shape[1], shape[2], 1)) # [batch_size, rows, cols, 1]
    input_batch = tf.cast(input_batch, dtype=tf.float32, name=""castData"")  
    blocks_cur = tf.image.extract_patches(images = input_batch,
                                       sizes=[1, block_shape_cur[0], block_shape_cur[1], 1],
                                       strides=[1, block_shape_cur[0], block_shape_cur[1], 1],
                                       rates=[1, 1, 1, 1],
                                       padding='VALID') 
    blocks_cur = tf.cast(tf.reshape(blocks_cur,[shape[0], blocks_num_cur, block_shape_cur[0], block_shape_cur[1]]), tf.float32) # [batch_size, patch_num, patch_row, patch_col]
    blocks_cur = tf.transpose(blocks_cur,[1, 0, 2, 3]) 
    
    return blocks_cur
```

### Standalone code to reproduce the issue

```shell
https://gist.github.com/RaymondMarzas/7bec1a8099aded089a29d775caef880e
```


### Relevant log output

```shell
Traceback (most recent call last):

  File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\py3compat.py"", line 356, in compat_exec
    exec(code, globals, locals)

  File ""d:\codes\pycoode\ocnn2nd\fanoutmodel_train_ocnn.py"", line 112, in <module>
    grads = tape.gradient(loss, model.variables)

  File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tensorflow\python\eager\backprop.py"", line 1106, in gradient
    unconnected_gradients=unconnected_gradients)

  File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tensorflow\python\eager\imperative_grad.py"", line 73, in imperative_grad
    compat.as_str(unconnected_gradients.value))

  File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tensorflow\python\eager\function.py"", line 1206, in _backward_function_wrapper
    processed_args, remapped_captures)

  File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tensorflow\python\eager\function.py"", line 1861, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))

  File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tensorflow\python\eager\function.py"", line 502, in call
    ctx=ctx)

  File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tensorflow\python\eager\execute.py"", line 55, in quick_execute
    inputs, attrs, num_outputs)

UnimplementedError: Graph execution error:

Detected at node 'gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul' defined at (most recent call last):
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\runpy.py"", line 193, in _run_module_as_main
      ""__main__"", mod_spec)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\runpy.py"", line 85, in _run_code
      exec(code, run_globals)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\console\__main__.py"", line 24, in <module>
      start.main()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\console\start.py"", line 340, in main
      kernel.start()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelapp.py"", line 712, in start
      self.io_loop.start()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tornado\platform\asyncio.py"", line 215, in start
      self.asyncio_loop.run_forever()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\asyncio\base_events.py"", line 541, in run_forever
      self._run_once()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\asyncio\base_events.py"", line 1786, in _run_once
      handle._run()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\asyncio\events.py"", line 88, in _run
      self._context.run(self._callback, *self._args)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 510, in dispatch_queue
      await self.process_one()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 499, in process_one
      await dispatch(*args)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 406, in dispatch_shell
      await result
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 730, in execute_request
      reply_content = await reply_content
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\ipkernel.py"", line 387, in do_execute
      cell_id=cell_id,
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\zmqshell.py"", line 528, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 2975, in run_cell
      raw_cell, store_history, silent, shell_futures, cell_id
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3029, in _run_cell
      return runner(coro)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\async_helpers.py"", line 78, in _pseudo_sync_runner
      coro.send(None)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3257, in run_cell_async
      interactivity=interactivity, compiler=compiler, result=result)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3472, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3552, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""C:\Users\xxx\AppData\Local\Temp\ipykernel_29532\1338903705.py"", line 1, in <module>
      runfile('D:/CODES/pycoode/OCNN2nd/fanoutmodel_train_ocnn.py', wdir='D:/CODES/pycoode/OCNN2nd')
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 526, in runfile
      post_mortem, current_namespace, stack_depth=1)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 613, in _exec_file
      capture_last_expression=False)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 469, in exec_code
      exec_fun(compile(ast_code, filename, 'exec'), ns_globals, ns_locals)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\py3compat.py"", line 356, in compat_exec
      exec(code, globals, locals)
    File ""d:\codes\pycoode\ocnn2nd\fanoutmodel_train_ocnn.py"", line 97, in <module>
      trn_pred = model(X_trn) # input[BATCH_SIZE,TARGET_ROWS*TARGET_COLS]], output[BATCH_SIZE,1]
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\engine\training.py"", line 490, in __call__
      return super().__call__(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\engine\base_layer.py"", line 1014, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
Node: 'gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul'
Detected at node 'gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul' defined at (most recent call last):
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\runpy.py"", line 193, in _run_module_as_main
      ""__main__"", mod_spec)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\runpy.py"", line 85, in _run_code
      exec(code, run_globals)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\console\__main__.py"", line 24, in <module>
      start.main()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\console\start.py"", line 340, in main
      kernel.start()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelapp.py"", line 712, in start
      self.io_loop.start()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tornado\platform\asyncio.py"", line 215, in start
      self.asyncio_loop.run_forever()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\asyncio\base_events.py"", line 541, in run_forever
      self._run_once()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\asyncio\base_events.py"", line 1786, in _run_once
      handle._run()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\asyncio\events.py"", line 88, in _run
      self._context.run(self._callback, *self._args)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 510, in dispatch_queue
      await self.process_one()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 499, in process_one
      await dispatch(*args)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 406, in dispatch_shell
      await result
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 730, in execute_request
      reply_content = await reply_content
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\ipkernel.py"", line 387, in do_execute
      cell_id=cell_id,
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\zmqshell.py"", line 528, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 2975, in run_cell
      raw_cell, store_history, silent, shell_futures, cell_id
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3029, in _run_cell
      return runner(coro)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\async_helpers.py"", line 78, in _pseudo_sync_runner
      coro.send(None)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3257, in run_cell_async
      interactivity=interactivity, compiler=compiler, result=result)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3472, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3552, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""C:\Users\xxx\AppData\Local\Temp\ipykernel_29532\1338903705.py"", line 1, in <module>
      runfile('D:/CODES/pycoode/OCNN2nd/fanoutmodel_train_ocnn.py', wdir='D:/CODES/pycoode/OCNN2nd')
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 526, in runfile
      post_mortem, current_namespace, stack_depth=1)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 613, in _exec_file
      capture_last_expression=False)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 469, in exec_code
      exec_fun(compile(ast_code, filename, 'exec'), ns_globals, ns_locals)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\py3compat.py"", line 356, in compat_exec
      exec(code, globals, locals)
    File ""d:\codes\pycoode\ocnn2nd\fanoutmodel_train_ocnn.py"", line 97, in <module>
      trn_pred = model(X_trn) # input[BATCH_SIZE,TARGET_ROWS*TARGET_COLS]], output[BATCH_SIZE,1]
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\engine\training.py"", line 490, in __call__
      return super().__call__(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\engine\base_layer.py"", line 1014, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
Node: 'gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul'
2 root error(s) found.
  (0) UNIMPLEMENTED:  A deterministic GPU implementation of SparseTensorDenseMatmulOp is not currently available.
	 [[{{node gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul}}]]
	 [[gradients/fan_weight_8/StatefulPartitionedCall_grad/PartitionedCall/gradients/ExtractImagePatches_grad/ExtractImagePatches/_63]]
  (1) UNIMPLEMENTED:  A deterministic GPU implementation of SparseTensorDenseMatmulOp is not currently available.
	 [[{{node gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference___backward_call_20302_28692]
```
"
62066,"ã€in C++ã€‘No OpKernel was registered to support Op 'PyFunc' used by {{node PyFunc}}with these attrs: [Tin=[], Tout=[DT_FLOAT], token=""pyfunc_0""] Registered devices: [CPU, XLA_CPU] Registered kernels:   <no registered kernels>           [[PyFunc]]","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 1.15.0

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

15001379103

### Python version

_No response_

### Bazel version

4.2.1

### GCC/compiler version

4.8.5

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

method: C API
version: 1.15.0
package link:https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.15.0.tar.gz





### Standalone code to reproduce the issue

```shell
#include ""app/federal/model.hpp""
#include <iostream>
#include <fstream>

namespace xcxxx {
    /**
     * Print Tensorflow version
    */
  void Model::PrintTfVersion() {
    std::cout << ""Hello from TensorFlow C library version \n"" <<  TF_Version() << std::endl;
  }

  void NoOpDeallocator(void* data, size_t a, void* b) {
  }

  void DeallocateBuffer(void* data, size_t) {
    std::free(data);
  }

  TF_Buffer* ReadBufferFromFile(std::string file) {
    std::ifstream f(file, std::ios::binary);
    if (f.fail() || !f.is_open()) {
      return nullptr;
    }

    f.seekg(0, std::ios::end);
    const auto fsize = f.tellg();
    f.seekg(0, std::ios::beg);

    if (fsize < 1) {
      f.close();
      return nullptr;
    }

    char* data = static_cast<char*>(std::malloc(fsize));
    f.read(data, fsize);
    f.close();

    TF_Buffer* buf = TF_NewBuffer();
    buf->data = data;
    buf->length = fsize;
    buf->data_deallocator = DeallocateBuffer;
    return buf;
  }
    /**
     * Init Model Instance
    */
   bool Model::Init(std::string model_file) {
    try {

        // create new graph
        TF_Buffer* buffer = ReadBufferFromFile(model_file);
        if (buffer == nullptr) {
          std::cout << ""code:10000"" <<  "", msg: "" <<  ""Error creating the session from  the given model path!""<< std::endl;
          return false;
        }
        TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();
        TF_Status* status = TF_NewStatus();

        m_graph_ = TF_NewGraph();
        TF_GraphImportGraphDef(m_graph_, buffer, opts, status);
        TF_DeleteImportGraphDefOptions(opts);
        TF_DeleteBuffer(buffer);
        if (TF_GetCode(status) != TF_OK) {
            std::cout << ""code:10001"" <<  "", msg: "" << TF_Message(status) << std::endl;
            TF_DeleteGraph(m_graph_);
            m_graph_ = nullptr;
            return false;
        }
        TF_DeleteStatus(status);

        // create new session
        status = TF_NewStatus();
        TF_SessionOptions* options = TF_NewSessionOptions();
        m_session_ = TF_NewSession(m_graph_, options, status);
        TF_DeleteSessionOptions(options);
        if (TF_GetCode(status) != TF_OK) {
            std::cout << ""code:10002"" <<  "", msg: "" << TF_Message(status) << std::endl;
            return false;
        }
        TF_DeleteStatus(status);
        std::cout << ""code:0"" <<  "", msg: load model suc!^_^"" << std::endl;
    
    } catch (std::exception& e) {
        std::cout << ""code:10003"" <<  "", msg: model init.Tf load fail!"" << std::endl;
        return false;
    }

    return true;
   }


  /**
     * Init Model Instance
    */
   bool Model::InitV2(std::string model_file) {
    try {
      TF_Buffer* buffer = ReadBufferFromFile(model_file);
      if (buffer == nullptr) {
        std::cout << ""code:10000"" <<  "", msg: model init.Tf load fail!"" << std::endl;
      }
      TF_Status* status = TF_NewStatus();
      TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();

      m_graph_ = TF_NewGraph();
      TF_GraphImportGraphDef(m_graph_, buffer, opts, status);
      TF_DeleteImportGraphDefOptions(opts);
      TF_DeleteBuffer(buffer);
      if (TF_GetCode(status) != TF_OK) {
        TF_DeleteGraph(m_graph_);
        m_graph_ = nullptr;
      }
      TF_DeleteStatus(status);

      // create session from graph
      status = TF_NewStatus();
      TF_SessionOptions* options = TF_NewSessionOptions();
      m_session_ = TF_NewSession(m_graph_, options, status);
      TF_DeleteSessionOptions(options);
    } catch (std::exception& e) {
        std::cout << ""code:10003"" <<  "", msg: model init.Tf load fail!"" << std::endl;
        return false;
    }

    return true;
   }

   /**
    *  infer
   */
  std::vector<float> Model::Infer(std::vector<float> input_data) {
    std::vector<float> ret;

    //****** 1ã€Define graph inputs/outputs
    // std::string train_input_name = ""top_model/inputs:0"";
    std::string train_input_name = ""top_model/inputs"";
    // std::string train_output_name = ""top_model/outputs:0"";
    std::string train_output_name = ""top_model/outputs"";

    // Define graph inputs
    uint16_t gr_input_nums = 1;
    TF_Output* inputs = (TF_Output*)malloc(sizeof(TF_Output) * gr_input_nums);
    TF_Output input0 = {TF_GraphOperationByName(m_graph_, train_input_name.c_str()), 0};


    size_t pos = 0;
    uint16_t i = 0;
    TF_Operation* oper;
    
    while ((oper = TF_GraphNextOperation(m_graph_, &pos)) != nullptr) {
      std::cout << ""index:"" << i << "", graphname:"" << TF_OperationName(oper) << std::endl;
      i++;
    }

    if (nullptr == input0.oper) {
      std::cout << ""code:20000"" <<  "", msg: Define graph inputs failure"" << std::endl;
      return ret;
    }
    inputs[0] = input0;

    // Define graph outpus
    uint16_t gr_output_nums = 1;
    TF_Output* outputs = (TF_Output*)malloc(sizeof(TF_Output) * gr_output_nums);
    TF_Output output0 = {TF_GraphOperationByName(m_graph_, train_output_name.c_str()), 0};
    if (nullptr == output0.oper) {
      std::cout << ""code:20001"" <<  "", msg: Define graph outputs failure"" << std::endl;
      return ret;
    }
    outputs[0] = output0;


    //****** 2ã€Create input tensor(s) and populate with feature
    TF_Tensor** input_values = (TF_Tensor**)malloc(sizeof(TF_Tensor*) * gr_input_nums);
    int16_t num_dims = 1;
    int64_t dims[] = {1};
    float_t* data = &input_data[0];
    TF_Tensor* float_tensor = TF_NewTensor(TF_FLOAT, dims, num_dims, data, sizeof(TF_FLOAT), &NoOpDeallocator, 0);
    if (nullptr == float_tensor) {
      std::cout << ""code:20002"" <<  "", msg: allocate failure"" << std::endl;
      return ret;
    }
    input_values[0] = float_tensor;

    TF_Tensor** output_values = (TF_Tensor**)malloc(sizeof(TF_Tensor*) * gr_output_nums);

    //****** 3ã€Run the session
    TF_Status* status = TF_NewStatus();
    TF_SessionRun(
      m_session_, 
      nullptr, 
      // Input tensors
      inputs, input_values, gr_input_nums,
      // Output tensors
      outputs, output_values, gr_output_nums,
      // Target operations
      nullptr, 0,
      // RunMedata
      nullptr,
      // Output status
      status
    );

    if (TF_OK != TF_GetCode(status)) {
      std::cout << ""code:20003"" <<  "", msg: infer failure!\nstatus:"" << TF_Message(status) << std::endl;
      return ret;
    }
    std::cout << ""code:0"" <<  "", msg: infer suc!^_^"" << std::endl;

    //****** 4ã€Output the result
    void* buff = TF_TensorData(output_values[0]);
    float* offsets = (float*)buff;
    std::cout << ""infer result:"" << offsets[0] << std::endl;


    TF_DeleteStatus(status);
    return ret;
  }

}
```


### Relevant log output

```shell
No OpKernel was registered to support Op 'PyFunc' used by {{node PyFunc}}with these attrs: [Tin=[], Tout=[DT_FLOAT], token=""pyfunc_0""]
Registered devices: [CPU, XLA_CPU]
Registered kernels:
  <no registered kernels>

         [[PyFunc]]
```
"
62065,MirroredStrategy distribution strategy gather hangs,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.14.0-rc1-21-g4dacf3f368e 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Python version

3.10

### CUDA/cuDNN version

11.8.89/8.6.0

### Current behavior?

The program (see referenced gist below) gets stuck at a distribution strategy `gather` method, while it should not. I'm using the `tf.distribute.MirroredStrategy`; when choosing `cross_device_ops=tf.distribute.ReductionToOneDevice()`, the issue does not occur. My expectation is that gather should not get stuck with the default `cross_device_ops=None`.

Note 0): When the program hangs, it can't be terminated with one or more ctrl+c interrupts, a SIGKILL does work (i.e. using `pkill python`).

Note 1): As can be observed when running the script, the `strategy.reduce` method does not get stuck, only the `gather` method. Further the first `gather` call, gathering the target labels that were placed on the different devices, does work. Only the second gather call, gathering the predicted labels, fails (hangs).

Note 2): I tried to reproduce the issue with `tf-nighly`, but unfortunately it refused to use (see) the GPUs.

### Standalone code to reproduce the issue

See this [gist to reproduce the issue](https://gist.github.com/visionscaper/85094b3505cbfeedd90167fe02c1d1d0), the program gets stuck at line 99, with `cross_device_ops=None` for the  `tf.distribute.MirroredStrategy` (see line 124).


### Relevant log output

```
$ python experiments/tensorflow/test_gather_deadlock.py
2023-10-08 10:19:39.457765: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-08 10:19:39.457812: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-08 10:19:39.457838: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-08 10:19:39.464945: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-08 10:19:41.429407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.429727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.430003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.430277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.434779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.435083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.435353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.435641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.435907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.436169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.436431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.436692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.773825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.774140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.774413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.774681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.774947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.775203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.775458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.775714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.775969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.776225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.776480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.776734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.797013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.797372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.797659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.797941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.798213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.798476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.798734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.798991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.799258: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.799509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9
2023-10-08 10:19:41.799886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.800135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22288 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:81:00.0, compute capability: 8.9
2023-10-08 10:19:41.800399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.800648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22288 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:82:00.0, compute capability: 8.9
2023-10-08 10:19:41.800909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-08 10:19:41.801164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22288 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:c1:00.0, compute capability: 8.9
loss_sum.values[-1].device           : /job:localhost/replica:0/task:0/device:GPU:0
loss_sum.values[-1].backing_device   : /job:localhost/replica:0/task:0/device:GPU:3
predictions.values[-1].device        : /job:localhost/replica:0/task:0/device:GPU:0
predictions.values[-1].backing_device: /job:localhost/replica:0/task:0/device:GPU:3
Gather labels from all devices ...
Gather predictions from all devices ...
```

When setting `cross_device_ops=tf.distribute.ReductionToOneDevice()` the program does not hang and continues to output the following:
```
loss    : 2.4461679458618164
accuracy: 0.1640625
```"
62064,Question about ptxas warning output of tensorflow-gpu 2.11,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.11

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The running environment is cuda 11.7 cuDNN 8.5.0 python 3.10 tensorflow-gpu 2.11
When I run the code, it show the warning message which seems weird. Since my cuda version is 11.7, not older than 11.1 and the log says Loaded cuDNN version 8904 while my cuDNN version is 8500. Althought the code can run successfully, i have doubts about this log, and I hope I can figure out why. Thanks in advance.

The warning log is as follows
 I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8904
2023-10-07 11:43:16.647615: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2023-10-07 11:43:16.651012: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6
2023-10-07 11:43:16.651030: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:237] Used ptxas at ptxas
2023-10-07 11:43:16.651094: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.

### Standalone code to reproduce the issue

```shell
any code that use tensorflow-gpu
concretely, the code i run is https://github.com/acctouhou/Prediction_of_battery/blob/main/1_Predicting/predict.py
```


### Relevant log output

_No response_"
62063,model_main_tf2.py issues with training object detection model,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.8

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The tf_slim module class tfexample_decode.py at line 453 calls control_flow_ops.case 
There is no ""case"" attribute to control_flow_ops

### Standalone code to reproduce the issue

```shell
Google Colab, SSD Moblenet v2 graph, tfrecords
```


### Relevant log output

```shell
I1007 00:12:20.542787 134709293735936 dataset_builder.py:162] Reading unweighted datasets: ['.../train.record']
INFO:tensorflow:Reading record datasets for input file: ['.../train/train.record']
I1007 00:12:20.543272 134709293735936 dataset_builder.py:79] Reading record datasets for input file: ['.../train.record']
INFO:tensorflow:Number of filenames to read: 1
I1007 00:12:20.543381 134709293735936 dataset_builder.py:80] Number of filenames to read: 1
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W1007 00:12:20.543440 134709293735936 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /content/models/research/./object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.
W1007 00:12:20.550345 134709293735936 deprecation.py:50] From /content/models/research/./object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.
WARNING:tensorflow:From /content/models/research/./object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
W1007 00:12:20.572327 134709293735936 deprecation.py:50] From /content/models/research/./object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
Traceback (most recent call last):
  File ""/content/models/research/object_detection/model_main_tf2.py"", line 116, in <module>
    tf.compat.v1.app.run()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/platform/app.py"", line 36, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.10/dist-packages/absl/app.py"", line 308, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.10/dist-packages/absl/app.py"", line 254, in _run_main
    sys.exit(main(argv))
  File ""/content/models/research/object_detection/model_main_tf2.py"", line 107, in main
    model_lib_v2.train_loop(
  File ""/content/models/research/./object_detection/model_lib_v2.py"", line 563, in train_loop
    train_input = strategy.experimental_distribute_datasets_from_function(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py"", line 383, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 1563, in experimental_distribute_datasets_from_function
    return self.distribute_datasets_from_function(dataset_fn, options)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 1554, in distribute_datasets_from_function
    return self._extended._distribute_datasets_from_function(  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 613, in _distribute_datasets_from_function
    return input_util.get_distributed_datasets_from_function(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_util.py"", line 144, in get_distributed_datasets_from_function
    return input_lib.DistributedDatasetsFromFunction(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1143, in __init__
    self.build()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1165, in build
    _create_datasets_from_function_with_input_context(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1680, in _create_datasets_from_function_with_input_context
    dataset = dataset_fn(ctx)
  File ""/content/models/research/./object_detection/model_lib_v2.py"", line 554, in train_dataset_fn
    train_input = inputs.train_input(
  File ""/content/models/research/./object_detection/inputs.py"", line 908, in train_input
    dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
  File ""/content/models/research/./object_detection/builders/dataset_builder.py"", line 250, in build
    dataset = dataset_map_fn(dataset, decoder.decode, batch_size,
  File ""/content/models/research/./object_detection/builders/dataset_builder.py"", line 235, in dataset_map_fn
    dataset = dataset.map_with_legacy_function(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py"", line 383, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 4128, in map_with_legacy_function
    return map_op._map_v1_with_legacy_function(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py"", line 85, in _map_v1_with_legacy_function
    _ParallelMapDataset(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py"", line 148, in __init__
    self._map_func = structured_function.StructuredFunctionWrapper(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py"", line 272, in __init__
    self._function.add_to_graph(ops.get_default_graph())
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/function.py"", line 579, in add_to_graph
    self._create_definition_if_needed()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/function.py"", line 412, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/function.py"", line 430, in _create_definition_if_needed_impl
    temp_graph = func_graph_from_py_func(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/function.py"", line 1007, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py"", line 178, in wrapped_fn
    ret = wrapper_helper(*args)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py"", line 161, in wrapper_helper
    ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 693, in wrapper
    raise e.ag_error_metadata.to_exception(e)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 690, in wrapper
    return converted_call(f, args, kwargs, options=options)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 439, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/__autograph_generated_fileqkyio7wq.py"", line 74, in tf__decode
    tensors = ag__.converted_call(ag__.ld(decoder).decode, (ag__.ld(serialized_example),), dict(items=ag__.ld(keys)), fscope)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 439, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/__autograph_generated_file1opxbxnu.py"", line 81, in tf__decode
    ag__.for_stmt(ag__.ld(items), None, loop_body_1, get_state_3, set_state_3, (), {'iterate_names': 'item'})
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 449, in for_stmt
    for_fn(iter_, extra_test, body, get_state, set_state, symbol_names, opts)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 500, in _py_for_stmt
    body(target)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 466, in protected_body
    original_body(protected_iter)
  File ""/tmp/__autograph_generated_file1opxbxnu.py"", line 77, in loop_body_1
    ag__.converted_call(ag__.ld(outputs).append, (ag__.converted_call(ag__.ld(handler).tensors_to_item, (ag__.ld(keys_to_tensors),), None, fscope),), None, fscope)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 441, in converted_call
    result = converted_f(*effective_args)
  File ""/tmp/__autograph_generated_fileok5ldq7m.py"", line 39, in tf__tensors_to_item
    ag__.if_stmt(ag__.ld(self)._repeated, if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1217, in if_stmt
    _py_if_stmt(cond, body, orelse)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1270, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/__autograph_generated_fileok5ldq7m.py"", line 35, in else_body
    retval_ = ag__.converted_call(ag__.ld(self)._decode, (ag__.ld(image_buffer), ag__.ld(image_format)), None, fscope)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 441, in converted_call
    result = converted_f(*effective_args)
  File ""/tmp/__autograph_generated_fileubbeey9m.py"", line 80, in tf___decode
    image = ag__.converted_call(ag__.ld(control_flow_ops).case, (ag__.ld(pred_fn_pairs),), dict(default=ag__.ld(check_jpeg), exclusive=True), fscope)
AttributeError: in user code:

    File ""/content/models/research/./object_detection/data_decoders/tf_example_decoder.py"", line 556, in decode  *
        tensors = decoder.decode(serialized_example, items=keys)
    File ""/usr/local/lib/python3.10/dist-packages/tf_slim/data/tfexample_decoder.py"", line 723, in decode  *
        outputs.append(handler.tensors_to_item(keys_to_tensors))
    File ""/usr/local/lib/python3.10/dist-packages/tf_slim/data/tfexample_decoder.py"", line 406, in tensors_to_item  *
        return self._decode(image_buffer, image_format)
    File ""/usr/local/lib/python3.10/dist-packages/tf_slim/data/tfexample_decoder.py"", line 454, in _decode  *
        image = control_flow_ops.case(

    AttributeError: module 'tensorflow.python.ops.control_flow_ops' has no attribute 'case'
```
"
62062,tensorflow-intel binary packaging,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13, 2.14

### Custom code

No

### OS platform and distribution

Windows x86_64

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tensorflow-intel` is packaged with the TF native libraries stored as pyd files which wrap in some of the Python code. Other versions of tf (e.g. macOS and Linux) package native libraries in the default formats (e.g. `so` or `dylib`). In SIG-JVM/TensorFlow-Java we're planning on redoing our native builds so that we pull in pre-built binaries from the TF python release as that will make our builds much simpler and less error prone, but we can't re-use the Windows builds as the native binary depends on Python symbols. I reached out to SIG-BUILD and they told me to open an issue here. @mraunak

### Standalone code to reproduce the issue

```shell
Download the whl and unzip it.
```


### Relevant log output

_No response_"
62061,Tensorflow 2.14 macOS wheel won't install for Python 3.11,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.14

### Custom code

No

### OS platform and distribution

arm64 macOS

### Mobile device

n/a

### Python version

3.11

### Bazel version

n/a

### GCC/compiler version

n/a

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

When installing tensorflow 2.14 for Python 3.11 I see:

```
ERROR: Could not find a version that satisfies the requirement wrapt<1.15,>=1.11.0 (from tensorflow) (from versions: 1.15.0rc1, 1.15.0)
```

Looking at the metadata of the [2.14 whl for py3.11](https://files.pythonhosted.org/packages/d3/4b/ae9037ea22ba94eb2cf267e991384c3444f3e6142fa49923352b4ab73e14/tensorflow_macos-2.14.0-cp311-cp311-macosx_12_0_arm64.whl) I can see: 

```
Requires-Dist: wrapt (<1.15,>=1.11.0)
```

but wrapt has no packages for Python 3.11 for that version range.

Looking at the metadata for the [2.13.1 whl for py3.11](https://files.pythonhosted.org/packages/c0/d1/d309dea6e67e1b8037f607872486eb67a1ff64fb91a96149086dbdc46ca4/tensorflow_macos-2.13.1-cp311-cp311-macosx_12_0_arm64.whl) I can see:

```
Requires-Dist: wrapt (>=1.11.0)
```

which can be satisfied with wrapt 1.15.0

### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_"
62060,Using Lambda layers to take different slices of a prevous layer's output causes earlier Lambda layers to be overwritten,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I used two Lambda layers to extract slices from the same input vector. The output of the first Lambda is somehow overwritten by the output of the second Lambda.

Note: I have confirmed this happens whether the Lambda layers take the model's input directly or the output of another layer. I have also confirmed that the problem is present whether or not the Lambda layers are the direct outputs of the model. But for the sample code below I've removed the extra layers.

### Standalone code to reproduce the issue

```shell
import sys

import tensorflow as tf

print(f""{tf.version.VERSION=} {tf.version.GIT_VERSION=} {tf.version.COMPILER_VERSION=}"")
print(f""{sys.version=}"")

dividers = [0, 2, 5]

assert all(divider >= 0 for divider in dividers)
sizes = [end - start for start, end in zip(dividers[:-1], dividers[1:])]
assert all(size > 0 for size in sizes)
channels = dividers[-1]

i = tf.keras.layers.Input((channels,), name='i')
o = [
    tf.keras.layers.Lambda(lambda x: x[..., start:end],
                           name=f'slice_{start}_{end}')(i)
    for start, end in zip(dividers[:-1], dividers[1:])
]
m = tf.keras.Model(i, o, name='m')
m.build((channels,))
m.summary()
print(f""{m.input_shape=}"")
print(f""{m.output_shape=}"")
print(f""{m.compute_output_shape(m.input_shape)=}"")
x = tf.zeros((1, channels))
print(f""{[y.shape for y in m(x)]=}"")
print(f""{[y.shape for y in m.predict(x)]=}"")
assert m.output_shape == m.compute_output_shape(m.input_shape)
```


### Relevant log output

```shell
tf.version.VERSION='2.10.0' tf.version.GIT_VERSION='v2.10.0-rc3-6-g359c3cdfc5f' tf.version.COMPILER_VERSION='9.3.1 20200408'
sys.version='3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]'
Model: ""m""
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 i (InputLayer)                 [(None, 5)]          0           []                               
                                                                                                  
 slice_0_2 (Lambda)             (None, 2)            0           ['i[0][0]']                      
                                                                                                  
 slice_2_5 (Lambda)             (None, 3)            0           ['i[0][0]']                      
                                                                                                  
==================================================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
__________________________________________________________________________________________________
m.input_shape=(None, 5)
m.output_shape=[(None, 2), (None, 3)]
m.compute_output_shape(m.input_shape)=[TensorShape([None, 3]), TensorShape([None, 3])]
[y.shape for y in m(x)]=[TensorShape([1, 3]), TensorShape([1, 3])]
1/1 [==============================] - 0s 286ms/step
[y.shape for y in m.predict(x)]=[(1, 3), (1, 3)]
Traceback (most recent call last):
  File ""/home/hosford42/PycharmProjects/LSLAM/tf_bug.py"", line 30, in <module>
    assert m.output_shape == m.compute_output_shape(m.input_shape)
AssertionError
```
"
62058,TFLite Python: interpreter._get_tensor_details(index) can't read tensor with sparsity (Segmentation Fault),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.14.0-rc1-21-g4dacf3f368e 2.14.0

### Custom code

No

### OS platform and distribution

Ubuntu 20 / Windows 11

### Mobile device

_No response_

### Python version

3.8, 3.10, 3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Seems to be the same problem as https://github.com/tensorflow/tensorflow/issues/55040. 

If I try to create an interpreter for the latest media pipe pose detector taken from the official mediapipe website it segfaults. 

1. download latest `pose_landmarker.task` (any lite/full/heavy) from [here](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker#models)
2. rename `pose_landmarker.task` to `pose_landmarker.zip`
3. extract `pose_detector.tflite`
4. run the attached example

doing some basic investigation it segfaults on `self._interpreter.TensorSparsityParameters(tensor_index, subgraph_index)` [here](https://github.com/tensorflow/tensorflow/blob/63bc8d2fd33f24699cc957e7b4e16996568736a2/tensorflow/lite/python/interpreter.py#L621C48-L621C72) same as the linked issue.

I am using conda, for package management.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

interpreter = tf.lite.Interpreter(""pose_detector.tflite"")
details = interpreter._get_tensor_details(15, subgraph_index=0)
print(details)
```


### Relevant log output

```shell
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
Segmentation Fault
```
"
62055,core dumped Error with tf.raw_ops.QuantizeAndDequantizeV4,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v1.12.1-100714-gd8e55c05473 2.15.0-dev20231005

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current behavior?

Description:

While using the tf.raw_ops.QuantizeAndDequantizeV4 operation, I encountered a core dumped error with specific input parameters.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf 

args = {
    'axis': -1, 
    'input': tf.random.uniform(shape=[]),
    'input_max': tf.random.uniform(shape=[1,8,5]),
    'input_min': tf.random.uniform(shape=[0,5]),
    'name': 'not defined', 
    'narrow_range': True, 
    'num_bits': 2, 
    'range_given': True,
    'round_mode': 'HALF_TO_EVEN',
    'signed_input': True
}

res = tf.raw_ops.QuantizeAndDequantizeV4(**args)
print(res)
```


### Relevant log output

```shell
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 23:52:34.898354: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 23:52:38.109515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1924] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6826 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:02:00.0, compute capability: 7.5
2023-10-05 23:52:38.110141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1924] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6826 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:04:00.0, compute capability: 7.5
2023-10-05 23:52:38.110688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1924] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6826 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:83:00.0, compute capability: 7.5
2023-10-05 23:52:38.111251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1924] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 4528 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:84:00.0, compute capability: 7.5
2023-10-05 23:52:38.268849: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 0)Must have a one element tensor
Aborted (core dumped)
```
"
62054,Issue Installing TensorFlow on Windows 11 with Python 3.12.0,"Hello,

I am encountering an issue while trying to install TensorFlow on my Windows 11 machine with Python 3.12.0 and pip 23.2.1 (64-bit). Despite several attempts, I keep receiving the following error messages:

```
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```

I have tried creating virtual environments using both Python 3.7.0 and Python 3.6.4
cleared the pip cache using the command `pip cache purge`
Install TensorFlow with various specific versions
`tensorflow==2.1.0`
`tensorflow==2.5.0`
`tensorflow==2.2.0rc4`

I hoping anyone know what the issue here"
62053,Custom Gradient Computation not working in TF 2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am using W&B's Keras callback `WandbCallback`. This callback has a feature to log gradients of each layer at every step. This feature works fine till TF 2.13.0 but is erroring out in TF 2.14.0.

This piece of code works fine in Tf 2.13.0 but errors out in TF 2.14.0:

```
import numpy as np
import tensorflow as tf
print(tf.__version__)
import wandb
from wandb.keras import WandbModelCheckpoint
from wandb.keras import WandbCallback

run = wandb.init(project=""keras"")

x = np.random.randint(255, size=(100, 28, 28, 1))
y = np.random.randint(10, size=(100,))

dataset = (x, y)


def get_model():
    m = tf.keras.Sequential()
    m.add(tf.keras.layers.Conv2D(3, 3, activation=""relu"", input_shape=(28, 28, 1)))
    m.add(tf.keras.layers.Flatten())
    m.add(tf.keras.layers.Dense(10, activation=""softmax""))
    return m


model = get_model()
model.compile(
    loss=""sparse_categorical_crossentropy"",
    optimizer=""sgd"",
    metrics=[""accuracy""],
)

model.fit(
    x,
    y,
    epochs=5,
    validation_data=(x, y),
    callbacks=[
        WandbCallback(
            save_model=False,
            log_gradients=True,
            training_data=(x,y)
        )
    ],
)

```

I investigated further and was able to narrow it down to the gradient logging logic which again works fine for 2.13.0 but not for 2.14.0. 

I think this has to do with the breaking changes with `tf.Tensor`.

The piece of code below is the gradient logging logic which errors out in the latest version.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.__version__)
import wandb
import numpy as np

_training_data_x = np.random.randint(255, size=(100, 28, 28, 1))
_training_data_y = np.random.randint(10, size=(100,))


def get_model():
    m = tf.keras.Sequential()
    m.add(tf.keras.layers.Conv2D(3, 3, activation=""relu"", input_shape=(28, 28, 1)))
    m.add(tf.keras.layers.Flatten())
    m.add(tf.keras.layers.Dense(10, activation=""softmax""))
    return m

model = get_model()
model.compile(
    loss=""sparse_categorical_crossentropy"",
    optimizer=""sgd"",
    metrics=[""accuracy""],
)


def _get_custom_optimizer_parent_class():
    from pkg_resources import parse_version

    if parse_version(tf.__version__) >= parse_version(""2.9.0""):
        custom_optimizer_parent_class = tf.keras.optimizers.legacy.Optimizer
    else:
        custom_optimizer_parent_class = tf.keras.optimizers.Optimizer

    return custom_optimizer_parent_class


_custom_optimizer_parent_class = _get_custom_optimizer_parent_class()
print(_custom_optimizer_parent_class)


class _CustomOptimizer(_custom_optimizer_parent_class):
    def __init__(self):
        super().__init__(name=""CustomOptimizer"")
        self._resource_apply_dense = tf.function(self._resource_apply_dense)
        self._resource_apply_sparse = tf.function(self._resource_apply_sparse)
        tf.print(self._resource_apply_dense)

    def _resource_apply_dense(self, grad, var):
        var.assign(grad)

    # this needs to be implemented to prevent a NotImplementedError when
    # using Lookup layers.
    def _resource_apply_sparse(self, grad, var, indices):
        pass

    def get_config(self):
        return super().get_config()


class _GradAccumulatorCallback(tf.keras.callbacks.Callback):
    """"""Accumulates gradients during a fit() call when used in conjunction with the CustomOptimizer above.""""""

    def set_model(self, model):
        super().set_model(model)
        self.og_weights = model.get_weights()
        self.grads = [np.zeros(tuple(w.shape)) for w in model.trainable_weights]

    def on_batch_end(self, batch, logs=None):
        for g, w in zip(self.grads, self.model.trainable_weights):
            g += w.numpy()
        self.model.set_weights(self.og_weights)

    def get_grads(self):
        return [g.copy() for g in self.grads]


inputs = model.inputs
print(inputs)
outputs = model(inputs)
grad_acc_model = tf.keras.models.Model(inputs, outputs)
grad_acc_model.compile(loss=model.loss, optimizer=_CustomOptimizer())

_grad_accumulator_model = grad_acc_model
_grad_accumulator_model.summary()

_grad_accumulator_callback = _GradAccumulatorCallback()


_grad_accumulator_model.fit(
    _training_data_x,
    _training_data_y,
    verbose=0,
    callbacks=[_grad_accumulator_callback],
)

weights = model.trainable_weights
grads = _grad_accumulator_callback.grads
print(weights)

metrics = {}
for weight, grad in zip(weights, grads):
    metrics[
        ""gradients/"" + weight.name.split("":"")[0] + "".gradient""
    ] = wandb.Histogram(grad)

print(metrics)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/ayushthakur/client/wandb/test_grad_logging.py"", line 88, in <module>
    _grad_accumulator_model.fit(
  File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/tmp/__autograph_generated_file4zq8l42d.py"", line 15, in tf__train_function
    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
TypeError: in user code:

    File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function  *
        return step_function(self, iterator)
    File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step  **
        outputs = model.train_step(data)
    File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1130, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py"", line 601, in minimize
        return self.apply_gradients(grads_and_vars, name=name)
    File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py"", line 760, in apply_gradients
        return tf.__internal__.distribute.interim.maybe_merge_call(
    File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py"", line 844, in _distributed_apply
        with tf.control_dependencies([tf.group(update_ops)]):

    TypeError: 'inputs' should be zero or more (nested) Tensors. Received 'None' with type '<class 'NoneType'>'.
```
"
62049,Compatibility with Python 3.12.0,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tensorflow 2.14.0

### Custom code

No

### OS platform and distribution

Mac OS

### Mobile device

_No response_

### Python version

3.12.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Python 3.12.0 is now available. I tried to pip install TensorFlow 2.14.0. The attempt failed as TF is only compatible with Python 3.11 at most at the moment.

When will a version of TensorFlow be available for the latest Python?  

Thanks.

### Standalone code to reproduce the issue

```shell
Terminal output:

(venv) ## my user ## % pip install tensorflow==2.14.0
ERROR: Could not find a version that satisfies the requirement tensorflow==2.14.0 (from versions: none)
```


### Relevant log output

_No response_"
62048,ValueError in tensorflow-probability,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tensorflow 2.15.0

### Custom code

Yes

### OS platform and distribution

Windows Subsystem for Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

v1.18.0

### GCC/compiler version

11.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to run a program which uses tensorflow agents & tensorflow probability at the back end. When I try to run the train.py using .yaml input file, I am getting the following error:

### Standalone code to reproduce the issue

```shell
lib/python3.10/site-packages/tensorflow_probability/python/internal/prefer_static.py"", line 84, in _copy_docstring raise ValueError(

ValueError: Arg specs do not match: original=FullArgSpec(args=['input', 'dtype', 'name', 'layout'], varargs=None, varkw=None, defaults=(None, None, None), kwonlyargs=[], kwonlydefaults=None, annotations={}), new=FullArgSpec(args=['input', 'dtype', 'name'], varargs=None, varkw=None, defaults=(None, None), kwonlyargs=[], kwonlydefaults=None, annotations={}), fn=<function ones_like_v2 at 0x7f8287141480> Please help me understand the issue & any suggestions to resolve the error is greatly appreciated.
```


### Relevant log output

_No response_"
62047,avx512fp16 causing invalid static_cast with cuda12.2 python 3.12,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux SuSE 15 SP4

### Mobile device

_No response_

### Python version

3.12.0

### Bazel version

3.6.0

### GCC/compiler version

gcc 12.3.0

### CUDA/cuDNN version

cuda 12.2.2 cudnn 8.9.5

### GPU model and memory

H100

### Current behavior?

I am aware this is an unsupported, non-working configuration - reporting early in case it's a real bug requiring fixing.
CPU is Sapphire Rapids; Compiling with --copt=-mavx512fp16
results in the following build error:

tensorflow/core/kernels/linalg/matrix_inverse_op.cc:113:31:   required from here
external/eigen_archive/Eigen/src/Core/MathFunctions.h:429:12: error: invalid â€˜static_castâ€™ from type â€˜const Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1>â€™ to type â€˜__vector(16) floatâ€™
  429 |     return static_cast<NewType>(x);
  
 Removing --copt=-mavx512fp16 allows the compile to finish. The following options resulted in a successfully build of a tensorflow whl file (although unusable until python 3.12 compatibility is available).
 --config=opt -c opt --copt=-mfpmath=sse --copt=-msse4.2 --copt=-mavx --copt=-mavx2
--copt=-mfma --copt=-mavx512f --copt=-mavx512vnni --copt=-mavx512f --copt=-mavx512bf16 --copt=-mavx512vl  


### Standalone code to reproduce the issue

```shell
TF_PYTHON_VERSION=3.11 CFLAGS=""-O3 -march=native -fPIC"" CXXFLAGS=$CFLAGS LIBRARY_PATH=$LD_RUN_PATH LD_LIBRARY_PATH=$LD_RUN_PATH \
LDFLAGS=""-fPIC  -Wl,--disable-new-dtags -Wl,--rpath -Wl,${LD_RUN_PATH}"" bazel build -j 24 --config=opt -c opt --copt=-mavx --copt=-mavx2 \
--copt=-mfma --copt=-mfpmath=sse --copt=-msse4.2 \
--copt=-mavx512f --copt=-mavx512vnni --copt=-mavx512f --copt=-mavx512fp16 --copt=-mavx512bf16 --copt=-mavx512vl \
--config=cuda --config=mkl --config=tensorrt  //tensorflow/tools/pip_package:build_pip_package --repo_env=TF_PYTHON_VERSION=3.11
```


### Relevant log output

```shell
external/eigen_archive/Eigen/src/Core/Matrix.h:227:24:   required from â€˜Eigen::Matrix<Scalar_, Rows_, Cols_, Options_, MaxRows_, MaxCols_>& Eigen::Matrix<Scalar_, Rows_, Cols_, Options_, MaxRows_, MaxCols_>::operator=(const Eigen::DenseBase<OtherDerived>&) [with OtherDerived = Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Scalar_ = float; int Rows_ = -1; int Cols_ = -1; int Options_ = 1; int MaxRows_ = -1; int MaxCols_ = -1]â€™
external/eigen_archive/Eigen/src/LU/PartialPivLU.h:135:12:   required from â€˜Eigen::PartialPivLU<MatrixType, PermutationIndex>& Eigen::PartialPivLU<MatrixType, PermutationIndex>::compute(const Eigen::EigenBase<OtherDerived>&) [with InputType = Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; MatrixType_ = Eigen::Matrix<float, -1, -1, 1, -1, -1>; PermutationIndex_ = int]â€™
tensorflow/core/kernels/linalg/matrix_inverse_op.cc:113:31:   required from here
external/eigen_archive/Eigen/src/Core/MathFunctions.h:429:12: error: invalid â€˜static_castâ€™ from type â€˜const Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1>â€™ to type â€˜__vector(16) floatâ€™
  429 |     return static_cast<NewType>(x);
      |            ^~~~~~~~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
```
"
62046,Building TF 2.14 using bazel with later NDK versions failing,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.1.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Building without CUDA support

### GPU model and memory

_No response_

### Current behavior?

In order to build Tensorflow Delegate Performance Benchmark tool, I am building latest TF (2.14) from source (using ./configure) using bazel 6.1.0 and Android NDK version 25.0.8775105. But this fails

WARNING: The NDK version in /home/<username>/AndroidSDK/ndk/25.0.8775105 is 25, which is not supported by Bazel (officially supported versions: [19, 20, 21]). Please use another version. Compiling Android targets may result in confusing errors.

Traceback (most recent call last):
  File ""./configure.py"", line 1466, in <module>
    main()
  File ""./configure.py"", line 1439, in main
    create_android_ndk_rule(environ_cp)
  File ""./configure.py"", line 658, in create_android_ndk_rule
    get_ndk_api_level(environ_cp, android_ndk_home_path))
  File ""./configure.py"", line 752, in get_ndk_api_level
    api_levels = sorted(os.listdir(platforms))
FileNotFoundError: [Errno 2] No such file or directory: '/home/<username>/AndroidSDK/ndk/25.0.8775105/platforms'

As far as I understand later NDK versions don't have platforms/

As a user I should be able to build tensorflow using later NDK versions. Could someone have a look into this please.
 

### Standalone code to reproduce the issue

```shell
Steps followed:
1. Clone TF : git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git
2. Clone bazel: https://bazel.build/install/ubuntu
3. Install AndroidSDK: sudo snap install androidsdk
4. Install Android NDK : androidsdk --install ""ndk;25.0.8775105""
5. Install android sources: androidsdk --install ""sources;android-30""
6. Install platforms: androidsdk --install ""platforms;android-30""
7. Install platform-tools: androidsdk --install ""platform-tools""
8. Run ./configure
These are the options we provided:

Please specify the location of python. [Default is /usr/bin/python3]:
 
 Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.8/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]
 
Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.
 
Do you wish to build TensorFlow with CUDA support? [y/N]:
No CUDA support will be enabled for TensorFlow.
 
Do you wish to download a fresh release of clang? (Experimental) [y/N]:
Clang will not be downloaded.
 
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: --config=opt
 
 
Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y
Searching for NDK and SDK installations.
 
Please specify the home path of the Android NDK to use. [Default is /home/<username>/Android/Sdk/ndk-bundle]: <provide path here>
```


### Relevant log output

_No response_"
62045,Can TensorFlow be used on other microcontrollers other than listed by TensorFlow? ,"I have created a TensorFlow model that I wish to run on STM32F407VGT6. 
Is it possible to run TensorFlow model on microcontrollers other than listed ones. 
Also, is there any documentation to create and explore new models other than Hello World, Miro Speech and Person detection to understand TensorFlow Lite Micro more?
Is TensorFlow expandable to other models apart from listed above?"
62044,Obtain tf-opt and flatbuffer_translate without compiling from source,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf-opt` and `flatbuffer_translate` are not found in the standard pip package. Those need to be compiled from source.

### Standalone code to reproduce the issue

```shell
$ pip install -U tensorflow
$ which tf-opt
```


### Relevant log output

_No response_"
62043,Illegal memory access when running tf.sparse_to_dense,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.11

### Custom code

No

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

[This](https://github.com/tensorflow/tensorflow/issues/59126) issue seems to suggest that it was fixed in tf-nightly release. But I am somehow not able to find the commit where this was fixed. Can you please point me to the commit so that I can verify it has been fixed on my side

### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

_No response_"
62040,2.14 Docker image errors,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.14

### Custom code

Yes

### OS platform and distribution

Linux 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Updating docker file to latest version should allow for building image

### Standalone code to reproduce the issue

```shell
FROM tensorflow/tensorflow:latest-gpu

RUN dpkg --configure -a
```


### Relevant log output

```shell
When building this I get the error 

dpkg: error: error executing hook 'if { test ""$DPKG_HOOK_ACTION"" = add-architecture || test ""$DPKG_HOOK_ACTION"" = remove-architecture; } && test -x /usr/share/pkg-config-dpkghook; then /usr/share/pkg-config-dpkghook update; fi', exit code 32512
```
"
62035,`pip install tf-nightly[and-cuda]` fails for recent nightlies,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

**The issue**

Attempting to run `pip install tf-nightly[and-cuda]` will download a ton of nightly candidates before installing one from mid-September (before tf bumped to cuda12).

Attempting to pin the more recent versions shows the error with recent nightlies.

```shell
pip install tf-nightly[and-cuda]==2.15.0.dev20231002
...
ERROR: Could not find a version that satisfies the requirement tensorrt-libs==8.6.1; extra == ""and-cuda"" (from tf-nightly[and-cuda]) (from versions: 9.0.0.post11.dev1, 9.0.0.post12.dev1, 9.0.1.post11.dev4, 9.0.1.post12.dev4)
ERROR: No matching distribution found for tensorrt-libs==8.6.1; extra == ""and-cuda""
```

You can work around this with `pip install tf-nightly[and-cuda] --extra-index-url https://pypi.nvidia.com`.

**What should happen**
`pip install tf-nightly[and-cuda]` should not self conflict, and recent nighties should be installable via PyPI.

### Standalone code to reproduce the issue

https://colab.research.google.com/gist/mattdangerw/00acd58e43aabe7f80a74d595788bd86/tf-nightly-and-cuda.ipynb


### Relevant log output

_No response_"
62032,TF 2.14 minimum nvidia driver version,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.14

### Custom code

No

### OS platform and distribution

Tensorflow Docker image

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

450.203.8

### GPU model and memory

_No response_

### Current behavior?

I have a question on if the minimum nvidia driver version has changed (I believe the current docs state `450.80.02` (https://www.tensorflow.org/install/pip)). The below script ran using the 2.13 docker image. Thank you.

When trying to run a test gpu benchmark, I get the following error:
```text
2023-10-02 16:01:32.433368: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:461] Possibly insufficient driver version: 450.203.8
```


### Standalone code to reproduce the issue

```shell
Below is the code being used:

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import timeit

# Download data and scale
(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()

# scaling image values between 0-1
X_train_scaled = X_train/255
X_test_scaled = X_test/255

# one hot encoding labels
y_train_encoded = keras.utils.to_categorical(y_train, num_classes = 10, dtype = 'float32')
y_test_encoded = keras.utils.to_categorical(y_test, num_classes = 10, dtype = 'float32')

# Define the model
def get_model():
    model = keras.Sequential([
        keras.layers.Flatten(input_shape=(32,32,3)),
        keras.layers.Dense(3000, activation='relu'),
        keras.layers.Dense(1000, activation='relu'),
        keras.layers.Dense(10, activation='sigmoid')
    ])
    model.compile(optimizer='SGD',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
    return model

# GPU Benchmark
def gpuBench():
    # GPU
    #strategy = tf.distribute.MirroredStrategy()
    #with strategy.scope():
    with tf.device('/GPU:0'):
        model_gpu = get_model()
        model_gpu.fit(X_train_scaled, y_train_encoded, epochs = 10)

gpuBench()
```


### Relevant log output

_No response_"
62029,Tensorflow lite linking issue on ChromeOS,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

ChromeOS top of tree

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

bazel 6.2.0

### GCC/compiler version

Chromium OS 17.0_pre498229-r6 clang version 17.0.0

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current behavior?

I am working on upgrading ChromeOS from tensorflow 2.8 to 2.12 and ran into a missing library because of \
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/build_defs.bzl
Specifically `-lnativewindow`.

AFAICT nativewindow is an Android library and it looks like it isn't really supposed to be added to all operating systems building tensorflow lite with the GPU delegate.

Can you all confirm? I have a local patch removing this requirement for now.

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

```shell
/usr/bin/x86_64-cros-linux-gnu-clang -o bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/run_eval -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/librun_eval_lib.lo -Wl,-no-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/stages/libobject_detection_stage.a bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/stages/libimage_preprocessing_stage.a bazel-out/k8-opt/bin/tensorflow/core/lib/jpeg/libjpeg_internal.a bazel-out/k8-opt/bin/external/com_googlesource_code_re2/libre2.a bazel-out/k8-opt/bin/external/highwayhash/libsip_hash.a bazel-out/k8-opt/bin/external/highwayhash/libarch_specific.a bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/stages/libobject_detection_average_precision_stage.a bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/stages/utils/libimage_metrics.a bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/stages/libtflite_inference_stage.a bazel-out/k8-opt/bin/tensorflow/lite/profiling/libtime.a bazel-out/k8-opt/bin/tensorflow/tsl/util/libstats_calculator_portable.a bazel-out/k8-opt/bin/tensorflow/tsl/framework/libdevice_type.a bazel-out/k8-opt/bin/tensorflow/tsl/platform/default/liblogging.a bazel-out/k8-opt/bin/tensorflow/tsl/platform/default/libenv_time.a bazel-out/k8-opt/bin/tensorflow/tsl/platform/default/libmutex.a bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/libtask_executor_main.a bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/libtask_executor.a bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/libevaluation_delegate_provider.a -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/tools/delegates/libcoreml_delegate_provider.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/tools/delegates/libdefault_execution_provider.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/tools/delegates/libexternal_delegate_provider.lo -Wl,-no-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/delegates/external/libexternal_delegate.a -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/tools/delegates/libgpu_delegate_provider.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/tools/delegates/libhexagon_delegate_provider.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/tools/delegates/libnnapi_delegate_provider.lo -Wl,-no-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/nnapi/sl/libnnapi_support_library.a -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/tools/delegates/experimental/stable_delegate/libdelegate_provider.lo -Wl,-no-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/delegates/utils/experimental/stable_delegate/libdelegate_loader.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/utils/experimental/stable_delegate/libtflite_settings_json_parser.a -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/tools/delegates/libxnnpack_delegate_provider.lo -Wl,-no-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/libutils.a -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/core/c/libc_api_without_op_resolver.lo -Wl,-no-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/c/libc_api_internal.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/libinterpreter_utils.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/nnapi/libnnapi_delegate_no_nnapi_implementation.a bazel-out/k8-opt/bin/tensorflow/lite/nnapi/libnnapi_util.a bazel-out/k8-opt/bin/tensorflow/lite/nnapi/libnnapi_implementation.a -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/libcreate_op_resolver_with_builtin_ops.lo -Wl,-no-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/core/kernels/libbuiltin_ops.a bazel-out/k8-opt/bin/tensorflow/lite/libtflite_with_xnnpack_optional.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/libbuiltin_op_kernels.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/liblstm_eval.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/internal/libaudio_utils.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/internal/libkernel_utils.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/libeigen_support.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/libvariable_op_kernels.a bazel-out/k8-opt/bin/external/fft2d/libfft2d.a bazel-out/k8-opt/bin/tensorflow/lite/core/experimental/acceleration/configuration/c/libxnnpack_plugin.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/xnnpack/libxnnpack_delegate.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/xnnpack/libquantization_util.a bazel-out/k8-opt/bin/external/XNNPACK/libxnnpack_for_tflite.a bazel-out/k8-opt/bin/external/XNNPACK/libsubgraph.a bazel-out/k8-opt/bin/external/XNNPACK/liboperators.a bazel-out/k8-opt/bin/external/XNNPACK/libindirection.a bazel-out/k8-opt/bin/external/XNNPACK/libjit.a bazel-out/k8-opt/bin/external/XNNPACK/libmicrokernel_configs.a bazel-out/k8-opt/bin/external/XNNPACK/libscalar_amalgam_microkernels.a bazel-out/k8-opt/bin/external/XNNPACK/libsse2_amalgam_microkernels.a bazel-out/k8-opt/bin/external/XNNPACK/libssse3_amalgam_microkernels.a bazel-out/k8-opt/bin/external/XNNPACK/libsse41_amalgam_microkernels.a bazel-out/k8-opt/bin/external/XNNPACK/libavx_amalgam_microkernels.a bazel-out/k8-opt/bin/external/XNNPACK/libf16c_amalgam_microkernels.a bazel-out/k8-opt/bin/external/XNNPACK/libxop_amalgam_microkernels.a bazel-out/k8-opt/bin/external/XNNPACK/libfma3_amalgam_microkernels.a bazel-out/k8-opt/bin/external/XNNPACK/libavx2_amalgam_microkernels.a bazel-out/k8-opt/bin/external/XNNPACK/libavx512f_amalgam_microkernels.a bazel-out/k8-opt/bin/external/XNNPACK/libavx512skx_amalgam_microkernels.a bazel-out/k8-opt/bin/external/XNNPACK/libavx512vbmi_amalgam_microkernels.a bazel-out/k8-opt/bin/external/XNNPACK/libtables.a bazel-out/k8-opt/bin/external/XNNPACK/libhardware_config.a bazel-out/k8-opt/bin/external/XNNPACK/libmicrokernel_utils.a bazel-out/k8-opt/bin/external/XNNPACK/libmicroparams_init.a bazel-out/k8-opt/bin/external/XNNPACK/libnormalization.a bazel-out/k8-opt/bin/external/XNNPACK/liboperator_utils.a bazel-out/k8-opt/bin/external/XNNPACK/libpacking.a bazel-out/k8-opt/bin/external/XNNPACK/libcache.a bazel-out/k8-opt/bin/external/XNNPACK/libmemory.a bazel-out/k8-opt/bin/external/XNNPACK/libmutex.a bazel-out/k8-opt/bin/external/XNNPACK/libpost_operation.a bazel-out/k8-opt/bin/external/XNNPACK/liballocator.a bazel-out/k8-opt/bin/external/XNNPACK/liblogging.a bazel-out/k8-opt/bin/external/XNNPACK/libparams.a bazel-out/k8-opt/bin/tensorflow/lite/tools/delegates/libdelegate_provider_lib.a bazel-out/k8-opt/bin/tensorflow/lite/tools/libcommand_line_flags.a bazel-out/k8-opt/bin/tensorflow/lite/tools/libtool_params.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/libdelegate.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libapi2.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libcompiler.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libfloat16_conversions.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/compiler/libfuse_auto_input.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/compiler/libfuse_inline.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/compiler/libfuse_inplace.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/compiler/libshader_codegen.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/compiler/libcompiled_node.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/compiler/librename.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/compiler/libobject_accessor.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/compiler/libvariable_accessor.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/compiler/libpreprocessor.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libegl_environment.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libegl_context.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libegl_surface.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/librequest_gpu_info.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libruntime.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libobject_manager.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libgl_texture.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libgl_texture_helper.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libconverter.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libcommand_queue.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libregistry.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libadd.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libconcat.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libconv.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libdepthwise_conv.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/workgroups/libideal_workgroup_picker.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libelementwise.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libfully_connected.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/liblstm.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libmul.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libpad.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libpooling.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libprelu.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libquantize_and_dequantize.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/librelu.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libmean.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libresampler.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libreshape.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libresize.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libslice.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libsoftmax.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libspace_to_depth.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libtile.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libtranspose_conv.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libconvert.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libmax_unpooling.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libcustom_registry.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/workgroups/libdefault_calculator.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/workgroups/libcalculator.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/libdelegate_options.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/libserialization.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libapi.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libgl_interop.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libegl_sync.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libgl_sync.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libgl_buffer.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libgl_program.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libgl_shader.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libgl_errors.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libinference_context.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/default/librecordable_queue.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_operation.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libgpu_model.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/liboperation_selector.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/default/libconvolution_selector.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconv_constants.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconv_metal_simd.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconv_weights_converter.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/default/libconvolution_transposed_selector.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconvolution_transposed.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconvolution_transposed_3x3.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconvolution_transposed_3x3_thin.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconvolution_transposed_4x4.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconvolution_transposed_thin.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/default/libdefault_selector.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/default/libdw_convolution_selector.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libdepthwise_conv_3x3.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libdepthwise_conv_3x3_stride_h2.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/default/libfully_connected_selector.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconv_generic.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libfully_connected.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/libsimple_selectors.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libadd.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libcast.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconcat_xy.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconcat_z.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libcumsum.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libdepthwise_conv.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libgather.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/liblstm.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libmax_unpooling.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libone_hot.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libpadding.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libpooling.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libquantize_and_dequantize.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libreduce.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libresampler.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libreshape.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libreshapex4.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libresize.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libselect_v2.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libsoftmax.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libsoftmax1x1.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libspace_to_depth.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libsplit.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libstrided_slice.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libtile.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libwinograd.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libwinograd_util.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libweights_conversion.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libweights_layout.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libtranspose.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/libspecial_selector.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libmean_stddev_normalization.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/special/libconv_pointwise.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/special/libdw7x7_conv2to6_concat_conv8to8.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/special/libfc_fc_add.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/special/libthin_pointwise_fuser.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libflops_util.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libelementwise.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libprelu.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/librelu.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/libsubgraph.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/transformations/libadd_bias.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/transformations/libglobal_pooling_to_reduce_op.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libmemory_management.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libserialization_base.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/kernels/libconverter.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_arguments.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libqcom_thin_filter.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libqcom_thin_filter_desc.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libenvironment.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libprogram_cache.a bazel-out/k8-opt/bin/external/farmhash_archive/libfarmhash.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libtensor.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libbuffer.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_command_queue.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_event.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_kernel.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_program.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libprofiling_info.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_context.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_device.a bazel-out/k8-opt/bin/tensorflow/lite/experimental/acceleration/compatibility/libandroid_info.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_image_format.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_memory.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libtensor_type_util.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/libapi.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconversion.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libgpu_operation.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libarguments.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libbuffer_desc.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libwork_group_picking.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libworkgroup_selection.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libtensor_desc.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libutil.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libgpu_info.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libprecision.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libutil.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/default/libutil.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libopencl_wrapper.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libmodel_builder.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/liblstm_parser.a bazel-out/k8-opt/bin/tensorflow/lite/liblogger.a bazel-out/k8-opt/bin/tensorflow/lite/liboptional_debug_tools.a -Wl,-whole-archive bazel-out/k8-o
...
pt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libdepthwise_conv.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/workgroups/libideal_workgroup_picker.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libelementwise.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libfully_connected.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/liblstm.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libmul.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libpad.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libpooling.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libprelu.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libquantize_and_dequantize.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/librelu.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libmean.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libresampler.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libreshape.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libresize.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libslice.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libsoftmax.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libspace_to_depth.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libtile.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libtranspose_conv.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libconvert.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libmax_unpooling.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/kernels/libcustom_registry.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/workgroups/libdefault_calculator.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/workgroups/libcalculator.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/libdelegate_options.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/libserialization.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libapi.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libgl_interop.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libegl_sync.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libgl_sync.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libgl_buffer.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libgl_program.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libgl_shader.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/gl/libgl_errors.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libinference_context.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/default/librecordable_queue.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_operation.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libgpu_model.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/liboperation_selector.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/default/libconvolution_selector.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconv_constants.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconv_metal_simd.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconv_weights_converter.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/default/libconvolution_transposed_selector.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconvolution_transposed.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconvolution_transposed_3x3.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconvolution_transposed_3x3_thin.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconvolution_transposed_4x4.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconvolution_transposed_thin.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/default/libdefault_selector.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/default/libdw_convolution_selector.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libdepthwise_conv_3x3.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libdepthwise_conv_3x3_stride_h2.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/default/libfully_connected_selector.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconv_generic.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libfully_connected.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/libsimple_selectors.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libadd.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libcast.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconcat_xy.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconcat_z.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libcumsum.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libdepthwise_conv.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libgather.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/liblstm.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libmax_unpooling.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libone_hot.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libpadding.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libpooling.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libquantize_and_dequantize.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libreduce.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libresampler.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libreshape.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libreshapex4.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libresize.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libselect_v2.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libsoftmax.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libsoftmax1x1.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libspace_to_depth.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libsplit.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libstrided_slice.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libtile.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libwinograd.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libwinograd_util.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libweights_conversion.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libweights_layout.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libtranspose.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/libspecial_selector.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libmean_stddev_normalization.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/special/libconv_pointwise.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/special/libdw7x7_conv2to6_concat_conv8to8.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/special/libfc_fc_add.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/special/libthin_pointwise_fuser.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libflops_util.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libelementwise.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libprelu.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/librelu.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/selectors/libsubgraph.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/transformations/libadd_bias.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/transformations/libglobal_pooling_to_reduce_op.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libmemory_management.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libserialization_base.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/kernels/libconverter.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_arguments.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libqcom_thin_filter.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libqcom_thin_filter_desc.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libenvironment.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libprogram_cache.a bazel-out/k8-opt/bin/external/farmhash_archive/libfarmhash.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libtensor.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libbuffer.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_command_queue.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_event.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_kernel.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_program.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libprofiling_info.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_context.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_device.a bazel-out/k8-opt/bin/tensorflow/lite/experimental/acceleration/compatibility/libandroid_info.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_image_format.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libcl_memory.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libtensor_type_util.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/libapi.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/tasks/libconversion.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libgpu_operation.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libarguments.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libbuffer_desc.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libwork_group_picking.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libworkgroup_selection.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libtensor_desc.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/task/libutil.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libgpu_info.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libprecision.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libutil.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/default/libutil.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/cl/libopencl_wrapper.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libmodel_builder.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/liblstm_parser.a bazel-out/k8-opt/bin/tensorflow/lite/liblogger.a bazel-out/k8-opt/bin/tensorflow/lite/liboptional_debug_tools.a -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/core/libcc_api_experimental.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/core/libcc_api_stable.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/core/libmodel_builder.lo -Wl,-no-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/delegates/libtelemetry.a bazel-out/k8-opt/bin/tensorflow/lite/profiling/libplatform_profiler.a bazel-out/k8-opt/bin/tensorflow/lite/libmutable_op_resolver.a bazel-out/k8-opt/bin/tensorflow/lite/libsignature_runner.a -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/core/libsubgraph.lo -Wl,-no-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/liballocation.a bazel-out/k8-opt/bin/tensorflow/lite/libarena_planner.a bazel-out/k8-opt/bin/tensorflow/lite/libsimple_memory_arena.a -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/libtensorflow_profiler_logger_shim.lo -Wl,-no-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/c/libcommon_internal.a bazel-out/k8-opt/bin/tensorflow/lite/experimental/remat/libmetadata_util.a bazel-out/k8-opt/bin/tensorflow/lite/libgraph_info.a bazel-out/k8-opt/bin/tensorflow/lite/experimental/resource/libresource.a bazel-out/k8-opt/bin/tensorflow/lite/profiling/libroot_profiler.a bazel-out/k8-opt/bin/tensorflow/lite/profiling/telemetry/libtelemetry.a bazel-out/k8-opt/bin/tensorflow/lite/profiling/telemetry/libprofiler.a bazel-out/k8-opt/bin/tensorflow/lite/profiling/telemetry/c/libtelemetry_setting_internal.a bazel-out/k8-opt/bin/tensorflow/lite/tools/versioning/libgpu_compatibility.a bazel-out/k8-opt/bin/tensorflow/lite/tools/versioning/libop_signature.a bazel-out/k8-opt/bin/tensorflow/lite/libstderr_reporter.a bazel-out/k8-opt/bin/tensorflow/lite/core/api/libapi.a bazel-out/k8-opt/bin/tensorflow/lite/core/api/libop_resolver.a bazel-out/k8-opt/bin/tensorflow/lite/core/api/liberror_reporter.a bazel-out/k8-opt/bin/tensorflow/lite/libminimal_logging.a bazel-out/k8-opt/bin/tensorflow/lite/schema/libschema_utils.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/transformations/libmodel_transformations.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/transformations/libadd_quant_adjustments.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/transformations/libfuse_add_to_conv.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/transformations/libfuse_mul_to_conv.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/transformations/libmake_fully_connected.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/transformations/libmerge_densify.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/transformations/libmake_padding.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/transformations/libmerge_padding_with.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/transformations/libremove_noop.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/default/libcustom_transformations.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libmodel_transformer.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/default/libcustom_parsers.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/liboperation_parser.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libobject_reader.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/libutils.a bazel-out/k8-opt/bin/tensorflow/lite/libutil.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libmodel_builder_helper.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libmodel.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/liboperations.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libdata_type.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libshape.a bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/common/libquantization_util.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/internal/utils/libsparsity_format_converter.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/internal/libtensor_utils.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/internal/libsse_tensor_utils.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/internal/libneon_tensor_utils.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/internal/libportable_tensor_utils.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/internal/libtranspose_utils.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/libcpu_backend_gemm.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/internal/libcpu_check.a bazel-out/k8-opt/bin/external/ruy/ruy/libcontext_get_ctx.a bazel-out/k8-opt/bin/external/ruy/ruy/libfrontend.a bazel-out/k8-opt/bin/external/ruy/ruy/libkernel_arm.a bazel-out/k8-opt/bin/external/ruy/ruy/libkernel_avx.a bazel-out/k8-opt/bin/external/ruy/ruy/libkernel_avx2_fma.a bazel-out/k8-opt/bin/external/ruy/ruy/libkernel_avx512.a bazel-out/k8-opt/bin/external/ruy/ruy/libapply_multiplier.a bazel-out/k8-opt/bin/external/ruy/ruy/libpack_arm.a bazel-out/k8-opt/bin/external/ruy/ruy/libpack_avx.a bazel-out/k8-opt/bin/external/ruy/ruy/libpack_avx2_fma.a bazel-out/k8-opt/bin/external/ruy/ruy/libpack_avx512.a bazel-out/k8-opt/bin/external/ruy/ruy/libprepare_packed_matrices.a bazel-out/k8-opt/bin/external/ruy/ruy/libtrmul.a bazel-out/k8-opt/bin/external/ruy/ruy/libblock_map.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/libcpu_backend_context.a bazel-out/k8-opt/bin/tensorflow/lite/libexternal_cpu_backend_context.a bazel-out/k8-opt/bin/external/pthreadpool/libpthreadpool.a bazel-out/k8-opt/bin/external/ruy/ruy/libcontext.a bazel-out/k8-opt/bin/external/ruy/ruy/libctx.a bazel-out/k8-opt/bin/external/ruy/ruy/liballocator.a bazel-out/k8-opt/bin/external/ruy/ruy/libhave_built_path_for_avx.a bazel-out/k8-opt/bin/external/ruy/ruy/libhave_built_path_for_avx2_fma.a bazel-out/k8-opt/bin/external/ruy/ruy/libhave_built_path_for_avx512.a bazel-out/k8-opt/bin/external/ruy/ruy/libprepacked_cache.a bazel-out/k8-opt/bin/external/ruy/ruy/libsystem_aligned_alloc.a bazel-out/k8-opt/bin/external/ruy/ruy/libtune.a bazel-out/k8-opt/bin/external/ruy/ruy/libcpuinfo.a bazel-out/k8-opt/bin/external/cpuinfo/libcpuinfo_impl.a bazel-out/k8-opt/bin/external/ruy/ruy/libthread_pool.a bazel-out/k8-opt/bin/external/ruy/ruy/libblocking_counter.a bazel-out/k8-opt/bin/external/ruy/ruy/libdenormal.a bazel-out/k8-opt/bin/external/ruy/ruy/libwait.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/libkernel_util.a bazel-out/k8-opt/bin/tensorflow/lite/kernels/internal/libquantization_util.a bazel-out/k8-opt/bin/external/ruy/ruy/profiler/libinstrumentation.a bazel-out/k8-opt/bin/tensorflow/lite/libstring_util.a -Wl,-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/core/c/libcommon.lo -Wl,-no-whole-archive bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/proto/libevaluation_config_proto.a bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/proto/libevaluation_stages_proto.a bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/proto/libpreprocessing_steps_proto.a -ldl -ldl -lpthread -lprotobuf -lgif -pthread -lz -ljpeg -ldl -lnsync_cpp -ldl -ldl -lrt -ldl -lm -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lpthread -lnativewindow -ldl -ldl -lflatbuffers -labsl_bad_any_cast_impl -pthread -pthread -pthread -lpthread -lm -pthread -labsl_hash -labsl_city -labsl_low_level_hash -labsl_bad_variant_access -labsl_raw_hash_set -labsl_hashtablez_sampler -labsl_exponential_biased -labsl_synchronization -pthread -labsl_graphcycles_internal -labsl_time -labsl_civil_time -labsl_time_zone -labsl_status -labsl_stacktrace -labsl_symbolize -labsl_debugging_internal -labsl_demangle_internal -labsl_malloc_internal -pthread -labsl_cord -labsl_cord_internal -labsl_cordz_functions -labsl_cordz_handle -labsl_cordz_info -labsl_cordz_sample_token -labsl_str_format_internal -labsl_strings -labsl_strings_internal -labsl_base -pthread -labsl_spinlock_wait -labsl_int128 -labsl_throw_delegate -labsl_bad_optional_access -labsl_raw_logging_internal -labsl_log_severity -Wl,-O2 -Wl,--as-needed -Wl,--gc-sections '-Wl,--icf=all' -lc++ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -B/usr/bin/ -Wl,--gc-sections '--sysroot=/build/amd64-generic')
# Configuration: 988c102d8dcfe503ba01d9bd423822b77b461f1bc452ff8587ae6ef206fa2111
# Execution platform: @local_execution_config_platform//:platform
ld.lld: error: unable to find library -lnativewindow
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```
```
"
62028,Can't import tensorflow,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

any version

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I want to install Tensorflow for my system. I installed it by ""pip install tensorflow"". 
I have:
python3.8
Ubuntu 20.04
Intel Core I9 12 Gen
Nvidia GeForce 3090 TI


### Standalone code to reproduce the issue

```shell
import tensorflow
Illegal instruction (core dumped)
```


### Relevant log output

_No response_"
62026,Models that trained and saved on Tensorflow2.14 cant be loaded on Tensorflow2.12,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14

### Custom code

Yes

### OS platform and distribution

Linux mint 21.2

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

gtx 1060 6gb

### Current behavior?

I expected that when I load a keras model that I trained on my home computer on tensorflow 2.14 wil be loaded on kaggle notebook which has tensorflow 2.12

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

#the test_model.keras was trained on tensorflow version 2.14
#the current tensorflow version in 2.12

model = tf.keras.models.load_model(""/home/sagi/Desktop/VsCode/Competiton/MODEL/test_model.keras"")
```


### Relevant log output

_No response_"
62024,AttributeError: module 'object_detection.protos.input_reader_pb2' has no attribute 'NUMERICAL_MASKS' ,"python3 train.py --logtostderr --train_dir=CAPTCHA_training_dir/ --pipeline_config_path=CAPTCHA_training/faster_rcnn_inception_v2_coco.config

While running this command I am getting error like this :

Traceback (most recent call last):
  File ""train.py"", line 51, in <module>
    from object_detection.builders import dataset_builder
  File ""/home/Desktop/test/test_env/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py"", line 32, in <module>
    from object_detection.builders import decoder_builder
  File ""/home/Desktop/test/test_env/lib/python3.8/site-packages/object_detection/builders/decoder_builder.py"", line 24, in <module>
    from object_detection.data_decoders import tf_example_decoder
  File ""/home/Desktop/test/test_env/lib/python3.8/site-packages/object_detection/data_decoders/tf_example_decoder.py"", line 131, in <module>
    class TfExampleDecoder(data_decoder.DataDecoder):
  File ""/home/Desktop/test/test_env/lib/python3.8/site-packages/object_detection/data_decoders/tf_example_decoder.py"", line 136, in TfExampleDecoder
    instance_mask_type=input_reader_pb2.NUMERICAL_MASKS,
AttributeError: module 'object_detection.protos.input_reader_pb2' has no attribute 'NUMERICAL_MASKS'

Please do help me to solve this issue.

"
62023,AttributeError: module 'object_detection.protos.input_reader_pb2' has no attribute 'NUMERICAL_MASKS' ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
62021,In TF-v2.12 there are no methods called `_set_hyper` and `_get_hyper`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom code

Yes

### OS platform and distribution

Kaggle kernel

### Mobile device

_No response_

### Python version

3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0]

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

There are no methods called `_set_hyper` and `_get_hyper`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

class MyAdamOptimizer(tf.keras.optimizers.Optimizer):
    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, name=""MyAdamOptimizer"", **kwargs):
        super(MyAdamOptimizer, self).__init__(name, **kwargs)
        
        self._set_hyper(""learning_rate"", kwargs.get(""lr"", learning_rate))
        self._set_hyper(""beta_1"", beta_1)
        self._set_hyper(""beta_2"", beta_2)
        self._set_hyper(""epsilon"", epsilon)
        
    def _create_slots(self, var_list):
        for var in var_list:
            self.add_slot(var, ""m"")
            self.add_slot(var, ""v"")
            
    def _resource_apply_dense(self, grad, var):
        lr = self._get_hyper(""learning_rate"", var_dtype=var.dtype.base_dtype)
        beta_1 = self._get_hyper(""beta_1"", var_dtype=var.dtype.base_dtype)
        beta_2 = self._get_hyper(""beta_2"", var_dtype=var.dtype.base_dtype)
        epsilon = self._get_hyper(""epsilon"", var_dtype=var.dtype.base_dtype)
        
        m = self.get_slot(var, ""m"")
        v = self.get_slot(var, ""v"")
        
        m.assign_add((1 - beta_1) * (grad - m))
        v.assign_add((1 - beta_2) * (tf.square(grad) - v))
        
        m_hat = m / (1 - tf.math.pow(beta_1, tf.cast(self.iterations + 1, tf.float32)))
        v_hat = v / (1 - tf.math.pow(beta_2, tf.cast(self.iterations + 1, tf.float32)))
        
        var_update = lr * m_hat / (tf.sqrt(v_hat) + epsilon)
        
        var.assign_sub(var_update)
        
        return var_update
        
    def _resource_apply_sparse(self, grad, var):
        raise NotImplementedError(""Sparse gradient updates are not supported."")

    
optimizer = MyAdamOptimizer(learning_rate=0.001)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[5], line 42
     38     def _resource_apply_sparse(self, grad, var):
     39         raise NotImplementedError(""Sparse gradient updates are not supported."")
---> 42 optimizer = MyAdamOptimizer(learning_rate=0.001)

Cell In[5], line 7, in MyAdamOptimizer.__init__(self, learning_rate, beta_1, beta_2, epsilon, name, **kwargs)
      4 def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, name=""MyAdamOptimizer"", **kwargs):
      5     super(MyAdamOptimizer, self).__init__(name, **kwargs)
----> 7     self._set_hyper(""learning_rate"", kwargs.get(""lr"", learning_rate))
      8     self._set_hyper(""beta_1"", beta_1)
      9     self._set_hyper(""beta_2"", beta_2)

AttributeError: 'MyAdamOptimizer' object has no attribute '_set_hyper'
```
"
62019,dead link,"On this page:

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/distributed_runtime

there is a link at the bottom which doesn't work. Its label is ""Distributed TensorFlow"".


"
62018,Tensorflow profiler client does not support IPv6,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.11.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3

### Mobile device

_No response_

### Python version

3.10.2

### Bazel version

6.0.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11/8.2

### GPU model and memory

_No response_

### Current behavior?

[https://github.com/google/tsl/blob/3dee2c5930eb8ee9c6a7486434240dafadf12fb4/tsl/profiler/utils/session_manager.cc#L188]()

`std::vectorabsl::string_view parts = absl::StrSplit(host_port, ':');`

When trying to connect to an IPv6 host-port pair, which is typically denoted as [xxxx:xxxx:blah]:port, `profiler_client.trace()` throws an error. It should have logic that supports both IPv4 and IPv6. 

### Standalone code to reproduce the issue

```shell
File 1:
jax.profiler.start_server(9876) #on IPv6 xxxx:xxxx:x:xxxx:xxxx::xxx

File 2:
from tensorflow.python.profiler import profiler_client
profiler_client.trace(
        '[xxxx:xxxx:x:xxxx:xxxx::xxx]:9876',
        duration_ms=1000,
        logdir='foo',
    )
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""foo.py"", line 2, in _trigger_trace
    profiler_client.trace(
  File "".../foo.runfiles/tensorflow_python_deps_tensorflow/tensorflow/python/profiler/profiler_client.py"", line 129, in trace
    _pywrap_profiler.trace(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Could not interpret ""[xxxx:xxxx:x:xxxx:xxxx::xxx]:9876"" as a host-port pair.
```
"
62016,Tensorflow debug build reports the error: relocation R_X86_64_PC32 against undefined symbol `_ZN15stream_executor3gpu12_GLOBAL__N_120CUDABlasLtMatmulPlan14kMaxBatchCountE' can not be used when making a shared object,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

1.16.0

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

12.2

### GPU model and memory

GTX 1080 & RTX 2080

### Current behavior?

Reported the following error:
```
WARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /root/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/bin/python3 --config=xla --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.7 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.0,6.1,6.2,7.0,7.2,7.5 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 --config=cuda --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /root/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /root/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /root/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file /root/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file /root/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:cuda in file /root/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file /root/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:dbg in file /root/tensorflow/.bazelrc: --config=opt -c dbg --cxxopt -DTF_LITE_DISABLE_X86_NEON --copt -DDEBUG_BUILD
INFO: Found applicable config definition build:opt in file /root/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare --define with_default_optimizations=true
INFO: Found applicable config definition build:linux in file /root/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /root/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build options --copt and --cxxopt have changed, discarding analysis cache.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 32967 targets configured).
INFO: Found 1 target...
ERROR: /root/tensorflow/tensorflow/BUILD:724:1: Linking of rule '//tensorflow:libtensorflow_framework.so.2.4.4' failed (Exit 1)
/usr/bin/ld: bazel-out/k8-dbg/bin/tensorflow/stream_executor/cuda/libcublas_plugin.lo(cuda_blas.pic.o): relocation R_X86_64_PC32 against undefined symbol `_ZN15stream_executor3gpu12_GLOBAL__N_120CUDABlasLtMatmulPlan14kMaxBatchCountE' can not be used when making a shared object; recompile with -fPIC
/usr/bin/ld: final link failed: bad value
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 7241.142s, Critical Path: 2298.04s
INFO: 6729 processes: 6729 local.
FAILED: Build did NOT complete successfully
```

### Standalone code to reproduce the issue

```shell
git clone -b r2.0 https://github.com/tensorflow/tensorflow.git
bazel build --config=cuda --config=dbg --copt=-fPIC --cxxopt=-fPIC //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_"
62015,implement llama 2 using Tensorflow,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.8

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

-

### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

```shell
-
```
"
62014,error with concatenation when converting QAT model to tflite model using EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,"### 1. System information

- Windows 11
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library : 2.13

I am attempting to convert a QAT model trained with int8 weights, int16 activations to a tflite model using
tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8. Unfortunately there is an issue with
converting the model using this opset. 
Minimal code to reproduce the error: 
```
import tensorflow as tf
import tensorflow_model_optimization as tfmot

inp1 = tf.keras.Input(shape=[2,4,8], batch_size = 1,name = 'input1')
inp2 = tf.keras.Input(shape=[2,4,8], batch_size = 1,name = 'input2')
r1 =tf.keras.layers.ReLU()(inp1)
r2 = tf.keras.layers.ReLU()(inp2)
c1 = tf.keras.layers.Concatenate(axis = -1)([r1,r2])

scheme_16_8 = tfmot.quantization.keras.experimental.default_n_bit.DefaultNBitQuantizeScheme(
    disable_per_axis=False, num_bits_weight=8, num_bits_activation=16)

test_model = tf.keras.Model(inputs=[inp1, inp2], outputs=c1)
annotated_model = tf.keras.models.clone_model(
        test_model,      
    )
ann_model = tfmot.quantization.keras.quantize_annotate_model(annotated_model)
q_model = tfmot.quantization.keras.quantize_apply(ann_model, scheme = scheme_16_8)

converter = tf.lite.TFLiteConverter.from_keras_model(q_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]#tf.lite.OpsSet.TFLITE_BUILTINS]
#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
quantized_tflite_model = converter.convert()
```

This yields the following error:
```
---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
Cell In[11], line 5
      3 converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]#tf.lite.OpsSet.TFLITE_BUILTINS]
      4 #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
----> 5 quantized_tflite_model = converter.convert()
      7 file_name = 'test_model.tflite'
      9     # Save the model.

File ~\.conda\envs\tf212\lib\site-packages\tensorflow\lite\python\lite.py:962, in _export_metrics.<locals>.wrapper(self, *args, **kwargs)
    959 @functools.wraps(convert_func)
    960 def wrapper(self, *args, **kwargs):
    961   # pylint: disable=protected-access
--> 962   return self._convert_and_export_metrics(convert_func, *args, **kwargs)

File ~\.conda\envs\tf212\lib\site-packages\tensorflow\lite\python\lite.py:940, in TFLiteConverterBase._convert_and_export_metrics(self, convert_func, *args, **kwargs)
    938 self._save_conversion_params_metric()
    939 start_time = time.process_time()
--> 940 result = convert_func(self, *args, **kwargs)
    941 elapsed_time_ms = (time.process_time() - start_time) * 1000
    942 if result:

File ~\.conda\envs\tf212\lib\site-packages\tensorflow\lite\python\lite.py:1373, in TFLiteKerasModelConverterV2.convert(self)
   1360 @_export_metrics
   1361 def convert(self):
   1362   """"""Converts a keras model based on instance variables.
   1363 
   1364   Returns:
   (...)
   1371       Invalid quantization parameters.
   1372   """"""
-> 1373   saved_model_convert_result = self._convert_as_saved_model()
   1374   if saved_model_convert_result:
   1375     return saved_model_convert_result

File ~\.conda\envs\tf212\lib\site-packages\tensorflow\lite\python\lite.py:1355, in TFLiteKerasModelConverterV2._convert_as_saved_model(self)
   1352   graph_def, input_tensors, output_tensors = (
   1353       self._convert_keras_to_saved_model(temp_dir))
   1354   if self.saved_model_dir:
-> 1355     return super(TFLiteKerasModelConverterV2,
   1356                  self).convert(graph_def, input_tensors, output_tensors)
   1357 finally:
   1358   shutil.rmtree(temp_dir, True)

File ~\.conda\envs\tf212\lib\site-packages\tensorflow\lite\python\lite.py:1166, in TFLiteConverterBaseV2.convert(self, graph_def, input_tensors, output_tensors)
   1161   logging.info(""Using new converter: If you encounter a problem ""
   1162                ""please file a bug. You can opt-out ""
   1163                ""by setting experimental_new_converter=False"")
   1165 # Converts model.
-> 1166 result = _convert_graphdef(
   1167     input_data=graph_def,
   1168     input_tensors=input_tensors,
   1169     output_tensors=output_tensors,
   1170     **converter_kwargs)
   1172 return self._optimize_tflite_model(
   1173     result, self._quant_mode, quant_io=self.experimental_new_quantizer)

File ~\.conda\envs\tf212\lib\site-packages\tensorflow\lite\python\convert_phase.py:212, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)
    210   else:
    211     report_error_message(str(converter_error))
--> 212   raise converter_error from None  # Re-throws the exception.
    213 except Exception as error:
    214   report_error_message(str(error))

File ~\.conda\envs\tf212\lib\site-packages\tensorflow\lite\python\convert_phase.py:205, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)
    202 @functools.wraps(func)
    203 def wrapper(*args, **kwargs):
    204   try:
--> 205     return func(*args, **kwargs)
    206   except ConverterError as converter_error:
    207     if converter_error.errors:

File ~\.conda\envs\tf212\lib\site-packages\tensorflow\lite\python\convert.py:817, in convert_graphdef(input_data, input_tensors, output_tensors, **kwargs)
    814   else:
    815     model_flags.output_arrays.append(util.get_tensor_name(output_tensor))
--> 817 data = convert(
    818     model_flags.SerializeToString(),
    819     conversion_flags.SerializeToString(),
    820     input_data.SerializeToString(),
    821     debug_info_str=debug_info.SerializeToString() if debug_info else None,
    822     enable_mlir_converter=enable_mlir_converter)
    823 return data

File ~\.conda\envs\tf212\lib\site-packages\tensorflow\lite\python\convert.py:322, in convert(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    320     for error_data in _metrics_wrapper.retrieve_collected_errors():
    321       converter_error.append_error(error_data)
--> 322     raise converter_error
    324 return _run_deprecated_conversion_binary(model_flags_str,
    325                                          conversion_flags_str, input_data_str,
    326                                          debug_info_str)

ConverterError: C:\Users\derry\.conda\envs\tf212\lib\site-packages\keras\layers\merging\concatenate.py:134:0: error: 'tfl.concatenation' op operand #0 must be tensor of 32-bit float or 64-bit signless integer or 32-bit signless integer or 16-bit signless integer or 8-bit signless integer or QI8 type or QUI8 type or 8-bit unsigned integer or 1-bit signless integer values, but got 'tensor<1x2x4x8x!quant.uniform<i16:f32, 1.8310826276035706E-4>>'
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
```
Doing the same process but in int8 yields no errors:
```
q_model_2 = tfmot.quantization.keras.quantize_apply(ann_model)

converter = tf.lite.TFLiteConverter.from_keras_model(q_model2)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
#converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]#tf.lite.OpsSet.TFLITE_BUILTINS]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
quantized_tflite_model2 = converter.convert()
```

"
62013,TFlite memory allocation thrashing with FlexDelegates,"### 1. System information

- OS Platform and Distribution macOS, iOS 16.6.1 iPhone 13 Pro:
- TensorFlow installation (pip package or built from source): 2.13.0
- TensorFlow library (version, if pip package or github SHA, if built from source): tflite built from 2.13.0 tag

### 2. Code

I have a model that converts correctly and flags the following flex delegates:

```python
  tf.AddV2(tensor<1x33xcomplex<f32>>, tensor<complex<f32>>) -> (tensor<1x33xcomplex<f32>>) : {device = """"}
  tf.AddV2(tensor<complex<f32>>, tensor<complex<f32>>) -> (tensor<complex<f32>>) : {device = """"}
  tf.Complex(tensor<1x33xf32>, tensor<1x33xf32>) -> (tensor<1x33xcomplex<f32>>) : {device = """"}
  tf.Complex(tensor<f32>, tensor<f32>) -> (tensor<complex<f32>>) : {device = """"}
  tf.ConcatV2(tensor<1x1x1xcomplex<f32>>, tensor<1x1x64xcomplex<f32>>, tensor<i32>) -> (tensor<1x1x33xcomplex<f32>>) : {device = """"}
  tf.GatherV2(tensor<1x1x33xcomplex<f32>>, tensor<i32>, tensor<i32>) -> (tensor<1x33xcomplex<f32>>) : {batch_dims = 0 : i64}
  tf.Pow(tensor<complex<f32>>, tensor<complex<f32>>) -> (tensor<complex<f32>>) : {device = """"}
  tf.RealDiv(tensor<1x33xcomplex<f32>>, tensor<1x33xcomplex<f32>>) -> (tensor<1x33xcomplex<f32>>) : {device = """"}
  tf.SelectV2(tensor<1x1x1xi1>, tensor<1x1x33xcomplex<f32>>, tensor<1x1x33xcomplex<f32>>) -> (tensor<1x1x33xcomplex<f32>>) : {device = """"}
  tf.StridedSlice(tensor<1x1x33xcomplex<f32>>, tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> (tensor<1x1x1xcomplex<f32>>) : {begin_mask = 7 : i64, ellipsis_mask = 0 : i64, end_mask = 3 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}
  tf.StridedSlice(tensor<1x1x33xcomplex<f32>>, tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> (tensor<1x1x64xcomplex<f32>>) : {begin_mask = 3 : i64, ellipsis_mask = 0 : i64, end_mask = 7 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}
  tf.Sub(tensor<complex<f32>>, tensor<complex<f32>>) -> (tensor<complex<f32>>) : {device = """"}
  tf.Transpose(tensor<1x1x33xcomplex<f32>>, tensor<3xi32>) -> (tensor<1x1x33xcomplex<f32>>) : {device = """"}
```

### 3. Failure after conversion
When running this model on iOS, and profiling the model, I can see the application spending a lot of time (30%) in `posix_memalign` that are emitted from the flex library which leads to poor performance of the model executing (highlighted in following image):

![image](https://github.com/tensorflow/tensorflow/assets/192171/f4950106-1b7f-47cc-8025-d3fc9818e643)

Is there a way to:
- figure out which flex delegate is causing this? (If I compile the flex library with symbols, I can't seem to turn on optimization which results in an unusable library for me)
- modify the memory allocation/de-allocation strategy to re-ruse existing memory? I suspect this issue is a side-effect of tflite <-> tensorflow interop but that is only a suspicion
- set something like max allowed persistent memory to prevent de-allocation? I also have a suspicion that it might not be tied to a single delegate but by the allocator itself"
62006,tf.train.Checkpoint.restore does not restore the tf.data.Iterator state.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current behavior?

`tf.data.Dataset` iterators are supported in `tf.train.Checkpoint`.  When I try to restore a checkpoint, I always get a `StopIteration` error.  This should not be the case because : 

1. I save the checkpoint only at the end of an epoch.
2. Before saving the checkpoint, I always call `db_iter=iter(dataset)` ,so it should give me a new iterator.

In prior versions of TF there were functions like `make_initializable_iterator()` which are now deprecated. Since, there is no native mechanism to reset a python iterator to beginning, I wonder why `tf.train.Checkpoint.restore()` does not save the iterator state properly.

I also understand well that one can just loop over the dataset like:

```
import tensorflow as tf
ds = tf.data.Dataset.range(50)
for index, data in ds:
    # Do something 
```

However, saving the dataset in the checkpoint has two problems:

a) **I have not explicitly tested this**  For a very large dataset like MSCOCO or OpenImages or even Imagenet, what does saving the dataset mean , if I already have the dataset as TFRecords ? Will it end up saving the whole dataset again inside the  checkpoint ? At least, `tf.data.Dataset.save()` points towards this. In case this is it, I would never like to save a large dataset directly into the checkpoint.

b) **This is fully tested** If I experiment with a small simple dataset ( e.g:- `tf.data.Dataset.range(10)` ) and save it in the checkpoint, then it does not restore the state of the iterator. This observation is the same as #48178 which is still unresolved. 

### Standalone code to reproduce the issue

```shell
import os
import numpy as np
import tensorflow as tf
from absl import app


class Net(tf.keras.Model):
    def __init__(self):
        super(Net, self).__init__()
        self.x = tf.Variable(tf.keras.initializers.GlorotUniform()(shape=[1]))

    def call(self, inputs):
        return inputs


def main(argv):
    del argv
    tf.random.set_seed(123)
    net = Net()
    optim = tf.optimizers.Adam(learning_rate=0.001)

    dataset = tf.data.Dataset.from_tensor_slices((np.arange(10)))
    dataset = dataset.shuffle(10, reshuffle_each_iteration=True)
    dataset = dataset.batch(2)
    db_iter = iter(dataset)
    step = tf.Variable(0)
    checkpoint = tf.train.Checkpoint(
        step=step, optimizer=optim, net=net, db_iter=db_iter
    )
    manager = tf.train.CheckpointManager(checkpoint, ""./ckpts"", max_to_keep=50)
    manager.restore_or_initialize()
    print(step)
    for epoch in range(10):
        manager.restore_or_initialize()
        for _ in range(dataset.cardinality()):
            batch = next(db_iter)
            step.assign_add(1)
            print(batch)
        db_iter = iter(dataset)
        manager.save()
        print(f""Epoch {epoch} finished."")


if __name__ == ""__main__"":
    app.run(main)
```
```


### Relevant log output

```shell
The first time, it will print something like : 

<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0>
tf.Tensor([3 6], shape=(2,), dtype=int64)
tf.Tensor([4 0], shape=(2,), dtype=int64)
tf.Tensor([1 5], shape=(2,), dtype=int64)
tf.Tensor([7 8], shape=(2,), dtype=int64)
tf.Tensor([2 9], shape=(2,), dtype=int64)
Epoch 0 finished.
tf.Tensor([6 3], shape=(2,), dtype=int64)
tf.Tensor([0 5], shape=(2,), dtype=int64)
tf.Tensor([2 8], shape=(2,), dtype=int64)
tf.Tensor([1 7], shape=(2,), dtype=int64)
tf.Tensor([4 9], shape=(2,), dtype=int64)
Epoch 1 finished.
tf.Tensor([0 3], shape=(2,), dtype=int64)
tf.Tensor([1 4], shape=(2,), dtype=int64)
tf.Tensor([6 7], shape=(2,), dtype=int64)
tf.Tensor([9 8], shape=(2,), dtype=int64)
tf.Tensor([5 2], shape=(2,), dtype=int64)
Epoch 2 finished.
tf.Tensor([3 7], shape=(2,), dtype=int64)
tf.Tensor([0 9], shape=(2,), dtype=int64)
tf.Tensor([4 2], shape=(2,), dtype=int64)
tf.Tensor([1 8], shape=(2,), dtype=int64)
tf.Tensor([5 6], shape=(2,), dtype=int64)

and so on....


From next time onwards:

    raise StopIteration
StopIteration
```
"
62004,TimeDistributed not compatible with multi-output models,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have been attempting to create a custom CNN-RNN hybrid model that incorporates a pretrained custom DenseNet-derived CNN. My model requires from the CNN both its final output and the intermediate outputs of its dense blocks (as I am attempting to create a U-Net like architecture). I attempted to create a new model from the given CNN that would output these intermediate convolutional layers along with the final output value. However, when I connected it to a TimeDistributed layer, I received a strange error with no clear explanation:

`AttributeError: 'list' object has no attribute 'shape'`

After some experimentation, I realized this error was arising because the TimeDistributed layer was not programmed to handle multi-output layers passed to it. I understand that this issue will likely require a fix to Tensorflow. In the meanwhile, any suggestions for workarounds would be appreciated.

### Standalone code to reproduce the issue

```shell
cnn_model = tf.keras.models.load_model(cnn_filepath)
cnn_final_output = cnn_model.layers[-1].output
cnn_intermediate_output = cnn_model.layers[-3].output
new_model = tf.keras.models.Model(inputs=cnn_input, outputs=[cnn_final_output,cnn_intermediate_output])

output = tfl.TimeDistributed(new_model)(input_images, mask=mask)
```


### Relevant log output

```shell
AttributeError                            Traceback (most recent call last)
/Users/[NAME REMOVED]/[NAME REMOVED] DL/TRG_project_recurrent_cnn_residual_instantaneous_train.ipynb Cell 1 line 1
    173 cnn_model.trainable = False
    174 model = RNNCNNResidualModel(N_A, cnn_model, [""average_pooling2d_6"", ""average_pooling2d_19"", ""average_pooling2d_44"", ""activation_239""])
--> 176 model.build([tf.TensorShape([None, None, 101, 101, 1]), tf.TensorShape([None, None, 1])])
    178 #Initialize inputs
    180 plot_model(model, to_file='rnn_model_plot.png', show_shapes=True, show_layer_names=True)

File ~/mambaforge/envs/tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py:521, in Model.build(self, input_shape)
    516     raise ValueError(
    517         ""You can only call `build()` on a model if its ""
    518         ""`call()` method accepts an `inputs` argument.""
    519     )
    520 try:
--> 521     self.call(x, **kwargs)
    522 except (tf.errors.InvalidArgumentError, TypeError) as e:
    523     raise ValueError(
    524         ""You cannot build your model by calling `build` ""
    525         ""if your layers do not support float type inputs. ""
   (...)
    529         f""`call` is: {e}.""
    530     )

File ~/[NAME REMOVED] DL/rnn_cnn_residual_model.py:41, in RNNCNNResidualModel.call(self, inputs, states, return_state, training)
     38 if states is None:
     39   states = self.lstm_cell.get_initial_state(input_timesteps)
---> 41 output = tfl.TimeDistributed(self.cnn_model)(input_images, mask=mask)
     42 fgr_output = output[0]
     43 lstm_input = tfl.Concatenate()([fgr_output, input_timesteps])

File ~/mambaforge/envs/tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/mambaforge/envs/tensorflow/lib/python3.10/site-packages/keras/src/backend.py:1534, in int_shape(x)
   1514 """"""Returns shape of tensor/variable as a tuple of int/None entries.
   1515 
   1516 Args:
   (...)
   1531 
   1532 """"""
   1533 try:
-> 1534     shape = x.shape
   1535     if not isinstance(shape, tuple):
   1536         shape = tuple(shape.as_list())

AttributeError: 'list' object has no attribute 'shape'
```
"
62003,Support Python 3.12,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15.0.dev

### Custom code

No

### OS platform and distribution

Linux Fedora 39

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

No Python3.12 compatbility and wheels.

This issue can be used as a tracking issue. This includes:

 - [ ] TensorFlow builds fully on Python 3.12
 - [ ] All TensorFlow tests pass on Python 3.12
 - [ ] All CI is run and green on Python 3.12
 - [ ] Wheels are uploaded to PyPI for [tf-nightly](https://pypi.org/project/tf-nightly/#files)
   - [ ] Linux
   - [ ] macOS
   - [ ] Windows
 - [ ] Wheels are uploaded to PyPI for at least one TensorFlow release

### Standalone code to reproduce the issue

```shell
python3.12 -m venv venv
source venv/bin/activate
pip install -U pip setuptools wheel
pip install tf-nightly --pre
```


### Relevant log output

```shell
ERROR: Could not find a version that satisfies the requirement tf-nightly (from versions: none)
ERROR: No matching distribution found for tf-nightly

Prior editiions:
- [2011/3.11](https://github.com/tensorflow/tensorflow/issues/58032#issuecomment-1279963648)
- [2010/3.10](https://github.com/tensorflow/tensorflow/issues/51776#issuecomment-934569048)
- [2009/3.9](https://github.com/tensorflow/tensorflow/issues/44485)
- [2008/3.8](https://github.com/tensorflow/tensorflow/issues/33374)

TF 2.14 has just shipped, meaning we'll likely have to wait for TF 2.15/2.16 again.
```
"
62002,cuBLAS Error in 2.14.0,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.14.0

### Custom code

No

### OS platform and distribution

Ubuntu 23.04

### Mobile device

_No response_

### Python version

3.11.5

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8 CUDNN 8.9.4

### GPU model and memory

Nvidia RTX 3080ti

### Current behavior?

in the shell terminal

install tensorflow via pip
```sh
pip install tensorflow==2.14.0
```

In the python terminal

input 

```python
import tensorflow as tf
```

then the output

```sh
2023-09-28 19:19:50.298229: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-09-28 19:19:50.298259: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-09-28 19:19:50.298302: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-09-28 19:19:50.303578: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-28 19:19:50.982905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
```



### Standalone code to reproduce the issue

```shell
no
```


### Relevant log output

_No response_"
62001,Failed to build tensorflow 2.14 on Apple silicon.,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.14

### Custom code

No

### OS platform and distribution

macOS 14.0

### Mobile device

None

### Python version

3.9, 3.10, 3.11

### Bazel version

6.1.0-homebrew

### GCC/compiler version

Apple clang version 15.0.0 (clang-1500.0.40.1)

### CUDA/cuDNN version

None

### GPU model and memory

None

### Current behavior?

I am trying to build `tensorflow 2.14` on Python `3.9`, `3.10`, and `3.11`, but I am encountering an error that says `ld: building exports trie: duplicate symbol '_copy_printf_domain'`, which is causing the build to fail.

### Standalone code to reproduce the issue

Default settings used for all options.

```shell
bazel build //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
WARNING: while reading option defaults file '/Users/sunruiqi/Desktop/tensorflow-2.14.0/.bazelrc':
  invalid command name 'startup:windows'.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=80
INFO: Reading rc options for 'build' from /Users/sunruiqi/Desktop/tensorflow-2.14.0/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/sunruiqi/Desktop/tensorflow-2.14.0/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /Users/sunruiqi/Desktop/tensorflow-2.14.0/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Users/sunruiqi/miniforge3/envs/tensorflow-macos/bin/python3 --action_env PYTHON_LIB_PATH=/Users/sunruiqi/miniforge3/envs/tensorflow-macos/lib/python3.10/site-packages --python_path=/Users/sunruiqi/miniforge3/envs/tensorflow-macos/bin/python3
INFO: Found applicable config definition build:short_logs in file /Users/sunruiqi/Desktop/tensorflow-2.14.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/sunruiqi/Desktop/tensorflow-2.14.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:macos in file /Users/sunruiqi/Desktop/tensorflow-2.14.0/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --features=archive_param_file --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=no_tfrt
INFO: Found applicable config definition build:no_tfrt in file /Users/sunruiqi/Desktop/tensorflow-2.14.0/.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
DEBUG: /Users/sunruiqi/Desktop/tensorflow-2.14.0/tensorflow/tools/toolchains/python/python_repo.bzl:21:14: 
TF_PYTHON_VERSION variable was not set correctly, using default version. 3.10 Python
will be used.

To set Python version, run
export TF_PYTHON_VERSION=3.9
WARNING: while reading option defaults file '/Users/sunruiqi/Desktop/tensorflow-2.14.0/.bazelrc':
  invalid command name 'startup:windows'.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (646 packages loaded, 41360 targets configured).
INFO: Found 1 target...
ERROR: /Users/sunruiqi/Desktop/tensorflow-2.14.0/tensorflow/python/BUILD:607:24: Linking tensorflow/python/_pywrap_tensorflow_internal.so failed: (Exit 1): cc_wrapper.sh failed: error executing command (from target //tensorflow/python:_pywrap_tensorflow_internal.so) external/local_config_cc/cc_wrapper.sh @bazel-out/darwin_arm64-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params
ld: building exports trie: duplicate symbol '_copy_printf_domain'
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Error in child process '/usr/bin/xcrun'. 1
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 4492.049s, Critical Path: 233.85s
INFO: 19651 processes: 4801 internal, 14850 local.
FAILED: Build did NOT complete successfully
```
"
62000,Tensorflow 2.14.0 published pypi metadata is missing platform specific dependencies installed via pip,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0

### Custom code

No

### OS platform and distribution

macOS-13.3-arm64-arm-64bit

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Tensorflow 2.14.0 still has wrong metadata published to pypi compared to what is actually installed
It does not list following platform specific dependencies.: 

```
Requires-Dist: tensorflow-macos (==2.14.0) ; platform_system == ""Darwin"" and platform_machine == ""arm64""
Requires-Dist: tensorflow-cpu-aws (==2.14.0) ; platform_system == ""Linux"" and (platform_machine == ""arm64"" or platform_machine == ""aarch64"")
Requires-Dist: tensorflow-intel (==2.14.0) ; platform_system == ""Windows""
```

This leads to poetry not being able to install tensorflow properly.
See #61477

### Standalone code to reproduce the issue

```shell
pip install tensorflow==2.14.0
cd python3.11/site-packages
cat tensorflow-2.14.0.dist-info/METADA

Requires-Dist: tensorflow-macos (==2.14.0) ; platform_system == ""Darwin"" and platform_machine == ""arm64""
Requires-Dist: tensorflow-cpu-aws (==2.14.0) ; platform_system == ""Linux"" and (platform_machine == ""arm64"" or platform_machine == ""aarch64"")
Requires-Dist: tensorflow-intel (==2.14.0) ; platform_system == ""Windows""
```


vs 

curl -s https://pypi.org/pypi/tensorflow/2.14.0/json | jq '.info.requires_dist'

```
[
  ""opt-einsum (>=2.3.2)"",
  ""absl-py (>=1.0.0)"",
  ""astunparse (>=1.6.0)"",
  ""flatbuffers (>=23.5.26)"",
  ""gast (!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1)"",
  ""google-pasta (>=0.1.1)"",
  ""h5py (>=2.9.0)"",
  ""libclang (>=13.0.0)"",
  ""ml-dtypes (==0.2.0)"",
  ""numpy (>=1.23.5)"",
  ""packaging"",
  ""protobuf (!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3)"",
  ""setuptools"",
  ""six (>=1.12.0)"",
  ""termcolor (>=1.1.0)"",
  ""typing-extensions (>=3.6.6)"",
  ""wrapt (<1.15,>=1.11.0)"",
  ""tensorflow-io-gcs-filesystem (>=0.23.1)"",
  ""grpcio (<2.0,>=1.24.3)"",
  ""tensorboard (<2.15,>=2.14)"",
  ""tensorflow-estimator (<2.15,>=2.14.0)"",
  ""keras (<2.15,>=2.14.0)"",
  ""nvidia-cuda-runtime-cu11 (==11.8.89) ; extra == 'and-cuda'"",
  ""nvidia-cublas-cu11 (==11.11.3.6) ; extra == 'and-cuda'"",
  ""nvidia-cufft-cu11 (==10.9.0.58) ; extra == 'and-cuda'"",
  ""nvidia-cudnn-cu11 (==8.7.0.84) ; extra == 'and-cuda'"",
  ""nvidia-curand-cu11 (==10.3.0.86) ; extra == 'and-cuda'"",
  ""nvidia-cusolver-cu11 (==11.4.1.48) ; extra == 'and-cuda'"",
  ""nvidia-cusparse-cu11 (==11.7.5.86) ; extra == 'and-cuda'"",
  ""nvidia-nccl-cu11 (==2.16.5) ; extra == 'and-cuda'"",
  ""nvidia-cuda-cupti-cu11 (==11.8.87) ; extra == 'and-cuda'"",
  ""nvidia-cuda-nvcc-cu11 (==11.8.89) ; extra == 'and-cuda'"",
  ""tensorrt (==8.5.3.1) ; extra == 'and-cuda'""
]
```
```


### Relevant log output

_No response_"
61999,"Checkpoint callback error ""save best only""  in .keras format.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.02

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Saving best model only with callback with .keras format as the whole model which works well when saving every epoch.

Locally I fixed this adding elif option to save best only as in the version with this parameter is False.

```python
 elif filepath.endswith("".keras""):
    self.model.save(filepath, overwrite=True)
```

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1e_imdDFEm-5qARqSbXm8-5JxZP43V_wg?usp=sharing
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/r.kaczmarek/repos/diarization-poc/tmp_train.py"", line 67, in <module>
    model.fit(
  File ""/home/r.kaczmarek/miniconda3/envs/tf-diarization/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/r.kaczmarek/miniconda3/envs/tf-diarization/lib/python3.10/site-packages/keras/src/saving/saving_api.py"", line 142, in save_model
    raise ValueError(
ValueError: The following argument(s) are not supported with the native Keras format: ['options']
```
"
61996,Why support for Python 3.8 has been removed in Tensorflow 2.14,"Not an issue, but a question. Just out of curiosity, Why has support for Python 3.8 been removed in Tensorflow 2.14?"
61995,can't convert keras(2.5B params) model to tflite,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22
- TensorFlow installation (pip package or built from source): pip 
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.14.0 or tf-nightly

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```
import tensorflow as tf

keras_model = tf.keras.models.load_model(keras_model_filename)
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
tflite_model = converter.convert()

file = open(tflite_model_filename, 'wbâ€˜ï¼‰
file.write(tflite_model)

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
![image](https://github.com/tensorflow/tensorflow/assets/68658008/dc88001c-02c2-42c6-8c74-8de447c5a776)
"
61993,"Packaging Problem with  ""pip install -U tensorflow[and-cuda]"" Installs tensorflow 2.13.1","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0

### Custom code

No

### OS platform and distribution

Any Linux

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

docker run -it --entrypoint /bin/bash --gpus all python:3.11-bookworm
pip install -U tensorflow[and-cuda]
wait...
root@16cbc0c15cdd:/# pip list
Package                      Version
---------------------------- ---------
absl-py                      2.0.0
astunparse                   1.6.3
cachetools                   5.3.1
certifi                      2023.7.22
charset-normalizer           3.2.0
flatbuffers                  23.5.26
gast                         0.4.0
google-auth                  2.23.1
google-auth-oauthlib         1.0.0
google-pasta                 0.2.0
grpcio                       1.58.0
h5py                         3.9.0
idna                         3.4
keras                        2.13.1
libclang                     16.0.6
Markdown                     3.4.4
MarkupSafe                   2.1.3
numpy                        1.24.3
oauthlib                     3.2.2
opt-einsum                   3.3.0
packaging                    23.1
pip                          23.2.1
protobuf                     4.24.3
pyasn1                       0.5.0
pyasn1-modules               0.3.0
requests                     2.31.0
requests-oauthlib            1.3.1
rsa                          4.9
setuptools                   65.5.1
six                          1.16.0
tensorboard                  2.13.0
tensorboard-data-server      0.7.1
tensorflow                   2.13.1
tensorflow-estimator         2.13.0
tensorflow-io-gcs-filesystem 0.34.0
termcolor                    2.3.0
typing_extensions            4.5.0
urllib3                      2.0.5
Werkzeug                     2.3.7
wheel                        0.41.2
wrapt                        1.15.0
root@16cbc0c15cdd:/#


### Standalone code to reproduce the issue

```shell
Not a code issue
```


### Relevant log output

```shell
root@f83803b4dfd4:/# pip install -U tensorflow[and-cuda]
Collecting tensorflow[and-cuda]
  Obtaining dependency information for tensorflow[and-cuda] from https://files.pythonhosted.org/packages/09/63/25e76075081ea98ec48f23929cefee58be0b42212e38074a9ec5c19e838c/tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Using cached tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Collecting absl-py>=1.0.0 (from tensorflow[and-cuda])
  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata
  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)
Collecting astunparse>=1.6.0 (from tensorflow[and-cuda])
  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Collecting flatbuffers>=23.5.26 (from tensorflow[and-cuda])
  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata
  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)
Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow[and-cuda])
  Using cached gast-0.5.4-py3-none-any.whl (19 kB)
Collecting google-pasta>=0.1.1 (from tensorflow[and-cuda])
  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)
Collecting h5py>=2.9.0 (from tensorflow[and-cuda])
  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/a7/d9/ac660616671e30d70c091e46ed4fdc50df48ca79b1ac99df5499a45de128/h5py-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Using cached h5py-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)
Collecting libclang>=13.0.0 (from tensorflow[and-cuda])
  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/ea/df/55525e489c43f9dbb6c8ea27d8a567b3dcd18a22f3c45483055f5ca6611d/libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata
  Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)
Collecting ml-dtypes==0.2.0 (from tensorflow[and-cuda])
  Obtaining dependency information for ml-dtypes==0.2.0 from https://files.pythonhosted.org/packages/87/91/d57c2d22e4801edeb7f3e7939214c0ea8a28c6e16f85208c2df2145e0213/ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)
Collecting numpy>=1.23.5 (from tensorflow[and-cuda])
  Obtaining dependency information for numpy>=1.23.5 from https://files.pythonhosted.org/packages/c4/36/161e2f8110f8c49e59f6107bd6da4257d30aff9f06373d0471811f73dcc5/numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Using cached numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)
Collecting opt-einsum>=2.3.2 (from tensorflow[and-cuda])
  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)
Collecting packaging (from tensorflow[and-cuda])
  Using cached packaging-23.1-py3-none-any.whl (48 kB)
Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow[and-cuda])
  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/bb/c3/6a06208ecf0934ecaf509b51c52a6cf688586f54ae81ac65c56124571494/protobuf-4.24.3-cp37-abi3-manylinux2014_x86_64.whl.metadata
  Using cached protobuf-4.24.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)
Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from tensorflow[and-cuda]) (68.2.2)
Collecting six>=1.12.0 (from tensorflow[and-cuda])
  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)
Collecting termcolor>=1.1.0 (from tensorflow[and-cuda])
  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)
Collecting typing-extensions>=3.6.6 (from tensorflow[and-cuda])
  Obtaining dependency information for typing-extensions>=3.6.6 from https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl.metadata
  Using cached typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)
Collecting wrapt<1.15,>=1.11.0 (from tensorflow[and-cuda])
  Using cached wrapt-1.14.1.tar.gz (50 kB)
  Preparing metadata (setup.py) ... done
Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow[and-cuda])
  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/4c/64/245746084cdd5fafa680a6e7effeecf87abeeac2796decfa835a99b397c7/tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata
  Using cached tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)
Collecting grpcio<2.0,>=1.24.3 (from tensorflow[and-cuda])
  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/98/18/10a3af9b1f2521ad765e9fd518783b8883268357fef397d1b57585d1bef8/grpcio-1.58.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Using cached grpcio-1.58.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)
Collecting tensorboard<2.15,>=2.14 (from tensorflow[and-cuda])
  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/bc/a2/ff5f4c299eb37c95299a76015da3f30211468e29d8d6f1d011683279baee/tensorboard-2.14.0-py3-none-any.whl.metadata
  Using cached tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)
Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow[and-cuda])
  Obtaining dependency information for tensorflow-estimator<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/d1/da/4f264c196325bb6e37a6285caec5b12a03def489b57cc1fdac02bb6272cd/tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata
  Using cached tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting keras<2.15,>=2.14.0 (from tensorflow[and-cuda])
  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata
  Using cached keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)
Collecting nvidia-cuda-runtime-cu11==11.8.89 (from tensorflow[and-cuda])
  Downloading nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 875.6/875.6 kB 3.6 MB/s eta 0:00:00
Collecting nvidia-cublas-cu11==11.11.3.6 (from tensorflow[and-cuda])
  Downloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 417.9/417.9 MB 7.3 MB/s eta 0:00:00
Collecting nvidia-cufft-cu11==10.9.0.58 (from tensorflow[and-cuda])
  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 168.4/168.4 MB 12.6 MB/s eta 0:00:00
Collecting nvidia-cudnn-cu11==8.7.0.84 (from tensorflow[and-cuda])
  Downloading nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 728.5/728.5 MB 4.2 MB/s eta 0:00:00
Collecting nvidia-curand-cu11==10.3.0.86 (from tensorflow[and-cuda])
  Downloading nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 58.1/58.1 MB 28.7 MB/s eta 0:00:00
Collecting nvidia-cusolver-cu11==11.4.1.48 (from tensorflow[and-cuda])
  Downloading nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 128.2/128.2 MB 15.8 MB/s eta 0:00:00
Collecting nvidia-cusparse-cu11==11.7.5.86 (from tensorflow[and-cuda])
  Downloading nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 204.1/204.1 MB 10.4 MB/s eta 0:00:00
Collecting nvidia-nccl-cu11==2.16.5 (from tensorflow[and-cuda])
  Downloading nvidia_nccl_cu11-2.16.5-py3-none-manylinux1_x86_64.whl (210.3 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 210.3/210.3 MB 5.7 MB/s eta 0:00:00
Collecting nvidia-cuda-cupti-cu11==11.8.87 (from tensorflow[and-cuda])
  Downloading nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13.1/13.1 MB 2.3 MB/s eta 0:00:00
Collecting nvidia-cuda-nvcc-cu11==11.8.89 (from tensorflow[and-cuda])
  Downloading nvidia_cuda_nvcc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (19.5 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 19.5/19.5 MB 30.9 MB/s eta 0:00:00
INFO: pip is looking at multiple versions of tensorflow[and-cuda] to determine which version is compatible with other requirements. This could take a while.
Collecting tensorflow[and-cuda]
  Obtaining dependency information for tensorflow[and-cuda] from https://files.pythonhosted.org/packages/df/0c/22cb1c82e0fbaca8c00c3e5e8f9cd1e1b618837f1c5641914fe251bdc9a5/tensorflow-2.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading tensorflow-2.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)
WARNING: tensorflow 2.13.1 does not provide the extra 'and-cuda'
Collecting gast<=0.4.0,>=0.2.1 (from tensorflow[and-cuda])
  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)
Collecting keras<2.14,>=2.13.1 (from tensorflow[and-cuda])
  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata
  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)
Collecting numpy<=1.24.3,>=1.22 (from tensorflow[and-cuda])
  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17.3/17.3 MB 32.1 MB/s eta 0:00:00
Collecting tensorboard<2.14,>=2.13 (from tensorflow[and-cuda])
  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5.6/5.6 MB 31.4 MB/s eta 0:00:00
Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow[and-cuda])
  Obtaining dependency information for tensorflow-estimator<2.14,>=2.13.0 from https://files.pythonhosted.org/packages/72/5c/c318268d96791c6222ad7df1651bbd1b2409139afeb6f468c0f327177016/tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata
  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow[and-cuda])
  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)
Collecting wrapt>=1.11.0 (from tensorflow[and-cuda])
  Downloading wrapt-1.15.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 78.9/78.9 kB 1.6 MB/s eta 0:00:00
Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.41.2)
Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/23/e4/abbb8763fdf6279c471443251b3f847ee9a172d1776742b266fe6de7ac86/google_auth-2.23.1-py2.py3-none-any.whl.metadata
  Using cached google_auth-2.23.1-py2.py3-none-any.whl.metadata (4.2 kB)
Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)
Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata
  Using cached Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)
Collecting requests<3,>=2.21.0 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for requests<3,>=2.21.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata
  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)
Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata
  Using cached tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)
Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/9b/59/a7c32e3d8d0e546a206e0552a2c04444544f15c1da4a01df8938d20c6ffc/werkzeug-2.3.7-py3-none-any.whl.metadata
  Using cached werkzeug-2.3.7-py3-none-any.whl.metadata (4.1 kB)
Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata
  Using cached cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)
Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)
Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Using cached rsa-4.9-py3-none-any.whl (34 kB)
Collecting urllib3>=2.0.5 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for urllib3>=2.0.5 from https://files.pythonhosted.org/packages/37/dc/399e63f5d1d96bb643404ee830657f4dfcf8503f5ba8fa3c6d465d0c57fe/urllib3-2.0.5-py3-none-any.whl.metadata
  Using cached urllib3-2.0.5-py3-none-any.whl.metadata (6.6 kB)
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)
Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/bc/85/ef25d4ba14c7653c3020a1c6e1a7413e6791ef36a0ac177efa605fc2c737/charset_normalizer-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Using cached charset_normalizer-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)
Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Using cached idna-3.4-py3-none-any.whl (61 kB)
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata
  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)
Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for MarkupSafe>=2.1.1 from https://files.pythonhosted.org/packages/fe/21/2eff1de472ca6c99ec3993eab11308787b9879af9ca8bbceb4868cf4f2ca/MarkupSafe-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Using cached MarkupSafe-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)
Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)
Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 130.2/130.2 kB 10.0 MB/s eta 0:00:00
Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)
Downloading grpcio-1.58.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5.3/5.3 MB 39.1 MB/s eta 0:00:00
Downloading h5py-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4.8/4.8 MB 37.4 MB/s eta 0:00:00
Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.7/1.7 MB 33.2 MB/s eta 0:00:00
Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 22.9/22.9 MB 32.5 MB/s eta 0:00:00
Downloading protobuf-4.24.3-cp37-abi3-manylinux2014_x86_64.whl (311 kB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 311.6/311.6 kB 13.7 MB/s eta 0:00:00
Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 440.8/440.8 kB 25.8 MB/s eta 0:00:00
Downloading tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.4/2.4 MB 24.8 MB/s eta 0:00:00
Downloading tensorflow-2.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.7 MB)
   â”â”â”â”â•¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 55.6/479.7 MB 49.5 MB/s eta 0:00:09
ERROR: Operation cancelled by user
root@f83803b4dfd4:/# ^C
root@f83803b4dfd4:/# pip install -U tensorflow[and-cuda] ^C
root@f83803b4dfd4:/# exit
exit
PS C:\Users\edbrown> docker run -it --entrypoint /bin/bash --gpus all python:3.11-bookworm
root@16cbc0c15cdd:/# pip install -U tensorflow[and-cuda]
Collecting tensorflow[and-cuda]
  Obtaining dependency information for tensorflow[and-cuda] from https://files.pythonhosted.org/packages/09/63/25e76075081ea98ec48f23929cefee58be0b42212e38074a9ec5c19e838c/tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Collecting absl-py>=1.0.0 (from tensorflow[and-cuda])
  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata
  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)
Collecting astunparse>=1.6.0 (from tensorflow[and-cuda])
  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Collecting flatbuffers>=23.5.26 (from tensorflow[and-cuda])
  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata
  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)
Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow[and-cuda])
  Downloading gast-0.5.4-py3-none-any.whl (19 kB)
Collecting google-pasta>=0.1.1 (from tensorflow[and-cuda])
  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 57.5/57.5 kB 764.8 kB/s eta 0:00:00
Collecting h5py>=2.9.0 (from tensorflow[and-cuda])
  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/a7/d9/ac660616671e30d70c091e46ed4fdc50df48ca79b1ac99df5499a45de128/h5py-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading h5py-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)
Collecting libclang>=13.0.0 (from tensorflow[and-cuda])
  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/ea/df/55525e489c43f9dbb6c8ea27d8a567b3dcd18a22f3c45483055f5ca6611d/libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata
  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)
Collecting ml-dtypes==0.2.0 (from tensorflow[and-cuda])
  Obtaining dependency information for ml-dtypes==0.2.0 from https://files.pythonhosted.org/packages/87/91/d57c2d22e4801edeb7f3e7939214c0ea8a28c6e16f85208c2df2145e0213/ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)
Collecting numpy>=1.23.5 (from tensorflow[and-cuda])
  Obtaining dependency information for numpy>=1.23.5 from https://files.pythonhosted.org/packages/c4/36/161e2f8110f8c49e59f6107bd6da4257d30aff9f06373d0471811f73dcc5/numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 58.5/58.5 kB 2.2 MB/s eta 0:00:00
Collecting opt-einsum>=2.3.2 (from tensorflow[and-cuda])
  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 65.5/65.5 kB 3.1 MB/s eta 0:00:00
Collecting packaging (from tensorflow[and-cuda])
  Downloading packaging-23.1-py3-none-any.whl (48 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 48.9/48.9 kB 2.9 MB/s eta 0:00:00
Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow[and-cuda])
  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/bb/c3/6a06208ecf0934ecaf509b51c52a6cf688586f54ae81ac65c56124571494/protobuf-4.24.3-cp37-abi3-manylinux2014_x86_64.whl.metadata
  Downloading protobuf-4.24.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)
Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from tensorflow[and-cuda]) (65.5.1)
Collecting six>=1.12.0 (from tensorflow[and-cuda])
  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)
Collecting termcolor>=1.1.0 (from tensorflow[and-cuda])
  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)
Collecting typing-extensions>=3.6.6 (from tensorflow[and-cuda])
  Obtaining dependency information for typing-extensions>=3.6.6 from https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl.metadata
  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)
Collecting wrapt<1.15,>=1.11.0 (from tensorflow[and-cuda])
  Downloading wrapt-1.14.1.tar.gz (50 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 50.9/50.9 kB 3.1 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow[and-cuda])
  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/4c/64/245746084cdd5fafa680a6e7effeecf87abeeac2796decfa835a99b397c7/tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata
  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)
Collecting grpcio<2.0,>=1.24.3 (from tensorflow[and-cuda])
  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/98/18/10a3af9b1f2521ad765e9fd518783b8883268357fef397d1b57585d1bef8/grpcio-1.58.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading grpcio-1.58.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)
Collecting tensorboard<2.15,>=2.14 (from tensorflow[and-cuda])
  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/bc/a2/ff5f4c299eb37c95299a76015da3f30211468e29d8d6f1d011683279baee/tensorboard-2.14.0-py3-none-any.whl.metadata
  Downloading tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)
Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow[and-cuda])
  Obtaining dependency information for tensorflow-estimator<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/d1/da/4f264c196325bb6e37a6285caec5b12a03def489b57cc1fdac02bb6272cd/tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata
  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting keras<2.15,>=2.14.0 (from tensorflow[and-cuda])
  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata
  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)
Collecting nvidia-cuda-runtime-cu11==11.8.89 (from tensorflow[and-cuda])
  Downloading nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 875.6/875.6 kB 5.4 MB/s eta 0:00:00
Collecting nvidia-cublas-cu11==11.11.3.6 (from tensorflow[and-cuda])
  Downloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 417.9/417.9 MB 7.4 MB/s eta 0:00:00
Collecting nvidia-cufft-cu11==10.9.0.58 (from tensorflow[and-cuda])
  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 168.4/168.4 MB 18.4 MB/s eta 0:00:00
Collecting nvidia-cudnn-cu11==8.7.0.84 (from tensorflow[and-cuda])
  Downloading nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 728.5/728.5 MB 6.4 MB/s eta 0:00:00
Collecting nvidia-curand-cu11==10.3.0.86 (from tensorflow[and-cuda])
  Downloading nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 58.1/58.1 MB 36.6 MB/s eta 0:00:00
Collecting nvidia-cusolver-cu11==11.4.1.48 (from tensorflow[and-cuda])
  Downloading nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 128.2/128.2 MB 19.2 MB/s eta 0:00:00
Collecting nvidia-cusparse-cu11==11.7.5.86 (from tensorflow[and-cuda])
  Downloading nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 204.1/204.1 MB 10.7 MB/s eta 0:00:00
Collecting nvidia-nccl-cu11==2.16.5 (from tensorflow[and-cuda])
  Downloading nvidia_nccl_cu11-2.16.5-py3-none-manylinux1_x86_64.whl (210.3 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 210.3/210.3 MB 11.3 MB/s eta 0:00:00
Collecting nvidia-cuda-cupti-cu11==11.8.87 (from tensorflow[and-cuda])
  Downloading nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13.1/13.1 MB 30.8 MB/s eta 0:00:00
Collecting nvidia-cuda-nvcc-cu11==11.8.89 (from tensorflow[and-cuda])
  Downloading nvidia_cuda_nvcc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (19.5 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 19.5/19.5 MB 29.9 MB/s eta 0:00:00
INFO: pip is looking at multiple versions of tensorflow[and-cuda] to determine which version is compatible with other requirements. This could take a while.
Collecting tensorflow[and-cuda]
  Obtaining dependency information for tensorflow[and-cuda] from https://files.pythonhosted.org/packages/df/0c/22cb1c82e0fbaca8c00c3e5e8f9cd1e1b618837f1c5641914fe251bdc9a5/tensorflow-2.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading tensorflow-2.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)
WARNING: tensorflow 2.13.1 does not provide the extra 'and-cuda'
Collecting gast<=0.4.0,>=0.2.1 (from tensorflow[and-cuda])
  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)
Collecting keras<2.14,>=2.13.1 (from tensorflow[and-cuda])
  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata
  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)
Collecting numpy<=1.24.3,>=1.22 (from tensorflow[and-cuda])
  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17.3/17.3 MB 29.0 MB/s eta 0:00:00
Collecting tensorboard<2.14,>=2.13 (from tensorflow[and-cuda])
  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5.6/5.6 MB 35.4 MB/s eta 0:00:00
Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow[and-cuda])
  Obtaining dependency information for tensorflow-estimator<2.14,>=2.13.0 from https://files.pythonhosted.org/packages/72/5c/c318268d96791c6222ad7df1651bbd1b2409139afeb6f468c0f327177016/tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata
  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow[and-cuda])
  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)
Collecting wrapt>=1.11.0 (from tensorflow[and-cuda])
  Downloading wrapt-1.15.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 78.9/78.9 kB 8.7 MB/s eta 0:00:00
Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.41.2)
Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/23/e4/abbb8763fdf6279c471443251b3f847ee9a172d1776742b266fe6de7ac86/google_auth-2.23.1-py2.py3-none-any.whl.metadata
  Downloading google_auth-2.23.1-py2.py3-none-any.whl.metadata (4.2 kB)
Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)
Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata
  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)
Collecting requests<3,>=2.21.0 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for requests<3,>=2.21.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata
  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)
Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata
  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)
Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/9b/59/a7c32e3d8d0e546a206e0552a2c04444544f15c1da4a01df8938d20c6ffc/werkzeug-2.3.7-py3-none-any.whl.metadata
  Downloading werkzeug-2.3.7-py3-none-any.whl.metadata (4.1 kB)
Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata
  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)
Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 181.3/181.3 kB 17.2 MB/s eta 0:00:00
Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading rsa-4.9-py3-none-any.whl (34 kB)
Collecting urllib3>=2.0.5 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for urllib3>=2.0.5 from https://files.pythonhosted.org/packages/37/dc/399e63f5d1d96bb643404ee830657f4dfcf8503f5ba8fa3c6d465d0c57fe/urllib3-2.0.5-py3-none-any.whl.metadata
  Downloading urllib3-2.0.5-py3-none-any.whl.metadata (6.6 kB)
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)
Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/bc/85/ef25d4ba14c7653c3020a1c6e1a7413e6791ef36a0ac177efa605fc2c737/charset_normalizer-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading charset_normalizer-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)
Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading idna-3.4-py3-none-any.whl (61 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 61.5/61.5 kB 7.3 MB/s eta 0:00:00
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata
  Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)
Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Obtaining dependency information for MarkupSafe>=2.1.1 from https://files.pythonhosted.org/packages/fe/21/2eff1de472ca6c99ec3993eab11308787b9879af9ca8bbceb4868cf4f2ca/MarkupSafe-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading MarkupSafe-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)
Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 83.9/83.9 kB 9.2 MB/s eta 0:00:00
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 151.7/151.7 kB 14.8 MB/s eta 0:00:00
Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 130.2/130.2 kB 416.7 kB/s eta 0:00:00
Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)
Downloading grpcio-1.58.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5.3/5.3 MB 38.2 MB/s eta 0:00:00
Downloading h5py-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4.8/4.8 MB 38.1 MB/s eta 0:00:00
Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.7/1.7 MB 26.1 MB/s eta 0:00:00
Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 22.9/22.9 MB 31.9 MB/s eta 0:00:00
Downloading protobuf-4.24.3-cp37-abi3-manylinux2014_x86_64.whl (311 kB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 311.6/311.6 kB 10.6 MB/s eta 0:00:00
Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 440.8/440.8 kB 14.4 MB/s eta 0:00:00
Downloading tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.4/2.4 MB 26.5 MB/s eta 0:00:00
Downloading tensorflow-2.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.7 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 479.7/479.7 MB 6.9 MB/s eta 0:00:00
Downloading google_auth-2.23.1-py2.py3-none-any.whl (181 kB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 181.9/181.9 kB 5.8 MB/s eta 0:00:00
Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 94.2/94.2 kB 3.6 MB/s eta 0:00:00
Downloading requests-2.31.0-py3-none-any.whl (62 kB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 62.6/62.6 kB 2.2 MB/s eta 0:00:00
Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6.6/6.6 MB 42.5 MB/s eta 0:00:00
Downloading werkzeug-2.3.7-py3-none-any.whl (242 kB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 242.2/242.2 kB 6.6 MB/s eta 0:00:00
Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)
Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 158.3/158.3 kB 6.1 MB/s eta 0:00:00
Downloading charset_normalizer-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 199.6/199.6 kB 7.2 MB/s eta 0:00:00
Downloading MarkupSafe-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)
Downloading urllib3-2.0.5-py3-none-any.whl (123 kB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 123.8/123.8 kB 9.7 MB/s eta 0:00:00
Installing collected packages: libclang, flatbuffers, wrapt, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, pyasn1, protobuf, packaging, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, rsa, requests, pyasn1-modules, opt-einsum, h5py, google-pasta, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow
Successfully installed MarkupSafe-2.1.3 absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.1 certifi-2023.7.22 charset-normalizer-3.2.0 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.23.1 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.58.0 h5py-3.9.0 idna-3.4 keras-2.13.1 libclang-16.0.6 markdown-3.4.4 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.1 protobuf-4.24.3 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 six-1.16.0 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.3.0 typing-extensions-4.5.0 urllib3-2.0.5 werkzeug-2.3.7 wrapt-1.15.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
root@16cbc0c15cdd:/# pip list
Package                      Version
---------------------------- ---------
absl-py                      2.0.0
astunparse                   1.6.3
cachetools                   5.3.1
certifi                      2023.7.22
charset-normalizer           3.2.0
flatbuffers                  23.5.26
gast                         0.4.0
google-auth                  2.23.1
google-auth-oauthlib         1.0.0
google-pasta                 0.2.0
grpcio                       1.58.0
h5py                         3.9.0
idna                         3.4
keras                        2.13.1
libclang                     16.0.6
Markdown                     3.4.4
MarkupSafe                   2.1.3
numpy                        1.24.3
oauthlib                     3.2.2
opt-einsum                   3.3.0
packaging                    23.1
pip                          23.2.1
protobuf                     4.24.3
pyasn1                       0.5.0
pyasn1-modules               0.3.0
requests                     2.31.0
requests-oauthlib            1.3.1
rsa                          4.9
setuptools                   65.5.1
six                          1.16.0
tensorboard                  2.13.0
tensorboard-data-server      0.7.1
tensorflow                   2.13.1
tensorflow-estimator         2.13.0
tensorflow-io-gcs-filesystem 0.34.0
termcolor                    2.3.0
typing_extensions            4.5.0
urllib3                      2.0.5
Werkzeug                     2.3.7
wheel                        0.41.2
wrapt                        1.15.0
root@16cbc0c15cdd:/#
```
"
61992,TFLite model produces wrong output for constant addition,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.15.0-dev20230926

### 2. Code

The TensorFlow Lite model in the example below should output `x1+x2+x2=7+1+1=9`. However, it produces a wrong output `x1+x1+x2=7+7+1=15`. This indicates that it confuses the order of the two inputs.

```
import tensorflow as tf
import numpy as np

a = tf.constant(7.0, shape=[1])
b = tf.constant(1.0, shape=[1])
input_data = [a, b]


def _evaluateTFLiteModel(tflite_model, input_data):
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Test model on random input data.
    for i in range(len(input_details)):
        # input_shape = input_details[i]['shape']
        # input_data_i = np.array(np.random.random_sample(input_shape), dtype=np.float32)
        interpreter.set_tensor(input_details[i]['index'], input_data[i])

    interpreter.invoke()

    # The function `get_tensor()` returns a copy of the tensor data.
    # Use `tensor()` in order to get a pointer to the tensor.
    output_data = [interpreter.get_tensor(output_details[i]['index'])
                   for i in range(len(output_details))]
    return output_data


class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()

    @tf.function(input_signature=[tf.TensorSpec(shape=x.shape, dtype=x.dtype) for x in input_data])
    def call(self, x1, x2):
        return ((x1 + x2) + x2)

m = Model()

converter = tf.lite.TFLiteConverter.from_keras_model(m)
tflite_model = converter.convert()

print(_evaluateTFLiteModel(tflite_model, input_data))
```
Output:
```
[array([15.], dtype=float32)]
```
Keras model:
```
import tensorflow as tf
print(tf.__version__)

import tensorflow as tf
import numpy as np


class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()

    def call(self, x1, x2):
        return ((x1 + x2) + x2)



m = Model()

a = tf.constant(7.0, shape=[1])
b = tf.constant(1.0, shape=[1])
input_data = [a, b]
print(m(a, b))
```
Output:
```
tf.Tensor([9.], shape=(1,), dtype=float32)
```

### 3. Failure after conversion
Wrong results.
"
61987,con't build_pip_package,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf1.14.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6.9

### Bazel version

bazel 0.24.1

### GCC/compiler version

gcc 4.8

### CUDA/cuDNN version

_No response_

### GPU model and memory

 intel i7-9750H   12g

### Current behavior?

tensorflow/python/lib/core/bfloat16.cc:608:60: note: no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void ()(char**, const long int, const long int*, void*)}'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 4163.570s, Critical Path: 161.91s
INFO: 5035 processes: 5035 local.
FAILED: Build did NOT complete successfully

### Standalone code to reproduce the issue

```shell
bazel build --jobs=6 //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_"
61986,tensorrt==8.5.3.1 from [and-cuda] not available in Python 3.11,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04


### Python version

3.11


### Current behavior?

tensorrt==8.5.3.1, a pinned dependency in TensorFlow[and-cuda], is only available up to Python 3.10 and therefore fails installing with Python 3.11

### Standalone code to reproduce the issue

```shell
Installation issue
`pip install ""tensorflow[and-cuda]>=2.14.0""` with Python 3.11+
```


### Relevant log output

```shell
ERROR: Could not find a version that satisfies the requirement tensorrt==8.5.3.1; extra == ""and-cuda"" (from tensorflow[and-cuda]) (from versions: 0.0.1.dev5, 0.0.1, 8.6.1, 8.6.1.post1, 9.0.0.post11.dev1, 9.0.0.post12.dev1, 9.0.1.post11.dev4, 9.0.1.post12.dev4)
ERROR: No matching distribution found for tensorrt==8.5.3.1; extra == ""and-cuda""
```
```
"
61985,About int16,"### 1. System information

- Linux Ubuntu 16.04
- TensorFlow lite
### 2. Issue

I would like to inquire if my input and output data can only be int16. Can I write ""converter. reference_input_type=tf. int16"" when converting tfliteï¼Ÿ or is there any other good method recommended?

Thanks!


"
61984,Could not find a version that satisfies the requirement tensorflow-macos==2.14.0;,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

No

### OS platform and distribution

mac os 13.5.1

### Mobile device

Apple silicon

### Python version

3.11.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Collecting tensorflow==2.14.0
  Obtaining dependency information for tensorflow==2.14.0 from https://files.pythonhosted.org/packages/de/ea/90267db2c02fb61f4d03b9645c7446d3cbca6d5c08522e889535c88edfcd/tensorflow-2.14.0-cp311-cp311-macosx_12_0_arm64.whl.metadata
  Using cached tensorflow-2.14.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)
INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.
ERROR: Could not find a version that satisfies the requirement tensorflow-macos==2.14.0; platform_system == ""Darwin"" and platform_machine == ""arm64"" (from tensorflow) (from versions: 2.12.0, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.14.0rc0, 2.14.0rc1)
ERROR: No matching distribution found for tensorflow-macos==2.14.0; platform_system == ""Darwin"" and platform_machine == ""arm64""

### Standalone code to reproduce the issue

```shell
pip install tensorflow==2.14.0
```


### Relevant log output

_No response_"
61980,ModuleNotFoundError: No module named 'resource',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.11.5

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Python test code
`import tensorflow_datasets as tfds`

Produces
ModuleNotFoundError: No module named 'resource'

### Standalone code to reproduce the issue

```shell
Python test code
import tensorflow_datasets as tfds
```


### Relevant log output

```shell
c:\xxxx\Devel Files\Other\Work\Python\image_class_transferlearning>python64.bat test.py
Traceback (most recent call last):
  File ""c:\xxxx\Devel Files\Other\Work\Python\image_class_transferlearning\test.py"", line 1, in <module>
    import tensorflow_datasets as tfds
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\__init__.py"", line 43, in <module>
    import tensorflow_datasets.core.logging as _tfds_logging
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\core\__init__.py"", line 22, in <module>
    from tensorflow_datasets.core import community
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\core\community\__init__.py"", line 18, in <module>
    from tensorflow_datasets.core.community.huggingface_wrapper import mock_builtin_to_use_gfile
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\core\community\huggingface_wrapper.py"", line 31, in <module>
    from tensorflow_datasets.core import dataset_builder
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 44, in <module>
    from tensorflow_datasets.core import split_builder as split_builder_lib
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\core\split_builder.py"", line 37, in <module>
    from tensorflow_datasets.core import writer as writer_lib
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\core\writer.py"", line 33, in <module>
    from tensorflow_datasets.core import shuffle
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\core\shuffle.py"", line 20, in <module>
    import resource
ModuleNotFoundError: No module named 'resource'
```
"
61979,TfLiteGpuDelegate Prepare: delegate is not initialized,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 12
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): v2.13.0


**Provide the text output from tflite_convert**

```
# Copy and paste here
2023-09-26 13:31:37.492599: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-26 13:31:37.500521: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2168003d970 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2023-09-26 13:31:37.500975: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2023-09-26 13:31:37.599984: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2023-09-26 13:31:37.600138: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.

```

**Standalone code to reproduce the issue** 
**java code**
Interpreter.Options options = new Interpreter.Options();
options.addDelegate(new GpuDelegate());
Interpreter  interpreter = new Interpreter(model, options);

**Convert code**
output_name = ['add_1']

input_shape = [288, 288, 3]

converter = tf.lite.TFLiteConverter.from_frozen_graph('model.pb', input_arrays=[""input_image""], output_arrays=output_name, input_shapes={""input_image"": input_shape})

tflite_model = converter.convert()
open('model.tflite', ""wb"").write(tflite_model)

GraphDef  model:
[GraphDef.zip](https://github.com/tensorflow/tensorflow/files/12723201/GraphDef.zip)

TFlite model:
[tflite.zip](https://github.com/tensorflow/tensorflow/files/12723204/tflite.zip)


**Any other info / logs**
**Java issue**
13:40:05.480  W  java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Init: DEPTHWISE_CONV_2D: ReadNonConstantTensor: value is a constant tensor: 4
13:40:05.480  W  TfLiteGpuDelegate Prepare: delegate is not initialized
13:40:05.480  W  Node number 69 (TfLiteGpuDelegateV2) failed to prepare.
13:40:05.480  W  Restored original execution plan after delegate application failure.
13:40:05.480  W  	at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)
13:40:05.480  W  	at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:110)
13:40:05.480  W  	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:58)
13:40:05.480  W  	at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:32)
13:40:05.480  W  	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:202)
13:40:05.480  W  	at com.unt.studio.pumpkin.core.style.DCTNet.init(DCTNet.java:33)
13:40:05.480  W  	at com.unt.studio.pumpkin.ui.fragment.StyleTransferFragment$1.run(StyleTransferFragment.java:184)
13:40:05.480  W  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
13:40:05.480  W  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
13:40:05.480  W  	at java.lang.Thread.run(Thread.java:920)"
61978,Build from source 2.13.0 fails on Rocky Linux 8.8,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Rocky Linux 8.8

### Mobile device

_No response_

### Python version

3.10.2

### Bazel version

5.3.0

### GCC/compiler version

clang 16.0.1 or gcc 11.2.0

### CUDA/cuDNN version

cuda 11.7.1 /  cuDNN 8.5.0.96

### GPU model and memory

_No response_

### Current behavior?

1. load modules (all built locally on the host  and provide an access to the specified software) 
 **module load  bazel/5.3.0    gcc/11.2.0    llvm/16.0.1   clang/16.0.1  python/3.10.2   cuda/11.7.1   tensorRT/8.4.2.4**  

2.  Configure tensroflow:
**./configure**

Resulting  .tf_configure.bazelrc is:
build --action_env PYTHON_BIN_PATH=""/opt/apps/python/3.10.2/bin/python3""
build --action_env PYTHON_LIB_PATH=""/opt/apps/python/3.10.2/lib/python3.10/site-packages""
build --python_path=""/opt/apps/python/3.10.2/bin/python3""
build --config=tensorrt
build --action_env TF_CUDA_VERSION=""11""
build --action_env TF_CUDNN_VERSION=""8""
build --action_env TF_TENSORRT_VERSION=""8""
build --action_env TF_NCCL_VERSION=""""
build --action_env TF_CUDA_PATHS=""/opt/apps/cuda/11.7.1,/opt/apps/tensorRT/8.4.2.4,/usr""
build --action_env CUDA_TOOLKIT_PATH=""/opt/apps/cuda/11.7.1""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""7.0""
build --action_env LD_LIBRARY_PATH=""/opt/apps/tensorRT/8.4.2.4/lib:/opt/apps/cuda/11.7.1/lib64:/opt/apps/cuda/11.7.1/nvvm/lib64:/opt/apps/cuda/11.7.1/cublas/lib64:/opt/apps/cuda/11.7.1/extras/CUPTI/lib64:/opt/apps/cuda/11.7.1
/extras/Debugger/lib64:/opt/apps/python/3.10.2/lib:/opt/apps/clang/16.0.1/lib:/opt/apps/llvm/16.0.1/lib:/opt/apps/gcc/11.2.0/lib64:/opt/apps/gcc/11.2.0/lib:/opt/apps/gcc/11.2.0/lib/gcc/x86_64-pc-linux-gnu/11.2.0""
build --config=cuda_clang
build --action_env CLANG_CUDA_COMPILER_PATH=""/opt/apps/clang/16.0.1/bin/clang""
build --config=cuda_clang
build:opt --copt=-mavx2
build:opt --host_copt=-mavx2
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_env=LD_LIBRARY_PATH
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-no_gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-no_gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-no_gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-no_gpu,-v1only

3 Run bazel build
**nohup bazel build --config=opt --jobs=8 --verbose_failures --verbose_explanations \
            --explain=/tmp/explain.txt //tensorflow/tools/pip_package:build_pip_package > build-out.txt &**

The build fails with an error. Attaching output log  files build-out.txt (collect errors) and explain.txt ( collect verbose output)
per above command:
[build-out.txt](https://github.com/tensorflow/tensorflow/files/12719249/build-out.txt)
[explain.txt](https://github.com/tensorflow/tensorflow/files/12719259/explain.txt)

If i compile without cuda/tensorRT, the clang compiler is not chosen and there is no way to choose is per current configure.py
as clang choose seem to be used only for gpu-enabled builds. In this case, gcc 11.2.0 is used and the error happens in a different area.

I tried to use different options adding **--config=cuda** or **--cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""** and every 
time i get  a failure on a different file yet always  the culprit seem to be these lines from the  respective build output:

In file included from /opt/rh/gcc-toolset-12/root/usr/lib/gcc/x86_64-redhat-linux/12/../../../../include/c++/12/memory:77:
In file included from /opt/rh/gcc-toolset-12/root/usr/lib/gcc/x86_64-redhat-linux/12/../../../../include/c++/12/bits/shared_ptr.h:53:
/opt/rh/gcc-toolset-12/root/usr/lib/gcc/x86_64-redhat-linux/12/../../../../include/c++/12/bits/shared_ptr_base.h:196:22: error: type name does not allow function specifier to be specified
bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include/crt/host_defines.h:83:24: note: expanded from macro '__noinline__'
        __attribute__((noinline))

Note, i dont have **/opt/rh...** path on this host so this must  be coming during compiling some external  dependencies 
and looking at my .tf_configure.bazelrc file i dont  really have control over how external dependencies are handled. 
Can reproduce with nightly builds as it uses different  (newer) version of tensorflow and  bazel.

I have previously compiled tensorflow 2.8.0 (with  its corresponding bazel version)  on the same host with the same cuda, python, and gcc using identical build  steps and the builds went perfectly file. I am confident that all prerequisite software is installed correctly (using it for many other packages builds outside tensorflow).

I would really appreciate if someone can point to what am  i doing wrong or what else i can try. 

No conda or docker please. Neither one will work in our environment where i need home built RPMS 
for installation on the HPC cluster. 

Thanks!

### Standalone code to reproduce the issue

```shell
No test case
```


### Relevant log output

_No response_"
61976,NotFoundError could not find registered transfer manager for platform Host -- check target linkage [Op:__inference__jit_compiled_convolution_op_26169] TPU-VM,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I was trying to use `groups` parameter from `keras.layers.Conv2D` API on TPU-VM device. While running on GPU works but on TPU, it doesn't if `groups > 1`.

### Standalone code to reproduce the issue

Full [Code.](https://www.kaggle.com/code/ipythonx/github-issue-655-keras-tpu-vm/notebook)

```python
input_shape = (4, 28, 28, 9)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
    27, 3, activation='relu', input_shape=input_shape[1:], groups=3
)(x)
print(y.shape)
```


### Relevant log output

```yaml
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
Cell In[133], line 3
      1 input_shape = (4, 28, 28, 9)
      2 x = tf.random.normal(input_shape)
----> 3 y = tf.keras.layers.Conv2D(
      4     27, 3, activation='relu', input_shape=input_shape[1:], groups=3
      5 )(x)
      7 print(y.shape)

File /usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File /usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50 try:
     51   ctx.ensure_initialized()
---> 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                       inputs, attrs, num_outputs)
     54 except core._NotOkStatusException as e:
     55   if name is not None:

NotFoundError: Exception encountered when calling layer 'conv2d_335' (type Conv2D).

could not find registered transfer manager for platform Host -- check target linkage [Op:__inference__jit_compiled_convolution_op_26236]

Call arguments received by layer 'conv2d_335' (type Conv2D):
  â€¢ inputs=tf.Tensor(shape=(4, 28, 28, 9), dtype=float32)
```
"
61975,tf.math.cumsum weird behaviour ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

ython 3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

T4

### Current behavior?

tf.math.cumsum produce weird results on float32 and float64 when device is GPU:
- results are different than on CPU
- results do not match manual cumsum calculation 
- cumsum value can change when adding 0 (last two values in log output). 
- float64 has similar problems as well
- tested on multiple GPUs: T4, GTX1080Ti and tensorflow versions: 2.13, 2.8.4

### Standalone code to reproduce the issue

```shell
with tf.device('/GPU'):
  arr = tf.constant(
        [  0.  ,     0.  ,     0.  ,     0.  ,  9759.35,  9759.35,
          9759.35,  9759.35,  9759.35,  9759.35,  9762.03,  9700.78,
          9700.78,  9700.78,  9700.78,  9700.78,  9700.78,  9660.83,
          9600.46,  9600.46,  9600.46,  9600.46,  9600.46,  9600.46,
          9715.65,  9742.31,  9742.31,  9742.31,  9742.31,  9742.31,
          9742.31,  9774.32,  9750.2 ,  9750.2 ,  9750.2 ,  9750.2 ,
          9750.2 ,  9750.2 ,  9796.23,  9824.72,  9824.72,  9824.72,
          9824.72,  9824.72,  9824.72, 11737.25,  9759.23,  9759.23,
          9759.23,  9759.23,  9759.23,  9759.23,  9551.41,  9551.47,
          9551.47,  9551.47,  9551.47,  9551.47,  9551.47,  9723.25,
          9693.61,  9693.61,  9693.61,  9693.61,  9693.61,  9693.61,
          9629.34,  9658.78,  9658.78,  9658.78,  9658.78,  9658.78,
          9658.78,  9986.66, 10005.04, 10005.04, 10005.04, 10005.04,
          10005.04, 10005.04,  9977.68,  9955.32,  9955.32,  9955.32,
          9955.32,  9955.32,  9955.32,  9875.19,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.], dtype=tf.float32)

  manual_cumsum = []
  for val in arr:
    if len(manual_cumsum) == 0:
      manual_cumsum.append(val)
    else:
       manual_cumsum.append(manual_cumsum[-1] + val)

  tf_cumsum = tf.math.cumsum(arr)

  for index in range(arr.shape[0]):
    print(""arr_value:"", arr[index].numpy(), ""tensor - manual cumsum diff:"", (tf_cumsum[index] - manual_cumsum[index]).numpy())
```
```


### Relevant log output

```shell
arr_value: 0.0 tensor - manual cumsum diff: 0.0
arr_value: 0.0 tensor - manual cumsum diff: 0.0
arr_value: 0.0 tensor - manual cumsum diff: 0.0
arr_value: 0.0 tensor - manual cumsum diff: 0.0
arr_value: 9759.35 tensor - manual cumsum diff: 0.0
arr_value: 9759.35 tensor - manual cumsum diff: 0.0
arr_value: 9759.35 tensor - manual cumsum diff: 0.0
arr_value: 9759.35 tensor - manual cumsum diff: 0.0
arr_value: 9759.35 tensor - manual cumsum diff: 0.0
arr_value: 9759.35 tensor - manual cumsum diff: 0.0
arr_value: 9762.03 tensor - manual cumsum diff: 0.0
arr_value: 9700.78 tensor - manual cumsum diff: 0.0
arr_value: 9700.78 tensor - manual cumsum diff: -0.0078125
arr_value: 9700.78 tensor - manual cumsum diff: -0.0078125
arr_value: 9700.78 tensor - manual cumsum diff: -0.0078125
arr_value: 9700.78 tensor - manual cumsum diff: -0.0078125
arr_value: 9700.78 tensor - manual cumsum diff: -0.0078125
arr_value: 9660.83 tensor - manual cumsum diff: -0.015625
arr_value: 9600.46 tensor - manual cumsum diff: -0.015625
arr_value: 9600.46 tensor - manual cumsum diff: -0.015625
arr_value: 9600.46 tensor - manual cumsum diff: 0.0
arr_value: 9600.46 tensor - manual cumsum diff: 0.0
arr_value: 9600.46 tensor - manual cumsum diff: 0.0
arr_value: 9600.46 tensor - manual cumsum diff: 0.0
arr_value: 9715.65 tensor - manual cumsum diff: 0.03125
arr_value: 9742.31 tensor - manual cumsum diff: 0.03125
arr_value: 9742.31 tensor - manual cumsum diff: 0.03125
arr_value: 9742.31 tensor - manual cumsum diff: 0.03125
arr_value: 9742.31 tensor - manual cumsum diff: 0.015625
arr_value: 9742.31 tensor - manual cumsum diff: 0.015625
arr_value: 9742.31 tensor - manual cumsum diff: 0.015625
arr_value: 9774.32 tensor - manual cumsum diff: 0.03125
arr_value: 9750.2 tensor - manual cumsum diff: 0.03125
arr_value: 9750.2 tensor - manual cumsum diff: 0.03125
arr_value: 9750.2 tensor - manual cumsum diff: 0.03125
arr_value: 9750.2 tensor - manual cumsum diff: 0.03125
arr_value: 9750.2 tensor - manual cumsum diff: 0.09375
arr_value: 9750.2 tensor - manual cumsum diff: 0.09375
arr_value: 9796.23 tensor - manual cumsum diff: 0.09375
arr_value: 9824.72 tensor - manual cumsum diff: 0.09375
arr_value: 9824.72 tensor - manual cumsum diff: 0.125
arr_value: 9824.72 tensor - manual cumsum diff: 0.125
arr_value: 9824.72 tensor - manual cumsum diff: 0.125
arr_value: 9824.72 tensor - manual cumsum diff: 0.125
arr_value: 9824.72 tensor - manual cumsum diff: 0.125
arr_value: 11737.25 tensor - manual cumsum diff: 0.125
arr_value: 9759.23 tensor - manual cumsum diff: 0.125
arr_value: 9759.23 tensor - manual cumsum diff: 0.125
arr_value: 9759.23 tensor - manual cumsum diff: 0.15625
arr_value: 9759.23 tensor - manual cumsum diff: 0.15625
arr_value: 9759.23 tensor - manual cumsum diff: 0.15625
arr_value: 9759.23 tensor - manual cumsum diff: 0.15625
arr_value: 9551.41 tensor - manual cumsum diff: 0.1875
arr_value: 9551.47 tensor - manual cumsum diff: 0.1875
arr_value: 9551.47 tensor - manual cumsum diff: 0.1875
arr_value: 9551.47 tensor - manual cumsum diff: 0.1875
arr_value: 9551.47 tensor - manual cumsum diff: 0.1875
arr_value: 9551.47 tensor - manual cumsum diff: 0.1875
arr_value: 9551.47 tensor - manual cumsum diff: 0.1875
arr_value: 9723.25 tensor - manual cumsum diff: 0.1875
arr_value: 9693.61 tensor - manual cumsum diff: 0.1875
arr_value: 9693.61 tensor - manual cumsum diff: 0.1875
arr_value: 9693.61 tensor - manual cumsum diff: 0.1875
arr_value: 9693.61 tensor - manual cumsum diff: 0.1875
arr_value: 9693.61 tensor - manual cumsum diff: 0.125
arr_value: 9693.61 tensor - manual cumsum diff: 0.125
arr_value: 9629.34 tensor - manual cumsum diff: 0.125
arr_value: 9658.78 tensor - manual cumsum diff: 0.125
arr_value: 9658.78 tensor - manual cumsum diff: 0.125
arr_value: 9658.78 tensor - manual cumsum diff: 0.125
arr_value: 9658.78 tensor - manual cumsum diff: 0.125
arr_value: 9658.78 tensor - manual cumsum diff: 0.125
arr_value: 9658.78 tensor - manual cumsum diff: 0.25
arr_value: 9986.66 tensor - manual cumsum diff: 0.25
arr_value: 10005.04 tensor - manual cumsum diff: 0.25
arr_value: 10005.04 tensor - manual cumsum diff: 0.25
arr_value: 10005.04 tensor - manual cumsum diff: 0.1875
arr_value: 10005.04 tensor - manual cumsum diff: 0.1875
arr_value: 10005.04 tensor - manual cumsum diff: 0.1875
arr_value: 10005.04 tensor - manual cumsum diff: 0.1875
arr_value: 9977.68 tensor - manual cumsum diff: 0.1875
arr_value: 9955.32 tensor - manual cumsum diff: 0.1875
arr_value: 9955.32 tensor - manual cumsum diff: 0.1875
arr_value: 9955.32 tensor - manual cumsum diff: 0.1875
arr_value: 9955.32 tensor - manual cumsum diff: 0.1875
arr_value: 9955.32 tensor - manual cumsum diff: 0.1875
arr_value: 9955.32 tensor - manual cumsum diff: 0.1875
arr_value: 9875.19 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.25
arr_value: 0.0 tensor - manual cumsum diff: 0.25
```
```
"
61974,AttributeError: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3

### Mobile device

_No response_

### Python version

2.13.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

OS : Ubuntu 22.04.3
Software: Pycharm
files: LogisticRegression_withTensorflow.ipynb

I dont know what happen, it just work yesterday
when I run this code:
`model.fit(X_train, y_train, epochs=150)`


### Standalone code to reproduce the issue

```shell
# import library
import numpy as np
import tensorflow as tf
from tensorflow.python.keras import Sequential
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.losses import BinaryCrossentropy
print(tf.__version__)
# 2.13.0
# import data
X = []
y = []
with open('../../Part1/Week2/data/ex2data1.txt') as file:
# with open('/home/wisdom/vs_code_repository/Python/AndrewNG_ML/Part1/Week2/data/ex2data1.txt', 'r') as file:
    for lines in file:
        colums = lines.strip().split(',')
        X.append([float(colums[0]), float(colums[1])])
        y.append(float(colums[2]))
X = np.array(X)
y = np.array(y)
# normalization
X_mean = np.mean(X, axis=0)
X_max = np.max(X, axis=0)
X_min = np.min(X, axis=0)
X = (X - X_mean) / (X_max - X_min)

# split data into training_set and testing_set
X_train = X[:80]
y_train = y[:80]
X_test = X[80:]
y_test = y[80:]
model = Sequential([Dense(units=32, activation='sigmoid'),
                    Dense(units=16, activation='sigmoid'),
                    Dense(units=1, activation='sigmoid')])
model.compile(loss=BinaryCrossentropy())
model.fit(X_train, y_train, epochs=150) # bug appears this line
```


### Relevant log output

```shell
AttributeError                            Traceback (most recent call last)
Cell In[13], line 1
----> 1 model.fit(X_train, y_train, epochs=150)

File /usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/training.py:1138, in Model.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1132   self._cluster_coordinator = cluster_coordinator.ClusterCoordinator(
   1133       self.distribute_strategy)
   1135 with self.distribute_strategy.scope(), \
   1136      training_utils.RespectCompiledTrainableState(self):
   1137   # Creates a `tf.data.Dataset` and handles batch and epoch iteration.
-> 1138   data_handler = data_adapter.get_data_handler(
   1139       x=x,
   1140       y=y,
   1141       sample_weight=sample_weight,
   1142       batch_size=batch_size,
   1143       steps_per_epoch=steps_per_epoch,
   1144       initial_epoch=initial_epoch,
   1145       epochs=epochs,
   1146       shuffle=shuffle,
   1147       class_weight=class_weight,
   1148       max_queue_size=max_queue_size,
   1149       workers=workers,
   1150       use_multiprocessing=use_multiprocessing,
   1151       model=self,
   1152       steps_per_execution=self._steps_per_execution)
   1154   # Container that configures and calls `tf.keras.Callback`s.
   1155   if not isinstance(callbacks, callbacks_module.CallbackList):

File /usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py:1398, in get_data_handler(*args, **kwargs)
   1396 if getattr(kwargs[""model""], ""_cluster_coordinator"", None):
   1397   return _ClusterCoordinatorDataHandler(*args, **kwargs)
-> 1398 return DataHandler(*args, **kwargs)

File /usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py:1152, in DataHandler.__init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)
   1149   self._steps_per_execution = steps_per_execution
   1150   self._steps_per_execution_value = steps_per_execution.numpy().item()
-> 1152 adapter_cls = select_data_adapter(x, y)
   1153 self._adapter = adapter_cls(
   1154     x,
   1155     y,
   (...)
   1164     distribution_strategy=distribute_lib.get_strategy(),
   1165     model=model)
   1167 strategy = distribute_lib.get_strategy()

File /usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py:988, in select_data_adapter(x, y)
    986 def select_data_adapter(x, y):
    987   """"""Selects a data adapter than can handle a given x and y.""""""
--> 988   adapter_cls = [cls for cls in ALL_ADAPTER_CLS if cls.can_handle(x, y)]
    989   if not adapter_cls:
    990     # TODO(scottzhu): This should be a less implementation-specific error.
    991     raise ValueError(
    992         ""Failed to find data adapter that can handle ""
    993         ""input: {}, {}"".format(
    994             _type_name(x), _type_name(y)))

File /usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py:988, in <listcomp>(.0)
    986 def select_data_adapter(x, y):
    987   """"""Selects a data adapter than can handle a given x and y.""""""
--> 988   adapter_cls = [cls for cls in ALL_ADAPTER_CLS if cls.can_handle(x, y)]
    989   if not adapter_cls:
    990     # TODO(scottzhu): This should be a less implementation-specific error.
    991     raise ValueError(
    992         ""Failed to find data adapter that can handle ""
    993         ""input: {}, {}"".format(
    994             _type_name(x), _type_name(y)))

File /usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py:707, in DatasetAdapter.can_handle(x, y)
    704 @staticmethod
    705 def can_handle(x, y=None):
    706   return (isinstance(x, (data_types.DatasetV1, data_types.DatasetV2)) or
--> 707           _is_distributed_dataset(x))

File /usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py:1699, in _is_distributed_dataset(ds)
   1698 def _is_distributed_dataset(ds):
-> 1699   return isinstance(ds, input_lib.DistributedDatasetInterface)

AttributeError: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'
```
"
61972,Unable to load TensorFlow saved model (AttributeError: '_UserObject' object has no attribute 'add_slot'),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.7.0

### Custom code

Yes

### OS platform and distribution

Windows 10 Enterprise LTSC

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2/8

### GPU model and memory

Nvidia Geforce RTX 3090 24 GB

### Current behavior?

I have several trained TensorFlow models stored on my device in saved_model format. All the models have the same architecture and serve the same purpose, but they were created at different points in time. While I have no problems to load the newest models (created from 5th September 2022 onward) with the command `tensorflow.keras.models.load_model(filepath)`, older models that were created before this date raise the following exception when this method is called:

> AttributeError: '_UserObject' object has no attribute 'add_slot'

The same exception is also raised when trying to load these models with: `tensorflow.saved_model.load(filepath)`, which was one of the suggestions I found to solve the issue. Another possible solution that I took from the related issue #52091 was to use another TF version, I tried it with the older v2.6 and the newest Windows native version v.2.10, but the result was the same.

### Standalone code to reproduce the issue

[EDIT] I could reproduce the exception on Google Colab using the newest TF version. Please find the notebook here: [Model_Load_Problem.ipynb](https://colab.research.google.com/drive/1YyyU6kyn947JCuTHOrjRiDGQb2XBOueu?usp=sharing)


### Relevant log output

```shell
2023-09-25 11:36:20.048929: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-25 11:36:21.096591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18788 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1a:00.0, compute capability: 8.6
Traceback (most recent call last):
  File ""C:\Users\icon\Desktop\Testing_Models\iCoNet\source\model.py"", line 699, in load_trained_model
    model = tf.keras.models.load_model(model_path)
  File ""C:\Users\icon\.conda\envs\iConNet\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\icon\.conda\envs\iConNet\lib\site-packages\tensorflow\python\saved_model\load.py"", line 466, in _load_nodes
    slot_variable = optimizer_object.add_slot(
AttributeError: '_UserObject' object has no attribute 'add_slot'
```
"
61971,Windows 10 Source build failure if python installed in a directory with spaces,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.10

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

5.1.1

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1.1

### GPU model and memory

Quadro P2000 / 5 Gb

### Current behavior?

Building 2.10 tensorflow with cuda support from sources using Bazel 5.1.1 would result in an error if the python is downloaded in a directory with spaces. In my case I had python installed under `C:/Program Files/python39`. The error showed that: `'c:/program' is not recognized as an internal or external command building tensorflow'`. I traced this error into the `tensorflow\third_party\gpus\cuda_configure.bzl` file. In this file there is a function called `_check_cuda_libs` (line 492). This function checks the precense of cuda libraries by compling a cmd. However, the cmd won't work if the python_bin points to a python executive which is located in a directory with spaces. This error was simply fixed by moving python to a directory without spaces (No need to change the scripts inside `.bzl` file).

### Standalone code to reproduce the issue

```shell
You should be able to reproduce the issue by following the ""Build from source on Windows"" instructions and downloading python for example under ""Program Files"" folder. Build should be configured to include CUDA support. I used the following build command: bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_"
61969,Use github.com/apssouza22/chatflow as a conversational layer. It would enable actual API requests to be carried out from natural language inputs.,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

all of em

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Fine as hell

### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

```shell
i <3 tensorflow
```
"
61968,TFLite model with `l2_normalize(tf.transpose(x))` produces wrong outputs,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.15.0-dev20230924

### 2. Code

```python

import tensorflow as tf
import numpy as np

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()
    self.b = tf.Variable(np.array([[1],[2]],dtype=np.float32))

  def call(self, x):
    x = tf.add(x,1)
    return tf.math.l2_normalize(tf.transpose(x))

# Initializing the model
m = Model()

# Call model
input_shape = [1, 1, 2]
x1 = tf.constant(1., shape=input_shape)
y1 = m(x1)
print('expected model output:')
print(y1)


converter = tf.lite.TFLiteConverter.from_keras_model(m)
tflite_model = converter.convert()

def _evaluateTFLiteModel(tflite_model, input_data):
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    for i in range(len(input_data)):
        interpreter.set_tensor(input_details[i]['index'], input_data[i])
    interpreter.invoke()
    output_data = [interpreter.get_tensor(output_details[i]['index'])
                   for i in range(len(output_details))]
    return output_data


actual_value = _evaluateTFLiteModel(tflite_model,[x1])
print('tflite model output:')
print(actual_value[0])
```
### 3. Failure after conversion
Output:
```
expected model output:
tf.Tensor(
[[[0.70710677]]

 [[0.70710677]]], shape=(2, 1, 1), dtype=float32)
tflite model output:
[[[1.]]

 [[1.]]]
```"
61967,TFLite model produces wrong output after fusion optimization,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.15.0-dev20230924

### 2. Code
```
import tensorflow as tf

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__(name=""model"")
    self.w1 = tf.Variable([[0.], [0.5]])
    self.b1 = tf.Variable([-4.])
    self.r = tf.Variable([-7.])
    self.c = tf.Variable(1.)
    self.m1 = tf.Variable([-4.])
    self.m2 = tf.Variable([1.])

  def call(self, x):
    x = x + self.m1
    x2 = tf.math.multiply(x, self.r)
    x3 = tf.linalg.matmul(x2, self.w1)
    x4 = tf.math.add(x3, self.b1)
    x5 = tf.math.multiply(x4, self.r)
    x6 = tf.math.add(x5, self.m2)
    x7 = tf.math.multiply(x6, self.r)
    return x7

# Initializing the model
m = Model()

# Inputs to the model
x = tf.constant([[2., -3.]], shape=[1, 2], dtype=tf.float32)

# Call model
y = m(x)
print('expected model output:')
print(y)


converter = tf.lite.TFLiteConverter.from_keras_model(m)
tflite_model = converter.convert()

def _evaluateTFLiteModel(tflite_model, input_data):
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    for i in range(len(input_data)):
        interpreter.set_tensor(input_details[i]['index'], input_data[i])
    interpreter.invoke()
    output_data = [interpreter.get_tensor(output_details[i]['index'])
                   for i in range(len(output_details))]
    return output_data


actual_value = _evaluateTFLiteModel(tflite_model,[x])
print('tflite model output:')
print(actual_value[0])
```

### 3. Failure after conversion
Output:
```
expected model output:
tf.Tensor([[997.5]], shape=(1, 1), dtype=float32)
tflite model output:
[[311.5]]
```

P.S. It seems multiple optimizations are triggered, including `FuseAddAndFullyConnected`, `FuseMulAndFullyConnected`, `FuseFullyConnectedAndMul`, `FuseFullyConnectedAndAdd`. It's unclear which one of them causes the bug and further inspection is needed."
61966,Color prediction result ,"Fastwin color prediction game results 
"
61965,XLA compiled `floordiv` allows `integer division by zero`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

XLA compiled `floordiv` allows `integer division by zero` where an exception `Integer division by zero` will be raised without XLA compilation.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

""""""
XLA Compiled
""""""
class Model(tf.keras.Model):
    @tf.function(jit_compile=True)
    def call(self, x1):
        x2 = tf.math.floordiv(3, 0)
        x3 = tf.math.multiply(x1, x2)
        return x3

m = Model()
x1 = tf.constant(1, shape=[])
print(m(x1)) # tf.Tensor(-1, shape=(), dtype=int32)

""""""
Without XLA
""""""
class Model(tf.keras.Model):
    def call(self, x1):
        x2 = tf.math.floordiv(3, 0)
        x3 = tf.math.multiply(x1, x2)
        return x3

m = Model()
x1 = tf.constant(1, shape=[])
print(m(x1))
```


### Relevant log output

```shell
InvalidArgumentError: Exception encountered when calling layer 'model_21' (type Model).

{{function_node __wrapped__FloorDiv_device_/job:localhost/replica:0/task:0/device:CPU:0}} Integer division by zero [Op:FloorDiv] name: 

Call arguments received by layer 'model_21' (type Model):
  â€¢ x1=tf.Tensor(shape=(), dtype=int32)
```
"
61964,"also missing from the include ""patch"" is file ""tensorflow/tsl/c/tsl_status.h"" (there might be others)","              also missing from the include ""patch"" is file ""tensorflow/tsl/c/tsl_status.h"" (there might be others)

_Originally posted by @esohns in https://github.com/tensorflow/tensorflow/issues/59762#issuecomment-1732376736_
            "
61963,KeyError: 'min',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

5.15.90.1-microsoft-standard-WSL2

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/operation_layers.py"", line 31, in convert_clip
    if params['min'] == 0:
KeyError: 'min'

### Standalone code to reproduce the issue

```shell
from onnx2keras import onnx_to_keras
import keras
import onnx
import sys
# sys.path.append(""/root/MR"")
onnx_model = onnx.load('ssd_bmv1_torch.onnx')
onnx_inputs = onnx_model.graph.input
print(""==========================="")
print(onnx_inputs)
# onnx_model = onnx.load('vgg11.onnx')
k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
keras.models.save_model(k_model, 'ssd_bmv1_torch.h5', overwrite=True, save_format=""h5"")

onnx file can be downloaded at https://pan.xunlei.com/s/VNf1O2DqsdqdbTYYpyemSqveA1?pwd=by77#
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/operation_layers.py"", line 31, in convert_clip
    if params['min'] == 0:
KeyError: 'min'
```
"
61962,KeyError: 'ConstantOfShape',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

5.15.90.1-microsoft-standard-WSL2

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
KeyError: 'ConstantOfShape'

### Standalone code to reproduce the issue

```shell
from onnx2keras import onnx_to_keras
import keras
import onnx
import sys
# sys.path.append(""/root/MR"")
onnx_model = onnx.load('patchcore_torch.onnx')
onnx_inputs = onnx_model.graph.input
print(""==========================="")
print(onnx_inputs)
# onnx_model = onnx.load('vgg11.onnx')
k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
keras.models.save_model(k_model, 'patchcore_torch.h5', overwrite=True, save_format=""h5"")

onnx file can be downloaded at https://pan.xunlei.com/s/VNf1NLiBYfIh_GKpSvMjqJQAA1?pwd=3wzb#
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
KeyError: 'ConstantOfShape'
```
"
61961,AttributeError: Number of inputs is not equal 1 for unsqueeze layer,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

5.15.90.1-microsoft-standard-WSL2

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['onnx::Unsqueeze_0'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/reshape_layers.py"", line 210, in convert_unsqueeze
    raise AttributeError('Number of inputs is not equal 1 for unsqueeze layer')
AttributeError: Number of inputs is not equal 1 for unsqueeze layer

### Standalone code to reproduce the issue

```shell
please run the below codes to reproduce:

from onnx2keras import onnx_to_keras
import keras
import onnx
import sys
# sys.path.append(""/root/MR"")
onnx_model = onnx.load('textcnn_torch.onnx')
onnx_inputs = onnx_model.graph.input
print(""==========================="")
print(onnx_inputs)
# onnx_model = onnx.load('vgg11.onnx')
k_model = onnx_to_keras(onnx_model, ['onnx::Unsqueeze_0'], name_policy='renumerate', verbose=True)
keras.models.save_model(k_model, 'textcnn_torch.h5', overwrite=True, save_format=""h5"")
```
onnx file can be downloaded at https://pan.xunlei.com/s/VNf1M2KIkqx6PBbrBdTiuTkMA1?pwd=wxqf#
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['onnx::Unsqueeze_0'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/reshape_layers.py"", line 210, in convert_unsqueeze
    raise AttributeError('Number of inputs is not equal 1 for unsqueeze layer')
AttributeError: Number of inputs is not equal 1 for unsqueeze layer
```
"
61960,AttributeError: Can't gather from tf tensor.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

5.15.90.1-microsoft-standard-WSL2

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['onnx::Cast_0', 'onnx::Cast_1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/reshape_layers.py"", line 87, in convert_gather
    raise AttributeError('Can\'t gather from tf tensor.')
AttributeError: Can't gather from tf tensor.

### Standalone code to reproduce the issue

```shell
please run the codes below to reproduce:

from onnx2keras import onnx_to_keras
import keras
import onnx
import sys
# sys.path.append(""/root/MR"")
onnx_model = onnx.load('fasttext_torch.onnx')
onnx_inputs = onnx_model.graph.input
print(""==========================="")
print(onnx_inputs)
# onnx_model = onnx.load('vgg11.onnx')
k_model = onnx_to_keras(onnx_model, ['onnx::Cast_0', 'onnx::Cast_1'], name_policy='renumerate', verbose=True)
keras.models.save_model(k_model, 'fasttext_torch.h5', overwrite=True, save_format=""h5"")
```
onnx file can be downloaded at https://pan.xunlei.com/s/VNf1LHoMqVe2TuzMTA8uhjO5A1?pwd=ibm3#
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['onnx::Cast_0', 'onnx::Cast_1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/reshape_layers.py"", line 87, in convert_gather
    raise AttributeError('Can\'t gather from tf tensor.')
AttributeError: Can't gather from tf tensor.
```
"
61959,AttributeError: Not implemented,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

5.15.90.1-microsoft-standard-WSL2

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/reshape_layers.py"", line 294, in convert_slice
    raise AttributeError('Not implemented')
AttributeError: Not implemented

### Standalone code to reproduce the issue

```shell
reproduce by running the following code:

from onnx2keras import onnx_to_keras
import keras
import onnx
import sys
# sys.path.append(""/root/MR"")
onnx_model = onnx.load('deeplabv3_torch.onnx')
onnx_inputs = onnx_model.graph.input
print(""==========================="")
print(onnx_inputs)
# onnx_model = onnx.load('vgg11.onnx')
k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
keras.models.save_model(k_model, 'deeplabv3_torch.h5', overwrite=True, save_format=""h5"")
```

onnx file can be downloaded at https://pan.xunlei.com/s/VNf1JfHu9m6WG6yE2IuzWtKpA1?pwd=ux7b#
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/reshape_layers.py"", line 294, in convert_slice
    raise AttributeError('Not implemented')
AttributeError: Not implemented
```
"
61958,"ValueError: Exception encountered when calling layer ""13"" (type Lambda).","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

5.15.90.1-microsoft-standard-WSL2

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/pengg/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/pengg/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<style>
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
.font5
	{color:windowtext;
	font-size:9.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:ç­‰çº¿;
	mso-generic-font-family:auto;
	mso-font-charset:134;}
tr
	{mso-height-source:auto;
	mso-ruby-visibility:none;}
col
	{mso-width-source:auto;
	mso-ruby-visibility:none;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:11.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:ç­‰çº¿;
	mso-generic-font-family:auto;
	mso-font-charset:134;
	mso-number-format:General;
	text-align:general;
	vertical-align:middle;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
.xl65
	{text-align:center;}
.xl66
	{text-align:center;
	white-space:normal;}
ruby
	{ruby-align:left;}
rt
	{color:windowtext;
	font-size:9.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:ç­‰çº¿;
	mso-generic-font-family:auto;
	mso-font-charset:134;
	mso-char-type:none;
	display:none;}
-->
</style>
</head>

<body link=""#0563C1"" vlink=""#954F72"">



ValueError:   Exception encountered when calling layer ""13"" (type Lambda).          Dimensions must be equal, but are 204 and 206 for '{{node 13/Add}} =   AddV2[T=DT_FLOAT](Placeholder, Placeholder_1)' with input shapes:   [?,64,204,204], [?,64,206,206].          Call arguments received by layer ""13"" (type Lambda):     Â  â€¢ inputs=['tf.Tensor(shape=(None,   64, 204, 204), dtype=float32)', 'tf.Tensor(shape=(None, 64, 206, 206),   dtype=float32)']     Â  â€¢ mask=None     Â  â€¢ training=None
--




</body>

</html>


### Standalone code to reproduce the issue

```shell
just run the following code to reproduce:

from onnx2keras import onnx_to_keras
import keras
import onnx
import sys
# sys.path.append(""/root/MR"")
onnx_model = onnx.load('yolov3_darknet53.onnx')
onnx_inputs = onnx_model.graph.input
print(""==========================="")
print(onnx_inputs)
# onnx_model = onnx.load('vgg11.onnx')
k_model = onnx_to_keras(onnx_model, ['x'], name_policy='renumerate', verbose=True)
keras.models.save_model(k_model, 'ssd_resnet50fpn_torch.h5', overwrite=True, save_format=""h5"")
```
the onnx file can be downloaded at https://pan.xunlei.com/s/VNf1FtBh2v6mP_QXJWVailBmA1?pwd=g43e#
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['x'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/elementwise_layers.py"", line 83, in convert_elementwise_add
    layers[node_name] = lambda_layer([input_0, input_1])
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/elementwise_layers.py"", line 76, in target_layer
    layer = tf.add(
ValueError: Exception encountered when calling layer ""LAYER_12"" (type Lambda).

Dimensions must be equal, but are 204 and 206 for '{{node LAYER_12/Add}} = AddV2[T=DT_FLOAT](Placeholder, Placeholder_1)' with input shapes: [?,64,204,204], [?,64,206,206].

Call arguments received by layer ""LAYER_12"" (type Lambda):
  â€¢ inputs=['tf.Tensor(shape=(None, 64, 204, 204), dtype=float32)', 'tf.Tensor(shape=(None, 64, 206, 206), dtype=float32)']
  â€¢ mask=None
  â€¢ training=None
```
"
61957,ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Window 10

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to implement [this](https://keras.io/examples/vision/image_classification_with_vision_transformer/) in tensorflow 2.13 but getting this error ""**ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported**"" after searching a lot I got to know we have to use -1 in case of None but even after trying that it was not working  I am getting this error in custom Patches layer 

patches = tf.reshape(patches, [batch_size, -1, patch_dims])


can anybody please help?

### Standalone code to reproduce the issue

```shell
class Patches(layers.Layer):

    def __init__(self, patch_size, **kwargs):
        super(Patches, self).__init__()
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0] # Get the Batch Size
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1], # only along the Height and Width Dimension
            strides=[1, self.patch_size, self.patch_size, 1], # The next patch should not overlap the previus patch
            rates=[1,1,1,1],
            padding='VALID'
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        
        return patches
    
    def get_config(self):
        config = super().get_config()
        config.update({
            ""path-size"": self.patch_size,
        })
        return config
```


### Relevant log output

```shell
history = model.fit(
     27     train_generator,
     28     validation_data=valid_generator,
     29     batch_size=BATCH_SIZE,
     30     epochs=NUM_EPOCHS,
     31     callbacks=[
     32         checkpoint_callback, 
     33         tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_Accuracy', mode='max' ,restore_best_weights=True)
     34     ],
     35 )
     37 model.load_weights(checkpoint_filepath)
     38 _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)

File ~\AppData\Roaming\Python\Python39\site-packages\keras\src\utils\traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~\AppData\Local\Temp\__autograph_generated_filelwsbm473.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)
     13 try:
     14     do_return = True
---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16 except:
     17     do_return = False

File ~\AppData\Local\Temp\__autograph_generated_filewl0d9m3r.py:13, in outer_factory.<locals>.inner_factory.<locals>.tf__call(self, images)
     11 patches = ag__.converted_call(ag__.ld(tf).image.extract_patches, (), dict(images=ag__.ld(images), sizes=[1, ag__.ld(self).patch_size, ag__.ld(self).patch_size, 1], strides=[1, ag__.ld(self).patch_size, ag__.ld(self).patch_size, 1], rates=[1, 1, 1, 1], padding='VALID'), fscope)
     12 patch_dims = ag__.ld(patches).shape[-1]
---> 13 patches = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(patches), [ag__.ld(batch_size), -1, ag__.ld(patch_dims)]), None, fscope)
     14 try:
     15     do_return = True

ValueError: in user code:

    File ""C:\Users\aksh1\AppData\Roaming\Python\Python39\site-packages\keras\src\engine\training.py"", line 1338, in train_function  *
        return step_function(self, iterator)
    File ""C:\Users\aksh1\AppData\Roaming\Python\Python39\site-packages\keras\src\engine\training.py"", line 1322, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""C:\Users\aksh1\AppData\Roaming\Python\Python39\site-packages\keras\src\engine\training.py"", line 1303, in run_step  **
        outputs = model.train_step(data)
    File ""C:\Users\aksh1\AppData\Roaming\Python\Python39\site-packages\keras\src\engine\training.py"", line 1080, in train_step
        y_pred = self(x, training=True)
    File ""C:\Users\aksh1\AppData\Roaming\Python\Python39\site-packages\keras\src\utils\traceback_utils.py"", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File ""C:\Users\aksh1\AppData\Local\Temp\__autograph_generated_filewl0d9m3r.py"", line 13, in tf__call
        patches = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(patches), [ag__.ld(batch_size), -1, ag__.ld(patch_dims)]), None, fscope)

    ValueError: Exception encountered when calling layer 'patches_3' (type Patches).
    
    in user code:
    
        File ""C:\Users\aksh1\AppData\Local\Temp\ipykernel_11744\2749589268.py"", line 17, in call  *
            patches = tf.reshape(patches, [batch_size, -1, patch_dims])
    
        ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.
    
    
    Call arguments received by layer 'patches_3' (type Patches):
      â€¢ images=tf.Tensor(shape=(None, None, None, None), dtype=float32)
```
"
61956,Error with protobuf during installation,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 12.3

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.1.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

/home/user1/.cache/bazel/_bazel_user1/2ed8e7afdea3ff827d1d2c14869018ce/external/com_google_protobuf/BUILD.bazel:459:10: Compiling src/google/protobuf/compiler/main.cc [for tool] failed: (Exit 1): clang failed: error executing command (from target @com_google_protobuf//:protoc) 
  (cd /home/user1/.cache/bazel/_bazel_user1/2ed8e7afdea3ff827d1d2c14869018ce/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64 \
    PATH=/home/user1/.cache/bazelisk/downloads/sha256/6c25a6d716545d6b672ec46f770521cd9ebb63d73617b8f4e6747825d1db1839/bin:/home/user1/bin:/usr/local/cuda-12.2/bin:/home/user1/anaconda3/bin:/home/user1/anaconda3/condabin:/home/user1/.local/bin:/home/user1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin \
    PWD=/proc/self/cwd \
  /usr/lib/llvm-16/bin/clang -MD -MF bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/_objs/protoc/main.d '-frandom-seed=bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/_objs/protoc/main.o' '-DBAZEL_CURRENT_REPOSITORY=""com_google_protobuf""' -iquote external/com_google_protobuf -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/zlib -iquote external/bazel_tools -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/bazel_tools -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/zlib -fmerge-all-constants -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-12.2' -g0 -w -g0 '-std=c++17' -c external/com_google_protobuf/src/google/protobuf/compiler/main.cc -o bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/_objs/protoc/main.o)
# Configuration: be1b6d30ad6b895c5d0cb4e11b08ee9fdfd7ce804f01eb0679a5e177ceb7e39b
# Execution platform: @local_execution_config_platform//:platform
In file included from external/com_google_protobuf/src/google/protobuf/compiler/main.cc:31:
external/com_google_protobuf/src/google/protobuf/compiler/cpp/generator.h:40:10: fatal error: 'string' file not found
#include <string>
         ^~~~~~~~
1 error generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/user1/tensorflow/tensorflow/tools/pip_package/BUILD:252:10 Middleman _middlemen/tensorflow_Stools_Spip_Upackage_Sbuild_Upip_Upackage-runfiles failed: (Exit 1): clang failed: error executing command (from target @com_google_protobuf//:protoc) 
  (cd /home/user1/.cache/bazel/_bazel_user1/2ed8e7afdea3ff827d1d2c14869018ce/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64 \
    PATH=/home/user1/.cache/bazelisk/downloads/sha256/6c25a6d716545d6b672ec46f770521cd9ebb63d73617b8f4e6747825d1db1839/bin:/home/user1/bin:/usr/local/cuda-12.2/bin:/home/user1/anaconda3/bin:/home/user1/anaconda3/condabin:/home/user1/.local/bin:/home/user1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin \
    PWD=/proc/self/cwd \
  /usr/lib/llvm-16/bin/clang -MD -MF bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/_objs/protoc/main.d '-frandom-seed=bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/_objs/protoc/main.o' '-DBAZEL_CURRENT_REPOSITORY=""com_google_protobuf""' -iquote external/com_google_protobuf -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/zlib -iquote external/bazel_tools -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/bazel_tools -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/zlib -fmerge-all-constants -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-12.2' -g0 -w -g0 '-std=c++17' -c external/com_google_protobuf/src/google/protobuf/compiler/main.cc -o bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/_objs/protoc/main.o)
# Configuration: be1b6d30ad6b895c5d0cb4e11b08ee9fdfd7ce804f01eb0679a5e177ceb7e39b
# Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 0.200s, Critical Path: 0.08s
INFO: 44 processes: 42 internal, 2 local.
FAILED: Build did NOT complete successfully


### Standalone code to reproduce the issue

```shell
bazel build //tensorflow/tools/pip_package:build_pip_package --verbose_failures
```


### Relevant log output

_No response_"
61955,linalg.svd - complex64 - NaN - bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf nightly, 2.13, 2.9 using docker container

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04 with Docker

### Python version

3.9

### Current behavior?

For a special matrix, as I tested, a `tf.complex64` matrix without **NaN**, the `tf.linalg.svd` gives the singular values with **NaN**, and the singular vector `u` and `v` also have **NaN**. However, numpy does not give any NaN. The tiny code and its output have provided, and u can also download it from [my github repo](https://github.com/yhao-z/tensorflow-svd-NaN).

**Notes:**
- i test this code in docker container, which is created from the offical images released by tensorflow.
- this code is normal in nightly edition of Linux Docker and 2.9 of Windows.
- the code encounter error in 2.9 and 2.13 editions of Linux Docker.
- based on the conflict results for different platform and editions, i think this issue may be a bug.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

M = np.load('NaN_matrix.npy')
print(tf.reduce_any(tf.math.is_nan(tf.abs(M))).numpy()) # False, no nan in M

# tensorflow svd
[s,u,v] = tf.linalg.svd(M)

print(s.numpy()[0:9]) # [nan 0.01783315 0.00682789 0.00398225 0.00252413 0.0014872 0.00082805 0.00066117 0.00051815]
print(tf.reduce_any(tf.math.is_nan(s)).numpy())         # True, nan in s
print(tf.reduce_any(tf.math.is_nan(tf.abs(u))).numpy()) # True, nan in u
print(tf.reduce_any(tf.math.is_nan(tf.abs(v))).numpy()) # True, nan in v

# numpy svd
[u,s,v] = np.linalg.svd(M)

print(s[0:9]) # [0.01783314 0.00682789 0.00398224 0.00252413 0.0014872 0.00082805 0.00066117 0.00051815 0.00050564]
print(np.isnan(s).any()) # False, no nan in s
print(np.isnan(u).any()) # False, no nan in u
print(np.isnan(v).any()) # False, no nan in v
```
"
61954,ExtensionType to be considered a complete nested structure,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13  (& 2.15 nightly)

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Tensorflow's current experimental [ExtensionType](https://www.tensorflow.org/api_docs/python/tf/experimental/ExtensionType) is a very useful data structure to replace python data structures when the modelled tensors are in fact more complex structures of tensors of varying shapes and types. One would intuitively think that this class would click with `tf.nest`, but unfortunately they are not among the valid nested structures:

```
collections.abc.Sequence (except string and bytes). This includes list, tuple, and namedtuple.
collections.abc.Mapping (with sortable keys). This includes dict and collections.OrderedDict.
collections.abc.MappingView (with sortable keys).
```

as taken from [Module: tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest).

Moreover, `tf.nest` is not registered as an api for [@tf.experimental.dispatch_for_api(tf_api)](https://www.tensorflow.org/api_docs/python/tf/experimental/dispatch_for_api), which would be helpful for redifining the behaviour of nesting inside an `ExtensionType`.

Currently, the only solutions that I have found were either to:

1. Use multiple inheritance and also inherit from one of the valid nested structures in order to allow `tf.nest` to work as an api. This also requires implementing all the abstract methods of the superclass which may not make sense all the time.
2. Rewrite all the functionalities that use nesting as methods inside the class implementing `ExtensionType`, which is not great when you are seeking code reusability.
3. Have a method to change the type to something that can be considered a structure and have that called whenever `tf.nest`ing is used, which as well requires some changes everywhere in the code base. 


### Standalone code to reproduce the issue

```python
# Not inheriting from Sequence

import tensorflow as tf

class MyComplexData(tf.experimental.ExtensionType):
    x: tf.Tensor
    y: tf.Tensor

    def __init__(self, x, y):
        self.x = x
        self.y = y

def gather_fn(tensor):
    indices = tf.constant([[0], [1]])
    return tf.gather_nd(tensor, indices)

data = MyComplexData(tf.constant([1, 2, 3]), tf.constant([[1.0, 2.0], [3.0, 4.0]]))

try:
    mapped_data = tf.nest.map_structure(gather_fn, data)
    print(mapped_data)
except Exception as e:
    print(f""Failed to apply map_structure with gather_fn on MyComplexData: {e}"")
```
```python
# Inheriting from Sequence

import tensorflow as tf
from collections.abc import Sequence

class MyComplexDataSequence(tf.experimental.ExtensionType, Sequence):
    x: tf.Tensor
    y: tf.Tensor

    def __init__(self, x, y):
        self.x = x
        self.y = y

    def __getitem__(self, index):
        return [self.x, self.y][index]
    
    def __len__(self):
        return 2

def gather_fn(tensor):
    indices = tf.constant([[0], [1]])
    return tf.gather_nd(tensor, indices)

data_seq = MyComplexDataSequence(tf.constant([1, 2, 3]), tf.constant([[1.0, 2.0], [3.0, 4.0]]))

try:
    mapped_data_seq = tf.nest.map_structure(gather_fn, data_seq)
    print(mapped_data_seq)
except Exception as e:
    print(f""Failed to apply map_structure with gather_fn on MyComplexDataSequence: {e}"")
```


### Relevant log output

```shell
# Not inheriting from Sequence

Failed to apply map_structure with gather_fn on MyComplexData: Attempt to convert a value (MyComplexData(x=<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>, y=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[1., 2.],
       [3., 4.]], dtype=float32)>)) with an unsupported type (<class '__main__.MyComplexData'>) to a Tensor.
```

```shell
# Inheriting from Sequence

MyComplexDataSequence(x=<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>, y=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[1., 2.],
       [3., 4.]], dtype=float32)>)
```
"
61953,SpectralNormalization layer is not trainable. Please help (OperatorNotAllowedInGraphError: Exception encountered when calling layer 'spectral_normalization' (type SpectralNormalization).),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.3 LTS and Google Colab

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cudatoolkit=11.8.0, nvidia-cudnn-cu11==8.6.0.163

### GPU model and memory

_No response_

### Current behavior?

SpectralNormalization layer is not trainable. Whenever I try to use the ""model.fit"" method, TF outputs the error ""Using a symbolic `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.""

Please help

Best

Kav

### Standalone code to reproduce the issue

```shell
LINK TO COLAB NOTEBOOK:
https://colab.research.google.com/drive/1TYoNIrrpk-bLBqpzNJXq5VOI6Mpln5Ty?usp=sharing


STANDALONE CODE:
batch = 1
height = 10
width = 10
channels = 1
filters = 4
kernel_size = 3

x_input = Input(shape=(height, width, channels))
conv2d = SpectralNormalization(Conv2D(filters, kernel_size))
x_output = conv2d(x_input)
model = Model(x_input, x_output)
model.compile(loss='mse')

x = np.random.rand(batch, height, width, channels)
y = np.random.rand(batch, height, width, filters)

model.fit(x, y)
```


### Relevant log output

```shell
---------------------------------------------------------------------------

OperatorNotAllowedInGraphError            Traceback (most recent call last)

<ipython-input-6-d3dc977168f5> in <cell line: 1>()
----> 1 model.fit(x, y)

1 frames

/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py in autograph_handler(*args, **kwargs)
     50     except Exception as e:  # pylint:disable=broad-except
     51       if hasattr(e, ""ag_error_metadata""):
---> 52         raise e.ag_error_metadata.to_exception(e)
     53       else:
     54         raise

OperatorNotAllowedInGraphError: in user code:

    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1338, in train_function  *
        return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1322, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1303, in run_step  **
        outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1080, in train_step
        y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None

    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'spectral_normalization' (type SpectralNormalization).
    
    Using a symbolic `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.
    
    Call arguments received by layer 'spectral_normalization' (type SpectralNormalization):
      â€¢ inputs=tf.Tensor(shape=(None, 10, 10, 1), dtype=float32)
      â€¢ training=True
```
"
61951,Can not find strtod_l function on Android device,"### 1. System information

- OS Platform and Distribution: Android 5.1, OPPO device
- TensorFlow library : Tensorflow lite 2.13.0

When I try to load model and get error:

`E/art: dlopen(""/data/app/com.app.demo-1/lib/arm64/libtensorflowlite_jni.so"", RTLD_LAZY) failed: dlopen failed: cannot locate symbol ""strtod_l"" referenced by ""/data/app/com.app.demo-1/lib/arm64/libtensorflowlite_jni.so""...`"
61950,TFLite benchmark tool with default cmake build shows weird time duration on DEPTHWISE_CONV_2D ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.9.3, tf 2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.0

### GCC/compiler version

gcc 9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following the guide for [build_cmake](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md)
`cmake ../tensorflow_src/tensorflow/lite`
then build the benchmark tool & label_image:
```
cmake --build . -j -t benchmark_model
cmake --build . -j -t label_image
```
I tested using the model [here](https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_quant_and_labels.zip).

The results shows:
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       15	    38.265	    58.415%	    58.415%	     0.000	       15
	       DEPTHWISE_CONV_2D	       13	    27.230	    41.569%	    99.985%	     0.000	       13
	         AVERAGE_POOL_2D	        1	     0.007	     0.011%	    99.995%	     0.000	        1
	                 SOFTMAX	        1	     0.003	     0.005%	   100.000%	     0.000	        1
	                 RESHAPE	        1	     0.000	     0.000%	   100.000%	     0.000	        1

In theory, 13 x DEPTHWISE_CONV_2D should consume MUCH less time than 15 CONV_2, but it is not the case here.

However, if I use the pre-built binary from [here](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/linux_x86-64_benchmark_model), the result is correct.


### Standalone code to reproduce the issue

```shell
benchmark_model --graph=./mobilenet_quant_v1_224.tflite --enable_op_profiling=true
```


### Relevant log output

```shell
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       15	    38.265	    58.415%	    58.415%	     0.000	       15
	       DEPTHWISE_CONV_2D	       13	    27.230	    41.569%	    99.985%	     0.000	       13
	         AVERAGE_POOL_2D	        1	     0.007	     0.011%	    99.995%	     0.000	        1
	                 SOFTMAX	        1	     0.003	     0.005%	   100.000%	     0.000	        1
	                 RESHAPE	        1	     0.000	     0.000%	   100.000%	     0.000	        1
```
"
61949,A call of the model which is reloaded from SavedModel format produces a TypeError,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

A call of the model which is reloaded from SavedModel format produces a `TypeError: '_UserObject' object is not callable`.
All necessary code is in [this tutorial](https://www.tensorflow.org/text/tutorials/transformer). Below I show code lines from there, where the error occurs.

### Standalone code to reproduce the issue

```shell
reloaded = tf.saved_model.load('translator')
reloaded('este Ã© o primeiro livro que eu fiz.').numpy()
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/molokanov/tensorflow/check.py"", line 13, in <module>
    reloaded('este Ã© o primeiro livro que eu fiz.').numpy()
TypeError: '_UserObject' object is not callable
```
"
61948,TensorFlow.js: export failure âŒ 0.6s: [Errno 2] No such file or directory: 'tensorflowjs_converter',"System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC OS
TensorFlow installed from (source or binary): binray
TensorFlow version (use command below): 2.13.0
TensorFlowJS version: 4.11.0
Python version: 3.9
When I using tensorflowjs_converter to convert pt model:

TensorFlow SavedModel: export success âœ… 29.7s, saved as yolov5n-seg_saved_model (7.9 MB)

TensorFlow GraphDef: starting export with tensorflow 2.13.0...
TensorFlow GraphDef: export success âœ… 3.7s, saved as yolov5n-seg.pb (7.9 MB)
WARNING âš ï¸ invalid check_version(4.11.0, ) requested, please check values.

TensorFlow.js: starting export with tensorflowjs 4.11.0...
TensorFlow.js: export failure âŒ 0.6s: [Errno 2] No such file or directory: 'tensorflowjs_converter'

tensorflowjs_converter not founded"
61947,TF 2.12: InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2.12

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

GTX970

### Current behavior?


I have install `pip install tensorflow ==2.12`. 
I followed the page: https://www.tensorflow.org/install/pip
and install 
`conda install -c conda-forge cudatoolkit=11.8.0` and
`nvidia-cudnn-cu11==8.6.0.163`
From the build from source it says that it requires cuda 11.8 
This is determined here `nvcc -version`.
```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Wed_Sep_21_10:33:58_PDT_2022
Cuda compilation tools, release 11.8, V11.8.89
Build cuda_11.8.r11.8/compiler.31833905_0
```

I tried to see if it is using GPU and it does. 

``` 2023-09-22 13:17:18.164612: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-22 13:17:18.813461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-22 13:17:21.116323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.117135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.117901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.118662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.141750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.142582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.143350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.144117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.144878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.145642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.146398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.147155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')] ```

However, when I tried to create a neural network: 
It gives me the following error: 

```shell
Traceback (most recent call last):
  File ""/home/trevor/distinguisher/main.py"", line 73, in <module>
    net_pp = train_preprocessor_triplet_loss(n=10000, nr=num_rounds, epochs=epoch)
  File ""/home/trevor/distinguisher/main.py"", line 53, in train_preprocessor_triplet_loss
    net_pp = make_resnet_preprocess(depth=1)
  File ""/home/trevor/distinguisher/main.py"", line 26, in make_resnet_preprocess
    conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(perm)
  File ""/home/trevor/anaconda3/envs/tf_2/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/trevor/anaconda3/envs/tf_2/lib/python3.9/site-packages/keras/backend.py"", line 2101, in random_uniform
    return tf.random.stateless_uniform(
tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version
```

I have searched online and say that the CUDA version is too high but from build it seems to be 11.8.
May I know what is the issue here? 







### Standalone code to reproduce the issue

```shell
def make_resnet_preprocess(num_blocks=2, num_filters=32, num_outputs=1, d1=64, d2=64, word_size=16, ks=3, depth=5,
                           reg_param=0.0001, final_activation='sigmoid'):
    # Input and preprocessing layers
    inp = Input(shape=(num_blocks * word_size * 2,))
    rs = Reshape((2 * num_blocks, word_size))(inp)
    perm = Permute((2, 1))(rs)
    # add a single residual layer that will expand the data to num_filters channels
    # this is a bit-sliced layer
    conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(perm)
    conv0 = BatchNormalization()(conv0)
    conv0 = Activation('relu')(conv0)
    # add residual blocks
    shortcut = conv0
    for i in range(depth):
        conv1 = Conv1D(num_filters, kernel_size=ks, padding='same', kernel_regularizer=l2(reg_param))(shortcut)
        conv1 = BatchNormalization()(conv1)
        conv1 = Activation('relu')(conv1)
        conv2 = Conv1D(num_filters, kernel_size=ks, padding='same', kernel_regularizer=l2(reg_param))(conv1)
        conv2 = BatchNormalization()(conv2)
        conv2 = Activation('relu')(conv2)
        shortcut = Add()([shortcut, conv2])
    # add prediction head
    flat1 = Flatten()(shortcut)
    dense1 = Dense(d1, kernel_regularizer=l2(reg_param))(flat1)
    dense1 = BatchNormalization()(dense1)
    dense1 = Activation('relu')(dense1)
    dense2 = Dense(d2, kernel_regularizer=l2(reg_param))(dense1)
    dense2 = BatchNormalization()(dense2)
    out = Activation('relu')(dense2)
    # out = Dense(num_outputs, activation=final_activation, kernel_regularizer=l2(reg_param))(dense2)
    model = Model(inputs=inp, outputs=out)
    return (model)
net_pp = train_preprocessor_triplet_loss(n=10 ** 7, nr=num_rounds, epochs=epoch)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/trevor/distinguisher/main.py"", line 73, in <module>
    net_pp = train_preprocessor_triplet_loss(n=10000, nr=num_rounds, epochs=epoch)
  File ""/home/trevor/distinguisher/main.py"", line 53, in train_preprocessor_triplet_loss
    net_pp = make_resnet_preprocess(depth=1)
  File ""/home/trevor/distinguisher/main.py"", line 26, in make_resnet_preprocess
    conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(perm)
  File ""/home/trevor/anaconda3/envs/tf_2/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/trevor/anaconda3/envs/tf_2/lib/python3.9/site-packages/keras/backend.py"", line 2101, in random_uniform
    return tf.random.stateless_uniform(
tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version```
```
"
61946,"TFLite with default cmake build, xnnpack does not work for quantized model","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.9.3, tf 2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.0

### GCC/compiler version

gcc 9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following the guide for [build_cmake](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md)
`cmake ../tensorflow_src/tensorflow/lite`
then build the benchmark tool & label_image:
```
cmake --build . -j -t benchmark_model
cmake --build . -j -t label_image
```
I tested using the model [here](https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_quant_and_labels.zip).

Both the benchmark_model tool & label_image example do **NOT** show different inference time, whether using xnnpack or not. The log shows _""Though XNNPACK delegate is explicitly applied, the model graph will not be executed by the `delegate""_`

**However, using the bazel build with following definitions:**
```
--define tflite_with_xnnpack_qs8=true
--define tflite_with_xnnpack_qu8=true
--define tflite_with_xnnpack=true
```
did show that using xnnpack, the inference time is 3x less.


### Standalone code to reproduce the issue

```shell
# for benchmark, with xnnpack
benchmark_model --graph=./mobilenet_quant_v1_224.tflite --use_xnnpack=true
# for benchmark, no xnnpack
benchmark_model --graph=./mobilenet_quant_v1_224.tflite --use_xnnpack=false
# label_image, with xnnpack
label_image -m ./mobilenet_quant_v1_224.tflite -i ./grace_hopper.bmp  -l labels.txt -x 1
# label_image, no xnnpack
label_image -m ./mobilenet_quant_v1_224.tflite -i ./grace_hopper.bmp  -l labels.txt -x 0
```


### Relevant log output

```shell
Log from benchmark_model with cmake build:
STARTING!
Log parameter values verbosely: [0]
Graph: [./mobilenet_quant_v1_224.tflite]
Use xnnpack: [1]
Loaded model ./mobilenet_quant_v1_224.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
XNNPACK delegate created.
Though XNNPACK delegate is explicitly applied, the model graph will not be executed by the delegate.
The input model file size (MB): 4.27635
Initialized session in 0.609ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=6 first=88887 curr=84456 min=84307 max=88887 avg=85234.3 std=1638
...
=========================================
Log from benchmark_model with bazel build:
STARTING!
Log parameter values verbosely: [0]
Graph: [./mobilenet_quant_v1_224.tflite]
Use xnnpack: [1]
Loaded model tflite_build_native_old/examples/label_image/mobilenet_quant_v1_224.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
XNNPACK delegate created.
Explicitly applied XNNPACK delegate, and the model graph will be partially executed by the delegate w/ 2 delegate kernels.
The input model file size (MB): 4.27635
Initialized session in 4.844ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=41 first=14002 curr=12270 min=12128 max=14002 avg=12400.6 std=398
...
```
"
61945,The metrics values in fit and in evaluate do not match (tf+keras),"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.9

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have the issue which I described under in detail. Might be it's a my mistake but I try to fix it not a one day.

I simplified it as much as possible fitting the model to show you exactly an issue.

Let it be not a pre-trained model ResNet50. And let's I provide a dataset by easy way where X_train and Y_train are the ndarray with correspond shapes [N, 32, 32, 3] and [N, class_num=3].

I set batch_size=128, shuffle=False. Now I fitting my model and after the last epoch I get the metric equals 0.976, BUT If I just make model.evaluate(X_train), where X_train is the same data I use in fit I get absolutely different value - 0.456. The question is - WHY?

That's logs of my fitting: 263/263 [==============================] - 7s 27ms/step - loss: 0.2063 - auc: 0.9899 - mc_f1: 0.9326

That's after evaluate: 1052/1052 [==============================] - 11s 9ms/step - loss: 0.6186 - auc: 0.9053 - mc_f1: 0.4993

To prevent questions - mc_f1 is a custom metric - averaged f1 calculated for each class separately for the multiclass case.

So the code:

    # Change last layers
    last_layer = model.output
    output = tf.keras.layers.Dense(classes, activation=""softmax"")(last_layer)
    model = tf.keras.models.Model(inputs=model.inputs, outputs=output)

    return model


mcf1 = MulticlassF1(num_classes=3)

resnet50 = get_resnet50_model([32, 32, 3], 3)
resnet50.compile(
    optimizer=Adam(),
    loss=tf.keras.losses.CategoricalCrossentropy(),
    metrics=[tf.metrics.AUC(name='auc'), mcf1],
)
```


Fitting:

```
deb_metric_callback = DebugMetricCallback(resnet50, (X_train, Y_train), mcf1)
resnet50.fit(
    X_train, Y_train,
    epochs=100,
    batch_size=128,
    # verbose=0,
    shuffle=False,
    callbacks=[deb_metric_callback],
)
```

I have the issue which I described under in detail. Might be it's a my mistake but I try to fix it not a one day.

I simplified it as much as possible fitting the model to show you exactly an issue.

Let it be not a pre-trained model ResNet50. And let's I provide a dataset by easy way where X_train and Y_train are the ndarray with correspond shapes [N, 32, 32, 3] and [N, class_num=3].

I set batch_size=128, shuffle=False. Now I fitting my model and after the last epoch I get the metric equals 0.976, BUT If I just make model.evaluate(X_train), where X_train is the same data I use in fit I get absolutely different value - 0.456. The question is - WHY?

That's logs of my fitting: 263/263 [==============================] - 7s 27ms/step - loss: 0.2063 - auc: 0.9899 - mc_f1: 0.9326

That's after evaluate: 1052/1052 [==============================] - 11s 9ms/step - loss: 0.6186 - auc: 0.9053 - mc_f1: 0.4993

To prevent questions - mc_f1 is a custom metric - averaged f1 calculated for each class separately for the multiclass case.

So the code:

The metric:
```

class MulticlassF1(tf.keras.metrics.Metric):

    def __init__(self, name='mc_f1', num_classes=None, **kwargs):
        super(MulticlassF1, self).__init__(name=name, **kwargs)
        self.__zero_support = tf.cast(1e-7, dtype=tf.float16)
        self.__cm = self.add_weight(name='fn', initializer='zeros', shape=[num_classes, num_classes])
        if num_classes is not None:
            self.__num_classes = num_classes

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_pred = K.argmax(y_pred, axis=1)
        y_true = K.argmax(y_true, axis=1)
        m = tf.math.confusion_matrix(y_true, y_pred, num_classes=self.__num_classes, dtype=tf.float32)
        self.__cm.assign_add(m)

    def reset_state(self):
        self.__cm.assign(tf.zeros((self.__num_classes, self.__num_classes)))

    def result(self):
        denominator = 0
        m = self.__cm
        for i in range(m.shape[0]):
            tp = m[i, i]
            fn = K.sum(m[:, i]) - tp
            fp = K.sum(m[i, :]) - tp
            tn = K.sum(K.flatten(m)) - (tp + fn + fp)
            tp = K.cast(tp, dtype=tf.float16)
            tn = K.cast(tn, dtype=tf.float16)
            fp = K.cast(fp, dtype=tf.float16)
            fn = K.cast(fn, dtype=tf.float16)
            precision = tp / ((tp + fp) + self.__zero_support) + self.__zero_support
            recall = tp / (tf.cast(tp + fn, dtype=tf.float16) + self.__zero_support) + self.__zero_support
            denominator += (1 / precision + 1 / recall)
        f1_combined = K.cast(2 * m.shape[0] / denominator, dtype=tf.float32)
        return f1_combined
```
The model declaration

```
def get_resnet50_model(window_size, classes):
    # Model initialization
    model = tf.keras.applications.ResNet50V2(
        include_top=False,
        weights=None,
        input_tensor=None,
        input_shape=window_size,
        pooling='max',
        classes=classes,

    )

    # Change last layers
    last_layer = model.output
    output = tf.keras.layers.Dense(classes, activation=""softmax"")(last_layer)
    model = tf.keras.models.Model(inputs=model.inputs, outputs=output)

    return model


mcf1 = MulticlassF1(num_classes=3)

resnet50 = get_resnet50_model([32, 32, 3], 3)
resnet50.compile(
    optimizer=Adam(),
    loss=tf.keras.losses.CategoricalCrossentropy(),
    metrics=[tf.metrics.AUC(name='auc'), mcf1],
)
```
Fitting:

```
deb_metric_callback = DebugMetricCallback(resnet50, (X_train, Y_train), mcf1)
resnet50.fit(
    X_train, Y_train,
    epochs=100,
    batch_size=128,
    # verbose=0,
    shuffle=False,
    callbacks=[deb_metric_callback],
)
```
I tried to do some debug and make a CallBack:

```
class DebugMetricCallback(Callback):
    def __init__(self, test, metr):
        super().__init__()
        self.test = test
        self.metric = metr


    def on_epoch_end(self, epoch, logs=None):
        print(self.metric.result())
        x, y = self.test
        y_pred = self.model.predict(x)
        inline_measure = MulticlassF1(num_classes=3)
        inline_measure.update_state(y, y_pred)
        print(self.metric.result().numpy(), inline_measure.result().numpy())
```

Here I get absolutely different confusion matrices.

Also I thought that this is the problem with BatchNormalization, but I save and compare model before and after evaluate it wasn't changed.

### Standalone code to reproduce the issue

```shell
https://github.com/xxraytz/temp.git
```


### Relevant log output

_No response_"
61944,"TF Lite runtime error: ""Didn't find op for builtin opcode 'PLACEHOLDER_FOR_GREATER_OP_CODES' version '1'""","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11, Python 3.10.11
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13.0

### 2. Code

Repro steps:

1) Download the `lite-model_efficientnet_lite4_uint8_2.tflite` model from here: https://tfhub.dev/tensorflow/lite-model/efficientnet/lite4/uint8/2 (click the 14.34Mb download [link](https://tfhub.dev/tensorflow/lite-model/efficientnet/lite4/uint8/2?lite-format=tflite) ; leave this page open in your browser)
2) Start a TF Lite application using Android or Flutter. I use Flutter and the latest ML Kit.
3) Use the model in your code : it works perfectly fine.
4) Back in your browser, now click the `TF` tab on the left side on the page your left open. The TF tab allows you to download the TF (not TF lite) version of the model. Download the 46.61Mb model file `efficientnet_lite4_classification_2.tar.gz` ([here](https://tfhub.dev/tensorflow/efficientnet/lite4/classification/2)). Uncompress this archive into a `saved_model` folder somewhere.
5) Use the following code to convert the TF model into a TF Lite model;

```
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, tags='train')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
outputPath = os.path.join(cur_dir, 'tf_converted_to_tflite.tflite')
fo = open(outputPath, ""wb"")
fo.write(tflite_model)
fo.close()
```
6) Following the instructions from https://www.tensorflow.org/lite/models/convert/metadata, add model metadata by using `metadata_writer_for_image_classifier.py`, and with the following model specification (updated lines ~60 to 70, as instructed):

```
_MODEL_INFO = {
    ""tf_converted_to_tflite.tflite"":
        ModelSpecificInfo(
            name=""EfficientNetB4 image classifier"",
            version=""v1"",
            image_width=380, # As specified in the TF tab in the web page (not the TF Lite tab, which input dimension is listed as 300x300)
            image_height=380, #
            image_min=0,
            image_max=255,
            mean=[127.5],
            std=[127.5],
            num_classes=1000,
            author=""TensorFlow"")
}
```
The processing completes normally.

7) Now use that model `tf_converted_to_tflite.tflite` instead of the original, readily-available TF Lite model available on TF Hub, `lite-model_efficientnet_lite4_uint8_2.tflite` (that was working fine).
In other words, we are now trying to use approximately the same model, except that this time we ran the TF -> TF Lite model conversion ourselves (quantization is a bit different, but it does not matter at all).
8) Run the same application. The following PlatformException is raised:

```
""com.google.mlkit.common.MlKitException: Failed to initialize detector.
Didn't find op for builtin opcode 'PLACEHOLDER_FOR_GREATER_OP_CODES' version '1'.
An older version of this builtin might be supported.
Are you using an old TFLite binary with a newer model?
```

### 3. Failure after conversion

I would expect the conversion process and runtime behavior to be smooth considered the model is hosted on TF Hub: the TF Lite model proposed by TF Hub works fine, but if I manually convert the TF model myself, it doesn't.
Is there anything wrong in the above conversion code? I have been debugging this for days, to no avail.
I have another (custom) model with a similar bug, and so far this is the best repro I could build.

### 4. First investigations

One thing I figured is that I should try to use the same version of TensorFlow on my PC and on my embedded system - as suggested in the following post from the TensorFlow Team:

https://discuss.tensorflow.org/t/tensorflowlite-error-didnt-find-op-for-builtin-opcode-softmax-version-1-an-older-version-of-this-builtin-might-be-supported-are-you-using-an-old-tflite-binary-with-a-newer-model/3885/6

Currently I am using TensorFlow 2.13.0 on my PC, however, how can I determine the version used in Flutter ? I use ML Kit on the Flutter side, which uses, on Android, the following Maven artifact:

https://mvnrepository.com/artifact/com.google.mlkit/image-labeling-custom/17.0.1

Which TF version is this artifact using?
"
61942,calling Model in a loop would leak memory,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.13.0

### Custom code

Yes

### OS platform and distribution

Windows Server 2019

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

there is a transformer model when i try to decode messages, translate input sentence to target sentence i get memory blow up, memory is good when i use transformer.fit(), but in a loop like below it blows up memory, tf.keras.backend.clear_session() doesâ€™t help, also accuracy decrease when i use that, gc.collect() doesnâ€™t work also
here is my code

```python
def decode_sequence(input_sentence):
    tokenized_input_sentence = input_vectorization([input_sentence])
    decoded_sentence = START_TOKEN
    for i in tf.range(max_decoded_sentence_length):
        tokenized_target_sentence = output_vectorization([decoded_sentence])#[:, :-1]
        
        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])
        
        
        sampled_token_index = np.argmax(predictions[0, i, :])
        sampled_token = output_index_lookup[sampled_token_index]
        decoded_sentence += sampled_token

        if sampled_token == END_TOKEN:
            break
    
   
    gc.collect()
    return decoded_sentence

from tqdm import tqdm
def overall_accuracy(pairs):
    corrects = 0
    inputs = pairs[2739:]
    iter = tqdm(inputs)
    for i, pair in enumerate(iter):
        input_text = pair[0]
        target = pair[1]
        predicted = decode_sequence(input_text)
        #guess = 'âœ“' if predicted == target else 'âœ—'
        #print('Sample Number : ', i, 'Predicted : ', predicted, 'Real : ', target, guess)
        if predicted == target:
            corrects += 1
        iter.set_postfix(corrects=corrects, accuracy=corrects / (i + 1))
    
    return corrects / len(inputs)

print(""Overall Acurracy : "", overall_accuracy(test_pairs))```

### Standalone code to reproduce the issue

```shell
calling model in a loop
```


### Relevant log output

_No response_"
61939,XLA autocluster will treat `-0.0` as `0.0`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230921

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

XLA autocluster will treat `-0.0` as `0.0`. That said, it will automatically transfer `-0.0` to `0.0`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

""""""
Without Autocluster
""""""
class Model(tf.keras.Model):
    def __init__(self):
        super(Model, self).__init__()
        self.v3_weight = tf.Variable(123.45)

    def call(self, x1):
        x4 = (self.v3_weight * (- 0.0))
        return x4
m = Model()
x = tf.constant(702.89)
print(m(x)) # tf.Tensor(-0.0, shape=(), dtype=float32)

""""""
With Autocluster
""""""
import os
os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit'
class Model(tf.keras.Model):
    def __init__(self):
        super(Model, self).__init__()
        self.v3_weight = tf.Variable(123.45)
    
    @tf.function
    def call(self, x1):
        x4 = (self.v3_weight * (- 0.0))
        return x4
m = Model()
x = tf.constant(702.89)
print(m(x)) # tf.Tensor(0.0, shape=(), dtype=float32)
```


### Relevant log output

_No response_"
61938,XLA compiled `Embedding` could work for out-of-bound input,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230921

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

XLA compiled `Embedding` could work for out-of-bound input. In the following code, the model has an embedding layer using `tf.keras.layers.Embedding(64, 128)`, which means it has 64 possible tokens (usually representing words or items, indexed from 0 to 63).

The input to it is `tf.constant([64], dtype=tf.int32)`, which is out of range because 64 exceeds the highest valid token index 63.

If we run the model without XLA compilation, it will raise an error as expected. However, after XLA compilation, the model could process such invalid input.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
""""""
XLA compiled
""""""
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(64, 128)

    @tf.function(jit_compile=True)
    def call(self, x1, x2):
        x3 = self.embedding(x1)
        return (x3 * x2)

tf.random.set_seed(42)
m = Model()
input_1 = tf.constant([64], dtype=tf.int32)
input_2 = tf.constant([[[[10.0]]]], dtype=tf.float32)
print(m(input_1, input_2))

""""""
Without XLA
""""""
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(64, 128)

    def call(self, x1, x2):
        x3 = self.embedding(x1)
        return (x3 * x2)

tf.random.set_seed(42)
m = Model()
input_1 = tf.constant([64], dtype=tf.int32)
input_2 = tf.constant([[[[10.0]]]], dtype=tf.float32)
print(m(input_1, input_2))

""""""
InvalidArgumentError: Exception encountered when calling layer 'embedding_4' (type Embedding).

{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0] = 64 is not in [0, 64) [Op:ResourceGather] name: 
""""""
```


### Relevant log output

_No response_"
61936,lite hexagonåŽç«¯ResizeNearestNeighborç®—å­å±‚æ‰§è¡ŒæŠ¥é”™,"**System information**
- OS Platform and Distribution ï¼šLinux Ubuntu 18.04
- TensorFlow installed from (github):
- TensorFlow version (devåˆ†æ”¯):


**Provide the text output from tflite_convert**
none

**Standalone code to reproduce the issue** 

tensorflow/lite/delegates/hexagon/builders/resize_nearest_neighbor_builder.cc

**Any other info / logs**
STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [1]
Graph: [/home/wangzhiqun/yolox_resize_nearest_neighbor.tflite]
Use Hexagon: [1]
Loaded model /home/wangzhiqun/yolox_resize_nearest_neighbor.tflite
INFO: Initialized TensorFlow Lite runtime.
Hexagon delegate created.
INFO: TfLiteHexagonDelegate delegate: 1 nodes delegated out of 1 nodes with 1 partitions.

INFO: Replacing 1 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 1 partitions.
[hexagon/nn] Add Node: tid(-1) nid(1)
[hexagon_nn] hexagon_nn_append_const_node(gid=1 tid=2 nid=2 type=0x3)
[hexagon/nn] Add Node from Op(331) mid(0) nid(3)
[hexagon_nn] hexagon_nn_append_const_node(gid=1 nid=4 type=0x3)
[hexagon_nn] hexagon_nn_append_const_node(gid=1 nid=5 type=0x3)
[hexagon/nn] Add Node: tid(-1) nid(6)
[hexagon_nn] hexagon_nn_append_node(gid=1 nid=1 type=0x0)
[hexagon_nn] hexagon_nn_append_node(nis=0 nos=1)
[hexagon_nn] hexagon_nn_append_node(outputs[0].elementsize=1)
[hexagon_nn] hexagon_const_node(gid=1 nid=2 type=0x3)
[hexagon_nn] hexagon_nn_append_node(gid=1 nid=3 type=0x14b)
[hexagon_nn] hexagon_nn_append_node(nis=6 nos=3)
Error adding node: id:3, op_type:331
[hexagon_nn] hexagon_const_node(gid=1 nid=4 type=0x3)
[hexagon_nn] hexagon_const_node(gid=1 nid=5 type=0x3)
[hexagon_nn] hexagon_nn_append_node(gid=1 nid=6 type=0x1)
[hexagon_nn] hexagon_nn_append_node(nis=1 nos=0)
----------------
Timestamp: Thu Sep 21 08:06:06 2023


Log
hexagon/src/newnode.c:413:node 3 (ResizeNearestNeighbor_8): bad input count 6
hexagon/src/newnode.c:763:node id=0x3 ctor fail
hexagon/src/prepare.c:4830:input 0x6:0 refers to nonexistent node 0x3
hexagon/src/prepare.c:4644:can't find id 0x3

----------------
ERROR: Failed: Failed to prepare graph.
.
ERROR: Node number 1 (TfLiteHexagonDelegate) failed to prepare.
ERROR: Restored original execution plan after delegate application failure.
Failed to apply Hexagon delegate.
Benchmarking failed.

"
61935,lite hexagonåŽç«¯ResizeNearestNeighborç®—å­å±‚æ‰§è¡ŒæŠ¥é”™,"hexagon/src/newnode.c:413:node 3 (ResizeNearestNeighbor_8): bad input count 6
hexagon/src/newnode.c:763:node id=0x3 ctor fail
hexagon/src/prepare.c:4830:input 0x6:0 refers to nonexistent node 0x3
hexagon/src/prepare.c:4644:can't find id 0x3

----------------
ERROR: Failed: Failed to prepare graph.
.
ERROR: Node number 1 (TfLiteHexagonDelegate) failed to prepare.
ERROR: Restored original execution plan after delegate application failure.
Failed to apply Hexagon delegate.
Benchmarking failed.

"
61934,The Depthwise Convolution operation result is incorrect from interpreter,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am testing network inference using Interpreter for the '.tflite'  that supports dynamic input shape.

However, the operational results of Depthwise Convolution are different from what was expected.

I simply reproduced the issue.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1WCOZBuWTYU7kn4EcjHHrG_fZm7mZcojN?usp=drive_link
```


### Relevant log output

```shell
# input : Inputs corresponding to output [0, 31, 33, 24] 
0) input[0, 61:64, 65:68, 24] 
[[0.978893   0.03620292 0.9301994 ]
 [0.34403667 0.4492776  0.8247143 ]
 [0.8075199  0.77361435 0.1642472 ]]

# weight: corresponding to output [0, 31, 33, 24]
1) weight[..., :24]
[[[ 0.09494528 -0.0925059   0.07752301]
  [-0.07614094  0.06238136 -0.03187032]
  [ 0.06566823  0.09477414 -0.05954333]]]

# bias: corresponding to output [0, 31, 33, 24]
2) bias[24]
0.024211471900343895

# tflite output: Interpreter result
# test output: Formula result of Depthwise Convolution
3) Deptwise Convoultion output
tflite output[0, 31, 33, 24] : 0.0
test output[0, 31, 33, 24]: 0.2780301570892334
```
"
61932,Can't get optimizer to apply gradients with Keras and DTensor based model,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0 (also tried with 2.9.1)

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Trying to do distributed training, for a rather wide model, with DTensors and Keras. I'm creating an Adam optimizer, trying to update it manually with the following code for a custom DTensor training step (this is within my own model object, with self.model being a Keras model):

 ```
  @tf.function
    def train_step(self, x, y, w, optimizer, train_metrics): 
        if not self.init:   # this runs once to initialize the model variables
            self.model(x)
            self.init = True 
        with tf.GradientTape() as tape:
            logits = self.model(x, training=True)
            logits = tf.reshape(logits, (logits.shape[1], logits.shape[0]))
            loss = tf.reduce_sum(tf.math.multiply(
                tf.keras.losses.binary_crossentropy(
                    y, logits, from_logits=True), w))
        gradients = tape.gradient(loss, self.model.trainable_variables)

        optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))

        loss_per_sample = loss / len(x)
        results = {'loss': loss_per_sample}
        for metric in train_metrics.values():
            metric.update_state(y_true=y, y_pred=logits)
        return results 
        
    def dtensor_fit(        
        self,
        x_train,
        y_train,
        x_val,
        y_val,
        w_train=None):
        num_epochs = 5
        train_metrics = {
            ""accuracy"" : metrics.Accuracy(),
            ""tp"": metrics.TruePositives(),
            ""fp"": metrics.FalsePositives(),
            ""fn"": metrics.FalseNegatives(),
            ""tn"": metrics.TrueNegatives(),
            ""auc"": metrics.AUC(curve=""PR""),
        }
        optimizer = tf.keras.dtensor.experimental.optimizers.Adam(mesh=self.mesh)

        eval_metrics = dict(train_metrics)   
        for epoch in range(num_epochs):
            print(""============================"") 
            print(""Epoch: "", epoch)
            for metric in train_metrics.values():
                metric.reset_state()
            step = 0
            results = {}
            pbar = tf.keras.utils.Progbar(target=None, stateful_metrics=[])
            def batch(x, y, w, n):
                num_samples = x.shape[0]
                l = 0
                while l < num_samples:
                    yield x[l:l+n], y[l:l+n], w[l:l+n]
                    l += n
            self.init = False
            for inputs, labels, weights in batch(x_train, y_train, w_train, NNModel.BATCH_SIZE):
                indices = np.transpose(inputs.nonzero())
                inputs.eliminate_zeros()
                values = inputs.data
                inputs, labels, weights = self.pack_dtensor_inputs(
                    tf.SparseTensor(indices=indices, values=values, dense_shape=(inputs.shape[0], self.input_size)),
                    tf.convert_to_tensor([labels], dtype =tf.bfloat16),
                    tf.convert_to_tensor([weights], dtype=tf.bfloat16), self.input_layout, self.label_layout, self.weight_layout)
                optimizer.build(self.model.trainable_variables)

                results.update(self.train_step(inputs, labels, weights, optimizer, train_metrics))
                for metric_name, metric in train_metrics.items():
                    results[metric_name] = metric.result()

                pbar.update(step, values=results.items(), finalize=False)
                step += 1 
``` 

I get the following error when I reach the apply_gradients step in the train_step function:

ValueError: in user code:

```
File ""/usr/local/home/bill/.cache/bazel/_bazel_bill/254c50c69c3701cca4e904bef759573b/execroot/_main/bazel-out/k8-fastbuild/bin/keystone/training/training.runfiles/_main/keystone/training/model.py"", line 263, in train_step  *
    optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
File ""/usr/local/home/bill/.cache/bazel/_bazel_bill/254c50c69c3701cca4e904bef759573b/execroot/_main/bazel-out/k8-fastbuild/bin/keystone/training/training.runfiles/rules_python~0.21.0~pip~pip_keras/site-packages/keras/dtensor/optimizers.py"", line 141, in apply_gradients  **
    optimizer_lib._BaseOptimizer.apply_gradients(self, grads_and_vars)
File ""/usr/local/home/bill/.cache/bazel/_bazel_bill/254c50c69c3701cca4e904bef759573b/execroot/_main/bazel-out/k8-fastbuild/bin/keystone/training/training.runfiles/rules_python~0.21.0~pip~pip_keras/site-packages/keras/optimizers/optimizer.py"", line 650, in apply_gradients
    iteration = self._internal_apply_gradients(grads_and_vars)
File ""/usr/local/home/bill/.cache/bazel/_bazel_bill/254c50c69c3701cca4e904bef759573b/execroot/_main/bazel-out/k8-fastbuild/bin/keystone/training/training.runfiles/rules_python~0.21.0~pip~pip_keras/site-packages/keras/dtensor/optimizers.py"", line 153, in _internal_apply_gradients
    optimizer_lib._BaseOptimizer._internal_apply_gradients(
File ""/usr/local/home/bill/.cache/bazel/_bazel_bill/254c50c69c3701cca4e904bef759573b/execroot/_main/bazel-out/k8-fastbuild/bin/keystone/training/training.runfiles/rules_python~0.21.0~pip~pip_keras/site-packages/keras/optimizers/optimizer.py"", line 680, in _internal_apply_gradients
    self._update_step(grad, var)
File ""/usr/local/home/bill/.cache/bazel/_bazel_bill/254c50c69c3701cca4e904bef759573b/execroot/_main/bazel-out/k8-fastbuild/bin/keystone/training/training.runfiles/rules_python~0.21.0~pip~pip_keras/site-packages/keras/optimizers/optimizer.py"", line 240, in _update_step
    self.update_step(gradient, variable)
File ""/usr/local/home/bill/.cache/bazel/_bazel_bill/254c50c69c3701cca4e904bef759573b/execroot/_main/bazel-out/k8-fastbuild/bin/keystone/training/training.runfiles/rules_python~0.21.0~pip~pip_keras/site-packages/keras/optimizers/adam.py"", line 194, in update_step
    m.assign_add((gradient - m) * (1 - self.beta_1))

ValueError: Dimensions must be equal, but are 5 and 0 for '{{node sub_2}} = Sub[T=DT_FLOAT](gradient_tape/keystone/feature/MatMul_1/Cast/Cast, sub_2/ReadVariableOp)' with input shapes: [8299614,5], [0].

```
It looks like the optimizer momentums are not of the correct shape. I tried explicitly adding `optimizer.build(self.model.trainable_variables) `after the `self.model(x)` in `train_step` to force correct population, but the variables are still wrong.

I have seen the same results with TF versions 2.9.1 and 2.13.0.

Is this a bug? How do I get the optimizer to be correctly set up for training?



### Standalone code to reproduce the issue

```shell
see above
```


### Relevant log output

_No response_"
61926,Failure to create build_pip_package Ubuntu 22.04 LTS / Python 3.10 / Cuda 11.7,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.9.1

### Custom code

No

### OS platform and distribution

Ubuntu 22.04 LTS

### Mobile device

/

### Python version

3.10

### Bazel version

5.0.0

### GCC/compiler version

11.4.0

### CUDA/cuDNN version

11.7/8.5.0

### GPU model and memory

RTX A6000, GTX 1070

### Current behavior?

I've installed CUDA 11.7 toolkit only and libcudnn 8.5.0 on Ubuntu 22.04 LTS.
I'm using python 3.10 using the `python3.10` cmd.

Here's the script I'm running

```bash
export PATH=$PATH:/usr/local/cuda-11.7/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.7/lib64
export TF_CUDA_VERSION=11.7
export TF_CUDNN_VERSION=8.5.0
export TF_CUBLAS_VERSION=11.10.1

python3.10 -m virtualenv venv
venv/bin/pip install numpy==1.23.5 wheel packaging requests opt_einsum
venv/bin/pip install keras_preprocessing --no-deps

mkdir tmp
cd tmp
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout v2.9.1

cd ..
. venv/bin/activate
cd tmp/tensorflow
./configure  # No ROCm, Yes CUDA, No TensorRT, 3.5,5.2,6.0,6.1,7.0,7.5,8.0 capabilities, No clang, default for the rest

bazel build --config=opt --verbose_failures //tensorflow:libtensorflow_cc.so

bazel build --config=opt --verbose_failures //tensorflow:install_headers

bazel build --config=opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package  # Failure here

./bazel-bin/tensorflow/tools/pip_package/build_pip_package ./bazel-bin/tensorflow/tools/pip_package
```

The output of `bazel build --config=opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package` is available in the relevant log output section.

The interesting line seems to be:
```
cp: cannot stat '/home/cluster/CN_TF/ContextCapture/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_dtype_api.h': No such file or directory
```
Indeed this file does not exist. I'm unsure why it is needed, but I tried different `numpy` version and failed to find one that works (either I'm missing this file, or `.doxyfile` or something else).

Can you advise which version of numpy should I be using for the compilation to be successful?

### Standalone code to reproduce the issue

```shell
I'm using vanilla Tensorflow checkout, no modifications are applied.
```


### Relevant log output

```shell
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=116
INFO: Reading rc options for 'build' from /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/bin/python3 --action_env PYTHON_LIB_PATH=/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages --python_path=/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/bin/python3 --action_env TF_CUDA_VERSION=11.7 --action_env TF_CUBLAS_VERSION=11.10.1 --action_env TF_CUDNN_VERSION=8.5.0 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.7 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.0,6.1,7.0,7.5,8.0 --action_env LD_LIBRARY_PATH=:/usr/local/cuda-11.7/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-11 --config=cuda
INFO: Reading rc options for 'build' from /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (236 packages loaded, 7837 targets configured).
INFO: Found 1 target...
ERROR: /home/cluster/.cache/bazel/_bazel_cluster/350d8f2feeb21f42226977fe6efcfb0a/external/local_config_python/BUILD:254:8: Executing genrule @local_config_python//:numpy_include failed: (Exit 1): bash failed: error executing command 
  (cd /home/cluster/.cache/bazel/_bazel_cluster/350d8f2feeb21f42226977fe6efcfb0a/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.7 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-11 \
    LD_LIBRARY_PATH=:/usr/local/cuda-11.7/lib64 \
    PATH=/home/cluster/.cache/bazelisk/downloads/sha256/399eedb225cff7a13f9f027f7ea2aad02ddb668a8eb89b1d975d222e4dc12ed9/bin:/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/bin:/home/cluster/.vscode-server/bin/8b617bd08fd9e3fc94d14adb8d358b56e3f72314/bin/remote-cli:/home/cluster/.local/bin:/home/cluster/miniconda3/bin:/home/cluster/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda-11.7/bin \
    PYTHON_BIN_PATH=/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/bin/python3 \
    PYTHON_LIB_PATH=/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CUBLAS_VERSION=11.10.1 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.0,6.1,7.0,7.5,8.0 \
    TF_CUDA_VERSION=11.7 \
    TF_CUDNN_VERSION=8.5.0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; 
cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/.doxyfile"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/.doxyfile"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/__multiarray_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/__multiarray_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/__ufunc_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/__ufunc_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_dtype_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_dtype_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_neighborhood_iterator_imp.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_neighborhood_iterator_imp.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_numpyconfig.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_numpyconfig.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_numpyconfig.h.in"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_numpyconfig.h.in"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/arrayobject.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/arrayobject.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/arrayscalars.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/arrayscalars.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/experimental_dtype_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/experimental_dtype_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/halffloat.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/halffloat.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/libdivide/LICENSE.txt"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/libdivide/LICENSE.txt"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/libdivide/libdivide.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/libdivide/libdivide.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/ndarrayobject.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/ndarrayobject.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/ndarraytypes.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/ndarraytypes.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/noprefix.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/noprefix.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_1_7_deprecated_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_3kcompat.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_3kcompat.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_common.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_common.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_cpu.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_cpu.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_endian.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_endian.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_interrupt.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_interrupt.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_math.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_math.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_no_deprecated_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_no_deprecated_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_os.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_os.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/numpyconfig.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/numpyconfig.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/old_defines.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/old_defines.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/oldnumeric.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/oldnumeric.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/random/bitgen.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/random/bitgen.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/random/distributions.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/random/distributions.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/ufuncobject.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/ufuncobject.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/utils.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/utils.h""
   ')
# Configuration: d04d20a1a4a46df9c7580a4efee273c7e78fdcec51eaaa1a5c3a4ea89b71ce88
# Execution platform: @local_execution_config_platform//:platform
cp: cannot stat '/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_dtype_api.h': No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/cluster/.cache/bazel/_bazel_cluster/350d8f2feeb21f42226977fe6efcfb0a/external/local_config_python/BUILD:66:11 Middleman _middlemen/@local_Uconfig_Upython_S_S_Cnumpy_Uheaders-BazelCppSemantics_build_arch_k8-opt failed: (Exit 1): bash failed: error executing command 
  (cd /home/cluster/.cache/bazel/_bazel_cluster/350d8f2feeb21f42226977fe6efcfb0a/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.7 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-11 \
    LD_LIBRARY_PATH=:/usr/local/cuda-11.7/lib64 \
    PATH=/home/cluster/.cache/bazelisk/downloads/sha256/399eedb225cff7a13f9f027f7ea2aad02ddb668a8eb89b1d975d222e4dc12ed9/bin:/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/bin:/home/cluster/.vscode-server/bin/8b617bd08fd9e3fc94d14adb8d358b56e3f72314/bin/remote-cli:/home/cluster/.local/bin:/home/cluster/miniconda3/bin:/home/cluster/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda-11.7/bin \
    PYTHON_BIN_PATH=/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/bin/python3 \
    PYTHON_LIB_PATH=/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CUBLAS_VERSION=11.10.1 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.0,6.1,7.0,7.5,8.0 \
    TF_CUDA_VERSION=11.7 \
    TF_CUDNN_VERSION=8.5.0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; 
cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/.doxyfile"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/.doxyfile"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/__multiarray_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/__multiarray_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/__ufunc_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/__ufunc_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_dtype_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_dtype_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_neighborhood_iterator_imp.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_neighborhood_iterator_imp.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_numpyconfig.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_numpyconfig.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_numpyconfig.h.in"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_numpyconfig.h.in"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/arrayobject.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/arrayobject.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/arrayscalars.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/arrayscalars.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/experimental_dtype_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/experimental_dtype_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/halffloat.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/halffloat.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/libdivide/LICENSE.txt"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/libdivide/LICENSE.txt"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/libdivide/libdivide.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/libdivide/libdivide.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/ndarrayobject.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/ndarrayobject.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/ndarraytypes.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/ndarraytypes.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/noprefix.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/noprefix.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_1_7_deprecated_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_3kcompat.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_3kcompat.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_common.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_common.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_cpu.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_cpu.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_endian.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_endian.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_interrupt.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_interrupt.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_math.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_math.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_no_deprecated_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_no_deprecated_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_os.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_os.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/numpyconfig.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/numpyconfig.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/old_defines.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/old_defines.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/oldnumeric.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/oldnumeric.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/random/bitgen.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/random/bitgen.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/random/distributions.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/random/distributions.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/ufuncobject.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/ufuncobject.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/utils.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/utils.h""
   ')
# Configuration: d04d20a1a4a46df9c7580a4efee273c7e78fdcec51eaaa1a5c3a4ea89b71ce88
# Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 136.025s, Critical Path: 44.21s
INFO: 977 processes: 370 internal, 607 local.
FAILED: Build did NOT complete successfully
```
"
61925,Unable to concatenate Keras Tensor and Eager Tensor using tf.keras.layers.Concatenate,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I want to pad the Keras Tensor **without using tf.pad or tf.concat**.
My approach is to create an empty tensor with zeros and concatenate the last layer with the Keras Tensor.

I have successfully saved the model. But when **I try to load the model, I am getting errors**, I have attached code to recreate the error and logs as well.

The expected output is concatenated Keras tensor on the last axis.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras.models import Model, save_model, load_model
from tensorflow.keras.layers import Input, Concatenate, Conv2D, ReLU

# Define the input shape
input_shape = (128, 128, 3)  # Replace with your input dimensions

# Create the input layer
input_layer = Input(shape=input_shape)

# Create empty channels
empty_channels = tf.zeros((input_shape[0], input_shape[1], 4))

# Expand the dimensions of empty_channels to match input_shape
empty_channels = tf.expand_dims(empty_channels, axis=0)

# Concatenate the input with the empty channels
concatenated = Concatenate(axis=-1)([input_layer, empty_channels])

# Create the model
model = Model(inputs=input_layer, outputs=concatenated)

# Display model summary
model.summary()

# Save the model to a file
model.save(""concatenated_model.h5"")

print(""Model has saved."")
# Now, to load the saved model
loaded_model = load_model(""concatenated_model.h5"")
print(""Model has loaded."")
```


### Relevant log output

```shell
python3 testPython.py                                                              î‚² 1 âœ˜ 
2023-09-20 13:14:52.746990: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-20 13:14:52.777548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: ""model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 128, 128, 3)]     0         
                                                                 
 concatenate (Concatenate)   (1, 128, 128, 7)          0         
                                                                 
=================================================================
Total params: 0 (0.00 Byte)
Trainable params: 0 (0.00 Byte)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
/home/hitech/.local/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
Model has saved.
Traceback (most recent call last):
  File ""testPython.py"", line 33, in <module>
    loaded_model = load_model(""concatenated_model.h5"")
  File ""/home/hitech/.local/lib/python3.8/site-packages/keras/src/saving/saving_api.py"", line 238, in load_model
    return legacy_sm_saving_lib.load_model(
  File ""/home/hitech/.local/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/hitech/.local/lib/python3.8/site-packages/keras/src/layers/merging/concatenate.py"", line 119, in build
    raise ValueError(err_msg)
ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received:
input_shape=[(None, 128, 128, 3), [[[[(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], [(), (), (), ()], .....
```
"
61924,Build issue tenserflow 2.11.0 for tensorflow quantum,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.11.0

### Custom code

Yes

### OS platform and distribution

Mac OS M1

### Mobile device

_No response_

### Python version

3.9

### Bazel version

bazel 5.3.0

### GCC/compiler version

Apple clang version 14.0.3 (clang-1403.0.22.14.1) Target: arm64-apple-darwin22.5.0 Thread model: posix

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Trying to build package using bazel compiler, as given [here](https://www.tensorflow.org/quantum/install).  Fails to build the file. Gives the following output mentioned in log output, after running the given command.

Output: 
<img width=""1680"" alt=""Screenshot 2023-09-20 at 11 47 36"" src=""https://github.com/tensorflow/tensorflow/assets/69144860/e699f11d-fc31-49af-83b7-34e3330f3358"">


### Standalone code to reproduce the issue

```shell
bazel build -c opt --cxxopt=""-O3"" --cxxopt=""-march=native"" --cxxopt=""-std=c++17"" --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=1"" //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /private/var/tmp/_bazel_gb/4de1e45218bdc87c870302861e9b2675/external/boringssl/BUILD:161:11: Compiling src/crypto/x509/t_x509.c [for host] failed: (Exit 1): cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics ... (remaining 44 arguments skipped)
external/boringssl/src/crypto/x509/t_x509.c:321:18: error: variable 'l' set but not used [-Werror,-Wunused-but-set-variable]
    int ret = 0, l, i;
                 ^
1 error generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 709.728s, Critical Path: 52.92s
INFO: 4419 processes: 1508 internal, 2911 local.
FAILED: Build did NOT complete successfully
```
"
61923,Keeping last layer names in Stacked Model,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

macOS 13.4

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using Model Stacking, I would like to access the last layer in the last stacked model to pass individual loss functions to them
```python
# ------------------ Encoder ------------------
encoder_model = Model(inputs=all_inputs, outputs=latent_space, name=""encoder"")
# ... irrelevant code 

# ------------------ Decoder ------------------
# Output Layers
numeric_output = Dense(
    self.numeric_dim, activation=""linear"", name=""numeric_output"")(decoder2)
binary_output = Dense(
    self.binary_dim, activation=""sigmoid"", name=""binary_output"")(decoder2)

decoder_output = [numeric_output] + [binary_output]
decoder_model = Model(inputs=latent_input, outputs=decoder_output, name=""decoder"")

# ------------------ Autoencoder ------------------
autoencoder_output = decoder_model(encoder_model(all_inputs))
autoencoder = Model(inputs=all_inputs, outputs=pass_through_layers, name=""autoencoder"")

# This will not work:
losses = {
            ""numeric_output"": ""mse"",
            ""binary_output"": ""binary_crossentropy""
}
autoencoder.compile(optimizer=Adam(learning_rate=lr), loss=losses)
```

The last layer will be renamed to decoder1, decoder2, etc. 
It would be much nicer if there was some passthrough argument in a model that would allow referencing the last layer in the stacked model directly

### Standalone code to reproduce the issue

```shell
This will fail


import numpy as np
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Input dimensions
input_dim = 10
latent_dim = 5
numeric_dim = 3
binary_dim = 2

# Learning rate
lr = 0.001

# Number of samples
n_samples = 1000

# Generate random data
X = np.random.rand(n_samples, input_dim)
y_numeric = np.random.rand(n_samples, numeric_dim)
y_binary = np.random.randint(0, 2, size=(n_samples, binary_dim))

# ------------------ Encoder ------------------
all_inputs = Input(shape=(input_dim,), name=""all_inputs"")
latent_space = Dense(latent_dim, activation=""relu"", name=""latent_space"")(all_inputs)
encoder_model = Model(inputs=all_inputs, outputs=latent_space, name=""encoder"")

# ------------------ Decoder ------------------
latent_input = Input(shape=(latent_dim,), name=""latent_input"")
decoder2 = Dense(10, activation=""relu"", name=""decoder2"")(latent_input)

numeric_output = Dense(numeric_dim, activation=""linear"", name=""numeric_output"")(decoder2)
binary_output = Dense(binary_dim, activation=""sigmoid"", name=""binary_output"")(decoder2)

decoder_output = [numeric_output, binary_output]
decoder_model = Model(inputs=latent_input, outputs=decoder_output, name=""decoder"")

# ------------------ Autoencoder ------------------
autoencoder_output = decoder_model(encoder_model(all_inputs))

# This will not work:
losses = {
    ""numeric_output"": ""mse"",
    ""binary_output"": ""binary_crossentropy""
}
autoencoder = Model(inputs=all_inputs, outputs=autoencoder_output, name=""autoencoder"")
autoencoder.compile(optimizer=Adam(learning_rate=lr), loss=losses)

# Fit the model
autoencoder.fit(
    x=X,
    y=[y_numeric, y_binary],
    epochs=10,
    batch_size=32
)
```


### Relevant log output

_No response_"
61922,Problem installing tensorflow 2.13.0,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04, Lambda Labs TensorBook

### Mobile device

n/a

### Python version

3.11.5

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

8.6.0.163

### GPU model and memory

_No response_

### Current behavior?

TensorFlow 2.1.13 won't run because of an undefined symbol in libtensorflow_cc, apparently a google.protobuf.Message symbol.

### Standalone code to reproduce the issue

```shell
I am on a LambdaLabs TensorBook. This does come with a preloaded tensorflow installation in /usr/lib/python3/dist-packages
However my reading of the error messages on install does not seem to suggest that conflict with that package is to blame here.

Installing per instructions here: https://www.tensorflow.org/install/pip

~~~
conda create --name cuda11
conda activate cuda11
conda install python
conda install -c conda-forge cudatoolkit=11.8.0
python3 -m pip install nvidia-cudnn-cu11==8.6.0.163 tensorflow==2.13.
python3 -c ""import tensorflow""
~~~

...yields the following error message...

~~~
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/yam/miniconda3/envs/cuda11/lib/python3.11/site-packages/tensorflow/__init__.py"", line 38, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/yam/miniconda3/envs/cuda11/lib/python3.11/site-packages/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/home/yam/miniconda3/envs/cuda11/lib/python3.11/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 26, in <module>
    self_check.preload_check()
  File ""/home/yam/miniconda3/envs/cuda11/lib/python3.11/site-packages/tensorflow/python/platform/self_check.py"", line 63, in preload_check
    from tensorflow.python.platform import _pywrap_cpu_feature_guard
ImportError: /home/yam/miniconda3/envs/cuda11/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2: undefined symbol: _ZN6google8protobuf7Message19CopyWithSourceCheckERS1_RKS1_
~~~

pip package info:

~~~
(cuda11) yam@TensorYam:~$ python -m pip list
Package                      Version
---------------------------- ---------
absl-py                      2.0.0
astunparse                   1.6.3
cachetools                   5.3.1
certifi                      2023.7.22
charset-normalizer           3.2.0
flatbuffers                  23.5.26
gast                         0.4.0
google-auth                  2.23.0
google-auth-oauthlib         1.0.0
google-pasta                 0.2.0
grpcio                       1.58.0
h5py                         3.9.0
idna                         3.4
keras                        2.13.1
libclang                     16.0.6
Markdown                     3.4.4
MarkupSafe                   2.1.3
numpy                        1.24.3
nvidia-cublas-cu11           11.11.3.6
nvidia-cudnn-cu11            8.6.0.163
oauthlib                     3.2.2
opt-einsum                   3.3.0
packaging                    23.1
pip                          23.2.1
protobuf                     4.24.3
pyasn1                       0.5.0
pyasn1-modules               0.3.0
requests                     2.31.0
requests-oauthlib            1.3.1
rsa                          4.9
setuptools                   68.0.0
six                          1.16.0
tensorboard                  2.13.0
tensorboard-data-server      0.7.1
tensorflow                   2.13.0
tensorflow-estimator         2.13.0
tensorflow-io-gcs-filesystem 0.34.0
termcolor                    2.3.0
typing_extensions            4.5.0
urllib3                      1.26.16
Werkzeug                     2.3.7
wheel                        0.38.4
wrapt                        1.15.0
~~~
```


### Relevant log output

_No response_"
61921,TensorFlow lite cmake compilation failed to allocate memory,"Any update with this issue? I'm having the same problem described by @Luca-Stefanescu

I am building Tensorflow Lite with cmake following the instruction given on the minimal example. I am building in a docker container with ubuntu using WSL 2 with docker desktop. The build seems to work until 91%. Then it will start to allocate all the memory (16gb of ram + 8gb of swap) until it fails to allocate throwing an allocation error or sometimes an input/output error.
I think that this error message could be helpful:
```
In file included from /workspaces/tfl-dev-env/tie/../tensorflow_src/tensorflow/lite/kernels/internal/runtime_shape.h:22,
                 from /workspaces/tfl-dev-env/tie/../tensorflow_src/tensorflow/lite/kernels/internal/types.h:24,
                 from /workspaces/tfl-dev-env/tie/../tensorflow_src/tensorflow/lite/kernels/internal/tensor_ctypes.h:22,
                 from /workspaces/tfl-dev-env/tensorflow_src/tensorflow/lite/kernels/embedding_lookup_sparse.cc:72:
/usr/include/c++/13/memory:81:12: fatal error: /workspaces/tfl-dev-env/tie/../tensorflow_src/bits/shared_ptr_atomic.h: Cannot allocate memory
   81 | #  include <bits/shared_ptr_atomic.h>
      |            ^~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
```

However trying to follow the same steps on a ubuntu VM using VMWare (and with less memory) seems to work.

_Originally posted by @lorenzodellagiustina in https://github.com/tensorflow/tensorflow/issues/61485#issuecomment-1725196272_
            "
61920,DenseFeatures Feature column combine order,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have this code to create DenseFeatures layer.

 ```
       # Define the input for each feature column.
        inputs = {}
        for col in feature_columns:
            if type(col) == type(tf.feature_column.numeric_column(""temp"")):
                dtype = tf.float32
                key = col.key
            else:
                dtype = tf.int64
                key = col.categorical_column.key
            inputs[key] = tf.keras.layers.Input(name=key, shape=(), dtype=dtype)

        // Now use a DenseFeatures layer to combine them
        x = tf.keras.layers.DenseFeatures(feature_columns)(inputs)
```

-----------------

Now, I am trying to implement the output of the DenseFeatures layer for inference. I am facing a situation where I cannot use the TensorFlow model due to latency constraints.

However, my issue is that the manner in which DenseFeatures combines the inputs does not follow the order specified in feature_columns, nor is it sorted based on the feature names.

Is there a way I can determine the order in which these feature columns are combined within the DenseFeatures layer?

As I understand, in most places it is mentioned that it should follow the same order as the feature columns, but this is not what I am observing. I am using TensorFlow 2.13.

### Standalone code to reproduce the issue

```shell
# Define the input for each feature column.
        inputs = {}
        for col in feature_columns:
            if type(col) == type(tf.feature_column.numeric_column(""temp"")):
                dtype = tf.float32
                key = col.key
            else:
                dtype = tf.int64
                key = col.categorical_column.key
            inputs[key] = tf.keras.layers.Input(name=key, shape=(), dtype=dtype)

        // Now use a DenseFeatures layer to combine them
        x = tf.keras.layers.DenseFeatures(feature_columns)(inputs)
```


### Relevant log output

_No response_"
61918,implementation 'org.tensorflow:tensorflow-lite-support:0.1.0',"implementation 'org.tensorflow:tensorflow-lite-support:0.1.0'
Why do I run this instruction to generate two dependent libraries, TensorFlow Lite and TensorFlow Lite Support?

How can I generate only tensorflow site support as a dependency library?"
61916,tf.linalg.cholesky output normal value on a complex64 matrix that is not positive definite.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On a randomly generated matrix that is not positive definite, tf.linalg.cholesky outputs nan if the matrix's dtype is float and outputs 0 if the matrix' dtype is complex. It may be not appropriate especially when tf.linalg.cholesky outputs 0 on an invalid input without giving any abnormal behaviors. For your inference, np.linalg.cholesky will directly raises with error message when receiving non-positive definite matrix in float or complex data type.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import warnings
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings(""ignore"")
np.random.seed(1234)
array = np.random.rand(4,4).astype(""complex64"")
try:
    print(""Numpy's result: "", np.linalg.cholesky(array))
except:
    print(""Numpy crash."")

print(""TensorFlow's result: "", tf.linalg.cholesky(tf.constant(array)))

array = np.random.rand(4,4).astype(""float64"")
try:
    print(""Numpy's result: "", np.linalg.cholesky(array))
except:
    print(""Numpy crash."")

print(""TensorFlow's result: "", tf.linalg.cholesky(tf.constant(array)))
```


### Relevant log output

```shell
Numpy crash.
TensorFlow's result:  tf.Tensor(
[[0.+0.j 0.+0.j 0.+0.j 0.+0.j]
 [0.+0.j 0.+0.j 0.+0.j 0.+0.j]
 [0.+0.j 0.+0.j 0.+0.j 0.+0.j]
 [0.+0.j 0.+0.j 0.+0.j 0.+0.j]], shape=(4, 4), dtype=complex64)
Numpy crash.
TensorFlow's result:  tf.Tensor(
[[nan  0.  0.  0.]
 [nan nan  0.  0.]
 [nan nan nan  0.]
 [nan nan nan nan]], shape=(4, 4), dtype=float64)
```
"
61915,'Adam' object has no attribute 'build' (saving and loading keras.optimizers.Adam),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

Yes

### OS platform and distribution

MacOS ARM M1

### Mobile device

_No response_

### Python version

3.10.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When running the code below we get the following error: **_AttributeError: 'Adam' object has no attribute 'build'_**

### Standalone code to reproduce the issue

```shell
from tensorflow import keras

if __name__ == '__main__':
  optimizer = keras.optimizers.Adam()
  vh  = keras.Input(shape=(2,3), name = 'vh')
  v1  = keras.layers.Dense(512)(vh)
  
  output  = keras.layers.Dense(1, activation='softmax', name='prediction')(v1)
  
  model = keras.Model(inputs=vh, outputs=[output], name=""antibody_model"")
  
  model.compile(optimizer=optimizer )
  
  model.save('nn_model.keras')
  test = keras.models.load_model('nn_model.keras')
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/Users/palmito/Development/federated-demo/backend/app/test.py"", line 15, in <module>
    test = keras.models.load_model('nn_model.keras')
  File ""/Users/palmito/.local/share/virtualenvs/backend-tPS_SUas/lib/python3.10/site-packages/keras/src/saving/saving_api.py"", line 230, in load_model
    return saving_lib.load_model(
  File ""/Users/palmito/.local/share/virtualenvs/backend-tPS_SUas/lib/python3.10/site-packages/keras/src/saving/saving_lib.py"", line 275, in load_model
    raise e
  File ""/Users/palmito/.local/share/virtualenvs/backend-tPS_SUas/lib/python3.10/site-packages/keras/src/saving/saving_lib.py"", line 240, in load_model
    model = deserialize_keras_object(
  File ""/Users/palmito/.local/share/virtualenvs/backend-tPS_SUas/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py"", line 710, in deserialize_keras_object
    instance.compile_from_config(compile_config)
  File ""/Users/palmito/.local/share/virtualenvs/backend-tPS_SUas/lib/python3.10/site-packages/keras/src/engine/training.py"", line 3582, in compile_from_config
    self.optimizer.build(self.trainable_variables)
  File ""/Users/palmito/.local/share/virtualenvs/backend-tPS_SUas/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py"", line 997, in __getattribute__
    raise e
  File ""/Users/palmito/.local/share/virtualenvs/backend-tPS_SUas/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py"", line 987, in __getattribute__
    return super().__getattribute__(name)
AttributeError: 'Adam' object has no attribute 'build'
```
```
"
61912,"Randomization generating repeated sequences with `tf.function`, `tf.random.set_seed` and `tf.cond`","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.15.0-dev20230919

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We have encountered an unexpected issue with `tf.function` compilation combined with the global/graph-level seed setting (`tf.random.set_seed`); under some circumstances the random variables are generating identical sequences within the same function. The issue seems to be connected with TensorFlow conditionals (`tf.cond`) within the function, particularly when the branches contain the randomization calls. Removing `tf.cond` results in the generation of unique random values, as expected.

We understand the following expected behavior under `tf.function` compilation when a global/graph-level seed is set, as per the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/random/set_seed):

> Note that `tf.function` acts like a re-run of a program in this case. When the global seed is set but operation seeds are not set, the sequence of random numbers are the same for each `tf.function`.

However, the problem we are observing arises within a single instance of the function, not across multiple instances. The behavior is as if the internal counters
in `tf.random.uniform` get reset when there is a TensorFlow conditional in the graph. Is this expected behavior?

To illustrate this issue, the example below presents two scenarios. The first one (""SAD"" mode) shows the function producing repeated random sequences when `tf.cond` is present. The second scenario (""HAPPY"" mode) shows the function generating unique random values once `tf.cond` is removed.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from pprint import pprint

tf.random.set_seed(42)

var = tf.Variable(0, dtype=tf.int32)
def get_value(happy):
    if happy:
        return tf.stack([tf.random.uniform(()) for _ in range(2)])
    else:
        return tf.cond(
            var == 0,
            lambda: tf.stack([tf.random.uniform(()) for _ in range(2)]),
            lambda: tf.stack([tf.random.uniform(()) for _ in range(2)]),
        )

def randomize(happy):
    return [get_value(happy) for _ in range(3)]

print(""SAD"")
pprint(tf.function(randomize)(False))
print(""HAPPY"")
pprint(tf.function(randomize)(True))
```


### Relevant log output

```shell
SAD
[<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.8354591 , 0.15012848], dtype=float32)>,
 <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.8354591 , 0.15012848], dtype=float32)>,
 <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.8354591 , 0.15012848], dtype=float32)>]
HAPPY
[<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.8354591 , 0.15012848], dtype=float32)>,
 <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.98781276, 0.63789964], dtype=float32)>,
 <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.00857747, 0.02621067], dtype=float32)>]
```
"
61911,`tf.device` context manager does not restore `cudaCurrentDevice` under some conditions,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current behavior?

When using `tf.device` context manager, the current device of cuda runtime remains ""dirty"" even after exiting the context manager. This happens when: 1. tensorflow is initializing GPU context on this line (tf.device), 2. there is no materialization of tensors on GPU.

For context, keeping a clean state of current device context is important to keep tensorflow in sync with other GPU based libraries such as [cuDF](github.com/rapidsai/cuDF). [RMM](github.com/rapidsai/rmm) memory allocators also depends on the assumption that the context stays the same throughout the lifetime of allocations.

### Standalone code to reproduce the issue

```shell
https://gist.github.com/isVoid/9eded87fca35e86a2c2dc85f603383c2
```


### Relevant log output

```shell
# Log output of the first cell. The second and third current device context should be 0.

(<cudaError_t.cudaSuccess: 0>, 0)
(<cudaError_t.cudaSuccess: 0>, 7)
(<cudaError_t.cudaSuccess: 0>, 7)
(<cudaError_t.cudaSuccess: 0>, 0)
```
"
61909,Training Vanilla Transformer on TPU gives InternalError,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.2 (Google Colab)

### Mobile device

Colab

### Python version

3.10.12

### Bazel version

Colab

### GCC/compiler version

Colab

### CUDA/cuDNN version

Colab

### GPU model and memory

Colab

### Current behavior?

Training Transformer model on TPU gives Internal Error 

I have some idea on the error that there's a incompatible tensor ops thats causing the problem but i can't pinpoint it.

I had already done a bigger model which is using pretrained embeddings and it went off without a hitch i tried to replicate the same but with different tfds dataset

If this is already solved please direct me to the relevant links

### Standalone code to reproduce the issue

```
[This is the notebook](https://colab.research.google.com/drive/1y3VEuaYXnsoB42TaVd8UBB-18U8UHFBt#scrollTo=Y0hKZ9yRC3FU)

[This notebook worked fine](https://colab.research.google.com/drive/1DJU058LhhyCfNsuHZ74kZ0E2ziHw-VYo)

Thankyou in advance. I will respond asap
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
[<ipython-input-12-013fa12d9e3a>](https://localhost:8080/#) in <cell line: 1>()
----> 1 model.fit(
      2     train_ds,
      3     validation_data=valid_ds,
      4     epochs=EPOCHS,
      5     steps_per_epoch=train_steps,

1 frames
[/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py](https://localhost:8080/#) in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---> 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

[/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/capture/capture_container.py](https://localhost:8080/#) in capture_by_value(self, graph, tensor, name)
    120         graph_const = self.by_val_internal.get(id(tensor))
    121         if graph_const is None:
--> 122           graph_const = tensor._capture_as_const(name)  # pylint: disable=protected-access
    123           if graph_const is None:
    124             # Some eager tensors, e.g. parallel tensors, are not convertible to

InternalError: failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:35437: Failed to connect to remote host: Connection refused
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:35437: Failed to connect to remote host: Connection refused {created_time:""2023-09-19T08:56:09.694479753+00:00"", grpc_status:14}
Executing non-communication op <MultiDeviceIteratorInit> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.
```
"
61908,bad_alloc,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When allocating memory, since the value of `required_size` is determined by the model parameters, a maliciously constructed model may cause memory allocation to fail, leading to a DOS.
```
// simple_memory_arena.cc
TfLiteStatus SimpleMemoryArena::Commit(TfLiteContext* context,
                                       bool* arena_reallocated) {
  size_t required_size = RequiredBufferSize();
  if (required_size > underlying_buffer_size_) {
    *arena_reallocated = true;
#ifdef TF_LITE_TENSORFLOW_PROFILER
    PauseHeapMonitoring(/*pause=*/true);
    OnTfLiteArenaAlloc(subgraph_index_, reinterpret_cast<std::uintptr_t>(this),
                       required_size);
#endif
    char* new_alloc = new char[required_size];  // here
    char* new_underlying_buffer_aligned_ptr = reinterpret_cast<char*>(
        AlignTo(arena_alignment_, reinterpret_cast<intptr_t>(new_alloc)));
```

Construct a malicious model with a shape size of `0x640000000 * 0x13 * 0x13 * 0x60`, ultimately resulting in size being an extremely large value.
```
//simple_memory_arena.cc
TfLiteStatus SimpleMemoryArena::Allocate(
    TfLiteContext* context, size_t alignment, size_t size, int32_t tensor,
    int32_t first_node, int32_t last_node,
    ArenaAllocWithUsageInterval* new_alloc) {
         ...
  // Update the required buffer size.
  high_water_mark_ = std::max(high_water_mark_, best_offset + size); // size can be externally controlled.
```

[malloc_large_1.zip](https://github.com/tensorflow/tensorflow/files/12656923/malloc_large_1.zip)


### Standalone code to reproduce the issue

```shell
When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).


â¯ ./benchmark_model --graph=../poc/malloc_large_1.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/malloc_large_1.tflite]
INFO: Loaded model ../poc/malloc_large_1.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
[1]    13318 abort (core dumped)  ./benchmark_model --graph=../poc/malloc_large_1.tflite
```


### Relevant log output

_No response_"
61907,tf.linalg.cholesky fails on half precision,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following the documentation: https://www.tensorflow.org/api_docs/python/tf/linalg/cholesky, tf.linalg.cholesky is expected to accept tensor in half precision but it fails.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
np.random.seed(2023)

# {'input_ndims': 2}
input = tf.constant(np.random.rand(3,3), dtype='half')
out = tf.linalg.cholesky(input)
```
```


### Relevant log output

```shell
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6654 def raise_from_not_ok_status(e, name):
   6655   e.message += ("" name: "" + str(name if name is not None else """"))
-> 6656   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   6657 
   6658 

NotFoundError: Could not find device for node: {{node Cholesky}} = Cholesky[T=DT_HALF]
All kernels registered for op Cholesky:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_HALF]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
 [Op:Cholesky] name:
```
"
61906,How to compile tflite-runtime to include gpu part?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04.2 LTS
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): 2.13


**Provide the text output from tflite_convert**

```
SUBCOMMAND: # @XNNPACK//:operators [action 'Compiling src/operators/convolution-nchw.c', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/k8-opt/bin/external/XNNPACK/_objs/operators/convolution-nchw.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/XNNPACK/_objs/operators/convolution-nchw.pic.o' -fPIC '-DXNN_IGNORED_PLATFORM_JIT=0' '-DXNN_LOG_LEVEL=0' -DPTHREADPOOL_NO_DEPRECATED_API '-DXNN_ENABLE_GEMM_M_SPECIALIZATION=1' '-DXNN_ENABLE_JIT=0' '-DXNN_ENABLE_ARM_FP16_SCALAR=0' '-DXNN_ENABLE_ARM_FP16_VECTOR=0' '-DXNN_ENABLE_ARM_BF16=0' '-DXNN_ENABLE_ARM_DOTPROD=0' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_DWCONV_MULTIPASS=0' -iquote external/XNNPACK -iquote bazel-out/k8-opt/bin/external/XNNPACK -iquote external/pthreadpool -iquote bazel-out/k8-opt/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/k8-opt/bin/external/FXdiv -iquote external/FP16 -iquote bazel-out/k8-opt/bin/external/FP16 -iquote external/cpuinfo -iquote bazel-out/k8-opt/bin/external/cpuinfo -Ibazel-out/k8-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/k8-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/k8-opt/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/k8-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -isystem external/XNNPACK/include -isystem bazel-out/k8-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/k8-opt/bin/external/XNNPACK/src -isystem external/pthreadpool/include -isystem bazel-out/k8-opt/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/k8-opt/bin/external/FXdiv/include -isystem external/FP16/include -isystem bazel-out/k8-opt/bin/external/FP16/include -isystem external/cpuinfo/include -isystem bazel-out/k8-opt/bin/external/cpuinfo/include -isystem external/cpuinfo/src -isystem bazel-out/k8-opt/bin/external/cpuinfo/src -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' -Iinclude -Isrc -Os '-std=c99' -O2 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/XNNPACK/src/operators/convolution-nchw.c -o bazel-out/k8-opt/bin/external/XNNPACK/_objs/operators/convolution-nchw.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,222 / 1,232] 2 actions, 1 running
    Compiling absl/strings/internal/charconv_bigint.cc; 0s local
    [Prepa] Compiling src/operators/convolution-nchw.c
[1,222 / 1,232] 2 actions running
    Compiling absl/strings/internal/charconv_bigint.cc; 0s local
    Compiling src/operators/convolution-nchw.c; 0s local
SUBCOMMAND: # @com_google_absl//absl/time:time [action 'Compiling absl/time/format.cc', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/com_google_absl/absl/time/_objs/time/format.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/com_google_absl/absl/time/_objs/time/format.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/com_google_absl/absl/time/format.cc -o bazel-out/k8-opt/bin/external/com_google_absl/absl/time/_objs/time/format.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,223 / 1,232] 2 actions, 1 running
    Compiling absl/strings/internal/charconv_bigint.cc; 0s local
    [Prepa] Compiling absl/time/format.cc
[1,223 / 1,232] 2 actions running
    Compiling absl/strings/internal/charconv_bigint.cc; 1s local
    Compiling absl/time/format.cc; 0s local
[1,224 / 1,232] 2 actions, 1 running
    Compiling absl/strings/internal/charconv_bigint.cc; 2s local
    [Scann] Compiling src/unpool-config.c
SUBCOMMAND: # @XNNPACK//:microkernel_configs [action 'Compiling src/unpool-config.c', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/k8-opt/bin/external/XNNPACK/_objs/microkernel_configs/unpool-config.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/XNNPACK/_objs/microkernel_configs/unpool-config.pic.o' -fPIC '-DXNN_IGNORED_PLATFORM_JIT=0' '-DXNN_ENABLE_ARM_FP16_SCALAR=0' '-DXNN_ENABLE_ARM_FP16_VECTOR=0' '-DXNN_ENABLE_ARM_BF16=0' '-DXNN_ENABLE_ARM_DOTPROD=0' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_DWCONV_MULTIPASS=0' '-DXNN_ENABLE_GEMM_M_SPECIALIZATION=1' '-DXNN_ENABLE_JIT=0' '-DXNN_LOG_LEVEL=0' -DPTHREADPOOL_NO_DEPRECATED_API -iquote external/XNNPACK -iquote bazel-out/k8-opt/bin/external/XNNPACK -iquote external/FXdiv -iquote bazel-out/k8-opt/bin/external/FXdiv -iquote external/pthreadpool -iquote bazel-out/k8-opt/bin/external/pthreadpool -iquote external/cpuinfo -iquote bazel-out/k8-opt/bin/external/cpuinfo -iquote external/FP16 -iquote bazel-out/k8-opt/bin/external/FP16 -Ibazel-out/k8-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/k8-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/k8-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -Ibazel-out/k8-opt/bin/external/FP16/_virtual_includes/FP16 -isystem external/XNNPACK/include -isystem bazel-out/k8-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/k8-opt/bin/external/XNNPACK/src -isystem external/FXdiv/include -isystem bazel-out/k8-opt/bin/external/FXdiv/include -isystem external/pthreadpool/include -isystem bazel-out/k8-opt/bin/external/pthreadpool/include -isystem external/cpuinfo/include -isystem bazel-out/k8-opt/bin/external/cpuinfo/include -isystem external/cpuinfo/src -isystem bazel-out/k8-opt/bin/external/cpuinfo/src -isystem external/FP16/include -isystem bazel-out/k8-opt/bin/external/FP16/include -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' -Iinclude -Isrc '-std=c99' -O2 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/XNNPACK/src/unpool-config.c -o bazel-out/k8-opt/bin/external/XNNPACK/_objs/microkernel_configs/unpool-config.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,224 / 1,232] 2 actions, 1 running
    Compiling absl/strings/internal/charconv_bigint.cc; 2s local
    [Prepa] Compiling src/unpool-config.c
SUBCOMMAND: # //tensorflow/lite/kernels:builtin_op_kernels [action 'Compiling tensorflow/lite/kernels/add.cc', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/add.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/add.pic.o' -fPIC -DTFLITE_KERNEL_USE_XNNPACK -DPTHREADPOOL_NO_DEPRECATED_API '-DEIGEN_NEON_GEBP_NR=4' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DXNNPACK_DELEGATE_ENABLE_QS8=1' '-DXNNPACK_DELEGATE_ENABLE_QU8=1' '-DEIGEN_ALTIVEC_USE_CUSTOM_PACK=0' '-DEIGEN_USE_AVX512_GEMM_KERNELS=0' '-DXNN_IGNORED_PLATFORM_JIT=0' '-DXNN_LOG_LEVEL=0' '-DXNN_ENABLE_ARM_FP16_SCALAR=0' '-DXNN_ENABLE_ARM_FP16_VECTOR=0' '-DXNN_ENABLE_ARM_BF16=0' '-DXNN_ENABLE_ARM_DOTPROD=0' '-DXNN_ENABLE_GEMM_M_SPECIALIZATION=1' '-DXNN_ENABLE_JIT=0' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_DWCONV_MULTIPASS=0' '-DXNN_ENABLE_SPARSE=1' '-DXNN_ENABLE_MEMOPT=1' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/ruy -iquote bazel-out/k8-opt/bin/external/ruy -iquote external/cpuinfo -iquote bazel-out/k8-opt/bin/external/cpuinfo -iquote external/gemmlowp -iquote bazel-out/k8-opt/bin/external/gemmlowp -iquote external/pthreadpool -iquote bazel-out/k8-opt/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/k8-opt/bin/external/FXdiv -iquote external/arm_neon_2_x86_sse -iquote bazel-out/k8-opt/bin/external/arm_neon_2_x86_sse -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/flatbuffers -iquote bazel-out/k8-opt/bin/external/flatbuffers -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/XNNPACK -iquote bazel-out/k8-opt/bin/external/XNNPACK -iquote external/FP16 -iquote bazel-out/k8-opt/bin/external/FP16 -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -Ibazel-out/k8-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -Ibazel-out/k8-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/k8-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/k8-opt/bin/external/flatbuffers/_virtual_includes/flatbuffers -Ibazel-out/k8-opt/bin/external/flatbuffers/src/_virtual_includes/flatbuffers -Ibazel-out/k8-opt/bin/external/flatbuffers/_virtual_includes/runtime_cc -Ibazel-out/k8-opt/bin/external/FP16/_virtual_includes/FP16 -isystem external/cpuinfo/include -isystem bazel-out/k8-opt/bin/external/cpuinfo/include -isystem external/cpuinfo/src -isystem bazel-out/k8-opt/bin/external/cpuinfo/src -isystem external/pthreadpool/include -isystem bazel-out/k8-opt/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/k8-opt/bin/external/FXdiv/include -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem tensorflow/lite/schema -isystem bazel-out/k8-opt/bin/tensorflow/lite/schema -isystem tensorflow/lite/experimental/acceleration/configuration -isystem bazel-out/k8-opt/bin/tensorflow/lite/experimental/acceleration/configuration -isystem external/XNNPACK/include -isystem bazel-out/k8-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/k8-opt/bin/external/XNNPACK/src -isystem external/FP16/include -isystem bazel-out/k8-opt/bin/external/FP16/include -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' '-std=c++17' -DFARMHASH_NO_CXX_STRING -msse4.2 -O3 -fno-exceptions '-Wno-error=reorder' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/lite/kernels/add.cc -o bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/add.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,225 / 1,232] 2 actions, 1 running
    Compiling absl/strings/internal/charconv_bigint.cc; 2s local
    [Prepa] Compiling tensorflow/lite/kernels/add.cc
SUBCOMMAND: # //tensorflow/lite:stderr_reporter [action 'Compiling tensorflow/lite/stderr_reporter.cc', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/tensorflow/lite/_objs/stderr_reporter/stderr_reporter.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/lite/_objs/stderr_reporter/stderr_reporter.pic.o' -fPIC -DTFLITE_KERNEL_USE_XNNPACK -iquote . -iquote bazel-out/k8-opt/bin -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' '-std=c++17' -DFARMHASH_NO_CXX_STRING -msse4.2 -O3 -fno-exceptions -Wall -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/lite/stderr_reporter.cc -o bazel-out/k8-opt/bin/tensorflow/lite/_objs/stderr_reporter/stderr_reporter.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,226 / 1,232] 2 actions, 1 running
    Compiling tensorflow/lite/kernels/add.cc; 0s local
    [Prepa] Compiling tensorflow/lite/stderr_reporter.cc
SUBCOMMAND: # @com_google_absl//absl/strings:cord [action 'Compiling absl/strings/cord.cc', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/com_google_absl/absl/strings/_objs/cord/cord.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/com_google_absl/absl/strings/_objs/cord/cord.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/com_google_absl/absl/strings/cord.cc -o bazel-out/k8-opt/bin/external/com_google_absl/absl/strings/_objs/cord/cord.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,227 / 1,232] 2 actions, 1 running
    Compiling tensorflow/lite/kernels/add.cc; 0s local
    [Prepa] Compiling absl/strings/cord.cc
[1,227 / 1,232] 2 actions running
    Compiling tensorflow/lite/kernels/add.cc; 0s local
    Compiling absl/strings/cord.cc; 0s local
[1,227 / 1,232] 2 actions running
    Compiling tensorflow/lite/kernels/add.cc; 1s local
    Compiling absl/strings/cord.cc; 1s local
[1,227 / 1,232] 2 actions running
    Compiling tensorflow/lite/kernels/add.cc; 2s local
    Compiling absl/strings/cord.cc; 2s local
[1,227 / 1,232] 2 actions running
    Compiling tensorflow/lite/kernels/add.cc; 3s local
    Compiling absl/strings/cord.cc; 3s local
[1,227 / 1,232] 2 actions running
    Compiling tensorflow/lite/kernels/add.cc; 4s local
    Compiling absl/strings/cord.cc; 4s local
[1,228 / 1,232] 2 actions, 1 running
    Compiling tensorflow/lite/kernels/add.cc; 5s local
    [Scann] Compiling src/google/protobuf/stubs/statusor.cc
SUBCOMMAND: # @com_google_protobuf//:protobuf_lite [action 'Compiling src/google/protobuf/stubs/statusor.cc', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/com_google_protobuf/_objs/protobuf_lite/statusor.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/com_google_protobuf/_objs/protobuf_lite/statusor.pic.o' -fPIC -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' '-std=c++17' -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/com_google_protobuf/src/google/protobuf/stubs/statusor.cc -o bazel-out/k8-opt/bin/external/com_google_protobuf/_objs/protobuf_lite/statusor.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,228 / 1,232] 2 actions, 1 running
    Compiling tensorflow/lite/kernels/add.cc; 5s local
    [Prepa] Compiling src/google/protobuf/stubs/statusor.cc
[1,228 / 1,232] 2 actions running
    Compiling tensorflow/lite/kernels/add.cc; 6s local
    Compiling src/google/protobuf/stubs/statusor.cc; 0s local
[1,229 / 1,232] 2 actions, 1 running
    Compiling tensorflow/lite/kernels/add.cc; 6s local
    [Scann] Compiling absl/synchronization/internal/create_thread_identity.cc
SUBCOMMAND: # @com_google_absl//absl/synchronization:synchronization [action 'Compiling absl/synchronization/internal/create_thread_identity.cc', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/com_google_absl/absl/synchronization/_objs/synchronization/create_thread_identity.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/com_google_absl/absl/synchronization/_objs/synchronization/create_thread_identity.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/com_google_absl/absl/synchronization/internal/create_thread_identity.cc -o bazel-out/k8-opt/bin/external/com_google_absl/absl/synchronization/_objs/synchronization/create_thread_identity.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,229 / 1,232] 2 actions, 1 running
    Compiling tensorflow/lite/kernels/add.cc; 6s local
    [Prepa] Compiling absl/synchronization/internal/create_thread_identity.cc
[1,229 / 1,232] 2 actions running
    Compiling tensorflow/lite/kernels/add.cc; 6s local
    Compiling .../synchronization/internal/create_thread_identity.cc; 0s local
[1,230 / 1,232] Compiling tensorflow/lite/kernels/add.cc; 7s local
[1,230 / 1,232] Compiling tensorflow/lite/kernels/add.cc; 9s local
[1,230 / 1,232] Compiling tensorflow/lite/kernels/add.cc; 10s local
[1,230 / 1,232] Compiling tensorflow/lite/kernels/add.cc; 11s local
[1,231 / 1,232] [Prepa] ...r_wrapper:_pywrap_tensorflow_interpreter_wrapper.so
SUBCOMMAND: # //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper.so [action 'Linking tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so-2.params)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,231 / 1,232] [Prepa] ...r_wrapper:_pywrap_tensorflow_interpreter_wrapper.so
[1,231 / 1,232] ...wrapper:_pywrap_tensorflow_interpreter_wrapper.so; 0s local
Target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper up-to-date (nothing to build)
[1,232 / 1,232] checking cached actions
INFO: Elapsed time: 1504.620s, Critical Path: 75.80s
[1,232 / 1,232] checking cached actions
INFO: 1232 processes: 204 internal, 1028 local.
[1,232 / 1,232] checking cached actions
INFO: Build completed successfully, 1232 total actions
INFO: Build completed successfully, 1232 total actions
+ cp /kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/../../../../bazel-bin/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so /kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime
+ chmod u+w /kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so
+ cd /kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ case ""${TENSORFLOW_TARGET}"" in
+ [[ -n '' ]]
+ python3 setup.py bdist bdist_wheel
/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
!!

        ********************************************************************************
        Please avoid running ``setup.py`` directly.
        Instead, use pypa/build, pypa/installer or other
        standards-based tools.

        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
        ********************************************************************************

!!
  self.initialize_options()
/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/dist.py:947: SetuptoolsDeprecationWarning: setup.py install is deprecated.
!!

        ********************************************************************************
        Please avoid running ``setup.py`` directly.
        Instead, use pypa/build, pypa/installer or other
        standards-based tools.

        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
        ********************************************************************************

!!
  command.initialize_options()
+ echo 'Output can be found here:'
Output can be found here:
+ find /kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/MANIFEST.in
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/tflite-runtime-2.13.0.linux-x86_64.tar.gz
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/tflite_runtime-2.13.0-cp310-cp310-linux_x86_64.whl
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/bdist.linux-x86_64
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/lib.linux-x86_64-cpython-310
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/lib.linux-x86_64-cpython-310/tflite_runtime
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/lib.linux-x86_64-cpython-310/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/lib.linux-x86_64-cpython-310/tflite_runtime/metrics_interface.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/lib.linux-x86_64-cpython-310/tflite_runtime/__init__.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/lib.linux-x86_64-cpython-310/tflite_runtime/interpreter.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/lib.linux-x86_64-cpython-310/tflite_runtime/metrics_portable.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/debian
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/debian/changelog
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/debian/copyright
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/debian/rules
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/debian/control
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/debian/compat
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/numpy.cc
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/interpreter_wrapper.h
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/python_utils.h
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/numpy.h
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/interpreter_wrapper_pybind11.cc
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/python_error_reporter.cc
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/python_error_reporter.h
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/python_utils.cc
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/interpreter_wrapper.cc
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/BUILD
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime.egg-info
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime.egg-info/PKG-INFO
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime.egg-info/dependency_links.txt
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime.egg-info/requires.txt
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime.egg-info/top_level.txt
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime.egg-info/SOURCES.txt
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime/metrics_interface.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime/__init__.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime/interpreter.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime/metrics_portable.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/setup.py
+ [[ '' != \y ]]
+ exit 0
total 7748
-rw-r--r-- 1 root root 3960944 Sep 19 03:23 tflite-runtime-2.13.0.linux-x86_64.tar.gz
-rw-r--r-- 1 root root 3966013 Sep 19 03:23 tflite_runtime-2.13.0-cp310-cp310-linux_x86_64.whl
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist
Processing ./tflite_runtime-2.13.0-cp310-cp310-linux_x86_64.whl
Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from tflite-runtime==2.13.0) (1.23.5)
**Installing collected packages: tflite-runtime
Successfully installed tflite-runtime-2.13.0**
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
Cell In[5], line 17
     15 get_ipython().run_line_magic('cd', '/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/')
     16 get_ipython().system('pip install tflite_runtime-2.13.0-cp310-cp310-linux_x86_64.whl')
**---> 17 import tflite_runtime.interpreter as tflite**

File /opt/conda/lib/python3.10/site-packages/tflite_runtime/interpreter.py:33
     30   from tensorflow.python.util.tf_export import tf_export as _tf_export
     31 else:
     32   # This file is part of tflite_runtime package.
---> 33   from tflite_runtime import _pywrap_tensorflow_interpreter_wrapper as _interpreter_wrapper
     34   from tflite_runtime import metrics_portable as metrics
     36   def _tf_export(*x, **kwargs):

**ImportError: /opt/conda/lib/python3.10/site-packages/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so: undefined symbol: _ZN6tflite29farthestpointsamplingLauncherEiiiPKfPfPi**
    
```

**Standalone code to reproduce the issue** 
I tried to add custom OP and built tflite-runtime with shim, see issue https://github.com/tensorflow/tensorflow/issues/61521.
And the custom OP includes some GPU code. 
I built tflite-runtime successfully and the installation seems successfully as well. 
But when tried to import tflite-runtime in python there raised errors about GPU API farthestpointsamplingLauncher. 
Missing the share library of GPU part?

[code and change]
https://github.com/kuangzy2011/tensorflow/blob/main/tensorflow/lite/kernels/shim/test_op/sampling_op.h
https://github.com/kuangzy2011/tensorflow/blob/main/tensorflow/lite/kernels/shim/test_op/sampling_tflite_op.cc
https://github.com/kuangzy2011/tensorflow/blob/main/tensorflow/lite/kernels/shim/test_op/sampling_tflite_op.h
https://github.com/kuangzy2011/tensorflow/blob/main/tensorflow/lite/kernels/shim/test_op/tf_sampling_gpu.cu.cc
https://github.com/kuangzy2011/tensorflow/blob/main/tensorflow/lite/kernels/shim/test_op/BUILD#L159
https://github.com/kuangzy2011/tensorflow/blob/main/tensorflow/lite/kernels/BUILD#L744

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
61905,FPE in DepthwiseConv2D,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Construct a malicious model for the DepthwiseConv2D operator, setting the `stride_width` to greater than `0xffff`. When initializing the DepthwiseParams structure in depthwise_conv.cc, due to precision loss, `op_params.stride_width` becomes 0. `TfLiteDepthwiseConvParams::stride_width` is a 4-byte int type, while `DepthwiseParams::stride_width` is a 2-byte int type.
```
// depthwise_conv.cc
template <KernelType kernel_type>
TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,
                       TfLiteDepthwiseConvParams* params, OpData* data,
                       const TfLiteTensor* input, const TfLiteTensor* filter,
                       const TfLiteTensor* bias, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;    <== here
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,
                                               &op_params.depth_multiplier));
```

The `stride` variable may be equal to 0, leading to a division by zero error.
```cpp
// depthwiseconv_float.h
inline void FloatDepthwiseConvAccumRowGeneric(
    int stride, int dilation_factor, int input_depth, int input_width,
    const float* input_data, int pad_width, int depth_multiplier,
    int filter_width, const float* filter_data, int out_x_buffer_start,
    int out_x_buffer_end, int output_depth, float* acc_buffer) {
  ruy::profiler::ScopeLabel label(""DepthwiseConvAccumRowGeneric (slow)"");
  const float* filter_base_ptr = filter_data;
  for (int filter_x = 0; filter_x < filter_width; ++filter_x) {
    const int out_x_loop_start = std::max(
        out_x_buffer_start,
        (pad_width - dilation_factor * filter_x + stride - 1) / stride);   // FPE
```

[DepthwiseConv2D_FPE.zip](https://github.com/tensorflow/tensorflow/files/12655660/DepthwiseConv2D_FPE.zip)


### Standalone code to reproduce the issue

```shell
When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).

â¯ ./benchmark_model --graph=../poc/DepthwiseConv2D_FPE.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/DepthwiseConv2D_FPE.tflite]
INFO: Loaded model ../poc/DepthwiseConv2D_FPE.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: The input model file size (MB): 0.000772
INFO: Initialized session in 14.64ms.
INFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
[1]    9351 floating point exception (core dumped)  ./benchmark_model --graph=../poc/DepthwiseConv2D_FPE.tflite
```


### Relevant log output

_No response_"
61904,"How to measure data fetching, forward and backward pass time during training","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:   No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**: 2.11.0
-   **Python version**: 3.8
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 12
-   **GPU model and memory**: A100
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
How to measure Data fetching, Data preparation, forward and backward pass time for training [script](https://www.tensorflow.org/tutorials/images/segmentation)?



### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
61903,Results with error set when running tf.raw_ops.StatelessParameterizedTruncatedNormal,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Maybe a better error message should be raised here? 
```
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
```

### Standalone code to reproduce the issue

```shell
results = dict()
import tensorflow as tf
import numpy as np
try:
  try:
    with tf.device('/CPU'):
      shape = []
      seed_0 = -0.28041645497635637
      seed_1 = -434
      seed = [seed_0,seed_1,]
      means = -382
      stddevs_tensor = tf.saturate_cast(tf.random.uniform([], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint64)
      stddevs = tf.identity(stddevs_tensor)
      minvals = []
      maxvals = []
      name_tensor = tf.random.uniform([], dtype=tf.bfloat16)
      name = tf.identity(name_tensor)
      out = tf.raw_ops.StatelessParameterizedTruncatedNormal(shape=shape,seed=seed,means=means,stddevs=stddevs,minvals=minvals,maxvals=maxvals,name=name,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      shape = []
      seed = [seed_0,seed_1,]
      stddevs = tf.identity(stddevs_tensor)
      stddevs = tf.cast(stddevs, tf.uint64)
      minvals = []
      maxvals = []
      name = tf.identity(name_tensor)
      name = tf.cast(name, tf.bfloat16)
      tf.raw_ops.StatelessParameterizedTruncatedNormal(shape=shape,seed=seed,means=means,stddevs=stddevs,minvals=minvals,maxvals=maxvals,name=name,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))

print(results)
```
```


### Relevant log output

```shell
2023-09-18 23:45:51.224040: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-18 23:45:51.345060: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-09-18 23:45:51.934234: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-09-18 23:45:51.934291: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-09-18 23:45:51.934301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-18 23:45:52.423299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-09-18 23:45:52.450695: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2023-09-18 23:45:52.450716: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-09-18 23:45:52.451038: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
{}
```
```
"
61902,FPE in Conv2d,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

Ubuntu 18.04.6

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Construct a malicious Conv2d operator model, so that `stride_width` and `stride_height` are set to 0. In the `eigen_spatial_convolutions-inl.h` file, having the `row_stride` and `col_stride` variables set to 0 leads to a division by zero exception.
```cpp
// eigen_spatial_convolutions-inl.h
SpatialConvolution(const Input& input, const Kernel& kernel,
                   const Index row_stride = 1, const Index col_stride = 1,
                   const PaddingType padding_type = PADDING_SAME,
                   const Index row_in_stride = 1, const Index col_in_stride = 1,
                   const OutputKernel& output_kernel = OutputKernel(),
                   Index padding_top = 0, Index padding_bottom = 0,
                   Index padding_left = 0, Index padding_right = 0) {
      ...
    case PADDING_SAME: {
      eigen_assert(!padding_explicit);
      out_height = divup(InputRows, row_stride);   // FPE
      out_width = divup(InputCols, col_stride);
```

[Conv2d_FPE.zip](https://github.com/tensorflow/tensorflow/files/12655589/Conv2d_FPE.zip)


### Standalone code to reproduce the issue

```shell
When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).


â¯ ./benchmark_model --graph=../poc/Conv2d_FPE.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/Conv2d_FPE.tflite]
INFO: Loaded model ../poc/Conv2d_FPE.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: The input model file size (MB): 0.000708
INFO: Initialized session in 31.286ms.
INFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
[1]    24937 floating point exception (core dumped)  ./benchmark_model --graph=../poc/Conv2d_FPE.tflite
```


### Relevant log output

_No response_"
61901,FPE in BatchMatMul,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?
Construct a malicious BatchMatMul operator model with a shape of 4x0x0x0. 
The `input_shape.Dims(i)`  variable may be equal to 0, leading to a division by zero error.
```cpp
// transpose_utils.cc
size_t Flatten(const RuntimeShape& input_shape,
               const RuntimeShape& output_shape, const TransposeParams& params,
               RuntimeShape* non_flatten_input_shape,
               RuntimeShape* non_flatten_output_shape,
               TransposeParams* non_flatten_params) {
  // Calculate the total size of non-flatten dimensions.
  int skip_dims_cnt = 0;
  size_t flat_size = input_shape.FlatSize();
  for (int i = 0; i < params.perm_count; ++i) {
    if (params.perm[i] == i) {
      flat_size /= input_shape.Dims(i);  // FPE
```
[BatchMatMul_FPE.zip](https://github.com/tensorflow/tensorflow/files/12655488/BatchMatMul_FPE.zip)


### Standalone code to reproduce the issue

```shell
When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOSï¼ˆcoredump).


â¯ ./benchmark_model --graph=../poc/BatchMatMul_FPE.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/BatchMatMul_FPE.tflite]
INFO: Loaded model ../poc/BatchMatMul_FPE.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: The input model file size (MB): 0.0006
INFO: Initialized session in 155.728ms.
INFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
[1]    26298 floating point exception (core dumped)  ./benchmark_model --graph=../poc/BatchMatMul_FPE.tflite
```
```


### Relevant log output

_No response_"
61900,module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Very new to coding so am unfamiliar with what to say. I used tensor flow to run a model to find the best fitted line. Last week the model run perfectly. This week the script incurs an error upon running the model. Ran a script that was even older and incurred that same problem.

Had read that the issue is to do with conflict between _tensorflow_ and _keras_ but I don't understand what that means with relation to my code. 

I also read not to use the term 'python' when importing a library.

from tensorflow.**python**.keras.models import Sequential

I have to use 'python' as part of the syntax as google colab reports this message if I don't 
_Import ""tensorflow.keras.models"" could not be resolved(reportMissingImports)_

Is this something to so with my google colab environment? I updated tensorflow to the latest version but the problem still exists. 

I have no idea what to do and am quite stuck.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.activations import linear
from tensorflow.python.keras.optimizers import adam_v2
from tensorflow.python.keras.losses import MeanSquaredError

x = [ [23], [45], [78], [12]]
y = [ [12], [22], [36], [6]]

model = Sequential([
    tf.keras.layers.Dense(units=25, activation='relu', name='layer1'),
    tf.keras.layers.Dense(input_shape=(25,), units=15, activation='relu', name='layer2'),
    tf.keras.layers.Dense(input_shape=(15,), units=1, activation='linear', name='layer3')
] ,name=""Model1""
)

loss=MeanSquaredError()
opt=adam_v2.Adam(learning_rate=0.001)

model.compile(
    loss=loss,
    optimizer=opt
    )
model.fit(x, y, epochs=100)
```


### Relevant log output

```shell
AttributeError                            Traceback (most recent call last)
<ipython-input-27-a1b963ff7586> in <cell line: 26>()
     24     optimizer=opt
     25     )
---> 26 model.fit(x, y, epochs=100)

6 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1136          training_utils.RespectCompiledTrainableState(self):
   1137       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.
-> 1138       data_handler = data_adapter.get_data_handler(
   1139           x=x,
   1140           y=y,

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in get_data_handler(*args, **kwargs)
   1396   if getattr(kwargs[""model""], ""_cluster_coordinator"", None):
   1397     return _ClusterCoordinatorDataHandler(*args, **kwargs)
-> 1398   return DataHandler(*args, **kwargs)
   1399 
   1400 

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)
   1150       self._steps_per_execution_value = steps_per_execution.numpy().item()
   1151 
-> 1152     adapter_cls = select_data_adapter(x, y)
   1153     self._adapter = adapter_cls(
   1154         x,

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in select_data_adapter(x, y)
    986 def select_data_adapter(x, y):
    987   """"""Selects a data adapter than can handle a given x and y.""""""
--> 988   adapter_cls = [cls for cls in ALL_ADAPTER_CLS if cls.can_handle(x, y)]
    989   if not adapter_cls:
    990     # TODO(scottzhu): This should be a less implementation-specific error.

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in <listcomp>(.0)
    986 def select_data_adapter(x, y):
    987   """"""Selects a data adapter than can handle a given x and y.""""""
--> 988   adapter_cls = [cls for cls in ALL_ADAPTER_CLS if cls.can_handle(x, y)]
    989   if not adapter_cls:
    990     # TODO(scottzhu): This should be a less implementation-specific error.

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in can_handle(x, y)
    705   def can_handle(x, y=None):
    706     return (isinstance(x, (data_types.DatasetV1, data_types.DatasetV2)) or
--> 707             _is_distributed_dataset(x))
    708 
    709   def __init__(self,

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _is_distributed_dataset(ds)
   1697 
   1698 def _is_distributed_dataset(ds):
-> 1699   return isinstance(ds, input_lib.DistributedDatasetInterface)

AttributeError: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'
```
"
61899,TFLite model cannot slice a zero-dim tensor,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.15.0-dev20230918


### 2. Code

```
import tensorflow as tf

class Model(tf.keras.Model):
  def __init__(self):
    super(Model, self).__init__()

  def call(self, x):
    values, indices = tf.math.top_k(x, k=2, sorted=False)
    y = tf.slice(values, tf.constant([0, 0]), tf.constant([0, 1]))
    return y

# Initializing the model
m = Model()

# Inputs to the model
x = tf.constant([1., 2.], shape=[1, 2])

expected_value = m(x)
print(expected_value.numpy())


converter = tf.lite.TFLiteConverter.from_keras_model(m)
tflite_model = converter.convert()

def _evaluateTFLiteModel(tflite_model, input_data):
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    for i in range(len(input_data)):
        interpreter.set_tensor(input_details[i]['index'], input_data[i])
    interpreter.invoke()
    output_data = [interpreter.get_tensor(output_details[i]['index'])
                   for i in range(len(output_details))]
    return output_data


actual_value = _evaluateTFLiteModel(tflite_model,[x])
print('tflite model output:')
print(actual_value[0])
```


### 3. Failure after conversion
Output:
```
keras model output:
[]
...

ValueError: Invalid tensor size.
```
The conversion is successful, but it throws an error during inference. This is because the `size` to `tf.slice` contains zero. If we change `size` to `[1,1]`, there's no error.

I think tflite model should support slice size = 0, similar to the original model."
61898,Extra semicolon in tensorflow/lite/micro/micro_profiler.h:90,"Remove extra semicolon in tensorflow/lite/micro/micro_profiler.h on line 90

Before:
```
TF_LITE_REMOVE_VIRTUAL_DELETE;
```

After:
```
TF_LITE_REMOVE_VIRTUAL_DELETE
```"
61896,Why are not all of my conv2d layer weights Int8 when converting with dynamic range quantization?,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Ventura 13.5.2 (22G91)
- TensorFlow installation (pip package or built from source): pip 
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13.0

### 2. Code
I have a [Keras model](https://github.com/tensorflow/tensorflow/files/12650691/model.zip) (here, I uploaded just an untrained sub-model which is equivalent to the beginning of my actual model) and follow the [Post-training dynamic range quantization guide](https://www.tensorflow.org/lite/performance/post_training_quant) to convert it to a tflite model.
```py
# model defined above
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
```

### 3. Failure after conversion
- Model produces correct results, but it is slower than the dynamic range Float16 quantized version.
- Tflite model has some Conv2D layers with Int8 weights (expected) and some Conv2D layers with Float32 weights (not expected)

### 5. (optional) Any other info / logs
I am confused because I didn't come across any documentation whatsoever stating that only parts of the model might be quantized. "
61895,Building benchmark_model using Buildroot failed,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

WSL2/Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

gcc compiler for aarch64

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello,

I am trying to build the benchmark_model from tensorflow-lite using Buildroot for my i.MX 8 platform. The tensorflow-lite and label_image examples are built successfully. But now when I am building the benchmark_model I am getting below error from `benchmark_tflite_model.cc`

``undefined reference to `absl::lts_20220623``

Buildroot is using `cmake` and build command is
`/usr/bin/cmake --build /.../build/tensorflow-lite-2.11.0/tensorflow/lite/buildroot-build -t benchmark_model`

Same build command was used for label_image and there were no errors related to linking.

### Standalone code to reproduce the issue

```shell
`/usr/bin/cmake --build /.../build/tensorflow-lite-2.11.0/tensorflow/lite/buildroot-build -t benchmark_model`
```


### Relevant log output

```shell
[ 98%] Building CXX object tools/benchmark/CMakeFiles/benchmark_model.dir/__/delegates/external_delegate_provider.cc.o
[100%] Linking CXX executable benchmark_model
...per-package/tensorflow-lite/host/opt/ext-toolchain/bin/../lib/gcc/aarch64-buildroot-linux-gnu/10.3.0/../../../../aarch64-buildroot-linux-gnu/bin/ld: CMakeFiles/benchmark_model.dir/benchmark_tflite_model.cc.o: in function `absl::lts_20220623::strings_internal::Splitter<absl::lts_20220623::ByChar, absl::lts_20220623::AllowEmpty, std::basic_string_view<char, std::char_traits<char> > >::ConvertToContainer<std::vector<std::basic_string_view<char, std::char_traits<char> >, std::allocator<std::basic_string_view<char, std::char_traits<char> > > >, std::basic_string_view<char, std::char_traits<char> >, false>::operator()(absl::lts_20220623::strings_internal::Splitter<absl::lts_20220623::ByChar, absl::lts_20220623::AllowEmpty, std::basic_string_view<char, std::char_traits<char> > > const&) const [clone .isra.0]':
benchmark_tflite_model.cc:(.text+0x204): undefined reference to `absl::lts_20220623::ByChar::Find(std::basic_string_view<char, std::char_traits<char> >, unsigned long) const'
...per-package/tensorflow-lite/host/opt/ext-toolchain/bin/../lib/gcc/aarch64-buildroot-linux-gnu/10.3.0/../../../../aarch64-buildroot-linux-gnu/bin/ld: benchmark_tflite_model.cc:(.text+0x284): undefined reference to `absl::lts_20220623::ByChar::Find(std::basic_string_view<char, std::char_traits<char> >, unsigned long) const'
...per-package/tensorflow-lite/host/opt/ext-toolchain/bin/../lib/gcc/aarch64-buildroot-linux-gnu/10.3.0/../../../../aarch64-buildroot-linux-gnu/bin/ld: CMakeFiles/benchmark_model.dir/benchmark_tflite_model.cc.o: in function `tflite::benchmark::SplitInputLayerNameAndValueFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >&)':
benchmark_tflite_model.cc:(.text+0xcf0): undefined reference to `absl::lts_20220623::StrReplaceAll[abi:cxx11](std::basic_string_view<char, std::char_traits<char> >, std::initializer_list<std::pair<std::basic_string_view<char, std::char_traits<char> >, std::basic_string_view<char, std::char_traits<char> > > >)'
...per-package/tensorflow-lite/host/opt/ext-toolchain/bin/../lib/gcc/aarch64-buildroot-linux-gnu/10.3.0/../../../../aarch64-buildroot-linux-gnu/bin/ld: benchmark_tflite_model.cc:(.text+0xdd4): undefined reference to `absl::lts_20220623::StrReplaceAll[abi:cxx11](std::basic_string_view<char, std::char_traits<char> >, std::initializer_list<std::pair<std::basic_string_view<char, std::char_traits<char> >, std::basic_string_view<char, std::char_traits<char> > > >)'
...per-package/tensorflow-lite/host/opt/ext-toolchain/bin/../lib/gcc/aarch64-buildroot-linux-gnu/10.3.0/../../../../aarch64-buildroot-linux-gnu/bin/ld: CMakeFiles/benchmark_model.dir/benchmark_tflite_model.cc.o: in function `tflite::benchmark::BenchmarkTfLiteModel::ValidateParams()':
benchmark_tflite_model.cc:(.text+0x8388): undefined reference to `absl::lts_20220623::numbers_internal::safe_strto32_base(std::basic_string_view<char, std::char_traits<char> >, int*, int)'
...per-package/tensorflow-lite/host/opt/ext-toolchain/bin/../lib/gcc/aarch64-buildroot-linux-gnu/10.3.0/../../../../aarch64-buildroot-linux-gnu/bin/ld: benchmark_tflite_model.cc:(.text+0x83a4): undefined reference to `absl::lts_20220623::numbers_internal::safe_strto32_base(std::basic_string_view<char, std::char_traits<char> >, int*, int)'
collect2: error: ld returned 1 exit status
```
"
61893,BMP decode channels = 1 support,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`decode_bmp` to support `channels=1`

This seems like an arbitrary limitation to not support this for BMP formats.

### Standalone code to reproduce the issue

```shell
import tensorflow

image = tensorflow.image.decode_image(bmp_bytes, channels=1)
```


### Relevant log output

```shell
`tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DecodeImage_device_/job:localhost/replica:0/task:0/device:CPU:0}} `channels` must be 0, 3 or 4 for BMP, but got 1 [Op:DecodeImage] name:`
```
"
61891,TFLite inference order is not the same as TensorFlow model,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13.0

### 2. Code

This is a custom depth-first inference layer, I try it using a (1, 7, 7, 4) tensor, the output is the same as the two layer cnn model and the output size is (1, 3, 3, 1).

```
create_conv_layer1 = Conv2D(filters=4, kernel_size=(3, 3), activation='relu', trainable=False, kernel_initializer=tf.initializers.Constant(0.5))
create_conv_layer2 = Conv2D(filters=1, kernel_size=(3, 3), activation='relu', trainable=False, kernel_initializer=tf.initializers.Constant(0.5))
```

```
class PatchBasedConv2D(Layer):
    def __init__(self, **kwargs):
        super(PatchBasedConv2D, self).__init__(**kwargs)
        self.layer_number = 2
        self.output_size = (3, 3, 1)
        self.expand_size = (1, 1)
        self.patch_stride = None
    
    @staticmethod
    def compute_last_patch_size(output_size, kernel_size, stride):
        input_height = (output_size[0] - 1) * stride[0] + kernel_size[0]
        input_width = (output_size[1] - 1) * stride[1] + kernel_size[1]
        return (input_height, input_width)
    
    def calculate_patch_count(self, input_size, patch_size, stride):
        return ((input_size[0] - patch_size[0]) // stride[0] + 1) * ((input_size[1] - patch_size[1]) // stride[1] + 1)
    
    def get_current_patch_possition(self, input_size, patch_size, stride, current_round):
        return ((current_round // ((input_size[1] - patch_size[1]) // stride[1] + 1)) * stride[0],
              (current_round % ((input_size[1] - patch_size[1]) // stride[1] + 1)) * stride[1])

    def build(self, input_shape):
        with tf.device('/CPU:0'):
            self.conv1 = create_conv_layer1
            self.conv2 = create_conv_layer2
            self.patch_size_tmp = self.compute_last_patch_size(self.expand_size, self.conv2.kernel_size, self.conv2.strides)
            self.patch_size = self.compute_last_patch_size(self.patch_size_tmp, self.conv1.kernel_size, self.conv1.strides)
            self.patch_stride = self.conv1.strides

    def call(self, inputs):
        if self.conv1.padding == 'same':
            inputs = tf.pad(inputs, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')
            
        num_patch = self.calculate_patch_count((inputs.shape[1], inputs.shape[2]), self.patch_size, self.patch_stride)
        
        number_patch_in_row = int(self.output_size[1] // self.expand_size[1])
        output_feature_map_tmp = None 
        output_feature_map = None
        
        for current_round in range(num_patch):
            position = get_current_patch_possition((inputs.shape[1], inputs.shape[2]), self.patch_size, self.patch_stride, current_round)
            patch_output = self.conv1(inputs[:, position[0]: position[0] + self.patch_size[0], position[1]: position[1] + self.patch_size[1], :])
            patch_output = self.conv2(patch_output)
            
            if output_feature_map_tmp is None:
                output_feature_map_tmp = patch_output
            else:
                output_feature_map_tmp = tf.concat([output_feature_map_tmp, patch_output], axis=2)
            
            if (current_round + 1) % number_patch_in_row == 0 and current_round != 0:
                if output_feature_map is None:
                    output_feature_map = output_feature_map_tmp
                else:
                    output_feature_map = tf.concat([output_feature_map, output_feature_map_tmp], axis=1)
                output_feature_map_tmp = None
        
        return output_feature_map
```

### 3. Failure after conversion
I am working on implementing depth-first inference on Tflite micro, above is my custom layer. You can see that in the call function, I picked out a patch of the input image and did 2 layers of Conv first, then went on to the next patch. It works just fine in my jupyter-notebook. However, when I convert it to tflite using TFLiteConverter, I found that it first expand all the patches, then do 2 Conv on all the patches. This turns out to be an inference layer-by-layer and uses up even more memory, why is that?


![æˆªåœ– 2023-09-18 ä¸‹åˆ7 33 19](https://github.com/tensorflow/tensorflow/assets/68526411/e3768de1-b03f-44a8-8a61-51318c8b8f92)"
61890,Build problems when using TensorFlow Lite from another project in Windows,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.11

### Bazel version

compiled using CMake

### GCC/compiler version

MSVC 17 2022

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When trying to create a project wheel which uses TensorFlow Lite, it is expected to work fine with:
`python setup.py bdist_wheel`

This command works fine for MacOS and Linux but does not work for Windows raising an error from the `tensorflow_src\tensorflow/lite/core/interpreter.h` file (you can see the error in the Relevant log output section).

For using TensorFlow Lite from the project, we followed the instructions in https://www.tensorflow.org/lite/guide/build_cmake#create_a_cmake_project_which_uses_tensorflow_lite as can be seen in its CMake files (https://github.com/Blosc/blosc2_btune/blob/main/src/CMakeLists.txt).

Also, there is no issue even on Windows for building TensorFlow Lite alone with 
```
cmake ../tensorflow_src/tensorflow/lite
cmake --build . -j
```

### Standalone code to reproduce the issue

```shell
The issue project is blosc2_btune (https://github.com/Blosc/blosc2_btune). You can clone it

`git clone https://github.com/Blosc/blosc2_btune.git`

install the requirements

`python -m pip install -r requirements-build.txt`

and reproduce the error with


prebuild.sh
python setup.py bdist_wheel
```
```


### Relevant log output

```shell
Generating Code...
  btune_model.cpp
C:\Users\marta\blosc2_btune\tensorflow_src\tensorflow/lite/core/interpreter.h(1000,40): error C2665: 'std::atomic_flag:
:atomic_flag': no overloaded function could convert all the argument types [C:\Users\marta\blosc2_btune\_skbuild\win-am
d64-3.11\cmake-build\src\blosc2_btune.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.37.32822\include\atomic(2886,1): messag
e : could be 'std::atomic_flag::atomic_flag(const std::atomic_flag &)' [C:\Users\marta\blosc2_btune\_skbuild\win-amd64-
3.11\cmake-build\src\blosc2_btune.vcxproj]
C:\Users\marta\blosc2_btune\tensorflow_src\tensorflow/lite/core/interpreter.h(1000,40): message : 'std::atomic_flag::at
omic_flag(const std::atomic_flag &)': cannot convert argument 1 from 'bool' to 'const std::atomic_flag &' [C:\Users\mar
ta\blosc2_btune\_skbuild\win-amd64-3.11\cmake-build\src\blosc2_btune.vcxproj]
C:\Users\marta\blosc2_btune\tensorflow_src\tensorflow/lite/core/interpreter.h(1000,41): message : Reason: cannot conver
t from 'bool' to 'const std::atomic_flag' [C:\Users\marta\blosc2_btune\_skbuild\win-amd64-3.11\cmake-build\src\blosc2_b
tune.vcxproj]
C:\Users\marta\blosc2_btune\tensorflow_src\tensorflow/lite/core/interpreter.h(1000,40): message : while trying to match
 the argument list '(bool)' [C:\Users\marta\blosc2_btune\_skbuild\win-amd64-3.11\cmake-build\src\blosc2_btune.vcxproj]
Traceback (most recent call last):
  File ""C:\Users\marta\miniconda3\envs\blosc2_btune2\Lib\site-packages\skbuild\setuptools_wrap.py"", line 674, in setup
    cmkr.make(make_args, install_target=cmake_install_target, env=env)
  File ""C:\Users\marta\miniconda3\envs\blosc2_btune2\Lib\site-packages\skbuild\cmaker.py"", line 697, in make
    self.make_impl(clargs=clargs, config=config, source_dir=source_dir, install_target=install_target, env=env)
  File ""C:\Users\marta\miniconda3\envs\blosc2_btune2\Lib\site-packages\skbuild\cmaker.py"", line 742, in make_impl
    raise SKBuildError(msg)

An error occurred while building with CMake.
  Command:
    'C:\Users\marta\miniconda3\envs\blosc2_btune2\Lib\site-packages\cmake\data\bin/cmake.exe' --build . --target install --config Release --
  Install target:
    install
  Source directory:
    C:\Users\marta\blosc2_btune
  Working directory:
    C:\Users\marta\blosc2_btune\_skbuild\win-amd64-3.11\cmake-build
Please check the install target is valid and see CMake's output for more information.
```
"
61889,tensorflow.tf concurrency issue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

unknown 2.11.0 (from nvcr.io/nvidia/tensorflow:23.03-tf2-py3))

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda_12.1.r12.1/compiler.32415258_0

### GPU model and memory

_No response_

### Current behavior?

We found that running tensorflow.concat operation will have concurrency issue only in the Nvidia tensorflow image. We also tried with other image but the issue cannot reproduce.

We expect that tensorflow.concat work fine for multi-thread environment. But it's not. Is it expected? 
FYI, adding a lock for concat can avoid such issue. What's the best practice?

Our environment:
GPU: A100
image: nvcr.io/nvidia/tensorflow:23.03-tf2-py3
driver: NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from concurrent.futures import ThreadPoolExecutor, as_completed

client_num = 10
repeat = 10000
executor = ThreadPoolExecutor(max_workers=client_num)
future_2_input_shapes = {}

# import threading
# lock = threading.Lock()

test_cases = [
    [[1, 2] * 31, [2, 3]],
    [[1, 2, 8, 90] * 31, [2, 3, 3, 1]],
    [[[7, 4, 3], [8, 4, 3]], [[2, 10, 3], [15, 11, 3]] * 63],
]


def do_concat(t1, t2):
    # with lock:
    #     return tf.concat([t1, t2], 0)
    return tf.concat([t1, t2], 0)


print(""creating task"")
for _ in range(repeat):
    for test_case in test_cases:
        a = tf.constant(test_case[0])
        b = tf.constant(test_case[1])
        future = executor.submit(do_concat, a, b)
        future_2_input_shapes[future] = a.shape, b.shape

print(""waiting task"")
count = 0
for future in as_completed(future_2_input_shapes.keys()):
    print(f""{count}: {future_2_input_shapes[future]}"")
    data = future.result()
    count = count + 1




```
```


### Relevant log output

```shell
...
17086: (TensorShape([62]), TensorShape([2]))
17087: (TensorShape([2, 3]), TensorShape([126, 3]))
17088: (TensorShape([124]), TensorShape([4]))
17089: (TensorShape([124]), TensorShape([4]))
17090: (TensorShape([2, 3]), TensorShape([126, 3]))
17091: (TensorShape([124]), TensorShape([4]))
17092: (TensorShape([2, 3]), TensorShape([126, 3]))
Traceback (most recent call last):
  File ""x.py"", line 37, in <module>
    data = future.result()
  File ""/usr/lib/python3.8/concurrent/futures/_base.py"", line 437, in result
    return self.__get_result()
  File ""/usr/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result
    raise self._exception
  File ""/usr/lib/python3.8/concurrent/futures/thread.py"", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""x.py"", line 22, in do_concat
    return tf.concat([t1, t2], 0)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 7215, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:GPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [124] vs. shape[1] = [126,3] [Op:ConcatV2] name: concat
```
"
61887,Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

master branch, commit: a442440

### Custom code

No

### OS platform and distribution

Windows Server 2022

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.3.2

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\CPPTES~1\AppData\Local\Temp\2\Bazel.runfiles_pr3ptqbl\runfiles\org_tensorflow\tensorflow\python\tools\api\generator\create_python_api.py"", line 22, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""C:\Users\CPPTES~1\AppData\Local\Temp\2\Bazel.runfiles_pr3ptqbl\runfiles\org_tensorflow\tensorflow\python\__init__.py"", line 37, in <module>
    from tensorflow.python.tpu import api
  File ""C:\Users\CPPTES~1\AppData\Local\Temp\2\Bazel.runfiles_pr3ptqbl\runfiles\org_tensorflow\tensorflow\python\tpu\api.py"", line 22, in <module>
    from tensorflow.python.tpu import bfloat16
  File ""C:\Users\CPPTES~1\AppData\Local\Temp\2\Bazel.runfiles_pr3ptqbl\runfiles\org_tensorflow\tensorflow\python\tpu\bfloat16.py"", line 20, in <module>
    from tensorflow.python.framework import dtypes
  File ""C:\Users\CPPTES~1\AppData\Local\Temp\2\Bazel.runfiles_pr3ptqbl\runfiles\org_tensorflow\tensorflow\python\framework\dtypes.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\CPPTES~1\AppData\Local\Temp\2\Bazel.runfiles_pr3ptqbl\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\CPPTES~1\AppData\Local\Temp\2\Bazel.runfiles_pr3ptqbl\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: F:/tensorflow/tensorflow/tensorflow/tools/pip_package/BUILD:252:10 Middleman _middlemen/tensorflow_Stools_Spip_Upackage_Sbuild_Upip_Upackage.exe-runfiles failed: (Exit 1): bash.exe failed: error executing command (from target //tensorflow:tf_python_api_gen_v2)

### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow.git F:\Tensorflow\tensorflow
cd /d F:\Tensorflow\tensorflow
pip3 uninstall -r tensorflow/tools/ci_build/release/requirements_common.txt --yes
pip3 install -r tensorflow/tools/ci_build/release/requirements_common.txt --upgrade
set PATH=F:\Tensorflow\tensorflow\..\tools;%path%
set PATH=F:\Tensorflow\tensorflow\..\tools\msys64\usr\bin;%path%
yes """" 2>nul | python ./configure.py
C:\Python39\python.exe -m pip install --upgrade pip
set TF_PYTHON_VERSION=3.9
set BAZEL_VC=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC
set BAZEL_VC_FULL_VERSION=14.37.32822
set PATH=F:\Tensorflow\tensorflow\..\tools;%path%
set PATH=F:\Tensorflow\tensorflow\..\tools\msys64\usr\bin;%path%
bazel --output_user_root F:\bazelTemp build --jobs 8 --config=opt --local_ram_resources=4096 --host_cxxopt=""/D_DISABLE_CONSTEXPR_MUTEX_CONSTRUCTOR"" --subcommands //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_"
61886,TF Lite produces wrong graph with a sequence of tensor reshape operators,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.15.0-dev20230917

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

```
x1 = tf.constant([[1., 2.], [3., 4.], [5., 6.]], shape=[3, 2])

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(input_signature=[tf.TensorSpec(x1.shape, x1.dtype)])
  def call(self, x):
    a = tf.reshape(x, [3, 2, 1])
    b = tf.unstack(a, axis=1)
    c = tf.concat(b, 0)
    d = tf.reshape(c, [3, 2])
    return d

m = Model()
expected_value = m(x1)
print('keras model output:')
print(expected_value.numpy())


converter = tf.lite.TFLiteConverter.from_keras_model(m)
tflite_model = converter.convert()

def _evaluateTFLiteModel(tflite_model, input_data):
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    for i in range(len(input_data)):
        interpreter.set_tensor(input_details[i]['index'], input_data[i])
    interpreter.invoke()
    output_data = [interpreter.get_tensor(output_details[i]['index'])
                   for i in range(len(output_details))]
    return output_data


actual_value = _evaluateTFLiteModel(tflite_model,[x1])
print('tflite model output:')
print(actual_value[0])
tf.lite.experimental.Analyzer.analyze(model_content=tflite_model)
```

### 3. Failure after conversion
The model conversion is successful, but it produces wrong results.
Output:
```
keras model output:
[[1. 3.]
 [5. 2.]
 [4. 6.]]
tflite model output:
[[1. 2.]
 [3. 4.]
 [5. 6.]]
```
TFLite ModelAnalyzer:
```
Your TFLite model has '1' subgraph(s). In the subgraph description below,
T# represents the Tensor numbers. Subgraph#0 main(T#0) -> [T#0]

Tensors of Subgraph#0
  T#0(serving_default_args_0:0) shape:[3, 2], type:FLOAT32

---------------------------------------------------------------
Your TFLite model has '1' signature_def(s).

Signature#0 key: 'serving_default'
- Subgraph: Subgraph#0
- Inputs: 
    'args_0' : T#0
- Outputs: 
    'output_1' : T#0
```
"
61885,Adding `@tf.function` changes model output,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230917

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

After adding `@tf.function` to the following model, the output `y` is wrong (`[[42, 36]]`). This issue only happens on CPU. On CUDA it is able to output the expected output (`[[42, 42]]`).

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x1 = tf.constant([[6., 7.]], shape=[1, 2])

##### With @tf.function #####
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()
    self.fc1 = tf.keras.layers.Dense(2, name='fc', kernel_initializer='ones', bias_initializer='ones')
    self.fc2 = tf.keras.layers.Dense(2, name='fc', kernel_initializer='ones', bias_initializer='ones')

  @tf.function
  def call(self, x):
    x = self.fc1(x)
    x = self.fc2(x)
    x = tf.constant([6.]) + x
    return tf.constant([7.]) + x

m = Model()
y = m(x1)
print(y.numpy())

##### Without @tf.function #####
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()
    self.fc1 = tf.keras.layers.Dense(2, name='fc', kernel_initializer='ones', bias_initializer='ones')
    self.fc2 = tf.keras.layers.Dense(2, name='fc', kernel_initializer='ones', bias_initializer='ones')

  def call(self, x):
    x = self.fc1(x)
    x = self.fc2(x)
    x = tf.constant([6.]) + x
    return tf.constant([7.]) + x

m = Model()
expected_value = m(x1)
print(expected_value.numpy())
```


### Relevant log output

```shell
[[42. 36.]]
[[42. 42.]]
```
"
61884,XLA doesn't do the DCE as autocluster,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

XLA doesn't do the DCE as autocluster. In the example below, the second line `sliced2 = tf.slice(x1, [0, 1, 0, 0], [-1, -1, 4, -1])` is dead code, which is deleted when enabling `autocluster`. However, the XLA compiled model will still execute this line.

It is expected to delete this line since it is dead code.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os

""""""
Autocluster
""""""

os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit'

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function
  def call(self, x1):
    sliced1 = tf.slice(x1, [0, 0, 1, 0], [-1, -1, 1, -1])
    sliced2 = tf.slice(x1, [0, 1, 0, 0], [-1, -1, 4, -1])
    return sliced1

# Initializing the model
x1_shape = (1, 3, 3, 2)
m = Model()

# Inputs to the model
x1 = tf.range(18)
x1 = tf.reshape(x1, x1_shape)

# Call model
output_tensor = m(x1)

""""""
XLA
""""""
os.environ['TF_XLA_FLAGS'] = ''

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(jit_compile=True)
  def call(self, x1):
    sliced1 = tf.slice(x1, [0, 0, 1, 0], [-1, -1, 1, -1])
    sliced2 = tf.slice(x1, [0, 1, 0, 0], [-1, -1, 4, -1])
    return sliced1

# Initializing the model
x1_shape = (1, 3, 3, 2)
m = Model()

# Inputs to the model
x1 = tf.range(18)
x1 = tf.reshape(x1, x1_shape)

# Call model
output_tensor = m(x1)

""""""
InvalidArgumentError: Exception encountered when calling layer 'model_5' (type Model).

Expected size[2] in [0, 3], but got 4
""""""
```


### Relevant log output

_No response_"
61882,XLA compiled model skip `build` method,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

XLA compiled model skip `build` method, which is not expected since the `build` method is expected to be invoked automatically before the first execution of `call()`invoked automatically before the first execution of call().

The example below shows that the XLA compiled model doesn't invoke `build` since `self.w` is still `[[1, 0], [0, 1]]` instead of the result by `add_weight`.

### Standalone code to reproduce the issue

```shell
""""""
Without XLA
""""""
import tensorflow as tf

class Model(tf.keras.Model):
  def __init__(self):
    super(Model, self).__init__()
    self.w = tf.Variable([[1., 0.], [0., 1.]])

  def build(self, input_shape):
    super(Model, self).build(input_shape)
    self.w = self.add_weight(""weight"", shape=input_shape[1:], trainable=True)
    
  def call(self, x):
    return tf.matmul(x, self.w), self.w

# Initializing the model
m = Model()
# Input to the model
x1 = tf.constant([[6., 7.], [2., 7.]], shape=[1,2,2])
print(m(x1))
""""""
(<tf.Tensor: shape=(1, 2, 2), dtype=float32, numpy=
array([[[4.0300035, 3.5133138],
        [3.804155 , 5.0799932]]], dtype=float32)>, <tf.Variable 'model_4/weight:0' shape=(2, 2) dtype=float32, numpy=
array([[ 0.05646205, -0.3916698 ],
       [ 0.5273187 ,  0.83761895]], dtype=float32)>)
""""""

""""""
With XLA
""""""

class Model(tf.keras.Model):
  def __init__(self):
    super(Model, self).__init__()
    self.w = tf.Variable([[1., 0.], [0., 1.]])

  def build(self, input_shape):
    super(Model, self).build(input_shape)
    self.w = self.add_weight(""weight"", shape=input_shape[1:], trainable=True)

  @tf.function(jit_compile=True)  
  def call(self, x):
    return tf.matmul(x, self.w), self.w

# Initializing the model
m = Model()
# Input to the model
x1 = tf.constant([[6., 7.], [2., 7.]], shape=[1,2,2])
print(m(x1))

""""""
(<tf.Tensor: shape=(1, 2, 2), dtype=float32, numpy=
array([[[6., 7.],
        [2., 7.]]], dtype=float32)>, <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[1., 0.],
       [0., 1.]], dtype=float32)>)
""""""
```


### Relevant log output

_No response_"
61881,XLA compiled `tf.matmul` can work for two size-incompatible matrices,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

XLA compiled `tf.matmul` can work for two size-incompatible matrices, like `[1, 5]` and `[10, 1]`. By contrast, if we run `tf.matmul` directly without XLA compilation, it will raise the error as expected: `Matrix size-incompatible: In[0]: [1,5], In[1]: [10,1] [Op:BatchMatMulV2] name: `

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
""""""
With XLA
""""""
class Model(tf.keras.Model):
  def __init__(self):
    super(Model, self).__init__()
    self.w = tf.Variable(tf.random.normal([10,1]),
            shape=tf.TensorShape(None),
            dtype='float32')
  @tf.function(jit_compile=True)
  def call(self, x):
    return tf.matmul(x, self.w)

m = Model()

# Inputs to the model
x1 = tf.constant([1., 2., 3., -3., 2.5], shape=[1, 5])
print(m(x1))
# tf.Tensor([[-11.37509]], shape=(1, 1), dtype=float32)

""""""
Without XLA
""""""
class Model(tf.keras.Model):
  def __init__(self):
    super(Model, self).__init__()
    self.w = tf.Variable(tf.random.normal([10,1]),
            shape=tf.TensorShape(None),
            dtype='float32')

  def call(self, x):
    return tf.matmul(x, self.w)

m = Model()

# Inputs to the model
x1 = tf.constant([1., 2., 3., -3., 2.5], shape=[1, 5])
print(m(x1))
""""""
InvalidArgumentError: Exception encountered when calling layer 'model' (type Model).

{{function_node __wrapped__BatchMatMulV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} Matrix size-incompatible: In[0]: [1,5], In[1]: [10,1] [Op:BatchMatMulV2] name: 

Call arguments received by layer 'model' (type Model):
  â€¢ x=tf.Tensor(shape=(1, 5), dtype=float32)
""""""
```


### Relevant log output

_No response_"
61880,org.tensorflow:tensorflow-lite-task-vision:,"implementation 'org.tensorflow:tensorflow-lite-task-vision:0.1.0'

Can I generate this dependency locally?

What instructions should I use to generate in TFlite?"
61879,Invalid `Conv2d` can be executed without compilation,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

An invalid `Conv2d` can be executed without compilation. By contrast, after using `@tf.function(jit_compile=True)`, it will raise error `Negative dimension size caused by subtracting 2 from 1 for '{{node conv2d_6/Conv2D}} ...`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

""""""
Don't use @tf.function(jit_compile=True)
""""""

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()
    self.conv2d = tf.keras.layers.Conv2D(5, 2, activation=tf.nn.relu)

  def call(self, x):
    conv2d = self.conv2d(x)
    return conv2d

# Initializing the model
m = Model()

# Inputs to the model
# This input results in a Conv2D kernel that has the shape (1, 2, 3, 5).
x1 = tf.constant([[[[1., 2., 3.], [4., 5., 6.]]]], shape=[1, 1, 2, 3])

y = m(x1)
print(y.shape)
# (1, 0, 1, 5)

""""""
Using @tf.function(jit_compile=True)
""""""
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()
    self.conv2d = tf.keras.layers.Conv2D(5, 2, activation=tf.nn.relu)
  @tf.function(jit_compile=True)
  def call(self, x):
    conv2d = self.conv2d(x)
    return conv2d
# Initializing the model
m = Model()

# Inputs to the model
# This input results in a Conv2D kernel that has the shape (1, 2, 3, 5).
x1 = tf.constant([[[[1., 2., 3.], [4., 5., 6.]]]], shape=[1, 1, 2, 3])

y = m(x1)
""""""

    ValueError: Exception encountered when calling layer 'conv2d_6' (type Conv2D).
    
    Negative dimension size caused by subtracting 2 from 1 for '{{node conv2d_6/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x, conv2d_6/Conv2D/ReadVariableOp)' with input shapes: [1,1,2,3], [2,2,3,5].
    
    Call arguments received by layer 'conv2d_6' (type Conv2D):
      â€¢ inputs=tf.Tensor(shape=(1, 1, 2, 3), dtype=float32)


Call arguments received by layer 'model_10' (type Model):
  â€¢ x=tf.Tensor(shape=(1, 1, 2, 3), dtype=float32)
""""""
```


### Relevant log output

_No response_"
61876,Cast from int32 to float32 changes the value.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 Here what I see is that the bug actually happens b/c as we are doing the conversion from int32 to float32 the value changes, as there is a rounding error that occurs when casting an integer to a floating-point type as float32 is not the high precision floating point type like float64, and the bug will also remain the same if we change them even to int64 from int32.

### Standalone code to reproduce the issue

```shell
> The code is below 
>> tf.cast(tf.constant(52479845, tf.int64), tf.float32)
<tf.Tensor: shape=(), dtype=float32, numpy=52479844.0>
>> tf.cast(tf.constant(52479843, tf.int64), tf.float32)
<tf.Tensor: shape=(), dtype=float32, numpy=52479843.0>
```


### Relevant log output

```shell
### Relevant log output

_No response_
```
"
61875,Cast from int32 to float32 changes the value.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.12.0 through tf-nightly (v1.12.1-99894-g5bef7ce6955 2.15.0-dev20230915)

### Custom code

Yes

### OS platform and distribution

nvcr.io/nvidia/tensorflow:23.07-tf2-py3 docker image

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Casting some int32 values to float32 causes a change in value.

### Standalone code to reproduce the issue

```shell
>>> tf.cast(tf.constant(52479843, tf.int32), tf.float32)
<tf.Tensor: shape=(), dtype=float32, numpy=52479844.0>
>>> tf.cast(tf.constant(52479845, tf.int32), tf.float32)
<tf.Tensor: shape=(), dtype=float32, numpy=52479844.0>
```
```


### Relevant log output

_No response_"
61873,ERROR: No matching distribution found for tensorflow==2.12.0,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 pip3 install tensorflow==2.12.0
 
 

### Standalone code to reproduce the issue

```shell
ERROR: Could not find a version that satisfies the requirement tensorflow==2.12.0 (from versions: 1.13.1, 1.13.2, 1.14.0, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 
2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 
2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0)
ERROR: No matching distribution found for tensorflow==2.12.0
```


### Relevant log output

_No response_"
61872,Fails to build on AARCH64,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

16.0.6

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

/tensorflow/lite/kernels/rng_util.h:26:12: error: use of undeclared identifier 'uint32_t'

### Standalone code to reproduce the issue

```shell
bazel test --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 --test_lang_filters=py --flaky_test_attempts=3 --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --action_env=PYTHON_BIN_PATH=/usr/local/bin/python3 --build_tag_filters=-no_oss,-oss_excluded,-oss_serial,-v1only,-benchmark-test,-no_aarch64,-gpu,-tpu,-no_oss_py39,-no_oss_py310 --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-v1only,-benchmark-test,-no_aarch64,-gpu,-tpu,-no_oss_py39,-no_oss_py310 --local_test_jobs=64 --build_tests_only -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/compiler/xrt/... -//tensorflow/core/tpu/... -//tensorflow/go/... -//tensorflow/java/... -//tensorflow/python/integration_testing/... -//tensorflow/tools/toolchains/... -//tensorflow/lite/... -//tensorflow/core/kernels/image:resize_bicubic_op_test -//tensorflow/core/grappler/optimizers:auto_mixed_precision_test_cpu -//tensorflow/core/grappler/optimizers:remapper_test_cpu
```


### Relevant log output

```shell
ERROR: /workspace/tensorflow/lite/kernels/BUILD:497:11: Compiling tensorflow/lite/kernels/rng_util.cc failed: (Exit 1): clang failed: error executing command (from target //tensorflow/lite/kernels:rng_util) 
  (cd /tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \
  exec env - \
    CACHEBUSTER=20220325 \
    CLANG_COMPILER_PATH=/usr/lib/llvm-16/bin/clang \
    LD_LIBRARY_PATH='' \
    PATH=/home/ubuntu/actions-runner/_work/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-arm64/bin:/home/ubuntu/actions-runner/_work/tensorflow/tensorflow/bazel-ci_build-cache/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
    *** \
    PYTHON_BIN_PATH=/usr/local/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
  /usr/lib/llvm-16/bin/clang -MD -MF bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/_objs/rng_util/rng_util.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/_objs/rng_util/rng_util.pic.o' '-DBAZEL_CURRENT_REPOSITORY=""""' -iquote . -iquote bazel-out/aarch64-opt/bin -fmerge-all-constants -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-gnu-offsetof-extensions -Wno-gnu-offsetof-extensions '-mtune=generic' '-march=armv8-a' -O3 -flax-vector-conversions '-std=c++17' -DFARMHASH_NO_CXX_STRING -DEIGEN_ALLOW_UNALIGNED_SCALARS -Wno-sign-compare -O3 -fno-exceptions '--sysroot=/dt10' -c tensorflow/lite/kernels/rng_util.cc -o bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/_objs/rng_util/rng_util.pic.o)
# Configuration: 91cacbf6409fd17883ece1a0e16168e33815822fe5d36a44e64642fa9b0e32ee
# Execution platform: @local_execution_config_platform//:platform
In file included from tensorflow/lite/kernels/rng_util.cc:15:
./tensorflow/lite/kernels/rng_util.h:26:12: error: use of undeclared identifier 'uint32_t'
std::array<uint32_t, 2> Threefry2x32(uint32_t key_0, uint32_t key_1,
           ^
./tensorflow/lite/kernels/rng_util.h:26:38: error: unknown type name 'uint32_t'
std::array<uint32_t, 2> Threefry2x32(uint32_t key_0, uint32_t key_1,
                                     ^
./tensorflow/lite/kernels/rng_util.h:26:54: error: unknown type name 'uint32_t'
std::array<uint32_t, 2> Threefry2x32(uint32_t key_0, uint32_t key_1,
                                                     ^
./tensorflow/lite/kernels/rng_util.h:27:49: error: use of undeclared identifier 'uint32_t'
                                     std::array<uint32_t, 2> ctr);
                                                ^
./tensorflow/lite/kernels/rng_util.h:32:12: error: use of undeclared identifier 'uint32_t'
std::array<uint32_t, 4> Philox4x32(uint32_t key_0, uint32_t key_1,
           ^
./tensorflow/lite/kernels/rng_util.h:32:36: error: unknown type name 'uint32_t'
std::array<uint32_t, 4> Philox4x32(uint32_t key_0, uint32_t key_1,
                                   ^
./tensorflow/lite/kernels/rng_util.h:32:52: error: unknown type name 'uint32_t'
std::array<uint32_t, 4> Philox4x32(uint32_t key_0, uint32_t key_1,
                                                   ^
./tensorflow/lite/kernels/rng_util.h:33:47: error: use of undeclared identifier 'uint32_t'
                                   std::array<uint32_t, 4> ctr);
                                              ^
8 errors generated.
```
"
61870,Significant performance drop in `ModifyGraphWithDelegate() `on Android 12 Using Hexagon Delegate,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

Tensorflow 2.7 & 2.11

### Custom code

No

### OS platform and distribution

Linux Ubuntu 18.04

### Mobile device

Android 12

### Python version

3.9

### Bazel version

3.7.2

### GCC/compiler version

7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

#### Significant performance drop

Using the same `libhexagon_nn_skel_v66.so`, `libhexagon_interface.so` in `v1.20.0.1` from the [official website](https://www.tensorflow.org/lite/android/delegates/hexagon).

`ModifyGraphWithDelegate()` of **hexagon delegate on Android 12 is 33x slower than the same function on Android 10**.

#### Possible bugs
We have found that the function `BuildGraph()` in the file of the path below is the source of the irregular execution time but don't know why. 

```
tensorflow_source_code_root_dir\tensorflow\lite\delegates\hexagon\hexagon_delegate_kernel.cc
```

#### The SoC

We run the code on Snapdragon XR2, but we think it's easy to reproduce it on other SoC with supported Hexagon.

### Standalone code to reproduce the issue

You can download some tflite model to reproduce this issue. for example, the [SSD-MobileNet from TFHub](https://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/default/1), can reproduce this performance drop.

```shell
std::unique_ptr<tflite::Interpreter> interpreter;
tflite::ops::builtin::BuiltinOpResolver resolver;

std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""/vendor/etc/ssd_mobilenet_v1_1_default_1.tflite"");
InterpreterBuilder builder(*model, resolver);
builder(&interperter);

TfLiteHexagonInitWithPath(""/vendor/lib/rfsa/adsp"");
TfLiteHexagonDelegateOptions params = {0};
auto *delegate_ptr = TfLiteHexagonDelegateCreate(&params);
Interpreter::TfLiteDelegatePtr delegate(delegate_ptr, [](TfLiteDelegate *delegate){TfLiteHexagonDelegateDelete(delegate)});

uint64_t start_time = std::chrono::duration_cast<std::chrono::microseconds>(std::chrono::steady_clock::now().time_since_epoch()).count();

if(interperter->ModifyGraphWithDelegate(delegate.get()) != kTfLiteOk){
        printf(""Fail to delegate with Hexagon\n"");
}else{
        printf(""Success to delegate with Hexagon\n"");
}

uint64_t end_time = std::chrono::duration_cast<std::chrono::microseconds>(std::chrono::steady_clock::now().time_since_epoch()).count();
uint64_t using_time = end_time > start_time ? end_time - start_time : 0;
printf(""using_time: %u micro seconds = %f seconds\n"", using_time, using_time / 1000000.f);

if(interperter->AllocateTensors() != kTfLiteOk){
        printf(""Fail to allocate\n"");
}
```


### Relevant log output

```shell
# On Android12->
Success to delegate with Hexagon
using_time: 4964460 micro seconds = 4.964 seconds

# On Android10->
Success to delegate with Hexagon
using_time: 150438 micro seconds = 0.15 seconds
```
"
61869,Wrong element_spec when mapping ragged dataset,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Linux, Rocky 9

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

```python
ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant([[1, 2, 3], [4, 5]]))
ds.element_spec
```

The `element_spec` is a `RaggedTensorSpec`

But after a `map`, like for instance
```
ds.map(tf.square)
````
the `element_spec` is just a `TensorSpec`

### Standalone code to reproduce the issue

```shell
.
```


### Relevant log output

_No response_"
61867,tf-nightly import fails with keras,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.15.0.dev20230914

### Custom code

No

### OS platform and distribution

Arch Linux

### Mobile device

_No response_

### Python version

Both 3.10 and 3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Running `import tensorflow.compat.v2 as tf` causes the following when keras is installed alongside tf-nightly:

```
/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/tensorflow/__init__.py:29: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptool
s or check PEP 632 for potential alternatives                                                                                                                                                  
  import distutils as _distutils                                                                                                                                                               
2023-09-14 14:47:40.395318: I external/local_tsl/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.                                                
2023-09-14 14:47:40.429289: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9511] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has alre
ady been registered                                                                                                                                                                            
2023-09-14 14:47:40.429321: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has alrea
dy been registered
2023-09-14 14:47:40.429377: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has a
lready been registered
2023-09-14 14:47:40.436510: I external/local_tsl/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-09-14 14:47:40.436868: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-14 14:47:42.270861: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/tensorflow/__init__.py"", line 483, in <module>
    importlib.import_module(""keras.optimizers"") 
  File ""/home/gap/.pyenv/versions/3.10.2-debug/lib/python3.10/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/__init__.py"", line 3, in <module>
    from keras import __internal__
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/__internal__/__init__.py"", line 3, in <module>
    from keras.__internal__ import backend
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/__internal__/backend/__init__.py"", line 3, in <module>
    from keras.src.backend import _initialize_variables as initialize_variables
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/__init__.py"", line 21, in <module>
    from keras.src import models
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/models/__init__.py"", line 18, in <module>
    from keras.src.engine.functional import Functional
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 25, in <module>
    from keras.src import backend
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/backend.py"", line 35, in <module>
    from keras.src.engine import keras_tensor
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/engine/keras_tensor.py"", line 19, in <module>
    from keras.src.utils import object_identity
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/utils/__init__.py"", line 20, in <module>
    from keras.src.saving.serialization_lib import deserialize_keras_object
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py"", line 28, in <module>
    from keras.src.saving.legacy.saved_model.utils import in_tf_saved_model_scope
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/saving/legacy/saved_model/utils.py"", line 30, in <module>
    from keras.src.utils.layer_utils import CallFunctionSpec
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/utils/layer_utils.py"", line 26, in <module>
    from keras.src import initializers
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/initializers/__init__.py"", line 23, in <module>
    from keras.src.initializers import initializers_v1
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/initializers/initializers_v1.py"", line 32, in <module>
    keras_export(
TypeError: api_export.__init__() got an unexpected keyword argument 'allow_multiple_exports'
```

### Standalone code to reproduce the issue

```shell
python3 -c 'import tensorflow.compat.v2 as tf'
```


### Relevant log output

_No response_"
61866,StringLookup / tf.lookup resource cleanup on model cleared,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Describe the problem.

Define a simple network with a StringLookup of non-trivial size

Reload the network in a for loop.

Memory goes to infinity.

Describe the current behavior.
Memory goes up every load

Describe the expected behavior.
Memory doesn't go up every load

When using StringLookup: 

![image](https://user-images.githubusercontent.com/4107771/267815955-966e2a02-135a-4e68-a185-c49120e90092.png)

When using Hashing it works:

![image](https://github.com/tensorflow/tensorflow/assets/4107771/2145a1d1-1eaf-4595-808c-8f53a8a6616f)


### Standalone code to reproduce the issue

```shell
import time
import os

import tensorflow as tf
import psutil

def model_fn():
    X = tf.keras.Input(shape=(1,), dtype=tf.string)
    lookup = tf.keras.layers.StringLookup(
        vocabulary=tf.constant([str(x) for x in range(100_000)])
    )(X)
    Y = tf.math.reduce_sum(lookup, axis=1)

    return tf.keras.Model(inputs=[X], outputs=[Y])

model = model_fn()
model.save(""/tmp/test-model"")

loaded_model = None

process = psutil.Process()

def get_current_mem():
    return process.memory_info().rss / 1e6

def load():
    global loaded_model
    loaded_model = tf.saved_model.load(""/tmp/test-model"")

print('==========================================================')
print(""starting process..."")

for i in range(100_000):
    start_mem = get_current_mem()
    start = time.time()

    print(f""i={i} loading..."", end='')
    load()

    curr_mem = get_current_mem()
    end = time.time()
    print(f""done (mem_usage={curr_mem - start_mem}mb took={int(end - start)}s)"")
    time.sleep(0.25)
```


### Relevant log output

_No response_"
61865,Gather out-of-range read will return different value with `jit_compile`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the index is out of range, `gather` will return different values with and without `jit_compile=True`.

The return values are expected to be consistent. Another solution is to throw errors in both cases.

### Standalone code to reproduce the issue

```shell
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(jit_compile=True)
  def call(self, input):
    x = tf.gather(input, indices=[5, 8, 7, 16, 256, 123], axis=0) # 256 is out of range
    return x

m = Model()

input_shape = [256]
x1 = tf.ones(input_shape)

# Call model
y = m(x1)
print(y)
# tf.Tensor([1. 1. 1. 1. 1. 1.], shape=(6,), dtype=float32), with jit_compile=True
# tf.Tensor([1. 1. 1. 1. 0. 1.], shape=(6,), dtype=float32), without jit_compile=True
```


### Relevant log output

_No response_"
61864,Body function of `while_loop` cannot access the external variable after compilation ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The body function of `while_loop` cannot access the external variable after compilation. It will raise the error `UnboundLocalError: local variable 'x' referenced before assignment`

However, if I run the model without the `@tf.function(jit_compile=True)`, the model can be executed without any error.

### Standalone code to reproduce the issue

```shell
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(jit_compile=True) # Comment this line, it will succeed
  def call(self, x):

    def cond(i, _):
      return i < 10

    def body(i, y):
      y = tf.math.add(y, 2.0)
      x = tf.math.multiply(x, 2.0)
      return [tf.math.subtract(i, 1), y + x]

    i = tf.constant(10)
    y = tf.constant(1.0)
    _, final_y = tf.while_loop(cond, body, [i, y], shape_invariants=[i.shape, y.shape])
    return final_y

m = Model()
input_shape = [1,2]
x = tf.constant([4.,5.], shape=input_shape)

y = m(x)
```


### Relevant log output

```shell
UnboundLocalError: Exception encountered when calling layer 'model_28' (type Model).

in user code:

    File ""<ipython-input-31-1eb50a9c2c75>"", line 16, in body  *
        x = tf.math.multiply(x, 2.0)

    UnboundLocalError: local variable 'x' referenced before assignment


Call arguments received by layer 'model_28' (type Model):
  â€¢ x=tf.Tensor(shape=(1, 2), dtype=float32)
```
"
61863,Compiled Model throws `pred must not be a Python bool`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have a `tf.cond` in the model with a constant `pred = True`. After `tf.function(jit_compile=True)`, it raises the error `TypeError: ('pred must not be a Python bool', True)`.

By contrast, there isn't any error if the model is executed without the optimization.

### Standalone code to reproduce the issue

```shell
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(jit_compile=True) # Comment this line, it will succeed
  def call(self, x):
    pred = True
    x1 = x
    y = tf.cond(pred, lambda: x1, lambda: x1)
    return y

m = Model()
input_shape = [4]
x = tf.constant([4.,5.,6.,7.], shape=input_shape)
y = m(x)
```


### Relevant log output

```shell
TypeError: Exception encountered when calling layer 'model_11' (type Model).

in user code:

    File ""<ipython-input-14-5a3ae524deb3>"", line 10, in call  *
        y = tf.cond(pred, lambda: x1, lambda: x1)

    TypeError: ('pred must not be a Python bool', True)


Call arguments received by layer 'model_11' (type Model):
  â€¢ x=tf.Tensor(shape=(4,), dtype=float32)
```
"
61862,Compiled x.shape throws Tuple out-of-range error,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.13, 2.15.0-dev20230913

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When a tensor has shape `(2,)`, `x.shape` returns `2`, and throws an error when I try to get `x.shape[0]`. Without `@tf.function(jit_compile=True)` it works well.

### Standalone code to reproduce the issue

```shell
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(jit_compile=True)
  def call(self, x1):
    def select_2_or_one(x):
      print(x.shape)
      if int(x.shape[0]) == 2:
        return x
      else:
        return tf.stack([x,x])
    return tf.cond(tf.less(x1[0], x1[1]),
        lambda: select_2_or_one(x1),
        lambda: select_2_or_one(x1[0]))

m = Model()

input_shape = [2,]
x1 = tf.constant([2.,3.], shape=input_shape)

y = m(x1)
print(y)
```


### Relevant log output

```shell
if int(x.shape[0]) == 2:

    IndexError: tuple index out of range


Call arguments received by layer 'model_1' (type Model):
  â€¢ x1=tf.Tensor(shape=(2,), dtype=float32)
```
"
61861,Elu activation implementation ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The implementation of the Elu function is wrong the Elu function required the alpha value as is given in this [paper](https://arxiv.org/abs/1511.07289) that 

![image](https://github.com/tensorflow/tensorflow/assets/83540902/31c1f146-a623-43b7-ab0b-7c5632b43536)

but in tensorflow the Elu is implemented  without the alpha.

![image](https://github.com/tensorflow/tensorflow/assets/83540902/8f1a8117-1ef8-47e9-8643-8a033a60b997)




### Standalone code to reproduce the issue

```shell
#I have compared the result with torch because the elu fucntion in torch has implemnted with alpha. and #manually with the numpy and you can see that the tf is not using the alpha. and returning the wrong #result.

import tensorflow as tf
import numpy as np
import torch
print(""tf version --> "",tf.version.VERSION,""\n"")

x = np.array([-3.0])
alpha = 0.5
np_result = np.where(x > 0, x, np.multiply(alpha, np.expm1(x))).astype(x.dtype)
tf_result = tf.nn.elu(tf.constant([-3.0]))
torch_result = torch.nn.functional.elu(torch.Tensor([-3.0]),alpha=alpha)
print(""tf result --> "",tf_result,""\n"")
print(""torch result --> "",torch_result,""\n"")
print(""numpy result --> "",np_result,""\n"")
```


### Relevant log output

```shell
tf version -->  2.13.0 

tf result -->  tf.Tensor([-0.95021296], shape=(1,), dtype=float32) 

torch result -->  tensor([-0.4751]) 

numpy result -->  [-0.47510647]
```
"
61860,Unable to install tflite_runtime on macos M2 processor,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tflite

### Custom code

No

### OS platform and distribution

macos 13.5.1

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

There are no wheel packages for tflite_runtime for macoc with apple M2 silicon.  Need to install tflite_runtime pip package for local development purposes.

### Standalone code to reproduce the issue

```shell
pip reports that there is no package available for my operating system

pip install tflite_runtime
```


### Relevant log output

```shell
pip install tflite_runtime
ERROR: Could not find a version that satisfies the requirement tflite_runtime (from versions: none)
ERROR: No matching distribution found for tflite_runtime
```
"
61859,restoring checkpoint loads weights partially,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am following this [tfa seq2seq tutorial](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt) for building a seq2seq network with LSTMs. I trained my model and got great accuracy. I saved my model with `tf.train.Checkpoint`. Then, I tried to reload my model with `checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))`. 

However, the model gets restored partially.

My encoder weights get restored, however the decoder does not.

How to resolve?

### Standalone code to reproduce the issue

```shell
from typing import Tuple

import tensorflow as tf
import tensorflow_addons as tfa

from tensorflow.keras.preprocessing.text import tokenizer_from_json

import json

import numpy as np

import unicodedata
import re
import os
import io
import time
from collections import Counter

MAX_SEQUENCE_LENGTH = 30

def math_tokenizer(expression):
    regex = r'(\d+|[\+\-\*\/\(\)\^]|[a-zA-Z]+|.)'

    # Use the regex to split the expression into tokens
    _tok = re.findall(regex, expression)

    # Split numbers into individual digits
    ret_tokens = []
    for token in _tok:
        if token.isdigit():
            ret_tokens.extend(list(token))
        else:
            ret_tokens.append(token)

    # Filter out empty strings
    ret_tokens = [token for token in ret_tokens if token.strip()]

    return ret_tokens

def change_variable(expression):
    # Define regular expressions to match different tokens
    tokenstream = math_tokenizer(expression)
    variable = None
    for idx, tok in enumerate(tokenstream):
        if len(tok) == 1 and tok.isalpha():
          variable = tok
          tokenstream[idx] = ""var""
    return ' '.join(tokenstream), variable

def text_cleaning(x):
  modified_text = [None] * len(x)
  tokens = set()
  tok_list = []

  variables = []

  for idx, dx in enumerate(x):
      lhs, rhs = dx.split(""="")
      tokenstream, v = change_variable('='.join([lhs[2:-4], rhs[:-1]]))
      tokens.update(tokenstream)
      tok_list.extend(tokenstream)
      variables.append(v)
      modified_text[idx] = ''.join(tokenstream)

  return modified_text, variables

def generate_train_test_dataset(data, TRAIN_SIZE):
  modified_text, variables = text_cleaning(data)
  inputs = []
  targets = []

  for idx, dx in enumerate(modified_text):
      inp, tgt = dx.split(""="")
      inputs.append(inp)
      targets.append(tgt)

  train_inputs = inputs[:TRAIN_SIZE]
  train_targets = targets[:TRAIN_SIZE]
  train_variables = variables[:TRAIN_SIZE]

  test_inputs = inputs[TRAIN_SIZE:]
  test_targets = targets[TRAIN_SIZE:]
  test_variables = variables[TRAIN_SIZE:]

  return train_inputs, train_targets, train_variables, test_inputs, test_targets, test_variables

  class MyDataset:
    def __init__(self, problem_type='calculus'):
        self.problem_type = 'calculus'
        self.inp_lang_tokenizer = None
        self.targ_lang_tokenizer = None


    def unicode_to_ascii(self, s):
        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')

    def preprocess_sentence(self, w):
        w = w.strip()

        w = 'start ' + w + ' end'
        return w

    def tokenize(self, lang, func):

        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=None, oov_token='<OOV>', analyzer=func)
        lang_tokenizer.fit_on_texts(lang)

        tensor = lang_tokenizer.texts_to_sequences(lang)

        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')

        return tensor, lang_tokenizer

    def load_dataset(self, dataset, func):
        # creating cleaned input, output pairs
        targ_lang, inp_lang = dataset

        targ_lang = [self.preprocess_sentence(w) for w in targ_lang]
        inp_lang = [self.preprocess_sentence(w) for w in inp_lang]

        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang, func)
        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang, func)

        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer


    def call(self, dataset, BUFFER_SIZE, BATCH_SIZE, func):
        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(dataset, func)

        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor))
        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

        return train_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer

class Encoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
    super(Encoder, self).__init__()
    self.batch_sz = batch_sz
    self.enc_units = enc_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)

    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')


  def call(self, x, hidden):
    x = self.embedding(x)
    output, h, c = self.lstm_layer(x, initial_state = hidden)
    return output, h, c

  def initialize_hidden_state(self):
    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]

class Decoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, max_length_input, max_length_output, attention_type='luong'):
    super(Decoder, self).__init__()
    self.batch_sz = batch_sz
    self.dec_units = dec_units
    self.attention_type = attention_type
    self.max_length_output = max_length_output

    # Embedding Layer
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)

    #Final Dense layer on which softmax will be applied
    self.fc = tf.keras.layers.Dense(vocab_size)

    # Define the fundamental cell for decoder recurrent structure
    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)



    # Sampler
    self.sampler = tfa.seq2seq.sampler.TrainingSampler()

    # Create attention mechanism with memory = None
    self.attention_mechanism = self.build_attention_mechanism(self.dec_units,
                                                              None, self.batch_sz*[max_length_input], self.attention_type)

    # Wrap attention mechanism with the fundamental rnn cell of decoder
    self.rnn_cell = self.build_rnn_cell(batch_sz)

    # Define the decoder with respect to fundamental rnn cell
    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)


  def build_rnn_cell(self, batch_sz):
    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell,
                                  self.attention_mechanism, attention_layer_size=self.dec_units)
    return rnn_cell

  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):
    # ------------- #
    # typ: Which sort of attention (Bahdanau, Luong)
    # dec_units: final dimension of attention outputs
    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)
    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)

    if(attention_type=='bahdanau'):
      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)
    else:
      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)

  def build_initial_state(self, batch_sz, encoder_state, Dtype):
    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)
    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)
    return decoder_initial_state


  def call(self, inputs, initial_state):
    x = self.embedding(inputs)
    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[self.max_length_output-1])
    return outputs

def loss_function(real, pred):
  # real shape = (BATCH_SIZE, max_length_output)
  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )
  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
  loss = cross_entropy(y_true=real, y_pred=pred)
  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1
  mask = tf.cast(mask, dtype=loss.dtype)
  loss = mask* loss
  loss = tf.reduce_mean(loss)
  return loss

import json
functions = '8exp^(9e)'

f = open('modified_train.txt', 'r')
modified_text = f.readlines()
f.close()

modified_text = [m[:-1] for m in modified_text]

BUFFER_SIZE = 32000
BATCH_SIZE = 64
# Let's limit the #training examples for faster training
num_examples = 30000

inputs = []
targets = []

N_TRAIN = 800000

for dx in (modified_text):
    try:
      inp, tgt = dx.split(""="")
      inputs.append(inp)
      targets.append(tgt)
    except:
      print(f""Error at: {dx}"")

train_inputs = inputs[:N_TRAIN]
train_targets = targets[:N_TRAIN]

test_inputs = inputs[N_TRAIN:]
test_targets = targets[N_TRAIN:]

data = (train_targets, train_inputs)

print(""Creating the dataset"")
dataset_creator = MyDataset('calculus')

# print(""Training the tokenizer"")
# train_dataset, inp_lang, targ_lang = dataset_creator.call(data, BUFFER_SIZE, BATCH_SIZE, math_tokenizer)

f = open('./inp_lang_tokenizer.json')
inp_json = f.read()
inp_json = json.loads(inp_json)
f.close()

f = open('./targ_lang_tokenizer.json')
targ_json = f.read()
targ_json = json.loads(targ_json)

inp_lang = tokenizer_from_json(inp_json)
targ_lang = tokenizer_from_json(targ_json)


# example_input_batch, example_target_batch = next(iter(train_dataset))
# example_input_batch.shape, example_target_batch.shape
vocab_inp_size = len(inp_lang.word_index)+1
vocab_tar_size = len(targ_lang.word_index)+1
max_length_input = 31
max_length_output = 31

embedding_dim = 128
units = 256
steps_per_epoch = num_examples//BATCH_SIZE

train_dataset, inp_lang, targ_lang = dataset_creator.call(data, BUFFER_SIZE, BATCH_SIZE, math_tokenizer)

## Test Encoder Stack
example_input_batch, example_target_batch = next(iter(train_dataset))
print(""Creating encoder"")
encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)
sample_hidden = encoder.initialize_hidden_state()
sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)

sample_hidden = encoder.initialize_hidden_state()

# Test decoder stack

print(""Creating decoder"")
# vocab_size, embedding_dim, dec_units, batch_sz, max_length_input, max_length_output, attention_type='luong'
decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, max_length_input, max_length_output, 'luong')
sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))
decoder.attention_mechanism.setup_memory(sample_output)
initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)

print(""Creating the optimizer"")
optimizer = tf.keras.optimizers.Adam()

checkpoint_enc_dir = './training_checkpoints/encoder'
checkpoint_enc_prefix = os.path.join(checkpoint_enc_dir, ""ckpt"")

checkpoint_dec_dir = './training_checkpoints/decoder'
checkpoint_dec_prefix = os.path.join(checkpoint_dec_dir, ""ckpt"")

checkpoint_enc = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder)
checkpoint_dec = tf.train.Checkpoint(optimizer=optimizer,
                                 decoder=decoder)

# restoring the latest checkpoint in checkpoint_dir
checkpoint_enc.restore(tf.train.latest_checkpoint(checkpoint_enc_dir))
checkpoint_dec.restore(tf.train.latest_checkpoint(checkpoint_dec_dir))

print(decoder.embedding.variables)
```


### Relevant log output

```shell
[]
```
"
61858,ImportError : cannot import name 'context from 'tensorflow.python.eager',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
[~\AppData\Local\Temp\ipykernel_23420\1827115418.py](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/go4av/OneDrive/Desktop/Dissertation/Code/2021fc04746/notebook/~/AppData/Local/Temp/ipykernel_23420/1827115418.py) in <module>
----> 1 from tensorflow import keras
      2 from tensorflow.keras.models import Sequential
      3 from tensorflow.keras.layers import Input, Dense, Activation, Dropout
      4 from tensorflow.keras.optimizers import Adam
      5 from keras import layers

[c:\Program](file:///C:/Program) Files\Python310\lib\site-packages\tensorflow\__init__.py in <module>
     36 import typing as _typing
     37 
---> 38 from tensorflow.python.tools import module_util as _module_util
     39 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 

[c:\Program](file:///C:/Program) Files\Python310\lib\site-packages\tensorflow\python\__init__.py in <module>
     35 
     36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
---> 37 from tensorflow.python.eager import context
     38 
     39 # pylint: enable=wildcard-import

ImportError: cannot import name 'context' from 'tensorflow.python.eager'

### Standalone code to reproduce the issue

```shell
from tensorflow import keras 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Activation, Dropout
from tensorflow.keras.optimizers import Adam
from keras import layers
from keras import regularizers
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
[~\AppData\Local\Temp\ipykernel_23420\1827115418.py](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/go4av/OneDrive/Desktop/Dissertation/Code/2021fc04746/notebook/~/AppData/Local/Temp/ipykernel_23420/1827115418.py) in <module>
----> 1 from tensorflow import keras
      2 from tensorflow.keras.models import Sequential
      3 from tensorflow.keras.layers import Input, Dense, Activation, Dropout
      4 from tensorflow.keras.optimizers import Adam
      5 from keras import layers

[c:\Program](file:///C:/Program) Files\Python310\lib\site-packages\tensorflow\__init__.py in <module>
     36 import typing as _typing
     37 
---> 38 from tensorflow.python.tools import module_util as _module_util
     39 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 

[c:\Program](file:///C:/Program) Files\Python310\lib\site-packages\tensorflow\python\__init__.py in <module>
     35 
     36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
---> 37 from tensorflow.python.eager import context
     38 
     39 # pylint: enable=wildcard-import

ImportError: cannot import name 'context' from 'tensorflow.python.eager'
```
"
61857,Add support for `ios-arm64-simulator` to `TensorFlowLiteSelectTfOps` pod,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

macOS 13.5.1

### Mobile device

iOS 16.4

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The current pod for `TensorFlowLiteC` includes binaries built for the iOS simulator. However, the pod for `TensorFlowLiteSelectTfOps` does not include any simulator binaries.

Please consider building `TensorFlowLiteSelectTfOps` for the sim architecture as well.

This would allow us to run some of our unit tests on the simulator. In the current behavior, our app does not build for the sim since the included framework does not include any sim compatible binaries. 

### Standalone code to reproduce the issue

```shell
1. Unpack the `TensorFlowLiteSelectTfOps` framework for release 2.13
2. Observe that there is no included binary for the sim architecture
```


### Relevant log output

_No response_"
61856,XLA compiled `tf.reshape` throws ValueError: Shape must be rank 1 but is rank 0,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.13, 2.15.0-dev20230913

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The following model calls `tf.reshape` on an input tensor, and throws ValueError when compiled with XLA. Without XLA compilation, it runs smoothly. 

This issue is observed on TF 2.13 and also nightly. See the [gist](https://colab.research.google.com/gist/dengyinlin/d9368054f16a011f9d1fed3cede96b95/xla-compiled-tf-reshape-throws-valueerror-shape-must-be-rank-1-but-is-rank-0.ipynb) for more detail.


### Standalone code to reproduce the issue

```python
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(jit_compile=True)
  def call(self, x1):
    x2 = tf.reshape(x1, (2, 2))
    return tf.reshape(x2, (4))

m = Model()

input_shape = (4)
x1 = tf.constant([4.,5.,6.,7.], shape=input_shape)

y = m(x1)
print(y)
```


### Relevant log output

```shell
ValueError: Shape must be rank 1 but is rank 0 for '{{node Reshape_1}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](Reshape, Reshape_1/shape)' with input shapes: [2,2], [].
```
"
61854,TFLite - Cannot execute with NNAPI on Samsung Galaxy Tab S9 (Snapdragon 8 Gen 2). On delegate CPU available,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.12.0

### Custom code

No

### OS platform and distribution

Android 13

### Mobile device

Samsung Galaxy Tab S9

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

clang 14

### CUDA/cuDNN version

_No response_

### GPU model and memory

Snapdragon 8 Gen 2

### Current behavior?

I have a C++/Kotlin Android application which make inference using Tensorflow Lite (TFLite library 2.12.0 get using conan center).
I use NNAPI delegate and it work great on Google Pixel tab and Samsung Galaxy Tab S8.

I try my application on the new tablet, Samsung Galaxy Tab S8 which have a processor 'snapdragon 8 gen 2', and this time, the inference work, BUT it use delegate for CPU and its much slower than S8...

Looking at the problem, when I call the function to build the interperter, I have this message: `INFO: Created TensorFlow Lite delegate for NNAPI.`, so everything since ok until that.

But, when I call `AllocateTensors()` on the interpreter, I have this messages:
```
Access denied finding property ""ro.mediatek.platform""
Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
```

Is there something special to do to used NNAPI on GPU/TPU in that case?

(You can see the full output of the build of interpreter + call to AllocateTensors() below)

### Standalone code to reproduce the issue

```shell
// Load the model
std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(filename);

// Build the interpreter
tflite::ops::builtin::BuiltinOpResolver resolver;
std::unique_ptr<tflite::Interpreter> interpreter;
tflite::InterpreterBuilder(*model, resolver)(&interpreter);

tflite::StatefulNnApiDelegate::Options options;
tflite::StatefulNnApiDelegate delegate(options);
interpreter->ModifyGraphWithDelegate(&delegate);

// Resize input tensors -> Here the delegate CPU will be used
interpreter->AllocateTensors();
```
```


### Relevant log output

```shell
12329-12329 Manager         I  findAvailableDevices
10477-10477 tflite          I  Created TensorFlow Lite delegate for NNAPI.
10477-10584 ....C++-Error   I  INFO: Created TensorFlow Lite delegate for NNAPI.
10477-10477 libc            W  Access denied finding property ""ro.mediatek.platform""
10477-10477 tflite          I  Created TensorFlow Lite XNNPACK delegate for CPU.
10477-10584 ....C++-Error   I  INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
```
```
"
61853,Using TFLite GPU in Android gives warning about duplicate namespaces,"**Standalone code to reproduce the issue**
See: https://github.com/bvschaik/tflite-android-warning

Basically:

1. Create an empty Android project
2. Add tflite gpu dependency to build.gradle: `implementation(""com.google.android.gms:play-services-tflite-gpu:16.2.0"")`
3. Compile app: `./gradlew assembleDebug`
4. Note the warning emitted: 

> [org.tensorflow:tensorflow-lite-api:2.12.0] ~/.gradle/caches/transforms-3/2eb840e608fe786447e3ad08d13f4541/transformed/tensorflow-lite-api-2.12.0/AndroidManifest.xml Warning:
> Namespace 'org.tensorflow.lite' is used in multiple modules and/or libraries: org.tensorflow:tensorflow-lite-api:2.12.0, org.tensorflow:tensorflow-lite-gpu-api:2.12.0. Please ensure that all modules and libraries have a unique namespace. For more information, See https://developer.android.com/studio/build/configure-app-module#set-namespace

Cause:

Using tflite-gpu adds both the -api and -gpu-api libraries (output from `./gradlew :app:dependencies`):

```
\--- com.google.android.gms:play-services-tflite-gpu:16.2.0
     +--- com.google.android.gms:play-services-base:18.1.0 (*)
     +--- com.google.android.gms:play-services-basement:18.1.0 (*)
     +--- com.google.android.gms:play-services-tasks:18.0.2 (*)
     +--- org.tensorflow:tensorflow-lite-api:2.12.0
     \--- org.tensorflow:tensorflow-lite-gpu-api:2.12.0
```

Both org.tensorflow:tensorflow-lite-api and org.tensorflow:tensorflow-lite-gpu-api specify the same namespace (`org.tensorflow.lite`) in their mainfest:

```xml
<manifest xmlns:android=""http://schemas.android.com/apk/res/android""
    package=""org.tensorflow.lite"">
...
```

Suggested solution:

Change the namespace in the manifest of org.tensorflow:tensorflow-lite-gpu-api to `org.tensorflow.lite.gpu` to make them unique."
61851,Cannot build interactive_graphviz,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

r1.15

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I would like to use interactive_graphviz to check how xla works and why there's a memory increasing while I used it. However, when I tried to build it, I hit some errors that there're undefined references. Here's the detail about the errors:
```
ERROR: /workspace/tensorflow1.15/tensorflow/tensorflow/compiler/xla/tools/BUILD:219:13: Linking tensorflow/compiler/xla/tools/interactive_graphviz failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/compiler/xla/tools/interactive_graphviz-2.params
bazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Scompiler_Sxla_Stools_Cinteractive_Ugraphviz___Utensorflow/libtensorflow_framework.so.1: error: undefined reference to 'tensorflow::ProtoDebugString[abi:cxx11](tensorflow::DeviceAttributes const&)'
bazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Scompiler_Sxla_Stools_Cinteractive_Ugraphviz___Utensorflow/libtensorflow_framework.so.1: error: undefined reference to 'tensorflow::ProtoShortDebugString[abi:cxx11](tensorflow::ConfigProto const&)'
bazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Scompiler_Sxla_Stools_Cinteractive_Ugraphviz___Utensorflow/libtensorflow_framework.so.1: error: undefined reference to 'stream_executor::dnn::AlgorithmProto::AlgorithmProto()'
bazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Scompiler_Sxla_Stools_Cinteractive_Ugraphviz___Utensorflow/libtensorflow_framework.so.1: error: undefined reference to 'stream_executor::dnn::AlgorithmProto::InternalSwap(stream_executor::dnn::AlgorithmProto*)'
bazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Scompiler_Sxla_Stools_Cinteractive_Ugraphviz___Utensorflow/libtensorflow_framework.so.1: error: undefined reference to 'stream_executor::dnn::AlgorithmProto::~AlgorithmProto()'
bazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Scompiler_Sxla_Stools_Cinteractive_Ugraphviz___Utensorflow/libtensorflow_framework.so.1: error: undefined reference to 'stream_executor::dnn::AlgorithmProto::AlgorithmProto(stream_executor::dnn::AlgorithmProto const&)'
bazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Scompiler_Sxla_Stools_Cinteractive_Ugraphviz___Utensorflow/libtensorflow_framework.so.1: error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::CopyFrom(stream_executor::dnn::TensorDescriptorProto const&)'
bazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Scompiler_Sxla_Stools_Cinteractive_Ugraphviz___Utensorflow/libtensorflow_framework.so.1: error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::TensorDescriptorProto(stream_executor::dnn::TensorDescriptorProto const&)'
bazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Scompiler_Sxla_Stools_Cinteractive_Ugraphviz___Utensorflow/libtensorflow_framework.so.1: error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::TensorDescriptorProto()'
bazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Scompiler_Sxla_Stools_Cinteractive_Ugraphviz___Utensorflow/libtensorflow_framework.so.1: error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::clear_layout_oneof()'
bazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Scompiler_Sxla_Stools_Cinteractive_Ugraphviz___Utensorflow/libtensorflow_framework.so.1: error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::~TensorDescriptorProto()'
bazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Scompiler_Sxla_Stools_Cinteractive_Ugraphviz___Utensorflow/libtensorflow_framework.so.1: error: undefined reference to 'stream_executor::dnn::TensorDescriptorProto::InternalSwap(stream_executor::dnn::TensorDescriptorProto*)'
bazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Scompiler_Sxla_Stools_Cinteractive_Ugraphviz___Utensorflow/libtensorflow_framework.so.1: error: undefined reference to 'stream_executor::dnn::ConvolutionDescriptorProto::ConvolutionDescriptorProto()'
bazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Scompiler_Sxla_Stools_Cinteractive_Ugraphviz___Utensorflow/libtensorflow_framework.so.1: error: undefined reference to 'stream_executor::dnn::ConvolutionDescriptorProto::~ConvolutionDescriptorProto()'
bazel-out/k8-opt/bin/tensorflow/compiler/xla/tools/_objs/interactive_graphviz/interactive_graphviz.o:interactive_graphviz.cc:function xla::tools::(anonymous namespace)::OpenUrl(xla::tools::(anonymous namespace)::Options const&, absl::string_view): error: undefined reference to 'tensorflow::SubProcess::SubProcess(int)'
bazel-out/k8-opt/bin/tensorflow/compiler/xla/tools/_objs/interactive_graphviz/interactive_graphviz.o:interactive_graphviz.cc:function xla::tools::(anonymous namespace)::OpenUrl(xla::tools::(anonymous namespace)::Options const&, absl::string_view): error: undefined reference to 'tensorflow::SubProcess::SetProgram(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)'
bazel-out/k8-opt/bin/tensorflow/compiler/xla/tools/_objs/interactive_graphviz/interactive_graphviz.o:interactive_graphviz.cc:function xla::tools::(anonymous namespace)::OpenUrl(xla::tools::(anonymous namespace)::Options const&, absl::string_view): error: undefined reference to 'tensorflow::SubProcess::Start()'
bazel-out/k8-opt/bin/tensorflow/compiler/xla/tools/_objs/interactive_graphviz/interactive_graphviz.o:interactive_graphviz.cc:function xla::tools::(anonymous namespace)::OpenUrl(xla::tools::(anonymous namespace)::Options const&, absl::string_view): error: undefined reference to 'tensorflow::SubProcess::~SubProcess()'
bazel-out/k8-opt/bin/tensorflow/compiler/xla/tools/_objs/interactive_graphviz/interactive_graphviz.o:interactive_graphviz.cc:function xla::tools::(anonymous namespace)::OpenUrl(xla::tools::(anonymous namespace)::Options const&, absl::string_view): error: undefined reference to 'tensorflow::SubProcess::SubProcess(int)'
bazel-out/k8-opt/bin/tensorflow/stream_executor/host/_objs/host_gpu_executor/host_gpu_executor.o:host_gpu_executor.cc:function stream_executor::host::HostExecutor::CreateDeviceDescription(int): error: undefined reference to 'tensorflow::profile_utils::CpuUtils::GetCycleCounterFrequency()'
collect2: error: ld returned 1 exit status
Target //tensorflow/compiler/xla/tools:interactive_graphviz failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 4.118s, Critical Path: 3.69s
INFO: 3 processes: 3 internal.
FAILED: Build did NOT complete successfully
```


### Standalone code to reproduce the issue

```shell
Just run this compile instruction:

bazel build --experimental_repo_remote_exec tensorflow/compiler/xla/tools:interactive_graphviz
```
```


### Relevant log output

_No response_"
61850,ConverterError: 'tf.FastWordpieceTokenizeWithOffsets' op is neither a custom op nor a flex op,"While doing tflite lite model conversion with input signature as string data type i am getting this error
ConverterError: /usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py:670:0: error: 'tf.FastWordpieceTokenizeWithOffsets' op is neither a custom op nor a flex op
:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py:670:0: note: Error code: ERROR_NEEDS_CUSTOM_OPS
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py:670:0: error: 'tf.TFText>FastWordpieceDetokenize' op is neither a custom op nor a flex op
:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py:670:0: note: Error code: ERROR_NEEDS_CUSTOM_OPS
:0: error: failed while converting: 'main':
Some ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom
Custom ops: FastWordpieceTokenizeWithOffsets, TFText>FastWordpieceDetokenize
Details:
tf.FastWordpieceTokenizeWithOffsets(tensor<?x!tf_type.string>, tensor<241460xui8>) -> (tensor<?x!tf_type.string>, tensor<?xi64>, tensor<?xi64>, tensor<?xi64>, tensor<?xi64>) : {device = ""/device:CPU:0""}
tf.TFText>FastWordpieceDetokenize(tensor<40xi32>, tensor<2xi64>, tensor<339052xui8>) -> (tensor<?x!tf_type.string>) : {device = """"}

kindly please check this below colab link
https://colab.research.google.com/drive/14_ajVH4r4NXN6cDw96XJNBtltoueoGfN?usp=sharing"
61849,Could not find matching concrete function to call loaded from the SavedModel,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.10

### Custom code

Yes

### OS platform and distribution

win10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

How to match data of signature?

### Standalone code to reproduce the issue

```shell
...

model.save(filepath=save_model_dir, save_format='tf', signatures=None)
local_model = tf.keras.models.load_model(filepath=save_model_dir)

y_local_pred = local_model.predict(x_test)
y_model_pred = model.predict(x_test)
print('y_local_pred == y_model_pred:', numpy.allclose(y_local_pred, y_model_pred))

user_inputs = [
    tf.TensorSpec.from_tensor(tf.convert_to_tensor(user_inputs[0]), name='inputs/0'),
    tf.TensorSpec.from_tensor(tf.convert_to_tensor(user_inputs[1]), name='inputs/1'),
    tf.TensorSpec.from_tensor(tf.convert_to_tensor(user_inputs[2]), name='inputs/2'),
]

user_outputs = local_model.user_fn(user_inputs)
```


### Relevant log output

```shell
Could not find matching concrete function to call loaded from the SavedModel. Got:
  Positional arguments (1 total):
    * [<tf.Tensor 'inputs/0:0' shape=(150, 5) dtype=float32>,
 <tf.Tensor 'inputs/1:0' shape=(150, 10) dtype=int32>,
 <tf.Tensor 'inputs/2:0' shape=(150, 3, 5) dtype=int32>]
  Keyword arguments: {}
 Expected these arguments to match one of the following 1 option(s):
Option 1:
  Positional arguments (1 total):
    * (TensorSpec(shape=(None, 5), dtype=tf.float32, name='inputs/0'),
 TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/1'),
 TensorSpec(shape=(None, 3, 5), dtype=tf.int32, name='inputs/2'))
  Keyword arguments: {}
```
"
61848,"tensorflow-macos 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.","### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

MacOS 13

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I can't use pydantic because it needs typing-extensions>=4.6.1 but tensorflow-macos doesn't support typing-extensions>4.6.0

please update tensorflow-macos for newer typing-extensions

pydantic-core 2.6.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.
pydantic 2.3.0 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.

tensorflow-macos 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.

Thanks

### Standalone code to reproduce the issue

```shell
just install the pip packages in MacOS terminal.
```


### Relevant log output

_No response_"
61845,Tensorflow fails to compile on Lubuntu 22.04.3,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

Latest

### Custom code

Yes

### OS platform and distribution

Linux Lubuntu 22.04.3

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.1

### GCC/compiler version

gcc-9

### CUDA/cuDNN version

_No response_

### GPU model and memory

1650 4Gb

### Current behavior?

I have tried to compile tensorflow using Docker following the instructions at 

https://www.tensorflow.org/install/source#linux

I installed the python and clang libraries but did not do the pip install as I assume they are not needed for Docker installation (is this correct?). Also, I checked the Lubuntu installation for these Python libraries (all appear present).

Initially I compiled with devel-gpu. Later tried with devel (cpu only).

In the configuration step the clang library was not visible. So I chose not to use it  as I understand it has problems and gcc should work.

bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package was run. Downloading and compilation started and ran for some time before failing. 

Following are the messages prior to failure:

bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
INFO: Reading 'startup' options from /tensorflow_src/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=87
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /tensorflow_src/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3
INFO: Found applicable config definition build:short_logs in file /tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file /tensorflow_src/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /tensorflow_src/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wno-error=array-parameter --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build options --action_env and --define have changed, discarding analysis cache.
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 43182 targets configured).
INFO: Found 1 target...
ERROR: /tensorflow_src/tensorflow/core/protobuf/BUILD:175:17: Compiling tensorflow/core/protobuf/tensor_bundle.pb.cc [for tool] failed: (Exit 1): gcc failed: error executing command (from target //tensorflow/core/protobuf:for_core_protos_cc_impl) /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 41 arguments skipped)
during GIMPLE pass: store-merging
In file included from external/com_google_protobuf/src/google/protobuf/unknown_field_set.h:53,
                 from external/com_google_protobuf/src/google/protobuf/generated_message_reflection.h:47,
                 from bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/core/protobuf/tensor_bundle.pb.h:28,
                 from bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/core/protobuf/tensor_bundle.pb.cc:4:
external/com_google_protobuf/src/google/protobuf/parse_context.h: In function 'const char* google::protobuf::internal::VarintParse(const char*, T*) [with T = long unsigned int]':
external/com_google_protobuf/src/google/protobuf/parse_context.h:538:32: internal compiler error: Segmentation fault
  538 | PROTOBUF_NODISCARD const char* VarintParse(const char* p, T* out) {
      |                                ^~~~~~~~~~~
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-9/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 673.078s, Critical Path: 52.90s
INFO: 1362 processes: 22 internal, 1340 local.
FAILED: Build did NOT complete successfully









### Standalone code to reproduce the issue

```shell
docker pull tensorflow/tensorflow:devel

docker run -it -w /tensorflow_src -v $PWD:/mnt -e HOST_PERMS=""$(id -u):$(id -g)"" tensorflow/tensorflow:devel bash

./configure

You have bazel 6.1.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 

Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.8/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: 
No CUDA support will be enabled for TensorFlow.

Do you want to use Clang to build TensorFlow? [Y/n]: n
GCC will be used to compile TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished

git pull

bazel build --config=opt --config=v2 //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_"
61840,load_associated_files does not load a txt file,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

latest

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On ""load_associated_files"", it emit longest `does not exist` error.
All the lines below reproduce the same result.

```
populator.load_associated_files([os.path.abspath(""./label_file.txt"")])

populator.load_associated_files([])

populator.load_associated_files(['label_file.txt'])
```

The label_file.txt exist in the same folder as main.py.
<img width=""981"" alt=""ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ 2023-09-12 12 15 38"" src=""https://github.com/tensorflow/tensorflow/assets/52132649/178ef07f-e6a3-446e-856e-865b12a36962"">



Also, what information am I supposed to put in the txt file ?
Is there any documentation for that ?

### Standalone code to reproduce the issue

```shell
# Creates model info.
model_meta = _metadata_fb.ModelMetadataT()
model_meta.name = ""Cup classifier""
model_meta.description = (""Identify a cup"")
model_meta.version = ""v1""
model_meta.author = ""Integro""
model_meta.license = (""Apache License. Version 2.0 ""
                      ""http://www.apache.org/licenses/LICENSE-2.0."")

# Creates input info.
input_meta = _metadata_fb.TensorMetadataT()

# Creates output info.
output_meta = _metadata_fb.TensorMetadataT()

input_meta.name = ""image""
input_meta.description = (
    ""Input image to be classified. The expected image is {0} x {1}, with ""
    ""three channels (red, blue, and green) per pixel. Each value in the ""
    ""tensor is a single byte between 0 and 255."".format(160, 160))
input_meta.content = _metadata_fb.ContentT()
input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()
input_meta.content.contentProperties.colorSpace = (
    _metadata_fb.ColorSpaceType.RGB)
input_meta.content.contentPropertiesType = (
    _metadata_fb.ContentProperties.ImageProperties)
input_normalization = _metadata_fb.ProcessUnitT()
input_normalization.optionsType = (
    _metadata_fb.ProcessUnitOptions.NormalizationOptions)
input_normalization.options = _metadata_fb.NormalizationOptionsT()
input_normalization.options.mean = [127.5]
input_normalization.options.std = [127.5]
input_meta.processUnits = [input_normalization]
input_stats = _metadata_fb.StatsT()
input_stats.max = [255]
input_stats.min = [0]
input_meta.stats = input_stats



# Creates output info.
output_meta = _metadata_fb.TensorMetadataT()
output_meta.name = ""probability""
output_meta.description = ""Probabilities of the 1001 labels respectively.""
output_meta.content = _metadata_fb.ContentT()
output_meta.content.content_properties = _metadata_fb.FeaturePropertiesT()
output_meta.content.contentPropertiesType = (
    _metadata_fb.ContentProperties.FeatureProperties)
output_stats = _metadata_fb.StatsT()
output_stats.max = [1.0]
output_stats.min = [0.0]
output_meta.stats = output_stats
label_file = _metadata_fb.AssociatedFileT()
label_file.name = 'label_file.txt'
label_file.description = ""Labels for objects that the model can recognize.""
label_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS
output_meta.associatedFiles = [label_file]



# Creates subgraph info.
subgraph = _metadata_fb.SubGraphMetadataT()
subgraph.inputTensorMetadata = [input_meta]
subgraph.outputTensorMetadata = [output_meta]
model_meta.subgraphMetadata = [subgraph]

b = flatbuffers.Builder(0)
b.Finish(
    model_meta.Pack(b),
    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)
metadata_buf = b.Output()

populator = _metadata.MetadataPopulator.with_model_file(tflite_model)
populator.load_metadata_buffer(metadata_buf)
populator.load_associated_files([os.path.abspath(""./label_file.txt"")])
populator.populate()
```


### Relevant log output

```shell
.....{q2\xbd\xf6\\\x9c\xbd\x80\xa1\x0c=\xa6\xfa\x1f\xbd\xc2\xdd\x99\xb9:x\xa9\xbb]B\xe4=\xd4\xef\x14\xbdm\xcb\x0c\xbdO8\xfb\xbdk\x8dl=}s\x11=\xb6\xcf\xab=\x07x\x0c=\xf2t\xe5\xbc:\x01\x8a\xb9\xdb+\xa0\xbd\x0f\xf5\xea\xbd\xf8\xaf\x1d\xbd\xe3s\xde\xbcm\x86\x87\xbd\x11\xa3\x18=\x1e\x0bM=\xb6\x00\x17=\x97:\xd0=\x10\xda\xf2;\x89\x1b>=.0\xa4\xbde\xd7\xad\xbd\x88\x00R=\x0f\xe7\x85=_\xda\x9b=\xd2v\xfe<\xa9\xe9\x07=\xba\x91\xaf=\xaa\xad\x98\xbc)\x9c\xbe\xbc""\x17s=0n\xfc\xbdB\x9c\xd1=\x9er\t=R\\\xad=\xac\xc9x=\xe9R\x94\xbdR\xd4\x13=\xb9\xc7D=\x91\x18T=J\x8e\x94=\xe2\xeb\x8a\xbd\xe3\xf8\x00\xbd6\xa2\x9b\xbc\xfa\t\x98=\xf9\x18\xdf<C\x1e\xa7=\xe2\x9a\xa0=]k\x0b\xbd\x86.7\xbc\xbar\xe4;\xcb\x9b\x0c>\xc2#\xd9=\xaaB\x1f\xbc\xa4\x03\xbc\xbd6\xea\xf0\xbd\xca\xae\xa8\xbd(\xb7\xfa\xbd\xff\x92D=\xa66\xd7\xbd\xf7\xea)=`\xf1\xa2=\x92\x120\xbd\x8eb\xa9=\x13\x8e\xfd\xbd\xe0\x9d)\xbd\n\xd1$=\xd7K\x12\xbd\xb6X\xa0<\xaeL\xd1=\x053\xa2=\x01X\xb3\xbd8\x9df\xbd\\\xef&\xbd+k&\xbc\xc8\xa0\x8a=G\x81\xc8\xbd\xeal5=\xd4`\xbf\xbd\xca\ny=T\xb0P\xbd<e\x7f\xbc\xcaU^<\x7f\xc2\xd1;p\xa3\xb8\xbd\x16\xefF\xbd%r\x87\xbd\x81{\xb1=\xf9\x13\xfb\xbc\xc1\xf8\xce;\x06\x8a\x87\xbd\r\x8fK\xbd\xb2\xd4\x14=\x9d\xecL\xbd\x0c\xe6""=F:o\xbd\xd8_\x9c\xbd\x18\xd6:=\xa6\x1b<=\xde\xd3\xa1=fY\xfd\xbcw),=\xec\xa4\x80=\x89\xdf\x9b\xbd\x8c\xf6\x06\xbe\xc9\xc7\xab\xbdO\x00\x93\xbd+\xf8\x98\xbd\x84\xeff\xbb5\xa3\xd2=v4\x19\xbc\xd3\x7f\xe7\xbd\xa3}\x84\xbd\xd6\xb2C\xbd\xda\xc6`\xbd\xf5\xde\xb5\xbdr\xed5<uo\x84<\x03\xd7\xdb\xbd\xa9\x00\x94\xbd\x06(\x88\xbdM\xa4\x82\xbd\x1a\xdb\xc9=\xcf\x9d\xcd\xbc~SG\xbd\x18\xeb~\xbd\xd0Wv<\x01[\x98=\x92\xba\x98= \x83\xd5\xbd\x10\x85b\xbc\x95+\x92=\x8a\x1f\x11\xbb\x18b$=<\xe9\xaf\xbdjfr=\x958\xed\xbd\xd8\xb2\xc2<Q[\x97\xbd\x14\x9e\x9e=\xdd\x19\x18\xbb\x8c\xa0\x81\xbb^0\x1a<\x7fW\x00\xbd\x1c\xc9\xd4=\x83\xa8\x04\xbd\xd7\x99k=-\xe5U=\x16\x83\x9b\xbcf\x90\xb9\xbdJ\x1f\xab=\x82\x00\xe3\xbd@3.=\x1e\x98\xac\xbdX\x8e\x1c\xbd\x91\x83\xca=\xed\xedJ\xbd@\x96\xd8=\x87\xb6\x0e=?q\xb4\xbd\xc4\xe9\xac=q\xe8\xbd\xbd\x7fe\xed<:\xb4\x1a=G\x9a\xc9\xbd\xf0\xc4\xc2=NN\xe8\xbd\xfdb\xf7<\xacs\xb7\xbd\xa6\xd9\xf1\xbcg\xff\x0b<\xdc\x8ea=u>\x98\xbb\xc2f\x86\xbd\xaf\xd8I\xbd|\xc4R=Pk\x04=\xde\xbfL=\xb8&\xa8\xbd:\xe0\xf0\xbdKo\xc9\xbd\xac\x1e-=\xfe\x0b\xa5=2\x02.\xbdk\xac\x8c\xbd\xf8\n;<w\xca)<\xc8\xae\xd9\xbdj\xd9\x9d\xbd\xd4.B\xbd\x80]\x02\xbd\x9a\xe1\xa3\xbdGLe\xba\xac\xf3N\xbd0\x81\x12=\xc0\xa0\xa5\xbd\x0c-\xf3<\xeb\xca\xda\xbd>\x0bE<}\xcf\x84=\xb1\xf9\xac=\xf5\x87X\xbdP""_\xbc\x8c?\xf4<\xb7\x94\x1d=\xd2\xdbG\xbb\xe5?<={Y\xe3=\xbf?\x83\xbd\xfe\xe8\xbb=*\xc2\x9f\xbd\xbd\xb3\xd0=\x18j\xf9=\x9b\x8d\xd0\xbc\x08\xb4^<\xa74*\xbc\xe7\x12\xea\xbc\x81\x88Y=o\r\x7f\xbdP\x8f\x8d<YH\xa8\xbba\x0c\xf9\xbc_\x87\xac\xbd7<_=\x05\xe53=\x01\xabH<%\xfe\x9f<%yP<w\xd3\x83\xbd\x9aL\xbe<&\xd3\xb9=\x93\x94\xbe=\x85\x11o\xbd\x11F\xc1<""\x8c\xa2=\xb5\x13\xb9\xbb\xae\xe4\xef\xbd\xb1y =\x92\xe5\xc5=\x10\xcb\xf8\xbd\xce\x8b\xd0\xbd\\\xceX=$\xb5\xe7<v\x10\x9f\xbd\xd3^\xbf=n\xff\x9f<\x83\xcb\x85\xbd\x14\x7f\x94=\xfb\xda\x95=\xa07q;\x8f\xe9\xb7\xbd\xdc16\xbc\xed_\xb3\xbd\xdf\xf1E\xbd_\xa1\x02\xbe`B\xb5\xbd\x1f\x15\x80=\xc9\xba\x9b<6\xa7\x83\xbd\x8c\x11\xd2\xbd\xe2\xaa\x94\xbc\xafHO=D\xeb\xcb\xbd\x19\xf6\x12\xbb9\xbf\x98=e\xa9\xb4\xbd\xde\x89\xa0\xbc\n\xe6\xcb<\xa5\x99\x89:\xc1z\xd6=\xb7h\xc0\xbde\xde\x13=Fo\xf3\xbd\xc2\xa5\xaf\xbd\x8b\x1bw=7\xf0\xac\xbd\x0e\x9a\x9d<4\x00\x9e\xbb\x8d?\xd1\xbd\x84\xf0w=]Y\'\xbd\x80Dz=\xe0""\xc2=bG?\xbc_7\xc3=\x83=\xc5=;\x11\xea\xbd\x84|\xbc=\xdc\xcf""<\x02\x01\xa8\xbc}\x96\xa0\xbd\xe5\xf9\xd7\xbd\xdaS\xa8=y\xd1\x96<\xe82U\xbdS\xbb\x9f\xbdr\xf7\xca=\x87\xb2\xa7\xbc\xde\x9c\xfa\xbd\xb0\x12\x01=\xef\\\xa3\xbc\x00.\xb4=\xdf\xfdS=\x87\xc3h=\xae \xa9\xbc\x08\xd8\xc9\xbd\xd2P\xcf\xbc\x991\xa4=\xa1s*\xbd\x07\x16\xee;\xb4\xbeO\xbd\x8b\x95\xfd<\xe8H\xb7<\xdd\xe4\xd7\xbd\x87\xefo=\x9f\xc8\xce=\x04A\xa6\xbd\x88\x95q=\xcd\xf9\xd7\xbdZ\xc4\xb3\xbd\xcap~<\xbd\xa8\x86\xbd\xf5\xf2\x83=\xac3^=\x1c\xc6%\xbdn\xd6\x9b\xbd|\xa5\xda\xbd\x12\xec\x98=\xd8\xd2q=H\x1f\x01=\xe7\xcf\xec<\xb6G\xd5\xbc\xf3>w=\x10\x05\xff\xbd\x92\xa2E;\xd4\x8fm\xbd\x07\xce\x1c=\xc3O:\xbd%\xca=\xbcKi\x8d\xbd;\xef,=\xee\x1d\x91=\xca=Z\xbd\x166w\xbc\x1a<\x83=\xbeG\x9e\xbd\xbcw\x04=\xf3$\x96\xbd\xdf\xd5\xa6<\xd8j\xd9=e\xe7\x9b\xbd3s\x81\xbd\x04\x15\xb7\xbc\xc8\x9e\xb0\xbd\xe8\xad\x9b<{\xab\x94<\xa1=\x9a<\x81\xfe\xcf\xbda\xbbr\xbc\x95\x9f\x8d=\r\x9e\xba\xbd\x84(&\xbd-L\xad=\x10)^=\xb1\xefQ=u\xde\xb68\xb7\xe2\xab<\x88\x1c\x9a=R\\\x00\xbd\xa2q\x96\xbd\xda\xc3,\xbd\xaf\xc4\xf3\xbd\xed<:=\xd3\xc1\xe8\xbd\x9d\x12\xa3\xbdG$\xb2\xbdC2\x1e\xbc\xb7o\x05\xbd\x10\xe0Q;\x08\x19\xe7\xbd\xd7\xf4\x94\xbdB\x85\x97\xbd\xbf\xf8\xab\xbd\xb9|m=/h\xa5\xbd\xbe7\x15=\xfd\xd8\xa5\xbd\xbb\xbc\xa8\xbd\x8c\xe3\x02\xbdx\x05\xfe<q}\xcd=G\xd8\xcf\xbc\r\x84\xb1\xbd\xea\n\x84=\xbe\xfai=\x80b\t=\xc5i\xa2=\xf5<""<+K~=_\xcb#\xbd\xfa\xe3N\xbd\x82(\xe1<\x19hs\xbd\xd6\xc9\x0f=\xad\\\x1e<\xb5\xdbL=&\r\x1e=I\x9f\xef<\x05\xbc\xeb=\x90\x8e\xa9\xbb\x1c\x88\xca\xbaT(?\xbd\xa0\x8e\xc7\xbd\x99\xa7 \xbd\x0e\xa8\x94=\xa5*\xea<\xd8\xc2g<\xb26\xc8\xbd\x92\xd4\xee\xbcPQ\xeb\xbd\xdf\x00\x14=\xc2s\xa6\xbd\xd6\xf0\x02\xbd~\x15\xc5=\xa4\x90\xce<\x88\x0b\xb8\xbc\xe2C\x8b\xbcV\xf5\xb2=\x97\xcc\xc7\xbd\xe9\xf7P=\xa3\xceL;\x92\xdb\xd8\xbdsn\x89= u\xf2\xbb\xcc\xf4\xd2<\x01\xfe\x96=\x04\x8c\xa8\xbc\xbca\xd3\xbdUg\xa2=\xd5\xda\xeb=\xb0:\xd0\xbbj\x18=\xbd\x00\xaf\x9a:R\xa0<=\xe0Af;\x98r\xbb\xbd\xb5\x0c\x96\xbd\x0c\xcb\x08\xbc6\xa2\xef\xbd$W\xa9\xbd]\x16\x9b\xbd\x16T\x9b\xbc$\xad\xa0<\x03\x83J\xbd\xf7\xac\xbe\xbda\x9cY<l_\xb3\xbdE_\xb6=5Z\x0b\xbd:N\x8a<o\x1e\x9b=\xc16\x87=\xe2[\x1c\xbc\xc0\xb0\xf3\xbcKo\xb3=\xd1\x04.\xbd\xb6c\xc2\xbd\x16w}=\x8ezY\xbd\x93\xf2\xc8\xbd\xa1]\xac\xbd\xd2\xaf_=\xad\xf0z\xbd\xca\x98\x99\xbdG\x8a\xc0\xbd\xbb\xfa\x9c=\x93\x00n=\xa8Q\xc1\xbc\'\xc8\x95\xbd\x97\xc3\x99=~\x98\x17=\x83\xdc\xcf\xbd\xd9\x04\xbd=\x05\xee\x94\xbd\xbcv\x9b\xbc\xf3\x88\xa2=*\xc9\xab\xbdd\x9c\xa2\xbd\x80d\x13\xbcaB\xd6=hgg\xbc\x90\xd0\xce\xbc\x03\x17\xc2=\n\x83\x93\xbd\xccac\xbd$\xcd\x83=P\xdab\xbc\x11\x1e\xb0\xbcmP\xb9\xbd\x10\xeb\xe6\xbc\x83\xa1\x93\xbd8\xdf\xad\xbd\x08\xf5J<\xfbW\xfe<\x1eF\xc2=\xd1\x11\xea;\xa5\x9e\xcc=\x9d[\r=\xc87\xf9\xbd\xf0\xafH\xbc W\xda<<x+<\x88\x86\x08<\xd7\xd5\x97<w\x99\xb5\xbc\xcd""B\xbd\xa1:\xfe\xbd\xd1m\x96\xbd\x12\xe2\xb0\xbd]~7=}\x80V=\xf0\xed\x94\xbdB\xcb\x17=\xe1\xfb\x8c=\xaa_\xba=\xb6)g\xbd\xde/\x9b\xbd\x94\x18\xc8\xbc<\xde\xc1<\x91\x9a\xa9=r\x04x\xbdZ\xfb|\xbd\x04\x15\x96<\x19C\xad=\xc7\xca\xc3\xbd[\x95\xe9\xbd\xd2.T\xbd\xb0_\t\xbd\x8e\xe2\x04=\xfbM\xbe=\x18g\xe3\xbcdf\x8f\xbd\xb6\x8d\xab\xbd&\x8c\xb9\xbd\x98\xc2\xee\xbd\xdd\x9a4=\x0e\xa65<\xfa,\x0f\xbd\x1dX\xe7<\x7f\x0c\xdc=!\x08\xb0\xbd>\xa2\xce\xbc7\xd7\xb9\xbd1K\xbf=\x98d\xcf\xbdt\xa2\xb7=\'\x18\xa9=\x92\xe7\xb7\xbd\xfe\xb7\xcf\xbd\x9bn\x85\xbd\xcd\x0ca=\xa1Cy=\x84\xb5\xe3<\xc7X_<\x8ey%=bK\xbe=\xab\x01\xb6=h\x88\xd4\xbc2\x92I\xbd\xfc\xee<=62\xef<!\x1c\x15\xbc\xf8$+\xbd|\xf6z=\x8b\xc6\x89=z\x8dp\xbd\x8a\xe0/\xbd\xf2g\xcf\xbd\x16\x18\x01\xbe{_\xc3=\x9b\xf7\xaf\xbc\xda\xa3\xbc;\x0bM\xe4\xbdv\xbf\xd9\xbcD:I=\xd4\xcc\xbc\xbd\xc6x\xdc=\xcdG\xa2=\x1e|\xdd\xbd\x0e#\x9c=\xb3\xf0\x8d=->\x83\xbd\xa1\xd2N\xbdVD\x84\xbc\xe2\x1c\xc8=\xa6\xab\xec<\x1a\xd4\x81=\xe0\x99\xdb<\xeb,\xc6\xbd1\xef\xcf=\x1f\x1f\xbf\xbd\xc2\x84\x19<\xd0\x8an=\xdbg\x86=fs\x97=\xe0=\x02=\x99\xe3\n\xbdX\x13\xe1<)\xe1u\xbc\xba\x9f\x03<\x8a\xc8Z\xbd^[\xbd=\xdfx\x9a\xbdR\x18\x9e=\xc7\x8f\x9c=\x98P\x80\xbd\xa65\x07\xbb\xbe|\xa1=\x04{\xc8=\x06\x94\x84\xbd\x86p4\xbc\xa7l9\xbc\x89\x0e\xa9\xbdh\x0f\xb8\xbc\x02\x9a$<\xfb6\x84\xbd\t\xbe\xc1=F\xbe\x14\xbc@\xb2\xb0=\x11s+=\n\xfb[=4\xdfM\xbb\xf1\x8f\x89\xbd\xad\xc1\x9a<3\xa1/\xbdz\xf8\x9a\xbc^\x96\xd9\xbc\x8c{\xa8=\x02\xff\xa3={\x9a\xa4=Z\xc5\x05=:\x81\x0b\xbd\x02\x95\xcd=a\xcc\xfc=A\xf6\xcc\xbdA\xa0\xd7=""\xa4\x9d=\xdd\xee\xcb\xbd\x8d\x0e\xce;Y\xc7\xfb\xbc\x1e\xa0\x8b=|\xa6\xf6\xbdMS\xa3\xbdE7\x10\xbd\xd5\xc0\x80\xbd\xf5\x8a\x8a=\xb7\'\x9d=\xb8\xea\xe1\xbcJF\xc0<p\x99P\xbd\xe0Uz\xbc\xd2\xf1\x80=\x01\x01,\xbdx\x87@\xbd\xef\x8b_=\x90\x99\xbb\xbdP\x81\xc1=~\xf6\xa6=\xa1\xa3\x99=\xd3mM\xbd\xb96\xe2\xbdP\x8e\xbd\xbd\x90fb\xbdH\x13\xcb\xbd\xd3&\xd0<;\xea\xb3<\x02\xc0\xd2\xbd\xef\xe4&<B.\xbf=\xf0\xc6\xaa\xbd\n\x16*=T\xcc\xae=\xcd\xd7<=\x94\xfdv\xbc\xb9g\x90\xbdp\x1e\xc8\xbd\xb3#\xef<\xb0\xf8\x15=\x9a\xf3Q\xba\xa9B\x8f\xbd\xf1j\x9b\xbd\xb1\x8f\xc6\xbd^\x07\x88=\xc6sj\xbd\xed\xf3\xea\xbd\xf0\xaa\xd8\xbd\xd5\x80e\xbd\xb5\xfch<\xc55\x83\xbd\xc9\xc5\xaa=\x06\x8d\x98\xbd\x9d\xa4\xc7;I\xab\xcf<\x02\xdcU\xbd\x83B\xd3=\x86\x18\xa1;\x1f`\x89=b\x16\xa6:l\xf9\xb0\xbdN\x16\\\xbd>|i\xbc>J\xf1\xbd\xd3\xe0\xb8\xbd\x90lz\xbd_\xae\xe2\xbd\xee\x84\xab=\xcc@\x83<\x99\xaf\xa5=\xe0Y\xcf\xbdH\xaf\x1e=\x16\x12\xd2\xbd\xfd\xcc\xcc=\xd17\xa3\xbcG\xf7\xfb=\xd1\xe9\x9b\xbc\xdc\x16\xf6:\xf4\xb4\xb3\xbd\xd4f\x81=\xf6\x98\x9a;\xf2I\x14\xbda\x98D\xbd\x16eS=\x9cB\x92\xbd\xd5\x07m\xbd)j\x1f=\xa0\x1aW=\xb1\xe8_=\x06\x84\x9b8n\xe7H=\x86\x83T\xbd\x85H\xbc\xbd\xdb\xda1<\x81N\x1f\xbd\x9ff\x1b=\x96\xa07=vCB\xbdL\xad\xbd\xbdjc\xce\xbd\xb3))<\x16$\xdd\xbd\xde~\x19\xbd\xd4\xb5\xa0<y\xfd\xda\xbc\xec\xc8\x84=\xf5\x1au\xbb\xdd\xe9\x0c\xbd\x10\xfd\x92=\x02\xe4\x01=z\x1f^\xbd\x8e\xef\xc9\xbd\x99\xf0\x85=\x90\xba\xc9\xbd\t\xa5\xc7\xbd\xc4\xaf\xe1<\xbf\xae\xc5\xbd\xbdu\xcc\xbcUI\x94=\xa0\xf9\xa5=\x93\x91\x02\xbdY\xed\x88<\xd0L\x16=\x07}a=\xc0\xab\xcc\xbc\x96\xbf\t\xbd\xe9]\x82\xbd\x1c6\xab=\x1d\x0e\x81\xbdm\xb1\xda<\x13`\x03<\xb9\x9d\x14=\xed*\xbc<\xd8V\x96=\xcf\xc1\x90\xbc\xf8\x81\x91=\x1bn\x9e=c\xf9\x93\xbc\xcf\x9d\xe3\xbc\x91\x04\x85\xbd\xb0\xbe\xb2=C%\xa2\xbc\x82\x8e\x99\xbd%\xe4\x86\xbd\'\xe6Z=\xfc\x89V\xbbI\xaf\xda<\x82\xec\x17\xbdp\xeer\xbd\n\x8e\x9c\xbd\x10&\x1f=\xfa:\x14\xbd\xea\xbf%\xbd\xdfB\x02\xbes\xeb\xff;\xfb \xc4\xbd\xcf\xcd\xed\xbd\xe7\x96\x83\xbdK0\x81=\xc0\xb0\xcc\xbd\x81\x05\x9d\xbd\xa4\xff\xc7=Lv7\xbdJ\x04\xf9\xbdp\\\xa3=\x0e\xa9 \xbd-\xc3\x9d\xbcy\xbeb=\xc9\x1b\x92=\x01\x97\xbb\xbd\xd6\xba\xb2=\xda\xee\xeb\xbc\xbb\x16\x94<\xdew\x03\xbe\t+8\xbd\x05<\x88<\xc3l\x02=\xad\xac\x91<\xde\x1f6\xbd\xf2U4\xbc\xab\xddx\xbd\x03\xf4\xc1\xbd\xaci\x10\xbd?\x9c\xc0\xbd\xffMh=dM\x14\xbd?\xa6n\xbd]j\x04\xbd\xa5]\x05\xbb\x0cs\xac\xbd\xf6%\xbd\xbd\xd4s\xa9<\xa8\x1f\xbd=\x19\xdb\xac<\x01\xda\xdb\xbd\x0c\xa6\xdd\xbc\xdcz\x85=<\xde}\xbd\r\xda:\xbd\xee\xa3\xcb=\x9bM\xac\xbdz\x8aT\xbcHF\xc3=!}\x7f\xbd\x1d>\xb2\xbd\xe9\xbe\xba=^\x0c\x9b=\xef-M\xbd\x11\x8f\xb9\xbd\xb9>\x8c\xbb8\xbf\x1d=\xd5\xdc!\xbd\x1a\xb0\xae\xbd\xb7E\xe5;.\x8fN\xbd\xaf&\xfc\xbd\xd2}\xfa;u\xa4;\xbc\xb9-\x93\xbd\x0880\xbc]\xba\xc5=[\xbc\x87\xbd8]\xe5\xbdz\xa4\x08\xbc\xe4\x87\x95=\x8f\xf7\xa6=3\xc9y=\x1a&b\xbd\xc4\x88\xd8=t\xde\xbb\xbb_m\xd1<\r\xa5\x9b=\xf8x\xc7=\xc4\xc1\x0c=w\xcf\xe0\xbd9\xc3P\xbb\xf5J\x8c\xbd\'\xc82=:\xe3\xb8=\rp\x7f\xbd<\xf0T\xbd\xdb3\xcd=\xfd\r\xcd=\x9d\x7f\xad=\xea\xc6\xe1\xbd-\x98\xa1\xbd\xde\xc6\xce=\xcc\x8b|\xbd\xd6\xb4\xc3<\x1ax/\xbdV,\xce=\xb9o\x1d\xbd\x1b\xac\x84\xbdtF\xd0;\xe2\x1f\xb0=\xf7\r\xb7\xbcb\xf6\xa4\xbdt/\xd3\xbd\x9f\xb4\xad\xbd\xd9\x06\x1c\xbdt\n\xce=\xa6\xc5\r=}t\xf1<;]\x93\xbd\xbb5\xee\xbd\x14\x8b\x8d=I2\xa9\xbd\xa7\xd4\t\xbc\xb7\n\xe2\xbb\xaca\xc6=\xc9!\xb7=\xef\x9c\xe2\xbc_,\xd2;\x1c\xbe\'<\x9d\xc5\x8b=\xd57\x1d\xbd\x9b\xd2\xbb=1X\xb6\xbd\xcc\xe6Q\xbd]E\xcb=\x87\xab(=g\xdf\xb0\xbd\xcb\x1d\x82\xbd\x0fb\xae\xbb\xb34\xb3=C&`\xbdw+a\xbd\x96m\x19\xbd)s5=\xfdF\xc3=\xf5F\xb5=\xa46\x9f\xbc\xc7\x96\x8b<u9\x05\xbd;\xe5\x88\xbdQN\x86\xbd\xa9zy<\xfa\x80H=\x15PJ=\xd8\xa4\xa1=\xb2\x0f\xa2=vl_\xbd\x93\x82\xbf\xbdrP3=k#\xc6<U]\x81\xbd\x9d\x7f\x99\xbd)\xe8\x7f\xbd\x82Y\xda=`:\x92=D\xf1O\xbd\x89\xf8 \xbd\x81|`\xbb\xd6\xa6\xea;6t\xa8\xbd\x9f.\xc7\xbc\xe0\xbe\xbe<\x90\xcd\x93\xbd2A&=:\xecm\xbbn\xaa\xa7\xbd\x1e\xe9\xfb<\xae=\x1a=g$\xde\xbdX\xa2Z\xbd\xd4\xde\x00\xbd\xe9)\xf2\xbc\xe4!\xdf\xbc\xea\xcc\x8a\xbc\x05\x94\x8c\xbd\xf6P\xcd=\x16\x12\x8f\xbd\xa6\xac\x15=\x8b$\x1c\xbd\xcas}=\xca\x92\xc7=\x1a\xb0\x12\xbd\xf0\xeck\xbd\xf03\x0b\xbd7\x89\x02>\'\xabM\xbd\xc5+r\xbc\xd8\xd7\xa6=\xa6\x0c\xb5=\x95\t\xb0=\xd5\xc1\xdb;\xc3\xbb\x86\xbd\x89\xe6\xde=\x87\xc0\x1e\xbd\x80A\xef=\x87\xfb\xc3\xbd\x06k\n\xbd""R8=\x99\xddK\xbd\r\xbf\xf9\xbda\xc9\xd2=\xca\xfb\x17=\xa5\xac\xdb\xbd\x12\xfd9\xbd2\xb4%=\x8c\x93\x92\xbdX\xb3\xa7\xbd\xf2\xef\x97\xbd\xbf\xad\xf0\xbd\x8f\x96\xda=\xe8\xc8\xf6\xbb\xafY\x11\xbd\x9d\x12\x07>\x9fE\xa2<M\xf0(=\x8a\xb1\xdc\xbb\x7f\xaa\xb6\xbd\xb9p\x8f\xbd\xb8\xe8\xb3\xbdA\x87\xbd=w+\xe4=\x8d\xcc\x1d=\x049\xad\xbd\x9f\xa4\xd8\xbd\xf6W\xf2<\x028H\xbc\x90\xb1\x82=r\xe2>\xbc\xf4\x8c\r\xbd4\xf3\xab\xbdd\x95\xde<\xcb`\n\xbe\\\xb38=\x81\xef\xa5\xbdq\xd2\x82\xbd<\xb2\xb9;\xb3\x82\x1a\xbd]\xad#=\x10\xb3\xbc\xbd7\xee\x84\xbd\xf037=\xfd\xaa[\xbd\x86\x92\xbb=\xc6\xcf==\xb6b;\xbd\xec\x01\xcd<g%\xbd=d\x11\xc2<\xca\xf7\xa7\xbd\xc39\x9b\xbd\xfaI\xaa\xbc:\x1a\xf5\xbc\x84F\x1e\xbd%;\x8a\xbd\xb6\xbb\xa9\xbd\xd3\x83\xb1\xbd\xf8\xf3\xdf=\x03~\xeb\xbc\xe1\xe8\xf1=\xe9k\xc2\xbd\xcaa\x91\xbc\xd1\x81\xe9\xbc\x06\xbaK=!0Y\xbdy\xba#=\xefL\xe9=\xea\xd6\xd0\xbb\xf3\r\x00\xbd\x0f\xc4\xf3<bX\xc2<\x8ai\xb3\xbd\x95&3=\xbb\xfbh=\xb9\xe1\xa3={\x06\x04>J/\xcb=\xb0\x8a\xaf\xbd\xd2\x18M\xbbm\xa4\x1b=\x99\xc8\xe8\xbds\x0b-\xbd\x8c\xed@=)\xe0\x8c\xbd\xef\x17\xed=\xdf4\x84\xbd\xb4\x08\x8b\xbc\'m[;\xc85\xbe\xbd\x82_\xd7<UJ{=\xadf\xd5\xbd\xb4F\xdc\xbdS=\x9d\xbd\xb7x\x96\xbc\n6\xc3\xbd\xf0\x8f2\xbd\x0cM\x97=\x9b5^=\xbbW}\xbdE\xdfM=8\xa5\xa5\xbb\xfd\x8a\xe9=r\x91\xc7=\x9d\\\xdc\xbd\xddV\x1c\xbd\xa7\xc7\xc1\xbd(\xad\xd9\xbc\xb8Y\xba\xbd\x1c\x9a?\xbd\x7f3\x8e\xbd\xb7\x11\x1d<\x03\xc5\xc6=#{\xa7\xbc\x94\xcf\xd1=\xb6\xf9\xc9\xbc\xbd!\xd1\xbd\xba8\xce\xbd\xaa/2=\xffm!=\x01\xc9\xd7=n\x852<\x8fU\xbf\xbc\x8b|\xa3<V}\xc0<\xa9\xbd\x9f=j\xbe&<\xb3$8=\xd6 \xb2\xbd\x98\xda\xf3\xbd\xce\xe7\xd9\xbd\xc3\x1e\xee\xbde%K=0\x9fZ=\xac\x85\xd6=&p\xab\xbd\xac{\x00<t7g\xbd{\x01\xd5=\xc4eI=A\xf7a\xbc\x18\x15F\xbd\x7fT\xff=\x08\xf4\x10=#l\xd7\xbd\\p\x9f\xbdxm\x0c\xbd\x8f\x92\x1e;O\xde$=\x8ev\xfb\xba\xc9|\x9e\xbc\xef\x8a\xdc=\xa3\n\xae=\xb9\xe4o\xbd\xfa9\xab\xbd\xa8\x1f\xd9=\x06\x15t\xbd\x9c.y\xbd\xc6BW=\xa2*u=\xda\x07d<q\xeb-=\xd3\xf2\x92=\xd8i^=e\x83\xd2=\x12\xa2\xbc:\xb6\xaa+\xbd.\xe7\xeb=]\x9f\x85=<\t\xa7\xbdbZ\xa3\xbd\xd2Z\x9a\xbc\x07\x87\xc5=\x84y\xb2\xbd\x97\xcc\xe6\xbc\xfb\xe2\x11\xbd\x1bG\xb4<\xa9\xfc\xb9=\xdam\x95=v\xf0\xdb<\x06\x1e\xb5\xbd\xbf\x87\xbd=\xa0\x1dA=\x8e\xfb\xcc\xbb#\x19\x15=Bb\xb2\xbd\xf3\xfd\xf0\xbd\xe8\xb9\xe7\xbc\xd5\x86\xc0=4|\x18\xbd\x82\xed\xfe\xbc\xc1\n\xf0=&6\xdb\xbc:\xe5E\xbd\xbd!\xd6=,\xa5\x89=\xae\xfcg\xbdk\xafT=\x15\x8d|\xbcb@\x81<Z\x00\xd8\xbd\x17@\x88\xbc\xaa\x84\x9c=\xc5\xa4\xeb\xbd\x08AX=\xa5t\xdc=\x8f\xb2\x88=\xda\xe0\xab\xbd\xefi\xeb\xbdJ\x05\x94\xbdD\xd9\xda\xbd\xebs\xb4\xbc\x19\xb9\xaa\xbdU^\xb2=\xae\x8f\x89<\xada^=\x93\x0e\x10=\x9f\xe8\x8a\xbc\xc4\xd0\x86=GR\xac;\x12\xd4\x9d\xbd\x95\xbf\xf3\xbcgk\x01\xbd\xacM\xc6=0\xd9\xec\xbc\xba\x02\x9a\xbc\xcc\xe5\xa8\xbbg\xe51\xbd\xee\x1e\xf1<\x04\xf6\xa0\xbd\xf7\xe5\xf3\xbd\xd3\xb4T\xbc\x7fI\x84=""\xea\x12=f\x89\xf7<\xd9\x00\xb7<\xe7K\xe7=\xb3\xcb\xb7=\xd1\xef\xc6\xbc.\xaf\x18\xbcb\x97\x01>a\xe0b=\xed\xbe\xa3\xbc\xb2a\xa1=e\x19T=NX\x95\xbd\xd3J\x8b=3\xf4\xd0<\xe6I\x96\xbdg4\x80\xbd\xf4\x82\xf7\xbd\xc1\x9d\xd8\xbc\xa1\x0e{\xbd\x8e\xf4\xfe\xbd\xa3\x16\xf3=3\xbc\x8b\xbc\xeb\xac\xb6=\x8f\xec\x99<q\xa2\xe3<\x14\xccA\xbd\x02\x9a\xc8\xbdf\xf0:=\xfe\x89\xc0\xbbj\xbe\xb0\xbc\x8bC\x80\xbc!\'5=\xdb\x83\xb2=\x1b\xae\\\xbd\xf3\xb2\xd5=\xe6i\xa6\xbdSi\x95=A\xb6G=\x1f\x95\x94\xbdf\xac\xf5;\x1b\x9c\xad\xbd:C8\xbc\xab\x95\x88\xbc\xe6\x83\x87\xbd\x92\x13\xee\xbd\x88\x11\xb8\xbd\xb1]\xcc=\x9doM\xbd\xde\xa2)=>\xc2v=\x8a\x97\x06\xbcj\xc8\xc0<\xfd*\x1f\xbc\xa8\xd2#=5|\x15\xbd\x05\x10\xcd\xbc\xf1\xa8\xb6\xbd\x85\x94z\xbd=\xf0x\xbdj[\x92\xbci\x03(=\xfeI~\xbd\xbdh\xa4=\x0e=\x06>\xdd\xf4\xba\xbd\x10\xa6\x9d=j\xe0\x96\xbd\xcb\x03\x91=C\xb3\xb1\xbdE\x8c \xbb0\x14\xee;\xc7\xf9&\xbb\\\xea\xf6\xbd\xdd\x1e\xe5\xbb\xd2\xd6\xa1\xbc\x96k\x06\xbd\xa2\xc5+:\x845\x1b\xbd{\xe0&\xbd\xb1\x83Q=L\xa5\xb4\xbd\xa2\x82\xcf=\x19\'\xce:\x9e\x96\xc8=\xec\xa7\xb4\xbbW\x0f\xc3=\n\x00\x9f\xbc\xb7-\xbc=hj\x98=\xd1s\xb1=d`s=\xbb\xd6 \xbc\xee\xd9\xa3\xbc\x16\x9d\xa7=\xc2\xfa\x05=2\x1b\xff\xba\xca\xd6g=\x92\xec\x92\xbdX\xb5)\xbbB\xb8\xc7\xbd\xacf\x95=A\xbf\xc2\xbc\xb7\xb5\xb7=\xa5\xa2>\xbd\xb2\x04\x9f=\x06m\xbb<\xe2\xc6\t\xbd\xc9\x19\xfd\xbd!\xa1;=~M7\xbc{\xe4\x03\xbc+\xdd\xbd=B\xaf \xbc\x82\x95\xcf;c\tS=\x1eU\xe5\xbd\xff\x86l\xbd\xf3\x06\xa5;w\xd6\x16\xbd\x900\x89<\xd5;?\xbd\x9d[\x98\xbd\xf2\xff\x08=\x84\x92\\;\xe3\xa5\x87\xbc\x84\xf4@=\xe0\x03&\xbb\xb9Q\xbe\xbd\x9d0!=\xc8_\x88\xbd{\xa5\xc8=\xaafw\xbdP\x15\xe5\xbc\x99\x03\x89\xbd\xe1b,==\x7f9\xbd\xccK\xe4\xbdp<\x95=\xf0\xc2\xcd=\xf5:\x88=}FF\xbdF4\x1f=\x80\xe4b\xbd\xec\x93\xc8\xbc\x0e\t\xff;Ga\xd7\xbd<l\xaa\xbc\n\xa9}\xbde\xb9w\xbd\xdb\x90\xb9;\x11\xb7\xab\xbd@A\x02\xbd)\x85_\xbd\x87C*\xbd\xd9\xbc\x85<5r\x9f\xbc_\xfa\xf3\xbdGJ\x00\xbe\xc7\xb6\x97\xbd\xf8M\x08=e?\xd4=\xdf\x9cG\xbb6\x8e7=m\x16\x08=\xe5J\r\xbd\xecS\x1c=\x83\xa5\xc0=\xfc\xe1\xcd\xbd\xba\xde\xa6\xbdy\xd7N\xbd\x98{B\xbc\x1a\xfe\xff\xff\x04\x00\x00\x00\x00\x01\x00\x00\xb6_)\xbc\xe4\x81,\xbc\x9b\xcdv\xbb\xf9\x90\x07=\xee\x96""\xbcH\x177\xbc\t\xb4@\xbc\xe2S\x1b\xbc\xd7kq\xbc:\xd3-\xbc\xbe\r\xc9<b\xbd\x92<\x98\xc5%\xbcE6\x13\xbc\x1d)8\xbc\x94N""\xbc!\xe4P\xbcnDG<\x02\xec-\xbb\xbb\x8c\xd7;&\xaa#\xbc\x1b\xec\x02=\xa33T\xbc5\xf5\r\xbc\x10\xf7Q\xbc\xf2p3\xbc[\xa7\x1d\xbco(\x01\xbcE\xe4B\xbc\xf6O\x0f\xbc\x8fJ\xfb\xbb\x03g\xf6;\r\x98$\xbc\xb9\xb9\x83;\xb6%Y\xbc\x82o\x96\xbb\xa6\xe3N\xbc8O\xf2<{U(\xbcg \xf1\xbbOp#\xbc\x8b&3\xbcnW\xdd;\x00\x00\x00\x00\x9b\x92\'\xbc+a|:\xb4\xb5\x98<\x9e\xc9\x8d\xbc\x06\\\xcd\xbb\xff\xc3\xe2\xbb\xcf\x1e\x10\xbc\xff :\xbc\x8es\xc4\xbb\x1b\xf8#\xbc\x0fD}\xbc\xf6a1\xbc\xbf\xca\x7f\xbcE\xcc\x13=]\xa2\x18\xbc\n\xe40\xbc6\xd8%\xbc\xfew\xc3\xbb\xc8\xb0G\xbc\xec\xfdQ\xbc&\xff\xff\xff\x04\x00\x00\x00\x80\x00\x00\x00\xf6c^=5\xd6\xbc<\xa5j4\xbcosb\xba\x16\xe0C\xbc\r\x179\xbc\x8e|\x1a\xbchlT\xbc\xab\x90\x16\xbc<t\x1e\xbc\xcd\x04h;C-\xfb\xbb\x0c\x90\x84<H,\x18\xbaZ\xa0/\xba\xaef\xfd\xb9W1\x07\xbc\xc0\xa37<\xcf\x19\xf4\xbb\xc8{\x14\xbc\xa7\xbbM\xbc\x1e\xc3\x0f=,\x0c.\xbc\xb1\xe26\xbc)\x87\x13\xbcoA\xf8\xbb\x1b\x03<\xbc\x1519\xbc\xebz\x14\xbcE\x89\x03;\xa9S\xab\xb8\xb5~\x15\xbc\xb2\xff\xff\xff\x04\x00\x00\x00@\x00\x00\x00K\x1bo\xbbKj?\xbb6\xee@;]r(\xbc\'\x83\xc1\xba\x1b*\x84<;\xde\xc8\xbc\xef\xd0\x9c\xbaWt%\xbc\xc8\'\x1b=K\xaa\x0c=*\xd4p<\xa3\x9d\x93\xbcJ0\x83; \x9a.<\xa8\x10h\xbc\x00\x00\x06\x00\x08\x00\x04\x00\x06\x00\x00\x00\x04\x00\x00\x00\xc0\x06\x00\x00\x9a\x07\xc7\xb9\xb4\x1f=\xba;,\xb9\xb8\xa0T\x0b\xb8`\xbb\x1b:/$\xc99?\xc8\x808\xbf2\xc89\xe1x_\xb7\xc3\xcd-:1""\xe59K\x00\xdf\xb9\x0e\xf7\x10\xba\xf8\x13\x1c\xb91]\xd6\xb6\xef\x04\x0e:\x87|\xb8\xb83\xfc\x949H\xfcd9\xab]\xe79Di\xf98\x84\xac!:\xe2\xb4\x85\xb7\x04\x8f\x8a\xb9\xa2\x94\x1e:\xfc\xca\x18:\x18\\r\xb7\x8d\xb1\x039\x8d\x0c(9\x1au@\xba[\xc7a96!^\xb9\x94\xbf\xc1\xb9F\xc0\n:\xf7\x84\xd89\x1fJD\xba&\xe0#\xba\xdcY\x16\xbaE\xf7,:\x18\xc3\x198HM\xeb\xb8a\xa8\x199`\x8c\xca\xb9m\'\x13\xb9\xc4\x05\xf79\xee*\xae\xb86g :rs\'9S\xe8\x1c\xba\xfc}o\xb8\xc3<\xd19\x1a\x0b\x8a\xb9-\xc8\x15:\xbd\xf0\x1d\xba\xf2\'$\xba\xa8\x1al\xb7\xcfZZ9\xb4L\xe09h\x16\xcc\xb9#\x00=9\xaa\xa6\x959Y\x8e6\xb9?\xaa\x9a\xb9?\xd0\xd09*M99\xaa\xac\xb1\xb9\xf3\xa30:o(n9\x8d\x82\n:\xc5\x15$:\x93\xd3""\xba\x11\x16\xfd\xb9\x16:\xca9\x90\xcaq\xb9\x06\xfa%:K{3\xba\x9bK\x8d\xb9\x03K\xbe\xb9\xac\x05\x16\xba\x06\x00)\xba\xf9]\x9d9\x85!\x1f\xba:\x142\xbat\x9a""\xb9\xb0\xd4*\xb9V\x90\x1b:~\xc5v9,e\xf99\xd6X]\xb9\xa2s\x1f\xba7X7\xb9\x8f\xe5\x05\xbar6\xcc\xb7%\xcd\x909x\x98\xa59\xc0\xfd\xb8\xb9\xc16\x1b:\xcbe2:W\xe2\xa39w}\xca\xb9\xf4\n\xd07\x89V""\xba\xfa\xbc\x05\xba\x97\xb6t\xb80,J\xb7\xc4\x00\xf19 M\xcc\xb9\x02%V8\xc7\x18\xce\xb9\x8d\x932:\xc6\xa9X:w!\x04\xbaJ\xbb\xd8\xb9\xba\xe7V:\xa8{\x869\x1c\x06\x0e\xbaC\x8b\xd19s\xc3\xa28\xbbT\n\xba\xe3x\x18:\xbe\xa7\xa3\xb9,\xb5\xae\xb9\x84\xb4\xee\xb9\xe5n\xa19\x9a\xca\xed8\xb9\x13\x869\x0bh8\xb9_\x13\x11\xba\xd7\xcb\xea70_f\xb9\xab\xdd\xd4\xb8\xaa\xfc\x8b9\xb0*#:]E\xc0\xb9\x9e\xa5S\xb9\x8f{\xc09p\xe6@9)M\xe8\xb7\x15\xe8r9:$9\xb9\x0b\xb3\x17\xb9\x9f\xc4\xff\xb8\x82\x03\xf2\xb9I\xa6+:\x13\x82\xf69\xe5}\xf3\xb7\xcf\x1c\xa1\xb90\xbc%:Sx""\xbaA\xf0\xb3\xb9\x11E\xc1\xb8t\xa7L\xb9\xc3M\x1a\xba\xcb1\x1c\xb6J\xf5\xb9\xb9\x80K\xfb\xb9\xa9\xe2\x8f9\xdb\x038:\xf4\xd3\xb48\x99\xb8\xcb9\x9f\xaf\xd4\xb9\x98""#\xb9^\xfc0:d\xfbR\xb8\x8c\x0e\x829C\xd8\xc0\xb97\x909:\x8f\xb4\xfe\xb9\x8d\xa1\xca\xb8\x04L@\xba\xf3\xc9\x9e9O\xd6\xc3\xb8\xb8_\x0c9<\x8a\xe17\xb8\xc7\xad\xb9\xd0b\xcb9\x8f\xd4\x9f9\xaed\x0e:0\xc4\xf59\xc3\x0f\xdd93!\xe78\xa0\xbeS9\x81\x88\xd39\xca\xf0:\xb9\t\x86w\xb9lV\x9f9\x05\x08\xb79\x1c\x03T8[%\xba9M-o\xb8\x9f\xa8\x12\xba\\l\x13:\xf1\x9f{9\x88\xc7\xd99\x140\x05\xbagaF\xb9]\x8d\xfb4\x18e\xa5\xb9\xa4\x87\xe0\xb9\x85\x0cQ7\x8aY\x079\x9f\xd6\xe9\xb9n\r\xe1\xb9Iw*\xb9\x88\xf9m7I\xce!:\x06`\x8c\xb9k\xdf\xc6\xb9\x83\xa9\xd89\xd8T\x8d\xb9;\xf9\xaf9\xf8\x95\xd6\xb9\xfc\x98\r\xba\x97\x19Q9\x83\xa9\xe0\xb8\xbd\xd7\x9f\xb9\xdb\xfaX\xb9\xc1\x10\xb09\xe8\xe7$:\xba\x8dP\xb8\x8b8\xd57\x95\xc1(:$6\xcd\xb8\xf3\xf2\x9d\xb9\x8a\x0e\x07\xba\x1cXP9x\xd1\x83\xb8\xc5!^9h\xe9\xa39\x01m>\xba\x03\xb8\x04\xb9H\xf3/\xba\xaf\xf3\x10\xba\xbe3\xad\xb9%&\xe09\xc7A29\x7f\xb4\x8b\xb8\xeb8\x1e\xbaN&\xe8\xb9D\xb2,\xbaw\xa3r\xb9!\xa5\x85\xb9B\xf6u\xb9\xe7\xcf.\xba\x95#\xd5\xb9Ps\xae8\x0c|\xce9U\x05\xa88g\xe8\x1b\xba\xab\x8e\x83\xb7\x07[\xb6\xb9@\x0e^8\xad\x1b\xad\xb9\xd5\x90""\xba\x99(\xe39a\x01\xea9/=P\xb8\xf9\xde<:P\xcf\x1e:\xcc\xde?\xba\xe4\xf0\xfc\xb9Ho\x1c\xb8I\n\x14\xba\x04\xd2\xa08`\x9f\xec8\xca\x1b\x1c\xba\x9ea\xf39\xc7\xdfM:\xbf\x9f\xc58*\xfc\x12\xb9\xbf\xe1\x84\xb8zF#\xba@\t\':k\xe9\xbd\xb9Z+\xe49\xbe\x11\xce\xb9.\xe8$:\x88:\xf1\xb9\xa3\xef\x10\xba\xcar\xe0\xb9\xa4I;\xb9\xae\xa3Z\xb9x\x0f\x1b:O4\x08\xba:x\x17\xba\xdb! :]{""\xba\xeaC\x9d\xb9\xae\xdc\xbe\xb9?i3\xba /\xc7\xb9\n\xb95:\x95\x01\x0e\xba\xe1M\xa09\xa6C(:\xdcD\xeb9\xf7$\xf58/[%\xbak\xb12\xba\x96r\xc49#D\xd79\x17\xd5<:.\x1cA7\xbf\xc6\xdf\xb6r\xe0\x089\xe7\xc7\x05:\x11\xa4/\xba\xf8\x8c?:\x02B@:ky\x969\xaa\xa6=:J\xad\xae9>H\xa2\xb9sl\x18:\xc6N\n\xbaq~;\xba9\x98}8\xfe\xf9""9\xf3d:\xba\xe3\t\xd49\xf6\xcc\xd0\xb9zs3\xba\\\x02\x0b\xbaS\xa4\x95\xb9G\xa8\xb09\xd7\xdd,:O\xfc\x0c\xba\x94\xc2\xe2\xb9<\x97R9\xa5\xf6\xa1\xb6\xf1\x8b\xe0\xb9\xf1\x89\xd2\xb9+f\xdf8\x12g\x04\xb9\xda\xd2\xda9~\x14\xf37\x8c<$:\xf53\x19:\xfcKx7\xae\x9c9\xba\x1f\x8d\xee\xb8\x0f\x19\x1e\xb9\x12\xa5\t:\xe6\xb0z9\xab&\xa6\xb7\xb5\xbcX\xb9\xa2\x9c\x129\x12G\x80\xb9\x91\x92\xf69\x92\x03\x01\xba\xfb\xfd\x0b:\xb4\xf4f9\xcf\x80\x148M\x0b\xb8\xb9\xeb\xebW8,@\xf8\xb9f\x00\xe78\xc64\xfd\xb9+[\x8c8\xec\xca\xd89\x89\xa6%\xb8\xeff\x13\xba\x11_\xf8\xb9)\xd0\xc29\xda4\x1f\xb9O<\x10:\xe3P\x0c:%f\xb8\xb9\x18\xe6\xfa9\x0b/|9\xd8\x8b\xcb9\x8e_\'\xba\x8f\xb5\xbf91r\xf5\xb9\xdf\xfb)\xbaYb\x17\xb8O=\xdf8Q\xea`\xb9\x88+\x058u\xba\x13\xbaG\x1f\x18981\xd09\xb3\x96\xc29\x99I\x1e\xb8G\xb3\x0e\xba|\x0e\x879\xb06\x049\xb1F,:o\x99@\xba\x1b\xa5#9\xf4\x00\xc7\xb7\xa6\x17Q\xba\x07Hp9Z\xc7&:\xe8\xa4\x02:k\xbd\xb19\xffU 8\x0c:R\xba\xb2\xcc\xe49\xfef\x9c9\xf3p6\xba\xe4\x10\xc1\xb9\x1cq9\xb8$\xa9\x06\xba)w\x189%\x18\x00:\xeat\xc9\xb8\x86+!\xb7\xe2\x10\x06\xbaMx48\x06\xf0\xf2\xb9S\x186\xb9\xdc\xc0\x16:\xb6\xb9O\xb9\n\n2\xbaKP\x8e9\xaf\x88\xc39\xb4\x9e6:\xba\xc8\x1a\xb9\x80\xb5\x0f:\xf1\xb8\xc39\xccy\x1f:\xbc\xbf\x1f\xba\xc8\xaf\xed9\x12\x82\x19:*\xfe\xff8\x10\x02\xfa9-\x11a90}5:\x82\x96\xa49\xf8N\x0f:I\xde:\xb9p\xf3\xff\xfft\xf3\xff\xff\x0f\x00\x00\x00MLIR Converted.\x00\x01\x00\x00\x00\x14\x00\x00\x00\x00\x00\x0e\x00\x18\x00\x14\x00\x10\x00\x0c\x00\x08\x00\x04\x00\x0e\x00\x00\x00\x14\x00\x00\x00\x1c\x00\x00\x00\x94\x02\x00\x00\x98\x02\x00\x00\x9c\x02\x00\x00\x04\x00\x00\x00main\x00\x00\x00\x00\t\x00\x00\x000\x02\x00\x00\xcc\x01\x00\x00|\x01\x00\x008\x01\x00\x00\xf8\x00\x00\x00\xb4\x00\x00\x00\x8c\x00\x00\x00<\x00\x00\x00\x04\x00\x00\x00b\xfe\xff\xff\x14\x00\x00\x00\x00\x00\x00\x08\x10\x00\x00\x00\x14\x00\x00\x00\x03\x00\x00\x00\x08\xf4\xff\xff\x01\x00\x00\x00\x14\x00\x00\x00\x03\x00\x00\x00\x13\x00\x00\x00\x0b\x00\x00\x00\x08\x00\x00\x00\x96\xfe\xff\xff\x1c\x00\x00\x00\x00\x00\x00\x08\x1c\x00\x00\x00 \x00\x00\x00\x03\x00\x00\x00\x00\x00\x06\x00\x08\x00\x07\x00\x06\x00\x00\x00\x00\x00\x00\x01\x01\x00\x00\x00\x13\x00\x00\x00\x03\x00\x00\x00\x12\x00\x00\x00\n\x00\x00\x00\t\x00\x00\x00\x00\x00\n\x00\x10\x00\x0c\x00\x08\x00\x04\x00\n\x00\x00\x00\x0c\x00\x00\x00\x10\x00\x00\x00\x02\x00\x00\x00\x01\x00\x00\x00\x12\x00\x00\x00\x02\x00\x00\x00\x11\x00\x00\x00\x07\x00\x00\x00\x06\xff\xff\xff\x14\x00\x00\x00\x00\x00\x00\x05$\x00\x00\x00(\x00\x00\x00\x01\x00\x00\x00\xf6\xfe\xff\xff\x02\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x01\x01\x00\x00\x00\x11\x00\x00\x00\x01\x00\x00\x00\x10\x00\x00\x00\xe6\xfe\xff\xff\x10\x00\x00\x00\x00\x00\x00\x01\x18\x00\x00\x00\x1c\x00\x00\x00\xd8\xfe\xff\xff\x00\x00\x00\x01\x01\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x10\x00\x00\x00\x03\x00\x00\x00\x0f\x00\x00\x00\x06\x00\x00\x00\x04\x00\x00\x00\x82\xff\xff\xff\x14\x00\x00\x00\x00\x00\x00\x05$\x00\x00\x00(\x00\x00\x00\x01\x00\x00\x00r\xff\xff\xff\x02\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x01\x01\x00\x00\x00\x0f\x00\x00\x00\x01\x00\x00\x00\x0e\x00\x00\x00b\xff\xff\xff\x10\x00\x00\x00\x00\x00\x00\x01\x18\x00\x00\x00\x1c\x00\x00\x00T\xff\xff\xff\x00\x00\x00\x01\x01\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x0e\x00\x00\x00\x03\x00\x00\x00\r\x00\x00\x00\x05\x00\x00\x00\x03\x00\x00\x00\x00\x00\x0e\x00\x1a\x00\x14\x00\x10\x00\x0c\x00\x0b\x00\x04\x00\x0e\x00\x00\x00$\x00\x00\x00\x00\x00\x00\x054\x00\x00\x008\x00\x00\x00\x01\x00\x00\x00\x00\x00\x0e\x00\x18\x00\x17\x00\x10\x00\x0c\x00\x08\x00\x04\x00\x0e\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x01\x01\x00\x00\x00\r\x00\x00\x00\x01\x00\x00\x00\x0c\x00\x00\x00\x00\x00\x0e\x00\x14\x00\x00\x00\x10\x00\x0c\x00\x0b\x00\x04\x00\x0e\x00\x00\x00\x1c\x00\x00\x00\x00\x00\x00\x01$\x00\x00\x00(\x00\x00\x00\x0c\x00\x10\x00\x00\x00\x0c\x00\x08\x00\x07\x00\x0c\x00\x00\x00\x00\x00\x00\x01\x01\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x0c\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00\x01\x00\x00\x00\x14\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x15\x00\x00\x00x\t\x00\x00\xac\x08\x00\x00<\x08\x00\x00\xe0\x07\x00\x00\x84\x07\x00\x00,\x07\x00\x00\xd4\x06\x00\x00\x88\x06\x00\x00\x18\x06\x00\x00\xc0\x05\x00\x00t\x05\x00\x00(\x05\x00\x00\\\x04\x00\x00\xe8\x03\x00\x00\x14\x03\x00\x00\x9c\x02\x00\x00\xc8\x01\x00\x00P\x01\x00\x00\xf0\x00\x00\x00`\x00\x00\x00\x04\x00\x00\x00\xf2\xf6\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00\x1c\x00\x00\x00\x1c\x00\x00\x00\x15\x00\x00\x004\x00\x00\x00\x02\x00\x00\x00\xff\xff\xff\xff\x0b\x00\x00\x00\xd4\xf6\xff\xff\x19\x00\x00\x00StatefulPartitionedCall:0\x00\x00\x00\x02\x00\x00\x00\x01\x00\x00\x00\x0b\x00\x00\x00J\xf7\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00\x1c\x00\x00\x00\x1c\x00\x00\x00\x14\x00\x00\x00h\x00\x00\x00\x02\x00\x00\x00\xff\xff\xff\xff\x80\x00\x00\x00,\xf7\xff\xffL\x00\x00\x00sequential_1/dense/MatMul;sequential_1/dense/Relu;sequential_1/dense/BiasAdd\x00\x00\x00\x00\x02\x00\x00\x00\x01\x00\x00\x00\x80\x00\x00\x00\xd6\xf7\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00\x1c\x00\x00\x00\x1c\x00\x00\x00\x13\x00\x00\x008\x00\x00\x00\x02\x00\x00\x00\xff\xff\xff\xff\x00y\x00\x00\xb8\xf7\xff\xff\x1c\x00\x00\x00sequential_1/flatten/Reshape\x00\x00\x00\x00\x02\x00\x00\x00\x01\x00\x00\x00\x00y\x00\x002\xf8\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00$\x00\x00\x00$\x00\x00\x00\x12\x00\x00\x00H\x00\x00\x00\x04\x00\x00\x00\xff\xff\xff\xff\x16\x00\x00\x00\x16\x00\x00\x00@\x00\x00\x00\x1c\xf8\xff\xff$\x00\x00\x00sequential_1/max_pooling2d_2/MaxPool\x00\x00\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00\x16\x00\x00\x00\x16\x00\x00\x00@\x00\x00\x00\xa6\xf8\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00$\x00\x00\x00$\x00\x00\x00\x11\x00\x00\x00\xa4\x00\x00\x00\x04\x00\x00\x00\xff\xff\xff\xff-\x00\x00\x00-\x00\x00\x00@\x00\x00\x00\x90\xf8\xff\xff\x82\x00\x00\x00sequential_1/conv2d_2/Relu;sequential_1/conv2d_2/BiasAdd;sequential_1/conv2d_2/Conv2D;sequential_1/conv2d_2/BiasAdd/ReadVariableOp\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00-\x00\x00\x00-\x00\x00\x00@\x00\x00\x00v\xf9\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00$\x00\x00\x00$\x00\x00\x00\x10\x00\x00\x00H\x00\x00\x00\x04\x00\x00\x00\xff\xff\xff\xff-\x00\x00\x00-\x00\x00\x00 \x00\x00\x00`\xf9\xff\xff$\x00\x00\x00sequential_1/max_pooling2d_1/MaxPool\x00\x00\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00-\x00\x00\x00-\x00\x00\x00 \x00\x00\x00\xea\xf9\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00$\x00\x00\x00$\x00\x00\x00\x0f\x00\x00\x00\xa4\x00\x00\x00\x04\x00\x00\x00\xff\xff\xff\xffZ\x00\x00\x00Z\x00\x00\x00 \x00\x00\x00\xd4\xf9\xff\xff\x82\x00\x00\x00sequential_1/conv2d_1/Relu;sequential_1/conv2d_1/BiasAdd;sequential_1/conv2d_1/Conv2D;sequential_1/conv2d_1/BiasAdd/ReadVariableOp\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00Z\x00\x00\x00Z\x00\x00\x00 \x00\x00\x00\xba\xfa\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00$\x00\x00\x00$\x00\x00\x00\x0e\x00\x00\x00D\x00\x00\x00\x04\x00\x00\x00\xff\xff\xff\xffZ\x00\x00\x00Z\x00\x00\x00\x10\x00\x00\x00\xa4\xfa\xff\xff""\x00\x00\x00sequential_1/max_pooling2d/MaxPool\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00Z\x00\x00\x00Z\x00\x00\x00\x10\x00\x00\x00*\xfb\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00$\x00\x00\x00$\x00\x00\x00\r\x00\x00\x00\x9c\x00\x00\x00\x04\x00\x00\x00\xff\xff\xff\xff\xb4\x00\x00\x00\xb4\x00\x00\x00\x10\x00\x00\x00\x14\xfb\xff\xff{\x00\x00\x00sequential_1/conv2d/Relu;sequential_1/conv2d/BiasAdd;sequential_1/conv2d/Conv2D;sequential_1/conv2d/BiasAdd/ReadVariableOp1\x00\x04\x00\x00\x00\x01\x00\x00\x00\xb4\x00\x00\x00\xb4\x00\x00\x00\x10\x00\x00\x00\xba\xfc\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x0c\x00\x00\x00(\x00\x00\x00\xc4\xfb\xff\xff\x1b\x00\x00\x00sequential_1/dense_1/MatMul\x00\x02\x00\x00\x00\x0b\x00\x00\x00\x80\x00\x00\x00\x02\xfd\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x0b\x00\x00\x00(\x00\x00\x00\x0c\xfc\xff\xff\x19\x00\x00\x00sequential_1/dense/MatMul\x00\x00\x00\x02\x00\x00\x00\x80\x00\x00\x00\x00y\x00\x00J\xfd\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\n\x00\x00\x008\x00\x00\x00T\xfc\xff\xff)\x00\x00\x00sequential_1/dense/BiasAdd/ReadVariableOp\x00\x00\x00\x01\x00\x00\x00\x80\x00\x00\x00\x9e\xfd\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\t\x00\x00\x008\x00\x00\x00\xa8\xfc\xff\xff+\x00\x00\x00sequential_1/dense_1/BiasAdd/ReadVariableOp\x00\x01\x00\x00\x00\x0b\x00\x00\x00\x00\x00\x16\x00\x1c\x00\x18\x00\x17\x00\x10\x00\x0c\x00\x08\x00\x00\x00\x00\x00\x00\x00\x07\x00\x16\x00\x00\x00\x00\x00\x00\x01\x14\x00\x00\x00\x14\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x02(\x00\x00\x00\x18\xfd\xff\xff\x1a\x00\x00\x00sequential_1/flatten/Const\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00R\xfe\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x07\x00\x00\x00,\x00\x00\x00\\\xfd\xff\xff\x1c\x00\x00\x00sequential_1/conv2d_2/Conv2D\x00\x00\x00\x00\x04\x00\x00\x00@\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00 \x00\x00\x00\xa6\xfe\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x06\x00\x00\x00,\x00\x00\x00\xb0\xfd\xff\xff\x1c\x00\x00\x00sequential_1/conv2d_1/Conv2D\x00\x00\x00\x00\x04\x00\x00\x00 \x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x10\x00\x00\x00\xfa\xfe\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x05\x00\x00\x00<\x00\x00\x00\x04\xfe\xff\xff,\x00\x00\x00sequential_1/conv2d_2/BiasAdd/ReadVariableOp\x00\x00\x00\x00\x01\x00\x00\x00@\x00\x00\x00R\xff\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x04\x00\x00\x00<\x00\x00\x00\\\xfe\xff\xff,\x00\x00\x00sequential_1/conv2d_1/BiasAdd/ReadVariableOp\x00\x00\x00\x00\x01\x00\x00\x00 \x00\x00\x00\xaa\xff\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x03\x00\x00\x008\x00\x00\x00\xb4\xfe\xff\xff*\x00\x00\x00sequential_1/conv2d/BiasAdd/ReadVariableOp\x00\x00\x01\x00\x00\x00\x10\x00\x00\x00\x00\x00\x16\x00\x18\x00\x14\x00\x00\x00\x10\x00\x0c\x00\x08\x00\x00\x00\x00\x00\x00\x00\x07\x00\x16\x00\x00\x00\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x02\x00\x00\x00\x88\x00\x00\x00 \xff\xff\xffz\x00\x00\x00sequential_1/conv2d/Relu;sequential_1/conv2d/BiasAdd;sequential_1/conv2d/Conv2D;sequential_1/conv2d/BiasAdd/ReadVariableOp\x00\x00\x04\x00\x00\x00\x10\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x00\x00\x16\x00\x1c\x00\x18\x00\x00\x00\x14\x00\x10\x00\x0c\x00\x00\x00\x00\x00\x08\x00\x07\x00\x16\x00\x00\x00\x00\x00\x00\x01\x14\x00\x00\x00(\x00\x00\x00(\x00\x00\x00\x01\x00\x00\x00H\x00\x00\x00\x04\x00\x00\x00\xff\xff\xff\xff\xb4\x00\x00\x00\xb4\x00\x00\x00\x03\x00\x00\x00\x04\x00\x04\x00\x04\x00\x00\x00""\x00\x00\x00serving_default_sequential_input:0\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00\xb4\x00\x00\x00\xb4\x00\x00\x00\x03\x00\x00\x00\x04\x00\x00\x00@\x00\x00\x00$\x00\x00\x00\x14\x00\x00\x00\x04\x00\x00\x00\xdc\xff\xff\xff\t\x00\x00\x00\x00\x00\x00\t\xe8\xff\xff\xff\x16\x00\x00\x00\x00\x00\x00\x16\xf4\xff\xff\xff\x11\x00\x00\x00\x00\x00\x00\x11\x0c\x00\x0c\x00\x0b\x00\x00\x00\x00\x00\x04\x00\x0c\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x03'', does not exist.
```
"
61839,Build fail on TFLite C API for Android with CMake,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0-rc1

### Custom code

No

### OS platform and distribution

macOS 13.5.2

### Mobile device

Android

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On version 2.14.0-rc1, Building TFLite C API for Android target with CMake fails by link error.
It works fine on version 2.13.0.

`ld: error: unable to find library -lpthread`
Error is caused by linking `pthread`. I guess pthread is unnecessary for android.

#### Extra information to my environment
- CMake: 3.26.4
- Android NDK: 25.2.9519653(r25c)

### Standalone code to reproduce the issue

```shell
mkdir tf-build
cd tf-build
cmake -DCMAKE_TOOLCHAIN_FILE=<NDK path>/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a ../tensorflow/lite/c
cmake --build . -j
```


### Relevant log output

```shell
...
[ 70%] Built target absl_status
[ 75%] Built target absl_flags
[ 95%] Built target tensorflow-lite
[100%] Linking CXX shared library libtensorflowlite_c.so
ld: error: unable to find library -lpthread
clang++: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: *** [libtensorflowlite_c.so] Error 1
make[1]: *** [CMakeFiles/tensorflowlite_c.dir/all] Error 2
make: *** [all] Error 2
```
"
61832,Model train failing,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.4.1

### Custom code

No

### OS platform and distribution

Windows 11 WSL Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.9.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Cudatoolkit 10.1.234 Cudann 7.6.5

### GPU model and memory

NVIDIA GeForce RTX 4080 deviceMemorySize: 15.99GiB

### Current behavior?

Running : jupyter notebook: TensorFlow 2 quickstart for experts
https://www.tensorflow.org/tutorials/quickstart/advanced

2023-09-08 23:55:01.853625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 4080 computeCapability: 8.9
coreClock: 2.505GHz coreCount: 76 deviceMemorySize: 15.99GiB deviceMemoryBandwidth: 667.63GiB/s


TRAINING THE MODEL:

2023-09-09 14:21:13.106797: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-09-09 14:21:13.108589: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3417595000 Hz
2023-09-09 14:21:13.135568: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2023-09-09 14:22:05.711952: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7

NOT LEARNING!!!
Epoch 14, Loss: 2.3014447689056396, Accuracy: 11.233333587646484, Test Loss: 2.3011932373046875, Test Accuracy: 11.34000015258789
Epoch 15, Loss: 2.301426410675049, Accuracy: 11.236666679382324, Test Loss: 2.3012006282806396, Test Accuracy: 11.329999923706055



### Standalone code to reproduce the issue

```shell
https://www.tensorflow.org/tutorials/quickstart/advanced
```


### Relevant log output

_No response_"
61831,Tersorflow deterministic training not working for different workflows even when GPU and envs are same,"### System information

- Ubuntu 20.04
- Tensorflow version :-  2.5.0
- python version :- 3.7
- GPU model and memory :- Nvidia RTX 4090, 24 GB GPU memory


### Describe the problem
We recently started setting environmental variable for enabling deterministic training on tensorflow version 2.5. We have 5 to 6 models, most of which are regression models and one of them is a classification model. The following seed values were set :- 
   ```
    os.environ[""PYTHONHASHSEED""] = str(seed)
    
    import tensorflow as tf
    import random
    import numpy as np
    
    # set seed for random, numpy and tensorflow
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

    # set env for determinism
    os.environ[""TF_DETERMINISTIC_OPS""] = ""1""
    os.environ[""TF_CUDNN_DETERMINISTIC""] = ""1""
   
    # to ensure determinism set threads to one
    tf.config.threading.set_inter_op_parallelism_threads(1)
    tf.config.threading.set_intra_op_parallelism_threads(1)
``````

 We created multiple workflows by keeping the same GPU version and all of the above environmental variables and seed values. But, for different workflows we get different results even though the initial weights for all models on both workflows are same. On the other hand, on training in the same workflow, we get similar results. kindly let us know what other seed values do we need to set and let us know a solution so that we can train the models in a deterministic way


### Source code / logs

Here is the code to compare initialised weights of model in different workflow but when performing inference after training for 70 epochs the results vary a lot

```
import tensorflow as tf
import numpy as np


tf_det_ini = tf.keras.models.load_model(""./tf_det/nobrainer_initial.h5"")
wb_diff_ini = tf.keras.models.load_model(""./wb_diff/nobrainer_initial.h5"")

diff_arr = []

cnt = 0
for tf_m, wb in zip(tf_det_ini.layers[1:], wb_diff_ini.layers[1:]):
    if tf_m.name == 'MobilenetV3large':
        tf_m_mob = tf_m.layers
        wb_mob = wb.layers

        for tf_m_mobl, wb_mob_l in zip(tf_m_mob[2:], wb_mob[2:]):
            try:
                diff = np.unique(tf_m_mobl.weights[0].numpy() - wb_mob_l.weights[0].numpy())
                diff_arr.extend(diff)
            except:
                continue
            # print(tf_m_mobl.weights[0].numpy())

    try:
        print(tf_m.name)
        diff = tf_m.weights[0].numpy() - wb.weights[0].numpy()
        diff_arr.extend(np.unique(diff))
    except:
        continue

print(max(diff_arr))
print(min(diff_arr))
```

output :- 

```
MobilenetV3large
global_average_pooling2d
extradata
concatenate
dense
0.0
0.0
```"
61830,"`#include ""tsl/framework/numeric_types.h""` is missing","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15

### Custom code

Yes

### OS platform and distribution

docker/tensorflow/tensorflow:nightly-gpu

### Mobile device

no

### Python version

3.11

### Bazel version

NR

### GCC/compiler version

NR

### CUDA/cuDNN version

NR

### GPU model and memory

NR

### Current behavior?

Create test.cc
```
#include ""tensorflow/core/framework/op.h""
```
compile it with flags provided by TensorFlow:
get_compile_flags():
```['-I/usr/local/lib/python3.11/dist-packages/tensorflow/include', '-D_GLIBCXX_USE_CXX11_ABI=1', '--std=c++17', '-DEIGEN_MAX_ALIGN_BYTES=64']```
get_link_flags():
```['-L/usr/local/lib/python3.11/dist-packages/tensorflow', '-l:libtensorflow_framework.so.2']```
Then
```
g++ test.cc -o /tmp/test.o -fPIC -I/usr/local/lib/python3.11/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=1 --std=c++17 -DEIGEN_MAX_ALIGN_BYTES=64 -L/usr/local/lib/python3.11/dist-packages/tensorflow -l:libtensorflow_framework.so.2  -O2```
The error
```
In file included from /usr/local/lib/python3.11/dist-packages/tensorflow/include/tensorflow/core/framework/bfloat16.h:19,
                 from /usr/local/lib/python3.11/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:24,
                 from /usr/local/lib/python3.11/dist-packages/tensorflow/include/tensorflow/core/framework/op_def_builder.h:28,
                 from /usr/local/lib/python3.11/dist-packages/tensorflow/include/tensorflow/core/framework/full_type_inference_util.h:24,
                 from /usr/local/lib/python3.11/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:27,
                 from test.cc:1:
/usr/local/lib/python3.11/dist-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:24:10: fatal error: tsl/framework/numeric_types.h: No such file or directory
   24 | #include ""tsl/framework/numeric_types.h""
```

### Standalone code to reproduce the issue

```shell
As above
```


### Relevant log output

```shell
As above
```
"
61828,Segmentation fault for TFLite Interpreter,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.03

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I create a simple tensorflow model and convert it to a TFLite model. 

Upon calling the TFLite interpreter's allocate_tensors() method, I find that TFLite segfaults, perhaps similar to [this issue](https://github.com/tensorflow/tensorflow/issues/47996). 

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
from tflite import Model
import tflite_runtime.interpreter as tflite


if __name__ == '__main__':

    data1 = np.arange(-128, 128).reshape((16, 16, 1)).astype(np.float32)
    data2 = np.arange(-128, 128).reshape((16, 16, 1)).astype(np.float32)

    _merge = tf.keras.layers.Subtract()
    def _dataset_generator():
        for _ in range(1):
            arr1 = data1.reshape(1, *data1.shape)
            arr2 = data2.reshape(1, *data2.shape)
            yield [arr1.astype(np.float32), arr2.astype(np.float32)]

    # base TF model
    _inputs1 = tf.keras.layers.Input(shape=data1.shape, dtype=np.float32)
    _inputs2 = tf.keras.layers.Input(shape=data2.shape, dtype=np.float32)
    _merge = _merge([_inputs1, _inputs2])
    tf_model = tf.keras.Model([_inputs1, _inputs2], _merge)

    # TFL model
    converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = _dataset_generator
    _tflite_quant_model = converter.convert()
    tfl_model = Model.GetRootAsModel(_tflite_quant_model, 0)

    # write model to file
    model_file = ""segfault_test.tfl""
    with open(model_file, ""wb"") as fp:
        fp.write(_tflite_quant_model)

    # create interpreter
    interpreter = tflite.Interpreter(
        model_path=model_file,
        experimental_op_resolver_type=tflite.OpResolverType.BUILTIN_REF)

    # this call causes a segfault
    interpreter.allocate_tensors()
```


### Relevant log output

```shell
2023-09-11 11:38:16.876852: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-09-11 11:38:16.877887: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-09-11 11:38:16.894424: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:7630] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-09-11 11:38:16.894443: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-09-11 11:38:16.894454: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-09-11 11:38:16.898344: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-09-11 11:38:16.898500: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-11 11:38:17.196991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/miniconda3/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
WARNING:tensorflow:From /home/miniconda3/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
2023-09-11 11:38:17.461212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-11 11:38:17.461403: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2158] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-09-11 11:38:17.559681: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpnwirm5mk
/home/miniconda3/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:928: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.
  warnings.warn(
2023-09-11 11:38:17.578426: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
2023-09-11 11:38:17.578436: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
2023-09-11 11:38:17.578732: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpnwirm5mk
2023-09-11 11:38:17.578885: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2023-09-11 11:38:17.578889: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpnwirm5mk
2023-09-11 11:38:17.579556: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled
2023-09-11 11:38:17.579648: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.
2023-09-11 11:38:17.582018: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpnwirm5mk
2023-09-11 11:38:17.583376: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 4645 microseconds.
2023-09-11 11:38:17.586236: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32
Aborted (core dumped)
```
"
61827,tf.linalg.eigvals outputs UnboundLocalError when receiving a float16 tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Given a float16 tensor, tf.linalg.eigvals outputs `UnboundLocalError: local variable 'out_dtype' referenced before assignment`. If tf.linalg.eigvals does not accept float16 tensor, it would be better if it can be explicit in the documentation and the error message can point this issue out.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tensor = tf.constant([[1,2],[3,4]], dtype=tf.float16)
tf_out = tf.linalg.eigvals(tf.constant(tensor))
print(""TensorFlow's result: "",tf_out)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
UnboundLocalError                         Traceback (most recent call last)
<ipython-input-69-5a0470cb5082> in <cell line: 3>()
      1 import tensorflow as tf
      2 tensor = tf.constant([[1,2],[3,4]], dtype=tf.float16)
----> 3 tf_out = tf.linalg.eigvals(tf.constant(tensor))
      4 print(""TensorFlow's result: "",tf_out)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/linalg_ops.py in eigvals(tensor, name)
    431   elif tensor.dtype == dtypes.float64 or tensor.dtype == dtypes.complex128:
    432     out_dtype = dtypes.complex128
--> 433   e, _ = gen_linalg_ops.eig(tensor, Tout=out_dtype, compute_v=False, name=name)
    434   return e
    435 

UnboundLocalError: local variable 'out_dtype' referenced before assignment
```
"
61826,"Inconsistent result between tf.linalg.eigvals and numpy/scipy on a (3,3) tensor","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Given the same tensor, tf.linalg.eigvals outputs results different from np.linalg.eigvals and scipy.linalg.eig. The latter two APIs are also computing the eigenvalues and it may be expected if tf.linalg.eigvals can output the same results as theirs.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import scipy
import numpy as np

np.random.seed(1234)
tensor = np.random.random((3, 3)).astype(np.float32)

tf_out = tf.linalg.eigvals(tf.constant(tensor))
np_out = np.linalg.eigvals(tensor)
scipy_out = scipy.linalg.eig(tensor)[0]
print(""TensorFlow's result: "",tf_out)
print(""Numpy's result: "", np_out)
print(""Scipy's result: "", scipy_out)
```


### Relevant log output

```shell
TensorFlow's result:  tf.Tensor([-0.20270659+0.j  1.7388148 +0.j  0.3935262 +0.j], shape=(3,), dtype=complex64)
Numpy's result:  [ 1.7388151  -0.20270674  0.39352626]
Scipy's result:  [ 1.7388152 +0.j -0.20270659+0.j  0.39352617+0.j]
```
"
61825,Pytorch solves gym environments faster than tensorflow using the same training implementation and network architecture,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.13

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

T4 GPU

### Current behavior?

I have encountered a performance difference between my PyTorch and TensorFlow implementations of the Double Deep Q-Network (DDQN) algorithm in a Gym environment. Both implementations share identical DDQN architectures, exploration routines, and training flows. However, the PyTorch implementation consistently converges faster, requiring fewer episodes to solve the environment.

Details:

**Network and Training Flow**: Both PyTorch and TensorFlow implementations employ the same dense neural network architecture (two hidden layers of [512, 128]) and training procedures. The timing for weight optimization is nearly identical between the two.

**Exploration**: Identical exploration strategies are employed in both implementations, ensuring consistency in agent behavior. Both of them use epsilon-greedy exploration.

The optimization code for each framework is:
**Tensorflow:**
```
@tf.function
def optimize_model(self,
                   experiences,
                   max_gradient_norm = float('inf')):
    states, actions, rewards, next_states, is_terminals = experiences
    #batch_size = len(is_terminals)

    with tf.GradientTape() as tape:

        # We get the argmax (or maximum action index using the online network)
        argmax_a_q_sp = tf.argmax(self.online_model(next_states), axis=1)
        # Then we use the target model to calculate the estimated Q-values
        q_sp = tf.stop_gradient(self.target_model(next_states))

        # And extract the max value using the index gotten with the online model
        max_a_q_sp = tf.expand_dims(tf.gather(q_sp, argmax_a_q_sp, axis = 1)[:,0], axis = 1)
        # Then we start computing the loss value
        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))

        # Flatten the column_indices tensor
        actions_ = tf.reshape(actions, [-1])

        # Use tf.range to create row indices
        row_indices = tf.range(actions_.shape[0])
        row_indices = tf.cast(row_indices, tf.int64)
        # Create combined indices
        combined_indices = tf.stack([row_indices, actions_], axis=1)

        # Gather elements from the second tensor using combined indices
        q_sa = tf.gather_nd(self.online_model(states), combined_indices)
        q_sa = tf.reshape(q_sa, (-1, 1))

        #q_sa = tf.gather(self.online_model(states), actions, axis=1)
        td_error = q_sa - target_q_sa
        value_loss = tf.reduce_mean(tf.square(td_error) * 0.5)
    variables = self.online_model.trainable_variables
    gradients = tape.gradient(value_loss, variables)

    self.value_optimizer.apply_gradients(zip(gradients, self.online_model.trainable_variables))
```
**Pytorch:**
```
def optimize_model(self,
                       experiences,
                       max_gradient_norm = float('inf')):

        states, actions, rewards, next_states, is_terminals = experiences
        batch_size = len(is_terminals)
        # We get the argmax (or maximum action index using the online network)
        argmax_a_q_sp = self.online_model(next_states).max(1)[1]
        # Then we use the target model to calculate the estimated Q-values
        q_sp = self.target_model(next_states).detach()
        # And extract the max value using the index gotten with the online model
        max_a_q_sp = q_sp[np.arange(batch_size), argmax_a_q_sp].unsqueeze(1)

        # Then we start computing the loss value
        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))
        q_sa = self.online_model(states).gather(1, actions)

        td_error = q_sa - target_q_sa
        value_loss = td_error.pow(2).mul(0.5).mean()
        self.value_optimizer.zero_grad()
        value_loss.backward()
        #torch.nn.utils.clip_grad_norm_(self.online_model.parameters(), max_gradient_norm)
        self.value_optimizer.step()
```

I'm seeking guidance on potential factors that might explain this performance gap. Could variations in internal library optimizations, autograd systems, GPU utilization, or other factors play a role in this discrepancy? Any insights or suggestions for further investigation would be greatly appreciated.

You can access to an example by following this link:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LuisFMCuriel/SwarmyMcQLearny/blob/main/notebooks/SwarmyMcQLearny.ipynb)

### Standalone code to reproduce the issue

```shell
You can find the code in this GitHub repository: [GitHub Repository](https://github.com/LuisFMCuriel/SwarmyMcQLearny).

Alternatively, you can easily access an example by following this link to a Colab notebook: 
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LuisFMCuriel/SwarmyMcQLearny/blob/main/notebooks/SwarmyMcQLearny.ipynb)
```


### Relevant log output

_No response_"
61823,[rocm] [build] sh: line 1: /opt/rocm/hip/bin/hipcc: No such file or directory,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Arch Linux

### Mobile device

_No response_

### Python version

3.11

### Bazel version

5.4.0

### GCC/compiler version

12.3.0

### CUDA/cuDNN version

ROCm 5.6.0

### GPU model and memory

_No response_

### Current behavior?

Build fails with the following error:
```
sh: line 1: /opt/rocm/hip/bin/hipcc: No such file or directory
```

### Standalone code to reproduce the issue

```shell
Build the code from source with ROCM 5.6.0
```


### Relevant log output

_No response_"
61822,Error installing build from source,"WSL2 - Ubantu-22.4
When configured I use 

""Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.""

But when I build I get this error 

""ERROR: Traceback (most recent call last):
        File ""/home/n3/.cache/bazel/_bazel_n3/6f14c15d85335b4cb3c9403635dfd45f/external/build_bazel_rules_apple/apple/internal/transition_support.bzl"", line 547, column 46, in <toplevel>
                _apple_platform_split_transition = transition(
Error in transition: Invalid transition input '//command_line_option:incompatible_enable_apple_toolchain_resolution'. Cannot transition on --experimental_* or --incompatible_* options
ERROR: /mnt/d/TensorflowC/tensorflow/WORKSPACE:88:14: error loading package '@com_github_grpc_grpc//src/compiler': at /home/n3/.cache/bazel/_bazel_n3/6f14c15d85335b4cb3c9403635dfd45f/external/com_github_grpc_grpc/bazel/grpc_build_system.bzl:28:6: at /home/n3/.cache/bazel/_bazel_n3/6f14c15d85335b4cb3c9403635dfd45f/external/build_bazel_rules_apple/apple/ios.bzl:33:5: at /home/n3/.cache/bazel/_bazel_n3/6f14c15d85335b4cb3c9403635dfd45f/external/build_bazel_rules_apple/apple/internal/ios_rules.bzl:95:5: initialization of module 'apple/internal/transition_support.bzl' failed and referenced by '//external:grpc_python_plugin'""

This is an iOS support error, right? 
I couldn't configure any iOS and Android WORKSPACE so why do I get this error?"
61821,Does TensorFloat-32 support `tf.compat.v1.nn.depthwise_conv2d_native`? How can I identify whether an op is supported by TensorFloat-32?,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0-rc1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm trying to use TensorFloat-32 to accelerate my model.
But I found that `tf.compat.v1.nn.depthwise_conv2d_native` is not currently supported by TensorFloat-32, by comparing the output with and without TensorFloat-32.

The [documentation](https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_tensor_float_32_execution) says _""Note TensorFloat-32 is not always used in supported ops, as only inputs of certain shapes are supported. Support for more input shapes and more ops may be added in the future.""_
I also found that #46168 and #58047 mention some ops that are not supported by TensorFloat-32.
So I'm wondering if there is a way to identify whether an op is supported by TensorFloat-32?


### Standalone code to reproduce the issue

```python
import numpy as np
import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION, flush=True)
print(tf.config.list_physical_devices(), flush=True)


x_in = np.random.rand(1,7,7,16)
kernel_in = np.random.rand(3,3,16,16)

tf.config.experimental.enable_tensor_float_32_execution(True)
x = tf.constant(x_in, dtype=tf.float32)
kernel = tf.constant(kernel_in, dtype=tf.float32)
res_tf32 = tf.compat.v1.nn.depthwise_conv2d_native(x, kernel, strides=[1, 1, 1, 1], padding='VALID')

tf.config.experimental.enable_tensor_float_32_execution(False)
x = tf.constant(x_in, dtype=tf.float32)
kernel = tf.constant(kernel_in, dtype=tf.float32)
res_full = tf.compat.v1.nn.depthwise_conv2d_native(x, kernel, strides=[1, 1, 1, 1], padding='VALID')

print(""res_tf32==res_full:"", np.all(res_tf32==res_full))
```


### Relevant log output

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
res_tf32==res_full: True
```
"
61820,The `mask` dtype should be asserted to be boolean in `tf.compat.v1.boolean_mask` (and `tf.boolean_mask`),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0-rc1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The `mask` dtype should be asserted to be boolean in `tf.compat.v1.boolean_mask` (and `tf.boolean_mask`).
The documentation [`tf.boolean_mask`](https://www.tensorflow.org/api_docs/python/tf/boolean_mask) states that the `mask` is a K-D boolean Tensor, K <= N and K must be known statically.
However, currently, the `mask` dtype is not asserted to be boolean in `tf.compat.v1.boolean_mask` (and `tf.boolean_mask`) and cast to boolean if it is not a boolean Tensor, which may lead to unexpected results.
Generally, only the `0` and `1` values in the `mask` can be directly treated as `False` and `True` respectively, while other values should be determined by certain rules in different scenarios (_e.g._ negative values can be treated as `False` in some scenarios, or close to `0` values can be treated as `False` in some other scenarios).

I think it's worth updating the documentation to state that the `mask` will be cast to boolean if it is not a boolean Tensor, or warning users that the `mask` dtype should be boolean.
As mentioned in #54412, it is better to raise an InvalidArgumentError Exception when the `mask` dtype is not boolean, which forces users to cast the `mask` to boolean explicitly.


### Standalone code to reproduce the issue

```python
import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION, flush=True)
print(tf.config.list_physical_devices(), flush=True)


try:
    tensor = [0,1,2,3]
    mask = tf.random.uniform([4], dtype=tf.float64)
    x1 = tf.compat.v1.boolean_mask(tensor, mask) 
    print(x1, flush=True)
except Exception as e:
    print(""Success! Error:"", str(e), flush=True)
else:
    print(""Failed!"", flush=True)
```


### Relevant log output

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
tf.Tensor([0 1 2 3], shape=(4,), dtype=int32)
Failed!
```
"
61816,Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have an issue with memory consumption during training. After each epoch more memory is used. In case of bigger dataset, the training crashed mid training. 
I use loading the training data from generator, where I sample the batch using pandas sample method. 
One of the warnings from tf is: 

`W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.`

I am a bit confused, whether using the generator and sampling from a pandas dataframe can cause memory accumulation during the training?



### Standalone code to reproduce the issue

```shell
def generate_samples(epochs: int, steps_per_epoch: int, token_y_df: pd.DataFrame, num_fields: int, batch_size: int = 32, max_len: int = 1024) -> tuple[np.array, np.array]:
        pad_value_y = get_padding_value_y(num_fields)
        while True:
            batch_df = token_y_df.sample(batch_size, replace=True)
            _x, _y = get_fixed_batch(batch_df, max_len, pad_value_y=pad_value_y,
                                     use_additional_features=use_additional_features)
            yield _x, _y

model.fit(generate_samples, ....)


The `get_padding` and the `get_fixed_batch` are longer functions, that pad and create the X and y np.arrays from the sampled pandas dataframe.
```


### Relevant log output

```shell
W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
```
"
61815,Wrong answer from tflite model with certain configurations of depthwise conv2d,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 11
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13.0

### 2. Code

```
import tensorflow as tf
import numpy as np

tf.keras.utils.set_random_seed(0)

# Layer parameter values
imageSize = (8, 8)
numChannels = 2
fmt = 'channels_last'
kernelSize = (3, 1)
padding = 'same'
depthMult = 1
useBias = False
strides = (2, 2)

# Determine input shape
if fmt == 'channels_last':
    inShape = imageSize + (numChannels,)
else:
    inShape = (numChannels,) + imageSize

# Determine input size and create input data
inSize = (1,) + inShape
rng = np.random.default_rng(seed=999)
x = rng.random(size=inSize, dtype=np.float32)

# Construct Keras model
inp = tf.keras.layers.Input(shape=inShape, batch_size=1)
dconv = tf.keras.layers.DepthwiseConv2D(kernelSize, strides=strides, padding=padding, depth_multiplier=depthMult,
                                        data_format=fmt, activation=None, use_bias=useBias)(inp)
kModel = tf.keras.models.Model(inputs=inp, outputs=dconv)
kModel.compile()

# Run Keras model predict
kY = kModel.predict(x)
print(kY)

# Convert model to tflite
converter = tf.lite.TFLiteConverter.from_keras_model(kModel)
tfModel = converter.convert()

# Run tflite interpreter
interpreter = tf.lite.Interpreter(model_content=tfModel)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.set_tensor(input_details[0]['index'], x)
interpreter.invoke()
tfY = interpreter.get_tensor(output_details[0]['index'])
print(tfY)

isequal = np.allclose(kY, tfY, rtol=1e-4, atol=1e-4)
print(isequal)
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results - in this case all 0s

5. (optional) Any other info / logs

Some general observations on when this wrong answer occurs (all of the following must be met):
- One of the kernel_size dimensions is 1 (1xN or Nx1)
- Stride is not 1
- Padding is same

"
61814,`tensorflow/python/compiler/xla/jit_test` fails on PPC with $HOME unset,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.11

### Bazel version

6.3.1

### GCC/compiler version

12.3

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Running the test `//tensorflow/python/compiler/xla:jit_test_cpu` through Bazels build system with a pre-installed numpy (build from source) yields a segmentation fault I cannot explain.

It happened after upgrading our toolchain from GCC 11 & Python 3.10 to GCC 12 and Python 3.11 but includes also a couple other software versions which changed but trying to narrow them down wasn't successful. But seemingly only Python and OpenSSL (instead of BoringSSL which doesn't work on PPC) as well as the mentioned numpy should be involved.

After a lot of digging I found that omitting `$HOME` when executing the test (which Bazel does) triggers the segmentation fault.   
Adding `--test_env HOME=/non-existing` works around this.

I further traced it to `testJITCreateOpsLambda` in particular keeping only the single `compute` call at https://github.com/tensorflow/tensorflow/blob/v2.13.0/tensorflow/python/compiler/xla/jit_test.py#L76 is enough while `self.compute(False, create_ops)` works which means it is related to XLA compilation.

Also replacing [`random_uniform`](https://github.com/tensorflow/tensorflow/blob/v2.13.0/tensorflow/python/compiler/xla/jit_test.py#L70) by `constant_op.constant(1)` avoids the error, so it is related to how that call is compiled/run but I failed to further follow how that happens

I'm at loss how to debug this further and whether this is an issue with TensorFlows XLA compilation or a bug elsewhere only triggered by that special environment

### Standalone code to reproduce the issue

```shell
`bazel test --config=noaws --config=nogcp --config=nohdfs --compilation_mode=opt --config=opt --copt=""-fPIC"" --action_env=CPATH='/sw/installed/OpenSSL/1.1/include' --host_action_env=CPATH='/sw/installed/OpenSSL/1.1/include' --action_env=LIBRARY_PATH='/sw/installed/OpenSSL/1.1/lib' --host_action_env=LIBRARY_PATH='/sw/installed/OpenSSL/1.1/lib' --action_env=PYTHONPATH --host_action_env=PYTHONPATH --action_env PYTHON_BIN_PATH --action_env PYTHON_LIB_PATH --python_path=$(which python)  -- //tensorflow/python/compiler/xla:jit_test_cpu`

or reduced to the actual invocation after bazel failed:

`export PYTHONPATH=bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/com_google_protobuf/python:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/tblib_archive/src:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/astunparse_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/com_google_protobuf:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/dill_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/gast_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/opt_einsum_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/six_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/tblib_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/termcolor_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/typing_extensions_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/wrapt:$PYTHONPATH
(cd /dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/bazel-root/9c88b62e77874bb73aea75868f86ebae/execroot/org_tensorflow && \
cd - &&
  exec env - \
    LD_LIBRARY_PATH=$LD_LIBRARY_PATH \
    PATH=$PATH \
    PYTHONNOUSERSITE=1 \
    PYTHONPATH=$PYTHONPATH \
    HOME2='/fake' \
python bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/compiler/xla/jit_test.py)`
```


### Relevant log output

```shell
Running tests under Python 3.11.3: /beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/bin/python
[ RUN      ] JITTest.testJITCreateOpsLambda
2023-09-08 10:32:04.231849: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled
2023-09-08 10:32:04.404724: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x201108083080 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2023-09-08 10:32:04.404765: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2023-09-08 10:32:04.603193: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
Fatal Python error: Segmentation fault

Thread 0x0000200000048800 (most recent call first):
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1455 in _call_tf_sessionrun
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1362 in _run_fn
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1379 in _do_call
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1372 in _do_run
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1192 in _run
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 969 in run
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2059 in run
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/compiler/xla/jit_test.py"", line 55 in compute
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/compiler/xla/jit_test.py"", line 81 in testJITCreateOpsLambda
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/case.py"", line 579 in _callTestMethod
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/case.py"", line 623 in run
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/case.py"", line 678 in __call__
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/suite.py"", line 122 in run
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/suite.py"", line 84 in __call__
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/suite.py"", line 122 in run
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/suite.py"", line 84 in __call__
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/runner.py"", line 217 in run
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py/absl/testing/_pretty_print_reporter.py"", line 86 in run
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/main.py"", line 274 in runTests
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/main.py"", line 102 in __init__
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2537 in _run_and_get_tests_result
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2568 in run_tests
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2156 in _run_in_app
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2049 in main
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py/absl/app.py"", line 258 in _run_main
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py/absl/app.py"", line 312 in run
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 489 in benchmarks_main
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 56 in main
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/compiler/xla/jit_test.py"", line 321 in <module>

Extension modules: google.protobuf.pyext._message, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, tensorflow.python.framework.fast_tensor_util (total: 15)
*** Received signal 11 ***
*** BEGIN MANGLED STACK TRACE ***
/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/../../../_solib_ppc/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2(+0xd76744)[0x200b0de86744]
[0x2000000504d8]
/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/libpython3.11.so.1.0(+0x14090c)[0x2000001b090c]
[0x2000000504d8]
/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/../../libtensorflow_cc.so.2(_ZN3xla3cpu13CpuExecutable22ExecuteComputeFunctionEPKNS_20ExecutableRunOptionsEN4absl12lts_202301254SpanIKNS_23MaybeOwningDeviceMemoryEEEPNS_19HloExecutionProfileE+0x150)[0x200b15956c40]
/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/../../libtensorflow_cc.so.2(+0x5be7c04)[0x200b15957c04]
/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/../../libtensorflow_cc.so.2(_ZN15stream_executor4host10HostStream8WorkLoopEv+0x1ac)[0x200b1d822b0c]
/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/../../libtensorflow_cc.so.2(+0xdab3274)[0x200b1d823274]
/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/../../../_solib_ppc/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2(+0xd85088)[0x200b0de95088]
/lib64/libpthread.so.0(+0x8b94)[0x2000008c8b94]
/lib64/libc.so.6(clone+0xe4)[0x200000a585f4]
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
	tsl::CurrentStackTrace[abi:cxx11]()
	
	__kernel_sigtramp_rt64
	
	__kernel_sigtramp_rt64
	xla::cpu::CpuExecutable::ExecuteComputeFunction(xla::ExecutableRunOptions const*, absl::lts_20230125::Span<xla::MaybeOwningDeviceMemory const>, xla::HloExecutionProfile*)
	
	stream_executor::host::HostStream::WorkLoop()
	
	
	
	clone
*** End stack trace ***
```
"
61813,ValueError: Unable to create dataset (name already exists),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Epoch 1/10
1/1 [==============================] - ETA: 0s - loss: 6.8405 - accuracy: 0.3250
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-57-865e41f45523> in <cell line: 1>()
----> 1 transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds,batch_size=BATCH_SIZE,callbacks=[early_stop, checkpoint_call, plot_losses])

2 frames
/usr/local/lib/python3.10/dist-packages/h5py/_hl/dataset.py in make_new_dset(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)
    161         sid = h5s.create_simple(shape, maxshape)
    162 
--> 163     dset_id = h5d.create(parent.id, name, tid, sid, dcpl=dcpl, dapl=dapl)
    164 
    165     if (data is not None) and (not isinstance(data, Empty)):

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/h5d.pyx in h5py.h5d.create()

ValueError: Unable to create dataset (name already exists)


i want to save each epoch as a checkpoint two weeks back back without any any error each checkpoint will save as a checkpoint but suddenly now getting error

### Standalone code to reproduce the issue

```shell
import matplotlib.pyplot as plt

from tensorflow.keras.callbacks import Callback


import os

checkpoint_dir = '/model/checkpoints_m_1'

if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)
from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss', patience=3)
# Set up the model checkpoint callback
checkpoint_call = ModelCheckpoint(filepath=checkpoint_dir+""/checkpoint_{epoch}.hdf5"",
                                  monitor='val_loss',
                                  save_best_only=True,
                                  save_weights_only=False,
                                  mode='min',
                                  save_freq='epoch')
transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds,batch_size=BATCH_SIZE,callbacks=[early_stop, checkpoint_call)
```


### Relevant log output

_No response_"
61812,Building TF Lite C++ static library for iOS,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS M1 Version 13.2.1
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: master branch of https://github.com/tensorflow/tensorflow/tree/master Sept 7, 2023
-   **TensorFlow version (use command below)**: TensorFlow 2.14.0
-   **Python version**: Python 3.9.6
-   **Bazel version (if compiling from source)**: bazel 6.1.0
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 
-   **GPU model and memory**: 8GB
-   **Exact command to reproduce**: bazel build --config=ios_fat -c opt --cxxopt=--std=c++17 \
  //tensorflow/lite:libtensorflowlite.so


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Currect tensorflow only generates dynamic or static framework and does not static library for iOS.  I need to integrate tensorflow with c++ code in WebAssembly without framework so that I could have same source code for Android and iOS build.  

### Source code / logs
bazel build --config=ios_fat -c opt --cxxopt=--std=c++17 \
  //tensorflow/lite:libtensorflowlite.a

AND

bazel build --config=ios_fat -c opt --cxxopt=--std=c++17
//tensorflow/lite/c:libtensorflowlite.a

Similar issue to https://github.com/tensorflow/tensorflow/issues/40438 but this source are outdated and does not apply to current master source.

"
61811,tf.data.Dataset.list_files(): You must feed a value for placeholder tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom code

No

### OS platform and distribution

Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Reading a dataset obtained with `tf.data.Dataset.list_files()` prints incomprehensible warnings.

Create two files:
```bash
touch a.txt
touch b.txt
```
Run this python program:
```python
import tensorflow as tf
dataset = tf.data.Dataset.list_files(['a.txt', 'b.txt'])
for f in dataset: 
    print(f)
```

Prints some incomprehensible warnings:
```
tf.Tensor(b'b.txt', shape=(), dtype=string)
tf.Tensor(b'a.txt', shape=(), dtype=string)

2023-09-07 17:19:04.634978: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2]
	 [[{{node Placeholder/_0}}]]
2023-09-07 17:19:04.635273: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2]
	 [[{{node Placeholder/_0}}]]
```
This is a ~duplicate of https://github.com/tensorflow/tensorflow/issues/41648 that was marked as resolved 3 years ago.

### Standalone code to reproduce the issue

```shell
With tensorflow==2.12.0:  
https://colab.research.google.com/drive/1_kjUH6BzcLnlM4rc8mY7JcnhE5NdaRGy?usp=sharing

No warning with tensorflow==2.11.1  
https://colab.research.google.com/drive/1QhatrE7hdJIxIUAIrYw5yDPQ50SeqFrI?usp=sharing
```


### Relevant log output

```shell
2023-09-07 17:19:04.635273: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2]
	 [[{{node Placeholder/_0}}]]
```
"
61808,Support mathematical operations for boolean tensors in tensorflow like in numpy,"### Issue type

bug/support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

```python
>>>import tensorflow as tf
>>>import numpy as np
>>>tnp = tf.experimental.numpy
>>>tf.constant([True, False])*tf.constant([True, True]) # same error for tnp.array([True, False])*tnp.array([True, True])
InvalidArgumentError                      Traceback (most recent call last)
[<ipython-input-5-5eb244ee10ef>](https://localhost:8080/#) in <cell line: 1>()
----> 1 tnp.array([True, False])*tnp.array([True, True])

1 frames
[/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py](https://localhost:8080/#) in raise_from_not_ok_status(e, name)
   5885 def raise_from_not_ok_status(e, name) -> NoReturn:
   5886   e.message += ("" name: "" + str(name if name is not None else """"))
-> 5887   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   5888 
   5889 

InvalidArgumentError: Value for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name:

>>> #but for numpy arrays this is not the case
>>> np.array([True, False])*np.array([True, True])
array([ True, False])
>>> np.array([True, False])/np.array([True, True])
array([1., 0.])
```

### Standalone code to reproduce the issue

[Google Colab link](https://colab.research.google.com/drive/1y41vpFs6Cd4NHmSY6IRmU53PXgl_SAFE?usp=sharing) 


### Relevant log output

_No response_"
61806,ADD Suppport for VEDV (https://github.com/yunielrc/vedv),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
61801,Linux tensorflow build from source: bash failed genrule-setup.sh has carriage return,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.3.0

### GCC/compiler version

LLVM = CLang = 16

### CUDA/cuDNN version

CPU only

### GPU model and memory

N/A

### Current behavior?

bazel build fails with ""bash failed: error executing command"", ""/bin/bash: line 1: $'\r': command not found""
(bash failed, genrule-setup.sh has carriage return (windows line endings))
modifying this .sh file does not work, because bazel won't even start the build, saying that file is modified, it might be corrupt.

Docs need to tell us we should check out tf repo after 
`git config --global core.autocrlf input`
Thanks.

### Standalone code to reproduce the issue

```shell
# git config --global core.autocrlf input  # very important!!!  otherwise your bazel build will fail, because an invoked shell script will have Windows line endings.
# git config --global core.eol lf
git clone https://github.com/tensorflow/tensorflow.git &
cd tensorflow
git checkout -b r2.13 origin/r2.13
apt install python3.10-venv
cd ..

python3 -m venv tf_venv
tf_venv/bin/pip install -U pip numpy wheel packaging requests opt_einsum
tf_venv/bin/pip install -U keras_preprocessing --no-deps

wget https://github.com/bazelbuild/bazelisk/releases/download/v1.18.0/bazelisk-linux-amd64
mv bazelisk-linux-amd64 bazel
chmod +x bazel
mv -v bazel /usr/local/bin

#install clang-16 

cd tensorflow
../tf_venv/bin/python3 configure.py   # (dl clang: N, opti flag: -msse4.1)
bazel build  --local_ram_resources=2048 --jobs=4 --verbose_failures  //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_"
61800,Incorrect behaviour of `tf.nn.sigmoid` on complex inputs,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf v1.12.1-96406-gfa4d29bfef8 2.14.0-dev20230706

### Custom code

Yes

### OS platform and distribution

WSL Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.nn.sigmoid` gives incorrect values for complex numbers with large, negative real parts (which should map to within a rounding error of 0). For example, when `x = -709-1j` (`complex128`) I expect `tf.nn.sigmoid(x) = 0`, but instead `tf.nn.sigmoid(x) = 1`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([-709-1j], dtype=tf.complex128)
tf.nn.sigmoid(x)  # <tf.Tensor: shape=(1,), dtype=complex128, numpy=array([1.+0.j])>
```


### Relevant log output

_No response_"
61797,bazel  test failed with some ctest,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.12

### Custom code

Yes

### OS platform and distribution

centos7.6

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3.0

### GCC/compiler version

9.3.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

After compiling tensorflow based on the source code, bazel test is executed for unit testing, some test items are passed, but there are many failures, and the reasons for the error are as follows
So I want to know if I'm executing the statement incorrectly, how should I set it up?
bazel test -c opt  --config=cuda  --test_sharding_strategy=disabled  //tensorflow/core/kernels/...
output information:
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
Executing tests from //tensorflow/core/kernels/image:resize_ops_test_gpu

![image](https://github.com/tensorflow/tensorflow/assets/30514703/bc876890-814c-4488-8c74-ee9f46960ada)


![image](https://github.com/tensorflow/tensorflow/assets/30514703/befa4291-9404-42d1-956a-11250f384cc4)


### Standalone code to reproduce the issue

```shell
bazel test -c opt  --config=cuda  --test_sharding_strategy=disabled  //tensorflow/core/kernels/...
```


### Relevant log output

_No response_"
61796,"Uncompliant tflite model when converting ""MultiHeadAttention"" layer","### 1. System information

- OS Platform and Distribution: WIndows 10
- TensorFlow installation: pip package
- TensorFlow library (version): 2.13.0

### 2. Code

```
import tensorflow as tf
from tensorflow.keras import layers
import numpy as np

def model(q, v):
    x = layers.MultiHeadAttention(num_heads=2, key_dim=2)(q, v)
    return x

def representative_dataset():
    for _ in range(100):
        q = np.log(np.random.random((8, 16)))
        v = np.log(np.random.random((4, 16)))

        yield [q.astype(np.float32), v.astype(np.float32)]

target = tf.keras.Input(shape=[8, 16])
source = tf.keras.Input(shape=[4, 16])
out = model(target, source)
model = tf.keras.Model(inputs=(target, source), outputs=out)
model.summary()

model.save('MultiHeadAttention.h5')

run_model = tf.function(model)
# let's fix the input size.
concrete_func = run_model.get_concrete_function((
    tf.TensorSpec([1, 8, 16], model.inputs[0].dtype), tf.TensorSpec([1, 4, 16], model.inputs[0].dtype)))

# model directory.
MODEL_DIR = ""MultiHeadAttention""
model.save(MODEL_DIR, save_format=""tf"", signatures=concrete_func)

converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.representative_dataset = representative_dataset
converter.inference_input_type = tf.int8  # or tf.uint8
converter.inference_output_type = tf.int8  # or tf.uint8
tflite_model = converter.convert()
# Save the model.
with open('MultiHeadAttention.tflite', 'wb') as f:
    f.write(tflite_model)

print(f'Created model: MultiHeadAttention.tflite')
```

generated tf (h5) model: [link](https://drive.google.com/file/d/1QuSu77qI6o5y6bnoGzFJanlO6NP-lQXz/view?usp=sharing)
generated tflite model: [link](https://drive.google.com/file/d/1_DA5bJleAhLeBxGqmj3dfnK8u0ZhZiu5/view?usp=sharing)


### 3. Failure after conversion
The conversion to quantized 8bit is successful, but the generated model has problems:
The ""MultiHeadAttention"" layer is converted to many simpler layers on the tflite model (which is fine),
but among those layers there are 4 fully connected (FC) layers that are not tflite compliant and when trying to run this model on  [tflite-micro](https://github.com/tensorflow/tflite-micro) engine (C/C++ engine) it fail with assert! The problem with those 4 FC layers is that their weights quantization is asymmetric where one of the fundamental assumptions of tflite quantization is that the weights (on FC or CONV layers) should be symmetric, i.e. the ""zero-point"" should be zero. And as can be seen on my model screenshot the zero_point is 18. On the layer attributes it looks like ""assymetric_quantization = false"", but that is not correct, as we see the zero-point is not 0.

![image](https://github.com/tensorflow/tensorflow/assets/6744658/3319d61a-0230-46cc-b985-fafe7b541bbd)

See more about the tflite weights symmetric quantization requirement [here](https://www.tensorflow.org/lite/performance/quantization_spec#:~:text=Weights%20are%20symmetric). 

See also the assert check in the tflite-micro code [here](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/kernels/fully_connected_common.cc).
Lines 67-71:
```
    // Filter weights will always be symmetric quantized since we only support
    // int8 quantization. See
    // https://github.com/tensorflow/tensorflow/issues/44912 for additional
    // context.
    TFLITE_DCHECK(filter->params.zero_point == 0);

```

**Note**: those FC layers take their weights from previous layers outputs (unlike simple FC layers where the weights are known in advance). Those layer outputs should have symmetric quantization to be qualified as FC weights. 
"
61792,Cannot create interpreter when using GPU-Delegate or NNAPI-Delegate,"**System information**
- Android Device information: samsung/a14mnseea/a14m:13/TP1A.220624.014/A145RXXU2AWG3:user/release-keys
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
  - com.google.android.gms:play-services-tflite-java:16.1.0
  - com.google.android.gms:play-services-tflite-support:16.1.0
  - com.google.android.gms:play-services-tflite-gpu:16.2.0
- Google Play Services version: 23.33.16

**Standalone code to reproduce the issue**

        var useGpu = Tasks.await(TfLiteGpu.isGpuDelegateAvailable(context));
        var optionsBuilder = TfLiteInitializationOptions.builder();

        optionsBuilder.setEnableGpuDelegateSupport(useGpu);

        Tasks.await(TfLite.initialize(context, optionsBuilder.build()));

        var options =  new InterpreterApi.Options();
        if(useGpu){
            options.addDelegateFactory(new GpuDelegateFactory());
        }

        /*delegate = new NnApiDelegate();
        options.addDelegate(delegate);
        options.setUseNNAPI(true);*/

        options.setRuntime(InterpreterApi.Options.TfLiteRuntime.FROM_SYSTEM_ONLY);

        //load Model from App assets
        interpreter = InterpreterApi.create(new File(modelPath), options);

**Any other info / logs**
I oriented my code on the official documentation [on here](https://www.tensorflow.org/lite/android/delegates/gpu)

Logcat-Output:
java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: 
                                                                                                    	at com.google.android.gms.tflite.NativeInterpreterWrapper.createInterpreter(Native Method)
                                                                                                    	at com.google.android.gms.tflite.NativeInterpreterWrapper.zzl(com.google.android.gms:play-services-tflite-java@@16.1.0:34)
                                                                                                    	at com.google.android.gms.tflite.NativeInterpreterWrapper.<init>(com.google.android.gms:play-services-tflite-java@@16.1.0:6)
                                                                                                    	at com.google.android.gms.tflite.zzd.<init>(com.google.android.gms:play-services-tflite-java@@16.1.0:1)
                                                                                                    	at com.google.android.gms.tflite.InterpreterFactoryImpl.create(com.google.android.gms:play-services-tflite-java@@16.1.0:2)
                                                                                                    	at org.tensorflow.lite.InterpreterApi.create(InterpreterApi.java:336)
                                                                                                    	at com.example.tfliteaudio.TFLiteEngine.initialize(TFLiteEngine.java:83)
                                                                                                    	at com.example.tfliteaudio.MainActivity.lambda$transcribeAudio$5(MainActivity.java:143)
                                                                                                    	at com.example.tfliteaudio.MainActivity.$r8$lambda$1xqJ9hAvPXTc26gXgWfy8QcV0VE(Unknown Source:0)
                                                                                                    	at com.example.tfliteaudio.MainActivity$$ExternalSyntheticLambda2.run(Unknown Source:2)
                                                                                                    	at java.lang.Thread.run(Thread.java:1012)
"
61791,Memory leak when using tf.Model and tf.Model.fit() in a loop. clear_session() does not help,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes (tf-nightly = ""2.15.0.dev20230904"")

### Source

source

### TensorFlow version

2.13, 2.12, 2.11

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.1 LTS (GNU/Linux 5.16.10 x86_64)

### Mobile device

_No response_

### Python version

3.10.0, 3.9.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuDNN 8600

### GPU model and memory

NVIDIA RTX A5000, 24GB

### Current behavior?

Memory usage steadily increases when using tf.keras.Model and tf.keras.Model.fit() in a loop, and leads to Out Of Memory exception saturating the memory eventually. clear_session() does not help. The same code with TF version == 2.9.2 has an almost constant memory usage instead, and works as expected.
I've also opened [this issue](https://github.com/keras-team/tf-keras/issues/286) on Keras's GitHub, months ago, with no solutions from the Keras team.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf # same issue with tf-nightly = ""2.15.0.dev20230904""
import time
import gc
import psutil # psutil == ""5.9.5""
import subprocess as sp

gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(
        device=gpu, enable=True
    )


def get_cpu_memory():
    memory_info = psutil.virtual_memory()
    # you can have the percentage of used RAM
    memory_percent = 100.0 - memory_info.percent
    memory_free_values = memory_info.available / (1024 * 1024)  # in MB
    # you can calculate percentage of available memory
    return memory_free_values, memory_percent


def get_gpu_memory():
    command = ""nvidia-smi --query-gpu=memory.free --format=csv""
    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\n')[:-1][1:]
    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)][0]
    memory_percent = (memory_free_values / 24564) * 100  # my gpu has 24564 MB of memory
    return memory_free_values, memory_percent


class MyModel(tf.keras.Model):

    def __init__(self):
        super().__init__()
        self.dense1 = tf.keras.layers.Dense(1000, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(10000, activation=tf.nn.relu)
        self.dense3 = tf.keras.layers.Dense(10000, activation=tf.nn.relu)
        self.dense4 = tf.keras.layers.Dense(1000, activation=tf.nn.softmax)

    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        x = self.dense3(x)
        x = self.dense4(x)
        return x


if __name__ == '__main__':
    print(f""Starting.."")
    memory_free_val_initial, memory_perc_initial = get_cpu_memory()
    print(f""[Memory monitoring] Free memory CPU {memory_free_val_initial} MB, {memory_perc_initial} %."")
    memory_free_val_initial_gpu, memory_perc_initial_gpu = get_gpu_memory()
    print(f""[Memory monitoring] Free memory GPU {memory_free_val_initial_gpu} MB, {memory_perc_initial_gpu} %."")

    for r in range(0, 1000):
        model = MyModel()
        # ds = tf.data.Dataset.from_tensor_slices((tf.random.uniform((64*4, 1000)), tf.ones((64*4))))
        ds = (lambda: tf.data.Dataset.from_tensor_slices((tf.random.uniform((64 * 20, 1000)), tf.ones((64 * 20)))))
        model.compile(optimizer='sgd', loss=tf.keras.losses.SparseCategoricalCrossentropy())

        model.fit(ds().batch(64), verbose=0)
        model.evaluate(ds().batch(64), verbose=0)
        tf.keras.backend.clear_session()

        if r % 5 == 0:
            # print every 5 model.fit
            print(f""Round: {r}"")
            memory_free_val, memory_perc = get_cpu_memory()
            print(f""[Memory monitoring] Free memory CPU {memory_free_val} MB, {memory_perc} %."")
            memory_free_val_gpu, memory_perc_gpu = get_gpu_memory()
            print(f""[Memory monitoring] Free memory GPU {memory_free_val_gpu} MB, {memory_perc_gpu} %."")
            if r == 0:
                memory_free_first = memory_free_val
                memory_free_first_gpu = memory_free_val_gpu
            # time.sleep(2)

        del model
        gc.collect()
        del ds

    print(f""[Memory monitoring CPU] Memory usage increased by {memory_free_first - memory_free_val} MB, ""
          ""during the process."")
    print(f""[Memory monitoring GPU] Memory usage increased by {memory_free_first_gpu - memory_free_val_gpu} MB, ""
          ""during the process."")
```


### Relevant log output

```shell
Round: 0
[Memory monitoring] Free memory CPU 56633.48046875 MB, 88.5 %.
[Memory monitoring] Free memory GPU 21180 MB, 86.2237420615535 %.


Round: 995
[Memory monitoring] Free memory CPU 49866.3046875 MB, 77.9 %.
[Memory monitoring] Free memory GPU 21156 MB, 86.12603810454324 %.

[Memory monitoring CPU] Memory usage increased by 6767.17578125 MB, during the process.
[Memory monitoring GPU] Memory usage increased by 24 MB, during the process.
```
"
61790,import tensorflow as tf delegate = tf.lite.experimental.load_delegate('/content/drive/MyDrive/test_delegate/libtensorflowlite_gpu_delegate.so')#with this we ca get faster predictions OSError: libEGL.so: cannot open shared object file: No such file or directory,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
61789,TensorFlow Lite in Play Services issue aguinigacervantesjerardo@gmail.com not my devices are the login in just this phone device that are not this one  getting them out of my server please send them a little massage to buy there phone and don't use my phone number and my email at all,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
61788,"Unable to open file (truncated file: eof = 2568306, sblock->base_addr = 0, stored_eof = 9406464)","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

No

### OS platform and distribution

mac os 13.5.1

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
Cell In[15], line 3
      1 # Create the base model from the pre-trained model MobileNet V2
      2 IMG_SHAPE = IMG_SIZE + (3,)
----> 3 base_model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=IMG_SHAPE,
      4                                                include_top=False,
      5                                                weights='imagenet')

File [~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/keras/src/applications/mobilenet_v2.py:481], in MobileNetV2(input_shape, alpha, include_top, weights, input_tensor, pooling, classes, classifier_activation, **kwargs)
    477         weight_path = BASE_WEIGHT_PATH + model_name
    478         weights_path = data_utils.get_file(
    479             model_name, weight_path, cache_subdir=""models""
    480         )
--> 481     model.load_weights(weights_path)
    482 elif weights is not None:
    483     model.load_weights(weights)

File [~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70], in filter_traceback..error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb
...
File h5py/_objects.pyx:55, in h5py._objects.with_phil.wrapper()

File h5py/h5f.pyx:106, in h5py.h5f.open()

OSError: Unable to open file (truncated file: eof = 2568306, sblock->base_addr = 0, stored_eof = 9406464)

### Standalone code to reproduce the issue

```shell
https://tensorflow.google.cn/tutorials/images/transfer_learning

# Create the base model from the pre-trained model MobileNet V2
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
```


### Relevant log output

_No response_"
61787,"hope keras.layers.HashedCrossing support feature crossing with shape [batch_size, None] like tf.feature_column.crossed_column  ","### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.12

### Custom code

Yes

### OS platform and distribution

ubuntu 16.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

now keras.layers.HashedCrossing asssets All `HashedCrossing` inputs should have shape `[]`, `[batch_size]` or `[batch_size, 1]`. Received: inputs=[, ]. I need to cross [batch_size, 1] with shape [batch_size, None]( indefinite length sequence which is stored as sparse tensor). tf.feature_column.crossed_column could satisfy the demand while keras.layers.HashedCrossing dose not.

I hope keras.layers.HashedCrossing could get new feature to satisfy the demand. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow import keras

# cross with shape [None]
indices = [[0,0], [1,0], [2,0], [3,0], [3,1], [4,0], [4,1], [5,0], [5,1], [5,2], [6,0], [6,1], [6,2], [6,3]]
values = list(""abcabacabcaaaa"")
shape = [7, 4]

sparse_feature_tensor = tf.sparse.SparseTensor(indices, values, dense_shape=shape)

""""""
[['a'            ]
 ['b'            ]
 ['c'            ]
 ['a' 'b'        ]
 ['a' 'c'        ]
 ['a' 'b' 'c'    ]
 ['a' 'a' 'a' 'a']]
""""""

dense_feature_tensor = tf.constant([[1], [1], [1], [2], [1], [1], [1], ])

sparse_feature = keras.Input(shape=(None, ), dtype=tf.string)
dense_feature = keras.Input(shape=(1,), dtype=tf.int64)
cross_layer = keras.layers.HashedCrossing(num_bins = 100)
cross_output = cross_layer((sparse_feature, dense_feature))
embedding_layer = keras.layers.Embedding(100+1, 8, sparse=True)
cross_embedding = embedding_layer(cross_output)
preprocessing_model = keras.Model({""sparse_feature"": sparse_feature, ""dense_feature"": dense_feature}, {""cross_output"": cross_output, ""cross_embedding"": cross_embedding})

# raise ValueError: Exception encountered when calling layer ""hashed_crossing"" (type HashedCrossing).
# All `HashedCrossing` inputs should have shape `[]`, `[batch_size]` or `[batch_size, 1]`. Received: inputs=[, ]
# Call arguments received by layer ""hashed_crossing"" (type HashedCrossing):
  â€¢ inputs=('tf.Tensor(shape=(None, None), dtype=string)', 'tf.Tensor(shape=(None, 1), dtype=int64)')



#  cross with shape [2]

dense_feature_tensor = tf.constant([[1], [1], [1], [2], [1], [1], [1], ])
dense_feature_2_tensor = tf.constant([[1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], ])
dense_feature_2 = keras.Input(shape=(2, ), dtype=tf.string)
dense_feature = keras.Input(shape=(1,), dtype=tf.int64)
cross_layer = keras.layers.HashedCrossing(num_bins = 100)
cross_output = cross_layer((dense_feature_2, dense_feature))
embedding_layer = keras.layers.Embedding(100+1, 8, sparse=True)
cross_embedding = embedding_layer(cross_output)
preprocessing_model = keras.Model({""dense_feature_2"": dense_feature_2, ""dense_feature"": dense_feature}, {""cross_output"": cross_output, ""cross_embedding"": cross_embedding})

# raise the same error like previous code block
```


### Relevant log output

```shell
File d:\software\conda\envs\py38\lib\site-packages\keras\layers\preprocessing\hashed_crossing.py:206, in HashedCrossing._check_input_shape_and_type(self, inputs)
    204 rank = len(first_shape)
    205 if rank > 2 or (rank == 2 and first_shape[-1] != 1):
--> 206     raise ValueError(
    207         ""All `HashedCrossing` inputs should have shape `[]`, ""
    208         ""`[batch_size]` or `[batch_size, 1]`. ""
    209         f""Received: inputs={inputs}""
    210     )
    211 if not all(x.shape.as_list() == first_shape for x in inputs[1:]):
    212     raise ValueError(
    213         ""All `HashedCrossing` inputs should have equal shape. ""
    214         f""Received: inputs={inputs}""
    215     )

ValueError: Exception encountered when calling layer ""hashed_crossing_1"" (type HashedCrossing).

All `HashedCrossing` inputs should have shape `[]`, `[batch_size]` or `[batch_size, 1]`. Received: inputs=[, ]

Call arguments received by layer ""hashed_crossing_1"" (type HashedCrossing):
  â€¢ inputs=('tf.Tensor(shape=(None, 2), dtype=string)', 'tf.Tensor(shape=(None, 1), dtype=int64)')
```
"
61786,How do I set the threshold(iou/score) of TFLite Detection PostProcess via interpreter?,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- TensorFlow installation (pip package or built from source): pip package, python3.8
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
Download tflite model from https://tfhub.dev/iree/lite-model/ssd_mobilenet_v1_100_320/fp32/nms/1?lite-format=tflite

#### Interpreter code
`
import numpy as np
import tensorflow as tf

//TensorFlow Lite model path
tflite_model_file = 'lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.tflite'

interpreter = tf.lite.Interpreter(model_path=tflite_model_file)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']
input_array =  np.array(np.random.random_sample(input_shape), dtype=np.float32)

//HOW TO SET THRESHOLD POST-PROCESS
// is there any method like 'interpreter.set_threshold()' ?

interpreter.set_tensor(input_details[0]['index'], input_array)
interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])

print(output_data)
`

### 3. Failure after conversion


### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
61783,Quantization aware training - label smoothing drops performance?,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu18.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13.0

### 2. Code
```python
# fine-tunining baseline model for few epochs
history = run_model(args, save_model_file)
    
# load trained weights for quantization aware training
model = setup_pretrained_model(args, save_model_file)
    
def apply_quantization_to_dense(layer):
    if isinstance(layer, tf.keras.layers.Dense):
        return tfmot.quantization.keras.quantize_annotate_layer(layer)
    return layer

annotated_model = tf.keras.models.clone_model(
    model,
    clone_function=apply_quantization_to_dense,
)

# Build Model
annotated_model.build((None, args.input_dim, args.input_dim ,3))

# Now that the Dense layers are annotated,
# `quantize_apply` actually makes the model quantization aware.
quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)

#     quant_aware_model.summary()
n_sample, train_ds, val_ds = load_data(args, args.input_dim)

# `quantize_model` requires a recompile.
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    args.lr / 10.0,
    decay_steps=100000,
    decay_rate=0.96,
    staircase=True)
    
# Compile the model
quant_aware_model.compile(optimizer=tf.keras.optimizers.Lion(lr=lr_schedule), 
              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=args.label_smoothing),
              metrics=['accuracy']
              )
```

### 3. Failure after conversion
model successfully converted into tflite with the example code flow of quantization aware training,
but the test accuracy for tflite model is so low (~80%) compared to .pb model file. (~97%)

Before adopting label smoothing, quantization aware trained model (tflite) was working as good as original .pb model.

Is there any relationship between label smoothing and quantization aware training process?

Thanks!
"
61781,Can't run bert_vocab_from_dataset without TypeError: Tensor is unhashable,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

This is the code from you [manual ](https://www.tensorflow.org/text/guide/subwords_tokenizer#generate_the_vocabulary)and I really don't understans that I get this error. Why is it? 

If I add 
tf.compat.v1.disable_eager_execution()
tf.compat.v1.disable_v2_behavior()

I get
RuntimeError: input_dataset: Attempting to capture an EagerTensor without building a function.

### Standalone code to reproduce the issue

```shell
data = tf.data.TextLineDataset([SENTENCES_PATH, TAGS_PATH])

from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab

tokens = bert_vocab.bert_vocab_from_dataset(
    data,
    # The target vocabulary size
    vocab_size = 50000,
    # Reserved tokens that must be included in the vocabulary
    reserved_tokens=[""[PAD]"", ""[UNK]"", ""[START]"", ""[END]""],
    # Arguments for `text.BertTokenizer`
    bert_tokenizer_params=dict(lower_case=True),
    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`
    learn_params={},
)
```


### Relevant log output

```shell
TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.
```
"
61780,Process aborted when running `tf.gather` and `tf.compat.v1.gather` on GPU with large parameters and indices,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0-rc1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Process aborted when running `tf.gather` and `tf.compat.v1.gather` on GPU with certain parameters and indices, while it throws an Exception on CPU.
It always happens on GPU favor when the `params` and `indices` are large, regardless of the `validate_indices` value.
Besides, the Check failed error still occurs when `validate_indices` is set to `True`, and it should be an Exception instead.
I think the root cause of this behavior should be related to the bug in #60276 and #60756, which face a Check failed error in `tf.raw_ops.Gather/V2` op.


### Standalone code to reproduce the issue

```python
import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION, flush=True)

try:
    validate_indices = False # True
    params = tf.saturate_cast(tf.random.uniform([13, 15, 7, 13, 14], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.half)
    indices = tf.saturate_cast(tf.random.uniform([11, 12, 6, 15, 11, 3], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.int64)
    res = tf.gather(
        validate_indices=validate_indices,
        params=params,
        indices=indices,
    )
    # res = tf.compat.v1.gather(
    #     validate_indices=validate_indices,
    #     params=params,
    #     indices=indices,
    # )
except Exception as e:
    print(""Error:"", str(e), flush=True)
print(""Success!"", flush=True)
```


### Relevant log output

On GPU, it throws a Check failed and core dumped.

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
2023-09-03 13:08:24.487989: F ./tensorflow/core/util/gpu_launch_config.h:160] Check failed: work_element_count >= 0 (0 vs. -549025096)
Aborted (core dumped)
```

On CPU, it throws a Exception.

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
Error: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[7,11,2,8,5,1] = 424 is not in [0, 13) [Op:GatherV2] name: 
Success!
```
"
61779,The model does not save and load correctly when containing `tf.keras.layers.experimental.preprocessing.StringLookup` layer,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0-rc1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The model does not save and load correctly when containing `tf.keras.layers.experimental.preprocessing.StringLookup` layer.
It seems that the `vocabulary` is not saved or loaded correctly, which is empty when loading the model.
This behavior may relate to #61369, but different API endpoint.


### Standalone code to reproduce the issue

```python
import pickle
import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION, flush=True)

model_input = tf.keras.Input(shape=(1,), dtype=tf.int64)
lookup = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['a', 'b'])(model_input)
output = tf.keras.layers.Dense(10)(lookup)
full_model = tf.keras.Model(model_input, output)

# this part works
try:
    model_bytes = pickle.dumps(full_model)
    model_recovered = pickle.loads(model_bytes)
except Exception as e:
    print(""Failed! Error:"", e, flush=True)
else:
    print(""Success!"", flush=True)

# this part throws an error
try:
    full_model.save(""/tmp/temp_model"")
    full_model_loaded = tf.keras.models.load_model(""/tmp/temp_model"")
    model_bytes = pickle.dumps(full_model_loaded)
    model_recovered = pickle.loads(model_bytes)
except Exception as e:
    print(""Failed! Error:"", e, flush=True)
else:
    print(""Success!"", flush=True)
```


### Relevant log output

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
Success!
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
Failed! Error: Error when deserializing class 'StringLookup' using config={'name': 'string_lookup', 'trainable': True, 'dtype': 'int64', 'invert': False, 'max_tokens': None, 'num_oov_indices': 1, 'oov_token': '[UNK]', 'mask_token': None, 'output_mode': 'int', 'sparse': False, 'pad_to_max_tokens': False, 'idf_weights': None, 'vocabulary': [], 'vocabulary_size': 3, 'encoding': 'utf-8'}.

Exception encountered: Cannot set an empty vocabulary, you passed [].
```
"
61778,Process aborted when running `tf.keras.layers.MaxPooling1D` on GPU with large `pool_size`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0-rc1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Process aborted with Check failed error when running `tf.keras.layers.MaxPooling1D` on GPU with large `pool_size`, while it throws an Exception on CPU.
When I reduce the `pool_size_0` to `1e+18`, it throws an Exception on GPU, which is a more reasonable behavior I think.
This behavior is similar to what described in #61642, which also throws a Check failed error by using a large `pool_size` for `tf.compat.v1.layers.MaxPooling1D` on GPU.


### Standalone code to reproduce the issue

```python
import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION, flush=True)

try:
    pool_size_0 = 1e+19
    pool_size = [pool_size_0,]
    strides_0 = 2
    strides = [strides_0,]
    padding = ""same""
    data_format = ""channels_last""
    arg_class = tf.keras.layers.MaxPooling1D(pool_size=pool_size,strides=strides,padding=padding,data_format=data_format,)
    # arg_class = tf.keras.layers.MaxPool1D(pool_size=pool_size,strides=strides,padding=padding,data_format=data_format,)
    # arg_class = tf.compat.v1.keras.layers.MaxPooling1D(pool_size=pool_size,strides=strides,padding=padding,data_format=data_format,)
    # arg_class = tf.compat.v1.keras.layers.MaxPool1D(pool_size=pool_size,strides=strides,padding=padding,data_format=data_format,)
    arg_input_0_tensor = tf.random.uniform([1, 5, 4], dtype=tf.float32)
    arg_input_0 = tf.identity(arg_input_0_tensor)
    arg_input = [arg_input_0,]
    out = arg_class(*arg_input)
except Exception as e:
    print(""Error:"", str(e), flush=True)
print(""Success!"", flush=True)
```


### Relevant log output

On GPU, it throws a Check failed and core dumped.

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
2023-09-03 11:34:09.267078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600
2023-09-03 11:34:09.267132: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:1019] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
Aborted (core dumped)
```

On CPU, it throws a Exception.

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
Error: Exception encountered when calling layer 'max_pooling1d' (type MaxPooling1D).

{{function_node __wrapped__MaxPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} Sliding window ksize for dimension 1 was zero. [Op:MaxPool]

Call arguments received by layer 'max_pooling1d' (type MaxPooling1D):
  â€¢ inputs=tf.Tensor(shape=(1, 5, 4), dtype=float32)
Success!
```

When I change the `pool_size_0` to `1e+18`, it throws a Exception on GPU.

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
2023-09-03 11:51:28.441325: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at maxpooling_op.cc:1260 : INVALID_ARGUMENT: Attr ksize has value 1000000000000000000 out of range for an int32
Error: Exception encountered when calling layer 'max_pooling1d' (type MaxPooling1D).

{{function_node __wrapped__MaxPool_device_/job:localhost/replica:0/task:0/device:GPU:0}} Attr ksize has value 1000000000000000000 out of range for an int32 [Op:MaxPool]

Call arguments received by layer 'max_pooling1d' (type MaxPooling1D):
  â€¢ inputs=tf.Tensor(shape=(1, 5, 4), dtype=float32)
Success!
```
"
61777,Add support for fftfreq() and rfftfreq() helper functions in tf.signal(),"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:No
-   **TensorFlow version (use command below)**: 2.12.0
-   **Python version**: 3.10.* 

### Describe the feature and the current behavior/state.
The current version of TensorFlow's tf.signal module provides extensive support for various Fourier Transform functions such as fft() and rfft(). However, it does not include helper functions like fftfreq() and rfftfreq() available in other libraries, such as NumPy and PyTorch. These functions are used to compute the discrete Fourier Transform sample frequencies for a signal of a given size, which is a common requirement in many signal-processing tasks.

Currently, to achieve similar functionality, users have to define custom functions or import existing functions from other libraries, such as SciPy. This process requires converting TensorFlow tensors to and from the other library's format, which may not always be efficient or convenient.

### Who will benefit from this feature?
Users who are working on signal processing tasks using TensorFlow will benefit from this feature as they won't have to switch to other libraries (like NumPy or PyTorch) to compute the Fourier Transform frequencies. This will make their code more consistent and potentially more efficient.

### Additional Info.
Adding these functions will make the tf.signal module more complete and competitive with other libraries' offerings in terms of signal processing capabilities. It'll also make TensorFlow more user-friendly for those who are accustomed to these functions in other libraries."
61775,What is generate_vocab func? ,"You referenced in [this ](https://www.tensorflow.org/text/guide/subwords_tokenizer#generate_the_vocabulary)tutorial to [generate_vocab.py](https://github.com/tensorflow/text/blob/master/tensorflow_text/tools/wordpiece_vocab/generate_vocab.py), if I understand correct, as a ready to prod highlevel func that I can use. But I don't have it in downloaded repository of tensorflow-text. 

Can you explain me a bit more how should be my attitude to this reference? 
"
61774,FAILED: //tensorflow/tools/proto_splitter/cc:saved_model_splitter_test on mac arm64 wheels,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14rc0

### Custom code

No

### OS platform and distribution

macOS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Test is failing.
This checkin the regression started:

```
commit e1e4de39de51064a359320f9b32936bf1599d4a0Author: Laura Pak <lpak@google.com>Date: Â  Tue Apr 26 16:51:30 2022 -0700Â  Â  Update highwayhash from fd3d9af80465e4383162e4a7c5e2f406e82dd968 to c13d28517a4db259d738ea4886b1f00352a3cc33.Â  Â  PiperOrigin-RevId: 444703993
```


Reverting that fixes the issue.

### Standalone code to reproduce the issue

```shell

bazel --bazelrc='./tensorflow/tools/ci_build/osx/arm64/.macos.bazelrc'  test //tensorflow/tools/proto_splitter/cc:saved_model_splitter_test
```


### Relevant log output

```shell
The test is failing with:

INFO: From Testing //tensorflow/tools/proto_splitter/cc:saved_model_splitter_test:==================== Test output for //tensorflow/tools/proto_splitter/cc:saved_model_splitter_test:dyld[1906]: symbol not found in flat namespace 

'__ZNK11highwayhash11HighwayHashILj16EEclERA4_KyPKcmPy'==================================================================================================== 

Test output for //tensorflow/tools/proto_splitter/cc:saved_model_splitter_test:dyld[1942]: symbol not found in flat namespace '__ZNK11highwayhash11HighwayHashILj16EEclERA4_KyPKcmPy'==================================================================================================== Test output for //tensorflow/tools/proto_splitter/cc:saved_model_splitter_test:dyld[1976]: symbol not found in flat namespace '__ZNK11highwayhash11HighwayHashILj16EEclERA4_KyPKcmPy'================================================================================
```
```


cc @learning-to-play , @nitins17 and @mihaimaruseac 
"
61772,GPU - unable minimize loss / CPU - behaves normal,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

OSX 12.4

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

With all other factors the same, GPU does not minimize loss and CPU does.
![GPU_behavoir](https://github.com/tensorflow/tensorflow/assets/130612303/33ff3317-94e2-48f0-9f5c-dadedc48e54c)
![CPU_behavior](https://github.com/tensorflow/tensorflow/assets/130612303/28b04dc4-2271-497f-a285-80b87835a29e)


### Standalone code to reproduce the issue

```shell
link to ipynb: https://storage.googleapis.com/codyfalkosky/TensorFlowIssue/strange%20M1%20GPU%20Behavior.ipynb
```


### Relevant log output

_No response_"
61771,Float16 mixed precision training,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

Python 3.10.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

A6000

### Current behavior?

Enabled float16 training by setting the mixed precision policy, but why I still need to manually cast the y tensors to float16 before calculating the loss?


Error when no manual cast the tensor:
![image](https://github.com/tensorflow/tensorflow/assets/25906607/46c5efcb-f2c2-4b7a-9483-46656009071b)


### Standalone code to reproduce the issue

```shell
Confidential.
```


### Relevant log output

_No response_"
61770,I am not able to install the correct version for typing-extensions,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Their are different support problems when I install tensorflow

I installed it by entering
`pip install tensorflow`

The output was

> Collecting tensorflow
  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/9e/b8/ed5f794359d05cd0bffb894c6418da87b93016ee17b669d55c45d1bd5d5b/tensorflow-2.13.0-cp311-cp311-win_amd64.whl.metadata
  Downloading tensorflow-2.13.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)
Collecting tensorflow-intel==2.13.0 (from tensorflow)
  Obtaining dependency information for tensorflow-intel==2.13.0 from https://files.pythonhosted.org/packages/2f/2f/3c84f675931ce3bcbc7e23acbba1e5d7f05ce769adab48322de57a9f5928/tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl.metadata
  Downloading tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)
Collecting absl-py>=1.0.0 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)
Collecting astunparse>=1.6.0 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Collecting flatbuffers>=23.1.21 (from tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for flatbuffers>=23.1.21 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata
  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)
Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)
Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)
Collecting h5py>=2.9.0 (from tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/d1/93/0f4cf5058095d749d464e4f770d2bf339930e5f3374331f0d2fa6ddfbf28/h5py-3.9.0-cp311-cp311-win_amd64.whl.metadata
  Downloading h5py-3.9.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)
Collecting libclang>=13.0.0 (from tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/02/8c/dc970bc00867fe290e8c8a7befa1635af716a9ebdfe3fb9dce0ca4b522ce/libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata
  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)
Collecting numpy<=1.24.3,>=1.22 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached numpy-1.24.3-cp311-cp311-win_amd64.whl (14.8 MB)
Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)
Requirement already satisfied: packaging in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.1)
Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/14/ff/10f746c03212fe48576b2c0f5ada73c3400b6d90f769728c4f07656d8b27/protobuf-4.24.2-cp310-abi3-win_amd64.whl.metadata
  Downloading protobuf-4.24.2-cp310-abi3-win_amd64.whl.metadata (540 bytes)
Requirement already satisfied: setuptools in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from tensorflow-intel==2.13.0->tensorflow) (65.5.0)
Requirement already satisfied: six>=1.12.0 in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)
Collecting termcolor>=1.1.0 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)
Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)
Collecting wrapt>=1.11.0 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached wrapt-1.15.0-cp311-cp311-win_amd64.whl (36 kB)
Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/8d/58/ede228c07bdf3780c5332660c89f3c7a37fe8bfb9bd73a97ad2614420bd4/grpcio-1.57.0-cp311-cp311-win_amd64.whl.metadata
  Downloading grpcio-1.57.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)
Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)
Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for tensorflow-estimator<2.14,>=2.13.0 from https://files.pythonhosted.org/packages/72/5c/c318268d96791c6222ad7df1651bbd1b2409139afeb6f468c0f327177016/tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata
  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata
  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)
Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)
Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for wheel<1.0,>=0.23.0 from https://files.pythonhosted.org/packages/b8/8b/31273bf66016be6ad22bb7345c37ff350276cfd46e389a0c2ac5da9d9073/wheel-0.41.2-py3-none-any.whl.metadata
  Using cached wheel-0.41.2-py3-none-any.whl.metadata (2.2 kB)
Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl.metadata
  Downloading google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)
Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)
Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata
  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)
Requirement already satisfied: requests<3,>=2.21.0 in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.31.0)
Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/da/61/6e9ff8258422d287eec718872fb71e05324356722ab658c8afda25f51539/tensorboard_data_server-0.7.1-py3-none-any.whl.metadata
  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl.metadata (1.1 kB)
Requirement already satisfied: werkzeug>=1.0.1 in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.3.7)
Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata
  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)
Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)
Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Using cached rsa-4.9-py3-none-any.whl (34 kB)
Collecting urllib3<2.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for urllib3<2.0 from https://files.pythonhosted.org/packages/c5/05/c214b32d21c0b465506f95c4f28ccbcba15022e000b043b72b3df7728471/urllib3-1.26.16-py2.py3-none-any.whl.metadata
  Downloading urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)
     ---------------------------------------- 48.4/48.4 kB 1.2 MB/s eta 0:00:00
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.0)
Requirement already satisfied: idna<4,>=2.5 in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2023.7.22)
Requirement already satisfied: MarkupSafe>=2.1.1 in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.3)
Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)
Using cached tensorflow-2.13.0-cp311-cp311-win_amd64.whl (1.9 kB)
Using cached tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl (276.6 MB)
Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)
Using cached grpcio-1.57.0-cp311-cp311-win_amd64.whl (4.3 MB)
Using cached h5py-3.9.0-cp311-cp311-win_amd64.whl (2.7 MB)
Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)
Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)
Using cached protobuf-4.24.2-cp310-abi3-win_amd64.whl (430 kB)
Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)
Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB)
Using cached Markdown-3.4.4-py3-none-any.whl (94 kB)
Using cached tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)
Using cached wheel-0.41.2-py3-none-any.whl (64 kB)
Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)
Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)
Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, oauthlib, numpy, markdown, keras, grpcio, google-pasta, gast, cachetools, absl-py, rsa, pyasn1-modules, opt-einsum, h5py, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow
  Attempting uninstall: urllib3
    Found existing installation: urllib3 2.0.4
    Uninstalling urllib3-2.0.4:
      Successfully uninstalled urllib3-2.0.4
  Attempting uninstall: typing-extensions
    Found existing installation: typing_extensions 4.7.1
    Uninstalling typing_extensions-4.7.1:
      Successfully uninstalled typing_extensions-4.7.1
  Attempting uninstall: numpy
    Found existing installation: numpy 1.25.2
    Uninstalling numpy-1.25.2:
      Successfully uninstalled numpy-1.25.2
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
pydantic 2.3.0 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.
pydantic-core 2.6.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.
Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.57.0 h5py-3.9.0 keras-2.13.1 libclang-16.0.6 markdown-3.4.4 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.24.2 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 typing-extensions-4.5.0 urllib3-1.26.16 wheel-0.41.2 wrapt-1.15.0

Due to this error, I installed typing-extensions 4.6.1 by
`pip install typing-extensions==4.6.1`

I got the output as

> Collecting typing-extensions==4.6.1
  Obtaining dependency information for typing-extensions==4.6.1 from https://files.pythonhosted.org/packages/82/ed/8ccf53a0ed10bf8fc8877b5833b40f5f99093cadfe6632b8892f74aead0f/typing_extensions-4.6.1-py3-none-any.whl.metadata
  Downloading typing_extensions-4.6.1-py3-none-any.whl.metadata (2.8 kB)
Downloading typing_extensions-4.6.1-py3-none-any.whl (31 kB)
Installing collected packages: typing-extensions
  Attempting uninstall: typing-extensions
    Found existing installation: typing_extensions 4.5.0
    Uninstalling typing_extensions-4.5.0:
      Successfully uninstalled typing_extensions-4.5.0
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.6.1 which is incompatible.
Successfully installed typing-extensions-4.6.1

Then, I again reinstalled typing-extensions 4.5.0 by
`pip install typing-extensions==4.5.0`

Then I again received the error

> Collecting typing-extensions==4.5.0
  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)
Installing collected packages: typing-extensions
  Attempting uninstall: typing-extensions
    Found existing installation: typing_extensions 4.6.1
    Uninstalling typing_extensions-4.6.1:
      Successfully uninstalled typing_extensions-4.6.1
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
pydantic 2.3.0 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.
pydantic-core 2.6.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.
Successfully installed typing-extensions-4.5.0

Please let me know how to fix this error

### Standalone code to reproduce the issue

```shell
Just use 
pip install tensorflow 
and you may receive this error
```


### Relevant log output

_No response_"
61769,AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '__dlpack__',"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.12.0

### Custom code

Yes

### OS platform and distribution

google colab

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

it looks like tensorflow tesnors doesn't support `__dlpack__` and `__dlpack_device__` dunders. Not sure, If it's already something down the road map. If not, can we add these dunders to tf tensors to conform array_api_standrads?

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tensor = tf.constant([1,3,2])
tensor.__dlpack__()
```


### Relevant log output

```shell
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
[<ipython-input-3-016b1ec8a3b2>](https://localhost:8080/#) in <cell line: 1>()
----> 1 tensor.__dlpack__()

[/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py](https://localhost:8080/#) in __getattr__(self, name)
    441         np_config.enable_numpy_behavior()
    442       """""")
--> 443     self.__getattribute__(name)
    444 
    445   @staticmethod

AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '__dlpack__'
```
"
61766,Android ,"
- Android Device information (use `adb shell getprop ro.build.
- Google Play Services version
  Settings

"
61765,"Tensorflow Profiler with 2.13. cuptiGetTimestamp: error 999, not reporting events, no data to show...","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Ubuntu 22

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8, 8.6

### GPU model and memory

RTX A6000, quatro RTX 8000

### Current behavior?

I'm trying to setup profiler for my trainer, using TF.data from TFRecords, but otherwise training with standard keras code and MirroredStrategy.

I've noticed some strange errors after adding `profile_batch=5` to my tensorboard callback. I've attached logs below.

It's worth noting that I started getting `Local randezvous recv item cancelled` as well after upgrade to 2.13 from 2.12.

I have conda environment with cudatoolkit==11.8.0, cuda-nvcc, and pip installed nvidia-cudnn-cu11==8.6.0.163, tensorflow==2.13.*, tensorboard_plugin_profile... Path to CUPTI should be setup well, as it's in the same path as cudatoolkit installed libcuda

### Standalone code to reproduce the issue

```shell
log_dir = ""logs/fit/"" + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=5)
```


### Relevant log output

```shell
2023-08-31 19:57:55.092538: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2023-08-31 19:57:55.092572: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2023-08-31 19:57:55.092639: I tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1671] Profiler found 8 GPUs
2023-08-31 19:57:55.093149: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:137] cuptiGetTimestamp: error 999: 
2023-08-31 19:57:55.093178: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:186] cuptiSubscribe: ignored due to a previous error.
2023-08-31 19:57:55.093189: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error.
2023-08-31 19:57:55.093200: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1723] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error 
2023-08-31 19:57:55.093257: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2023-08-31 19:57:55.093422: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:142] cuptiFinalize: ignored due to a previous error.
2023-08-31 19:57:55.093435: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error.
2023-08-31 19:57:55.093445: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1814] function cupti_interface_->Finalize()failed with error 
Epoch 1/100
...
2023-08-31 19:59:33.523444: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
      4/Unknown - 109s 3s/step - loss: 0.7206 - binary_accuracy: 0.5451 - f1_score: 0.1138 - auc: 0.49592023-08-31 19:59:44.532963: I tensorflow/tsl/profiler/lib/profiler_session.cc:10
4] Profiler session initializing.             
2023-08-31 19:59:44.533038: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.                                                                            
2023-08-31 19:59:44.533099: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.
2023-08-31 19:59:44.533216: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:186] cuptiSubscribe: ignored due to a previous error.
2023-08-31 19:59:44.533247: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error.
2023-08-31 19:59:44.533273: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1723] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, t
his)failed with error                         
2023-08-31 19:59:48.299203: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.                                                                     
2023-08-31 19:59:48.352778: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:142] cuptiFinalize: ignored due to a previous error.
2023-08-31 19:59:48.352844: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error.
2023-08-31 19:59:48.352869: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1814] function cupti_interface_->Finalize()failed with error 
2023-08-31 19:59:49.390774: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.
2023-08-31 19:59:49.390830: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.
2023-08-31 19:59:49.390843: I tensorflow/compiler/xla/backends/profiler/gpu/cupti_collector.cc:541]  GpuTracer has collected 0 callback api events and 0 activity events. 
2023-08-31 19:59:49.413800: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.                                                                          
2023-08-31 19:59:49.415262: I tensorflow/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logs/fit/20230831-195755/plugins/profile/2023_08_31_19_59_49/****.net.xplane.pb
```
"
61764,Internal interfaces could be explicitly marked,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.12.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

There are internal interfaces within the tensorflow package that I think would benefit from being marked more explicitly. According to [the PEP 8 style guide](https://peps.python.org/pep-0008/#public-and-internal-interfaces), *""Even with `__all__` set appropriately, internal interfaces (packages, modules, classes, functions, attributes or other names) should still be prefixed with a single leading underscore.""*

While following all of PEP 8 to the letter is perhaps understandably not always adhered to in all projects, this particular section seems very reasonable. You can have no documentation for a module, and exclude it from `__all__`, both of which already indicate the module is not intended to be public, but it is still recommended to explicitly mark that module as private. Because this is commonly adhered to in the python ecosystem, some users may even expect modules not marked private to be intentionally public.

There exist old issues such as [this one](https://github.com/tensorflow/tensorflow/issues/33075) wherein it is stated that the only supported way to import anything is `import tensorflow` or possibly `from tensorflow.keras import ...`; and that `tensorflow.python` for example (or *""any other modules""*) is unsupported. To quote directly from that thread:

> so to confirm, `tf.python.keras` is *private*, intended for *development*, rather than public use? Thanks

> Yes, that's exactly the case. Anything under `tf.python` is private

If this module is intended to be private, then to be in accordance with the above guidelines, the module should be renamed from `python` to `_python`, as should all other internal interfaces. There seem several advantages to doing so: a user knows instantly that importing from `package._module` is dangerous (even if python technically permits it); an IDE may also be configured to treat such modules differently, such as excluding their contents from being displayed to the user in autocompletion, search results, etc. Doing so would also help prevent users such as the one in the issue above wondering why they encountered problems while using what on the surface looks like a public interface.

### Standalone code to reproduce the issue

```shell
Note that `import tensorflow.python` is possible, where we might expect `import tensorflow._python` indicating this is an internal interface not intended for the user to access. Running code with this import as a user would thus have unintended consequences.
```


### Relevant log output

_No response_"
61761,No quantized type support for Enter/Exit Op,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0rc0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Quantized types such as QINT8/QUINT8 are not registered for Enter/Exit.

Furthermore, why are some Control Flow Ops registered on `CPU Device` (e.g. `Merge/Switch/LoopCond`) but others not (e.g. `Enter/Exit/NextIteration`) ? 

Are these expected?

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

x = tf.constant(np.ones((5, 6)), dtype=tf.qint8)
enter = tf.raw_ops.Enter(data=x, frame_name=""test"", is_constant=True)
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node Enter}} = Enter[T=DT_QINT8, frame_name=""test"", is_constant=true, parallel_iterations=10]
All kernels registered for op Enter:
  device='DEFAULT'; T in [DT_RESOURCE]
  device='DEFAULT'; T in [DT_STRING]
  device='DEFAULT'; T in [DT_INT32]
  device='DEFAULT'; T in [DT_VARIANT]
  device='DEFAULT'; T in [DT_BOOL]
  device='DEFAULT'; T in [DT_COMPLEX128]
  device='DEFAULT'; T in [DT_COMPLEX64]
  device='DEFAULT'; T in [DT_INT8]
  device='DEFAULT'; T in [DT_UINT8]
  device='DEFAULT'; T in [DT_INT16]
  device='DEFAULT'; T in [DT_UINT16]
  device='DEFAULT'; T in [DT_UINT32]
  device='DEFAULT'; T in [DT_INT64]
  device='DEFAULT'; T in [DT_UINT64]
  device='DEFAULT'; T in [DT_DOUBLE]
  device='DEFAULT'; T in [DT_FLOAT]
  device='DEFAULT'; T in [DT_BFLOAT16]
  device='DEFAULT'; T in [DT_HALF]
  device='GPU'; T in [DT_RESOURCE]
  device='GPU'; T in [DT_STRING]
  device='GPU'; T in [DT_INT32]
  device='GPU'; T in [DT_VARIANT]
  device='GPU'; T in [DT_BOOL]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_INT8]
  device='GPU'; T in [DT_UINT8]
  device='GPU'; T in [DT_INT16]
  device='GPU'; T in [DT_UINT16]
  device='GPU'; T in [DT_UINT32]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_UINT64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_BFLOAT16]
  device='GPU'; T in [DT_HALF]
  device='TPU'
  device='TPU_SYSTEM'
```
"
61760,Full integer quantization not possible with grouped convolution,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 12.5
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

```
train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=""training"",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

def representative_data_gen():
  for input_value, labels in train_ds:
    yield [input_value]

converter = tf.lite.TFLiteConverter.from_saved_model('./model.pb')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen

tflite_model = converter.convert()

with open('model.tflite', 'wb') as f:
    f.write(tflite_model)
```

The error that I get is 

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[15], line 8
      3 #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]
      4 #converter.inference_input_type = tf.int8
      5 #converter.inference_output_type = tf.int8
      6 converter.representative_dataset = representative_data_gen
----> 8 tflite_model = converter.convert()
     10 with open('model.tflite', 'wb') as f:
     11     f.write(tflite_model)

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1065, in _export_metrics.<locals>.wrapper(self, *args, **kwargs)
   1062 @functools.wraps(convert_func)
   1063 def wrapper(self, *args, **kwargs):
   1064   # pylint: disable=protected-access
-> 1065   return self._convert_and_export_metrics(convert_func, *args, **kwargs)

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1042, in TFLiteConverterBase._convert_and_export_metrics(self, convert_func, *args, **kwargs)
   1040 self._save_conversion_params_metric()
   1041 start_time = time.process_time()
-> 1042 result = convert_func(self, *args, **kwargs)
   1043 elapsed_time_ms = (time.process_time() - start_time) * 1000
   1044 if result:

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1390, in TFLiteSavedModelConverterV2.convert(self)
   1384 else:
   1385   self._debug_info = _get_debug_info(
   1386       _convert_debug_info_func(self._trackable_obj.graph_debug_info),
   1387       graph_def,
   1388   )
-> 1390 return self._convert_from_saved_model(graph_def)

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1257, in TFLiteConverterBaseV2._convert_from_saved_model(self, graph_def)
   1254 converter_kwargs.update(quant_mode.converter_flags())
   1256 result = _convert_saved_model(**converter_kwargs)
-> 1257 return self._optimize_tflite_model(
   1258     result, quant_mode, quant_io=self.experimental_new_quantizer
   1259 )

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:215, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)
    213 except Exception as error:
    214   report_error_message(str(error))
--> 215   raise error from None

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)
    202 @functools.wraps(func)
    203 def wrapper(*args, **kwargs):
    204   try:
--> 205     return func(*args, **kwargs)
    206   except ConverterError as converter_error:
    207     if converter_error.errors:

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:991, in TFLiteConverterBase._optimize_tflite_model(self, model, quant_mode, quant_io)
    989   q_allow_float = quant_mode.is_allow_float()
    990   q_variable_quantization = quant_mode.enable_mlir_variable_quantization
--> 991   model = self._quantize(
    992       model,
    993       q_in_type,
    994       q_out_type,
    995       q_activations_type,
    996       q_bias_type,
    997       q_allow_float,
    998       q_variable_quantization,
    999   )
   1001 m_in_type = in_type if in_type else _dtypes.float32
   1002 m_out_type = out_type if out_type else _dtypes.float32

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:710, in TFLiteConverterBase._quantize(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization)
    706 calibrate_quantize = _calibrator.Calibrator(
    707     result, custom_op_registerers_by_name, custom_op_registerers_by_func
    708 )
    709 if self._experimental_calibrate_only or self.experimental_new_quantizer:
--> 710   calibrated = calibrate_quantize.calibrate(
    711       self.representative_dataset.input_gen
    712   )
    714 if self._experimental_calibrate_only:
    715   return calibrated

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:215, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)
    213 except Exception as error:
    214   report_error_message(str(error))
--> 215   raise error from None

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)
    202 @functools.wraps(func)
    203 def wrapper(*args, **kwargs):
    204   try:
--> 205     return func(*args, **kwargs)
    206   except ConverterError as converter_error:
    207     if converter_error.errors:

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py:254, in Calibrator.calibrate(self, dataset_gen)
    244 @convert_phase(Component.OPTIMIZE_TFLITE_MODEL, SubComponent.CALIBRATE)
    245 def calibrate(self, dataset_gen):
    246   """"""Calibrates the model with specified generator.
    247 
    248   Returns:
   (...)
    252     dataset_gen: A generator that generates calibration samples.
    253   """"""
--> 254   self._feed_tensors(dataset_gen, resize_input=True)
    255   return self._calibrator.Calibrate()

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py:143, in Calibrator._feed_tensors(self, dataset_gen, resize_input)
    139     self._calibrator.Prepare(
    140         [list(s.shape) for s in input_array], signature_key
    141     )
    142   else:
--> 143     self._calibrator.Prepare([list(s.shape) for s in input_array])
    144 else:
    145   if signature_key is not None:

RuntimeError: tensorflow/lite/kernels/conv.cc:352 input_channel % filter_input_channel != 0 (2 != 0)Node number 6 (CONV_2D) failed to prepare.
```

The model itself got converted from pytorch to onnx and then to pb. The issue here is that the model has a grouped convolutional layers. This is fixed for dynamic range quantisation but for full integer quantisation using a representative dataset this still seems to fail. Any quick fix possible?
"
61755,"XLA compiled tf.where with known output shape error (set_shape, tf.concat / tf.stack)","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Output of `tf.where`, when used inside `tf.function` with `jit_compile=True`, can sometimes be used correctly (as with sum), and sometimes raises shape mismatch error (as with concatenation). This error is present even if output shape is set manually with `set_shape`. 

The code below runs without `jit_compile` or with sum instead of `tf.concat`, and only fails if concatenating inside a compiled function. 

Note: `autoclustering` solves the issue on the toy example, but _not_ on the codebase I am working on.

### Standalone code to reproduce the issue

Colab: https://colab.research.google.com/drive/1FuboVMSao8eCZLcZ2F7Fdsa1UvlnHQmG?usp=sharing

```python
import tensorflow as tf

def fun(x, y):
    x = tf.where(x == 1)
    print(f'Shape before (unknown): {x.shape}') 
    x.set_shape(shape=[y.shape[0], 2])
    print(f'Shape after (known): {x.shape}') 
    return tf.concat([x,y], axis=1)  # Concatentation fails
    # return x + y  # Sum would succeed

x = tf.constant([[0,0,1,1,0],
                  [0,1,0,1,0],
                  [1,0,0,0,1],
                  [1,0,1,0,0],
                  [0,1,1,0,0],], dtype=tf.int32)
y = tf.expand_dims(tf.range(x.shape[0] * 2, dtype=tf.int64), axis=-1)

fun(x, y)

tf.function(fun)(x,y)

tf.function(fun, jit_compile=True)(x,y)  # Fails as described above
```


### Relevant log output

```shell
Shape before (unknown): (None, 2)
Shape after (known): (10, 2)
InvalidArgumentError: Cannot concatenate arrays that differ in dimensions other than the one being concatenated. Dimension 0 in both shapes must be equal: s64[<=25,2] vs s64[10,1].
```
"
61753,Memory leak using SavedModel format,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12 / 2.13

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Simply, I did

- Build the model (`model`)
- Save it using (a). `model.save(my_model)` , (b). `model.save_weights(my_model.h5)`.

Next, 


## Case 1

1. Using `model.load_weight(my_model.h5)`, and pass `jit_compile=True` followed by `model.fit`. It works, though 11GB consumed out of 16GB. 
2. Using `tf.keras.load_model(``my_model`), and pass `jit_compile=True`, followed by `model.fit`. It doesn't work, simply extremly slow and consumed 15.8GB instantly. Sudden jump from 3GB to 15.8GB. But without passing `jit_compile`, it runs normally but memory consumption remains same. 

## Case 2

1. If I initialize model within `with strategy.scope():`, and try to save the weight (by `model.save_weight`), it gives `OSError: Unable to create link (name already exists)` but without strategy scope, this error doesn't occur.

### Standalone code to reproduce the issue

```shell
to do.
```


### Relevant log output

_No response_"
61746,incompatible shapes error,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm trying to train a model to do binary classification.  I am getting an error about incompatible shapes, but my data seems to be in the correct shape. 









































































































































































































































































































































































### Standalone code to reproduce the issue

```shell
!pip install transformers

import tensorflow as tf
import tensorflow_hub as hub
from transformers import LongformerTokenizer, TFLongformerForSequenceClassification
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import LearningRateScheduler
import math
import numpy as np

positive_sentences = [
    ""The weather is absolutely gorgeous today."",
    ""I'm so grateful for the support of my friends and family."",
    ""I achieved my personal best in the race!"",
    ""The new cafe in town serves amazing coffee."",
    ""I love spending time with my adorable pets."",
    ""I received a surprise gift from a dear friend."",
    ""The sunrise this morning was breathtaking."",
    ""I'm excited about the upcoming vacation."",
    ""The concert last night was incredibly entertaining."",
    ""I'm proud of my hard work paying off."",
    ""The park is a peaceful place to relax."",
    ""I found a great book that I can't put down."",
    ""The team's collaboration led to a successful project."",
    ""I'm enjoying learning a new skill."",
    ""Spending time with loved ones always brightens my day."",
    ""I got a promotion at work, and it's a fantastic feeling."",
    ""The movie I watched last night was heartwarming."",
    ""I'm making positive changes in my daily routine."",
    ""The delicious aroma of home-cooked food fills the air."",
    ""I'm surrounded by inspiring and supportive people.""
]

negative_sentences = [
    ""The constant rain is making me feel gloomy."",
    ""I'm disappointed that my plans got canceled."",
    ""The traffic was horrendous this morning."",
    ""I made a mistake on the important presentation."",
    ""I'm feeling overwhelmed with work and tasks."",
    ""The internet connection is frustratingly slow."",
    ""The food I ordered was cold and tasteless."",
    ""I'm exhausted after a long and stressful day."",
    ""My phone battery died at the worst time."",
    ""The store was out of stock of the item I needed."",
    ""I lost my wallet and it's been a hassle."",
    ""The loud construction noise is giving me a headache."",
    ""I'm struggling to meet my deadlines."",
    ""The movie I was looking forward to was disappointing."",
    ""I'm not feeling well and it's affecting my mood."",
    ""My computer crashed and I lost my unsaved work."",
    ""The rude customer service ruined my experience."",
    ""I'm frustrated with the constant delays."",
    ""The rainy weather is putting me in a bad mood."",
    ""I'm stressed about the upcoming exams.""
]

sentences = positive_sentences + negative_sentences

labels = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
    ,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
]

df = pd.DataFrame({'sentences':sentences,'labels':labels})

data = df.copy()

display(data.head(5))

tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')
longformer_model = TFLongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096')

X_train, X_test, y_train, y_test = train_test_split(df['sentences'],df['labels'], stratify=df['labels'])
X_train.head(4)
display(y_train)

x_train_tokens = tokenizer(list(X_train), padding=True, truncation=True, return_tensors=""tf"")
x_test_tokens = tokenizer(list(X_test), padding=True, truncation=True, return_tensors=""tf"")

y_train_encoded = np.array(y_train)
y_test_encoded = np.array(y_test)

display(x_train_tokens)

input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32)
attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32)
outputs = longformer_model(input_ids, attention_mask=attention_mask)[0]
output_layer = tf.keras.layers.Dense(1, activation='sigmoid', name=""output"")(outputs)

model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=[output_layer])

def lr_schedule(epoch):
    initial_lr = 0.001  # Set your initial learning rate here
    drop = 0.75
    epochs_drop = 5  # Adjust this value based on your preference
    lr = initial_lr * math.pow(drop, math.floor((1 + epoch) / epochs_drop))
    return lr

lr_scheduler = LearningRateScheduler(lr_schedule)

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(
    [x_train_tokens['input_ids'], x_train_tokens['attention_mask']]
    , y_train
    , epochs=5
    , batch_size=5
    , validation_data=([x_test_tokens['input_ids'], x_test_tokens['attention_mask']], y_test)
    , callbacks=[lr_scheduler]
)
```


### Relevant log output

```shell
Epoch 1/5

---------------------------------------------------------------------------

InvalidArgumentError                      Traceback (most recent call last)

<ipython-input-12-24f08ed0005f> in <cell line: 3>()
      1 model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
      2 
----> 3 history = model.fit(
      4     [x_train_tokens['input_ids'], x_train_tokens['attention_mask']]
      5     , y_train

1 frames

/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Graph execution error:

Detected at node 'gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1' defined at (most recent call last):
    File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
      self.io_loop.start()
    File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
      self._run_once()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
      handle._run()
    File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
      lambda f: self._run_callback(functools.partial(callback, future))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
      ret = callback()
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
      self.ctx_run(self.run)
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
      yielded = self.gen.send(value)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
      yield gen.maybe_future(dispatch(*args))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
      yield gen.maybe_future(handler(stream, idents, msg))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
      self.do_execute(
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
      result = self._run_cell(
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
      return runner(coro)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
      coro.send(None)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-12-24f08ed0005f>"", line 3, in <cell line: 3>
      history = model.fit(
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1742, in fit
      tmp_logs = self.train_function(iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1338, in train_function
      return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1322, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1303, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1084, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 543, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 276, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1'
Detected at node 'gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1' defined at (most recent call last):
    File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
      self.io_loop.start()
    File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
      self._run_once()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
      handle._run()
    File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
      lambda f: self._run_callback(functools.partial(callback, future))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
      ret = callback()
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
      self.ctx_run(self.run)
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
      yielded = self.gen.send(value)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
      yield gen.maybe_future(dispatch(*args))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
      yield gen.maybe_future(handler(stream, idents, msg))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
      self.do_execute(
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
      result = self._run_cell(
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
      return runner(coro)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
      coro.send(None)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-12-24f08ed0005f>"", line 3, in <cell line: 3>
      history = model.fit(
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1742, in fit
      tmp_logs = self.train_function(iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1338, in train_function
      return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1322, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1303, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1084, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 543, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 276, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1'
2 root error(s) found.
  (0) INVALID_ARGUMENT:  Incompatible shapes: [5,512,12,514] vs. [5,512,12,513]
	 [[{{node gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1}}]]
	 [[model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._6/attention/self/cond_2/pivot_t/_676/_1027]]
  (1) INVALID_ARGUMENT:  Incompatible shapes: [5,512,12,514] vs. [5,512,12,513]
	 [[{{node gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_131688]
```
"
61743,how to download or install .so file for tflite conversion with gpu delegate,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
61742,Kaggle TPU: TFLite-Model-Maker installation issue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.9.3

### Custom code

No

### OS platform and distribution

Kaggle TPU

### Mobile device

_No response_

### Python version

3.8.16

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

TPU VM v3-8

### Current behavior?

1. Installed TFlite-model-maker using command `!pip install -q tflite-model-maker`
2. Tried to print the installed version using command `import tflite_model_maker
print(tflite_model_maker.__version__)`
3. Got error `ImportError: libusb-1.0.so.0: cannot open shared object file: No such file or directory`
I am attached here the error log.
[kaggle_tpu_error.txt](https://github.com/tensorflow/tensorflow/files/12460630/kaggle_tpu_error.txt)


### Standalone code to reproduce the issue

```shell
In Kaggle, enable TPU accelerator.
Install the TFLite Model Maker library.
Print the version of installed library.
```


### Relevant log output

_No response_"
61741,"""ImportError: random_device could not be read"" when importing duckdb after importing tensorflow","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

python:3.10-slim-bookworm

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In a linux x86_64 machine in a python3.10 venv that contains `tensorflow==2.13.0` and `duckdb==0.8.1`, running this python code:
```python
import tensorflow
import duckdb
```
either hangs indefinitely or crashes with 
```
ImportError: random_device could not be read
```

This happens with tensorflow 2.12 and tensorflow 2.13, but not tensorflow 2.11 or 2.11.1.
It happens with both duckdb 0.7.1 and duckdb 0.8.1.

It also happens with all nightlies from July up to `tf-nightly==2.15.0.dev20230803`, but the problem is fixed starting on `tf-nightly==2.15.0.dev20230804` and all nightlies after that.

### Standalone code to reproduce the issue

```shell
python3.10 -m venv ./venv
source ./venv/bin/activate
pip install tensorflow==2.13.0 duckdb==0.8.1
python -c 'import tensorflow; print(""done importing tensorflow""); import duckdb'
```


### Relevant log output

_No response_"
61736,tf.data.Dataset.save deadlocks on error ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When saving a dataset to disk, TF will silently brick if there's an error in the data input pipeline. Methinks it should rather raise on the save instead of deadlocking.

Here's a reproducible example:

```python
import tensorflow as tf
# '2.13.0'

dataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])
dataset = dataset.map(lambda x: tf.debugging.check_numerics(1. / x, ""error""))
dataset.save('/tmp/hello')
# => deadlock
```

You can add an `ignore_errors` to the pipeline and it'll rightfully ignore, but it's nonintuitive to track down why the input bricks. 

```python
dataset = dataset.ignore_errors()
# => OK save
```

### Standalone code to reproduce the issue

```shell
See above.
```


### Relevant log output

_No response_"
61732,rnn with initial_state model can't be loaded with load_model ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A simple RNN with LSTMcell model.
I want to initialize the states with `initial_state_h` and `initial_state_c`. 
```
batch_size= 16
inputs = tf.keras.layers.Input(shape=(20,5),batch_size=batch_size)
units = 8
lstm_cell_fw = tf.keras.layers.LSTMCell(units)

initial_state_h = tf.random.normal(shape = (batch_size,units), mean=0., stddev=10., dtype=tf.dtypes.float32)
initial_state_c = tf.random.normal(shape = (batch_size,units), mean=0., stddev=10., dtype=tf.dtypes.float32)
lstm_layer_fw = tf.keras.layers.RNN(lstm_cell_fw, stateful=True, return_state=True, return_sequences=False)
outputs,states_h_fw, states_c_fw= lstm_layer_fw(inputs,initial_state = [initial_state_h,initial_state_c])

lstm_dense1 = tf.keras.layers.Dense(16, activation = 'relu')
lstm_dense2 = tf.keras.layers.Dense(2, activation = 'softmax')
out=lstm_dense2(lstm_dense1(outputs))

model = tf.keras.models.Model(inputs, out)
```
After compile and train, the model is saved with `model.save('my_model_test.keras')`.

```
model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])
model.summary()

xTrain = np.random.rand(96,20,5)
yTrain = np.random.rand(96,2)

for i in range(10):
  model.fit(xTrain, yTrain,batch_size=batch_size)

model.save('my_model_test.keras')

```
But when I try to load it with `load_model = tf.keras.models.load_model('my_model_test.keras')`, it gives error:
```
13 frames
[/usr/local/lib/python3.10/dist-packages/keras/src/backend.py](https://localhost:8080/#) in int_shape(x)
   1530     """"""
   1531     try:
-> 1532         shape = x.shape
   1533         if not isinstance(shape, tuple):
   1534             shape = tuple(shape.as_list())

AttributeError: 'float' object has no attribute 'shape'

```
I tried to save in other format, `.h5`, `.json`, etc. All give the same error.

But, if I don't use `initial_state` in `outputs,states_h_fw, states_c_fw= lstm_layer_fw(inputs)`, everything goes well. No problem with `load_model`.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1uKEpnddzeYSRG_1vKjtcQ4OLNwLuQeqy?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-7-8e0130abf25e> in <cell line: 1>()
----> 1 load_model = tf.keras.models.load_model('my_model_test.keras')

13 frames
/usr/local/lib/python3.10/dist-packages/keras/src/backend.py in int_shape(x)
   1530     """"""
   1531     try:
-> 1532         shape = x.shape
   1533         if not isinstance(shape, tuple):
   1534             shape = tuple(shape.as_list())

AttributeError: 'float' object has no attribute 'shape'
```
```
"
61720,Looking for selective post training quantization for 8 bit weights and 16 bit activations,"**System information**

TensorFlow version (you are using): TF 2.13.0
Are you willing to contribute it (Yes/No): No
Describe the feature and the current behavior/state.

Dear TF developers, I'm currently experimenting with PTQ using 8 bit weights and 16 bit activations (W8A16), and I've gotten great results. However, after some experimentation I have identified that only a certain part of my network requires the 16 bit activations. In other word, using 16 bit activations for the entire model is sub-optimal for my use-case.

Hence, I'm looking for a way to selectively quantize a part of my model to 8 bit weights and activations (W8A8), and the other part to W8A16.

In the current state, would this be possible somehow ?

**Who will benefit with this feature?**
Platforms that support mixed-precision execution of activations.

**Any Other info.**"
61719,tflite-model-maker cannot be installed correctly for different reasons on different configurations,"If you try to install tflite-model-maker-nightly basically it starts to download **all nightly build wheels** since the first release rather than latest one as supposed.

This seems caused by a bad configuration.  Many people have reported this issue [many months ago](https://github.com/tensorflow/tensorflow/issues/60431), it remained unsolved as this [other issue](https://github.com/tensorflow/tensorflow/issues/61337).

But they aren't the only setup issues that affects tflite-model-maker.

If you try to install the latest build with `pip install tflite-model-maker` it raises various errors 
e.g.
tflite-support dependency that hasn't any wheel available in repository (for windows since the 0.4.0 released in May 2022 there is no wheel at all and the latest is 0.3.1 that has no wheel for latest Python versions).

Tried on Linux arm64 and needed wheels are missing too.
 
How is decided to upgrade version dependencies requirements without release updated dependencies coherently, considering these are developed by same company?

If it isn't followed a coherent path, it is obvious that troubles arise and you have to waste time trying to find if there is a combination of OS type, architecture, Python version, package version, dependency version, that may work, digging in the repository release history and requirements. 

tflite-model-maker has been released a couple of years ago to simplify model training and it was very good at this, before the project maintenance started to become erratic and lackluster creating a lot of compatibility issues and conflicts. 

Is this project development still active?"
61717,_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.6.2

### Custom code

Yes

### OS platform and distribution

ubuntu20.04

### Mobile device

rk3588

### Python version

python3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory

### Standalone code to reproduce the issue

```shell
import tensorflow
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory
```


### Relevant log output

_No response_"
61716,ClassNotFoundException when creating a new Model,"We get a class not found exception when calling

```
Model.createModel(
            FileUtil.loadMappedFile(
                TfLiteFileProvider(
                    context!!
                ).getModelFile(modelFileName)
            ),
            modelFileName,
            runningOptions!!
        )
```
The problem is that inside `GpuDelegateProxy` you are using reflection that creates `org.tensorflow.lite.gpu.GpuDelegate`. This dependency is not part of the play services dependencies anymore. It should rather point to `com.google.android.gms.tflite.gpu.GpuDelegate`.

#### Dependencies

```
tflite_vision = { module = ""org.tensorflow:tensorflow-lite-task-vision-play-services"", version= ""0.4.4"" }
tflite_gpu = { module = ""com.google.android.gms:play-services-tflite-gpu"", version = ""16.1.0"" }
tflite_java = { module = ""com.google.android.gms:play-services-tflite-java"", version = ""16.0.1"" }
tflite_support = { module = ""com.google.android.gms:play-services-tflite-support"", version = ""16.0.1"" }
tflite_metadata = { module = ""org.tensorflow:tensorflow-lite-metadata"", version = ""0.4.0"" }
```
#### Crash Log

```
Failed to create the GpuDelegate dynamically.
java.lang.ClassNotFoundException: org.tensorflow.lite.gpu.GpuDelegate
	at java.lang.Class.classForName(Native Method)
	at java.lang.Class.forName(Class.java:454)
	at java.lang.Class.forName(Class.java:379)
	at org.tensorflow.lite.support.model.GpuDelegateProxy.maybeNewInstance(GpuDelegateProxy.java:38)
	at org.tensorflow.lite.support.model.Model.createModel(Model.java:204)
	at com..kcc.domain.usecase.poseestimation.Model2Stack257.<init>(Model2Stack257.kt:170)
	at com..kcc.domain.usecase.InferPoseModelTFLiteUseCase.tfModel$lambda$0(InferPoseModelTFLiteUseCase.kt:27)
	at com..kcc.domain.usecase.InferPoseModelTFLiteUseCase.$r8$lambda$xjFI4TuW8y9D4mlU8833KNnigbg(Unknown Source:0)
	at com..kcc.domain.usecase.InferPoseModelTFLiteUseCase$$ExternalSyntheticLambda0.then(Unknown Source:2)
	at com.google.android.gms.tasks.zzc.run(com.google.android.gms:play-services-tasks@@18.0.2:3)
	at android.os.Handler.handleCallback(Handler.java:942)
	at android.os.Handler.dispatchMessage(Handler.java:99)
	at android.os.Looper.loopOnce(Looper.java:226)
	at android.os.Looper.loop(Looper.java:313)
	at android.app.ActivityThread.main(ActivityThread.java:8757)
	at java.lang.reflect.Method.invoke(Native Method)
	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:571)
	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1067)
Caused by: java.lang.ClassNotFoundException: Didn't find class ""org.tensorflow.lite.gpu.GpuDelegate"" on path: DexPathList[[zip file ""/data/app/~~7PczWMC5IEgE71QUXcKhDQ==/com..mt-cQ_JVkbM4HelJiyQApEttQ==/base.apk""],nativeLibraryDirectories=[/data/app/~~7PczWMC5IEgE71QUXcKhDQ==/com..mt-cQ_JVkbM4HelJiyQApEttQ==/lib/arm64, /data/app/~~7PczWMC5IEgE71QUXcKhDQ==/com..mt-cQ_JVkbM4HelJiyQApEttQ==/base.apk!/lib/arm64-v8a, /system/lib64, /system/s
	at dalvik.system.BaseDexClassLoader.findClass(BaseDexClassLoader.java:259)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:379)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:312)
	at java.lang.Class.classForName(Native Method)Â 
	at java.lang.Class.forName(Class.java:454)Â 
	at java.lang.Class.forName(Class.java:379)Â 
	at org.tensorflow.lite.support.model.GpuDelegateProxy.maybeNewInstance(GpuDelegateProxy.java:38)Â 
	at org.tensorflow.lite.support.model.Model.createModel(Model.java:204)Â 
	at com..kcc.domain.usecase.poseestimation.Model2Stack257.<init>(Model2Stack257.kt:170)Â 
	at com..kcc.domain.usecase.InferPoseModelTFLiteUseCase.tfModel$lambda$0(InferPoseModelTFLiteUseCase.kt:27)Â 
	at com..kcc.domain.usecase.InferPoseModelTFLiteUseCase.$r8$lambda$xjFI4TuW8y9D4mlU8833KNnigbg(Unknown Source:0)Â 
	at com..kcc.domain.usecase.InferPoseModelTFLiteUseCase$$ExternalSyntheticLambda0.then(Unknown Source:2)Â 
	at com.google.android.gms.tasks.zzc.run(com.google.android.gms:play-services-tasks@@18.0.2:3)Â 
	at android.os.Handler.handleCallback(Handler.java:942)Â 
	at android.os.Handler.dispatchMessage(Handler.java:99)Â 
	at android.os.Looper.loopOnce(Looper.java:226)Â 
	at android.os.Looper.loop(Looper.java:313)Â 
	at android.app.ActivityThread.main(ActivityThread.java:8757)Â 
```
"
61715,Tensorflow Lite C++ error while building with cmake on windows with GPU support,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14

### Custom code

Yes

### OS platform and distribution

Widnows 10

### Mobile device

Asus pc

### Python version

3.11

### Bazel version

6.1.4

### GCC/compiler version

gcc version 6.3.0 (MinGW.org GCC-6.3.0-1)

### CUDA/cuDNN version

-

### GPU model and memory

NVIDIA GeForce GTX 960m

### Current behavior?

Relevant stackoverflow question: https://stackoverflow.com/questions/76990961/tensorflow-c-error-while-building-with-cmake-on-windows-with-gpu-support

I'm trying to get a tensorflow C++ build (or tensorflow lite) for Windows that runs on GPU (WITHOUT using CUDA, it should work on AMD). I decided to opt in for tensorflow lite with the -DTFLITE_ENABLE_GPU=ON flag to enable OpenCL.



### Standalone code to reproduce the issue

```shell
The steps I followed:

- Clone tensorflow github repo to tensorflow_src and checout to r2.14
- Create a folder called tflite-opencl next to the cloned repo
- Go to tflite-opencl and run cmake C:/Users/Asus/Desktop/tensorflow/tensorflow/lite -DTFLITE_ENABLE_GPU=ON 
- Run cmake --build . -j
```


### Relevant log output

```shell
When I try to run `cmake --build . -j` I get the following error:

C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\delegates\gpu\common\selectors\operation_selector.cc(313,20): error
 C2039: 'any_cast': is not a member of 'std' [C:\Users\Asus\Desktop\tflite-opencl\tensorflow-lite.vcxproj]

...
C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\delegates\gpu\common\selectors\operation_selector.cc(313,1): error
C2065: 'any_cast': undeclared identifier [C:\Users\Asus\Desktop\tflite-opencl\tensorflow-lite.vcxproj]
C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\delegates\gpu\common\selectors\operation_selector.cc(313,29): error
 C2275: 'tflite::gpu::ElementwiseAttributesBase<tflite::gpu::DataType::BOOL,T>': expected an expression instead of a
 type [C:\Users\Asus\Desktop\tflite-opencl\tensorflow-lite.vcxproj]
          with
          [
              T=bool
          ]
C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\delegates\gpu\common\selectors\operation_selector.cc(317,1): error
C3536: 'attr': cannot be used before it is initialized [C:\Users\Asus\Desktop\tflite-opencl\tensorflow-lite.vcxproj]
C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\delegates\gpu\common\selectors\operation_selector.cc(317,17): error
 C2672: 'CreateElementwiseWithBroadcast': no matching overloaded function found [C:\Users\Asus\Desktop\tflite-opencl
\tensorflow-lite.vcxproj]

...


C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\delegates\gpu\common\selectors\operation_selector.cc(321,17): error
 C2672: 'CreateElementwise': no matching overloaded function found [C:\Users\Asus\Desktop\tflite-opencl\tensorflow-l
ite.vcxproj]

...

C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\delegates\gpu\common\selectors\operation_selector.cc(313,29): error
 C2275: 'tflite::gpu::ElementwiseAttributesBase<tflite::gpu::DataType::INT32,T>': expected an expression instead of
a type [C:\Users\Asus\Desktop\tflite-opencl\tensorflow-lite.vcxproj]
          with
          [
              T=int32_t
          ]
C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\delegates\gpu\common\selectors\operation_selector.cc(313,29): error
 C2275: 'tflite::gpu::ElementwiseAttributesBase<tflite::gpu::DataType::FLOAT32,float>': expected an expression inste
ad of a type [C:\Users\Asus\Desktop\tflite-opencl\tensorflow-lite.vcxproj]

...

C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\delegates\gpu\common\tasks\special\conv_pointwise.cc(129,12): error
 C2039: 'any_cast': is not a member of 'std' [C:\Users\Asus\Desktop\tflite-opencl\tensorflow-lite.vcxproj]
C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.36.32532\include\variant(30,1): message : s
ee declaration of 'std' [C:\Users\Asus\Desktop\tflite-opencl\tensorflow-lite.vcxproj]
C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\delegates\gpu\common\tasks\special\conv_pointwise.cc(129,20): error
 C2065: 'any_cast': undeclared identifier [C:\Users\Asus\Desktop\tflite-opencl\tensorflow-lite.vcxproj]
C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\delegates\gpu\common\tasks\special\conv_pointwise.cc(129,21): error
 C2275: 'tflite::gpu::ReduceAttributes': expected an expression instead of a type [C:\Users\Asus\Desktop\tflite-open
cl\tensorflow-lite.vcxproj]
C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\delegates\gpu\common\tasks\special\conv_pointwise.cc(130): error C3
536: 'reduce_attr': cannot be used before it is initialized [C:\Users\Asus\Desktop\tflite-opencl\tensorflow-lite.vcx
proj]
```

But despite these errors it kept on compiling and eventually ended on: 
```
...
 tensorflow_profiler_logger_shim.cc
  tflite_with_xnnpack_optional.cc
  minimal_logging_default.cc
C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\minimal_logging_default.cc(37,9): warning C4068: unknown pragma 'cl
ang' [C:\Users\Asus\Desktop\tflite-opencl\tensorflow-lite.vcxproj]
C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\minimal_logging_default.cc(38,9): warning C4068: unknown pragma 'cl
ang' [C:\Users\Asus\Desktop\tflite-opencl\tensorflow-lite.vcxproj]
C:\Users\Asus\Desktop\tensorflow\tensorflow\lite\minimal_logging_default.cc(40,9): warning C4068: unknown pragma 'cl
ang' [C:\Users\Asus\Desktop\tflite-opencl\tensorflow-lite.vcxproj]
  platform_profiler.cc
  root_profiler.cc
  profiler.cc
  sparsity_format_converter.cc
  schema_utils.cc
  Generating Code...

C:\Users\Asus\Desktop\tflite-opencl>
```
I think I'm getting the `any_cast is not a member of std` because my C++ standard version is below 14. But I've been coding in windows for a while and I'm pretty sure I'm above 17 as I use many modern features. I've updated g++ from Visual Studio Installer but I'm not sure how to properly update my C++ version on Windows.

I'm sure this build has failed, but regardless I searched for the dll. 

This failed build gave me a Visual Studio solution inside tflite-opencl. I need to import tflite into a huge project so I need either the dll files or the lib files. I tried looking under `tflite-opencl/Release`, `tflite-opencl/Debug` and `tflite-opencl/x64` but found nothing. I'm also adviced to look under `\bazel-bin\tensorflow\lite\kernels` on [my previous question][1], which I can find under `tensorflow_src` and I can't find any dll's in it. 

How can I fix that error and change my C++ standard on Windows? How can I get these dll or lib files to use under my project? If I should build `INSTALL` how am I supposed to do it? I developed mainly on Linux environments so I don't know how to use Visual Studio propperly.
```
"
61714,How to limit GPU memory usage when only prediction in c++?,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.4

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.0 / 8.1.0

### GPU model and memory

RTX 3090 / 24GB

### Current behavior?

I loaded the saved model using the already compiled tensorflow-gpu 2.4.0.
When this model was used for prediction, it was confirmed that all available memory of the gpu was used.
I've seen limiting using the growing method in python, but I don't know how to use it in c++. could you please tell me how?



### Standalone code to reproduce the issue

```shell
#include <stdlib.h>
#include <stdio.h>
#include <tensorflow/c/c_api.h>
#include <iostream>
#include <fstream>
#include <sstream>
#include <string>
#include <vector>
void NoOpDeallocator(void* data, size_t a, void* b) {}

int main() {
    TF_Graph* Graph = TF_NewGraph();
    TF_Status* Status = TF_NewStatus();

    TF_SessionOptions* SessionOpts = TF_NewSessionOptions();
    TF_Buffer* RunOpts = NULL;

    const char* saved_model_dir = ""H:\\my_model\\""; // Path of the model
    const char* tags = ""serve""; // default model serving tag; can change in future
    int ntags = 1;

    TF_Session* Session = TF_LoadSessionFromSavedModel(SessionOpts, RunOpts, saved_model_dir, &tags, ntags, Graph, NULL, Status);
    if (TF_GetCode(Status) == TF_OK)
    {
        printf(""TF_LoadSessionFromSavedModel OK\n"");
    }
    else
    {
        printf(""%s"", TF_Message(Status));
    }
}
```


### Relevant log output

_No response_"
61712,ExtensionType does not allow creating tf.function methods,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13.0, tf 2.12.0

### Custom code

Yes

### OS platform and distribution

Colab, Mac

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When calling a method on ExtensionType, it seems that `self` is flattened, making it incompatible with `tf.function` and `input_signature`. Is this the intended behavior?

### Standalone code to reproduce the issue

Colab: https://colab.research.google.com/drive/19b65SIbsUgV4CKGX0DGgS0IcbXeOsOw9?usp=sharing

```shell
import tensorflow as tf

print(""tf.__version__:"", tf.__version__)

class A(tf.experimental.ExtensionType):
    x1: tf.Tensor
    x2: tf.Tensor

_a = A.Spec(tf.TensorSpec((5, 6)), tf.TensorSpec((5, 6)))

class B(tf.experimental.ExtensionType):
    y1: tf.Tensor
    y2: tf.Tensor
    y3: int

_b = B.Spec(tf.TensorSpec((7, 8)), tf.TensorSpec((7, 8)), 9)

class C(tf.experimental.ExtensionType):
    a1: A
    a2: A
    a3: A
    a4: A

    @tf.function(input_signature=[_b])
    def f(self, b: B):
        return b

x = tf.zeros((5, 6))
y = tf.zeros((7, 8))
a = A(x, x)
b = B(y, y, 9)
c = C(a, a, a, a)
c.f(b)
```


### Relevant log output

Colab:
```
tf.__version__: 2.12.0
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-6a3d1772c2f1> in <cell line: 33>()
     31 b = B(y, y, 9)
     32 c = C(a, a, a, a)
---> 33 c.f(b)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/function_spec.py in cast_inputs_to_signature(inputs, input_signature)
    538         check_types=False)  # lists are convert to tuples for `tf.data`.
    539   except ValueError:
--> 540     raise ValueError(""Structure of Python function inputs does not match ""
    541                      ""input_signature:\n""
    542                      f""{format_error_message(inputs, input_signature)}."")

ValueError: Structure of Python function inputs does not match input_signature:
  inputs: (
    C(a1=A(x1=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>, x2=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>), a2=A(x1=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>, x2=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>), a3=A(x1=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>, x2=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>), a4=A(x1=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>, x2=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>)),
    B(y1=<tf.Tensor: shape=(7, 8), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>, y2=<tf.Tensor: shape=(7, 8), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>, y3=9))
  input_signature: (
    B.Spec(y1=TensorSpec(shape=(7, 8), dtype=tf.float32, name=None), y2=TensorSpec(shape=(7, 8), dtype=tf.float32, name=None), y3=9)).
```

Mac:
```shell
tf.__version__: 2.13.0
Traceback (most recent call last):
  File ""/tmp/a.py"", line 31, in <module>
    c.f(b)
  File ""/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py"", line 181, in __call__
    raise ValueError(
ValueError: Signature specifies 4 arguments, got: 10.
```

Mac (tf_nightly-2.15.0.dev20230827-cp311-cp311-macosx_12_0_arm64):
```shell
tf.__version__: 2.15.0-dev20230827
Traceback (most recent call last):
  File ""/tmp/a.py"", line 33, in <module>
    c.f(b)
  File ""/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_type.py"", line 391, in unpack_inputs
    p.type_constraint._to_tensors(bound_parameters.arguments[p.name])  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyError: 'y1'
```
"
61711,Will there a MLP model in the future version?,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When building deep learning models like Multi-Layer Perceptrons (MLPs), code reusability and conciseness are crucial factors. Currently, using `tf.keras.Sequential` in TensorFlow allows for convenient creation of sequential models. However, manually adding common operations such as Batch Normalization or Dropout to each layer can lead to code redundancy and an increased burden in terms of coding and maintenance. Therefore, proposing the addition of a feature in TensorFlow to directly create MLPs with Batch Normalization and Dropout is highly beneficial.

Here are several reasons why this feature would be advantageous for TensorFlow users:

1. **Simplified Code**: Users won't need to manually add Batch Normalization and Dropout operations to each layer, resulting in cleaner, more readable, and maintainable code.

2. **Reduced Error Rate**: Manual copy-pasting of code is error-prone, especially as model complexity increases. Automating the integration of Batch Normalization and Dropout operations can reduce issues arising from code errors.

3. **Increased Productivity**: Developers can build and iterate on models more swiftly, focusing on design and parameter tuning rather than rewriting the same code segments for every new model.

4. **Education and Learning**: For newcomers to TensorFlow, this feature can provide a quicker onboarding process, lowering the learning curve and enabling them to grasp and apply deep learning concepts faster.

Certainly, here's the additional information:

I also believe that PyTorch has implemented MLP functionality quite effectively. An example of this can be found in the following URL: [PyTorch MLP](https://pytorch.org/vision/main/generated/torchvision.ops.MLP.html). PyTorch's approach to creating MLPs provides a good reference for how TensorFlow could potentially integrate similar features.

### Standalone code to reproduce the issue

origin
```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10),
])
```


with MLP model
```python
model = tf.keras.MLP(
    hidden_channels=[128, 64, 32, 10],
    norm_layer=tf.keras.layers.BatchNormalization,
    activation_layer=tf.keras.layers.ReLU,
    dropout=0.2,
)
```

### Relevant log output

_No response_"
61710,Will there a MLP model in the future version?,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When building deep learning models like Multi-Layer Perceptrons (MLPs), code reusability and conciseness are crucial factors. Currently, using `tf.keras.Sequential` in TensorFlow allows for convenient creation of sequential models. However, manually adding common operations such as Batch Normalization or Dropout to each layer can lead to code redundancy and an increased burden in terms of coding and maintenance. Therefore, proposing the addition of a feature in TensorFlow to directly create MLPs with Batch Normalization and Dropout is highly beneficial.

Here are several reasons why this feature would be advantageous for TensorFlow users:

1. **Simplified Code**: Users won't need to manually add Batch Normalization and Dropout operations to each layer, resulting in cleaner, more readable, and maintainable code.

2. **Reduced Error Rate**: Manual copy-pasting of code is error-prone, especially as model complexity increases. Automating the integration of Batch Normalization and Dropout operations can reduce issues arising from code errors.

3. **Increased Productivity**: Developers can build and iterate on models more swiftly, focusing on design and parameter tuning rather than rewriting the same code segments for every new model.

4. **Education and Learning**: For newcomers to TensorFlow, this feature can provide a quicker onboarding process, lowering the learning curve and enabling them to grasp and apply deep learning concepts faster.

Certainly, here's the additional information:

I also believe that PyTorch has implemented MLP functionality quite effectively. An example of this can be found in the following URL: [PyTorch MLP](https://pytorch.org/vision/main/generated/torchvision.ops.MLP.html). PyTorch's approach to creating MLPs provides a good reference for how TensorFlow could potentially integrate similar features.

### Standalone code to reproduce the issue

```shell
origin

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10),
])


with MLP model
```python
model = tf.keras.MLP(
    hidden_channels=[128, 64, 32, 10],
    norm_layer=tf.keras.layers.BatchNormalization,
    activation_layer=tf.keras.layers.ReLU,
    dropout=0.2,
)
```
```


### Relevant log output

_No response_"
61709,Issues with Running Custom TensorFlow Lite Model in C++,"### 1. System information

- Platform and Linux distribution kubuntu 22.04:
- TensorFlow is built from C++ source code:
- Tensorflow 2.11:

### 2. Code

- Link to models that I trained and tried but they don't work in C++ - https://github.com/asuemg1/models_hub/tree/main/Tensorflow%20Lite/Object%20Detection/my_ssd_mobnet/Optimized%20Models
- Link to the model that works in C++ - https://github.com/ankdesh/tflite/blob/master/Android-TensorFlow-Lite-Example/app/src/main/assets/mobilenet_quant_v1_224.tflite
- Link to C++ code (mainwindow.cpp file):
https://drive.google.com/file/d/1u87yK-1qqKeHBjUKq-Lkxi0LMQKkdrPg/view?usp=sharing

### 3. Crash after conversion

- The model does not work in C++.

Please tell me how you can run the Tensorflow Lite model (tflite format) for object detection or image classification in C ++.

My steps:
- Trained the model for object detection using Tensorflow 2 API object detection.
- After training, I converted the model to the savedmodel format, and then to tflite.
- Next, I needed to embed this model into a C++ project. In order to use it in the future on low-power devices such as rasberry pi

My actions:
- Compiled the Tensorflow Lite library for C++.
- Found a test case using the mobilenet_quant_v1_224.tflite model. In this test case, the model runs successfully. However, when trying to use my own model, it does not work, although it has been tested and works in Python.

What was found out:

- The mobilenet_quant_v1_224.tflite model was quantized and had no metadata and no internal labelmap.txt file.
- TensorFlow Lite API 2 for C++ does not currently support metadata.

If you have any information on how to get my tflite model to work in C++ please share."
61708,Null pointer exception in constant folding of grappler.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Segmentation fault.


### Standalone code to reproduce the issue


[NPE.zip](https://github.com/tensorflow/tensorflow/files/12444694/NPE.zip)

Run the poc.py in the zip.
In constant folding, after `FoldNode`, fold nodes will be updated with the new name at ã€1ã€‘. But later, the old node name will still be referenced in `MergeConcat` (ã€2ã€‘) with `GetNode`(ã€3ã€‘), leading to an NPE.



```C++
//tensorflow/core/grappler/optimizers/constant_folding.cc
Status ConstantFolding::FoldNode(NodeDef* node, GraphDef* output_graph,
                                 bool* result_too_large) {
    ...
    else if (port < static_cast<int>(const_nodes.size()) &&
               !const_nodes[port].name().empty()) {
          //ã€1ã€‘
          node_map_->UpdateInput(output->name(), NodeName(output->input(i)),
                                   const_nodes[port].name());
          *output->mutable_input(i) = const_nodes[port].name();
 } 

bool ConstantFolding::MergeConcat(bool use_shape_info,
                                  GraphProperties* properties,
                                  GraphDef* optimized_graph, NodeDef* node) {
   â€¦â€¦
   for (int i = 0; i < num_regular_inputs - 1; ++i) {
       //ã€2ã€‘
       const NodeDef* input_node = node_map_->GetNode(node->input(i));
       if (!IsReallyConstant(*input_node)) {â€¦â€¦}
}

//tensorflow/core/grappler/utils.h
NodeDefT* GetNode(const string& name) const {
    const string node_name = NodeName(name);
    auto it = nodes_.find(node_name);
    if (it == nodes_.end()) {
      VLOG(1) << ""Node could not be found: "" << name;
      return nullptr; //ã€3ã€‘
    }
    return it->second;
 }
```


### Relevant log output

_No response_"
61707,Cannot build package with bazel,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

newest

### Custom code

Yes

### OS platform and distribution

Debian 12

### Mobile device

_No response_

### Python version

Python 3.9

### Bazel version

newest

### GCC/compiler version

Clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have searched everywhere but cannot find anything about this error

### Standalone code to reproduce the issue

```shell
./bazel-bin/tensorflow/tools/pip_package/build_pip_package /home/drowsiness/
TensorFlow Version: 2.15.0
TensorFlow Major Version: 2
TMPDIR: /tmp/tmp.BINOHRyJHF
Fri Aug 25 09:26:15 PM PDT 2023 : === Preparing sources in dir: /tmp/tmp.BINOHRyJHF
Symlink already exists: bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/libtensorflow_cc.so.2
~/tensorflow ~/tensorflow
~/tensorflow
~/tensorflow ~/tensorflow
~/tensorflow
~/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow ~/tensorflow
~/tensorflow
./bazel-bin/tensorflow/tools/pip_package/build_pip_package: line 255: patchelf: command not found
```


### Relevant log output

_No response_"
61705,tensorflowlite.dll is huge (compared to other platforms),"Opened on behalf of @sztomi, Issue is current as of this writing. Previous Issue: https://github.com/tensorflow/tensorflow/issues/48118

*Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template*

#### System information

* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
* TensorFlow installed from (source or binary): Source
* TensorFlow version: https://github.com/tensorflow/tensorflow/commit/5d6cc7bf97a226c1e6a73ad4fc391c154dd622ac
* Python version: n/a
* Installed using virtualenv? pip? conda?: n/a
* Bazel version (if compiling from source): 3.7.2
* GCC/Compiler version (if compiling from source): MSVC 2019 (cl: 19.28.29334)
* CUDA/cuDNN version: n/a
* GPU model and memory: n/a

#### Describe the problem

#### Provide the exact sequence of commands / steps that you executed before running into the problem
```
bazel build -c opt //tensorflow/lite/tensorflowlite.dll
```
This yields a 15MB statically linked binary when compiled for 32-bit windows (i.e. the 64-bit version will be even larger). Compared to other platforms, this is a 5x increase that is difficult to explain, especially in lieu of tools like bloaty on Windows. Unfortunately I don't know where to start poking this at all - but I do believe that even accounting for platform differences, a 5x size difference is unexpected. The same issue was noted here: https://github.com/tensorflow/tensorflow/issues/33634#issuecomment-620645664 (with a suggestion to use the C API instead, but the C API is a binding that cannot be used independent from the C++ binary).

#### Any other info / logs
Let me know if I can include any more info."
61701,tf.math.top_k gradient taping fails when `index_type=tf.int64`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda_11.8.r11.8/compiler.31833905_0 / cuDNN version 8700

### GPU model and memory

NVIDIA GeForce RTX 2080 Ti

### Current behavior?

top_k gradient taping fails when `index_type=tf.int64`, unfortunately switching to `int32` leads to this other issue  https://github.com/tensorflow/tensorflow/issues/61692.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

x = tf.Variable(np.ones((3, 3)))

with tf.GradientTape() as tape:
    y = tf.math.top_k(x, k=1, index_type=tf.int64)

dy_dx = tape.gradient(y, x)
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int64 tensor but is a int32 tensor [Op:AddV2] name:
```
"
61700,TensorFlow Not Detecting GPU on Compute Node,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-99044-gc6ecfeac886 2.15.0-dev20230825

### Custom code

No

### OS platform and distribution

CentOS Linux release 7.9.2009 (Core)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8.0 / 8.7.0.84

### GPU model and memory

_No response_

### Current behavior?

## Behavior summary ðŸ–¥ï¸

I'm trying to set up TensorFlow on a compute node within our scientific computing cluster and am running into a problem. After setting up a new environment and installing TensorFlow, running TensorFlow's `tf.config.list_physical_devices('GPU')` method returns an empty list, indicating no GPU devices are detected. However, `nvidia-smi` shows a Quadro RTX 6000 GPU present on the system. The TensorFlow CPU validation works without issues, but the GPU validation does not.

## Possible Areas of Concern ðŸš©

1. **Modules vs. Conda/Pip Installations:** I've loaded specific versions of CUDA and cuDNN using the cluster's module system. However, I also used conda and pip to install these within my environment. Could this mixed approach cause any conflicts for TensorFlow? I'm unsure which version TensorFlow might prioritize or if it would create any confusion.
2. **CUDA Version Mismatch?:** When I run `nvidia-smi`, it indicates a CUDA version of 12.2. Yet, I've loaded and installed a CUDA version of 11.8 using both modules and conda. I'm wondering if this difference could lead to any issues. Does TensorFlow need a strict match with the CUDA version?
3. **Different cuDNN Version:** The TensorFlow installation guide mentioned cuDNN version 8.6.0.163, but I installed 8.7.0.84 for what I thought might be better consistency with the modules I loaded. Could this version difference be problematic? Is TensorFlow particular about the cuDNN version it works with?

I'm looking for insights from anyone who might have navigated similar issues or can provide clarity on these points. Also of course if anyone knows how I might go about further troubleshooting/diagnosing this that'd be great. Thanks for the help!  

### Standalone code to reproduce the issue

```shell
# Setting up a new conda environment with python 3.9
conda create --name myenv python=3.9
conda activate myenv

# Running NVIDIA's System Management Interface to check GPU
nvidia-smi

# Loading necessary modules for CUDA/cuDNN
module load cuda-11.8.0-gcc-8.2.0-rqftjjg
module load cudnn-8.7.0.84-11.8-gcc-8.2.0-qibz3ue

# Installing CUDA Toolkit and cuDNN using Conda and Pip
conda install -c conda-forge cudatoolkit=11.8.0
pip install nvidia-cudnn-cu11==8.7.0.84

# Configuring system paths
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
echo 'CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
echo 'export LD_LIBRARY_PATH=$CUDNN_PATH/lib:$CONDA_PREFIX/lib/:$LD_LIBRARY_PATH' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh

# Upgrading pip and installing TensorFlow
pip install --upgrade pip
pip install tensorflow==2.13.*

# Running CPU validation
python3 -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))""

# Running GPU validation
python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```


### Relevant log output

```shell
# CPU validation
2023-08-25 14:11:07.056941: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-08-25 14:11:07.105836: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-25 14:11:08.620525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-25 14:11:10.459520: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2023-08-25 14:11:10.459591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: scu-node018.scu
2023-08-25 14:11:10.459608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: scu-node018.scu
2023-08-25 14:11:10.459712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 535.104.5
2023-08-25 14:11:10.459783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.104.5
2023-08-25 14:11:10.459811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 535.104.5
tf.Tensor(-964.9978, shape=(), dtype=float32)

# GPU validation
2023-08-25 14:11:35.537281: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-08-25 14:11:35.582391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-25 14:11:36.286683: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-25 14:11:36.956393: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2023-08-25 14:11:36.956458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: scu-node018.scu
2023-08-25 14:11:36.956495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: scu-node018.scu
2023-08-25 14:11:36.956591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 535.104.5
2023-08-25 14:11:36.956655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.104.5
2023-08-25 14:11:36.956683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 535.104.5
[]

# nvidia-smi Outpu
Fri Aug 25 13:59:33 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro RTX 6000                Off | 00000000:AF:00.0 Off |                    0 |
| N/A   31C    P0              54W / 250W |      0MiB / 23040MiB |      5%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```
"
61697,autograph become so slow in multi gpu and newest tf version,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

i test autograph speed in different version of tf from 2.5.0 to 2.14.0 with different gpu card num(1 card and 4 cards).
i found multi card and newer tf both will slow down autograph speed. 
autograph speed in 4 card gpu is 4x slower than 1 card.
and autograph seed in tf 2.14.0 is 10x slower than tf 2.5.0.
Any suggestion? 
<img width=""649"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/39215341/08580964-f401-4ccf-883a-cc71873b1304"">


### Standalone code to reproduce the issue

```shell
import os
import sys
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""
import tensorflow as tf
import time
print(""TensorFlow version:"", tf.__version__)

physical_devices = tf.config.list_physical_devices('GPU')
for physical_device in physical_devices:
    tf.config.experimental.set_memory_growth(physical_device, True)
gpu_names = [device.name.replace(""/physical_device:"", """") for device in physical_devices]
strategy = tf.distribute.MirroredStrategy(gpu_names)
from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model

with strategy.scope():
    mnist = tf.keras.datasets.mnist
    
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    
    x_test = x_test[..., tf.newaxis].astype(""float32"")
    
    train_ds = tf.data.Dataset.from_tensor_slices(
        (x_train, y_train)).shuffle(10000).batch(32)
    
    test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)
    
    class MyModel(Model):
      def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = Conv2D(32, 3, activation='relu')
        self.flatten = Flatten()
        self.layer = dict()
        for i in range(500):
            self.layer[f""d{i}""]= Dense(128, activation='relu')
        self.d = Dense(10)
    
      def call(self, x):
        x = tf.expand_dims(x, -1)
        x = self.conv1(x)
        x = self.flatten(x)
        for i in range(500):
            x = self.layer[f""d{i}""](x)
        return self.d(x)
    
    model = MyModel()
    """"""Choose an optimizer and loss function for training:""""""
    
    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    
    optimizer = tf.keras.optimizers.Adam()
    
    """"""Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result.""""""
    
    train_loss = tf.keras.metrics.Mean(name='train_loss')
    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
    
    test_loss = tf.keras.metrics.Mean(name='test_loss')
    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')
    
    """"""Use `tf.GradientTape` to train the model:""""""
    
    def train_step(images, labels):
      print(images)
      with tf.GradientTape() as tape:
        predictions = model(images, training=True)
        loss = tf.reduce_mean(predictions)
        loss = tf.distribute.get_replica_context().all_reduce(tf.distribute.ReduceOp.SUM, loss)
      gradients = tape.gradient(loss, model.trainable_variables)
      optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    
    @tf.function
    def train(images, labels):
        print(images)
        strategy.run(train_step, args=(images, labels,))
    """"""Test the model:""""""
    
    
    for epoch in range(1):
      time_s = time.time()
      time_e = time.time()
      for images, labels in train_ds:
        time_s = time.time()
        train.get_concrete_function(images, labels)
        time_e = time.time()
        break;
      times = time_e - time_s
      print(
        f'tf version:{tf.__version__};  times: {times}; '
      )
```


### Relevant log output

_No response_"
61696,LLVM conflict with libosmesa6,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Debian Bullseye 11.7

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Tensorflow 2.13.0 binary causes some conflict with libOSMesa which uses LLVM-11. Tensorflow 2.7.1 didn't have this problem.

libosmesa6 (20.3.5-1) is installed through apt-get
https://packages.debian.org/bullseye/libosmesa6

![image](https://github.com/tensorflow/tensorflow/assets/4276548/d7a109e1-f545-4692-a9e9-3088dbb1ed87)





### Standalone code to reproduce the issue

```shell
#include <stdio.h>
#include <tensorflow/c/c_api.h>

#include <dlfcn.h>

int main() {
  printf(""Hello from TensorFlow C library version %s\n"", TF_Version());

  dlopen(""libOSMesa.so.6"", RTLD_NOW);

  return 0;
}
```


### Relevant log output

```shell
gcc hello_tf.c -ltensorflow -ldl -o hello_tf

./hello_tf
2023-08-25 13:50:27.935004: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Hello from TensorFlow C library version 2.13.0
: CommandLine Error: Option 'debug-counter' registered more than once!
LLVM ERROR: inconsistency in registered CommandLine options
Aborted
```
"
61695,Segmentation Fault (Core Dumped) when convert whisper with int8 quantization,"System information
Linux 20.04
pip Tensorflow==2.12.0
using tranformers WhisperForConditionalgeneration 

I'm trying to convert from TF to tflite and quantized to int8 Whisper, using the whisper model from tranformers WhisperForConditionalGeneration. At some point the conversion crash. 
Here is the colab for more details: 
Colab: https://colab.research.google.com/drive/1oAVoUxRFZLkS1uqqFN8HdgRVk0IWAlsN?usp=sharing

Also I attach the Error Trace from my server running in CPU and also running in GPU (TITAN RTX 24GB). 
CPU: [TraceTflite.txt](https://github.com/tensorflow/tensorflow/files/12438668/TraceTflite.txt)

GPU: [TraceTflite_GPU.txt](https://github.com/tensorflow/tensorflow/files/12452007/TraceTflite_GPU.txt)
"
61694,"extensions eglQueryDevicesEXT, eglQueryDeviceAttribEXT and eglGetPlatformDisplayEXT not available","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

1.13.1

### Custom code

Yes

### OS platform and distribution

Unbuntu 22

### Mobile device

_No response_

### Python version

Python 2.7

### Bazel version

_No response_

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

10

### GPU model and memory

GTX 1060 

### Current behavior?

Hi, I got this error when I run Dirt model with my tensorflow-gpu 1.13.1 . 

`2023-08-25 15:23:37.711796: F /home/engineer1/dirt/csrc/gl_common.h:46] extensions eglQueryDevicesEXT, eglQueryDeviceAttribEXT and eglGetPlatformDisplayEXT not available`

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
import dirt

canvas_width, canvas_height = 128, 128
centre_x, centre_y = 32, 64
square_size = 16


def get_non_dirt_pixels():
    xs, ys = tf.meshgrid(tf.range(canvas_width), tf.range(canvas_height))
    xs = tf.cast(xs, tf.float32) + 0.5
    ys = tf.cast(ys, tf.float32) + 0.5
    x_in_range = tf.less_equal(tf.abs(xs - centre_x), square_size / 2)
    y_in_range = tf.less_equal(tf.abs(ys - centre_y), square_size / 2)
    return tf.cast(tf.logical_and(x_in_range, y_in_range), tf.float32)


def get_dirt_pixels():

    # Build square in screen space
    square_vertices = tf.constant([[0, 0], [0, 1], [1, 1], [1, 0]], dtype=tf.float32) * square_size - square_size / 2.
    square_vertices += [centre_x, centre_y]

    # Transform to homogeneous coordinates in clip space
    square_vertices = square_vertices * 2. / [canvas_width, canvas_height] - 1.
    square_vertices = tf.concat([square_vertices, tf.zeros([4, 1]), tf.ones([4, 1])], axis=1)

    return dirt.rasterise(
        vertices=square_vertices,
        faces=[[0, 1, 2], [0, 2, 3]],
        vertex_colors=tf.ones([4, 1]),
        background=tf.zeros([canvas_height, canvas_width, 1]),
        height=canvas_height, width=canvas_width, channels=1
    )[:, :, 0]


def main():

    if '.' in tf.__version__ and int(tf.__version__.split('.')[0]) < 2:

        session = tf.Session()
        with session.as_default():

            non_dirt_pixels = get_non_dirt_pixels().eval()
            dirt_pixels = get_dirt_pixels().eval()

    else:

        non_dirt_pixels = get_non_dirt_pixels().numpy()
        dirt_pixels = get_dirt_pixels().numpy()

    if np.all(non_dirt_pixels == dirt_pixels):
        print('successful: all pixels agree')
    else:
        print('failed: {} pixels disagree'.format(np.sum(non_dirt_pixels != dirt_pixels)))


if __name__ == '__main__':
    main()
```


### Relevant log output

```shell
2023-08-25 15:23:37.181664: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2023-08-25 15:23:37.186756: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2496000000 Hz
2023-08-25 15:23:37.189184: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x24d6bf0 executing computations on platform Host. Devices:2023-08-25 15:23:37.189205: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2023-08-25 15:23:37.478413: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-08-25 15:23:37.478518: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x233ae60 executing computations on platform CUDA. Devices:2023-08-25 15:23:37.478540: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): NVIDIA GeForce GTX 1060 6GB, Compute Capability 6.1
2023-08-25 15:23:37.478666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: NVIDIA GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:01:00.0
totalMemory: 6.00GiB freeMemory: 5.09GiB
2023-08-25 15:23:37.478681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2023-08-25 15:23:37.478801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-08-25 15:23:37.478813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2023-08-25 15:23:37.478817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2023-08-25 15:23:37.478968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1194] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.
2023-08-25 15:23:37.479033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4913 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
2023-08-25 15:23:37.711796: F /home/engineer1/dirt/csrc/gl_common.h:46] extensions eglQueryDevicesEXT, eglQueryDeviceAttribEXT and eglGetPlatformDisplayEXT not available
Aborted
```
"
61692,JIT yields inconsistent results using tf.math.top_k,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.15.0-dev20230824

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda_11.8.r11.8/compiler.31833905_0 / cuDNN version 8700

### GPU model and memory

NVIDIA GeForce RTX 2080 Ti

### Current behavior?

JIT yields inconsistent results using `tf.math.top_k` when `index_type=tf.int32` (no issue with `index_type=tf.int64`).

### Standalone code to reproduce the issue

```shell
import tensorflow as tf


@tf.function(jit_compile=True)
def tf_func(shape):
    x = tf.random.stateless_normal(shape, seed=(1, 2))
    x = tf.transpose(x, perm=[1, 0])
    topk_max, indices = tf.math.top_k(x, 1, sorted=False, index_type=tf.int32)
    reduce_max = tf.reduce_max(x, axis=1, keepdims=True)
    return topk_max - reduce_max


def check(shape):
    should_be_all_zero = tf_func(shape)
    print(f""should_be_all_zero shape {shape}:\n{should_be_all_zero}"")


check((1024, 3))
check((1023, 3))
check((1024, 2))
```


### Relevant log output

```shell
should_be_all_zero shape (1024, 3):
[[-0.7787831 ]
 [ 0.47324872]
 [ 0.30553436]]
should_be_all_zero shape (1023, 3):
[[0.]
 [0.]
 [0.]]
should_be_all_zero shape (1024, 2):
[[0.]
 [0.]]
```
"
61691,Nvidia RTX 3500 Ada generation graphic card not recognised by tensorflow?,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

v11

### GPU model and memory

RTX 3500 Ada generation mobile 12GB

### Current behavior?

The RTX 3500 card cannot be recognised by tensorflow. I ran tf.config.list_physical_devices('GPU') and tf.test.is_gpu_available(). The results returned ""0 GPUs available"" and ""False"".

The microsoft visual studio 2017 is also installed.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))

tf.test.is_gpu_available()
```


### Relevant log output

_No response_"
61689,Error starting TensorFlow in python,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

-

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

CUDA 12.0 / cuDNN 8.9.4

### GPU model and memory

GEFORCE RTX 2080 SUPER 

### Current behavior?

I don't understand why the error occurs..please help me...

### Standalone code to reproduce the issue

```shell
$ python3
Python 3.8.10 (default, May 26 2023, 14:05:08) 
[GCC 9.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorrt
>>> import tensorflow
2023-08-25 11:11:47.521702: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-08-25 11:11:47.546940: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-08-25 11:11:47.547250: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
```


### Relevant log output

```shell
$ nvidia-smi
Fri Aug 25 11:15:19 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
|  0%   46C    P8    15W / 250W |    366MiB /  8192MiB |     25%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A       924      G   /usr/lib/xorg/Xorg                 35MiB |
|    0   N/A  N/A      1649      G   /usr/lib/xorg/Xorg                130MiB |
|    0   N/A  N/A      1790      G   /usr/bin/gnome-shell               36MiB |
|    0   N/A  N/A      2098      G   /usr/lib/firefox/firefox          152MiB |
+-----------------------------------------------------------------------------+

$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243


$ pip show tensorflow
Name: tensorflow
Version: 2.13.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/chemistry/.local/lib/python3.8/site-packages
Requires: libclang, six, absl-py, opt-einsum, typing-extensions, astunparse, tensorboard, packaging, tensorflow-estimator, keras, protobuf, h5py, termcolor, numpy, gast, google-pasta, tensorflow-io-gcs-filesystem, setuptools, grpcio, flatbuffers, wrapt
Required-by: 

$ pip show tensorrt
Name: tensorrt
Version: 8.6.1
Summary: A high performance deep learning inference library
Home-page: https://developer.nvidia.com/tensorrt
Author: NVIDIA Corporation
Author-email: None
License: Proprietary
Location: /home/chemistry/.local/lib/python3.8/site-packages
Requires: 
Required-by:

$ cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2
#define CUDNN_MAJOR 8
#define CUDNN_MINOR 9
#define CUDNN_PATCHLEVEL 4
--
#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)

/* cannot use constexpr here since this is a C-only file */
```
"
61688,Error starting Tensorflow in Python,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Rocky Linux 8.8

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12

### GPU model and memory

24 GB

### Current behavior?

Unable to load tensorflow in python with CUDA 12.2. Looking for some pointers

### Standalone code to reproduce the issue

```shell
pip install tensorflow
python
>>> import tensorflow as tf
 python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""                                                                                
2023-08-24 15:39:47.107770: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-08-24 15:39:47.108805: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.                                                                          
2023-08-24 15:39:47.130307: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.                                                                          
2023-08-24 15:39:47.130612: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.                       
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.                                                                            
2023-08-24 15:39:47.496076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT                                                                                      
2023-08-24 15:39:48.008342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355                                                                                
2023-08-24 15:39:48.023762: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would
like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.                                                              
Skipping registering GPU devices...
[]
```


### Relevant log output

_No response_"
61687,Dropout Layer training=True -- layout failed: INVALID_ARGUMENT,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

RHELS 7.9

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8 / 8.8

### GPU model and memory

NVIDIA A100-SXM4-40GB, Compute Capability 8.0

### Current behavior?

```
2023-08-24 11:01:15.073207: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inCustom_MC_Dropout_CNN/monte_carlo_dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
```

This arises from keras Dropout layer with training set to True for MC Dropout implementation.  Unclear why this happens in 2.13 and does not happen in previous versions of Tensorflow.  

gist can be found [here](https://gist.github.com/meekus-fischer/46d92232d9c503be3bfa76f225315b72)

### Standalone code to reproduce the issue

```shell
Custom model doing binary classification.  

gist can be found [here](https://gist.github.com/meekus-fischer/46d92232d9c503be3bfa76f225315b72)
```


### Relevant log output

```shell
Epoch 1/2000
2023-08-24 11:01:15.073207: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inCustom_MC_Dropout_CNN/monte_carlo_dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
2023-08-24 11:01:18.489988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8800
2023-08-24 11:01:19.296731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8800
2023-08-24 11:01:20.504944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8800
2023-08-24 11:01:21.410170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8800
2023-08-24 11:01:22.693953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
NCCL version 2.13.4+cudaCUDA_MAJOR.CUDA_MINOR
2023-08-24 11:01:25.775749: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0008dbb5b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-08-24 11:01:25.775881: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0
2023-08-24 11:01:25.775907: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100-SXM4-40GB, Compute Capability 8.0
2023-08-24 11:01:25.775920: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100-SXM4-40GB, Compute Capability 8.0
2023-08-24 11:01:25.775931: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100-SXM4-40GB, Compute Capability 8.0
2023-08-24 11:01:25.786715: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-08-24 11:01:25.786798: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-08-24 11:01:26.258524: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
  10622/Unknown - 346s 30ms/step - Accuracy: 0.8316 - Loss: 0.2066 - Precision: 0.0749 - Recall: 0.5010 - F1: 0.13022023-08-24 11:06:50.967839: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10230612448716145652
2023-08-24 11:06:50.968030: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15407084872456274683
2023-08-24 11:06:50.968252: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17592376860366209384
2023-08-24 11:06:50.968315: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 59586944474759984
2023-08-24 11:06:50.968338: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4843790225391952165
2023-08-24 11:06:50.968526: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5723252417712899607
2023-08-24 11:06:50.968555: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7116492272812476647
2023-08-24 11:06:50.968575: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6169583800350491318
2023-08-24 11:06:50.968603: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14279008608205112804
2023-08-24 11:06:50.968624: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2035256671229577509
2023-08-24 11:06:50.968637: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16934632576416694905
2023-08-24 11:06:50.968693: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3559188353333742787
2023-08-24 11:06:50.968719: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15823058833201008187
2023-08-24 11:06:50.968737: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14146042709357596467
2023-08-24 11:06:50.968754: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7736629887233675932
2023-08-24 11:06:50.968772: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8429871751161250943
2023-08-24 11:06:50.968797: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10517342425574574684
2023-08-24 11:06:50.968818: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6970197396290489839
2023-08-24 11:06:50.968831: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4931681574040413300
2023-08-24 11:06:50.968875: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 626925344230971222
2023-08-24 11:06:50.968901: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7470810518621668803
2023-08-24 11:06:50.968925: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12385602897828204054
2023-08-24 11:06:50.968946: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18188314560958681369
2023-08-24 11:06:50.968958: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13624849890299536610
2023-08-24 11:06:50.969001: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17807137559239481777
2023-08-24 11:06:50.969025: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2139352706329042097
2023-08-24 11:06:50.969044: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1640340635315586973
2023-08-24 11:06:50.969069: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6223702484981372229
2023-08-24 11:06:50.969089: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10172410999163213764
2023-08-24 11:06:50.969122: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6497789849068078695
2023-08-24 11:06:50.969166: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14547651345659747702
2023-08-24 11:06:50.969193: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17623891083240824387
2023-08-24 11:06:50.969211: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13064636275713549317
2023-08-24 11:06:50.969228: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12396583141502166397
2023-08-24 11:06:50.969246: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5913395196567159933
2023-08-24 11:06:50.969271: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13627801007941572809
2023-08-24 11:06:50.969293: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14421550386438117999
2023-08-24 11:06:50.969307: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17275079556186662745
2023-08-24 11:06:50.969351: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17233527917734094487
2023-08-24 11:06:50.969377: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1559338696184663628
2023-08-24 11:06:50.969400: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5466597647379635179
2023-08-24 11:06:50.969419: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3733731331292716199
2023-08-24 11:06:50.969433: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1612774677619861335
2023-08-24 11:06:50.969477: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8582450577167281147
2023-08-24 11:06:50.969501: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2782820814590454977
2023-08-24 11:06:50.969519: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3393985160674389282
2023-08-24 11:06:50.969537: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10933067372207001568
2023-08-24 11:06:50.969561: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12568695295317122124
2023-08-24 11:06:50.969581: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9541194832674758364
2023-08-24 11:06:50.969594: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8581202827114056180
2023-08-24 11:06:50.969638: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6233296683631670242
2023-08-24 11:06:50.969672: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9668322976265270062
2023-08-24 11:06:50.969689: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14184838215839790752
2023-08-24 11:06:50.969706: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7224663333712770602
2023-08-24 11:06:50.969723: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7397044618240997942
2023-08-24 11:06:50.969740: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8130222007033543507
2023-08-24 11:06:50.969784: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9705492144130363178
2023-08-24 11:06:50.969807: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16679483023940330274
2023-08-24 11:06:50.969828: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3293157344159655099
2023-08-24 11:06:50.969841: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16377046709051984690
2023-08-24 11:06:50.969887: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15402017112026381377
2023-08-24 11:06:50.969912: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 9526661262069731973
2023-08-24 11:06:50.969934: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13997955057405387944
2023-08-24 11:06:50.969954: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7535581894821128379
2023-08-24 11:06:50.969967: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8329440098288561250
2023-08-24 11:06:50.970010: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9064305880271633177
2023-08-24 11:06:50.970033: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10920339178713879663
2023-08-24 11:06:50.970054: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17331780224083848384
2023-08-24 11:06:50.970073: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4387767839775239829
2023-08-24 11:06:50.970087: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14235745259444133525
2023-08-24 11:06:50.970129: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15121875451797684147
2023-08-24 11:06:50.970156: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2086183684301986528
2023-08-24 11:06:50.970177: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13396972508038279194
2023-08-24 11:06:50.970197: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3769933078929813764
2023-08-24 11:06:50.970210: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9191688026028395470
2023-08-24 11:06:50.970257: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16698476566770966153
2023-08-24 11:06:50.970278: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14753529190433239649
2023-08-24 11:06:50.970298: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13900114517410444549
2023-08-24 11:06:50.970314: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2047052490661242150
2023-08-24 11:06:50.970342: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11154800211652874765
2023-08-24 11:06:50.970361: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9142657018531009635
2023-08-24 11:06:50.970377: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5427900937422839708
2023-08-24 11:06:50.970398: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16660179110683671619
2023-08-24 11:06:50.970415: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12852018438355285325
2023-08-24 11:06:50.970464: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 607352058025025138
2023-08-24 11:06:50.970486: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5852029023401935321
2023-08-24 11:06:50.970504: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3708094119504533135
2023-08-24 11:06:50.970528: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12012444379425185333
2023-08-24 11:06:50.970548: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2694129016930453301
2023-08-24 11:06:50.970565: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16690881604261796875
2023-08-24 11:06:50.970588: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3200238774805194023
2023-08-24 11:06:50.970606: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12303021566759651641
2023-08-24 11:06:50.970623: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12559606935649894050
2023-08-24 11:06:50.970652: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14073952303962254143
2023-08-24 11:06:50.970672: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12728231451437746801
2023-08-24 11:06:50.970689: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 275694950219046675
2023-08-24 11:06:50.970712: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8794029419552633053
2023-08-24 11:06:50.970731: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10700741266189635836
2023-08-24 11:06:50.970747: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12210410409207542263
2023-08-24 11:06:50.970761: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1426965020033241286
2023-08-24 11:06:50.970776: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14005291578614903458
2023-08-24 11:06:50.970789: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7129919436717380486
2023-08-24 11:06:50.970804: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 305166255698457613
2023-08-24 11:06:50.970818: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2298180445646427191
2023-08-24 11:06:50.970832: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2169690745073579443
2023-08-24 11:06:50.970846: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4743653862056645976
2023-08-24 11:06:50.970860: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13014453433668203963
2023-08-24 11:06:50.970874: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9907502355052484286
2023-08-24 11:06:50.970888: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14023629169546197118
2023-08-24 11:06:50.970902: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8147796953383623256
2023-08-24 11:06:50.970916: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11731520165517458923
2023-08-24 11:06:50.970940: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13054994235575065371
2023-08-24 11:06:50.970959: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9747176552551324986
2023-08-24 11:06:50.970973: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17117647137538334341
2023-08-24 11:06:50.970989: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5153611993980018316
2023-08-24 11:06:50.971003: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15004025391910118056
2023-08-24 11:06:50.971018: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3063790399388444401
2023-08-24 11:06:50.971052: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3830525127148040556
2023-08-24 11:06:50.971068: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4749045392559338476
2023-08-24 11:06:50.971115: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16071133848988202381
2023-08-24 11:06:50.971131: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17981742675600701201
2023-08-24 11:06:50.971146: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17623561159131070056
2023-08-24 11:06:50.971168: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15285201522450726881
2023-08-24 11:07:29.289624: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4075359580615049412
2023-08-24 11:07:29.289849: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15086777354078093967
2023-08-24 11:07:29.289868: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7764170359290675737
2023-08-24 11:07:29.289886: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4281287878382289967
2023-08-24 11:07:29.289902: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15716950925811516672
2023-08-24 11:07:29.289920: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18437880324272310277
2023-08-24 11:07:29.289936: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 84132605581087162
2023-08-24 11:07:29.289954: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9426939307156020373
2023-08-24 11:07:29.290198: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2146862422121390738
2023-08-24 11:07:29.290235: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3462541954341437694
2023-08-24 11:07:29.290430: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4757251772246970932
2023-08-24 11:07:29.290456: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2310900862945371053
2023-08-24 11:07:29.290496: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8093777233699734653
2023-08-24 11:07:29.290526: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3771959543230644229
2023-08-24 11:07:29.290571: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5900557290590721187
2023-08-24 11:07:29.290597: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2455567737869551008
2023-08-24 11:07:29.290686: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6098076008741815878
2023-08-24 11:07:29.290723: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1553848879136391844
2023-08-24 11:07:29.290769: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13917212505871489327
2023-08-24 11:07:29.290795: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9809958572552914958
2023-08-24 11:07:29.290821: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 198252566913829033
2023-08-24 11:07:29.290837: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5073347325328566998
2023-08-24 11:07:29.290855: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5978006840724482833
2023-08-24 11:07:29.290872: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12595934042829111589
2023-08-24 11:07:29.290916: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9387251194334273292
2023-08-24 11:07:29.290940: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16968343877100670238
2023-08-24 11:07:29.290963: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15297105432650139094
2023-08-24 11:07:29.290990: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4081987408384352687
2023-08-24 11:07:29.291016: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5493947754659818087
2023-08-24 11:07:29.291028: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6850845529302741243
2023-08-24 11:07:29.291071: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15022677012545259154
2023-08-24 11:07:29.291096: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13233726862595811952
2023-08-24 11:07:29.291123: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14234134314023218357
2023-08-24 11:07:29.291139: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12483608611790893202
2023-08-24 11:07:29.291156: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12148270049744368847
2023-08-24 11:07:29.291171: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5321921797854171573
2023-08-24 11:07:29.291197: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1832801212251014439
2023-08-24 11:07:29.291222: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8390992797373478667
2023-08-24 11:07:29.291245: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15510758115605779543
2023-08-24 11:07:29.291271: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5470046073078944888
2023-08-24 11:07:29.291283: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14658565871101820964
2023-08-24 11:07:29.291327: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14317614836688813112
2023-08-24 11:07:29.291351: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6236598275352729223
2023-08-24 11:07:29.291384: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8714587429891558384
2023-08-24 11:07:29.291412: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11498223346710789742
2023-08-24 11:07:29.291435: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14476188790886421798
2023-08-24 11:07:29.291458: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12475594966318812074
2023-08-24 11:07:29.291483: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11601898139673393738
2023-08-24 11:07:29.291506: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4976030311990886841
2023-08-24 11:07:29.291529: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6617209893611589109
2023-08-24 11:07:29.291551: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13177787027967964473
2023-08-24 11:07:29.291574: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16943892717213549101
2023-08-24 11:07:29.291596: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2223833743739703384
2023-08-24 11:07:29.291618: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10219062227532962953
2023-08-24 11:07:29.291645: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13724040127314953106
2023-08-24 11:07:29.291668: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2067624533502072576
2023-08-24 11:07:29.291684: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2693354250884363784
2023-08-24 11:07:29.291707: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5384013371876801148
2023-08-24 11:07:29.291731: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16838618676439021446
2023-08-24 11:07:29.291753: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8576730697822492479
2023-08-24 11:07:29.291776: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7205413612177464488
2023-08-24 11:07:29.291798: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7419410273381722556
2023-08-24 11:07:29.291820: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12387571022272485464
2023-08-24 11:07:29.291842: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10096942157971947615
2023-08-24 11:07:29.291865: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17235744156686445141
2023-08-24 11:07:29.291886: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7157660774755047909
2023-08-24 11:07:29.291908: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3223238886696215390
2023-08-24 11:07:29.291930: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6959224094144924436
2023-08-24 11:07:29.291952: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 320303901872528994
2023-08-24 11:07:29.291974: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4678411222501451155
2023-08-24 11:07:29.292004: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11466697275319965486
2023-08-24 11:07:29.292027: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16940283489605771424
2023-08-24 11:07:29.292050: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4318812230278670660
2023-08-24 11:07:29.292074: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10793570788563462816
2023-08-24 11:07:29.292096: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16389789610319880674
2023-08-24 11:07:29.292118: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15412920732005214795
2023-08-24 11:07:29.292141: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9706362058363342851
2023-08-24 11:07:29.292162: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 282440926482311630
2023-08-24 11:07:29.292184: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16604411440337789159
2023-08-24 11:07:29.292209: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2305311315300667846
2023-08-24 11:07:29.292231: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13471908878414400865
2023-08-24 11:07:29.292252: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5920473717615552089
2023-08-24 11:07:29.292276: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17321862789316030721
2023-08-24 11:07:29.292297: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 230923248894930545
2023-08-24 11:07:29.292319: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9790266787664553869
2023-08-24 11:07:29.292341: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8329428023832785896
2023-08-24 11:07:29.292364: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7908458004814008345
2023-08-24 11:07:29.292385: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4993197094509241984
2023-08-24 11:07:29.292407: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4091926352704099496
2023-08-24 11:07:29.292429: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8630806193082773190
2023-08-24 11:07:29.292451: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1510013706380373244
2023-08-24 11:07:29.292473: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7061673450255603616
2023-08-24 11:07:29.292496: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16228952560203272745
2023-08-24 11:07:29.292517: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15134061215888175049
2023-08-24 11:07:29.292539: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2181129643598772086
2023-08-24 11:07:29.292561: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12275612442691882545
2023-08-24 11:07:29.292582: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11044755321133213194
2023-08-24 11:07:29.292604: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6335108143304021966
2023-08-24 11:07:29.292644: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5901561706985968925
2023-08-24 11:07:29.292668: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15343645954539090124
2023-08-24 11:07:29.292692: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7889586265790665191
2023-08-24 11:07:29.292719: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9687346430831728350
2023-08-24 11:07:29.292741: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10459441727503677044
2023-08-24 11:07:29.292764: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9104442354481575726
2023-08-24 11:07:29.292786: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15547032933648534830
2023-08-24 11:07:29.292807: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7890544443613869694
2023-08-24 11:07:29.292830: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4123678406397018076
2023-08-24 11:07:29.292852: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7440280401194482935
2023-08-24 11:07:29.292875: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8646180232994160174
2023-08-24 11:07:29.292894: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15193230168019467790
2023-08-24 11:07:29.292917: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9199876006741229390
```
```
"
61683,Batch matmul imprecision,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13 / 2.14.0-dev20230706

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

```
$ mamba list | grep cu
cuda-nvcc                 12.2.128                      0    nvidia
cudatoolkit               11.8.0              h4ba93d1_12    conda-forge
nvidia-cublas-cu11        2022.4.8                 pypi_0    pypi
nvidia-cublas-cu117       11.10.1.25               pypi_0    pypi
nvidia-cuda-nvrtc-cu11    2022.4.8                 pypi_0    pypi
nvidia-cuda-nvrtc-cu117   11.7.50                  pypi_0    pypi
nvidia-cudnn-cu11         8.9.4.25                 pypi_0    pypi
```
### GPU model and memory

NVIDIA GeForce RTX 3060 - 12 GB
Driver Version: 520.61.05

### Current behavior?

I'm trying to do a batch matrix multiply (i.e. I've got a bunch of m x n matrices, all in one tensor, and I want to do a matrix multiply of each of them with some other matrix). However, I'm getting slightly different results than I get from Numpy (and previous TensorFlow versions, this script passed for me in 2.9). 

One interesting thing is that the value 25 (the second dimension of `x`) is significant. If I reduce this to 16 or below, it passes. Also, if I change the second dimension of `c` from 2 to 1, it also passes.

### Standalone code to reproduce the issue

```python
import numpy as np
import tensorflow as tf

print(f""file: {tf.__file__}"")
print(f""git version: {tf.version.GIT_VERSION}"")
print(f""version: {tf.__version__}"")

rng = np.random.RandomState(0)
x = rng.uniform(-1, 1, size=(1, 25, 5)).astype(np.float32)
c = rng.uniform(-1, 1, size=(5, 2)).astype(np.float32)

y = x @ c

x2 = np.tile(x, (2, 1, 1))
y2 = x2 @ c

for y2i in y2:
    np.testing.assert_allclose(y2i, y.squeeze(0))

tols = dict(atol=1e-7, rtol=1e-5)

z = tf.matmul(x, c).numpy()
# z = tf.einsum(""...tq,...qr->...tr"", x, c)
np.testing.assert_allclose(z, y, **tols)

z2 = tf.matmul(x2, c).numpy()
# z2 = tf.einsum(""...tq,...qr->...tr"", x2, c)
np.testing.assert_allclose(z2, y2, **tols)
```


### Relevant log output

```shell
file: /home/ehunsber/mambaforge/envs/tf213/lib/python3.8/site-packages/tensorflow/__init__.py
git version: v1.12.1-96406-gfa4d29bfef8
version: 2.14.0-dev20230706
Traceback (most recent call last):
  File ""test_batch.py"", line 26, in <module>
    np.testing.assert_allclose(z2, y2)
  File ""/home/ehunsber/mambaforge/envs/tf213/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 1592, in assert_allclose
    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
  File ""/home/ehunsber/mambaforge/envs/tf213/lib/python3.8/contextlib.py"", line 75, in inner
    return func(*args, **kwds)
  File ""/home/ehunsber/mambaforge/envs/tf213/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 862, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=1e-07, atol=0

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 0.00046098
Max relative difference: 0.01047752
 x: array([[[-0.187503,  0.005696],
        [-0.255552, -0.84404 ],
        [ 0.268836, -1.250793],...
 y: array([[[-0.187499,  0.005691],
        [-0.255404, -0.844322],
        [ 0.268935, -1.251033],...
```
"
61680,Share layer weights between processes in cluster,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.10.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current behavior?

Hey guys, I have a question regarding sharing layer weights between processes of a cluster.

I am in the flied of Deep Reinforcement Learning, where often there are multiple actors, generating experience (samples) and a central learner, who updates the network weights based on the gathered experience. For this to work efficient actors and learner are running in different processes on a single machine or possible on multiple machines. `tf.distribute` contains a lot functionalities useful for distributed Supervised Learning, but due to the different nature of DRL, `tf.distribute` can hardly be applied for this field.

Is there currently a way, or possibly a planned feature, to share trainable variables between TensorFlow instances running on different processes of a cluster? This is already done under the hood in `tf.distribute.experimental.MultiWorkerMirroredStrategy` and `tf.distribute.experimental.ParameterServerStrategy`, but as I mentioned above, are these distribution strategies not really suitable for DRL.

In the code below I create a cluster of processes using `tf.config.experimental_connect_to_cluster`, since running eager code and/or `tf.function` directly on a `tf.distribute.Server` is not possible. Experience is send to the learner via a shared `tf.queue.FIFOQueue` and now I would like to share the layer weights between the actor and learner.

### Standalone code to reproduce the issue

```shell
from absl import app
import multiprocessing as mp
import tensorflow as tf
from tensorflow.keras.layers import Dense


def run_actor_process(cluster_spec : tf.train.ClusterSpec) -> None:
    """"""
    Runs an actor which gathers experience and enqueues it to the learner.

    Args:
        cluster_spec (tf.distribute.Server):        The cluster specification the actor belongs to.
    """"""

    # only use first GPU
    physical_devices = tf.config.list_physical_devices('GPU')
    tf.config.set_visible_devices([physical_devices[0]], 'GPU')
    tf.config.experimental.set_memory_growth(physical_devices[0], True)

    # connect process to cluster
    tf.config.experimental_connect_to_cluster(cluster_spec, job_name='actor', task_index=0, protocol='grpc')

    # use shared queue on actor
    # NOTE: This needs to be done before any real TensorFlow operation is executed, otherwise the shared queue will not work.
    #       Not sure why this happens.
    with tf.device('/job:learner/replica:0/task:0/CPU:0'):
        queue = tf.queue.FIFOQueue(1, [tf.float32, tf.float32], shapes=[(5, ), (2, )], shared_name='trajectory_buffer')

    # NOTE: Ideally this layer's weights and biases would be shared between the processes.
    dense_layer = Dense(units=2,
                        activation=None,
                        use_bias=True,
                        kernel_initializer='glorot_uniform',
                        bias_initializer='zeros',
                        name='model_dense')

    @tf.function
    def run_actor(input_tensor : tf.Tensor):

        output_tensor = dense_layer(input_tensor)
        _ = queue.enqueue([tf.squeeze(input_tensor), tf.squeeze(output_tensor)])
        return output_tensor

    index = 1
    while True:

        observation = tf.range(index, index + 5, delta=1, dtype=tf.float32, name='range')
        observation = tf.expand_dims(observation, 0)

        output = run_actor(observation)

        print(f'Actor enqueued [{observation[0].numpy()}, {output[0].numpy()}].')
        index += 1


def run_server_process(cluster_spec : tf.distribute.Server) -> None:
    """"""
    Runs a TensorFlow distributed server.

    Args:
        cluster_spec (tf.distribute.Server):        The cluster specification the server belongs to.
    """"""

    # disable GPUs for TensorFlow session and server
    tf.config.set_visible_devices([], 'GPU')
    server_config = tf.compat.v1.ConfigProto(device_count={'GPU': 0})

    unused_server = tf.distribute.Server(cluster_spec,
                                         job_name='ps',
                                         task_index=0,
                                         protocol='grpc',
                                         config=server_config)
    unused_server.join()

def main(unused_argv):

    mp.set_start_method('spawn')

    # only use first GPU
    physical_devices = tf.config.list_physical_devices('GPU')
    tf.config.set_visible_devices([physical_devices[0]], 'GPU')
    tf.config.experimental.set_memory_growth(physical_devices[0], True)

    # define training cluster
    cluster_spec = tf.train.ClusterSpec({'ps' : ['localhost:7000'],
                                         'learner': ['localhost:7001'],
                                         'actor' : ['localhost:7002']})

    # run the actor in a separate process
    actor_process = mp.Process(target=run_actor_process, args=(cluster_spec, ), daemon=True)
    actor_process.start()

    # start parameter sever to initialize cluster
    # NOTE: This running server is solely needed to mimic a running cluster which the actor and learner can
    #       connect to using 'tf.config.experimental_connect_to_cluster'.
    server_process = mp.Process(target=run_server_process, args=(cluster_spec, ), daemon=True)
    server_process.start()

    # NOTE: The learner should not see the actor, otherwise the learner will wait until the actor is connected
    #       and the actor will wait until the leaner is connected, resulting in a deadlock.
    learner_cluster_spec = tf.train.ClusterSpec({'ps' : ['localhost:7000'],
                                                 'learner': ['localhost:7001']})

    # connect process to cluster
    tf.config.experimental_connect_to_cluster(learner_cluster_spec,
                                              job_name='learner',
                                              task_index=0,
                                              protocol='grpc')

    # use shared queue on learner
    # NOTE: This needs to be done before any real TensorFlow operation is executed, otherwise the shared queue will not work.
    #       Not sure why this happens.
    with tf.device('/job:learner/replica:0/task:0/CPU:0'):
        queue = tf.queue.FIFOQueue(1, [tf.float32, tf.float32], shapes=[(5, ), (2, )], shared_name='trajectory_buffer')

    # NOTE: Ideally this layer's weights and biases would be shared between the processes.
    dense_layer = Dense(units=2,
                        activation=None,
                        use_bias=True,
                        kernel_initializer='glorot_uniform',
                        bias_initializer='zeros',
                        name='model_dense')

    @tf.function
    def run_learner():

        dequeued = queue.dequeue_many(1)
        return dequeued, dense_layer(dequeued[0])

    while True:

        dequeued, learner_result = run_learner()

        # NOTE: If layer weights would be shared between learner and actor 'dequeued[1][0]' and 'learner_result[0]' would be equal.
        print(f'Learner dequeued [{dequeued[0][0].numpy()}, {dequeued[1][0].numpy()}] with result {learner_result[0].numpy()}.')


if __name__ == ""__main__"":
    try:
        app.run(main)
    except SystemExit:
        pass
```


### Relevant log output

_No response_"
61678,Recent update of xnnpack broke compilation of TensorFlow Lite,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Didn't tried

### Source

source

### TensorFlow version

7701a45e3893bbf8c451f6437d039172db89d548

### Custom code

No

### OS platform and distribution

Raspberry Pi 4, Raspbian 

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Raspberry Pi 4

### Current behavior?

Updating **xnnpack** in https://github.com/tensorflow/tensorflow/commit/7701a45e3893bbf8c451f6437d039172db89d548 broke the compilation with this error:

```
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c: In function â€˜init_f16_gemm_configâ€™:
gmake[3]: Entering directory '/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
cd /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build && /home/pi/.local/pipx/venvs/cmake/lib/python3.9/site-packages/cmake/data/bin/cmake -E cmake_depends ""Unix Makefiles"" /home/pi/src/tensorflow_src/tensorflow/lite /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/abseil-cpp/absl/strings /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/_deps/abseil-cpp-build/absl/strings /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/_deps/abseil-cpp-build/absl/strings/CMakeFiles/absl_cordz_handle.dir/DependInfo.cmake --color=
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c:159:18: error: â€˜cpuinfo_uarch_cortex_a715â€™ undeclared (first use in this function); did you mean â€˜cpuinfo_uarch_cortex_a710â€™?
  159 |             case cpuinfo_uarch_cortex_a715:
      |                  ^~~~~~~~~~~~~~~~~~~~~~~~~
      |                  cpuinfo_uarch_cortex_a710
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c:159:18: note: each undeclared identifier is reported only once for each function it appears in
gmake[3]: Entering directory '/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c:162:18: error: â€˜cpuinfo_uarch_cortex_x3â€™ undeclared (first use in this function); did you mean â€˜cpuinfo_uarch_cortex_x2â€™?
  162 |             case cpuinfo_uarch_cortex_x3:
      |                  ^~~~~~~~~~~~~~~~~~~~~~~
      |                  cpuinfo_uarch_cortex_x2
gmake[3]: Leaving directory '/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
/usr/bin/gmake  -f _deps/abseil-cpp-build/absl/flags/CMakeFiles/absl_flags_program_name.dir/build.make _deps/abseil-cpp-build/absl/flags/CMakeFiles/absl_flags_program_name.dir/build
```

The version of **cpuinfo** downloaded in `tensorflow/lite/tools/cmake/modules/cpuinfo.cmake` doesn't include `cpuinfo_uarch_cortex_a715` or `cpuinfo_uarch_cortex_x3`

### Standalone code to reproduce the issue

```shell
Follow steps from https://www.tensorflow.org/lite/guide/build_cmake_pip
```


### Relevant log output

```shell
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c: In function â€˜init_f16_gemm_configâ€™:
gmake[3]: Entering directory '/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
cd /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build && /home/pi/.local/pipx/venvs/cmake/lib/python3.9/site-packages/cmake/data/bin/cmake -E cmake_depends ""Unix Makefiles"" /home/pi/src/tensorflow_src/tensorflow/lite /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/abseil-cpp/absl/strings /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/_deps/abseil-cpp-build/absl/strings /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/_deps/abseil-cpp-build/absl/strings/CMakeFiles/absl_cordz_handle.dir/DependInfo.cmake --color=
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c:159:18: error: â€˜cpuinfo_uarch_cortex_a715â€™ undeclared (first use in this function); did you mean â€˜cpuinfo_uarch_cortex_a710â€™?
  159 |             case cpuinfo_uarch_cortex_a715:
      |                  ^~~~~~~~~~~~~~~~~~~~~~~~~
      |                  cpuinfo_uarch_cortex_a710
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c:159:18: note: each undeclared identifier is reported only once for each function it appears in
gmake[3]: Entering directory '/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c:162:18: error: â€˜cpuinfo_uarch_cortex_x3â€™ undeclared (first use in this function); did you mean â€˜cpuinfo_uarch_cortex_x2â€™?
  162 |             case cpuinfo_uarch_cortex_x3:
      |                  ^~~~~~~~~~~~~~~~~~~~~~~
      |                  cpuinfo_uarch_cortex_x2
gmake[3]: Leaving directory '/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
/usr/bin/gmake  -f _deps/abseil-cpp-build/absl/flags/CMakeFiles/absl_flags_program_name.dir/build.make _deps/abseil-cpp-build/absl/flags/CMakeFiles/absl_flags_program_name.dir/build
```
"
61677,Unit test failure when built with clang,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

16.0.6

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

//tensorflow/compiler/mlir/lite/sparsity:sparsify_model_test throws a segfault

### Standalone code to reproduce the issue

```shell
bazel test --cache_test_results=no --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --build_tests_only -- //tensorflow/compiler/mlir/lite/sparsity:sparsify_model_test
```


### Relevant log output

```shell
==================== Test output for //tensorflow/compiler/mlir/lite/sparsity:sparsify_model_test:
2023-08-23 14:23:54.976080: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-08-23 14:23:54.977641: W tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay. The old value will be erased in order to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.
Running main() from gmock_main.cc
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from SparsifyModelTest
[ RUN      ] SparsifyModelTest.MetadataIsAddedToOutputModel
================================================================================
Target //tensorflow/compiler/mlir/lite/sparsity:sparsify_model_test up-to-date:
  bazel-bin/tensorflow/compiler/mlir/lite/sparsity/sparsify_model_test
INFO: Elapsed time: 557.635s, Critical Path: 355.66s
INFO: 4267 processes: 707 internal, 3560 local.
INFO: Build completed, 1 test FAILED, 4267 total actions
//tensorflow/compiler/mlir/lite/sparsity:sparsify_model_test             FAILED in 1.0s
  /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/lite/sparsity/sparsify_model_test/test.log

Executed 1 out of 1 test: 1 fails locally.
andrew@8bde10e59b61:/workspace$ gdb bazel-bin/tensorflow/compiler/mlir/lite/sparsity/sparsify_model_test
GNU gdb (Ubuntu 9.2-0ubuntu1~20.04.1) 9.2
Copyright (C) 2020 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
Type ""show copying"" and ""show warranty"" for details.
This GDB was configured as ""aarch64-linux-gnu"".
Type ""show configuration"" for configuration details.
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
    <http://www.gnu.org/software/gdb/documentation/>.

For help, type ""help"".
Type ""apropos word"" to search for commands related to ""word""...
Reading symbols from bazel-bin/tensorflow/compiler/mlir/lite/sparsity/sparsify_model_test...
(gdb) run
Starting program: /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/sparsity/sparsify_model_test 
warning: Error disabling address space randomization: Operation not permitted
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/aarch64-linux-gnu/libthread_db.so.1"".
2023-08-23 14:26:40.332993: W tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay. The old value will be erased in order to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.
Running main() from gmock_main.cc
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from SparsifyModelTest
[ RUN      ] SparsifyModelTest.MetadataIsAddedToOutputModel

Program received signal SIGSEGV, Segmentation fault.
0x0000ffff80a09040 in ?? () from /lib/aarch64-linux-gnu/libc.so.6
(gdb) bt
#0  0x0000ffff80a09040 in ?? () from /lib/aarch64-linux-gnu/libc.so.6
#1  0x0000ffff8b3df258 in std::__fill_a1<unsigned char> (__first=0xaaaaf5cbc9c1 """", __last=0xaaaaf5cbc9c0 """", __c=@0xaaaaf5cbc9c0: 0 '\000')
    at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_algobase.h:893
#2  std::__fill_a<unsigned char*, unsigned char> (__first=0xaaaaf5cbc9c1 """", __last=0xaaaaf5cbc9c0 """", __value=@0xaaaaf5cbc9c0: 0 '\000')
    at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_algobase.h:914
#3  std::__fill_n_a<unsigned char*, unsigned long, unsigned char> (__n=18446744073708562752, __value=@0xaaaaf5cbc9c0: 0 '\000', 
    __first=<optimized out>) at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_algobase.h:1065
#4  std::fill_n<unsigned char*, unsigned long, unsigned char> (__n=18446744073708562752, __value=@0xaaaaf5cbc9c0: 0 '\000', 
    __first=<optimized out>) at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_algobase.h:1094
#5  std::__uninitialized_default_n_1<true>::__uninit_default_n<unsigned char*, unsigned long> (__first=0xaaaaf5cbc9c0 """", __n=<optimized out>)
    at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_uninitialized.h:598
#6  std::__uninitialized_default_n<unsigned char*, unsigned long> (__first=0xaaaaf5cbc9c0 """", __n=<optimized out>)
    at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_uninitialized.h:632
#7  std::__uninitialized_default_n_a<unsigned char*, unsigned long, unsigned char> (__first=0xaaaaf5cbc9c0 """", __n=<optimized out>)
    at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_uninitialized.h:698
#8  std::vector<unsigned char, std::allocator<unsigned char> >::_M_default_initialize (this=0xffffe8e7e728, __n=<optimized out>)
    at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:1606
#9  std::vector<unsigned char, std::allocator<unsigned char> >::vector (this=0xffffe8e7e728, __n=0, __a=...)
    at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:512
#10 (anonymous namespace)::Translator::BuildCustomOperator (this=this@entry=0xffffe8e801b8, inst=<optimized out>, op=..., 
    operands=std::vector of length 1, capacity 1 = {...}, results=std::vector of length 1, capacity 1 = {...})
    at tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1248
#11 0x0000ffff8b3d7ca0 in (anonymous namespace)::Translator::BuildOperator (this=this@entry=0xffffe8e801b8, inst=inst@entry=0xaaaaf5d48ec0, 
    operands=std::vector of length 1, capacity 1 = {...}, results=std::vector of length 1, capacity 1 = {...}, 
    intermediates=std::vector of length 0, capacity 0) at tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1434
#12 0x0000ffff8b3d0774 in (anonymous namespace)::Translator::BuildSubGraph (this=this@entry=0xffffe8e801b8, name=""main"", region=0x0, 
    index=index@entry=0) at tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2036
#13 0x0000ffff8b3c7934 in (anonymous namespace)::Translator::TranslateInternal[abi:cxx11]() (this=<optimized out>, this@entry=0xffffe8e801b8)
    at tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2429
#14 0x0000ffff8b3c5bb8 in (anonymous namespace)::Translator::Translate (module=..., toco_flags=..., Python Exception <class 'gdb.error'> No type named std::__detail::_Hash_node<class std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, true>.: 
tags=std::unordered_set with 0 elements, 
    op_or_arg_name_mapper=0xffffe8e7fbc0, metadata=Python Exception <class 'AttributeError'> 'NoneType' object has no attribute 'pointer': 
std::map with 1 element) at tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2331
#15 tflite::MlirToFlatBufferTranslateFunction (module=..., options=..., serialized_flatbuffer=serialized_flatbuffer@entry=0xffffe8e80a00)
    at tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2838
#16 0x0000ffff8b443498 in mlir::lite::SparsifyModel (input_model=..., builder=builder@entry=0xffffe8e80c08, 
    error_reporter=error_reporter@entry=0xffffe8e80c00) at tensorflow/compiler/mlir/lite/sparsity/sparsify_model.cc:91
#17 0x0000aaaae03023c0 in mlir::lite::(anonymous namespace)::SparsifyModelTest_MetadataIsAddedToOutputModel_Test::TestBody (
    this=<optimized out>) at tensorflow/compiler/mlir/lite/sparsity/sparsify_model_test.cc:67
#18 0x0000ffff81043fd4 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void> (
    method=&virtual testing::Test::TestBody(), location=0xffff80fef955 ""the test body"", object=<optimized out>)
    at external/com_google_googletest/googletest/src/gtest.cc:2599
#19 testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void> (object=0xaaaaf5c74320, 
    method=(void (testing::Test::*)(class testing::Test * const)) 0x20, location=0xffff80fef955 ""the test body"")
    at external/com_google_googletest/googletest/src/gtest.cc:2635
#20 0x0000ffff81043e6c in testing::Test::Run (this=0xaaaaf5c74320) at external/com_google_googletest/googletest/src/gtest.cc:2674
#21 0x0000ffff810454f8 in testing::TestInfo::Run (this=0xaaaaf5c67960) at external/com_google_googletest/googletest/src/gtest.cc:2853
#22 0x0000ffff81046480 in testing::TestSuite::Run (this=0xaaaaf5c740b0) at external/com_google_googletest/googletest/src/gtest.cc:3012
#23 0x0000ffff81057db4 in testing::internal::UnitTestImpl::RunAllTests (this=0xaaaaf5c73d30)
    at external/com_google_googletest/googletest/src/gtest.cc:5870
#24 0x0000ffff81057844 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (
    method=(bool (testing::internal::UnitTestImpl::*)(class testing::internal::UnitTestImpl * const)) 0xffff810579e8 <testing::internal::UnitTestImpl::RunAllTests()>, location=0xffff80fee5f4 ""auxiliary test code (environments or event listeners)"", object=<optimized out>)
--Type <RET> for more, q to quit, c to continue without paging--q
```
"
61676,Image storage,"When TFLite processes image data, which variable will the read in image data be stored in? Is it TfLiteTensor.data?


How can I directly process a single image using TensorFlow Lite source code without relying on an Android app?"
61675,tf.linalg.pinv isn't ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.6.22

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Pinv should give similar gradient to inv. Here's the code snippet and result.
![image](https://github.com/tensorflow/tensorflow/assets/10631563/de9cdb82-1c93-4a18-a79e-95c0b0834ae6)


![image](https://github.com/tensorflow/tensorflow/assets/10631563/cfdadbb9-5506-4181-a868-69830c5ee96b)


### Standalone code to reproduce the issue

```shell
See figures in behavior section.
```


### Relevant log output

_No response_"
61674,oneDNN matmul is not supported to have a dynamic dimension,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-98433-gb64e3d4ae2e 2.15.0-dev20230812

### Custom code

Yes

### OS platform and distribution

Linux Debian 6.3.11

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The following code works for nightly version 2.15.0-dev20230811, but fails for the version 2.15.0-dev20230812. The issue happens when we compile the function.

### Standalone code to reproduce the issue

```shell
import tensorflow.compat.v2 as tf

class _Slice2Idx:
  """"""Utility to convert numpy basic slices into TF scatter_nd indices.""""""

  def __init__(self, tensor):
    self.ranges = [tf.range(d, dtype=tf.int32) for d in tensor.shape]

  def __getitem__(self, slices):
    grid = tf.meshgrid(*(rng[sl] for rng, sl in zip(self.ranges, slices)), indexing='ij')
    return tf.stack(grid, axis=-1)

def no_pivot_ldl(matrix, name='no_pivot_ldl'):
  with tf.name_scope(name) as name:
    matrix = tf.convert_to_tensor(matrix)
    triangular_factor = tf.linalg.band_part(matrix, num_lower=-1, num_upper=0)
    slix = _Slice2Idx(triangular_factor)

    def fn(triangular_factor, i):
      column_tail = triangular_factor[..., i+1:, i]
      x = tf.einsum('...i,...j->...ij', column_tail, column_tail)
      idx = slix[i+1:, i+1:]
      triangular_factor = tf.tensor_scatter_nd_update(triangular_factor, idx, x)
      return triangular_factor

    triangular_factor = tf.foldl(
        fn=fn,
        elems=tf.range(tf.shape(triangular_factor)[-1]),
        initializer=triangular_factor)
    return triangular_factor

inp = tf.Variable([[2., 1.], [1., 2.]])
alt_chol_jit = tf.function(no_pivot_ldl, autograph=False, jit_compile=True)
print(no_pivot_ldl(inp))
alt_chol_jit(inp)
```
```


### Relevant log output

```shell
tf.Tensor(
[[2. 0.]
 [1. 1.]], shape=(2, 2), dtype=float32)
2023-08-23 11:31:54.059605: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5db9ec0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2023-08-23 11:31:54.059681: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2023-08-23 11:31:54.068228: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-08-23 11:31:54.093913: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:625 : UNIMPLEMENTED: CustomCall ""__onednn$matmul"" is not supported to have a dynamic dimension

tensorflow.python.framework.errors_impl.UnimplementedError: CustomCall ""__onednn$matmul"" is not supported to have a dynamic dimension [Op:__inference_no_pivot_ldl_264]
```
"
61673,Use old project folder on tensorflow 2.13 using wls,"Hi everyone, I was working on a project using tensorflow 2.10 with native gpu support. I wrongly updated the tf version to 2.13 that does not support gpu. So, I installed tensorflow 2.13 using wls2, now i don't know how to use the previous folder of my project on the conda environment created. Or intsead how to reinstall tf 2.10"
61671,Performance difference of passing np array vs tf tensor into tf.train.FloatList,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

docker tensorflow/tensorflow:latest-gpu and Ubuntu

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA Version: 12.0 NVIDIA_SMI 525.125.06

### GPU model and memory

RTX4090

### Current behavior?

Processing numpy array is much faster than processing the same tensor. This is likely connected to https://github.com/tensorflow/tensorflow/issues/30372 (they also serialize TF tensors). Happy to submit a workaround (conversion from tensor to np array). Both CPU and GPU are affected.

### Standalone code to reproduce the issue

```shell
""""""Easily reproducible in Google Colab:
https://colab.research.google.com/drive/1gzI7kYRICSepX7OpC715zYj5_0qwrbPx
""""""

import numpy as np
import tensorflow as tf
import timeit


np_val = np.random.random(size=(10_000))
tf_val = tf.constant(np_val)
  
print(timeit.timeit(lambda: tf.train.FloatList(value=np_val), number=10))
print(timeit.timeit(lambda: tf.train.FloatList(value=tf_val), number=10))
print(timeit.timeit(lambda: tf.train.FloatList(value=tf_val.numpy()), number=10))
```


### Relevant log output

```shell
0.02872585691511631
25.905678499955684
0.02939283801242709
```
"
61669,Please help ! ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

1.13.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.0

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/compiler version

10.5.0

### CUDA/cuDNN version

10

### GPU model and memory

RTX 3060 12G

### Current behavior?

I  am running the Octopus repo (https://github.com/thmoa/octopus) which use tensorflow-gpu version 1.13.1 . When I run that model with python, I got some errors from tensorflow. Please help me. 

### Standalone code to reproduce the issue

```shell
import os
import argparse
import tensorflow as tf
import keras.backend as K




from glob import glob

from lib.io import openpose_from_file, read_segmentation, write_mesh
from model.octopus import Octopus



def main(weights, name, segm_dir, pose_dir, out_dir, opt_pose_steps, opt_shape_steps):
    segm_files = sorted(glob(os.path.join(segm_dir, '*.png')))
    pose_files = sorted(glob(os.path.join(pose_dir, '*.json')))

    if len(segm_files) != len(pose_files) or len(segm_files) == len(pose_files) == 0:
        exit('Inconsistent input.')

    K.set_session(tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))))

    model = Octopus(num=len(segm_files))
    model.load(weights)

    segmentations = [read_segmentation(f) for f in segm_files]

    joints_2d, face_2d = [], []
    for f in pose_files:
        j, f = openpose_from_file(f)

        assert(len(j) == 25)
        assert(len(f) == 70)

        joints_2d.append(j)
        face_2d.append(f)

    if opt_pose_steps:
        print('Optimizing for pose...')
        model.opt_pose(segmentations, joints_2d, opt_steps=opt_pose_steps)

    if opt_shape_steps:
        print('Optimizing for shape...')
        model.opt_shape(segmentations, joints_2d, face_2d, opt_steps=opt_shape_steps)

    print('Estimating shape...')
    pred = model.predict(segmentations, joints_2d)

    write_mesh('{}/{}.obj'.format(out_dir, name), pred['vertices'][0], pred['faces'])

    print('Done.')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument(
        'name',
        type=str,
        help=""Sample name"")

    parser.add_argument(
        'segm_dir',
        type=str,
        help=""Segmentation images directory"")

    parser.add_argument(
        'pose_dir',
        type=str,
        help=""2D pose keypoints directory"")

    parser.add_argument(
        '--opt_steps_pose', '-p', default=5, type=int,
        help=""Optimization steps pose"")

    parser.add_argument(
        '--opt_steps_shape', '-s', default=15, type=int,
        help=""Optimization steps"")

    parser.add_argument(
        '--out_dir', '-od',
        default='out',
        help='Output directory')

    parser.add_argument(
        '--weights', '-w',
        default='weights/octopus_weights.hdf5',
        help='Model weights file (*.hdf5)')

    args = parser.parse_args()
    main(args.weights, args.name, args.segm_dir, args.pose_dir, args.out_dir, args.opt_steps_pose, args.opt_steps_shape)

```


### Relevant log output

```shell
Processing sample...
> Optimizing for pose...
  0%|                                                                                                                                                                                                                 | 0/10 [00:00<?, ?it/s]2023-08-22 16:24:18.296359: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2023-08-22 16:25:50.156420: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x55fa094fdcf0
2023-08-22 16:26:08.284736: E tensorflow/stream_executor/cuda/cuda_blas.cc:698] failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_EXECUTION_FAILED
2023-08-22 16:26:08.284773: E tensorflow/stream_executor/cuda/cuda_blas.cc:2620] Internal: failed BLAS call, see log for details
2023-08-22 16:26:08.326578: I tensorflow/stream_executor/stream.cc:5014] [stream=0x55fa0950bb90,impl=0x55fa093dbf20] did not memcpy device-to-host; source: 0x813bc6700
2023-08-22 16:26:08.326623: F tensorflow/core/framework/op_kernel.cc:1408] Check failed: nullptr == ctx->op_kernel().AsAsync() (nullptr vs. 0x55fa38108400)Use OP_REQUIRES_ASYNC in AsyncOpKernel implementations.
Aborted
```
"
61668,TF Quantizer breaks pip package,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/compiler version

16.0.6

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

The commit https://github.com/tensorflow/tensorflow/commit/fee881376914381062deb767759d144d0f07d760 added pywrap_quantize_model.so to the pip package but the build fails to set the RPATH for _pywrap_tensorflow_internal.so in the new .so leading to a failure in auditwheel when attempting to 'repair' to make it manylinux2014 compatible.

### Standalone code to reproduce the issue

```shell
$ python3 -m auditwheel repair --plat manylinux2014_aarch64 ./tensorflow-pkg/tensorflow_aarch64-2.15.0-cp311-cp311-linux_aarch64.whl --wheel-dir ./whl/
```


### Relevant log output

```shell
$ python3 -m auditwheel repair --plat manylinux2014_aarch64 ./tensorflow-pkg/tensorflow_aarch64-2.15.0-cp311-cp311-linux_aarch64.whl --wheel-dir ./whl/
INFO:auditwheel.main_repair:Repairing tensorflow_aarch64-2.15.0-cp311-cp311-linux_aarch64.whl
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""/usr/local/lib/python3.11/dist-packages/auditwheel/__main__.py"", line 6, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/auditwheel/main.py"", line 59, in main
    rval = args.func(args, p)
           ^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/auditwheel/main_repair.py"", line 173, in execute
    out_wheel = repair_wheel(
                ^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/auditwheel/repair.py"", line 80, in repair_wheel
    raise ValueError(
ValueError: Cannot repair wheel, because required library ""_pywrap_tensorflow_internal.so"" could not be located

Also

$ ldd venv_bad/lib/python3.11/site-packages/tensorflow/compiler/mlir/quantization/tensorflow/python/pywrap_quantize_model.so
	linux-vdso.so.1 (0x0000ffffb8af9000)
	libtensorflow_framework.so.2 => /workspace/venv_bad/lib/python3.11/site-packages/tensorflow/compiler/mlir/quantization/tensorflow/python/../../../../../libtensorflow_framework.so.2 (0x0000ffffb6800000)
	_pywrap_tensorflow_internal.so => not found
	libdl.so.2 => /lib/aarch64-linux-gnu/libdl.so.2 (0x0000ffffb8923000)
	libpthread.so.0 => /lib/aarch64-linux-gnu/libpthread.so.0 (0x0000ffffb88f2000)
	libm.so.6 => /lib/aarch64-linux-gnu/libm.so.6 (0x0000ffffb8847000)
	libstdc++.so.6 => /lib/aarch64-linux-gnu/libstdc++.so.6 (0x0000ffffb661b000)
	libgcc_s.so.1 => /lib/aarch64-linux-gnu/libgcc_s.so.1 (0x0000ffffb8823000)
	libc.so.6 => /lib/aarch64-linux-gnu/libc.so.6 (0x0000ffffb64a8000)
	/lib/ld-linux-aarch64.so.1 (0x0000ffffb8ac9000)
	librt.so.1 => /lib/aarch64-linux-gnu/librt.so.1 (0x0000ffffb8809000)

Compare with

$ ldd venv_bad/lib/python3.11/site-packages/tensorflow/python/saved_model/pywrap_saved_model.so
	linux-vdso.so.1 (0x0000ffffa40a2000)
	libtensorflow_framework.so.2 => /workspace/venv_bad/lib/python3.11/site-packages/tensorflow/python/saved_model/../../libtensorflow_framework.so.2 (0x0000ffffa1e00000)
	_pywrap_tensorflow_internal.so => /workspace/venv_bad/lib/python3.11/site-packages/tensorflow/python/saved_model/../_pywrap_tensorflow_internal.so (0x0000ffffa1c0a000)
	libdl.so.2 => /lib/aarch64-linux-gnu/libdl.so.2 (0x0000ffffa3ee5000)
	libm.so.6 => /lib/aarch64-linux-gnu/libm.so.6 (0x0000ffffa3e3a000)
	libpthread.so.0 => /lib/aarch64-linux-gnu/libpthread.so.0 (0x0000ffffa3e09000)
	libstdc++.so.6 => /lib/aarch64-linux-gnu/libstdc++.so.6 (0x0000ffffa1a25000)
	libgcc_s.so.1 => /lib/aarch64-linux-gnu/libgcc_s.so.1 (0x0000ffffa3de5000)
	libc.so.6 => /lib/aarch64-linux-gnu/libc.so.6 (0x0000ffffa18b2000)
	/lib/ld-linux-aarch64.so.1 (0x0000ffffa4072000)
	librt.so.1 => /lib/aarch64-linux-gnu/librt.so.1 (0x0000ffffa3dcd000)
	libtensorflow_cc.so.2 => /workspace/venv_bad/lib/python3.11/site-packages/tensorflow/python/saved_model/../../libtensorflow_cc.so.2 (0x0000ffff8be00000)
	libml_dtypes.so.so => /workspace/venv_bad/lib/python3.11/site-packages/tensorflow/python/saved_model/../../tsl/python/lib/core/libml_dtypes.so.so (0x0000ffffa3d99000)
	libomp.so.5 => /usr/lib/llvm-16/lib/libomp.so.5 (0x0000ffff8bcc0000)
```
"
61667,Android NDK Prefabs for Tensorflow Lite,"Hey all! I'm trying to use Tensorflow Lite in an Android NDK/C++ environment.

My `build.gradle` looks something like this:

```groovy
...
android {
  ...
  buildFeatures {
    prefab true
  }
}

dependencies {
  ...
  implementation ""org.tensorflow:tensorflow-lite:2.13.0""
}
```

And in my `CMakeLists.txt` I try to find `tensorflow-lite` as a prefab:

```cmake
...
find_package(tensorflow-lite REQUIRED CONFIG)
...

target_link_libraries(
  ${PACKAGE_NAME}
  ...
  tensorflow-lite::tensorflow-lite
)
```

However, CMake is not able to find the package:

```
CMake Error at CMakeLists.txt:10 (find_package):
  Could not find a package configuration file provided by ""tensorflow-lite""
  with any of the following names:

    tensorflow-liteConfig.cmake
    tensorflow-lite-config.cmake

  Add the installation prefix of ""tensorflow-lite"" to CMAKE_PREFIX_PATH or
  set ""tensorflow-lite_DIR"" to a directory containing one of the above files.
  If ""tensorflow-lite"" provides a separate development package or SDK, be
  sure it has been installed.
```

Does TensorFlow Lite not provide a prefab publish? This would make it a lot easier to integrate in such environments. Now I have to manually extract the .aar/zip and incldue the headers."
61666,python3.11.3 mismatch with tensorflow 2.13.0,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

macOS Venture 13.4

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

![image](https://github.com/tensorflow/tensorflow/assets/11846497/3459b246-e377-499d-9d9f-e5f666fb7957)


### Standalone code to reproduce the issue

```shell
import tensorflow
```


### Relevant log output

_No response_"
61665,`GLIBCXX_3.4.30' not found,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

r2.13

### Custom code

No

### OS platform and distribution

amazon linux 2023

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3.0

### GCC/compiler version

gcc 12.2

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

ERROR: /root/tensorflow/tensorflow/core/transforms/remapper/BUILD:14:18: TdGenerate tensorflow/core/transforms/remapper/pdll/MklPDLLPatterns.h.inc [for host] failed: (Exit 1): mlir-pdll failed: error executing command bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll '-x=cpp' tensorflow/core/transforms/remapper/pdll/mkl_patterns.pdll -I ./tensorflow/core/transforms/include -I ... (remaining 15 arguments skipped)
bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll)


### Standalone code to reproduce the issue

```shell
ERROR: /root/tensorflow/tensorflow/core/transforms/remapper/BUILD:14:18: TdGenerate tensorflow/core/transforms/remapper/pdll/MklPDLLPatterns.h.inc [for host] failed: (Exit 1): mlir-pdll failed: error executing command bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll '-x=cpp' tensorflow/core/transforms/remapper/pdll/mkl_patterns.pdll -I ./tensorflow/core/transforms/include -I ... (remaining 15 arguments skipped)
bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll)
```


### Relevant log output

```shell
ERROR: /root/tensorflow/tensorflow/core/transforms/remapper/BUILD:14:18: TdGenerate tensorflow/core/transforms/remapper/pdll/MklPDLLPatterns.h.inc [for host] failed: (Exit 1): mlir-pdll failed: error executing command bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll '-x=cpp' tensorflow/core/transforms/remapper/pdll/mkl_patterns.pdll -I ./tensorflow/core/transforms/include -I ... (remaining 15 arguments skipped)
bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll)
```
"
61664,"  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/interpreter.py"", line 915, in invoke     self._interpreter.Invoke() RuntimeError: tensorflow/lite/kernels/concatenation.cc:158 t->dims->data[d] != t0->dims->data[d] (1 != 2)Node number 304 (CONCATENATION) failed to prepare.","### 1. System information
![image](https://github.com/tensorflow/tensorflow/assets/35233355/cfc8e9d3-9790-4a89-86db-062b97d3bbbc)

- OS Platform and Distribution (Linux Ubuntu 20.04):
- TensorFlow installation (2.9.1+nv22.9):

### 2. Code
`import numpy as np
import tensorflow as tf

def main():
    # Load the TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path=""model.tflite"")
    interpreter.allocate_tensors()
    
    #return

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    # Test the model on random input data.

    input_shape = input_details[0]['shape']
    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
    #interpreter.set_tensor(input_details[0]['index'], input_data)

    signatures = interpreter.get_signature_list()
    print(signatures)

    interpreter.invoke()
    return
    # The function `get_tensor()` returns a copy of the tensor data.
    # Use `tensor()` in order to get a pointer to the tensor.
    output_data = interpreter.get_tensor(output_details[0]['index'])
    print(output_data)


if __name__ == '__main__':
    main()`

**How to get the model:**
I used a library here to get a unet with efficientnetb0 as encoder
https://github.com/qubvel/segmentation_models.
So you can try to use my script to get the model:
`    BACKBONE = 'efficientnetb0'
    BATCH_SIZE = 1
    CLASSES = [""background"", ""target"", ""others""]
    LR = 0.0001
    EPOCHS = 10

    preprocess_input = sm.get_preprocessing(BACKBONE)

    # define network parameters
    n_classes = 1 if len(CLASSES) == 1 else (len(CLASSES) + 1)  # case for binary and multiclass segmentation
    activation = 'sigmoid' if n_classes == 1 else 'softmax'

    #create model
    model = sm.Unet(BACKBONE, classes=n_classes, activation=activation)`

"
61662,Ops listed in 'experimental_select_user_tf_ops' not being recognized by tf lite converter,"### 1. System information

- Google Colab Notebook

### 2. Code

_#### Model Definition ---------_
```
class CppTfTest(tf.Module):

    def __init__(self, name=None):
        super().__init__(name=name)

    @tf.function
    def call(self):

        frames = tf.range(600)

        bpm = tf.random.uniform(
            tf.TensorShape([600,]),
            minval=0,
            maxval=90,
            dtype=tf.dtypes.float64
            )

        return bpm, frames
```

_#### Model Saving ------------_
```
cpp_tf_test = CppTfTest()
tf.saved_model.save(
    cpp_tf_test,
    'cpp_tf_test',
    signatures=cpp_tf_test.call.get_concrete_function()
    )
```

_#### Model Conversion -----------_
```
converter = tf.lite.TFLiteConverter.from_saved_model('cpp_tf_test')

converter.target_spec = tf.lite.TargetSpec(
    supported_ops=[tf.lite.OpsSet.TFLITE_BUILTINS],
    experimental_select_user_tf_ops=[
        'RandomUniform', 'Mul'
        ]
)

tflite_model = converter.convert()

with open('cpp_tf_test.tflite', 'wb') as f:
  f.write(tflite_model)
```


---------------------------------------------------------------------------

> 
> ConverterError                            Traceback (most recent call last)
> 
> [<ipython-input-88-4a7f170f3f72>](https://localhost:8080/#) in <cell line: 47>()
>      45 #converter.allow_custom_ops=True
>      46 
> ---> 47 tflite_model = converter.convert()
>      48 
>      49 with open('cpp_tf_test.tflite', 'wb') as f:
> 
> 7 frames
> 
> [/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in wrapper(self, *args, **kwargs)
>     960   def wrapper(self, *args, **kwargs):
>     961     # pylint: disable=protected-access
> --> 962     return self._convert_and_export_metrics(convert_func, *args, **kwargs)
>     963     # pylint: enable=protected-access
>     964 
> 
> [/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _convert_and_export_metrics(self, convert_func, *args, **kwargs)
>     938     self._save_conversion_params_metric()
>     939     start_time = time.process_time()
> --> 940     result = convert_func(self, *args, **kwargs)
>     941     elapsed_time_ms = (time.process_time() - start_time) * 1000
>     942     if result:
> 
> [/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in convert(self)
>    1245           graph_def)
>    1246 
> -> 1247     return self._convert_from_saved_model(graph_def)
>    1248 
>    1249 
> 
> [/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _convert_from_saved_model(self, graph_def)
>    1128     converter_kwargs.update(quant_mode.converter_flags())
>    1129 
> -> 1130     result = _convert_saved_model(**converter_kwargs)
>    1131     return self._optimize_tflite_model(
>    1132         result, quant_mode, quant_io=self.experimental_new_quantizer)
> 
> [/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
>     210         else:
>     211           report_error_message(str(converter_error))
> --> 212         raise converter_error from None  # Re-throws the exception.
>     213       except Exception as error:
>     214         report_error_message(str(error))
> 
> [/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
>     203     def wrapper(*args, **kwargs):
>     204       try:
> --> 205         return func(*args, **kwargs)
>     206       except ConverterError as converter_error:
>     207         if converter_error.errors:
> 
> [/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py](https://localhost:8080/#) in convert_saved_model(**kwargs)
>     830   model_flags = build_model_flags(**kwargs)
>     831   conversion_flags = build_conversion_flags(**kwargs)
> --> 832   data = convert(
>     833       model_flags.SerializeToString(),
>     834       conversion_flags.SerializeToString(),
> 
> [/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py](https://localhost:8080/#) in convert(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
>     320       for error_data in _metrics_wrapper.retrieve_collected_errors():
>     321         converter_error.append_error(error_data)
> --> 322       raise converter_error
>     323 
>     324   return _run_deprecated_conversion_binary(model_flags_str,
> 
> ConverterError: <unknown>:0: error: loc(callsite(callsite(fused[""RandomUniform:"", ""random_uniform/RandomUniform@__inference_call_11165""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_11173""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): 'tf.RandomUniform' op is neither a custom op nor a flex op
> <unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
> <unknown>:0: note: loc(callsite(callsite(fused[""RandomUniform:"", ""random_uniform/RandomUniform@__inference_call_11165""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_11173""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): Error code: ERROR_NEEDS_FLEX_OPS
> <unknown>:0: error: loc(callsite(callsite(fused[""Mul:"", ""random_uniform/Mul@__inference_call_11165""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_11173""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): 'tf.Mul' op is neither a custom op nor a flex op
> <unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
> <unknown>:0: note: loc(callsite(callsite(fused[""Mul:"", ""random_uniform/Mul@__inference_call_11165""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_11173""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): Error code: ERROR_NEEDS_FLEX_OPS
> <unknown>:0: error: failed while converting: 'main': 
> Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
> TF Select ops: Mul, RandomUniform
> Details:
> 	tf.Mul(tensor<600xf64>, tensor<f64>) -> (tensor<600xf64>) : {device = """"}
> 	tf.RandomUniform(tensor<1xi32>) -> (tensor<600xf64>) : {device = """", seed = 0 : i64, seed2 = 0 : i64}


I'm trying to convert this simple model to Tensorflow Lite using the _experimental_select_user_tf_ops_ flag to tell the converter what operations from the tf_ops set to include. I need this to run with just a subset of the tf_ops set since I have a bigger model that I need to optimize for a mobile app. I've try many things but the _experimental_select_user_tf_ops_ flag just doesn't seem to work."
61661,ERROR: C:/users/ayush/_bazel_ayush/xv6zejqw/external/llvm_openmp/BUILD.bazel:233:34: Compiling external/llvm_openmp/z_Windows_NT-586_asm.S failed: not all outputs were created or valid,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

5.3.0

### GCC/compiler version

Visual Studio Build Tools 2019 with C++ tools

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

It was fetching, extracting and compiling everything was going well and afterwards suddenly it outputs failed to build 

### Standalone code to reproduce the issue

```shell
C:\tensorflow>echo %BAZEL_VS%
C:/Program Files(x86)/Microsoft Visual Studio/2019/BuildTools

C:\tensorflow>set BAZEL_VC=C:/Program Files(x86)/Microsoft Visual Studio/2019/BuildTools/VC

C:\tensorflow>set BAZEL_SH=C:/msys64/usr/bin/bash.exe

C:\tensorflow>set BAZEL_WINSDK_FULL_VERSION=10.0.19041.0

C:\tensorflow>git checkout r2.13
Updating files: 100% (6982/6982), done.
Switched to a new branch 'r2.13'
branch 'r2.13' set up to track 'origin/r2.13'.

C:\tensorflow>set PYTHON_BIN_PATH=C:\Users\Ayush\AppData\Local\Programs\Python\Python311\python.exe

C:\tensorflow>set PYTHON_LIB_PATH=C:\Users\Ayush\AppData\Local\Programs\Python\Python311\Lib\site-packages

C:\tensorflow>set PYTHON_DIRECTORY=C:\Users\Ayush\AppData\Local\Programs\Python\Python311

C:\tensorflow>python configure.py
You have bazel 5.3.0 installed.
Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.


WARNING: Cannot build with CUDA support on Windows.
Starting in TF 2.11, CUDA build is not supported for Windows. For using TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.

C:\tensorflow>bazel build --config=opt --config=mkl --copt=-msse --copt=-msse2 --copt=-msse3 --copt=-msse4.1 --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (614 packages loaded, 36592 targets configured).
INFO: Found 1 target...
ERROR: C:/users/ayush/_bazel_ayush/xv6zejqw/external/llvm_openmp/BUILD.bazel:233:34: output 'external/llvm_openmp/_objs/libiomp5md.dll/z_Windows_NT-586_asm.obj' was not created
ERROR: C:/users/ayush/_bazel_ayush/xv6zejqw/external/llvm_openmp/BUILD.bazel:233:34: Compiling external/llvm_openmp/z_Windows_NT-586_asm.S failed: not all outputs were created or valid
MASM : warning A4018:invalid command-line option : /bigobj
MASM : warning A4018:invalid command-line option : /Zm500
MASM : warning A4018:invalid command-line option : /Z500
MASM : warning A4018:invalid command-line option : /Z00
MASM : warning A4018:invalid command-line option : /Z0
MASM : warning A4018:invalid command-line option : /EHsc
MASM : warning A4018:invalid command-line option : /wd4351
MASM : warning A4018:invalid command-line option : /wd4291
MASM : warning A4018:invalid command-line option : /wd4250
MASM : warning A4018:invalid command-line option : /wd4996
MASM : warning A4018:invalid command-line option : /showIncludes
 Assembling: bazel-out/x64_windows-opt/bin/external/llvm_openmp/z_Windows_NT-586_asm.S
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 6810.134s, Critical Path: 970.02s
INFO: 1408 processes: 307 internal, 1101 local.
FAILED: Build did NOT complete successfully
```
"
61659,"Error in PredictCost() for the op: op: ""CropAndResize"" attr","### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We have a model that used to run on python 3.7 + tensorflow 2.4.1. When upgrading to tensorflow 2.13.0, we're seeing this error message, which seems to correlate with the degradation of our model performance. Any idea what might be leading to this error and what could be changed? 

It does seem like when I remove `tf.function` decorator, this error message goes away. So, I'm assuming it has something to do with tracing, but not entirely sure why it's erroring out in tf 2.13 and not in 2.4.1...

```
Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_FLOAT } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_FLOAT shape { dim { size: -569 } dim { size: 128 } dim { size: 224 } dim { size: 2 } } } inputs { dtype: DT_FLOAT shape { dim { size: -16 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -16 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } tensor_content: ""\030\000\000\000\020\000\000\000"" } } device { type: ""GPU"" vendor: ""NVIDIA"" model: ""Tesla V100-SXM2-16GB"" frequency: 1530 num_cores: 80 environment { key: ""architecture"" value: ""7.0"" } environment { key: ""cuda"" value: ""11080"" } environment { key: ""cudnn"" value: ""8600"" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 6291456 shared_memory_size_per_multiprocessor: 98304 memory_size: 11614814208 bandwidth: 898048000 } outputs { dtype: DT_FLOAT shape { dim { size: -16 } dim { size: 24 } dim { size: 16 } dim { size: 2 } } }
```

### Standalone code to reproduce the issue

```shell
I don't have a standalone code to reproduce the issue, but would just like some feedback on what might be causing the above issue...
```


### Relevant log output

_No response_"
61658,Unable to serialize VariableSpec,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

windows 11

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A graph

### Standalone code to reproduce the issue

```shell
from models import *
from conftest import DDPGAgent
import matplotlib as plt
import pytest
import time

# Just disables the warning, doesn't take advantage of AVX/FMA to run faster
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# setting for hidden layers
Layer1 = 400
Layer2 = 300


class MecTer(object):
    """"""
    MEC terminal parent class
    """"""

    def __init__(self, user_config, train_config):
        self.rate = user_config['rate']
        self.dis = user_config['dis']
        self.id = user_config['id']
        self.state_dim = user_config['state_dim']
        self.action_dim = user_config['action_dim']
        self.action_bound = user_config['action_bound']
        self.data_buf_size = user_config['data_buf_size']
        self.t_factor = user_config['t_factor']
        self.penalty = user_config['penalty']

        self.sigma2 = train_config['sigma2']
        self.init_path = ''
        self.isUpdateActor = True
        self.init_seqCnt = 0

        if 'model' not in user_config:
            self.channelModel = MarkovModel(self.dis, seed=train_config['random_seed'])
        else:
            n_t = 1
            n_r = user_config['num_r']
            self.channelModel = ARModel(self.dis, n_t, n_r, seed=train_config['random_seed'])

        self.DataBuf = 0
        self.Channel = self.channelModel.getCh()
        self.SNR = 0
        self.Power = np.zeros(self.action_dim)
        self.Reward = 0
        self.State = []

        # some pre-defined parameters
        self.k = 1e-27
        self.t = 0.001
        self.L = 500

    def localProc(self, p):
        return np.power(p / self.k, 1.0 / 3.0) * self.t / self.L / 1000

    def localProcRev(self, b):
        return np.power(b * 1000 * self.L / self.t, 3.0) * self.k

    def offloadRev(self, b):
        return (np.power(2.0, b) - 1) * self.sigma2 / np.power(np.linalg.norm(self.Channel), 2)

    def offloadRev2(self, b):
        return self.action_bound if self.SNR <= 1e-12 else (np.power(2.0, b) - 1) / self.SNR

    def getCh(self):
        return self.Channel

    def setSNR(self, snr):
        self.SNR = snr
        self.sampleCh()
        channel_gain = np.power(np.linalg.norm(self.Channel), 2) / self.sigma2
        self.State = np.array([self.DataBuf, snr, channel_gain])

    def sampleData(self):
        data_t = np.log2(1 + self.Power[0] * self.SNR)
        data_p = self.localProc(self.Power[1])
        over_power = 0

        self.DataBuf -= data_t + data_p
        if self.DataBuf < 0:
            over_power = self.Power[1] - self.localProcRev(np.fmax(0, self.DataBuf + data_p))
            self.DataBuf = 0

        data_r = np.random.poisson(self.rate)
        self.DataBuf += data_r
        return data_t, data_p, data_r, over_power

    def sampleCh(self):
        # self.Channel = self.channelModel.sampleCh()

        # Calculate channel gain using channel quantization
        raw_channel_gain = np.linalg.norm(self.channelModel.sampleCh())
        min_val = np.min(self.Channel)
        max_val = np.max(self.Channel)

        # Quantize the channel gain into 10 levels
        quantized_channel_gain = min_val + (max_val - min_val) * (raw_channel_gain - min_val) / (max_val - min_val)
        quantized_channel_gain = np.clip(quantized_channel_gain, min_val, max_val)
        self.Channel = quantized_channel_gain

        return self.Channel

    def reset(self, rate, seqCount):
        self.rate = rate
        self.DataBuf = np.random.randint(0, self.data_buf_size - 1) / 2.0
        self.sampleCh()

        if seqCount >= self.init_seqCnt:
            self.isUpdateActor = True

        return self.DataBuf


class MecTermRL(MecTer):
    """"""
    MEC terminal class using RL
    """"""

    # rate:packet poisson arrival, dis: distance in meters
    def __init__(self, user_config, train_config):
        MecTer.__init__(self, user_config, train_config)
        self.agent = DDPGAgent(user_config, train_config)

        if 'init_path' in user_config and len(user_config['init_path']) > 0:
            self.init_path = user_config['init_path']
            self.init_seqCnt = user_config['init_seqCnt']
            self.isUpdateActor = False

    def feedback(self, snr, done):
        isOverflow = 0
        self.SNR = snr

        # update the data buffer
        [data_t, data_p, data_r, over_power] = self.sampleData()

        # get the reward for the current slot
        self.Reward = -self.t_factor * np.sum(self.Power) * 10 - (1 - self.t_factor) * self.DataBuf

        # estimate the channel for next slot
        self.sampleCh()

        # update the actor and critic network
        channel_gain = np.power(np.linalg.norm(self.Channel), 2) / self.sigma2
        next_state = np.array([self.DataBuf, snr, channel_gain])

        self.agent.update(self.State, self.Power, self.Reward, done, next_state, self.isUpdateActor)

        # update system state
        self.State = next_state
        # return the reward in this slot
        sum_power = np.sum(self.Power) - over_power
        return self.Reward, sum_power, over_power, data_t, data_p, data_r, self.DataBuf, channel_gain, isOverflow

    def predict(self, isRandom):
        power, noise = self.agent.predict(self.State, self.isUpdateActor)
        self.Power = np.fmax(0, np.fmin(self.action_bound, power))

        return self.Power, noise


class MecSvrEnv(object):
    """"""
    Simulation environment
    """"""

    def __init__(self, user_list, num_att, sigma2, max_len):
        self.user_list = user_list
        self.num_user = len(user_list)
        self.num_att = num_att
        self.sigma2 = sigma2
        self.count = 0
        self.seqCount = 0
        self.max_len = max_len

        # specially designed for Greedy agent training

    #         self.data_set = []

    def init_target_network(self):
        for user in self.user_list:
            user.critic.init_target_network(path='data_set_OGD.npz')

    def plot_channel_gains_histogram(self):
        # Get the channel gains for all users
        channel_gains = [np.abs(user.getCh()) for user in self.user_list]

        # Flatten the channel gains to a 1D array
        flat_channel_gains = np.concatenate(channel_gains)

        # plot a histogram for the channel gains
        plt.hist(np.abs(flat_channel_gains), bins=20, edgecolor='black')
        plt.title(""Channel Gains Histogram"")
        plt.xlabel(""Channel Gain Magnitude"")
        plt.ylabel(""Frequency"")
        plt.show()

    def step_transmit(self, isRandom=True):
        # get the channel vectors
        channels = np.transpose([user.getCh() for user in self.user_list])
        # get the transmit powers
        powers = []
        noises = []

        for i in range(self.num_user):
            p, n = self.user_list[i].predict(isRandom)
            powers.append(p.copy())
            noises.append(n.copy())
        # compute the snr for each user

        powers = np.array(powers)
        noises = np.array(noises)
        snr_list = self.compute_snr(channels, powers[:, 0])

        rewards = np.zeros(self.num_user)
        powers = np.zeros(self.num_user)
        over_powers = np.zeros(self.num_user)
        data_ts = np.zeros(self.num_user)
        data_ps = np.zeros(self.num_user)
        data_rs = np.zeros(self.num_user)
        data_buf_sizes = np.zeros(self.num_user)
        next_channels = np.zeros(self.num_user)
        isOverflows = np.zeros(self.num_user)

        self.count += 1
        # feedback the snr to each user
        for i in range(self.num_user):
            [rewards[i], powers[i], over_powers[i], data_ts[i], data_ps[i], data_rs[i], data_buf_sizes[i],
             next_channels[i], isOverflows[i]] = self.user_list[i].feedback(snr_list[i], self.count >= self.max_len)

        return rewards, self.count >= self.max_len, powers, over_powers, noises, data_ts, data_ps, data_rs, data_buf_sizes, next_channels, isOverflows

    def compute_snr(self, channels, powers):
        # FDD - Computing SNR
        H_inv = np.linalg.pinv(channels)
        total_signal_power = np.power(np.linalg.norm(channels, axis=1), 2)
        noise = np.power(np.linalg.norm(H_inv, axis=1), 2) * self.sigma2
        snr_list = total_signal_power / noise

        return snr_list

    def reset(self, isTrain=True):
        self.count = 0

        if isTrain:
            init_data_buf_size = [user.reset(user.rate, self.seqCount) for user in self.user_list]
            # get the channel vectors
            channels = np.transpose([user.getCh() for user in self.user_list])
            # get the transmit powers to start
            powers = [np.random.uniform(0, user.action_bound) for user in self.user_list]
            # compute the snr for each user
            snr_list = self.compute_snr(channels, powers)
        else:
            init_data_buf_size = [0 for user in self.user_list]
            snr_list = [0 for user in self.user_list]

        for i in range(self.num_user):
            self.user_list[i].setSNR(snr_list[i])

        self.seqCount += 1
        return init_data_buf_size


# Create the environment
# def env():
#     envi = MecSvrEnv(user_list, NUM_R, SIGMA2, MAX_EPISODE_LEN)
#     return envi

# env = MecSvrEnv(user_list, NUM_R, SIGMA2, MAX_EPISODE_LEN)
# env.init_target_network()

train_config = {
    'sigma2': 0.01,
    'minibatch_size': 64,
    'actor_lr': 0.0001,
    'tau': 0.001,
    'critic_lr': 0.001,
    'gamma': 0.99,
    'buffer_size': 250000,
    'random_seed': int(time.perf_counter() * 1000 % 1000),
    'noise_sigma': 0.12
}

# Define user_list_info with user information
user_list_info = [
    {'state_dim': 3,
     'action_dim': 1,
     'id': '1',
     'action_bound': 1,
     'model': 'AR',
     'num_r': 4,
     'rate': 3.0,
     'dis': 100,
     'data_buf_size': 100,
     't_factor': 1.0,
     'penalty': 1000, }
]

# sess = tf.compat.v1.Session()
# Create instances of the User class from the dictionary in user_list
user_list = [
    MecTermRL(user_config=user_info, train_config=train_config)
    for user_info in user_list_info
]

# Initialize variables
for user in user_list:
    user.agent.init_target_network()
    # (
    #     path=""C:/Users/USER/PycharmProjects/mec_drl-masterr/mec_drl-master/mec_drl-master/data_set_OGD.npz""
    # )

@pytest.fixture
def env():
    # Create and return the environment object
    # Make sure to adjust this to properly create your environment instance
    return MecSvrEnv(user_list, NUM_R, SIGMA2, MAX_EPISODE_LEN)
```


### Relevant log output

```shell
WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'dense_5/bias:0' shape=(300,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.
Traceback (most recent call last):
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\saving\legacy\saved_model\json_utils.py"", line 207, in get_json_type
    type_spec_name = type_spec_registry.get_name(type(obj))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\tensorflow\python\framework\type_spec_registry.py"", line 75, in get_name
    raise ValueError(""TypeSpec %s.%s has not been registered."" %
ValueError: TypeSpec tensorflow.python.ops.resource_variable_ops.VariableSpec has not been registered.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\USER\PycharmProjects\mec_drl-masterr\mec_drl-master\mec_drl-master\test.py"", line 303, in <module>
    user_list = [
                ^
  File ""C:\Users\USER\PycharmProjects\mec_drl-masterr\mec_drl-master\mec_drl-master\test.py"", line 304, in <listcomp>
    MecTermRL(user_config=user_info, train_config=train_config)
  File ""C:\Users\USER\PycharmProjects\mec_drl-masterr\mec_drl-master\mec_drl-master\test.py"", line 125, in __init__
    self.agent = DDPGAgent(user_config, train_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\PycharmProjects\mec_drl-masterr\mec_drl-master\mec_drl-master\conftest.py"", line 24, in __init__
    self.critic = CriticNetwork(self.state_dim, self.action_dim, float(train_config['critic_lr']),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\PycharmProjects\mec_drl-masterr\mec_drl-master\mec_drl-master\ddpg.py"", line 102, in __init__
    self.target_model = tf.keras.models.clone_model(self.model)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\models\cloning.py"", line 539, in clone_model
    return _clone_functional_model(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\models\cloning.py"", line 222, in _clone_functional_model
    model_configs, created_layers = _clone_layers_and_model_config(
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\models\cloning.py"", line 298, in _clone_layers_and_model_config
    config = functional.get_network_config(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\engine\functional.py"", line 1583, in get_network_config
    node_data = node.serialize(
                ^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\engine\node.py"", line 219, in serialize
    kwargs = tf.nest.map_structure(_serialize_keras_tensor, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\tensorflow\python\util\nest.py"", line 624, in map_structure
    return nest_util.map_structure(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\tensorflow\python\util\nest_util.py"", line 1054, in map_structure
    return _tf_core_map_structure(func, *structure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\tensorflow\python\util\nest_util.py"", line 1094, in _tf_core_map_structure
    [func(*x) for x in entries],
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\tensorflow\python\util\nest_util.py"", line 1094, in <listcomp>
    [func(*x) for x in entries],
     ^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\engine\node.py"", line 215, in _serialize_keras_tensor
    return (_COMPOSITE_TYPE, json_utils.Encoder().encode(t))
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\saving\legacy\saved_model\json_utils.py"", line 55, in encode
    return super().encode(_encode_tuple(obj))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\json\encoder.py"", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\json\encoder.py"", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\saving\legacy\saved_model\json_utils.py"", line 52, in default
    return get_json_type(obj)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\saving\legacy\saved_model\json_utils.py"", line 225, in get_json_type
    ""spec"": get_json_type(spec),
            ^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\saving\legacy\saved_model\json_utils.py"", line 214, in get_json_type
    raise ValueError(
ValueError: Unable to serialize VariableSpec(shape=(300,), dtype=tf.float32, trainable=True, alias_id=None) to JSON, because the TypeSpec class <class 'tensorflow.python.ops.resource_variable_ops.VariableSpec'> has not been registered.
```
"
61656," W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: ""Conv2D"" attr { key: ""T"" value { type: DT_FLOAT } } attr { key: ""data_format"" value { s: ""NCHW"" } } attr { key: ""dilations"" value ","### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf.2.7.0

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.6

### GPU model and memory

GPU 8 / NVIDIA SMI 512.98

### Current behavior?

I am getting the below error message while loading the model on tf 2.7.0. But there is no issue with the prediction. 
I have converted Keras model into tf2 and loaded them in production. 



### Standalone code to reproduce the issue

```shell
: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: ""Conv2D"" attr { key: ""T"" value { type: DT_FLOAT } } attr { key: ""data_format"" value { s: ""NCHW"" } } attr { key: ""dilations"" value { list { i: 1 i: 1 i: 1 i: 1 } } } attr { key: ""explicit_paddings"" value { list { } } } attr { key: ""padding"" value { s: ""SAME"" } } attr { key: ""strides"" value { list { i: 1 i: 1 i: 1 i: 1 } } } attr { key: ""use_cudnn_on_gpu"" value { b: true } } inputs { dtype: DT_FLOAT shape { dim { } dim { size: 62 } dim { size: 4 } dim { size: 4 } } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 1 } dim { size: 62 } dim { size: 31 } } } device { type: ""GPU"" vendor: ""NVIDIA"" model: ""NVIDIA GeForce GTX 1070"" frequency: 1695 num_cores: 16 environment { key: ""architecture"" value: ""6.1"" } environment { key: ""cuda"" value: ""11020"" } environment { key: ""cudnn"" value: ""8100"" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 2097152 shared_memory_size_per_multiprocessor: 98304 memory_size: 6952124416 bandwidth: 256256000 } outputs { dtype: DT_FLOAT shape { dim { } dim { size: 31 } dim { size: 4 } dim { size: 4 } } }
```


### Relevant log output

_No response_"
61655,Compiling src/amalgam/gen/neonfp16arith.c failed: (Exit 70): clang failed: error executing command (from target @XNNPACK//:neonfp16arith_amalgam_microkernels) external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang '-D__ANDROID_API__=26' -isystemexternal/androidndk/ndk/sysroot/usr/include/arm-linux-androideabi -target ... (remaining 71 arguments skipped),"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Ubuntu18.04

### Mobile device

pixel 4

### Python version

3.6

### Bazel version

6.1.0

### GCC/compiler version

null

### CUDA/cuDNN version

null

### GPU model and memory

null

### Current behavior?

I run the following command â€œbazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain   //tensorflow/lite/java:tensorflow-lite-gpuâ€ and want to build ""tensorflow-lite-gpu.aar"",But the following error occurred.

### Standalone code to reproduce the issue

```shell
I run the following command â€œbazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain   //tensorflow/lite/java:tensorflow-lite-gpuâ€ and want to build ""tensorflow-lite-gpu.aar"",But the following error occurred.
```


### Relevant log output

```shell
ERROR: /home/ferey/.cache/bazel/_bazel_ferey/b105c31c70ad75e6928e90fd6d84ab22/external/XNNPACK/BUILD.bazel:3480:19: Compiling src/amalgam/gen/neonfp16arith.c failed: (Exit 70): clang failed: error executing command (from target @XNNPACK//:neonfp16arith_amalgam_microkernels) external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang '-D__ANDROID_API__=26' -isystemexternal/androidndk/ndk/sysroot/usr/include/arm-linux-androideabi -target ... (remaining 71 arguments skipped)
warning: unknown warning option '-Werror=unused-but-set-variable'; did you mean '-Werror=unused-const-variable'? [-Wunknown-warning-option]
fatal error: error in backend: Cannot select: 0x5e9cd18: v8f16 = bitcast 0x5e999a8, external/XNNPACK/src/amalgam/gen/neonfp16arith.c:51:28
  0x5e999a8: v8i16,ch = ARMISD::VLD1DUP<(load 2 from %ir.17)> 0x5e16428, 0x5e9cb78, Constant:i32<2>, external/XNNPACK/src/amalgam/gen/neonfp16arith.c:51:50
    0x5e9cb78: i32 = add nuw 0x5e9a230, Constant:i32<2>, external/XNNPACK/src/amalgam/gen/neonfp16arith.c:51:50
      0x5e9a230: i32,ch = load<(load 4 from %fixed-stack.0, align 8)> 0x5e16428, FrameIndex:i32<-7>, undef:i32
        0x5e9a1c8: i32 = FrameIndex<-7>
        0x5e99ce8: i32 = undef
      0x5e9cb10: i32 = Constant<2>
    0x5e9cb10: i32 = Constant<2>
In function: xnn_f16_avgpool_minmax_ukernel_9p8x__neonfp16arith_c8
clang: error: clang frontend command failed with exit code 70 (use -v to see invocation)
Android (5058415 based on r339409) clang version 8.0.2 (https://android.googlesource.com/toolchain/clang 40173bab62ec746213857d083c0e8b0abb568790) (https://android.googlesource.com/toolchain/llvm 7a6618d69e7e8111e1d49dc9e7813767c5ca756a) (based on LLVM 8.0.2svn)
Target: armv7-none-linux-android
Thread model: posix
InstalledDir: external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin
clang: note: diagnostic msg: PLEASE submit a bug report to https://bugs.llvm.org/ and include the crash backtrace, preprocessed source, and associated run script.
clang: note: diagnostic msg: 
********************

PLEASE ATTACH THE FOLLOWING FILES TO THE BUG REPORT:
Preprocessed source(s) and associated run script(s) are located at:
clang: note: diagnostic msg: /tmp/neonfp16arith-376495.c
clang: note: diagnostic msg: /tmp/neonfp16arith-376495.sh
clang: note: diagnostic msg: 

********************
Target //tensorflow/lite/java:tensorflow-lite-gpu failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 224.110s, Critical Path: 19.33s
INFO: 335 processes: 3 internal, 332 local.
FAILED: Build did NOT complete successfully
```
"
61654,SIGBUS from libc on Android during inference,"### 1. System information

Server that generated the TFLite file:

- OS Platform and Distribution: `Linux debian 6.1.0-9-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.27-1 (2023-05-08) x86_64 GNU/Linux`.
- TensorFlow installation: Pip.
- TensorFlow library: tensorflow 2.13.0.

Android client that interprets this TFLite file and crashed:

- OS Platform and Distribution: Huawei `Hebe-BD00` running version 12.0.1 (presumably HarmonyOS).
- TensorFlow library: [Gradle dependencies on latest TFLite, GPU, support, and select TF Ops](https://github.com/FedCampus/FedKit/blob/f1ba4d438d19b5d984fe8d0fc6defd3710cc5892/android/fed_kit_train/build.gradle.kts).

### 2. Code

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

This code is [under `gen_tflite` in FedKit](https://github.com/FedCampus/FedKit/tree/f1ba4d438d19b5d984fe8d0fc6defd3710cc5892/gen_tflite).
It is used to create [the TFLite files](https://github.com/FedCampus/FedKit/files/12391810/fed_mcrnn1.tflite.zip).

<details>
<summary>gen_tflite/__init__.py</summary>

```python
import tensorflow as tf

SAVED_MODEL_DIR = ""saved_model""


def red(string: str) -> str:
    return f""\033[91m{string}\033[0m""


class BaseTFLiteModel(tf.Module):
    """"""Base TFLite model class to inherit from.
    # Usage
    Inherent from this class and then annotate with `@tflite_model_class`.
    Override these attributes:
    - `X_SHAPE`: Shape of the input to the model.
    - `Y_SHAPE`: Shape of the output from the model.
    - `model`: A `tf.keras.Model` initialized in `__init__`.
    # Functionality
    Provides default implementation of `train`, `infer`, `parameters`, `restore`.
    These methods are not annotated with `@tf.function`;
    they are supposed to be converted by `@tflite_model_class`.""""""

    X_SHAPE: list[int]
    Y_SHAPE: list[int]
    model: tf.keras.Model

    def train(self, x, y):
        return self.model.train_step((x, y))

    def infer(self, x):
        return {""logits"": self.model(x)}

    def parameters(self):
        return {
            f""a{index}"": weight.read_value()
            for index, weight in enumerate(self.model.weights)
        }

    def restore(self, **parameters):
        for index, weight in enumerate(self.model.weights):
            parameter = parameters[f""a{index}""]
            weight.assign(parameter)
        assert self.parameters is not None
        return self.parameters()


def tflite_model_class(cls):
    """"""Convert `cls` that inherits from `BaseTFLiteModel` to a TFLite model class.
    Convert `cls`'s methods using `@tf.function` with proper `input_signature`
    according to `X_SHAPE` and `Y_SHAPE`.
    The converted methods are `train`, `infer`, `parameters`, `restore`.
    Only `restore`'s `input_signature` is not specified because it need to be
    determined after examples of parameters are given.""""""
    cls.x_spec = tf.TensorSpec([None] + cls.X_SHAPE, tf.float32)  # type: ignore
    cls.y_spec = tf.TensorSpec([None] + cls.Y_SHAPE, tf.float32)  # type: ignore
    cls.train = tf.function(
        cls.train,
        input_signature=[cls.x_spec, cls.y_spec],
    )
    cls.infer = tf.function(
        cls.infer,
        input_signature=[cls.x_spec],
    )
    cls.parameters = tf.function(cls.parameters, input_signature=[])
    cls.restore = tf.function(cls.restore)
    return cls


def save_model(model, saved_model_dir):
    parameters = model.parameters.get_concrete_function()
    init_params = parameters()
    print(f""Initial parameters is {init_params}."")
    restore = model.restore.get_concrete_function(**init_params)
    restore_test = restore(**init_params)
    print(f""Restore test result: {restore_test}."")
    tf.saved_model.save(
        model,
        saved_model_dir,
        signatures={
            ""train"": model.train.get_concrete_function(),
            ""infer"": model.infer.get_concrete_function(),
            ""parameters"": parameters,
            ""restore"": restore,
        },
    )

    converted_params = [
        param.numpy() for param in parameters_from_raw_dict(init_params)
    ]
    shape = f""{[list(param.shape) for param in converted_params]}""
    print(f""Model parameter shape: {red(shape)}."")
    byte_sizes = f""{[param.size * param.itemsize for param in converted_params]}""
    print(f""Model parameter sizes in bytes: {red(byte_sizes)}."")
    return converted_params


def convert_saved_model(saved_model_dir):
    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
        tf.lite.OpsSet.SELECT_TF_OPS,  # enable TensorFlow ops.
    ]

    converter.experimental_enable_resource_variables = True
    tflite_model = converter.convert()

    return tflite_model


def parameters_from_raw_dict(raw_dict):
    parameters = []
    index = 0
    while True:
        parameter = raw_dict.get(f""a{index}"")
        if parameter is None:
            break
        parameters.append(parameter)
        index += 1
    return parameters


def save_tflite_model(tflite_model, tflite_file):
    with open(tflite_file, ""wb"") as model_file:
        return model_file.write(tflite_model)
```

</details>

<details>
<summary>gen_tflite/fed_mcrnn_eg/run.py</summary>

```python
from os import path

from .. import *
from . import FedMCRNNModel

DIR = path.dirname(__file__)


TFLITE_FILE = f""fed_mcrnn1.tflite""


def main():
    model = FedMCRNNModel()
    save_model(model, SAVED_MODEL_DIR)
    tflite_model = convert_saved_model(SAVED_MODEL_DIR)
    save_tflite_model(tflite_model, TFLITE_FILE)


main() if __name__ == ""__main__"" else None
```

</details>

<details>
<summary>gen_tflite/fed_mcrnn_eg/__init__.py</summary>

```python
from tensorflow import keras

from .. import *


@tflite_model_class
class FedMCRNNModel(BaseTFLiteModel):
    X_SHAPE = [7, 8]
    Y_SHAPE = [1]

    def __init__(self):
        self.model = self.build_model()

    def build_model(self):
        """"""Written and tuned by Aicha Slaitane in Aug 2023.""""""
        model = keras.Sequential()
        # For the first LSTM layer, specify the input_shape
        model.add(
            keras.layers.LSTM(
                # Tune number of units separately.
                units=384,
                input_shape=self.X_SHAPE,
                return_sequences=True,
            )
        )
        model.add(keras.layers.LeakyReLU(0.523629795960645))
        model.add(keras.layers.Dropout(0.372150795833))

        # For subsequent LSTM layers, no need to specify input_shape
        model.add(
            keras.layers.LSTM(
                units=64,
                return_sequences=True,
            )
        )
        model.add(keras.layers.LeakyReLU(0.523629795960645))
        model.add(keras.layers.Dropout(0.372150795833))

        model.add(
            keras.layers.LSTM(
                units=480,
                return_sequences=True,
            )
        )
        model.add(keras.layers.LeakyReLU(0.523629795960645))
        model.add(keras.layers.Dropout(0.372150795833))
        model.add(keras.layers.Flatten())
        model.add(keras.layers.Dense(1))

        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.00668472266354),
            loss=""mean_squared_error"",
            metrics=[""mean_absolute_error""],
        )
        return model
```

</details>

Command to build the TFLite file: `python3 -m gen_tflite.fed_mcrnn_eg.run`.

---

Code used on the Android side is [in `FlowerClient.kt` in FedKit](https://github.com/FedCampus/FedKit/blob/f1ba4d438d19b5d984fe8d0fc6defd3710cc5892/android/fed_kit_train/src/main/java/org/eu/fedcampus/fed_kit_train/FlowerClient.kt).

<details>
<summary>Relevant code</summary>

```kotlin
/**
 * Flower client that handles TensorFlow Lite model [Interpreter] and sample data.
 * @param tfliteFileBuffer TensorFlow Lite model file.
 * @param spec Specification for the samples, see [SampleSpec].
 */
class FlowerClient<X : Any, Y : Any>(
    tfliteFileBuffer: MappedByteBuffer,
    val layersSizes: IntArray,
    val spec: SampleSpec<X, Y>,
) : AutoCloseable {
    val interpreter = Interpreter(tfliteFileBuffer)
    val interpreterLock = ReentrantLock()
    val trainingSamples = mutableListOf<Sample<X, Y>>()
    val testSamples = mutableListOf<Sample<X, Y>>()
    val trainSampleLock = ReentrantReadWriteLock()
    val testSampleLock = ReentrantReadWriteLock()

    /**
     * Run inference on [x] using [interpreter] and return the result.
     */
    fun inference(x: Array<X>): Array<Y> {
        val inputs = mapOf(""x"" to x)
        val logits = spec.emptyY(x.size)
        val outputs = mapOf(""logits"" to logits)
        runSignatureLocked(inputs, outputs, ""infer"")
        return logits
    }

    private fun runSignatureLocked(
        inputs: Map<String, Any>,
        outputs: Map<String, Any>,
        signatureKey: String
    ) {
        interpreterLock.withLock {
            interpreter.runSignature(inputs, outputs, signatureKey)
        }
    }
}
```

</details>


### 3. Failure after conversion

See also [issue `Training Fatel Signal` on FedKit](https://github.com/FedCampus/FedKit/issues/15):

<details>
<summary>Crash log</summary>

```ruby
F/libc    (25011): Fatal signal 7 (SIGBUS), code 2 (BUS_ADRERR), fault addr 0x77db8b7690 in tid 10606 (DefaultDispatch), pid 25011 (.cuhk.fedcampus)
*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
Build fingerprint: 'Hinova/TINA-AN00/TS-TINA-Q:11/HinovaHebe-BD00/102.0.1.166C11:user/release-keys'
Revision: '0'
ABI: 'arm64'
Timestamp: 2023-08-21 09:42:39+0800
pid: 25011, tid: 10606, name: DefaultDispatch  >>> com.cuhk.fedcampus <<<
uid: 10263
signal 7 (SIGBUS), code 2 (BUS_ADRERR), fault addr 0x77db8b7690
    x0  b4000077a668b344  x1  00000077db8b7690  x2  0000000000000004  x3  0000000000000001
    x4  00000077db8b7694  x5  b4000077a668b348  x6  0000000000000008  x7  0000000000000008
    x8  00000077db8b7690  x9  0000000000000000  x10 b4000077da4d1bc0  x11 00000077db8b7690
    x12 3ddde2cdbc34570b  x13 3deefded3df00685  x14 0000000000000003  x15 00000000ebad6a89
    x16 00000077ce7e1ed0  x17 00000078ef4d8c40  x18 00000077d1238000  x19 0000000000000004
    x20 b4000077a668b340  x21 b4000077f8cc3b10  x22 0000000000000000  x23 0000000000000001
    x24 0000000000000001  x25 0000000000000001  x26 0000000000000001  x27 0000000000000001
    x28 0000000000000002  x29 00000078589791f0
    lr  00000077ce5999ec  sp  0000007858979110  pc  00000078ef4d8b44  pst 0000000080001000
backtrace:
#00 pc 0000000000086b44  /apex/com.android.runtime/lib64/bionic/libc.so (__memcpy+116) (BuildId: ed6fa1d1056492860af901caffabe1a6)
      #01 pc 000000000014e9e8  /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!libtensorflowlite_jni.so (offset 0x8263000)
      #02 pc 00000000002d5d68  /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!libtensorflowlite_jni.so (offset 0x8263000)
      #03 pc 00000000002d575c  /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!libtensorflowlite_jni.so (offset 0x8263000)
      #04 pc 00000000002c25a8  /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!libtensorflowlite_jni.so (offset 0x8263000)
      #05 pc 0000000000025668  /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!libtensorflowlite_jni.so (offset 0x8263000) (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+100)
      #06 pc 000000000014fed4  /apex/com.android.art/lib64/libart.so (art_quick_generic_jni_trampoline+148) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #07 pc 00000000001467e8  /apex/com.android.art/lib64/libart.so (art_quick_invoke_static_stub+568) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #08 pc 00000000001bc26c  /apex/com.android.art/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+236) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #09 pc 00000000003361cc  /apex/com.android.art/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+376) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #10 pc 000000000032c440  /apex/com.android.art/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+996) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #11 pc 00000000006ce704  /apex/com.android.art/lib64/libart.so (MterpInvokeStatic+548) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #12 pc 0000000000140994  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_static+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #13 pc 00000000003f9aea  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.run+122)
      #14 pc 00000000003236fc  /apex/com.android.art/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame&, art::JValue, bool, bool) (.llvm.15210458325005997145)+348) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #15 pc 00000000006abad0  /apex/com.android.art/lib64/libart.so (artQuickToInterpreterBridge+780) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #16 pc 000000000014fff8  /apex/com.android.art/lib64/libart.so (art_quick_to_interpreter_bridge+88) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #17 pc 000000000206b0f4  /memfd:jit-cache (deleted) (offset 0x2000000) (org.tensorflow.lite.NativeInterpreterWrapper.runSignature+1044)
      #18 pc 0000000000146564  /apex/com.android.art/lib64/libart.so (art_quick_invoke_stub+548) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #19 pc 00000000001bc250  /apex/com.android.art/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+208) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #20 pc 00000000003361cc  /apex/com.android.art/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+376) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #21 pc 000000000032c440  /apex/com.android.art/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+996) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #22 pc 00000000006cb78c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+848) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #23 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #24 pc 00000000003f8e14  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (org.tensorflow.lite.Interpreter.runSignature+36)
      #25 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #26 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #27 pc 00000000003f6440  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (org.eu.fedcampus.fed_kit_train.FlowerClient.runSignatureLocked+20)
      #28 pc 00000000006ce0c8  /apex/com.android.art/lib64/libart.so (MterpInvokeDirect+1248) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #29 pc 0000000000140914  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_direct+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #30 pc 00000000003f5aa0  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (org.eu.fedcampus.fed_kit_train.FlowerClient.inference+84)
      #31 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #32 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #33 pc 00000000003f616e  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (org.eu.fedcampus.fed_kit_train.FlowerClient.evaluate+146)
      #34 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #35 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #36 pc 0000000000001284  [anon:dalvik-classes7.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!classes7.dex] (com.cuhk.fedcampus.train.FedmcrnnClient$evaluate$1.invokeSuspend+52)
      #37 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #38 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #39 pc 0000000000001234  [anon:dalvik-classes7.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!classes7.dex] (com.cuhk.fedcampus.train.FedmcrnnClient$evaluate$1.invoke+16)
      #40 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #41 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #42 pc0000000000001208  [anon:dalvik-classes7.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!classes7.dex] (com.cuhk.fedcampus.train.FedmcrnnClient$evaluate$1.invoke+4)
      #43 pc 00000000006cd464  /apex/com.android.art/lib64/libart.so (MterpInvokeInterface+1808) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #44 pc 0000000000140a14  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_interface+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #45 pc 0000000000001b00  [anon:dalvik-classes7.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!classes7.dex] (com.cuhk.fedcampus.train.FedmcrnnClient$tryLaunch$1.invokeSuspend+72)
      #46 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #47 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #48 pc 0000000000363596  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith+42)
      #49 pc 00000000006cd464  /apex/com.android.art/lib64/libart.so (MterpInvokeInterface+1808) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #50 pc 0000000000140a14  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_interface+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #51 pc 00000000003a9a98  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (kotlinx.coroutines.DispatchedTask.run+448)
      #52 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #53 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #54 pc 00000000003ea81e  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (kotlinx.coroutines.scheduling.CoroutineScheduler.runSafely+2)
      #55 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #56 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #57 pc 00000000003e93fa  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.executeTask+34)
      #58 pc 00000000006ce0c8  /apex/com.android.art/lib64/libart.so (MterpInvokeDirect+1248) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #59 pc 0000000000140914  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_direct+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #60 pc 00000000003e9528  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.runWorker+56)
      #61 pc 00000000006ce0c8  /apex/com.android.art/lib64/libart.so (MterpInvokeDirect+1248) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #62 pc 0000000000140914  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_direct+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #63 pc 00000000003e94d8  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.run)
      #64 pc 00000000003236fc  /apex/com.android.art/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame&, art::JValue, bool, bool) (.llvm.15210458325005997145)+348) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #65 pc 00000000006abad0  /apex/com.android.art/lib64/libart.so (artQuickToInterpreterBridge+780) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #66 pc 000000000014fff8  /apex/com.android.art/lib64/libart.so (art_quick_to_interpreter_bridge+88) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #67 pc 0000000000146564  /apex/com.android.art/lib64/libart.so (art_quick_invoke_stub+548) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #68 pc 00000000001bc250  /apex/com.android.art/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+208) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #69 pc 0000000000591268  /apex/com.android.art/lib64/libart.so (art::JValue art::InvokeVirtualOrInterfaceWithJValues<art::ArtMethod*>(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, art::ArtMethod*, jvalue const*)+460) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #70 pc 00000000005e3bd0  /apex/com.android.art/lib64/libart.so (art::Thread::CreateCallback(void*)+1364) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #71 pc 00000000000ed068  /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void*)+64) (BuildId: ed6fa1d1056492860af901caffabe1a6)
      #72 pc 000000000008d5e0  /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (BuildId: ed6fa1d1056492860af901caffabe1a6)
Lost connection to device.
Exited (sigterm)
```

</details>
"
61653,Check failed when running tensorflow.python.ops.gen_nn_ops.max_pool_grad_with_argmax,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Specific input combination is caused check failure.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([2, 3, 3, 1], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([2, 2, 2, 1], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.random.uniform([2, 2, 2, 1], minval=-256, maxval=257, dtype=tf.int64)
      arg_2 = tf.identity(arg_2_tensor)
      ksize_0 = 1
      ksize_1 = 2
      ksize_2 = 2
      ksize_3 = 1
      ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
      strides_0 = 1
      strides_1 = 1
      strides_2 = 1
      strides_3 = 1
      strides = [strides_0,strides_1,strides_2,strides_3,]
      padding = ""VALID""
      include_batch_in_index = False
      out = gen_nn_ops.max_pool_grad_with_argmax(arg_0,arg_1,arg_2,ksize=ksize,strides=strides,padding=padding,include_batch_in_index=include_batch_in_index,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.int64)
      ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
      strides = [strides_0,strides_1,strides_2,strides_3,]
      gen_nn_ops.max_pool_grad_with_argmax(arg_0,arg_1,arg_2,ksize=ksize,strides=strides,padding=padding,include_batch_in_index=include_batch_in_index,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-21 00:36:14.553160: F tensorflow/core/kernels/maxpooling_op.cc:1081] Check failed: grad_out_index >= output_start && grad_out_index < output_end Invalid output gradient index: 77, 0, 18
Aborted

```
```
"
61652,Check failed when running tensorflow.python.ops.gen_nn_ops.max_pool_grad_with_argmax (colab+local),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

probably due to negative large input tensor

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.constant(-105687333925307, shape=[2, 3, 3, 1], dtype=tf.float32,)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([2, 2, 2, 1], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.random.uniform([2, 2, 2, 1], minval=-256, maxval=257, dtype=tf.int64)
      arg_2 = tf.identity(arg_2_tensor)
      ksize_0 = 1
      ksize_1 = 2
      ksize_2 = 2
      ksize_3 = 1
      ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
      strides_0 = 1
      strides_1 = 1
      strides_2 = 1
      strides_3 = 1
      strides = [strides_0,strides_1,strides_2,strides_3,]
      padding = ""VALID""
      include_batch_in_index = False
      out = gen_nn_ops.max_pool_grad_with_argmax(arg_0,arg_1,arg_2,ksize=ksize,strides=strides,padding=padding,include_batch_in_index=include_batch_in_index,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.int64)
      ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
      strides = [strides_0,strides_1,strides_2,strides_3,]
      gen_nn_ops.max_pool_grad_with_argmax(arg_0,arg_1,arg_2,ksize=ksize,strides=strides,padding=padding,include_batch_in_index=include_batch_in_index,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-21 00:24:51.784661: F tensorflow/core/kernels/maxpooling_op.cc:1081] Check failed: grad_out_index >= output_start && grad_out_index < output_end Invalid output gradient index: 205, 0, 18
Aborted
```
```
"
61651,Tensorflow on Manjaro,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.8

### Custom code

Yes

### OS platform and distribution

Linux Manjaro

### Mobile device

Android

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

How to setup and perform main configuration on Manjaro platform

### Standalone code to reproduce the issue

```shell
I see nothin expected...
```


### Relevant log output

_No response_"
61650,Activation function of a Dense hidden layer not getting invoked.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

Yes

### OS platform and distribution

MacOS 13.4, MacBook Pro M2 Max

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Issue:
In the given auto-encoder setup, the encoder layers activation function (relu) is not getting invoked.

1. We create a simple auto-encoder, with Input size 3, hidden size 2, and output back to 3.
2. The activation function of the encoder layer is set a relu.
3. The weights of the encoder layers are all made negative. Idea is, if input is +ve, all the neutrons will have negative value and relu will o/p zero.
4. Give input as [1, 0, 0].
5. We expect the final decoder o/p layer, which has sigmoid activation, to o/p all [0.5, 0.5, 0.5] as the input to this layer from the encoder should have been [0, 0, 0]. 
6. But we find that is not the case, which clearly shows that 'relu' activation of the hidden layer is not getting invoked.

Installation:
pip install tensorflow-macos
pip install tensorflow-metal

### Standalone code to reproduce the issue

```shell
# https://colab.research.google.com/drive/14KKrdiBg8FT2cdUC5pjqJHi5S3BkTOHi?usp=sharing
# The above colab will run fine, but the same code on Mac with the said config has issue.
# Copying the code here for quick reference.

import tensorflow as tf    
import tensorflow.keras
import tensorflow as tf
import platform
import sys
from tensorflow.keras.layers import Input, Dense, Layer
from tensorflow.keras.models import Model

# Print versions:
print(f""Python {sys.version}"")
print(f""Python Platform: {platform.platform()}"")
print(f""Tensor Flow Version: {tf.__version__}"")
gpu = len(tf.config.list_physical_devices('GPU'))>0
print(""GPU is"", ""available"" if gpu else ""NOT AVAILABLE"")

# Setup input
import numpy as np
X_check = np.array([[1, 0, 0]])

# Setup autoencoder model
input_layer = Input(shape=(X_check.shape[1]))
bottleneck = Dense(2, activation='relu', name='bottleneck')(input_layer)
output = Dense(X_check.shape[1], activation='sigmoid', name='output')(bottleneck)
autoencoder = Model(input_layer, output)

# Set encoder layer weights to all negative.
layer = autoencoder.layers[1]
weights = np.array([[-1, -1],[-1, -1], [-1, -1]])
biases = np.array([0, 0])
layer.set_weights([weights, biases])

# create encoder model.
encoder = Model(input_layer, bottleneck)

# create decoder model.
decoder_input = Input(shape=(2,), name='decoder_input')
decoder_layer = autoencoder.layers[-1]
decoder = Model(decoder_input, decoder_layer(decoder_input))

# Run auto-encoder, with [1, 0, 0], since encoder has all negative weights,
# and has 'relu' activation o/p of enocder should all be zeros. And that being
# the input of next sigmod we should get output [0.5, 0.5, 0.5]
output_data = autoencoder.predict(X_check)
print(output_data)
```


### Relevant log output

```shell
Python 3.8.17 (default, Jul  5 2023, 15:45:03) 
[Clang 14.0.6 ]
Python Platform: macOS-13.4-arm64-arm-64bit
Tensor Flow Version: 2.13.0
GPU is available
1/1 [==============================] - 0s 38ms/step
[[0.287966   0.85427195 0.28276426]]
```
"
61649,GPU error Tensorflow with 2.1,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.1

### Custom code

Yes

### OS platform and distribution

CentOS Linux release 7.4.1708 

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

10/7

### GPU model and memory

Nvidia A100 partitioned virtually in two 40GB GPUs, I am using one of them

### Current behavior?

I am working on 3D_ U-net. I am getting the ptax error with Tensorflow 2.1 when I run the 3D U-net , I am using tensorflow-large-model-support to scale the algorithm



### Standalone code to reproduce the issue

```shell
https://github.com/junaidjawaid1/3d_U-Net-TFLMS/tree/main```


### Relevant log output

```shell
Traceback (most recent call last):

File ""<string>"", line 1, in <module>

ModuleNotFoundError: No module named 'nvidia'

dirname: missing operand

Try 'dirname --help' for more information.

/opt/gridengine/default/spool/compute-0-3/job_scripts/108258: line 8: $'\342\200\213': command not found

/opt/gridengine/default/spool/compute-0-3/job_scripts/108258: line 10: $'\342\200\213': command not found

Traceback (most recent call last):

File ""<string>"", line 1, in <module>

ModuleNotFoundError: No module named 'nvidia'

dirname: missing operand

Try 'dirname --help' for more information.

2023-08-18 18:25:55.025052: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2

2023-08-18 18:25:56.869820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.7

2023-08-18 18:25:56.881355: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.7

2023-08-18 18:26:00.093020: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1

2023-08-18 18:26:00.188610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:26:00.192419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:

pciBusID: 2c8f7:00:00.0 name: NVIDIA A100 80GB PCIe MIG 1c.4g.40gb computeCapability: 8.0

coreClock: 1.41GHz coreCount: 14 deviceMemorySize: 39.25GiB deviceMemoryBandwidth: 901.22GiB/s

2023-08-18 18:26:00.192453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2

2023-08-18 18:26:00.192490: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10

2023-08-18 18:26:00.218166: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10

2023-08-18 18:26:00.304953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10

2023-08-18 18:26:00.356784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10

2023-08-18 18:26:00.396554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10

2023-08-18 18:26:00.396599: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7

2023-08-18 18:26:00.396731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:26:00.398058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:26:00.399160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0

2023-08-18 18:26:00.411267: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA

2023-08-18 18:26:00.434767: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2249595000 Hz

2023-08-18 18:26:00.439151: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fc06e5d220 initialized for platform Host (this does not guarantee that XLA will be used). Devices:

2023-08-18 18:26:00.439189: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version

2023-08-18 18:26:00.648000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:26:00.648956: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fc06ec3b80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:

2023-08-18 18:26:00.648984: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): NVIDIA A100 80GB PCIe MIG 1c.4g.40gb, Compute Capability 8.0

2023-08-18 18:26:00.649332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:26:00.650184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:

pciBusID: 2c8f7:00:00.0 name: NVIDIA A100 80GB PCIe MIG 1c.4g.40gb computeCapability: 8.0

coreClock: 1.41GHz coreCount: 14 deviceMemorySize: 39.25GiB deviceMemoryBandwidth: 901.22GiB/s

2023-08-18 18:26:00.650216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2

2023-08-18 18:26:00.650232: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10

2023-08-18 18:26:00.650249: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10

2023-08-18 18:26:00.650258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10

2023-08-18 18:26:00.650267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10

2023-08-18 18:26:00.650276: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10

2023-08-18 18:26:00.650294: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7

2023-08-18 18:26:00.650351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:26:00.651127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:26:00.651846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0

2023-08-18 18:26:00.651877: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2

2023-08-18 18:32:04.446608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:

2023-08-18 18:32:04.446938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] 0

2023-08-18 18:32:04.446953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0: N

2023-08-18 18:32:04.447365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:32:04.448403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:32:04.449678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 120000 MB memory) -> physical GPU (device: 0, name: NVIDIA A100 80GB PCIe MIG 1c.4g.40gb, pci bus id: 2c8f7:00:00.0, compute capability: 8.0)

2023-08-18 18:32:04.460014: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 117.19G (125829120000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.464233: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 105.47G (113246208000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.468360: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 94.92G (101921587200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.472455: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 85.43G (91729428480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.476553: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 76.89G (82556485632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.480809: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 69.20G (74300833792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.484893: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 62.28G (66870747136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.488957: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 56.05G (60183670784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.493121: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 50.45G (54165303296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.497205: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 45.40G (48748773376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.501259: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 40.86G (43873894400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

WARNING:tensorflow:sample_weight modes were coerced from

...

to

['...']

WARNING:tensorflow:sample_weight modes were coerced from

...

to

['...']

2023-08-18 18:33:25.857187: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10

2023-08-18 18:35:29.287470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7

2023-08-18 18:46:02.247907: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal : Value 'sm_80' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. This message will be only logged once.

2023-08-18 18:46:36.474700: F tensorflow/stream_executor/cuda/cuda_dnn.cc:516] Check failed: cudnnSetTensorNdDescriptor(handle_.get(), elem_type, nd, [dims.data](https://dims.data/)(), [strides.data](https://strides.data/)()) == CUDNN_STATUS_SUCCESS (3 vs. 0)batch_descriptor: {count: 7 feature_map_count: 146 spatial: 64 0 64 value_min: 0.000000 value_max: 0.000000 layout: BatchDepthYX}

/opt/gridengine/default/spool/compute-0-3/job_scripts/108258: line 13: 237129 Aborted python $PYTHON_SCRIPT
```
"
61648,Cannot build tensorflow from source,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

latest

### Custom code

Yes

### OS platform and distribution

Debian 12

### Mobile device

_No response_

### Python version

Python 3.8

### Bazel version

Latest

### GCC/compiler version

Clang 16

### CUDA/cuDNN version

Dont have

### GPU model and memory

Dont have

### Current behavior?

I tried several times, still same error. Searched on google found nothing

### Standalone code to reproduce the issue

```shell
I follow the guide from tensorflow.com but still faced this error. Please help
```


### Relevant log output

```shell
(myenv) root@drowsiness:~/tensorflow# bazel build -j 2 --local_ram_resources=3000 --config=opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=189
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /root/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/root/anaconda3/envs/myenv/bin/python3 --action_env PYTHON_LIB_PATH=/root/anaconda3/envs/myenv/lib/python3.8/site-packages --python_path=/root/anaconda3/envs/myenv/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/lib/llvm-16/bin/clang --repo_env=CC=/usr/lib/llvm-16/bin/clang --repo_env=BAZEL_COMPILER=/usr/lib/llvm-16/bin/clang --copt=-Wno-gnu-offsetof-extensions
INFO: Found applicable config definition build:short_logs in file /root/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /root/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file /root/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /root/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /root/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (648 packages loaded, 42257 targets configured).
INFO: Found 1 target...
INFO: Deleting stale sandbox base /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/sandbox
ERROR: /root/tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:475:11: Compiling tensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc failed: (Killed): clang failed: error executing command (from target //tensorflow/compiler/mlir/tensorflow:tensorflow_ops) 
  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \
  exec env - \
    CLANG_COMPILER_PATH=/usr/lib/llvm-16/bin/clang \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-x86_64/bin:/root/anaconda3/envs/myenv/bin:/root/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/root/anaconda3/envs/myenv/bin/python3 \
    PYTHON_LIB_PATH=/root/anaconda3/envs/myenv/lib/python3.8/site-packages \
    TF2_BEHAVIOR=1 \
  /usr/lib/llvm-16/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/_objs/tensorflow_ops/tf_ops.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/_objs/tensorflow_ops/tf_ops.pic.o' -fPIC '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT="".so""' '-DLLVM_PLUGIN_EXT="".so""' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_DEREGISTER_FRAME=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_H=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_SETENV_R=1' '-DHAVE_STRERROR_R=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_MALLINFO=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' '-DLLVM_NATIVE_ARCH=""X86""' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' '-DLLVM_HOST_TRIPLE=""x86_64-unknown-linux-gnu""' '-DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64-unknown-linux-gnu""' '-DLLVM_VERSION_MAJOR=18' '-DLLVM_VERSION_MINOR=0' '-DLLVM_VERSION_PATCH=0' '-DLLVM_VERSION_STRING=""18.0.0git""' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS '-DBLAKE3_USE_NEON=0' -DBLAKE3_NO_AVX2 -DBLAKE3_NO_AVX512 -DBLAKE3_NO_SSE2 -DBLAKE3_NO_SSE41 -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY '-DBAZEL_CURRENT_REPOSITORY=""""' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/llvm-project -iquote bazel-out/k8-opt/bin/external/llvm-project -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/ml_dtypes -iquote bazel-out/k8-opt/bin/external/ml_dtypes -iquote external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/k8-opt/bin/external/snappy -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithCanonicalizationIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AsmParserTokenKinds -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include -isystem external/llvm-project/mlir/include -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/include -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/ml_dtypes -isystem bazel-out/k8-opt/bin/external/ml_dtypes -isystem external/ml_dtypes/ml_dtypes -isystem bazel-out/k8-opt/bin/external/ml_dtypes/ml_dtypes -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-gnu-offsetof-extensions -Wno-sign-compare '-std=c++17' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc -o bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/_objs/tensorflow_ops/tf_ops.pic.o)
# Configuration: 4332b06bceb8e99a0d8ed4f75fa26218a779c66acf683ffad0936df1e9f625df
# Execution platform: @local_execution_config_platform//:platform
In file included from tensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc:16:
In file included from ./tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h:38:
In file included from ./tensorflow/compiler/mlir/tensorflow/ir/tf_attributes.h:21:
In file included from ./tensorflow/core/ir/types/dialect.h:31:
bazel-out/k8-opt/bin/tensorflow/core/ir/types/dialect.h.inc:31:19: warning: 'parseType' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]
     ::mlir::Type parseType(::mlir::DialectAsmParser &parser) const;
                  ^
external/llvm-project/mlir/include/mlir/IR/Dialect.h:107:16: note: overridden virtual function is here
  virtual Type parseType(DialectAsmParser &parser) const;
               ^
In file included from tensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc:16:
In file included from ./tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h:38:
In file included from ./tensorflow/compiler/mlir/tensorflow/ir/tf_attributes.h:21:
In file included from ./tensorflow/core/ir/types/dialect.h:31:
bazel-out/k8-opt/bin/tensorflow/core/ir/types/dialect.h.inc:32:11: warning: 'printType' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]
     void printType(::mlir::Type type, ::mlir::DialectAsmPrinter &printer) const;
          ^
external/llvm-project/mlir/include/mlir/IR/Dialect.h:110:16: note: overridden virtual function is here
  virtual void printType(Type, DialectAsmPrinter &) const {
               ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3990.011s, Critical Path: 343.16s
INFO: 991 processes: 3 internal, 988 local.
FAILED: Build did NOT complete successfully
```
"
61647,docs: spurious favicon in template,"I canâ€™t work out where the template for this lives, but the Tensorflow docs (at least the homepage and a couple of random pages I spot-checked) have a spurious favicon link to an image that doesnâ€™t exist:

<pre>&lt;link href=""<a href=""https://www.tensorflow.org/static/en/site-assets/images/marketing/favicon.png"">/static/en/site-assets/images/marketing/favicon.png</a>"" rel=""shortcut icon""/&gt;</pre>

This is shadowed by another `<link>` page earlier in the head and so doesnâ€™t seem to be causing any immediate problems, but it still seems like it might be a good idea to clean it up."
61646,Check failure when running tf.config.experimental_connect_to_host,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

GTX 1660 TI

### Current behavior?

Due to feeding NaN input Argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
try:
  try:
    with tf.device('/CPU'):
      arg_0 = ""nan""
      out = tf.config.experimental_connect_to_host(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      tf.config.experimental_connect_to_host(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-19 19:02:09.775956: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-19 19:02:10.305057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-19 19:02:10.742608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.761041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.761185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.762359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.762491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.762611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.826641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.826771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.826888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.826971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3389 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-19 19:02:10.829417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3389 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-19 19:02:10.838878: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:600] INVALID_ARGUMENT: Could not interpret ""nan"" as a host-port pair.
E0819 19:02:10.839114974  187448 completion_queue.cc:244]    assertion failed: queue.num_items() == 0
Aborted

```
```
"
61645,CKPT to TFLite,"How can I convert ckpt file to TF Lite, while I've only .ckpt file. No meta-file present"
61644,"`tf.image.crop_to_bounding_box()` assumes `tf.int32` arguments, but not documented as such","### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04 (WSL 2)

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.image.crop_to_bounding_box()` implicitly assumes that the target width and height are `tf.int32`, but this is not documented anywhere. The cause for this is using `tf.shape()` which has the default `dtype` of `tf.int32`, in a stack operation:
https://github.com/tensorflow/tensorflow/blob/c9fafed9bc8cb0238a775fd4a0680e648c06b5b6/tensorflow/python/ops/image_ops_impl.py#L1250-L1254

### Standalone code to reproduce the issue

```python
import tensorflow as tf

image = tf.zeros([1000, 2000, 3], dtype=tf.uint8)
offset = tf.constant([0, 0], dtype=tf.int64)
size = tf.constant([900, 1500], dtype=tf.int64)
tf.image.crop_to_bounding_box(image, offset[0], offset[1], size[0], size[1])
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/.../test.py"", line 6, in <module>
    tf.image.crop_to_bounding_box(image, offset[0], offset[1], size[0], size[1])
  File ""/.../.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/.../.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 6656, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute Pack as input #1(zero-based) was expected to be a int32 tensor but is a int64 tensor [Op:Pack] name: stack
```
"
61643,Check failure when running tf.keras.layers.RepeatVector,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to a very large integer variable as input to the API

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  n = 1152921504606846975
  arg_class = tf.keras.layers.RepeatVector(n=n,)
  arg_input_0_tensor = tf.random.uniform([2, 2], dtype=tf.float32)
  arg_input_0 = tf.identity(arg_input_0_tensor)
  arg_input = [arg_input_0,]
  out = arg_class(*arg_input)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
023-08-18 22:31:57.943444: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-18 22:31:58.502559: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-18 22:31:58.969005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:31:58.987690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:31:58.987833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:31:58.989274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:31:58.989436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:31:58.989557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:31:59.044537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:31:59.044666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:31:59.044756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:31:59.044834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4249 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
{}
2023-08-18 22:31:59.138979: F tensorflow/tsl/framework/bfc_allocator.cc:797] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum) 
Aborted

```
```
"
61642,Check failure when running tf.compat.v1.layers.MaxPooling1D,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to feeding Large list elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  pool_size_0 = 1e+38
  pool_size = [pool_size_0,]
  strides_0 = 2
  strides = [strides_0,]
  padding = ""same""
  data_format = ""channels_last""
  arg_class = tf.compat.v1.layers.MaxPooling1D(pool_size=pool_size,strides=strides,padding=padding,data_format=data_format,)
  arg_input_0_tensor = tf.random.uniform([1, 5, 4], dtype=tf.float32)
  arg_input_0 = tf.identity(arg_input_0_tensor)
  arg_input = [arg_input_0,]
  out = arg_class(*arg_input)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
023-08-18 22:04:07.309328: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-18 22:04:07.824681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-18 22:04:08.293378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:04:08.311735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:04:08.311868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:04:08.313249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:04:08.313383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:04:08.313504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:04:08.379287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:04:08.379413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:04:08.379501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-18 22:04:08.379578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4308 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-18 22:04:08.473502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600
2023-08-18 22:04:08.473555: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:983] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
Aborted

```
```
"
61639,Softmax overflow issue with a large tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0 dev20230706

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

A100 80G

### Current behavior?

Non-OK-status: GpuLaunchKernel( GenerateNormalizedProb<T, acc_type, kUnroll>, numBlocks, numThreadsPerBlock, 0, cu_stream, reinterpret_cast<const T*>(logits_in_.flat<T>().data()), reinterpret_cast<const acc_type*>( sum_probs.flat<acc_type>().data()), reinterpret_cast<const T*>(max_logits.flat<T>().data()), const_cast<T*>(softmax_out->flat<T>().data()), rows, cols, log_) status: INTERNAL: invalid configuration argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

print(tf.__version__)

x = tf.constant(1., shape=(1, 1))
y = tf.tile(x, (2**28, 9))  # Number of elements cannot fit in int32 value
print(y.numpy())
z = tf.math.softmax(y)
print(z.numpy())
```


### Relevant log output

```shell
2023-08-18 13:34:53.496914: I tensorflow/core/common_runtime/placer.cc:125] logits: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
Softmax: (Softmax): /job:localhost/replica:0/task:0/device:GPU:0
2023-08-18 13:34:53.496979: I tensorflow/core/common_runtime/placer.cc:125] Softmax: (Softmax): /job:localhost/replica:0/task:0/device:GPU:0
softmax_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-08-18 13:34:53.496988: I tensorflow/core/common_runtime/placer.cc:125] softmax_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-08-18 13:34:53.497454: I tensorflow/core/common_runtime/eager/execute.cc:1747] Executing op Softmax in device /job:localhost/replica:0/task:0/device:GPU:0
2023-08-18 13:34:53.507131: F tensorflow/core/kernels/softmax_op_gpu.cu.cc:255] Non-OK-status: GpuLaunchKernel( GenerateNormalizedProb<T, acc_type, kUnroll>, numBlocks, numThreadsPerBlock, 0, cu_stream, reinterpret_cast<const T*>(logits_in_.flat<T>().data()), reinterpret_cast<const acc_type*>( sum_probs.flat<acc_type>().data()), reinterpret_cast<const T*>(max_logits.flat<T>().data()), const_cast<T*>(softmax_out->flat<T>().data()), rows, cols, log_) status: INTERNAL: invalid configuration argument
Aborted (core dumped)
```
"
61635,external/XNNPACK/src/qs8-igemm/gen/4x16c8-minmax-avx512skx.c:242:15: error: implicit declaration of function '_kshiftri_mask64' is invalid in C99,"I run commands
""bazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain   //tensorflow/lite/java:tensorflow-lite""  
use 
tensorflow 2.4.0
 bazel 3.1.0 ,NDK  27 ,SDK 29 ,But what should I do to resolve the following errorsï¼ï¼ï¼

ERROR: /home/ferey/.cache/bazel/_bazel_ferey/97c7558b0863433a4def08aa5708cd20/external/XNNPACK/BUILD.bazel:3516:1: C++ compilation of rule '@XNNPACK//:avx512skx_ukernels' failed (Exit 1)
external/XNNPACK/src/qs8-igemm/gen/4x16c8-minmax-avx512skx.c:242:15: error: implicit declaration of function '_kshiftri_mask64' is invalid in C99 [-Werror,-Wimplicit-function-declaration]
      vmask = _kshiftri_mask64(vmask, 16);
              ^
1 error generated.
Target //tensorflow/lite/java:tensorflow-lite failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 229.018s, Critical Path: 17.13s
INFO: 1788 processes: 1788 local.
FAILED: Build did NOT complete successfully

"
61633,"libtensorflowlite_jni.so (offset 0x2f3000):signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xa59fdbc0","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

org.tensorflow:tensorflow-lite:0.0.0-nightly org.tensorflow:tensorflow-lite-gpu:2.3.0 org.tensorflow:tensorflow-lite-support:0.1.0

### Custom code

Yes

### OS platform and distribution

Andorid 13

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Our process com.vt.tv.aipq use tensorflow-lite cause process crash:
*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
Build fingerprint: 'Hisense/songshan-FFM/songshan:11/RTT2.220118.001/00.00.00.40:user/release-keys'
Revision: '1234'
ABI: 'arm'
Timestamp: 2023-05-06 14:36:47+0800
pid: 14794, tid: 28796, name: TFService-T  >>> com.vt.tv.aipq <<<
uid: 1000
signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xa59fdbc0
    r0  00000004  r1  00000020  r2  00000300  r3  a59fdbc0
    r4  31a05640  r5  000002f0  r6  31a05740  r7  a59fe4c0
    r8  31a056c0  r9  a5674220  r10 a59fe1c0  r11 31a055c0
    ip  a59fdec0  sp  84db73e8  lr  832ded97  pc  832f08ac

backtrace:
      #00 pc 000258ac  /system_ext/app/HiAIPQ/HiAIPQ.apk!libtensorflowlite_jni.so (offset 0x2f3000)

### Standalone code to reproduce the issue

```shell
The Google server background statistics process crash information, unable to clarify the steps to reproduce the problem.
```


### Relevant log output

_No response_"
61631,Overflow when running tf.compat.v1.manip.tile,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large element in the input list

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  input_tensor = tf.random.uniform([1, 355, 768], dtype=tf.float32)
  input = tf.identity(input_tensor)
  multiples_0 = 125091515651
  multiples_1 = True
  multiples_2 = 125091515651
  multiples = [multiples_0,multiples_1,multiples_2,]
  name = None
  out = tf.compat.v1.manip.tile(input=input,multiples=multiples,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Tile_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 44407488056105 with 96070284019968, result: -1
	 [[{{node Tile}}]] [Op:Tile]

```
```
"
61630,Overflow bug when running tf.image.resize,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in the input list

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  arg_0_tensor = tf.constant(-1000000, shape=[577, 700, 3, 1], dtype=tf.float64,)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 1610637938
  arg_1_1 = 1250999896764
  arg_1 = [arg_1_0,arg_1_1,]
  out = tf.image.resize(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__ResizeBilinear_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 929338090226 with 1164413628, result: -1 [Op:ResizeBilinear] name:
```
```
"
61629,Overflow bug when running tf.compat.v1.image.resize,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in input list

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  images_tensor = tf.constant(-15621075306911, shape=[218, 178, 3, 1], dtype=tf.int64,)
  images = tf.identity(images_tensor)
  size_0 = 8968073515812833920
  size_1 = 536870912
  size = [size_0,size_1,]
  method = ""nearest""
  align_corners = False
  preserve_aspect_ratio = False
  name = None
  out = tf.compat.v1.image.resize(images=images,size=size,method=method,align_corners=align_corners,preserve_aspect_ratio=preserve_aspect_ratio,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__ResizeNearestNeighbor_device_/job:localhost/replica:0/task:0/device:CPU:0}} Encountered overflow when multiplying 20527214848 with 536870912, result: -7426279517443850240 [Op:ResizeNearestNeighbor] name: 
```
```
"
61628,Overflow when running tf.compat.v1.linalg.diag,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in the input list

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  diagonal_0_0_0_0 = 1111
  diagonal_0_0_0_1 = 1112
  diagonal_0_0_0 = [diagonal_0_0_0_0,diagonal_0_0_0_1,]
  diagonal_0_0_1_0 = 1121
  diagonal_0_0_1_1 = 1122
  diagonal_0_0_1 = [diagonal_0_0_1_0,diagonal_0_0_1_1,]
  diagonal_0_0 = [diagonal_0_0_0,diagonal_0_0_1,]
  diagonal_0_1_0_0 = 1211
  diagonal_0_1_0_1 = 1212
  diagonal_0_1_0 = [diagonal_0_1_0_0,diagonal_0_1_0_1,]
  diagonal_0_1_1_0 = 1221
  diagonal_0_1_1_1 = 1222
  diagonal_0_1_1 = [diagonal_0_1_1_0,diagonal_0_1_1_1,]
  diagonal_0_1 = [diagonal_0_1_0,diagonal_0_1_1,]
  diagonal_0 = [diagonal_0_0,diagonal_0_1,]
  diagonal_1_0_0_0 = 2111
  diagonal_1_0_0_1 = 2112
  diagonal_1_0_0 = [diagonal_1_0_0_0,diagonal_1_0_0_1,]
  diagonal_1_0_1_0 = 2121
  diagonal_1_0_1_1 = 2122
  diagonal_1_0_1 = [diagonal_1_0_1_0,diagonal_1_0_1_1,]
  diagonal_1_0 = [diagonal_1_0_0,diagonal_1_0_1,]
  diagonal_1_1_0_0 = 2211
  diagonal_1_1_0_1 = 2212
  diagonal_1_1_0 = [diagonal_1_1_0_0,diagonal_1_1_0_1,]
  diagonal_1_1_1_0 = 2221
  diagonal_1_1_1_1 = 2222
  diagonal_1_1_1 = [diagonal_1_1_1_0,diagonal_1_1_1_1,]
  diagonal_1_1 = [diagonal_1_1_0,diagonal_1_1_1,]
  diagonal_1 = [diagonal_1_0,diagonal_1_1,]
  diagonal = [diagonal_0,diagonal_1,]
  name = ""diag_part""
  k = 1610637938
  padding_value = 0
  align = ""RIGHT_LEFT""
  out = tf.compat.v1.linalg.diag(diagonal=diagonal,name=name,k=k,padding_value=padding_value,align=align,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__MatrixDiagV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Encountered overflow when multiplying 12885103520 with 1610637940, result: -1 [Op:MatrixDiagV3]
```
```
"
61627,Overflow bug when running tf.compat.v1.manip.tile,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in the input tensor

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  input_tensor = tf.random.uniform([1, 355, 768], dtype=tf.float32)
  input = tf.identity(input_tensor)
  multiples_0 = 125091515651
  multiples_1 = True
  multiples_2 = 125091515651
  multiples = [multiples_0,multiples_1,multiples_2,]
  name = None
  out = tf.compat.v1.manip.tile(input=input,multiples=multiples,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Tile_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 44407488056105 with 96070284019968, result: -1
	 [[{{node Tile}}]] [Op:Tile]

```
```
"
61626,Overflow when running tf.compat.v1.tile,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in the input list

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  input_tensor = tf.random.uniform([370, 1, 1024], dtype=tf.float32)
  input = tf.identity(input_tensor)
  multiples_0 = 125091515651
  multiples_1 = 125091515651
  multiples_2 = 125091515651
  multiples = [multiples_0,multiples_1,multiples_2,]
  name_tensor = tf.random.uniform([], dtype=tf.int32, maxval=66860669291904)
  name = tf.identity(name_tensor)
  name = tf.Variable(name)
  out = tf.compat.v1.tile(input=input,multiples=multiples,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Tile_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 46283860790870 with 125091515651, result: -1
	 [[{{node Tile}}]] [Op:Tile]

```
```
"
61625,Overflow bug when running tf.compat.v1.keras.layers.ZeroPadding2D,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in the input list

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  padding_0_0 = 125091515651
  padding_0_1 = 125091515651
  padding_0 = [padding_0_0,padding_0_1,]
  padding_1_0 = 125091515651
  padding_1_1 = 125091515651
  padding_1 = [padding_1_0,padding_1_1,]
  padding = [padding_0,padding_1,]
  data_format = None
  arg_class = tf.compat.v1.keras.layers.ZeroPadding2D(padding=padding,data_format=data_format,)
  arg_input_0_tensor = tf.random.uniform([3, 14, 14, 576], dtype=tf.float32)
  arg_input_0 = tf.identity(arg_input_0_tensor)
  arg_input = [arg_input_0,]
  out = arg_class(*arg_input)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
{{function_node __wrapped__Pad_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 750549093948 with 250183031316, result: -1
	 [[{{node Pad}}]] [Op:Pad]

Call arguments received by layer 'zero_padding2d' (type ZeroPadding2D):
  â€¢ inputs=tf.Tensor(shape=(3, 14, 14, 576), dtype=float32)

```
```
"
61624,Overflow bug when running tf.raw_ops.ResizeNearestNeighbor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large list element

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  images_tensor = tf.constant(-256, shape=[16, 4, 5, 1], dtype=tf.float16,)
  images = tf.identity(images_tensor)
  size_0 = 1250999896764
  size_1 = 1610637938
  size = [size_0,size_1,]
  align_corners = False
  half_pixel_centers = False
  name = None
  out = tf.raw_ops.ResizeNearestNeighbor(images=images,size=size,align_corners=align_corners,half_pixel_centers=half_pixel_centers,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__ResizeNearestNeighbor_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 18630618048 with 1610637938, result: -1 [Op:ResizeNearestNeighbor]

```
```
"
61623,Overflow bug when running tf.raw_ops.Tile,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in input list

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  input_tensor = tf.random.uniform([4, 1, 1, 20], dtype=tf.float32)
  input = tf.identity(input_tensor)
  multiples_0 = 125091515651
  multiples_1 = True
  multiples_2 = 125091515651
  multiples_3 = 125091515651
  multiples = [multiples_0,multiples_1,multiples_2,multiples_3,]
  name = None
  out = tf.raw_ops.Tile(input=input,multiples=multiples,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Tile_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 500366062604 with 125091515651, result: -1
	 [[{{node Tile}}]] [Op:Tile]

```
```
"
61622,Overflow bug when running tf.keras.layers.ZeroPadding2D,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large list element

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  padding_0_0 = 125091515651
  padding_0_1 = False
  padding_0 = [padding_0_0,padding_0_1,]
  padding_1_0 = 125091515651
  padding_1_1 = 125091515651
  padding_1 = [padding_1_0,padding_1_1,]
  padding = [padding_0,padding_1,]
  arg_class = tf.keras.layers.ZeroPadding2D(padding=padding,)
  arg_input_0_tensor = tf.random.uniform([3, 300, 300, 192], dtype=tf.float32)
  arg_input_0 = tf.identity(arg_input_0_tensor)
  arg_input = [arg_input_0,]
  out = arg_class(*arg_input)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
['{{function_node __wrapped__Pad_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 2501999793529 with 2501999793530, result: -1\n', '\t [[{{node Pad}}]] [Op:Pad]\n', '\n', ""Call arguments received by layer 'zero_padding2d' (type ZeroPadding2D):\n"", ' â€¢ inputs=tf.Tensor(shape=(1, 1, 2, 2), dtype=float32)\n']
```
```
"
61621,Overflow bug when running tf.keras.layers.ZeroPadding3D,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large integer value

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  padding = 1610612736
  arg_class = tf.keras.layers.ZeroPadding3D(padding=padding,)
  arg_input_0_tensor = tf.random.uniform([1, 1, 2, 2, 3], dtype=tf.float32)
  arg_input_0 = tf.identity(arg_input_0_tensor)
  arg_input = [arg_input_0,]
  out = arg_class(*arg_input)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
Error:Exception encountered when calling layer 'zero_padding3d' (type ZeroPadding3D).

{{function_node __wrapped__Pad_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 3221225473 with 3221225474, result: -8070450522584252414 [Op:Pad]

Call arguments received by layer 'zero_padding3d' (type ZeroPadding3D):
  â€¢ inputs=tf.Tensor(shape=(1, 1, 2, 2, 3), dtype=float32)

```
```
"
61620,Overflow bug when running tf.linalg.diag on colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large integer list element

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  diagonal_0_0_0_0 = 1111
  diagonal_0_0_0_1 = 1112
  diagonal_0_0_0 = [diagonal_0_0_0_0,diagonal_0_0_0_1,]
  diagonal_0_0_1_0 = 1121
  diagonal_0_0_1_1 = 1122
  diagonal_0_0_1 = [diagonal_0_0_1_0,diagonal_0_0_1_1,]
  diagonal_0_0 = [diagonal_0_0_0,diagonal_0_0_1,]
  diagonal_0_1_0_0 = 1211
  diagonal_0_1_0_1 = 1212
  diagonal_0_1_0 = [diagonal_0_1_0_0,diagonal_0_1_0_1,]
  diagonal_0_1_1_0 = 1221
  diagonal_0_1_1_1 = 1222
  diagonal_0_1_1 = [diagonal_0_1_1_0,diagonal_0_1_1_1,]
  diagonal_0_1 = [diagonal_0_1_0,diagonal_0_1_1,]
  diagonal_0 = [diagonal_0_0,diagonal_0_1,]
  diagonal_1_0_0_0 = 2111
  diagonal_1_0_0_1 = 2112
  diagonal_1_0_0 = [diagonal_1_0_0_0,diagonal_1_0_0_1,]
  diagonal_1_0_1_0 = 2121
  diagonal_1_0_1_1 = 2122
  diagonal_1_0_1 = [diagonal_1_0_1_0,diagonal_1_0_1_1,]
  diagonal_1_0 = [diagonal_1_0_0,diagonal_1_0_1,]
  diagonal_1_1_0_0 = 2211
  diagonal_1_1_0_1 = 2212
  diagonal_1_1_0 = [diagonal_1_1_0_0,diagonal_1_1_0_1,]
  diagonal_1_1_1_0 = 2221
  diagonal_1_1_1_1 = 2222
  diagonal_1_1_1 = [diagonal_1_1_1_0,diagonal_1_1_1_1,]
  diagonal_1_1 = [diagonal_1_1_0,diagonal_1_1_1,]
  diagonal_1 = [diagonal_1_0,diagonal_1_1,]
  diagonal = [diagonal_0,diagonal_1,]
  name = ""None""
  k = -92233720368
  padding_value = 0
  align = ""RIGHT_LEFT""
  out = tf.linalg.diag(diagonal=diagonal,name=name,k=k,padding_value=padding_value,align=align,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__MatrixDiagV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Encountered overflow when multiplying 16315257232 with 2039407154, result: -1 [Op:MatrixDiagV3]
```
```
"
61618,Colab session crashes for unknown reasons when when running tf.raw_ops.ResizeBilinear on colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large list element

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  images_tensor = tf.random.uniform([1, 5, 5, 1], minval=-256, maxval=257, dtype=tf.int32)
  images = tf.identity(images_tensor)
  size_0 = 125091515651
  size_1 = True
  size = [size_0,size_1,]
  align_corners = False
  half_pixel_centers = False
  name = None
  out = tf.raw_ops.ResizeBilinear(images=images,size=size,align_corners=align_corners,half_pixel_centers=half_pixel_centers,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.592 NotebookApp] Searching ['/root/.jupyter', '/root/.local/etc/jupyter', '/usr/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files"",""time"":""2023-08-17T21:55:14.593Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.592 NotebookApp] Looking for jupyter_config in /etc/jupyter"",""time"":""2023-08-17T21:55:14.598Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.594 NotebookApp] Looking for jupyter_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T21:55:14.600Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.601 NotebookApp] Looking for jupyter_config in /usr/etc/jupyter"",""time"":""2023-08-17T21:55:14.603Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.602 NotebookApp] Looking for jupyter_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T21:55:14.603Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.602 NotebookApp] Looking for jupyter_config in /root/.jupyter"",""time"":""2023-08-17T21:55:14.604Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.604 NotebookApp] Looking for jupyter_notebook_config in /etc/jupyter"",""time"":""2023-08-17T21:55:14.605Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.604 NotebookApp] Loaded config file: /etc/jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T21:55:14.608Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.605 NotebookApp] Looking for jupyter_notebook_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T21:55:14.608Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.597 NotebookApp] Searching ['/root/.jupyter', '/root/.local/etc/jupyter', '/usr/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files"",""time"":""2023-08-17T21:55:14.598Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.609 NotebookApp] Loaded config file: /usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:14.610Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.610 NotebookApp] Looking for jupyter_notebook_config in /usr/etc/jupyter"",""time"":""2023-08-17T21:55:14.611Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.610 NotebookApp] Looking for jupyter_notebook_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T21:55:14.611Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.610 NotebookApp] Looking for jupyter_notebook_config in /root/.jupyter"",""time"":""2023-08-17T21:55:14.611Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.612 NotebookApp] Loaded config file: /root/.jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T21:55:14.613Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.597 NotebookApp] Looking for jupyter_config in /etc/jupyter"",""time"":""2023-08-17T21:55:14.599Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.597 NotebookApp] Looking for jupyter_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T21:55:14.601Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.598 NotebookApp] Looking for jupyter_config in /usr/etc/jupyter"",""time"":""2023-08-17T21:55:14.601Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.598 NotebookApp] Looking for jupyter_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T21:55:14.602Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.598 NotebookApp] Looking for jupyter_config in /root/.jupyter"",""time"":""2023-08-17T21:55:14.602Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.599 NotebookApp] Looking for jupyter_notebook_config in /etc/jupyter"",""time"":""2023-08-17T21:55:14.610Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.600 NotebookApp] Loaded config file: /etc/jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T21:55:14.612Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.601 NotebookApp] Looking for jupyter_notebook_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T21:55:14.612Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.603 NotebookApp] Loaded config file: /usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:14.612Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.603 NotebookApp] Looking for jupyter_notebook_config in /usr/etc/jupyter"",""time"":""2023-08-17T21:55:14.613Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.603 NotebookApp] Looking for jupyter_notebook_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T21:55:14.614Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.603 NotebookApp] Looking for jupyter_notebook_config in /root/.jupyter"",""time"":""2023-08-17T21:55:14.614Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.606 NotebookApp] Loaded config file: /root/.jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T21:55:14.614Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.051Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json"",""time"":""2023-08-17T21:55:15.056Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.056Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.059Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.060Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.061Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret"",""time"":""2023-08-17T21:55:15.074Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Authentication of /metrics is OFF, since other authentication is disabled."",""time"":""2023-08-17T21:55:15.076Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""google.colab serverextension initialized."",""time"":""2023-08-17T21:55:15.159Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.173Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json"",""time"":""2023-08-17T21:55:15.175Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.177Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.179Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.180Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.183Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret"",""time"":""2023-08-17T21:55:15.192Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Authentication of /metrics is OFF, since other authentication is disabled."",""time"":""2023-08-17T21:55:15.193Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""google.colab serverextension initialized."",""time"":""2023-08-17T21:55:15.213Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Serving notebooks from local directory: /"",""time"":""2023-08-17T21:55:19.313Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Serving notebooks from local directory: /"",""time"":""2023-08-17T21:55:19.314Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Jupyter Notebook 6.5.5 is running at:"",""time"":""2023-08-17T21:55:19.314Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""http://172.28.0.2:9000/"",""time"":""2023-08-17T21:55:19.314Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Use Control-C to stop this server and shut down all kernels (twice to skip confirmation)."",""time"":""2023-08-17T21:55:19.315Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Jupyter Notebook 6.5.5 is running at:"",""time"":""2023-08-17T21:55:19.314Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""http://172.28.0.12:9000/"",""time"":""2023-08-17T21:55:19.316Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Use Control-C to stop this server and shut down all kernels (twice to skip confirmation)."",""time"":""2023-08-17T21:55:19.316Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Kernel started: 92d4365c-be07-4243-a024-4094c7317470, name: python3"",""time"":""2023-08-17T21:55:37.205Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Got events for closed stream <zmq.eventloop.zmqstream.ZMQStream object at 0x79d03bf475b0>"",""time"":""2023-08-17T21:55:52.871Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 21:55:56.928312: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T21:55:56.928Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T21:55:56.928Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 21:55:58.601992: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T21:55:58.602Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 21:56:04.276761: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at tile_ops.cc:193 : INVALID_ARGUMENT: Encountered overflow when multiplying 500366062604 with 125091515651, result: -1"",""time"":""2023-08-17T21:56:04.276Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Task exception was never retrieved"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""future: <Task finished name='Task-35' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /usr/local/lib/python3.10/dist-packages/tornado/websocket.py:1085> exception=WebSocketClosedError()>"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Traceback (most recent call last):"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""  File \""/usr/local/lib/python3.10/dist-packages/tornado/websocket.py\"", line 1087, in wrapper"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    await fut"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""tornado.iostream.StreamClosedError: Stream is closed"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""During handling of the above exception, another exception occurred:"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Traceback (most recent call last):"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""  File \""/usr/lib/python3.10/asyncio/tasks.py\"", line 232, in __step"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    result = coro.send(None)"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""  File \""/usr/local/lib/python3.10/dist-packages/tornado/websocket.py\"", line 1089, in wrapper"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    raise WebSocketClosedError()"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""tornado.websocket.WebSocketClosedError"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Got events for closed stream <zmq.eventloop.zmqstream.ZMQStream object at 0x79d03bffc0d0>"",""time"":""2023-08-17T23:11:18.034Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:17:17.631931: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19327655268 exceeds 10% of free system memory."",""time"":""2023-08-17T23:17:17.634Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T23:17:28.209Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 92d4365c-be07-4243-a024-4094c7317470 restarted"",""time"":""2023-08-17T23:17:28.209Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:19:54.827450: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T23:19:54.827Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T23:19:54.827Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:19:58.360271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T23:19:58.360Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:20:03.457377: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 15342764032 exceeds 10% of free system memory."",""time"":""2023-08-17T23:20:03.457Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T23:20:40.231Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 92d4365c-be07-4243-a024-4094c7317470 restarted"",""time"":""2023-08-17T23:20:40.234Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:25:19.461222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T23:25:19.461Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T23:25:19.461Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:25:21.643520: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T23:25:21.643Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:25:25.628780: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2149856268 exceeds 10% of free system memory."",""time"":""2023-08-17T23:25:25.629Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T23:25:46.242Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 92d4365c-be07-4243-a024-4094c7317470 restarted"",""time"":""2023-08-17T23:25:46.242Z"",""v"":0}
```
```
"
61617,Overflow bug when running tf.clip_by_value on colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large tensor 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  arg_0_tensor = tf.random.uniform([2, 472, 496, 4, 1, 1024], dtype=tf.float64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = 0
  arg_2 = False
  out = tf.clip_by_value(arg_0,arg_1,arg_2,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.592 NotebookApp] Searching ['/root/.jupyter', '/root/.local/etc/jupyter', '/usr/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files"",""time"":""2023-08-17T21:55:14.593Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.592 NotebookApp] Looking for jupyter_config in /etc/jupyter"",""time"":""2023-08-17T21:55:14.598Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.594 NotebookApp] Looking for jupyter_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T21:55:14.600Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.601 NotebookApp] Looking for jupyter_config in /usr/etc/jupyter"",""time"":""2023-08-17T21:55:14.603Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.602 NotebookApp] Looking for jupyter_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T21:55:14.603Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.602 NotebookApp] Looking for jupyter_config in /root/.jupyter"",""time"":""2023-08-17T21:55:14.604Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.604 NotebookApp] Looking for jupyter_notebook_config in /etc/jupyter"",""time"":""2023-08-17T21:55:14.605Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.604 NotebookApp] Loaded config file: /etc/jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T21:55:14.608Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.605 NotebookApp] Looking for jupyter_notebook_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T21:55:14.608Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.597 NotebookApp] Searching ['/root/.jupyter', '/root/.local/etc/jupyter', '/usr/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files"",""time"":""2023-08-17T21:55:14.598Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.609 NotebookApp] Loaded config file: /usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:14.610Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.610 NotebookApp] Looking for jupyter_notebook_config in /usr/etc/jupyter"",""time"":""2023-08-17T21:55:14.611Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.610 NotebookApp] Looking for jupyter_notebook_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T21:55:14.611Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.610 NotebookApp] Looking for jupyter_notebook_config in /root/.jupyter"",""time"":""2023-08-17T21:55:14.611Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.612 NotebookApp] Loaded config file: /root/.jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T21:55:14.613Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.597 NotebookApp] Looking for jupyter_config in /etc/jupyter"",""time"":""2023-08-17T21:55:14.599Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.597 NotebookApp] Looking for jupyter_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T21:55:14.601Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.598 NotebookApp] Looking for jupyter_config in /usr/etc/jupyter"",""time"":""2023-08-17T21:55:14.601Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.598 NotebookApp] Looking for jupyter_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T21:55:14.602Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.598 NotebookApp] Looking for jupyter_config in /root/.jupyter"",""time"":""2023-08-17T21:55:14.602Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.599 NotebookApp] Looking for jupyter_notebook_config in /etc/jupyter"",""time"":""2023-08-17T21:55:14.610Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.600 NotebookApp] Loaded config file: /etc/jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T21:55:14.612Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.601 NotebookApp] Looking for jupyter_notebook_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T21:55:14.612Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.603 NotebookApp] Loaded config file: /usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:14.612Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.603 NotebookApp] Looking for jupyter_notebook_config in /usr/etc/jupyter"",""time"":""2023-08-17T21:55:14.613Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.603 NotebookApp] Looking for jupyter_notebook_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T21:55:14.614Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.603 NotebookApp] Looking for jupyter_notebook_config in /root/.jupyter"",""time"":""2023-08-17T21:55:14.614Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.606 NotebookApp] Loaded config file: /root/.jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T21:55:14.614Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.051Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json"",""time"":""2023-08-17T21:55:15.056Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.056Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.059Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.060Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.061Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret"",""time"":""2023-08-17T21:55:15.074Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Authentication of /metrics is OFF, since other authentication is disabled."",""time"":""2023-08-17T21:55:15.076Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""google.colab serverextension initialized."",""time"":""2023-08-17T21:55:15.159Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.173Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json"",""time"":""2023-08-17T21:55:15.175Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.177Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.179Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.180Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.183Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret"",""time"":""2023-08-17T21:55:15.192Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Authentication of /metrics is OFF, since other authentication is disabled."",""time"":""2023-08-17T21:55:15.193Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""google.colab serverextension initialized."",""time"":""2023-08-17T21:55:15.213Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Serving notebooks from local directory: /"",""time"":""2023-08-17T21:55:19.313Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Serving notebooks from local directory: /"",""time"":""2023-08-17T21:55:19.314Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Jupyter Notebook 6.5.5 is running at:"",""time"":""2023-08-17T21:55:19.314Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""http://172.28.0.2:9000/"",""time"":""2023-08-17T21:55:19.314Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Use Control-C to stop this server and shut down all kernels (twice to skip confirmation)."",""time"":""2023-08-17T21:55:19.315Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Jupyter Notebook 6.5.5 is running at:"",""time"":""2023-08-17T21:55:19.314Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""http://172.28.0.12:9000/"",""time"":""2023-08-17T21:55:19.316Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Use Control-C to stop this server and shut down all kernels (twice to skip confirmation)."",""time"":""2023-08-17T21:55:19.316Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Kernel started: 92d4365c-be07-4243-a024-4094c7317470, name: python3"",""time"":""2023-08-17T21:55:37.205Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Got events for closed stream <zmq.eventloop.zmqstream.ZMQStream object at 0x79d03bf475b0>"",""time"":""2023-08-17T21:55:52.871Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 21:55:56.928312: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T21:55:56.928Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T21:55:56.928Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 21:55:58.601992: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T21:55:58.602Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 21:56:04.276761: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at tile_ops.cc:193 : INVALID_ARGUMENT: Encountered overflow when multiplying 500366062604 with 125091515651, result: -1"",""time"":""2023-08-17T21:56:04.276Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Task exception was never retrieved"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""future: <Task finished name='Task-35' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /usr/local/lib/python3.10/dist-packages/tornado/websocket.py:1085> exception=WebSocketClosedError()>"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Traceback (most recent call last):"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""  File \""/usr/local/lib/python3.10/dist-packages/tornado/websocket.py\"", line 1087, in wrapper"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    await fut"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""tornado.iostream.StreamClosedError: Stream is closed"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""During handling of the above exception, another exception occurred:"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Traceback (most recent call last):"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""  File \""/usr/lib/python3.10/asyncio/tasks.py\"", line 232, in __step"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    result = coro.send(None)"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""  File \""/usr/local/lib/python3.10/dist-packages/tornado/websocket.py\"", line 1089, in wrapper"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    raise WebSocketClosedError()"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""tornado.websocket.WebSocketClosedError"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Got events for closed stream <zmq.eventloop.zmqstream.ZMQStream object at 0x79d03bffc0d0>"",""time"":""2023-08-17T23:11:18.034Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:17:17.631931: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19327655268 exceeds 10% of free system memory."",""time"":""2023-08-17T23:17:17.634Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T23:17:28.209Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 92d4365c-be07-4243-a024-4094c7317470 restarted"",""time"":""2023-08-17T23:17:28.209Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:19:54.827450: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T23:19:54.827Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T23:19:54.827Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:19:58.360271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T23:19:58.360Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:20:03.457377: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 15342764032 exceeds 10% of free system memory."",""time"":""2023-08-17T23:20:03.457Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T23:20:40.231Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 92d4365c-be07-4243-a024-4094c7317470 restarted"",""time"":""2023-08-17T23:20:40.234Z"",""v"":0}
```
```
"
61616, Test_on_batch() gives the same loss output on different batches in a single run,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes


### TensorFlow version

tf 2.10.0

### Custom code

Yes

### OS platform and distribution

windows 10

### Python version

3.10.12

### Current behavior?

I noticed the problem when I got a straight horizontal line on plotting the test results on the trained network. I used the sequential models.

I use train_on_batch(), which gives converging losses. When I switch to test_on_batch(), the losses remain the same for different batches. When I restart the test with different test files, it will give a different loss value, which remains the same for all the batches. In other words, the loss from test_on_batch() remains the some for all batches in a single run.

It's a sequential model.

Here is the code of the section:

    print('mfccs3 value = ', tf.keras.backend.eval(mfccs3[1,:]) )               
    #logs = vadModel.train_on_batch(mfccs3, vadLabel)
    logs = vadModel.test_on_batch(mfccs3, vadLabel) 
    print('string logs = ', str(logs))
The result is:
index = 1

```
mfccs3 value = [[-8.2793800e+01 -5.9538417e+00 9.8302096e-01 -3.5255635e-01
3.0392697e-01 -6.4597696e-01 2.2358397e-02 2.5344249e-02
-6.8171650e-01 -3.7053981e-01 -3.4044239e-01 -8.1056818e-02]]
```

string logs = 0.2398043

index = 2

```
mfccs3 value = [[-69.159195 -2.2269542 4.2501264 -1.3486748 0.62957734
-3.2606528 -3.253118 -3.5308673 -1.1313365 -1.1839466
-2.330786 -1.6313086 ]]
```

string logs = 0.2398043

index = 3

```
mfccs3 value = [[-64.894104 -1.892648 0.11392474 -0.81098145 -1.4640433
-1.1901256 -1.7744782 -0.85753983 -0.9694403 -0.8149232
-1.0680746 -1.0442001 ]]
```

string logs = 0.2398043

You can see that the inputs for test_on_batch() have changed. However, the loss remains the same. I use the same code for train_on_batch(), which gives converging losses.



### Standalone code to reproduce the issue

```shell
logs = vadModel.train_on_batch(mfccs3, vadLabel)
"""""" vs.  """"""
    logs = vadModel.test_on_batch(mfccs3, vadLabel) 

it's just these two lines for a sequential model.
```


### Relevant log output

```shell
I even tried latest Tensorflow version. It has the same problem.

tensorflow version: 2.15.0-dev20230817
listOfFiles 1681
2023-08-17 15:42:12.560549: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
model length =  7
Model: ""sequential""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 multiple                  2640      
                                                                 
 dense (Dense)               multiple                  210       
                                                                 
 dense_1 (Dense)             multiple                  11        
                                                                 
=================================================================
Total params: 2861 (11.18 KB)
Trainable params: 2861 (11.18 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
```
"
61615,Overflow bug when running tf.raw_ops.Pad,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to the large list of elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  input_tensor = tf.random.uniform([16, 16, 16, 512], dtype=tf.float32)
  input = tf.identity(input_tensor)
  paddings_0_0 = 125091515651
  paddings_0_1 = 125091515651
  paddings_0 = [paddings_0_0,paddings_0_1,]
  paddings_1_0 = 125091515651
  paddings_1_1 = False
  paddings_1 = [paddings_1_0,paddings_1_1,]
  paddings_2_0 = 125091515651
  paddings_2_1 = 125091515651
  paddings_2 = [paddings_2_0,paddings_2_1,]
  paddings_3_0 = 125091515651
  paddings_3_1 = 125091515651
  paddings_3 = [paddings_3_0,paddings_3_1,]
  paddings = [paddings_0,paddings_1,paddings_2,paddings_3,]
  name = None
  out = tf.raw_ops.Pad(input=input,paddings=paddings,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Pad_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 250183031318 with 125091515667, result: -1
	 [[{{node Pad}}]] [Op:Pad]

```
```
"
61614,Overflow when running tf.raw_ops.PadV2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to the large list of element

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  input_tensor = tf.random.uniform([16, 4, 4, 512], dtype=tf.float32)
  input = tf.identity(input_tensor)
  paddings_0_0 = 125091515651
  paddings_0_1 = 125091515651
  paddings_0 = [paddings_0_0,paddings_0_1,]
  paddings_1_0 = 125091515651
  paddings_1_1 = 125091515651
  paddings_1 = [paddings_1_0,paddings_1_1,]
  paddings_2_0 = 125091515651
  paddings_2_1 = 125091515651
  paddings_2 = [paddings_2_0,paddings_2_1,]
  paddings_3_0 = 125091515651
  paddings_3_1 = 125091515651
  paddings_3 = [paddings_3_0,paddings_3_1,]
  paddings = [paddings_0,paddings_1,paddings_2,paddings_3,]
  constant_values = 0
  name = None
  out = tf.raw_ops.PadV2(input=input,paddings=paddings,constant_values=constant_values,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__PadV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 250183031318 with 250183031306, result: -1
	 [[{{node PadV2}}]] [Op:PadV2]

```
```
"
61613,Overflow bug when running tf.tile,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large list elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  arg_0_tensor = tf.random.uniform([452, 1, 768], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 125091515651
  arg_1_1 = 125091515651
  arg_1_2 = 125091515651
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,]
  out = tf.tile(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Tile_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 56541365074252 with 125091515651, result: -1
	 [[{{node Tile}}]] [Op:Tile]

```
```
"
61612,Overflow when running tf.image.pad_to_bounding_box on colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large list element

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  arg_0_0_0_0 = 1.0
  arg_0_0_0_1 = 2.0
  arg_0_0_0_2 = 3.0
  arg_0_0_0 = [arg_0_0_0_0,arg_0_0_0_1,arg_0_0_0_2,]
  arg_0_0_1_0 = 4.0
  arg_0_0_1_1 = 5.0
  arg_0_0_1_2 = 6.0
  arg_0_0_1 = [arg_0_0_1_0,arg_0_0_1_1,arg_0_0_1_2,]
  arg_0_0 = [arg_0_0_0,arg_0_0_1,]
  arg_0_1_0_0 = 7.0
  arg_0_1_0_1 = 8.0
  arg_0_1_0_2 = 9.0
  arg_0_1_0 = [arg_0_1_0_0,arg_0_1_0_1,arg_0_1_0_2,]
  arg_0_1_1_0 = 10.0
  arg_0_1_1_1 = 11.0
  arg_0_1_1_2 = 12.0
  arg_0_1_1 = [arg_0_1_1_0,arg_0_1_1_1,arg_0_1_1_2,]
  arg_0_1 = [arg_0_1_0,arg_0_1_1,]
  arg_0 = [arg_0_0,arg_0_1,]
  arg_1 = 1
  arg_2 = 1
  arg_3 = 4
  arg_4 = 1676240524292489355
  out = tf.image.pad_to_bounding_box(arg_0,arg_1,arg_2,arg_3,arg_4,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Pad_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 6704962097169957420 with 3, result: -1
	 [[{{node Pad}}]] [Op:Pad] name: 

```
```
"
61611,Overflow bug when running tf.raw_ops.Tile on colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large list elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  input_tensor = tf.random.uniform([4, 1, 1, 20], dtype=tf.float32)
  input = tf.identity(input_tensor)
  multiples_0 = 125091515651
  multiples_1 = True
  multiples_2 = 125091515651
  multiples_3 = 125091515651
  multiples = [multiples_0,multiples_1,multiples_2,multiples_3,]
  name = None
  out = tf.raw_ops.Tile(input=input,multiples=multiples,name=name,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Tile_device_/job:localhost/replica:0/task:0/device:CPU:0}} Encountered overflow when multiplying 500366062604 with 125091515651, result: -1 [Op:Tile]
```
```
"
61610,Overflow when running tf.keras.layers.ZeroPadding2D,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large list elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  padding_0_0 = 125091515651
  padding_0_1 = False
  padding_0 = [padding_0_0,padding_0_1,]
  padding_1_0 = 125091515651
  padding_1_1 = 125091515651
  padding_1 = [padding_1_0,padding_1_1,]
  padding = [padding_0,padding_1,]
  arg_class = tf.keras.layers.ZeroPadding2D(padding=padding,)
  arg_input_0_tensor = tf.random.uniform([3, 300, 300, 192], dtype=tf.float32)
  arg_input_0 = tf.identity(arg_input_0_tensor)
  arg_input = [arg_input_0,]
  out = arg_class(*arg_input)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:Exception encountered when calling layer 'zero_padding2d' (type ZeroPadding2D).

{{function_node __wrapped__Pad_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 375274547853 with 250183031602, result: -1
	 [[{{node Pad}}]] [Op:Pad]

Call arguments received by layer 'zero_padding2d' (type ZeroPadding2D):
  â€¢ inputs=tf.Tensor(shape=(3, 300, 300, 192), dtype=float32)
{}

```
```
"
61609,Overflow bug when running tf.keras.layers.ZeroPadding3D,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to the large integer value

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  padding = 1610612736
  arg_class = tf.keras.layers.ZeroPadding3D(padding=padding,)
  arg_input_0_tensor = tf.random.uniform([1, 1, 2, 2, 3], dtype=tf.float32)
  arg_input_0 = tf.identity(arg_input_0_tensor)
  arg_input = [arg_input_0,]
  out = arg_class(*arg_input)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:Exception encountered when calling layer 'zero_padding3d' (type ZeroPadding3D).

{{function_node __wrapped__Pad_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 3221225473 with 3221225474, result: -8070450522584252414 [Op:Pad]

Call arguments received by layer 'zero_padding3d' (type ZeroPadding3D):
  â€¢ inputs=tf.Tensor(shape=(1, 1, 2, 2, 3), dtype=float32)
{}

```
```
"
61608,Overflow bug when running tf.pad on colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large input tensor

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([12, 2, 256, 513], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([4, 2], dtype=tf.int32, maxval=54676034958255)
      arg_1 = tf.identity(arg_1_tensor)
      out = tf.pad(arg_0,arg_1,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.int32)
      tf.pad(arg_0,arg_1,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Pad_device_/job:localhost/replica:0/task:0/device:CPU:0}} Encountered overflow when multiplying 2206572623628733836 with 75929941, result: -1 [Op:Pad] name: 
Error:{{function_node __wrapped__Pad_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 2206572623628733836 with 75929941, result: -1 [Op:Pad] name
```
```
"
61607,Issue with nightly-gpu docker image,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230816

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using the tensorflow/tensorflow:nightly-gpu docker image I get an error saying the ""DNN library is not found""

However, when I change the base image to tensorflow/tensorflow:latest-gpu my code works fine.

Perhaps the nightly image broke something with the cuda / cudnn library paths?

### Standalone code to reproduce the issue

```shell
It seems that using a Conv1D layer is what causes the issue... see the log output below.
```


### Relevant log output

```shell
Detected at node 'peak_conv_1/Conv1D' defined at (most recent call last):
Node: 'peak_conv_1/Conv1D'
DNN library is not found.
         [[{{node peak_conv_1/Conv1D}}]] [Op:__inference_train_step_224831]
```
"
61606,"Cannot type ""I Accept"" to extract from hexagon_nn_skel","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

hexagon_nn_skel_v1.20.0.1

### Custom code

No

### OS platform and distribution

Android

### Mobile device

Android

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Same as here https://github.com/tensorflow/tensorflow/issues/61378, running the ./tflite_hexagon_nn_skel_v1.20.0.1.run instantly prints:
```
license. Extraction aborted.
./extract.sh[5]: read: -p: no coprocess
Aborting extraction.. Done
```
Without giving time to write ""I ACCEPT"".
Neither does --accept option work.


### Standalone code to reproduce the issue

```shell
adb push tflite_hexagon_nn_skel_v1.20.0.1.run /data/local/tmp
adb shell
cd /data/local/tmp
chmod +x tflite_hexagon_nn_skel_v1.20.0.1.run
./tflite_hexagon_nn_skel_v1.20.0.1.run [--accept]
```


### Relevant log output

```shell
...
Limited or its designated affiliate. LICENSEE shall be solely responsible to
obtain such separate license from Apical Limited. The provision or license of a
PKLA Product Kit to LICENSEE does not convey any license or other right under
any patents of QUALCOMM Incorporated or SnapTrack, Inc.


Type ""I ACCEPT"" if you agree to the terms of the license: You didn't accept the
license. Extraction aborted.
./extract.sh[5]: read: -p: no coprocess
Aborting extraction.. Done
HNNTH:/data/local/tmp $
```
"
61605,Crash when running tf.keras.layers.MaxPool2D on colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in the input list

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  arg_0_0 = 1e+38
  arg_0_1 = 16777216
  arg_0 = [arg_0_0,arg_0_1,]
  strides_0 = 2
  strides_1 = 2
  strides = [strides_0,strides_1,]
  padding = ""same""
  arg_class = tf.keras.layers.MaxPool2D(arg_0,strides=strides,padding=padding,)
  arg_input_0_tensor = tf.random.uniform([3, 74, 74, 256], dtype=tf.float32)
  arg_input_0 = tf.identity(arg_input_0_tensor)
  arg_input = [arg_input_0,]
  out = arg_class(*arg_input)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.837 NotebookApp] Searching ['/root/.jupyter', '/root/.local/etc/jupyter', '/usr/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files"",""time"":""2023-08-17T14:24:43.838Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.838 NotebookApp] Looking for jupyter_config in /etc/jupyter"",""time"":""2023-08-17T14:24:43.846Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.840 NotebookApp] Looking for jupyter_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T14:24:43.847Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.842 NotebookApp] Looking for jupyter_config in /usr/etc/jupyter"",""time"":""2023-08-17T14:24:43.848Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.842 NotebookApp] Looking for jupyter_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T14:24:43.848Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.842 NotebookApp] Looking for jupyter_config in /root/.jupyter"",""time"":""2023-08-17T14:24:43.850Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.843 NotebookApp] Looking for jupyter_notebook_config in /etc/jupyter"",""time"":""2023-08-17T14:24:43.850Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.847 NotebookApp] Loaded config file: /etc/jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T14:24:43.854Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.847 NotebookApp] Looking for jupyter_notebook_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T14:24:43.855Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.849 NotebookApp] Loaded config file: /usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:43.855Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.849 NotebookApp] Looking for jupyter_notebook_config in /usr/etc/jupyter"",""time"":""2023-08-17T14:24:43.862Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.849 NotebookApp] Looking for jupyter_notebook_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T14:24:43.863Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.849 NotebookApp] Looking for jupyter_notebook_config in /root/.jupyter"",""time"":""2023-08-17T14:24:43.864Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.857 NotebookApp] Loaded config file: /root/.jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T14:24:43.865Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.049 NotebookApp] Searching ['/root/.jupyter', '/root/.local/etc/jupyter', '/usr/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files"",""time"":""2023-08-17T14:24:44.050Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /etc/jupyter"",""time"":""2023-08-17T14:24:44.056Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T14:24:44.057Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /usr/etc/jupyter"",""time"":""2023-08-17T14:24:44.057Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T14:24:44.057Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /root/.jupyter"",""time"":""2023-08-17T14:24:44.057Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.059 NotebookApp] Looking for jupyter_notebook_config in /etc/jupyter"",""time"":""2023-08-17T14:24:44.065Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.059 NotebookApp] Loaded config file: /etc/jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T14:24:44.066Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Looking for jupyter_notebook_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T14:24:44.066Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Loaded config file: /usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.066Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Looking for jupyter_notebook_config in /usr/etc/jupyter"",""time"":""2023-08-17T14:24:44.066Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Looking for jupyter_notebook_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T14:24:44.067Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Looking for jupyter_notebook_config in /root/.jupyter"",""time"":""2023-08-17T14:24:44.067Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.061 NotebookApp] Loaded config file: /root/.jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T14:24:44.067Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.713Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json"",""time"":""2023-08-17T14:24:44.715Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.715Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.717Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.719Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.723Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret"",""time"":""2023-08-17T14:24:44.743Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Authentication of /metrics is OFF, since other authentication is disabled."",""time"":""2023-08-17T14:24:44.745Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""google.colab serverextension initialized."",""time"":""2023-08-17T14:24:44.800Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.806Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json"",""time"":""2023-08-17T14:24:44.808Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.808Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.809Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.810Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.811Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret"",""time"":""2023-08-17T14:24:44.831Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Authentication of /metrics is OFF, since other authentication is disabled."",""time"":""2023-08-17T14:24:44.831Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""google.colab serverextension initialized."",""time"":""2023-08-17T14:24:44.856Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Serving notebooks from local directory: /"",""time"":""2023-08-17T14:24:49.405Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Jupyter Notebook 6.4.8 is running at:"",""time"":""2023-08-17T14:24:49.405Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""http://172.28.0.12:9000/"",""time"":""2023-08-17T14:24:49.406Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Use Control-C to stop this server and shut down all kernels (twice to skip confirmation)."",""time"":""2023-08-17T14:24:49.406Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Serving notebooks from local directory: /"",""time"":""2023-08-17T14:24:49.430Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Jupyter Notebook 6.4.8 is running at:"",""time"":""2023-08-17T14:24:49.431Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""http://172.28.0.2:9000/"",""time"":""2023-08-17T14:24:49.432Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Use Control-C to stop this server and shut down all kernels (twice to skip confirmation)."",""time"":""2023-08-17T14:24:49.432Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Kernel started: 5b91d383-2a5d-4d17-8f8d-5f59277ecb11, name: python3"",""time"":""2023-08-17T14:26:06.571Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:29.531813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:26:29.531Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:26:29.532Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:32.617224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:26:32.617Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.113889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.114Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.787982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.788Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.788362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.788Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.803571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.803Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.803974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.804Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.804329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.804Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000771: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13664 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.077428: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6093750902 exceeds 10% of free system memory."",""time"":""2023-08-17T14:26:40.077Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:42.860963: F tensorflow/core/framework/tensor_shape.cc:587] Check failed: size >= 0 (0 vs. -1248091845)"",""time"":""2023-08-17T14:26:42.861Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:26:45.571Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:26:45.573Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:36.094812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:27:36.100Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:27:36.101Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:37.142240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:27:37.142Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.764145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.764Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.816688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.816Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.826117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.828Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.831423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.831Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.832476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.832Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.833383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.833Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.665264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:43.665Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.667057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:43.667Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.668052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:43.668Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.668828: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:27:43.668Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.669432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13664 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:27:43.669Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.701398: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6093750902 exceeds 10% of free system memory."",""time"":""2023-08-17T14:27:43.701Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:48.308263: F tensorflow/core/framework/tensor_shape.cc:587] Check failed: size >= 0 (0 vs. -1248091845)"",""time"":""2023-08-17T14:27:48.308Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:27:51.576Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:27:51.576Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:09.645133: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:31:09.645Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:31:09.646Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:10.714033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:31:10.714Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.844776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.844Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.892144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.892Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.894009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.894Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.896074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.896Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.897012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.897Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.897897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.897Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.331805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:14.332Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.332225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:14.333Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.332548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:14.333Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.332751: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:31:14.333Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.332800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13664 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:31:14.333Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:18.149537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900"",""time"":""2023-08-17T14:31:18.149Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:18.151181: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:959] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)"",""time"":""2023-08-17T14:31:18.151Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:31:18.580Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:31:18.581Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:32:58.023248: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:32:58.024Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:32:58.024Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:33:00.925573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:33:00.925Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.817441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.817Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.920819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.920Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.921484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.921Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.923040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.923Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.923548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.923Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.924029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.924Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.323413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:24.323Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.324064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:24.324Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.324360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:24.324Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.324522: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:34:24.325Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.324601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13664 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:34:24.325Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:29.102979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900"",""time"":""2023-08-17T14:34:29.103Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:29.104423: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:959] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)"",""time"":""2023-08-17T14:34:29.104Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:34:30.587Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:34:30.587Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:39.499788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:34:39.500Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:34:39.500Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:41.797362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:34:41.797Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Kernel restarted: 5b91d383-2a5d-4d17-8f8d-5f59277ecb11"",""time"":""2023-08-17T14:34:51.616Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:59.388548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:34:59.392Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:34:59.392Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:35:01.250311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:35:01.250Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.143899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.144Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.181829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.182Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.182572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.182Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.184389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.184Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.184737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.185Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.185055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.185Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.232416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:07.232Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.232868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:07.233Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.233157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:07.233Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.233367: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:36:07.233Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.233419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13692 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:36:07.233Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.261994: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at matrix_diag_op.cc:273 : INVALID_ARGUMENT: Encountered overflow when multiplying 12884901888 with 1610612736, result: -1"",""time"":""2023-08-17T14:36:07.262Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:57.056526: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6093750902 exceeds 10% of free system memory."",""time"":""2023-08-17T14:36:57.056Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:37:03.543980: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at matrix_diag_op.cc:273 : INVALID_ARGUMENT: Encountered overflow when multiplying 3046875451 with 3046875451, result: -9163294059803098215"",""time"":""2023-08-17T14:37:03.544Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:37:04.268280: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6093750784 exceeds 10% of free system memory."",""time"":""2023-08-17T14:37:04.268Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:37:08.172313: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at matrix_diag_op.cc:273 : INVALID_ARGUMENT: Encountered overflow when multiplying 3046875392 with 3046875392, result: -9163294419334397952"",""time"":""2023-08-17T14:37:08.172Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:38:16.953443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8900"",""time"":""2023-08-17T14:38:16.953Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:38:16.953795: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:983] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)"",""time"":""2023-08-17T14:38:16.954Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:38:18.617Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:38:18.617Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:41:46.795637: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:41:46.795Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:41:46.795Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:41:47.897333: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:41:47.897Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:41:50.174067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:41:50.174Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:41:50.217161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:41:50.217Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:41:50.217535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:41:50.217Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:15.973272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:53:15.973Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:15.977064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:53:15.977Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:15.977438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:53:15.977Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:17.404160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:53:17.404Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:17.405355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:53:17.405Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:17.406091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:53:17.406Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:17.406821: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:53:17.406Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:17.407403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13692 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:53:17.407Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:17.447492: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at matrix_diag_op.cc:273 : INVALID_ARGUMENT: Encountered overflow when multiplying 9984734776 with 1248091847, result: -5984878005326580344"",""time"":""2023-08-17T14:53:17.447Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:56:00.832626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8900"",""time"":""2023-08-17T14:56:00.832Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:56:00.832804: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:983] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)"",""time"":""2023-08-17T14:56:00.833Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:56:03.623Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:56:03.623Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:16:21.348874: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T15:16:21.349Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T15:16:21.349Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:16:22.465628: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T15:16:22.465Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:16:24.726483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T15:16:24.726Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:16:24.772576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T15:16:24.772Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:16:24.772996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T15:16:24.773Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:16:24.774456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T15:16:24.774Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:16:24.774875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T15:16:24.775Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:16:24.775178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T15:16:24.775Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:16:26.155580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T15:16:26.155Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:16:26.156925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T15:16:26.157Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:16:26.157283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T15:16:26.157Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:16:26.157467: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T15:16:26.157Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:16:26.157517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13692 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T15:16:26.158Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:16:26.192012: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at matrix_diag_op.cc:273 : INVALID_ARGUMENT: Encountered overflow when multiplying 12885103520 with 1610637940, result: -1"",""time"":""2023-08-17T15:16:26.192Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:18:07.005069: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at matrix_diag_op.cc:273 : INVALID_ARGUMENT: Encountered overflow when multiplying 12885103520 with 1610637940, result: -1"",""time"":""2023-08-17T15:18:07.005Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:19:44.626474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8900"",""time"":""2023-08-17T15:19:44.626Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 15:19:44.626614: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:983] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)"",""time"":""2023-08-17T15:19:44.627Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T15:19:45.628Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T15:19:45.629Z"",""v"":0}
```
```
"
61604,Integer overflow when running tf.compat.v1.matrix_diag on colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in the input lists

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  diagonal_0_0_0_0 = 1111
  diagonal_0_0_0_1 = 1112
  diagonal_0_0_0 = [diagonal_0_0_0_0,diagonal_0_0_0_1,]
  diagonal_0_0_1_0 = 1121
  diagonal_0_0_1_1 = 1122
  diagonal_0_0_1 = [diagonal_0_0_1_0,diagonal_0_0_1_1,]
  diagonal_0_0 = [diagonal_0_0_0,diagonal_0_0_1,]
  diagonal_0_1_0_0 = 1211
  diagonal_0_1_0_1 = 1212
  diagonal_0_1_0 = [diagonal_0_1_0_0,diagonal_0_1_0_1,]
  diagonal_0_1_1_0 = 1221
  diagonal_0_1_1_1 = 1222
  diagonal_0_1_1 = [diagonal_0_1_1_0,diagonal_0_1_1_1,]
  diagonal_0_1 = [diagonal_0_1_0,diagonal_0_1_1,]
  diagonal_0 = [diagonal_0_0,diagonal_0_1,]
  diagonal_1_0_0_0 = 2111
  diagonal_1_0_0_1 = 2112
  diagonal_1_0_0 = [diagonal_1_0_0_0,diagonal_1_0_0_1,]
  diagonal_1_0_1_0 = 2121
  diagonal_1_0_1_1 = 2122
  diagonal_1_0_1 = [diagonal_1_0_1_0,diagonal_1_0_1_1,]
  diagonal_1_0 = [diagonal_1_0_0,diagonal_1_0_1,]
  diagonal_1_1_0_0 = 2211
  diagonal_1_1_0_1 = 2212
  diagonal_1_1_0 = [diagonal_1_1_0_0,diagonal_1_1_0_1,]
  diagonal_1_1_1_0 = 2221
  diagonal_1_1_1_1 = 2222
  diagonal_1_1_1 = [diagonal_1_1_1_0,diagonal_1_1_1_1,]
  diagonal_1_1 = [diagonal_1_1_0,diagonal_1_1_1,]
  diagonal_1 = [diagonal_1_0,diagonal_1_1,]
  diagonal = [diagonal_0,diagonal_1,]
  name = ""None""
  k = 1610637938
  padding_value = 0
  align = ""RIGHT_LEFT""
  out = tf.compat.v1.matrix_diag(diagonal=diagonal,name=name,k=k,padding_value=padding_value,align=align,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__MatrixDiagV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Encountered overflow when multiplying 12885103520 with 1610637940, result: -1 [Op:MatrixDiagV3]
{}

```
```
"
61603,Crash when running tf.compat.v1.keras.layers.MaxPool2D on colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in input lists

### Standalone code to reproduce the issue

```shell
results = dict()
import tensorflow as tf
import os
import numpy as np
try:
  pool_size_0 = 1e+38
  pool_size_1 = 1048576
  pool_size = [pool_size_0,pool_size_1,]
  strides_0 = 2
  strides_1 = 2
  strides = [strides_0,strides_1,]
  padding = ""same""
  data_format = None
  arg_class = tf.compat.v1.keras.layers.MaxPool2D(pool_size=pool_size,strides=strides,padding=padding,data_format=data_format,)
  arg_input_0_tensor = tf.random.uniform([3, 74, 74, 256], dtype=tf.float32)
  arg_input_0 = tf.identity(arg_input_0_tensor)
  arg_input = [arg_input_0,]
  out = arg_class(*arg_input)
except Exception as e:
  print(""Error:""+str(e))

print(results)
```
```


### Relevant log output

```shell
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.837 NotebookApp] Searching ['/root/.jupyter', '/root/.local/etc/jupyter', '/usr/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files"",""time"":""2023-08-17T14:24:43.838Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.838 NotebookApp] Looking for jupyter_config in /etc/jupyter"",""time"":""2023-08-17T14:24:43.846Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.840 NotebookApp] Looking for jupyter_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T14:24:43.847Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.842 NotebookApp] Looking for jupyter_config in /usr/etc/jupyter"",""time"":""2023-08-17T14:24:43.848Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.842 NotebookApp] Looking for jupyter_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T14:24:43.848Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.842 NotebookApp] Looking for jupyter_config in /root/.jupyter"",""time"":""2023-08-17T14:24:43.850Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.843 NotebookApp] Looking for jupyter_notebook_config in /etc/jupyter"",""time"":""2023-08-17T14:24:43.850Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.847 NotebookApp] Loaded config file: /etc/jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T14:24:43.854Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.847 NotebookApp] Looking for jupyter_notebook_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T14:24:43.855Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.849 NotebookApp] Loaded config file: /usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:43.855Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.849 NotebookApp] Looking for jupyter_notebook_config in /usr/etc/jupyter"",""time"":""2023-08-17T14:24:43.862Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.849 NotebookApp] Looking for jupyter_notebook_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T14:24:43.863Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.849 NotebookApp] Looking for jupyter_notebook_config in /root/.jupyter"",""time"":""2023-08-17T14:24:43.864Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.857 NotebookApp] Loaded config file: /root/.jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T14:24:43.865Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.049 NotebookApp] Searching ['/root/.jupyter', '/root/.local/etc/jupyter', '/usr/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files"",""time"":""2023-08-17T14:24:44.050Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /etc/jupyter"",""time"":""2023-08-17T14:24:44.056Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T14:24:44.057Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /usr/etc/jupyter"",""time"":""2023-08-17T14:24:44.057Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T14:24:44.057Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /root/.jupyter"",""time"":""2023-08-17T14:24:44.057Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.059 NotebookApp] Looking for jupyter_notebook_config in /etc/jupyter"",""time"":""2023-08-17T14:24:44.065Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.059 NotebookApp] Loaded config file: /etc/jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T14:24:44.066Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Looking for jupyter_notebook_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T14:24:44.066Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Loaded config file: /usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.066Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Looking for jupyter_notebook_config in /usr/etc/jupyter"",""time"":""2023-08-17T14:24:44.066Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Looking for jupyter_notebook_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T14:24:44.067Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Looking for jupyter_notebook_config in /root/.jupyter"",""time"":""2023-08-17T14:24:44.067Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.061 NotebookApp] Loaded config file: /root/.jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T14:24:44.067Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.713Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json"",""time"":""2023-08-17T14:24:44.715Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.715Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.717Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.719Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.723Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret"",""time"":""2023-08-17T14:24:44.743Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Authentication of /metrics is OFF, since other authentication is disabled."",""time"":""2023-08-17T14:24:44.745Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""google.colab serverextension initialized."",""time"":""2023-08-17T14:24:44.800Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.806Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json"",""time"":""2023-08-17T14:24:44.808Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.808Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.809Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.810Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.811Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret"",""time"":""2023-08-17T14:24:44.831Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Authentication of /metrics is OFF, since other authentication is disabled."",""time"":""2023-08-17T14:24:44.831Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""google.colab serverextension initialized."",""time"":""2023-08-17T14:24:44.856Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Serving notebooks from local directory: /"",""time"":""2023-08-17T14:24:49.405Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Jupyter Notebook 6.4.8 is running at:"",""time"":""2023-08-17T14:24:49.405Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""http://172.28.0.12:9000/"",""time"":""2023-08-17T14:24:49.406Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Use Control-C to stop this server and shut down all kernels (twice to skip confirmation)."",""time"":""2023-08-17T14:24:49.406Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Serving notebooks from local directory: /"",""time"":""2023-08-17T14:24:49.430Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Jupyter Notebook 6.4.8 is running at:"",""time"":""2023-08-17T14:24:49.431Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""http://172.28.0.2:9000/"",""time"":""2023-08-17T14:24:49.432Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Use Control-C to stop this server and shut down all kernels (twice to skip confirmation)."",""time"":""2023-08-17T14:24:49.432Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Kernel started: 5b91d383-2a5d-4d17-8f8d-5f59277ecb11, name: python3"",""time"":""2023-08-17T14:26:06.571Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:29.531813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:26:29.531Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:26:29.532Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:32.617224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:26:32.617Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.113889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.114Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.787982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.788Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.788362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.788Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.803571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.803Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.803974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.804Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.804329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.804Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000771: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13664 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.077428: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6093750902 exceeds 10% of free system memory."",""time"":""2023-08-17T14:26:40.077Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:42.860963: F tensorflow/core/framework/tensor_shape.cc:587] Check failed: size >= 0 (0 vs. -1248091845)"",""time"":""2023-08-17T14:26:42.861Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:26:45.571Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:26:45.573Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:36.094812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:27:36.100Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:27:36.101Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:37.142240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:27:37.142Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.764145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.764Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.816688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.816Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.826117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.828Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.831423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.831Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.832476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.832Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.833383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.833Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.665264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:43.665Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.667057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:43.667Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.668052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:43.668Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.668828: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:27:43.668Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.669432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13664 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:27:43.669Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.701398: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6093750902 exceeds 10% of free system memory."",""time"":""2023-08-17T14:27:43.701Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:48.308263: F tensorflow/core/framework/tensor_shape.cc:587] Check failed: size >= 0 (0 vs. -1248091845)"",""time"":""2023-08-17T14:27:48.308Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:27:51.576Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:27:51.576Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:09.645133: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:31:09.645Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:31:09.646Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:10.714033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:31:10.714Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.844776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.844Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.892144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.892Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.894009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.894Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.896074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.896Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.897012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.897Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.897897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.897Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.331805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:14.332Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.332225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:14.333Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.332548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:14.333Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.332751: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:31:14.333Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.332800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13664 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:31:14.333Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:18.149537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900"",""time"":""2023-08-17T14:31:18.149Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:18.151181: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:959] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)"",""time"":""2023-08-17T14:31:18.151Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:31:18.580Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:31:18.581Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:32:58.023248: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:32:58.024Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:32:58.024Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:33:00.925573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:33:00.925Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.817441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.817Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.920819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.920Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.921484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.921Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.923040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.923Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.923548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.923Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.924029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.924Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.323413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:24.323Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.324064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:24.324Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.324360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:24.324Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.324522: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:34:24.325Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.324601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13664 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:34:24.325Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:29.102979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900"",""time"":""2023-08-17T14:34:29.103Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:29.104423: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:959] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)"",""time"":""2023-08-17T14:34:29.104Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:34:30.587Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:34:30.587Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:39.499788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:34:39.500Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:34:39.500Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:41.797362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:34:41.797Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Kernel restarted: 5b91d383-2a5d-4d17-8f8d-5f59277ecb11"",""time"":""2023-08-17T14:34:51.616Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:59.388548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:34:59.392Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:34:59.392Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:35:01.250311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:35:01.250Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.143899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.144Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.181829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.182Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.182572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.182Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.184389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.184Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.184737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.185Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.185055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.185Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.232416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:07.232Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.232868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:07.233Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.233157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:07.233Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.233367: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:36:07.233Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.233419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13692 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:36:07.233Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.261994: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at matrix_diag_op.cc:273 : INVALID_ARGUMENT: Encountered overflow when multiplying 12884901888 with 1610612736, result: -1"",""time"":""2023-08-17T14:36:07.262Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:57.056526: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6093750902 exceeds 10% of free system memory."",""time"":""2023-08-17T14:36:57.056Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:37:03.543980: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at matrix_diag_op.cc:273 : INVALID_ARGUMENT: Encountered overflow when multiplying 3046875451 with 3046875451, result: -9163294059803098215"",""time"":""2023-08-17T14:37:03.544Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:37:04.268280: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6093750784 exceeds 10% of free system memory."",""time"":""2023-08-17T14:37:04.268Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:37:08.172313: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at matrix_diag_op.cc:273 : INVALID_ARGUMENT: Encountered overflow when multiplying 3046875392 with 3046875392, result: -9163294419334397952"",""time"":""2023-08-17T14:37:08.172Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:38:16.953443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8900"",""time"":""2023-08-17T14:38:16.953Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:38:16.953795: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:983] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)"",""time"":""2023-08-17T14:38:16.954Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:38:18.617Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:38:18.617Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:41:46.795637: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:41:46.795Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:41:46.795Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:41:47.897333: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:41:47.897Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:41:50.174067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:41:50.174Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:41:50.217161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:41:50.217Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:41:50.217535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:41:50.217Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:15.973272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:53:15.973Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:15.977064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:53:15.977Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:15.977438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:53:15.977Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:17.404160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:53:17.404Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:17.405355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:53:17.405Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:17.406091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:53:17.406Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:17.406821: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:53:17.406Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:17.407403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13692 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:53:17.407Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:53:17.447492: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at matrix_diag_op.cc:273 : INVALID_ARGUMENT: Encountered overflow when multiplying 9984734776 with 1248091847, result: -5984878005326580344"",""time"":""2023-08-17T14:53:17.447Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:56:00.832626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8900"",""time"":""2023-08-17T14:56:00.832Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:56:00.832804: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:983] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)"",""time"":""2023-08-17T14:56:00.833Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:56:03.623Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:56:03.623Z"",""v"":0}
```
```
"
61602,Integer overflow when running tf.raw_ops.MatrixDiagV3 on colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in input lists

### Standalone code to reproduce the issue

```shell
results = dict()
import tensorflow as tf
import os
import numpy as np
try:
  diagonal_0_0_0_0 = 1111
  diagonal_0_0_0_1 = 1112
  diagonal_0_0_0 = [diagonal_0_0_0_0,diagonal_0_0_0_1,]
  diagonal_0_0_1_0 = 1121
  diagonal_0_0_1_1 = 1122
  diagonal_0_0_1 = [diagonal_0_0_1_0,diagonal_0_0_1_1,]
  diagonal_0_0 = [diagonal_0_0_0,diagonal_0_0_1,]
  diagonal_0_1_0_0 = 1211
  diagonal_0_1_0_1 = 1212
  diagonal_0_1_0 = [diagonal_0_1_0_0,diagonal_0_1_0_1,]
  diagonal_0_1_1_0 = 1221
  diagonal_0_1_1_1 = 1222
  diagonal_0_1_1 = [diagonal_0_1_1_0,diagonal_0_1_1_1,]
  diagonal_0_1 = [diagonal_0_1_0,diagonal_0_1_1,]
  diagonal_0 = [diagonal_0_0,diagonal_0_1,]
  diagonal_1_0_0_0 = 2111
  diagonal_1_0_0_1 = 2112
  diagonal_1_0_0 = [diagonal_1_0_0_0,diagonal_1_0_0_1,]
  diagonal_1_0_1_0 = 2121
  diagonal_1_0_1_1 = 2122
  diagonal_1_0_1 = [diagonal_1_0_1_0,diagonal_1_0_1_1,]
  diagonal_1_0 = [diagonal_1_0_0,diagonal_1_0_1,]
  diagonal_1_1_0_0 = 2211
  diagonal_1_1_0_1 = 35.0
  diagonal_1_1_0 = [diagonal_1_1_0_0,diagonal_1_1_0_1,]
  diagonal_1_1_1_0 = 2221
  diagonal_1_1_1_1 = 2222
  diagonal_1_1_1 = [diagonal_1_1_1_0,diagonal_1_1_1_1,]
  diagonal_1_1 = [diagonal_1_1_0,diagonal_1_1_1,]
  diagonal_1 = [diagonal_1_0,diagonal_1_1,]
  diagonal = [diagonal_0,diagonal_1,]
  k = 3046875451
  num_rows = -1
  num_cols = -1
  padding_value = 0
  align = ""RIGHT_LEFT""
  name = ""diag_part""
  out = tf.raw_ops.MatrixDiagV3(diagonal=diagonal,k=k,num_rows=num_rows,num_cols=num_cols,padding_value=padding_value,align=align,name=name,)
except Exception as e:
  print(""Error:""+str(e))

print(results)
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__MatrixDiagV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 9984734776 with 1248091847, result: -5984878005326580344
	 [[{{node MatrixDiagV3}}]] [Op:MatrixDiagV3]
{}
```
```
"
61601,Crash when running tf.keras.layers.MaxPooling2D,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

PRETTY_NAME=""Ubuntu 22.04.2 LTS"" NAME=""Ubuntu"" VERSION_ID=""22.04"" VERSION=""22.04.2 LTS (Jammy Jellyfish)"" VERSION_CODENAME=jammy ID=ubuntu ID_LIKE=debian HOME_URL=""https://www.ubuntu.com/"" SUPPORT_URL=""https://help.ubuntu.com/"" BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/"" PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"" UBUNTU_CODENAME=jammy

### Mobile device

_No response_

### Python version

  3.10.12 (main, Jun 11 2023, 05:26:28) 

### Bazel version

_No response_

### GCC/compiler version

[GCC 11.4.0]

### CUDA/cuDNN version

nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Wed_Sep_21_10:33:58_PDT_2022 Cuda compilation tools, release 11.8, V11.8.89 Build cuda_11.8.r11.8/compiler.31833905_0

### GPU model and memory

T4

### Current behavior?

Due to the large list of elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  arg_0_0 = 1e+20
  arg_0_1 = True
  arg_0 = [arg_0_0,arg_0_1,]
  strides_0 = 2
  strides_1 = 2
  strides = [strides_0,strides_1,]
  arg_class = tf.keras.layers.MaxPooling2D(arg_0,strides=strides,)
  arg_input_0_tensor = tf.random.uniform([2, 17, 17, 768], dtype=tf.float32)
  arg_input_0 = tf.identity(arg_input_0_tensor)
  arg_input = [arg_input_0,]
  out = arg_class(*arg_input)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.837 NotebookApp] Searching ['/root/.jupyter', '/root/.local/etc/jupyter', '/usr/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files"",""time"":""2023-08-17T14:24:43.838Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.838 NotebookApp] Looking for jupyter_config in /etc/jupyter"",""time"":""2023-08-17T14:24:43.846Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.840 NotebookApp] Looking for jupyter_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T14:24:43.847Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.842 NotebookApp] Looking for jupyter_config in /usr/etc/jupyter"",""time"":""2023-08-17T14:24:43.848Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.842 NotebookApp] Looking for jupyter_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T14:24:43.848Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.842 NotebookApp] Looking for jupyter_config in /root/.jupyter"",""time"":""2023-08-17T14:24:43.850Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.843 NotebookApp] Looking for jupyter_notebook_config in /etc/jupyter"",""time"":""2023-08-17T14:24:43.850Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.847 NotebookApp] Loaded config file: /etc/jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T14:24:43.854Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.847 NotebookApp] Looking for jupyter_notebook_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T14:24:43.855Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.849 NotebookApp] Loaded config file: /usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:43.855Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.849 NotebookApp] Looking for jupyter_notebook_config in /usr/etc/jupyter"",""time"":""2023-08-17T14:24:43.862Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.849 NotebookApp] Looking for jupyter_notebook_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T14:24:43.863Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.849 NotebookApp] Looking for jupyter_notebook_config in /root/.jupyter"",""time"":""2023-08-17T14:24:43.864Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:43.857 NotebookApp] Loaded config file: /root/.jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T14:24:43.865Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.049 NotebookApp] Searching ['/root/.jupyter', '/root/.local/etc/jupyter', '/usr/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files"",""time"":""2023-08-17T14:24:44.050Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /etc/jupyter"",""time"":""2023-08-17T14:24:44.056Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T14:24:44.057Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /usr/etc/jupyter"",""time"":""2023-08-17T14:24:44.057Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T14:24:44.057Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.056 NotebookApp] Looking for jupyter_config in /root/.jupyter"",""time"":""2023-08-17T14:24:44.057Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.059 NotebookApp] Looking for jupyter_notebook_config in /etc/jupyter"",""time"":""2023-08-17T14:24:44.065Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.059 NotebookApp] Loaded config file: /etc/jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T14:24:44.066Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Looking for jupyter_notebook_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T14:24:44.066Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Loaded config file: /usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.066Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Looking for jupyter_notebook_config in /usr/etc/jupyter"",""time"":""2023-08-17T14:24:44.066Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Looking for jupyter_notebook_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T14:24:44.067Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.060 NotebookApp] Looking for jupyter_notebook_config in /root/.jupyter"",""time"":""2023-08-17T14:24:44.067Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""[D 14:24:44.061 NotebookApp] Loaded config file: /root/.jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T14:24:44.067Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.713Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json"",""time"":""2023-08-17T14:24:44.715Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.715Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.717Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.719Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.723Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret"",""time"":""2023-08-17T14:24:44.743Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Authentication of /metrics is OFF, since other authentication is disabled."",""time"":""2023-08-17T14:24:44.745Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""google.colab serverextension initialized."",""time"":""2023-08-17T14:24:44.800Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.806Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json"",""time"":""2023-08-17T14:24:44.808Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.808Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/usr/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.809Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.810Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \t/root/.jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T14:24:44.811Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret"",""time"":""2023-08-17T14:24:44.831Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Authentication of /metrics is OFF, since other authentication is disabled."",""time"":""2023-08-17T14:24:44.831Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""google.colab serverextension initialized."",""time"":""2023-08-17T14:24:44.856Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Serving notebooks from local directory: /"",""time"":""2023-08-17T14:24:49.405Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Jupyter Notebook 6.4.8 is running at:"",""time"":""2023-08-17T14:24:49.405Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""http://172.28.0.12:9000/"",""time"":""2023-08-17T14:24:49.406Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Use Control-C to stop this server and shut down all kernels (twice to skip confirmation)."",""time"":""2023-08-17T14:24:49.406Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Serving notebooks from local directory: /"",""time"":""2023-08-17T14:24:49.430Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Jupyter Notebook 6.4.8 is running at:"",""time"":""2023-08-17T14:24:49.431Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""http://172.28.0.2:9000/"",""time"":""2023-08-17T14:24:49.432Z"",""v"":0}
{""pid"":6,""type"":""jupyter"",""level"":30,""msg"":""Use Control-C to stop this server and shut down all kernels (twice to skip confirmation)."",""time"":""2023-08-17T14:24:49.432Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Kernel started: 5b91d383-2a5d-4d17-8f8d-5f59277ecb11, name: python3"",""time"":""2023-08-17T14:26:06.571Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:29.531813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:26:29.531Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:26:29.532Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:32.617224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:26:32.617Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.113889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.114Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.787982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.788Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.788362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.788Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.803571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.803Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.803974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.804Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:36.804329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:36.804Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000771: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.000816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13664 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:26:40.001Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:40.077428: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6093750902 exceeds 10% of free system memory."",""time"":""2023-08-17T14:26:40.077Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:26:42.860963: F tensorflow/core/framework/tensor_shape.cc:587] Check failed: size >= 0 (0 vs. -1248091845)"",""time"":""2023-08-17T14:26:42.861Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:26:45.571Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:26:45.573Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:36.094812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:27:36.100Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:27:36.101Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:37.142240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:27:37.142Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.764145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.764Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.816688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.816Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.826117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.828Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.831423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.831Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.832476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.832Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:41.833383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:41.833Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.665264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:43.665Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.667057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:43.667Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.668052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:27:43.668Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.668828: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:27:43.668Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.669432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13664 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:27:43.669Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:43.701398: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6093750902 exceeds 10% of free system memory."",""time"":""2023-08-17T14:27:43.701Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:27:48.308263: F tensorflow/core/framework/tensor_shape.cc:587] Check failed: size >= 0 (0 vs. -1248091845)"",""time"":""2023-08-17T14:27:48.308Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:27:51.576Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:27:51.576Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:09.645133: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:31:09.645Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:31:09.646Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:10.714033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:31:10.714Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.844776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.844Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.892144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.892Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.894009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.894Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.896074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.896Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.897012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.897Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:12.897897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:12.897Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.331805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:14.332Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.332225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:14.333Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.332548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:31:14.333Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.332751: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:31:14.333Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:14.332800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13664 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:31:14.333Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:18.149537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900"",""time"":""2023-08-17T14:31:18.149Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:31:18.151181: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:959] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)"",""time"":""2023-08-17T14:31:18.151Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:31:18.580Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:31:18.581Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:32:58.023248: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:32:58.024Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:32:58.024Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:33:00.925573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:33:00.925Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.817441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.817Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.920819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.920Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.921484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.921Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.923040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.923Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.923548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.923Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:22.924029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:22.924Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.323413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:24.323Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.324064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:24.324Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.324360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:34:24.324Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.324522: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:34:24.325Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:24.324601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13664 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:34:24.325Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:29.102979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900"",""time"":""2023-08-17T14:34:29.103Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:29.104423: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:959] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)"",""time"":""2023-08-17T14:34:29.104Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:34:30.587Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:34:30.587Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:39.499788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:34:39.500Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:34:39.500Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:41.797362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:34:41.797Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Kernel restarted: 5b91d383-2a5d-4d17-8f8d-5f59277ecb11"",""time"":""2023-08-17T14:34:51.616Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:34:59.388548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T14:34:59.392Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T14:34:59.392Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:35:01.250311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T14:35:01.250Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.143899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.144Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.181829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.182Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.182572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.182Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.184389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.184Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.184737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.185Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:06.185055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:06.185Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.232416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:07.232Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.232868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:07.233Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.233157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355"",""time"":""2023-08-17T14:36:07.233Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.233367: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0."",""time"":""2023-08-17T14:36:07.233Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.233419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13692 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5"",""time"":""2023-08-17T14:36:07.233Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:07.261994: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at matrix_diag_op.cc:273 : INVALID_ARGUMENT: Encountered overflow when multiplying 12884901888 with 1610612736, result: -1"",""time"":""2023-08-17T14:36:07.262Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:36:57.056526: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6093750902 exceeds 10% of free system memory."",""time"":""2023-08-17T14:36:57.056Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:37:03.543980: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at matrix_diag_op.cc:273 : INVALID_ARGUMENT: Encountered overflow when multiplying 3046875451 with 3046875451, result: -9163294059803098215"",""time"":""2023-08-17T14:37:03.544Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:37:04.268280: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6093750784 exceeds 10% of free system memory."",""time"":""2023-08-17T14:37:04.268Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:37:08.172313: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at matrix_diag_op.cc:273 : INVALID_ARGUMENT: Encountered overflow when multiplying 3046875392 with 3046875392, result: -9163294419334397952"",""time"":""2023-08-17T14:37:08.172Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:38:16.953443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8900"",""time"":""2023-08-17T14:38:16.953Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 14:38:16.953795: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:983] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)"",""time"":""2023-08-17T14:38:16.954Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T14:38:18.617Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 5b91d383-2a5d-4d17-8f8d-5f59277ecb11 restarted"",""time"":""2023-08-17T14:38:18.617Z"",""v"":0}
```
```
"
61600,Integer overflow when running tf.linalg.diag,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

PRETTY_NAME=""Ubuntu 22.04.2 LTS"" NAME=""Ubuntu"" VERSION_ID=""22.04"" VERSION=""22.04.2 LTS (Jammy Jellyfish)"" VERSION_CODENAME=jammy ID=ubuntu ID_LIKE=debian HOME_URL=""https://www.ubuntu.com/"" SUPPORT_URL=""https://help.ubuntu.com/"" BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/"" PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"" UBUNTU_CODENAME=jammy

### Mobile device

_No response_

### Python version

3.10.12 (main, Jun 11 2023, 05:26:28)

### Bazel version

_No response_

### GCC/compiler version

[GCC 11.4.0]

### CUDA/cuDNN version

[nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0](nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Wed_Sep_21_10:33:58_PDT_2022 Cuda compilation tools, release 11.8, V11.8.89 Build cuda_11.8.r11.8/compiler.31833905_0)

### GPU model and memory

T4

### Current behavior?

Due to the large list of elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  diagonal_0_0_0 = []
  diagonal_0_0_1_0 = True
  diagonal_0_0_1_1 = 1122
  diagonal_0_0_1 = [diagonal_0_0_1_0,diagonal_0_0_1_1,]
  diagonal_0_0 = [diagonal_0_0_0,diagonal_0_0_1,]
  diagonal_0_1_0_0 = 1211
  diagonal_0_1_0_1 = 1212
  diagonal_0_1_0 = [diagonal_0_1_0_0,diagonal_0_1_0_1,]
  diagonal_0_1_1_0 = 1221
  diagonal_0_1_1_1 = 1222
  diagonal_0_1_1 = [diagonal_0_1_1_0,diagonal_0_1_1_1,]
  diagonal_0_1 = [diagonal_0_1_0,diagonal_0_1_1,]
  diagonal_0 = [diagonal_0_0,diagonal_0_1,]
  diagonal_1_0_0_0 = True
  diagonal_1_0_0_1 = """"
  diagonal_1_0_0 = [diagonal_1_0_0_0,diagonal_1_0_0_1,]
  diagonal_1_0_1_0 = 2121
  diagonal_1_0_1_1 = 2122
  diagonal_1_0_1 = [diagonal_1_0_1_0,diagonal_1_0_1_1,]
  diagonal_1_0 = [diagonal_1_0_0,diagonal_1_0_1,]
  diagonal_1_1_0_0 = 2211
  diagonal_1_1_0_1 = 2212
  diagonal_1_1_0 = [diagonal_1_1_0_0,diagonal_1_1_0_1,]
  diagonal_1_1_1_0 = 30.0
  diagonal_1_1_1_1 = 2222
  diagonal_1_1_1 = [diagonal_1_1_1_0,diagonal_1_1_1_1,]
  diagonal_1_1 = [diagonal_1_1_0,diagonal_1_1_1,]
  diagonal_1 = [diagonal_1_0,diagonal_1_1,]
  diagonal = [diagonal_0,diagonal_1,]
  name = ""None""
  k = 1610612736
  padding_value = 0
  align = ""RIGHT_LEFT""
  out = tf.linalg.diag(diagonal=diagonal,name=name,k=k,padding_value=padding_value,align=align,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__MatrixDiagV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 12884901888 with 1610612736, result: -1
	 [[{{node MatrixDiagV3}}]] [Op:MatrixDiagV3]
```
```
"
61599,Integer overflow when running tf.experimental.numpy.identity,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

PRETTY_NAME=""Ubuntu 22.04.2 LTS"" NAME=""Ubuntu"" VERSION_ID=""22.04"" VERSION=""22.04.2 LTS (Jammy Jellyfish)"" VERSION_CODENAME=jammy ID=ubuntu ID_LIKE=debian HOME_URL=""https://www.ubuntu.com/"" SUPPORT_URL=""https://help.ubuntu.com/"" BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/"" PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"" UBUNTU_CODENAME=jammy

### Mobile device

_No response_

### Python version

3.10.12 (main, Jun 11 2023, 05:26:28)

### Bazel version

_No response_

### GCC/compiler version

[GCC 11.4.0]

### CUDA/cuDNN version

[nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0](nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Wed_Sep_21_10:33:58_PDT_2022 Cuda compilation tools, release 11.8, V11.8.89 Build cuda_11.8.r11.8/compiler.31833905_0)

### GPU model and memory

T4

### Current behavior?

Due to large integer variable

### Standalone code to reproduce the issue

```shell
results = dict()
import tensorflow as tf
import numpy as np
try:
  try:
    with tf.device('/CPU'):
      n_tensor = 3046875451 
      n = tf.identity(n_tensor)
      dtype = tf.uint16
      out = tf.experimental.numpy.identity(n=n,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      n = tf.identity(n_tensor)
      n = tf.cast(n, tf.complex64)
      dtype = tf.uint16
      tf.experimental.numpy.identity(n=n,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))

print(results)
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__MatrixDiagV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Encountered overflow when multiplying 3046875451 with 3046875451, result: -9163294059803098215 [Op:MatrixDiagV3] name: diag

/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py:1035: ComplexWarning: Casting complex values to real discards the imaginary part
  return int(self._numpy())

Error:{{function_node __wrapped__MatrixDiagV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Encountered overflow when multiplying 3046875392 with 3046875392, result: -9163294419334397952 [Op:MatrixDiagV3] name: diag
{}


```
```
"
61598,AttributeError: module 'tensorflow.python.pywrap_mlir' has no attribute 'experimental_convert_saved_model_v1',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-96406-gfa4d29bfef8 2.14.0-dev20230706

### Custom code

No

### OS platform and distribution

UIbuntu 20.04

### Mobile device

UIbuntu 20.04

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

-

### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

```shell
File ""/home/fastDisk/jiahao/research/iree/.venv/lib/python3.8/site-packages/iree/tools/tf/scripts/iree_import_tf/__main__.py"", line 54, in main
    import_saved_model(
  File ""/home/fastDisk/jiahao/research/iree/.venv/lib/python3.8/site-packages/iree/tools/tf/scripts/iree_import_tf/__main__.py"", line 102, in import_saved_model
    result = convert_saved_model_v1(
  File ""/home/fastDisk/jiahao/research/iree/.venv/lib/python3.8/site-packages/tensorflow/python/compiler/mlir/mlir.py"", line 141, in convert_saved_model_v1
    return pywrap_mlir.experimental_convert_saved_model_v1(
AttributeError: module 'tensorflow.python.pywrap_mlir' has no attribute 'experimental_convert_saved_model_v1'
```
"
61596,Could not find a version that satisfies the requirement tensorflow-compression~=2.12.0 (from versions: none),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

fail

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I try to pip intsall tensorflow-federated it fail, so i build it on source, i pip install --requirement ""requirements.txt"" then happened this error. can i know how to solve this problem?

### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

```shell
ERROR: Could not find a version that satisfies the requirement tensorflow-compression~=2.12.0 (from versions: none)
ERROR: No matching distribution found for tensorflow-compression~=2.12.0
```
"
61594,TFLite GPU delegate: Broadcast output incorrect,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

Nightly at 09cf1b2a39023e617e003a51be39d419702c2d36

### Custom code

Yes

### OS platform and distribution

Android 12 2023-03-01

### Mobile device

Vivo X80

### Python version

_No response_

### Bazel version

CMake 3.19.0

### GCC/compiler version

Android NDK r25

### CUDA/cuDNN version

_No response_

### GPU model and memory

Mali-G710 MC10

### Current behavior?

TFLite model file:  [model.tflite.zip](https://github.com/tensorflow/tensorflow/files/12360271/model.tflite.zip)

The provided TFLite model contains a single broadcast operation, which when executed by the provided C++ program, should produce the following output:

```
1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  
2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  
3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  
4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  
5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  
6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  
7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  
8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8 
```

However, when using the GPU delegate the following output is produced:

```
1  0  0  1  1  0  0  1  1  0  0  1  1  0  0  1  
2  0  0  1  2  0  0  1  2  0  0  1  2  0  0  1  
3  0  0  1  3  0  0  1  3  0  0  1  3  0  0  1  
4  0  0  1  4  0  0  1  4  0  0  1  4  0  0  1  
5  0  0  1  5  0  0  1  5  0  0  1  5  0  0  1  
6  0  0  1  6  0  0  1  6  0  0  1  6  0  0  1  
7  0  0  1  7  0  0  1  7  0  0  1  7  0  0  1  
8  0  0  1  8  0  0  1  8  0  0  1  8  0  0  1 
```

The correct output is produced when using the CPU and not the GPU delegate.  We are concerned that this could be a security issue if memory is being accessed incorrectly.

Content of `model.tflite`:

```
Your TFLite model has '1' subgraph(s). In the subgraph description below,
T# represents the Tensor numbers. For example, in Subgraph#0, the MUL op takes
tensor #0 and tensor #1 as input and produces tensor #2 as output.

Subgraph#0 main(T#0) -> [T#2]
  Op#0 MUL(T#0, T#1) -> [T#2]

Tensors of Subgraph#0
  T#0(serving_default_input:0) shape:[8, 1], type:FLOAT32
  T#1(BroadcastTo) shape:[8, 16], type:FLOAT32 RO 512 bytes, buffer: 2, data:[1, 1, 1, 1, 1, ...]
  T#2(PartitionedCall:0) shape:[8, 16], type:FLOAT32

---------------------------------------------------------------
Your TFLite model has '1' signature_def(s).

Signature#0 key: 'serving_default'
- Subgraph: Subgraph#0
- Inputs: 
    'input' : T#0
- Outputs: 
    'output' : T#2

---------------------------------------------------------------
              Model size:       1400 bytes
    Non-data buffer size:        780 bytes (55.71 %)
  Total data buffer size:        620 bytes (44.29 %)
    (Zero value buffers):          0 bytes (00.00 %)
```

### Standalone code to reproduce the issue

```shell
#include <memory>
#include <stdio.h>

#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/delegates/gpu/delegate.h""

int main() {
    std::unique_ptr<tflite::FlatBufferModel> model =
        tflite::FlatBufferModel::BuildFromFile(""model.tflite"");
    
    tflite::ops::builtin::BuiltinOpResolver resolver;
    tflite::InterpreterBuilder builder(*model, resolver);
    std::unique_ptr<tflite::Interpreter> interpreter;
    builder(&interpreter);

    TfLiteGpuDelegateOptionsV2 options = TfLiteGpuDelegateOptionsV2Default();
    auto* delegate = TfLiteGpuDelegateV2Create(&options);
    if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return 1;

    const TfLiteTensor *inTensor = interpreter->input_tensor(0);
    const TfLiteIntArray *inShape = inTensor->dims;
    float *input = inTensor->data.f;

    for (int i = 0; i < inShape->data[0]; i++) {
        input[i] = i + 1;
    }

    if (interpreter->Invoke() != kTfLiteOk) return 1;

    std::vector<unsigned long> outShapeVec;
    const TfLiteTensor *outTensor = interpreter->output_tensor(0);
    const TfLiteIntArray *outShape = outTensor->dims;
    const float *output = outTensor->data.f;

    const float *outPtr = output;
    for (size_t i = 0; i < outShape->data[0]; i++) {
        for (size_t j = 0; j < outShape->data[1]; j++) {
            printf(""%.2g  "", *(outPtr++));
        }
        printf(""\n"");
    }
}
```


### Relevant log output

_No response_"
61593,"Build: Protobuf fails with ""File already exists in database""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.1.1

### GCC/compiler version

11.3

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Using the TF_SYSTEMLIBS version of protobuf (i.e. a preinstalled protobuf) when building TensorFlow from source results in

```
[libprotobuf ERROR /build/protobuf/3.19.4/GCCcore-11.3.0/protobuf-3.19.4/src/google/protobuf/descriptor_database.cc:641] File already exists in database: tensorflow/dtensor/proto/layout.proto
[libprotobuf FATAL /build/protobuf/3.19.4/GCCcore-11.3.0/protobuf-3.19.4/src/google/protobuf/descriptor.cc:2021] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
```

I can't make much sense out of that failure and Google yielded results related to having the protobuf file in loaded in shared libraries multiple times.

All I could do is indeed trace it to `from tensorflow.dtensor.proto import layout_pb2` in tensorflow/dtensor/python/layout.py

### Standalone code to reproduce the issue

```shell
The failing build step invokes `/bin/bash -c 'bazel-out/ppc-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/ppc-opt/bin/tensorflow/_api/v2/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow/compat_template_v1.__init__.py --compat_init_template=tensorflow/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=default bazel-out/ppc-opt/bin/tensorflow/tf_python_api_gen_v2.params'`
```


### Relevant log output

```shell
# Configuration: 020ca55738349851eb8a5a672fc7b8d08dc15ebd99b324f52fddbad2f32820b8
# Execution platform: @local_execution_config_platform//:platform
ERROR: /build/TensorFlow/tensorflow-2.13.0/tensorflow/BUILD:1646:19: Action tensorflow/_api/v2/v2.py failed: (Aborted): bash failed: error executing command 
  (cd /build/TensorFlow/bazel-root/663b1bf019e1a9ec9827eae691fce071/execroot/org_tensorflow && \
  exec env - \
    CPATH=/sw/installed/cURL/7.83.0-GCCcore-11.3.0/include:/software/double-conversion/3.2.0-GCCcore-11.3.0/include:/software/flatbuffers/2.0.7-GCCcore-11.3.0/include:/sw/installed/giflib/5.2.1-GCCcore-11.3.0/include:/sw/installed/hwloc/2.7.1-GCCcore-11.3.0/include:/sw/installed/ICU/71.1-GCCcore-11.3.0/include:/software/JsonCpp/1.9.5-GCCcore-11.3.0/include:/sw/installed/libjpeg-turbo/2.1.3-GCCcore-11.3.0/include:/sw/installed/libpng/1.6.37-GCCcore-11.3.0/include:/software/nsync/1.25.0-GCCcore-11.3.0/include:/software/protobuf/3.19.4-GCCcore-11.3.0/include:/sw/installed/pybind11/2.9.2-GCCcore-11.3.0/include:/software/snappy/1.1.9-GCCcore-11.3.0/include:/sw/installed/SQLite/3.38.3-GCCcore-11.3.0/include:/sw/installed/zlib/1.2.12-GCCcore-11.3.0/include:/sw/installed/OpenSSL/1.1/include \
    LD_LIBRARY_PATH=/software/RE2/2022-06-01-GCCcore-11.3.0/lib:/software/snappy/1.1.9-GCCcore-11.3.0/lib:/sw/installed/libpng/1.6.37-GCCcore-11.3.0/lib:/software/nsync/1.25.0-GCCcore-11.3.0/lib:/sw/installed/libjpeg-turbo/2.1.3-GCCcore-11.3.0/lib:/software/JsonCpp/1.9.5-GCCcore-11.3.0/lib:/sw/installed/ICU/71.1-GCCcore-11.3.0/lib:/sw/installed/giflib/5.2.1-GCCcore-11.3.0/lib:/software/flatbuffers/2.0.7-GCCcore-11.3.0/lib:/software/double-conversion/3.2.0-GCCcore-11.3.0/lib:/sw/installed/HDF5/1.12.2-gompi-2022a/lib:/sw/installed/Szip/2.1.1-GCCcore-11.3.0/lib:/sw/installed/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages/numpy/core/lib:/sw/installed/ScaLAPACK/2.2.0-gompi-2022a-fb/lib:/sw/installed/FFTW.MPI/3.3.10-gompi-2022a/lib:/sw/installed/FFTW/3.3.10-GCC-11.3.0/lib:/sw/installed/FlexiBLAS/3.2.0-GCC-11.3.0/lib:/sw/installed/OpenBLAS/0.3.20-GCC-11.3.0/lib:/sw/installed/OpenMPI/4.1.4-GCC-11.3.0/lib:/sw/installed/UCC/1.0.0-GCCcore-11.3.0/lib:/sw/installed/PMIx/4.1.2-GCCcore-11.3.0/lib:/sw/installed/libfabric/1.15.1-GCCcore-11.3.0/lib:/sw/installed/UCX/1.12.1-GCCcore-11.3.0/lib:/sw/installed/libevent/2.1.12-GCCcore-11.3.0/lib:/sw/installed/hwloc/2.7.1-GCCcore-11.3.0/lib:/sw/installed/libpciaccess/0.16-GCCcore-11.3.0/lib:/sw/installed/numactl/2.0.14-GCCcore-11.3.0/lib:/sw/installed/Python/3.10.4-GCCcore-11.3.0/lib:/sw/installed/libffi/3.4.2-GCCcore-11.3.0/lib64:/sw/installed/GMP/6.2.1-GCCcore-11.3.0/lib:/sw/installed/SQLite/3.38.3-GCCcore-11.3.0/lib:/sw/installed/Tcl/8.6.12-GCCcore-11.3.0/lib:/sw/installed/bzip2/1.0.8-GCCcore-11.3.0/lib:/sw/installed/binutils/2.38-GCCcore-11.3.0/lib:/sw/installed/DB/18.1.40-GCCcore-11.3.0/lib:/sw/installed/libreadline/8.1.2-GCCcore-11.3.0/lib:/sw/installed/gettext/0.21-GCCcore-11.3.0/lib:/sw/installed/ncurses/6.3-GCCcore-11.3.0/lib:/sw/installed/libxml2/2.9.13-GCCcore-11.3.0/lib:/sw/installed/XZ/5.2.5-GCCcore-11.3.0/lib:/sw/installed/expat/2.4.8-GCCcore-11.3.0/lib:/sw/installed/cURL/7.83.0-GCCcore-11.3.0/lib:/sw/installed/OpenSSL/1.1/lib:/sw/installed/zlib/1.2.12-GCCcore-11.3.0/lib:/software/protobuf/3.19.4-GCCcore-11.3.0/lib:/sw/installed/Java/11.0.6-ppc64le/lib:/sw/installed/GCCcore/11.3.0/lib64:/usr/local/cuda/lib64 \
    LIBRARY_PATH=/sw/installed/cURL/7.83.0-GCCcore-11.3.0/lib:/software/double-conversion/3.2.0-GCCcore-11.3.0/lib:/software/flatbuffers/2.0.7-GCCcore-11.3.0/lib:/sw/installed/giflib/5.2.1-GCCcore-11.3.0/lib:/sw/installed/hwloc/2.7.1-GCCcore-11.3.0/lib:/sw/installed/ICU/71.1-GCCcore-11.3.0/lib:/software/JsonCpp/1.9.5-GCCcore-11.3.0/lib:/sw/installed/libjpeg-turbo/2.1.3-GCCcore-11.3.0/lib:/sw/installed/libpng/1.6.37-GCCcore-11.3.0/lib:/software/nsync/1.25.0-GCCcore-11.3.0/lib:/software/protobuf/3.19.4-GCCcore-11.3.0/lib:/sw/installed/pybind11/2.9.2-GCCcore-11.3.0/lib:/software/snappy/1.1.9-GCCcore-11.3.0/lib:/sw/installed/SQLite/3.38.3-GCCcore-11.3.0/lib:/sw/installed/zlib/1.2.12-GCCcore-11.3.0/lib:/sw/installed/OpenSSL/1.1/lib \
    PATH=/sw/installed/libpng/1.6.37-GCCcore-11.3.0/bin:/sw/installed/libjpeg-turbo/2.1.3-GCCcore-11.3.0/bin:/sw/installed/NASM/2.15.05-GCCcore-11.3.0/bin:/sw/installed/ICU/71.1-GCCcore-11.3.0/sbin:/sw/installed/ICU/71.1-GCCcore-11.3.0/bin:/sw/installed/giflib/5.2.1-GCCcore-11.3.0/bin:/software/flatbuffers/2.0.7-GCCcore-11.3.0/bin:/software/dill/0.3.6-GCCcore-11.3.0/bin:/sw/installed/HDF5/1.12.2-gompi-2022a/bin:/sw/installed/SciPy-bundle/2022.05-foss-2022a/bin:/sw/installed/FFTW/3.3.10-GCC-11.3.0/bin:/sw/installed/FlexiBLAS/3.2.0-GCC-11.3.0/bin:/sw/installed/OpenMPI/4.1.4-GCC-11.3.0/bin:/sw/installed/UCC/1.0.0-GCCcore-11.3.0/bin:/sw/installed/PMIx/4.1.2-GCCcore-11.3.0/bin:/sw/installed/libfabric/1.15.1-GCCcore-11.3.0/bin:/sw/installed/UCX/1.12.1-GCCcore-11.3.0/bin:/sw/installed/libevent/2.1.12-GCCcore-11.3.0/bin:/sw/installed/hwloc/2.7.1-GCCcore-11.3.0/bin:/sw/installed/numactl/2.0.14-GCCcore-11.3.0/bin:/sw/installed/UnZip/6.0-GCCcore-11.3.0/bin:/sw/installed/pybind11/2.9.2-GCCcore-11.3.0/bin:/sw/installed/Python/3.10.4-GCCcore-11.3.0/bin:/sw/installed/SQLite/3.38.3-GCCcore-11.3.0/bin:/sw/installed/Tcl/8.6.12-GCCcore-11.3.0/bin:/sw/installed/bzip2/1.0.8-GCCcore-11.3.0/bin:/sw/installed/binutils/2.38-GCCcore-11.3.0/bin:/sw/installed/git/2.36.0-GCCcore-11.3.0-nodocs/bin:/sw/installed/Perl/5.34.1-GCCcore-11.3.0/bin:/sw/installed/DB/18.1.40-GCCcore-11.3.0/bin:/sw/installed/gettext/0.21-GCCcore-11.3.0/bin:/sw/installed/ncurses/6.3-GCCcore-11.3.0/bin:/sw/installed/libxml2/2.9.13-GCCcore-11.3.0/bin:/sw/installed/XZ/5.2.5-GCCcore-11.3.0/bin:/sw/installed/expat/2.4.8-GCCcore-11.3.0/bin:/sw/installed/cURL/7.83.0-GCCcore-11.3.0/bin:/sw/installed/OpenSSL/1.1/bin:/software/protobuf/3.19.4-GCCcore-11.3.0/bin:/software/Bazel/5.1.1-GCCcore-11.3.0/bin:/sw/installed/Java/11.0.6-ppc64le:/sw/installed/Java/11.0.6-ppc64le/bin:/sw/installed/GCCcore/11.3.0/bin:/home/s3248973/.local/EasyBuildDev/easybuild-framework:/home/s3248973/.yarn/bin:/home/s3248973/.config/yarn/global/node_modules/.bin:/home/s3248973/.local/bin:/usr/local/cuda/bin:/usr/lib64/qt-3.3/bin:/sw/taurus/tools/slurmtools/default/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin \
    PYTHONNOUSERSITE=1 \
    PYTHONPATH=/software/TensorFlow/2.13.0-foss-2022a/lib/python3.10/site-packages:/software/TensorFlow/2.13.0-foss-2022a/lib/python3.10/site-packages:/software/protobuf-python/3.19.4-GCCcore-11.3.0/lib/python3.10/site-packages:/software/flatbuffers/2.0.7-GCCcore-11.3.0/lib/python3.10/site-packages:/software/dill/0.3.6-GCCcore-11.3.0/lib/python3.10/site-packages:/software/h5py/3.7.0-foss-2022a/lib/python3.10/site-packages:/sw/installed/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages:/sw/installed/pybind11/2.9.2-GCCcore-11.3.0/lib/python3.10/site-packages:/sw/installed/Python/3.10.4-GCCcore-11.3.0/easybuild/python \
    PYTHON_BIN_PATH=/sw/installed/Python/3.10.4-GCCcore-11.3.0/bin/python \
    PYTHON_LIB_PATH=/software/TensorFlow/2.13.0-foss-2022a/lib/python3.10/site-packages \
    TF2_BEHAVIOR=1 \
    TF_SYSTEM_LIBS=absl_py,astor_archive,astunparse_archive,boringssl,com_google_protobuf,curl,cython,dill_archive,double_conversion,flatbuffers,functools32_archive,gast_archive,gif,hwloc,icu,jsoncpp_git,libjpeg_turbo,nasm,nsync,opt_einsum_archive,org_sqlite,pasta,png,pybind11,six_archive,snappy,tblib_archive,termcolor_archive,typing_extensions_archive,wrapt,zlib \
  /bin/bash -c 'bazel-out/ppc-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/ppc-opt/bin/tensorflow/_api/v2/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow/compat_template_v1.__init__.py --compat_init_template=tensorflow/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=default bazel-out/ppc-opt/bin/tensorflow/tf_python_api_gen_v2.params')
```
"
61590,Train Simple Audio Recognition - TinyML,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

**Issue Report** - Error in Training Audio Recognition Model

**Description**:

I am trying to train a simple audio recognition model as described in the book ""TinyML."" I am using Google Colab to train the model. However, I encountered errors during both the installation of dependencies and the training process.

**Error during Install Dependencies**:

When attempting to install dependencies using the command 

!pip uninstall -y tensorflow tensorflow_estimator tensorboard
!pip install -q tf-estimator-nightly==1.14.0.dev2019072901 tf-nightly-gpu==1.15.0.dev20190729

 I encountered the following error:


ERROR: Could not find a version that satisfies the requirement tf-nightly-gpu==1.15.0.dev20190729 (from versions: 2.12.0)
ERROR: No matching distribution found for tf-nightly-gpu==1.15.0.dev20190729
Error during Training - ModuleNotFoundError:

**Error to Begin Training**:

Upon running the training script with TensorFlow in the ""Begin Training"" Section, I received the following error:

Traceback (most recent call last):
  File ""/content/tensorflow/tensorflow/examples/speech_commands/train.py"", line 81, in <module>
    import input_data
  File ""/content/tensorflow/tensorflow/examples/speech_commands/input_data.py"", line 35, in <module>
    from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio
ModuleNotFoundError: No module named 'tensorflow.contrib'


**Observations**:

I suspect that these errors are occurring because the code provided in the book is intended for TensorFlow 1.15, while I am using TensorFlow 2.12.0. Since TensorFlow 1.15 is no longer supported. Not sure how I can resolve this. Please help me to fix this issue.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1t2QMMiCdIxP3xnoNyiz9FjmPmbrF0_iD?usp=sharing
```


### Relevant log output

```shell
ERROR: Could not find a version that satisfies the requirement tf-nightly-gpu==1.15.0.dev20190729 (from versions: 2.12.0)
ERROR: No matching distribution found for tf-nightly-gpu==1.15.0.dev20190729





Traceback (most recent call last):
  File ""/content/tensorflow/tensorflow/examples/speech_commands/train.py"", line 81, in <module>
    import input_data
  File ""/content/tensorflow/tensorflow/examples/speech_commands/input_data.py"", line 35, in <module>
    from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio
ModuleNotFoundError: No module named 'tensorflow.contrib'
```
"
61587,pybind11_proto from python to C++,"@BlaziusMaximus thanks for the explanation.
I've been exploring how to update the [`import_graph_def()`](https://github.com/tensorflow/tensorflow/blob/v2.13.0/tensorflow/python/framework/importer.py#L353-L411) code-path to use pybind11_protobuf and I could use your help with the following:  Similar to how pybind11_protobuf allows us to pass protos directly from C++ to Python, is there a way to pass a `GraphDef` proto from python to C++ without performing serialization? This would be needed to invoke the [TF_GraphImportGraphDefWithResults](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L1801) from pywrap session in C++.

_Originally posted by @othakkar in https://github.com/tensorflow/community/issues/453#issuecomment-1674101660_
            "
61584,Build failure on AARCH64 - undeclared identifier 'memset',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/compiler version

16.0.6

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

Build fails since commit https://github.com/tensorflow/tensorflow/commit/4993fb9fe4e4dbe26657b3bb88dab152ab397b8c

### Standalone code to reproduce the issue

```shell
bazel build --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 -- //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /workspace/tensorflow/lite/kernels/internal/BUILD:448:11: Compiling tensorflow/lite/kernels/internal/optimized/4bit/neon_fully_connected.cc failed: (Exit 1): clang failed: error executing command (from target //tensorflow/lite/kernels/internal:optimized_4bit) 
  (cd /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \
  exec env - \
    CACHEBUSTER=20220325 \
    PATH=/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-arm64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/lib/llvm-16/bin/clang -MD -MF bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/internal/_objs/optimized_4bit/neon_fully_connected.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/internal/_objs/optimized_4bit/neon_fully_connected.pic.o' -DFC_4BIT_NEON '-DBAZEL_CURRENT_REPOSITORY=""""' -iquote . -iquote bazel-out/aarch64-opt/bin -iquote external/cpuinfo -iquote bazel-out/aarch64-opt/bin/external/cpuinfo -isystem external/cpuinfo/include -isystem bazel-out/aarch64-opt/bin/external/cpuinfo/include -isystem external/cpuinfo/src -isystem bazel-out/aarch64-opt/bin/external/cpuinfo/src -fmerge-all-constants -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-gnu-offsetof-extensions '-mtune=generic' '-march=armv8-a' -O3 -flax-vector-conversions '-std=c++17' -DFARMHASH_NO_CXX_STRING -Wno-sign-compare -O3 -fno-exceptions -O3 '--sysroot=/dt10' -c tensorflow/lite/kernels/internal/optimized/4bit/neon_fully_connected.cc -o bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/internal/_objs/optimized_4bit/neon_fully_connected.pic.o)
# Configuration: 70a2ceb8c9b79ab96bab8f0b73bbfb70969f7e2a66f605b1d1332a62f7eef342
# Execution platform: @local_execution_config_platform//:platform
tensorflow/lite/kernels/internal/optimized/4bit/neon_fully_connected.cc:284:3: error: use of undeclared identifier 'memset'
  memset(*dest, static_cast<uint8_t>(119), sizeof(uint8_t) * size);
  ^
tensorflow/lite/kernels/internal/optimized/4bit/neon_fully_connected.cc:313:3: error: use of undeclared identifier 'memset'
  memset(data, 0, sizeof(int8_t) * size);
  ^
tensorflow/lite/kernels/internal/optimized/4bit/neon_fully_connected.cc:314:3: error: use of undeclared identifier 'memset'
  memset(input_offsets, 0, sizeof(int32_t) * layout_rows);
  ^
3 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 23.741s, Critical Path: 7.00s
INFO: 439 processes: 339 internal, 100 local.
FAILED: Build did NOT complete successfully
```
"
61580,Crash when running tensorflow.python.ops.nn_ops.fractional_max_pool,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to feeding Large integer value

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.random.uniform([5, 20, 20, 3], dtype=tf.float64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 1
  arg_1_1 = -13.0
  arg_1_2 = 1.5
  arg_1_3 = 1
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  seed = 125091515651
  seed2 = 1
  deterministic = True
  out = nn_ops.fractional_max_pool(arg_0,arg_1,seed=seed,seed2=seed2,deterministic=deterministic,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 22:05:06.501205: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-14 22:05:07.064824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:05:07.082027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:05:07.082168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:05:07.082457: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 22:05:07.083701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:05:07.083818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:05:07.083916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:05:07.144960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:05:07.145099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:05:07.145198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:05:07.145279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4205 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
WARNING:tensorflow:From /home/nimashiri/anaconda3/envs/fuzzer_tf_2.10.0/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: fractional_max_pool (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` and `deterministic` args are deprecated.  Use fractional_max_pool_v2.
Segmentation fault

```
```
"
61579,Abort when running tensorflow.python.ops.nn_ops.pool,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to Large list element

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  input_tensor = tf.constant(-8968073515812833920, shape=[2, 9, 10, 2], dtype=tf.float32,)
  input = tf.identity(input_tensor)
  window_shape_0 = 1e+38
  window_shape_1 = 536870912
  window_shape = [window_shape_0,window_shape_1,]
  padding = ""SAME""
  pooling_type = ""MAX""
  dilation_rate_0 = 1
  dilation_rate_1 = 1
  dilation_rate = [dilation_rate_0,dilation_rate_1,]
  strides_0 = 1
  strides_1 = 1
  strides = [strides_0,strides_1,]
  out = nn_ops.pool(input=input,window_shape=window_shape,padding=padding,pooling_type=pooling_type,dilation_rate=dilation_rate,strides=strides,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 22:03:21.255791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.273228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.273370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.273661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 22:03:21.274910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.275021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.275124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.328191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.328333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.328435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.328519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4361 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 22:03:21.392635: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600
2023-08-14 22:03:21.392690: F tensorflow/stream_executor/cuda/cuda_dnn.cc:886] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
Aborted

```
```
"
61578,Abort when running tensorflow.python.ops.nn_ops.max_pool,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to an invalid list element

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.random.uniform([1, 6, 8, 1], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 1
  arg_1_1 = -54.0
  arg_1_2 = 2
  arg_1_3 = True
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  arg_2_0 = 1
  arg_2_1 = 1
  arg_2_2 = 1
  arg_2_3 = 1
  arg_2 = [arg_2_0,arg_2_1,arg_2_2,arg_2_3,]
  arg_3 = ""VALID""
  out = nn_ops.max_pool(arg_0,arg_1,arg_2,arg_3,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 22:00:35.715971: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-14 22:00:36.276384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:00:36.293827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:00:36.293973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:00:36.294262: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 22:00:36.295471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:00:36.295585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:00:36.295682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:00:36.356219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:00:36.356397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:00:36.356535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:00:36.356654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4375 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 22:00:36.419723: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600
2023-08-14 22:00:36.419777: F tensorflow/stream_executor/cuda/cuda_dnn.cc:886] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)

```
```
"
61577,Crash when running tensorflow.python.eager.context.check_alive,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Probably due to an invalid string argument.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.eager import context
try:
  try:
    with tf.device('/CPU'):
      arg_0 = ""/job:remote_device/replica:0/task:1""
      out = context.check_alive(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      context.check_alive(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
023-08-14 21:43:55.028346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:43:55.045718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:43:55.045863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:43:55.046195: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 21:43:55.047241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:43:55.047355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:43:55.047452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:43:55.101589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:43:55.101732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:43:55.101832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:43:55.101916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4018 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
"
61576,Crash when running tensorflow.python.ops.gen_image_ops.resize_area,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to negative large tensor

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_image_ops
try:
  arg_0_tensor = tf.constant(-1610612736, shape=[0, 6, 6, 1], dtype=tf.bfloat16,)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.constant(-45932682421089, shape=[2], dtype=tf.int32,)
  arg_1 = tf.identity(arg_1_tensor)
  align_corners = False
  out = gen_image_ops.resize_area(arg_0,arg_1,align_corners=align_corners,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 21:39:36.565382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-14 21:39:37.126658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.144298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.144439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.144729: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 21:39:37.146083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.146208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.146312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.200697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.200846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.200954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.201045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4036 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
"
61575,Abort when running tensorflow.python.ops.gen_ctc_ops.ctc_loss,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to large input tensor

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_ctc_ops
try:
  arg_0_tensor = tf.constant(-8968073515812833920, shape=[2, 2, 3], dtype=tf.float32,)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.constant(8968073515812833920, shape=[4, 2], dtype=tf.int64,)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2_tensor = tf.random.uniform([4], minval=-256, maxval=257, dtype=tf.int32)
  arg_2 = tf.identity(arg_2_tensor)
  arg_3_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int32)
  arg_3 = tf.identity(arg_3_tensor)
  preprocess_collapse_repeated = False
  ctc_merge_repeated = True
  ignore_longer_outputs_than_inputs = False
  out = gen_ctc_ops.ctc_loss(arg_0,arg_1,arg_2,arg_3,preprocess_collapse_repeated=preprocess_collapse_repeated,ctc_merge_repeated=ctc_merge_repeated,ignore_longer_outputs_than_inputs=ignore_longer_outputs_than_inputs,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 21:36:56.173477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:36:56.190838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:36:56.190979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:36:56.191266: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 21:36:56.191900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:36:56.192006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:36:56.192110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:36:56.252143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:36:56.252287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:36:56.252387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:36:56.252470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4025 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 21:36:56.314614: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600
2023-08-14 21:36:56.327748: F tensorflow/stream_executor/cuda/cuda_dnn.cc:804] Check failed: cudnnSetConvolutionGroupCount( handle_.get(), convolution_descriptor.group_count()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
Aborted

```
```
"
61574,Abort when running tensorflow.python.ops.nn_ops.conv3d_transpose_v2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to invalid list elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.random.uniform([2, 4, 4, 4, 3], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.random.uniform([2, 2, 2, 5, 3], dtype=tf.float32)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2_0 = 2
  arg_2_1 = 8
  arg_2_2 = 8
  arg_2_3 = 8
  arg_2_4 = False
  arg_2 = [arg_2_0,arg_2_1,arg_2_2,arg_2_3,arg_2_4,]
  arg_3 = 2
  out = nn_ops.conv3d_transpose_v2(arg_0,arg_1,arg_2,arg_3,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 21:33:38.109289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-14 21:33:38.668776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.686385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.686530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.686817: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 21:33:38.687712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.687825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.687920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.749104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.749246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.749344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.749426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3978 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 21:33:38.812494: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600
2023-08-14 21:33:38.825580: F tensorflow/stream_executor/cuda/cuda_dnn.cc:804] Check failed: cudnnSetConvolutionGroupCount( handle_.get(), convolution_descriptor.group_count()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
Aborted

```
```
"
61573,Abort when running tensorflow.python.ops.nn_ops.conv3d_transpose,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to feeding invalid list element

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.random.uniform([2, 5, 6, 4, 3], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.random.uniform([3, 3, 3, 2, 3], dtype=tf.float32)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2_0 = 2
  arg_2_1 = 11
  arg_2_2 = 13
  arg_2_3 = 9
  arg_2_4 = False
  arg_2 = [arg_2_0,arg_2_1,arg_2_2,arg_2_3,arg_2_4,]
  strides_0 = 1
  strides_1 = 2
  strides_2 = 2
  strides_3 = 2
  strides_4 = 1
  strides = [strides_0,strides_1,strides_2,strides_3,strides_4,]
  padding = ""VALID""
  out = nn_ops.conv3d_transpose(arg_0,arg_1,arg_2,strides=strides,padding=padding,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 21:32:34.731642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:32:34.748917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:32:34.749058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:32:34.749350: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 21:32:34.750631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:32:34.750742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:32:34.750840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:32:34.804553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:32:34.804756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:32:34.804893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:32:34.804979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4009 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 21:32:34.866964: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600
2023-08-14 21:32:34.880076: F tensorflow/stream_executor/cuda/cuda_dnn.cc:804] Check failed: cudnnSetConvolutionGroupCount( handle_.get(), convolution_descriptor.group_count()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
Aborted

```
```
"
61572,Crash when running tensorflow.python.framework.kernels.get_registered_kernels_for_op,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to feeding None argument.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.framework import kernels
try:
  arg_0 = None
  out = kernels.get_registered_kernels_for_op(arg_0,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 21:28:52.279973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Segmentation fault

```
```
"
61571,Crash when running tensorflow.python.framework.importer._PopulateTFImportGraphDefOptions,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to None argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.framework import importer
try:
  arg_0 = None
  arg_1 = """"
  arg_2 = None
  arg_3_0 = ""A""
  arg_3_1 = ""B""
  arg_3 = [arg_3_0,arg_3_1,]
  arg_4 = True
  out = importer._PopulateTFImportGraphDefOptions(arg_0,arg_1,arg_2,arg_3,arg_4,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 21:26:55.810615: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Segmentation fault

```
```
"
61570,Crash when running tensorflow.python.framework.importer._GatherReturnElements,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to None argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.framework import importer
try:
  arg_0_0 = ""Placeholder:0""
  arg_0_1 = ""add:0""
  arg_0 = [arg_0_0,arg_0_1,]
  arg_1 = None
  arg_2 = None
  out = importer._GatherReturnElements(arg_0,arg_1,arg_2,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 21:23:49.892933: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Segmentation fault
```
```
"
61568,Internal Assertion Failure when running tensorflow.python.eager.remote.connect_to_remote_host,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to feeding NaN input

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.eager import remote
try:
  try:
    with tf.device('/CPU'):
      arg_0 = ""nan""
      out = remote.connect_to_remote_host(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      remote.connect_to_remote_host(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 19:46:47.563173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:46:47.580413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:46:47.580559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:46:47.580847: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 19:46:47.582229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:46:47.582354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:46:47.582453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:46:47.643352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:46:47.643499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:46:47.643600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:46:47.643684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4110 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 19:46:47.645827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:46:47.645936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:46:47.646029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:46:47.646141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:46:47.646236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:46:47.646304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4110 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 19:46:47.654175: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:580] INVALID_ARGUMENT: Could not interpret ""nan"" as a host-port pair.
E0814 19:46:47.654333018   18949 completion_queue.cc:244]    assertion failed: queue.num_items() == 0
Aborted

```
```
"
61567,Abort when running tensorflow.python.ops.gen_array_ops.mirror_pad_grad,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

due to NEGATIVE LARGE TENSOR

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_array_ops
try:
  arg_0_tensor = tf.constant(-17, shape=[1, 4, 7, 1], dtype=tf.int64,)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.constant(-43871863081293, shape=[4, 2], dtype=tf.int32,)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2 = ""REFLECT""
  out = gen_array_ops.mirror_pad_grad(arg_0,arg_1,arg_2,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 19:39:35.212387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.230641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.230799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.231097: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 19:39:35.231945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.232079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.232187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.287820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.287970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.288088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.288181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4039 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 19:39:35.341105: F tensorflow/core/framework/tensor_shape.cc:404] Check failed: 0 <= new_num_elements (0 vs. -1)
Aborted

```
```
"
61566,Crash when running tensorflow.python.ops.gen_array_ops.lower_bound,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to mismatch between input tensors

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_array_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([2, 3], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([2], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2 = tf.int32
      arg_3 = False
      out = gen_array_ops.lower_bound(arg_0,arg_1,arg_2,arg_3,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.int32
      gen_array_ops.lower_bound(arg_0,arg_1,arg_2,arg_3,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 19:37:09.497421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:37:09.514487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:37:09.514629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:37:09.514917: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 19:37:09.515817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:37:09.515932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:37:09.516035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:37:09.573389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:37:09.573531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:37:09.573629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:37:09.573710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4043 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
free(): invalid pointer
Aborted

```
```
"
61564,Abort when running tensorflow.python.ops.gen_math_ops.sobol_sample,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to an empty input argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  arg_0 = 2
  arg_1 = 4
  arg_2 = [()]
  dtype = None
  out = gen_math_ops.sobol_sample(arg_0,arg_1,arg_2,dtype=dtype,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 19:14:54.117266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.134520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.134663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.135032: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 19:14:54.135857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.135969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.136093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.189645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.189788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.189886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.189967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4373 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 19:14:54.237059: F tensorflow/core/framework/tensor.cc:733] Check failed: 1 == NumElements() (1 vs. 0)Must have a one element tensor
Aborted

```
```
"
61563,Crash when running tensorflow.python.ops.gen_list_ops.tensor_list_reserve,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to the large integer value

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_list_ops
try:
  element_shape_tensor = tf.random.uniform([0], minval=-256, maxval=257, dtype=tf.int32)
  element_shape = tf.identity(element_shape_tensor)
  num_elements = 1250999896764
  element_dtype = tf.float64
  out = gen_list_ops.tensor_list_reserve(element_shape=element_shape,num_elements=num_elements,element_dtype=element_dtype,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 19:12:02.093783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:12:02.111206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:12:02.111351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:12:02.111663: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 19:12:02.113096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:12:02.113270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:12:02.113369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:12:02.167538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:12:02.167673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:12:02.167772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:12:02.167855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4163 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
"
61562,Floating point exception when running tensorflow.python.ops.gen_array_ops.depth_to_space,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to Large integer value

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_array_ops
try:
  arg_0_tensor = tf.random.uniform([3, 2, 3, 4], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = 536870912
  arg_2 = ""NHWC""
  out = gen_array_ops.depth_to_space(arg_0,arg_1,arg_2,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 19:09:51.665038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:09:51.681990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:09:51.682143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:09:51.682456: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 19:09:51.683591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:09:51.683711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:09:51.683809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:09:51.738771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:09:51.738915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:09:51.739018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:09:51.739104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4184 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Floating point exception

```
```
"
61561,Abort when running tensorflow.python.ops.gen_sparse_ops.sparse_slice,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to Large List Elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_sparse_ops
try:
  indices_0 = []
  indices = [indices_0,]
  values_0 = 0
  values = [values_0,]
  shape_0 = 1
  shape_1 = 1
  shape = [shape_0,shape_1,]
  start_0 = 4611686018427387904
  start_1 = -1
  start = [start_0,start_1,]
  size_0 = 4611686018427387904
  size_1 = 4611686018427387904
  size = [size_0,size_1,]
  out = gen_sparse_ops.sparse_slice(indices=indices,values=values,shape=shape,start=start,size=size,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 19:04:18.473689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.490933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.491133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.491455: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 19:04:18.492102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.492212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.492308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.560188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.560326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.560424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.560508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4029 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 19:04:18.619230: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2023-08-14 19:04:18.619470: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1
Aborted

```
```
"
61560,Crash when running tensorflow.python.ops.gen_math_ops._histogram_fixed_width,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to Negative Float values

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_0_0 = -1.0
      arg_0_0_1 = 0.0
      arg_0_0_2 = 1.5
      arg_0_0 = [arg_0_0_0,arg_0_0_1,arg_0_0_2,]
      arg_0_1_0 = 2.0
      arg_0_1_1 = 5.0
      arg_0_1_2 = 15
      arg_0_1 = [arg_0_1_0,arg_0_1_1,arg_0_1_2,]
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1_0 = -1e+40
      arg_1_1 = -0.0001
      arg_1 = [arg_1_0,arg_1_1,]
      arg_2 = 5
      dtype = tf.int32
      out = gen_math_ops._histogram_fixed_width(arg_0,arg_1,arg_2,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0_0 = [arg_0_0_0,arg_0_0_1,arg_0_0_2,]
      arg_0_1 = [arg_0_1_0,arg_0_1_1,arg_0_1_2,]
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1 = [arg_1_0,arg_1_1,]
      dtype = tf.int32
      gen_math_ops._histogram_fixed_width(arg_0,arg_1,arg_2,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 19:02:34.037518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:02:34.054273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:02:34.054440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:02:34.054728: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 19:02:34.055658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:02:34.055769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:02:34.055866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:02:34.125810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:02:34.125958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:02:34.126058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:02:34.126209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4061 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
"
61559,Abort when running tensorflow.python.ops.math_ops.sobol_sample,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.0

### Custom code

No

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to empty input value

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import math_ops
try:
  arg_0 = 10
  arg_1 = 50
  arg_2 = [()]
  dtype = None
  out = math_ops.sobol_sample(arg_0,arg_1,arg_2,dtype=dtype,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 18:57:53.081061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:57:53.098644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:57:53.098788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:57:53.099088: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 18:57:53.099985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:57:53.100112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:57:53.100209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:57:53.168279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:57:53.168422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:57:53.168524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:57:53.168607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3574 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 18:57:53.212928: F tensorflow/core/framework/tensor.cc:733] Check failed: 1 == NumElements() (1 vs. 0)Must have a one element tensor
Aborted

```
```
"
61558,Crash when running tensorflow.python.ops.gen_data_flow_ops.record_input,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to very large integer values

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_data_flow_ops
try:
  file_pattern = ""/tmp/record_input_testzsuyf9ap/tmpsqjnp5o1/basic.*""
  file_buffer_size = 1
  file_parallelism = 1676240524292489355
  file_shuffle_shift_ratio = 125091515651
  batch_size = 1
  file_random_seed = 125091515651
  compression_type = ""GZIP""
  out = gen_data_flow_ops.record_input(file_pattern=file_pattern,file_buffer_size=file_buffer_size,file_parallelism=file_parallelism,file_shuffle_shift_ratio=file_shuffle_shift_ratio,batch_size=batch_size,file_random_seed=file_random_seed,compression_type=compression_type,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-14 18:25:00.245344: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-14 18:25:01.275751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.292863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.293001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.294465: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 18:25:01.295740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.295849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.295947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.362610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.363085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.363185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.363266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4105 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
"
61557,Crash when running tensorflow.python.ops.list_ops.tensor_list_from_tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to very large input tensor

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import list_ops
try:
  try:
    with tf.device('/CPU'):
      tensor_tensor = tf.constant(True, shape=[1610612736,36028797018963968])
      tensor = tf.identity(tensor_tensor)
      element_shape = None
      out = list_ops.tensor_list_from_tensor(tensor=tensor,element_shape=element_shape,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      tensor = tf.identity(tensor_tensor)
      tensor = tf.cast(tensor, tf.bool)
      element_shape = None
      list_ops.tensor_list_from_tensor(tensor=tensor,element_shape=element_shape,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 18:16:26.168565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-14 18:16:26.732779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:16:26.750396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:16:26.750545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:16:26.750866: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 18:16:26.751699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:16:26.751813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:16:26.751913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:16:26.812496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:16:26.812638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:16:26.812740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:16:26.812825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4042 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
"
61556,Crash when running tensorflow.python.eager.context.add_function,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to Feeding None value

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.eager import context
try:
  arg_0 = None
  out = context.add_function(arg_0,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 18:09:38.431471: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-14 18:09:39.008742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:09:39.026368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:09:39.026511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:09:39.026801: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 18:09:39.027611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:09:39.027718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:09:39.027812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:09:39.086029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:09:39.086174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:09:39.086268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:09:39.086349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4023 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
"
61555,Segmentation fault when running tensorflow.python.ops.gen_math_ops._histogram_fixed_width,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to negative float argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_0_0 = -1.0
      arg_0_0_1 = 0.0
      arg_0_0_2 = 1.5
      arg_0_0 = [arg_0_0_0,arg_0_0_1,arg_0_0_2,]
      arg_0_1_0 = 2.0
      arg_0_1_1 = 5.0
      arg_0_1_2 = 15
      arg_0_1 = [arg_0_1_0,arg_0_1_1,arg_0_1_2,]
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1_0 = -1.7976931348623157e+308
      arg_1_1 = -1.4013e-45
      arg_1 = [arg_1_0,arg_1_1,]
      arg_2 = 5
      dtype = tf.int32
      out = gen_math_ops._histogram_fixed_width(arg_0,arg_1,arg_2,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0_0 = [arg_0_0_0,arg_0_0_1,arg_0_0_2,]
      arg_0_1 = [arg_0_1_0,arg_0_1_1,arg_0_1_2,]
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1 = [arg_1_0,arg_1_1,]
      dtype = tf.int32
      gen_math_ops._histogram_fixed_width(arg_0,arg_1,arg_2,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 17:17:24.916189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-14 17:17:25.460939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.478482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.478631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.478922: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 17:17:25.479696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.479802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.479896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.548142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.548279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.548375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.548456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3932 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault
(fuzzer_tf_2.11.0) n
```
```
"
61554,segmentation fault when running tensorflow.python.ops.nn_ops.fractional_max_pool,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to large input tensor

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.random.uniform([5, 20, 20, 3], dtype=tf.float64)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 4.0
  arg_1_1 = 1.5
  arg_1_2 = 1.5
  arg_1_3 = 1
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]
  seed = 1
  seed2 = 1
  deterministic = True
  out = nn_ops.fractional_max_pool(arg_0,arg_1,seed=seed,seed2=seed2,deterministic=deterministic,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-14 16:35:31.365070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-14 16:35:31.920926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 16:35:31.937761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 16:35:31.937904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 16:35:31.938205: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 16:35:31.939360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 16:35:31.939470: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 16:35:31.939568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 16:35:32.006452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 16:35:32.006593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 16:35:32.006693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 16:35:32.006775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4461 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
WARNING:tensorflow:From /home/nimashiri/anaconda3/envs/fuzzer_tf_2.11.0/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: fractional_max_pool (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` and `deterministic` args are deprecated.  Use fractional_max_pool_v2.
Segmentation fault

```
```
"
61552,Avoid partially saved ckpt from preempted device (e.g. TPU),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf.2.11

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I accidentally discovered from a TPU preemption that my ckpt cannot be used to resume training (the preemption occurs during the process of saving ckpt). The error is that some weights cannot be matched. This phenomenon is happening for the first time and has never happened before.

### Standalone code to reproduce the issue

```shell
It is very hard to reproduce because it must be preemption while saving the ckpt.
```


### Relevant log output

_No response_"
61551,Protobuf 4.24.0 break tensorflow and causes segfault with TF 2.12,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

TF 2.12

### Custom code

No

### OS platform and distribution

Linux,Ubuntu

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

With source build as well as pip install TF execution models fails with segfaults and the root cause seems to be because of protobuf version 4.24.0. It works fine till protobuf 4.23.4. adding a constraint in setup.py can be an immediate fix.

### Standalone code to reproduce the issue

```shell
Any CNN model execution causes the workflow to fail.
```


### Relevant log output

```shell
2023-08-14 08:01:56.274633: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
WARNING:tensorflow:From /home/anaconda3/envs/test/lib/python3.8/site-packages/tensorflow/python/tools/strip_unused_lib.py:84: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.
scripts/benchmark_CNN.sh: line 243: 1488387 Segmentation fault
```
"
61550,UnicodeDecodeError when loading model from a path with Chinese characters.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

Windows 11 22h2

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Current behavior:
When I attempt to use tf.keras.models.load_model to load a saved model from a folder path that contains Chinese characters, TensorFlow throws a UnicodeDecodeError. This suggests that TensorFlow may not be parsing Chinese characters correctly in the folder path.

Expected behavior:
I expect tf.keras.models.load_model to be able to load the model correctly from any folder path, regardless of whether it contains Chinese characters or not. If there are limitations on the folder name, it should be clearly documented or provide a more descriptive error message.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Create a simple model
model = tf.keras.models.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
model.compile(optimizer='adam', loss='mse')
model.save('æ¨¡åž‹/')  # 'æ¨¡åž‹' is Chinese for 'model'

# Now, try to load the saved model
loaded_model = tf.keras.models.load_model('æ¨¡åž‹/')
```


### Relevant log output

```shell
Traceback (most recent call last):
...
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa1 in position 27: invalid start byte
```
"
61549,"Based on FP16, the network training of resnet50 was carried out, and the existence accuracy randomly converged to 0.76","### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.11

### Custom code

Yes

### OS platform and distribution

centos7.6

### Mobile device

_No response_

### Python version

3.7ã€3.8ã€3,ã€‚9

### Bazel version

5.3.0

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Using the inagenet dataset, on the resnet50 network, based on FP16 training with the model-2.11.0 code,  the network has random convergence, is this phenomenon known, is there any solution at present?  Is there any good way to debug this problem? Thanks.

### Standalone code to reproduce the issue

```shell
python3  /public/home/TF_test/rocm5.4/tf2.11/models-master_fp16/official/benchmark/models/resnet_imagenet_main.py      --num_gpus=4     --batch_size=512  --train_epochs=90  --use_synthetic_data=false    --data_dir=/public/software/apps/DeepLearning/Data/ImageNet-tensorflow/  --enable_checkpoint_and_export  --dtype=fp16   --model_dir=./Checkpoint/
```


### Relevant log output

```shell
I0717 10:42:45.093785 47025319389696 controller.py:291]  eval | step:  10009 | eval time:   74.7 sec | output: {'test_accuracy': 0.08882, 'test_loss': 1.2578498}
I0717 11:19:43.333361 47025319389696 controller.py:291]  eval | step:  20018 | eval time:   47.9 sec | output: {'test_accuracy': 0.18752, 'test_loss': 1.0495228}
I0717 11:56:49.300491 47025319389696 controller.py:291]  eval | step:  30027 | eval time:   48.0 sec | output: {'test_accuracy': 0.2584, 'test_loss': 0.9102226}
I0717 12:33:43.809209 47025319389696 controller.py:291]  eval | step:  40036 | eval time:   47.3 sec | output: {'test_accuracy': 0.33168, 'test_loss': 0.77455497}
I0717 13:10:37.399015 47025319389696 controller.py:291]  eval | step:  50045 | eval time:   48.6 sec | output: {'test_accuracy': 0.32974, 'test_loss': 0.7866249}
I0717 13:47:31.637742 47025319389696 controller.py:291]  eval | step:  60054 | eval time:   48.0 sec | output: {'test_accuracy': 0.32832, 'test_loss': 0.7928696}
I0717 14:24:25.244748 47025319389696 controller.py:291]  eval | step:  70063 | eval time:   47.4 sec | output: {'test_accuracy': 0.36952, 'test_loss': 0.7357325}
I0717 15:01:26.831945 47025319389696 controller.py:291]  eval | step:  80072 | eval time:   49.6 sec | output: {'test_accuracy': 0.33716, 'test_loss': 0.7839137}
I0717 15:39:10.716763 47025319389696 controller.py:291]  eval | step:  90081 | eval time:   51.2 sec | output: {'test_accuracy': 0.32314, 'test_loss': 0.82485163}
I0717 16:17:14.804115 47025319389696 controller.py:291]  eval | step:  100090 | eval time:   49.1 sec | output: {'test_accuracy': 0.36102, 'test_loss': 0.7599265}
I0717 16:54:38.272225 47025319389696 controller.py:291]  eval | step:  110099 | eval time:   49.5 sec | output: {'test_accuracy': 0.42732, 'test_loss': 0.6477533}
I0717 17:32:14.305328 47025319389696 controller.py:291]  eval | step:  120108 | eval time:   49.1 sec | output: {'test_accuracy': 0.3769, 'test_loss': 0.745124}
I0717 18:19:19.995545 47025319389696 controller.py:291]  eval | step:  130117 | eval time:  196.2 sec | output: {'test_accuracy': 0.43222, 'test_loss': 0.63742775}
I0717 19:01:38.542571 47025319389696 controller.py:291]  eval | step:  140126 | eval time:   49.1 sec | output: {'test_accuracy': 0.43536, 'test_loss': 0.63410264}
I0717 19:39:52.303068 47025319389696 controller.py:291]  eval | step:  150135 | eval time:   46.7 sec | output: {'test_accuracy': 0.43782, 'test_loss': 0.6309778}
I0717 20:17:52.557711 47025319389696 controller.py:291]  eval | step:  160144 | eval time:   49.5 sec | output: {'test_accuracy': 0.44036, 'test_loss': 0.6212317}
I0717 20:55:54.236690 47025319389696 controller.py:291]  eval | step:  170153 | eval time:   47.7 sec | output: {'test_accuracy': 0.4211, 'test_loss': 0.6638591}
I0717 21:33:46.721763 47025319389696 controller.py:291]  eval | step:  180162 | eval time:   48.7 sec | output: {'test_accuracy': 0.41044, 'test_loss': 0.6806779}
I0717 22:11:56.432586 47025319389696 controller.py:291]  eval | step:  190171 | eval time:   48.9 sec | output: {'test_accuracy': 0.4227, 'test_loss': 0.6592407}
I0717 22:49:48.895009 47025319389696 controller.py:291]  eval | step:  200180 | eval time:   47.1 sec | output: {'test_accuracy': 0.41614, 'test_loss': 0.6689654}
I0717 23:27:56.641305 47025319389696 controller.py:291]  eval | step:  210189 | eval time:   49.0 sec | output: {'test_accuracy': 0.43086, 'test_loss': 0.6471556}
I0718 00:06:08.226011 47025319389696 controller.py:291]  eval | step:  220198 | eval time:   49.0 sec | output: {'test_accuracy': 0.43288, 'test_loss': 0.651372}
I0718 00:43:44.846733 47025319389696 controller.py:291]  eval | step:  230207 | eval time:   48.6 sec | output: {'test_accuracy': 0.44876, 'test_loss': 0.61514467}
I0718 01:21:01.860804 47025319389696 controller.py:291]  eval | step:  240216 | eval time:   47.8 sec | output: {'test_accuracy': 0.39994, 'test_loss': 0.6896438}
I0718 01:58:26.185002 47025319389696 controller.py:291]  eval | step:  250225 | eval time:   49.3 sec | output: {'test_accuracy': 0.43974, 'test_loss': 0.63887787}
I0718 02:35:51.395891 47025319389696 controller.py:291]  eval | step:  260234 | eval time:   48.7 sec | output: {'test_accuracy': 0.42148, 'test_loss': 0.65387714}
I0718 03:12:58.957236 47025319389696 controller.py:291]  eval | step:  270243 | eval time:   48.0 sec | output: {'test_accuracy': 0.42988, 'test_loss': 0.64366835}
I0718 03:50:05.619722 47025319389696 controller.py:291]  eval | step:  280252 | eval time:   49.1 sec | output: {'test_accuracy': 0.4292, 'test_loss': 0.64544094}
I0718 04:27:19.267962 47025319389696 controller.py:291]  eval | step:  290261 | eval time:   47.7 sec | output: {'test_accuracy': 0.43688, 'test_loss': 0.633886}
I0718 05:04:16.388976 47025319389696 controller.py:291]  eval | step:  300270 | eval time:   47.0 sec | output: {'test_accuracy': 0.4378, 'test_loss': 0.6427796}
I0718 05:41:28.633920 47025319389696 controller.py:291]  eval | step:  310279 | eval time:   47.1 sec | output: {'test_accuracy': 0.60604, 'test_loss': 0.41581097}
I0718 06:18:23.006313 47025319389696 controller.py:291]  eval | step:  320288 | eval time:   47.6 sec | output: {'test_accuracy': 0.60958, 'test_loss': 0.41247183}
I0718 06:55:19.940962 47025319389696 controller.py:291]  eval | step:  330297 | eval time:   47.5 sec | output: {'test_accuracy': 0.62418, 'test_loss': 0.3925642}
I0718 07:32:11.406366 47025319389696 controller.py:291]  eval | step:  340306 | eval time:   49.3 sec | output: {'test_accuracy': 0.60878, 'test_loss': 0.41140762}
I0718 08:09:13.098378 47025319389696 controller.py:291]  eval | step:  350315 | eval time:   46.3 sec | output: {'test_accuracy': 0.61416, 'test_loss': 0.40241298}
I0718 08:46:11.470759 47025319389696 controller.py:291]  eval | step:  360324 | eval time:   48.7 sec | output: {'test_accuracy': 0.61666, 'test_loss': 0.40103617}
I0718 09:23:19.292029 47025319389696 controller.py:291]  eval | step:  370333 | eval time:   47.6 sec | output: {'test_accuracy': 0.64238, 'test_loss': 0.3717267}
I0718 09:59:58.652599 47025319389696 controller.py:291]  eval | step:  380342 | eval time:   46.9 sec | output: {'test_accuracy': 0.63504, 'test_loss': 0.37905875}
I0718 10:36:30.269412 47025319389696 controller.py:291]  eval | step:  390351 | eval time:   46.7 sec | output: {'test_accuracy': 0.62588, 'test_loss': 0.38983408}
I0718 11:12:59.710509 47025319389696 controller.py:291]  eval | step:  400360 | eval time:   47.5 sec | output: {'test_accuracy': 0.62752, 'test_loss': 0.39042756}
I0718 11:49:34.702973 47025319389696 controller.py:291]  eval | step:  410369 | eval time:   47.6 sec | output: {'test_accuracy': 0.63086, 'test_loss': 0.38619643}
I0718 12:26:10.658050 47025319389696 controller.py:291]  eval | step:  420378 | eval time:   46.4 sec | output: {'test_accuracy': 0.61492, 'test_loss': 0.40776753}
I0718 13:02:54.373109 47025319389696 controller.py:291]  eval | step:  430387 | eval time:   47.1 sec | output: {'test_accuracy': 0.61482, 'test_loss': 0.40698445}
I0718 13:39:56.424064 47025319389696 controller.py:291]  eval | step:  440396 | eval time:   46.6 sec | output: {'test_accuracy': 0.61064, 'test_loss': 0.40885204}
I0718 14:16:56.972168 47025319389696 controller.py:291]  eval | step:  450405 | eval time:   47.2 sec | output: {'test_accuracy': 0.6101, 'test_loss': 0.41232613}
I0718 14:53:49.710101 47025319389696 controller.py:291]  eval | step:  460414 | eval time:   50.2 sec | output: {'test_accuracy': 0.62022, 'test_loss': 0.39717302}
I0718 15:31:00.055065 47025319389696 controller.py:291]  eval | step:  470423 | eval time:   47.8 sec | output: {'test_accuracy': 0.63738, 'test_loss': 0.3755849}
I0718 16:26:43.115331 47025319389696 controller.py:291]  eval | step:  480432 | eval time:   48.6 sec | output: {'test_accuracy': 0.63156, 'test_loss': 0.38420147}
I0718 17:14:57.232034 47025319389696 controller.py:291]  eval | step:  490441 | eval time:   47.9 sec | output: {'test_accuracy': 0.61762, 'test_loss': 0.40265194}
I0718 18:08:04.532505 47025319389696 controller.py:291]  eval | step:  500450 | eval time:   47.4 sec | output: {'test_accuracy': 0.63422, 'test_loss': 0.3809521}
I0718 18:45:18.862020 47025319389696 controller.py:291]  eval | step:  510459 | eval time:   50.4 sec | output: {'test_accuracy': 0.62098, 'test_loss': 0.39487627}
I0718 19:22:18.786072 47025319389696 controller.py:291]  eval | step:  520468 | eval time:   47.2 sec | output: {'test_accuracy': 0.61512, 'test_loss': 0.40484685}
I0718 19:59:17.472421 47025319389696 controller.py:291]  eval | step:  530477 | eval time:   47.3 sec | output: {'test_accuracy': 0.63776, 'test_loss': 0.37440324}
I0718 20:46:59.059379 47025319389696 controller.py:291]  eval | step:  540486 | eval time:  203.9 sec | output: {'test_accuracy': 0.62722, 'test_loss': 0.39110208}
I0718 21:45:11.991321 47025319389696 controller.py:291]  eval | step:  550495 | eval time:   47.9 sec | output: {'test_accuracy': 0.63562, 'test_loss': 0.38335004}
I0718 22:22:17.513960 47025319389696 controller.py:291]  eval | step:  560504 | eval time:   46.5 sec | output: {'test_accuracy': 0.61826, 'test_loss': 0.40100682}
I0718 22:59:05.393987 47025319389696 controller.py:291]  eval | step:  570513 | eval time:   46.1 sec | output: {'test_accuracy': 0.63496, 'test_loss': 0.3817627}
I0718 23:36:01.059101 47025319389696 controller.py:291]  eval | step:  580522 | eval time:   47.2 sec | output: {'test_accuracy': 0.61354, 'test_loss': 0.40860355}
I0719 00:12:57.767053 47025319389696 controller.py:291]  eval | step:  590531 | eval time:   48.7 sec | output: {'test_accuracy': 0.62766, 'test_loss': 0.3902482}
I0719 00:50:08.519172 47025319389696 controller.py:291]  eval | step:  600540 | eval time:   47.3 sec | output: {'test_accuracy': 0.62992, 'test_loss': 0.38722458}
I0719 01:27:14.241234 47025319389696 controller.py:291]  eval | step:  610549 | eval time:   49.5 sec | output: {'test_accuracy': 0.6785, 'test_loss': 0.3319877}
I0719 02:04:12.080565 47025319389696 controller.py:291]  eval | step:  620558 | eval time:   47.3 sec | output: {'test_accuracy': 0.68068, 'test_loss': 0.3297141}
I0719 02:41:31.931695 47025319389696 controller.py:291]  eval | step:  630567 | eval time:   47.5 sec | output: {'test_accuracy': 0.67544, 'test_loss': 0.33620217}
I0719 03:18:53.615640 47025319389696 controller.py:291]  eval | step:  640576 | eval time:   47.8 sec | output: {'test_accuracy': 0.67902, 'test_loss': 0.32960042}
I0719 03:56:03.865268 47025319389696 controller.py:291]  eval | step:  650585 | eval time:   47.2 sec | output: {'test_accuracy': 0.68312, 'test_loss': 0.32697994}
I0719 04:33:13.600398 47025319389696 controller.py:291]  eval | step:  660594 | eval time:   49.1 sec | output: {'test_accuracy': 0.68692, 'test_loss': 0.3218242}
I0719 05:10:26.761108 47025319389696 controller.py:291]  eval | step:  670603 | eval time:   47.5 sec | output: {'test_accuracy': 0.68098, 'test_loss': 0.33020604}
I0719 05:47:24.072698 47025319389696 controller.py:291]  eval | step:  680612 | eval time:   47.8 sec | output: {'test_accuracy': 0.68784, 'test_loss': 0.32333767}
I0719 06:24:23.891359 47025319389696 controller.py:291]  eval | step:  690621 | eval time:   47.1 sec | output: {'test_accuracy': 0.68598, 'test_loss': 0.32332215}
I0719 07:01:34.794637 47025319389696 controller.py:291]  eval | step:  700630 | eval time:   48.9 sec | output: {'test_accuracy': 0.68566, 'test_loss': 0.3250208}
I0719 07:38:47.115363 47025319389696 controller.py:291]  eval | step:  710639 | eval time:   47.2 sec | output: {'test_accuracy': 0.68696, 'test_loss': 0.32350788}
I0719 08:15:57.504423 47025319389696 controller.py:291]  eval | step:  720648 | eval time:   52.0 sec | output: {'test_accuracy': 0.68832, 'test_loss': 0.3217886}
I0719 08:53:14.727922 47025319389696 controller.py:291]  eval | step:  730657 | eval time:   49.9 sec | output: {'test_accuracy': 0.6852, 'test_loss': 0.32565758}
I0719 09:30:20.804430 47025319389696 controller.py:291]  eval | step:  740666 | eval time:   49.2 sec | output: {'test_accuracy': 0.68326, 'test_loss': 0.33065847}
I0719 10:07:20.642869 47025319389696 controller.py:291]  eval | step:  750675 | eval time:   48.0 sec | output: {'test_accuracy': 0.68372, 'test_loss': 0.32719776}
I0719 10:46:23.875730 47025319389696 controller.py:291]  eval | step:  760684 | eval time:   47.9 sec | output: {'test_accuracy': 0.68192, 'test_loss': 0.32962936}
I0719 11:23:17.199512 47025319389696 controller.py:291]  eval | step:  770693 | eval time:   47.2 sec | output: {'test_accuracy': 0.68262, 'test_loss': 0.32934195}
I0719 12:00:04.367842 47025319389696 controller.py:291]  eval | step:  780702 | eval time:   46.9 sec | output: {'test_accuracy': 0.67706, 'test_loss': 0.3367395}
I0719 12:37:04.218576 47025319389696 controller.py:291]  eval | step:  790711 | eval time:   49.9 sec | output: {'test_accuracy': 0.68592, 'test_loss': 0.32515442}
I0719 13:14:02.169136 47025319389696 controller.py:291]  eval | step:  800720 | eval time:   49.5 sec | output: {'test_accuracy': 0.6774, 'test_loss': 0.33403492}
I0719 13:51:13.950588 47025319389696 controller.py:291]  eval | step:  810729 | eval time:   48.3 sec | output: {'test_accuracy': 0.68796, 'test_loss': 0.32154444}
I0719 14:28:19.452734 47025319389696 controller.py:291]  eval | step:  820738 | eval time:   47.6 sec | output: {'test_accuracy': 0.68378, 'test_loss': 0.32751894}
I0719 15:05:49.973363 47025319389696 controller.py:291]  eval | step:  830747 | eval time:   47.5 sec | output: {'test_accuracy': 0.6856, 'test_loss': 0.3268724}
I0719 15:42:51.213348 47025319389696 controller.py:291]  eval | step:  840756 | eval time:   46.4 sec | output: {'test_accuracy': 0.68698, 'test_loss': 0.32407942}
I0719 16:19:54.159197 47025319389696 controller.py:291]  eval | step:  850765 | eval time:   48.8 sec | output: {'test_accuracy': 0.6875, 'test_loss': 0.32387343}
I0719 16:57:06.514332 47025319389696 controller.py:291]  eval | step:  860774 | eval time:   48.9 sec | output: {'test_accuracy': 0.6877, 'test_loss': 0.32420784}
I0719 17:34:12.212647 47025319389696 controller.py:291]  eval | step:  870783 | eval time:   48.5 sec | output: {'test_accuracy': 0.688, 'test_loss': 0.3242049}
I0719 18:11:14.854687 47025319389696 controller.py:291]  eval | step:  880792 | eval time:   48.6 sec | output: {'test_accuracy': 0.6882, 'test_loss': 0.324394}
I0719 18:48:31.401537 47025319389696 controller.py:291]  eval | step:  890801 | eval time:   49.7 sec | output: {'test_accuracy': 0.68516, 'test_loss': 0.32722312}
I0719 19:25:37.033689 47025319389696 controller.py:291]  eval | step:  900810 | eval time:   48.7 sec | output: {'test_accuracy': 0.68694, 'test_loss': 0.32629344}
 eval | step:  10009 | eval time:   74.7 sec | output: {'test_accuracy': 0.08882, 'test_loss': 1.2578498}
 eval | step:  20018 | eval time:   47.9 sec | output: {'test_accuracy': 0.18752, 'test_loss': 1.0495228}
 eval | step:  30027 | eval time:   48.0 sec | output: {'test_accuracy': 0.2584, 'test_loss': 0.9102226}
 eval | step:  40036 | eval time:   47.3 sec | output: {'test_accuracy': 0.33168, 'test_loss': 0.77455497}
 eval | step:  50045 | eval time:   48.6 sec | output: {'test_accuracy': 0.32974, 'test_loss': 0.7866249}
 eval | step:  60054 | eval time:   48.0 sec | output: {'test_accuracy': 0.32832, 'test_loss': 0.7928696}
 eval | step:  70063 | eval time:   47.4 sec | output: {'test_accuracy': 0.36952, 'test_loss': 0.7357325}
 eval | step:  80072 | eval time:   49.6 sec | output: {'test_accuracy': 0.33716, 'test_loss': 0.7839137}
 eval | step:  90081 | eval time:   51.2 sec | output: {'test_accuracy': 0.32314, 'test_loss': 0.82485163}
 eval | step:  100090 | eval time:   49.1 sec | output: {'test_accuracy': 0.36102, 'test_loss': 0.7599265}
 eval | step:  110099 | eval time:   49.5 sec | output: {'test_accuracy': 0.42732, 'test_loss': 0.6477533}
 eval | step:  120108 | eval time:   49.1 sec | output: {'test_accuracy': 0.3769, 'test_loss': 0.745124}
 eval | step:  130117 | eval time:  196.2 sec | output: {'test_accuracy': 0.43222, 'test_loss': 0.63742775}
 eval | step:  140126 | eval time:   49.1 sec | output: {'test_accuracy': 0.43536, 'test_loss': 0.63410264}
 eval | step:  150135 | eval time:   46.7 sec | output: {'test_accuracy': 0.43782, 'test_loss': 0.6309778}
 eval | step:  160144 | eval time:   49.5 sec | output: {'test_accuracy': 0.44036, 'test_loss': 0.6212317}
 eval | step:  170153 | eval time:   47.7 sec | output: {'test_accuracy': 0.4211, 'test_loss': 0.6638591}
 eval | step:  180162 | eval time:   48.7 sec | output: {'test_accuracy': 0.41044, 'test_loss': 0.6806779}
 eval | step:  190171 | eval time:   48.9 sec | output: {'test_accuracy': 0.4227, 'test_loss': 0.6592407}
 eval | step:  200180 | eval time:   47.1 sec | output: {'test_accuracy': 0.41614, 'test_loss': 0.6689654}
 eval | step:  210189 | eval time:   49.0 sec | output: {'test_accuracy': 0.43086, 'test_loss': 0.6471556}
 eval | step:  220198 | eval time:   49.0 sec | output: {'test_accuracy': 0.43288, 'test_loss': 0.651372}
 eval | step:  230207 | eval time:   48.6 sec | output: {'test_accuracy': 0.44876, 'test_loss': 0.61514467}
 eval | step:  240216 | eval time:   47.8 sec | output: {'test_accuracy': 0.39994, 'test_loss': 0.6896438}
 eval | step:  250225 | eval time:   49.3 sec | output: {'test_accuracy': 0.43974, 'test_loss': 0.63887787}
 eval | step:  260234 | eval time:   48.7 sec | output: {'test_accuracy': 0.42148, 'test_loss': 0.65387714}
 eval | step:  270243 | eval time:   48.0 sec | output: {'test_accuracy': 0.42988, 'test_loss': 0.64366835}
 eval | step:  280252 | eval time:   49.1 sec | output: {'test_accuracy': 0.4292, 'test_loss': 0.64544094}
 eval | step:  290261 | eval time:   47.7 sec | output: {'test_accuracy': 0.43688, 'test_loss': 0.633886}
 eval | step:  300270 | eval time:   47.0 sec | output: {'test_accuracy': 0.4378, 'test_loss': 0.6427796}
 eval | step:  310279 | eval time:   47.1 sec | output: {'test_accuracy': 0.60604, 'test_loss': 0.41581097}
 eval | step:  320288 | eval time:   47.6 sec | output: {'test_accuracy': 0.60958, 'test_loss': 0.41247183}
 eval | step:  330297 | eval time:   47.5 sec | output: {'test_accuracy': 0.62418, 'test_loss': 0.3925642}
 eval | step:  340306 | eval time:   49.3 sec | output: {'test_accuracy': 0.60878, 'test_loss': 0.41140762}
 eval | step:  350315 | eval time:   46.3 sec | output: {'test_accuracy': 0.61416, 'test_loss': 0.40241298}
 eval | step:  360324 | eval time:   48.7 sec | output: {'test_accuracy': 0.61666, 'test_loss': 0.40103617}
 eval | step:  370333 | eval time:   47.6 sec | output: {'test_accuracy': 0.64238, 'test_loss': 0.3717267}
 eval | step:  380342 | eval time:   46.9 sec | output: {'test_accuracy': 0.63504, 'test_loss': 0.37905875}
 eval | step:  390351 | eval time:   46.7 sec | output: {'test_accuracy': 0.62588, 'test_loss': 0.38983408}
 eval | step:  400360 | eval time:   47.5 sec | output: {'test_accuracy': 0.62752, 'test_loss': 0.39042756}
 eval | step:  410369 | eval time:   47.6 sec | output: {'test_accuracy': 0.63086, 'test_loss': 0.38619643}
 eval | step:  420378 | eval time:   46.4 sec | output: {'test_accuracy': 0.61492, 'test_loss': 0.40776753}
 eval | step:  430387 | eval time:   47.1 sec | output: {'test_accuracy': 0.61482, 'test_loss': 0.40698445}
 eval | step:  440396 | eval time:   46.6 sec | output: {'test_accuracy': 0.61064, 'test_loss': 0.40885204}
 eval | step:  450405 | eval time:   47.2 sec | output: {'test_accuracy': 0.6101, 'test_loss': 0.41232613}
 eval | step:  460414 | eval time:   50.2 sec | output: {'test_accuracy': 0.62022, 'test_loss': 0.39717302}
 eval | step:  470423 | eval time:   47.8 sec | output: {'test_accuracy': 0.63738, 'test_loss': 0.3755849}
 eval | step:  480432 | eval time:   48.6 sec | output: {'test_accuracy': 0.63156, 'test_loss': 0.38420147}
 eval | step:  490441 | eval time:   47.9 sec | output: {'test_accuracy': 0.61762, 'test_loss': 0.40265194}
 eval | step:  500450 | eval time:   47.4 sec | output: {'test_accuracy': 0.63422, 'test_loss': 0.3809521}
 eval | step:  510459 | eval time:   50.4 sec | output: {'test_accuracy': 0.62098, 'test_loss': 0.39487627}
 eval | step:  520468 | eval time:   47.2 sec | output: {'test_accuracy': 0.61512, 'test_loss': 0.40484685}
 eval | step:  530477 | eval time:   47.3 sec | output: {'test_accuracy': 0.63776, 'test_loss': 0.37440324}
 eval | step:  540486 | eval time:  203.9 sec | output: {'test_accuracy': 0.62722, 'test_loss': 0.39110208}
 eval | step:  550495 | eval time:   47.9 sec | output: {'test_accuracy': 0.63562, 'test_loss': 0.38335004}
 eval | step:  560504 | eval time:   46.5 sec | output: {'test_accuracy': 0.61826, 'test_loss': 0.40100682}
 eval | step:  570513 | eval time:   46.1 sec | output: {'test_accuracy': 0.63496, 'test_loss': 0.3817627}
 eval | step:  580522 | eval time:   47.2 sec | output: {'test_accuracy': 0.61354, 'test_loss': 0.40860355}
 eval | step:  590531 | eval time:   48.7 sec | output: {'test_accuracy': 0.62766, 'test_loss': 0.3902482}
 eval | step:  600540 | eval time:   47.3 sec | output: {'test_accuracy': 0.62992, 'test_loss': 0.38722458}
 eval | step:  610549 | eval time:   49.5 sec | output: {'test_accuracy': 0.6785, 'test_loss': 0.3319877}
 eval | step:  620558 | eval time:   47.3 sec | output: {'test_accuracy': 0.68068, 'test_loss': 0.3297141}
 eval | step:  630567 | eval time:   47.5 sec | output: {'test_accuracy': 0.67544, 'test_loss': 0.33620217}
 eval | step:  640576 | eval time:   47.8 sec | output: {'test_accuracy': 0.67902, 'test_loss': 0.32960042}
 eval | step:  650585 | eval time:   47.2 sec | output: {'test_accuracy': 0.68312, 'test_loss': 0.32697994}
 eval | step:  660594 | eval time:   49.1 sec | output: {'test_accuracy': 0.68692, 'test_loss': 0.3218242}
 eval | step:  670603 | eval time:   47.5 sec | output: {'test_accuracy': 0.68098, 'test_loss': 0.33020604}
 eval | step:  680612 | eval time:   47.8 sec | output: {'test_accuracy': 0.68784, 'test_loss': 0.32333767}
 eval | step:  690621 | eval time:   47.1 sec | output: {'test_accuracy': 0.68598, 'test_loss': 0.32332215}
 eval | step:  700630 | eval time:   48.9 sec | output: {'test_accuracy': 0.68566, 'test_loss': 0.3250208}
 eval | step:  710639 | eval time:   47.2 sec | output: {'test_accuracy': 0.68696, 'test_loss': 0.32350788}
 eval | step:  720648 | eval time:   52.0 sec | output: {'test_accuracy': 0.68832, 'test_loss': 0.3217886}
 eval | step:  730657 | eval time:   49.9 sec | output: {'test_accuracy': 0.6852, 'test_loss': 0.32565758}
 eval | step:  740666 | eval time:   49.2 sec | output: {'test_accuracy': 0.68326, 'test_loss': 0.33065847}
 eval | step:  750675 | eval time:   48.0 sec | output: {'test_accuracy': 0.68372, 'test_loss': 0.32719776}
 eval | step:  760684 | eval time:   47.9 sec | output: {'test_accuracy': 0.68192, 'test_loss': 0.32962936}
 eval | step:  770693 | eval time:   47.2 sec | output: {'test_accuracy': 0.68262, 'test_loss': 0.32934195}
 eval | step:  780702 | eval time:   46.9 sec | output: {'test_accuracy': 0.67706, 'test_loss': 0.3367395}
 eval | step:  790711 | eval time:   49.9 sec | output: {'test_accuracy': 0.68592, 'test_loss': 0.32515442}
 eval | step:  800720 | eval time:   49.5 sec | output: {'test_accuracy': 0.6774, 'test_loss': 0.33403492}
 eval | step:  810729 | eval time:   48.3 sec | output: {'test_accuracy': 0.68796, 'test_loss': 0.32154444}
 eval | step:  820738 | eval time:   47.6 sec | output: {'test_accuracy': 0.68378, 'test_loss': 0.32751894}
 eval | step:  830747 | eval time:   47.5 sec | output: {'test_accuracy': 0.6856, 'test_loss': 0.3268724}
 eval | step:  840756 | eval time:   46.4 sec | output: {'test_accuracy': 0.68698, 'test_loss': 0.32407942}
 eval | step:  850765 | eval time:   48.8 sec | output: {'test_accuracy': 0.6875, 'test_loss': 0.32387343}
 eval | step:  860774 | eval time:   48.9 sec | output: {'test_accuracy': 0.6877, 'test_loss': 0.32420784}
 eval | step:  870783 | eval time:   48.5 sec | output: {'test_accuracy': 0.688, 'test_loss': 0.3242049}
 eval | step:  880792 | eval time:   48.6 sec | output: {'test_accuracy': 0.6882, 'test_loss': 0.324394}
 eval | step:  890801 | eval time:   49.7 sec | output: {'test_accuracy': 0.68516, 'test_loss': 0.32722312}
 eval | step:  900810 | eval time:   48.7 sec | output: {'test_accuracy': 0.68694, 'test_loss': 0.32629344}
```
"
61548,I need TensorFlow 2.2.0 but it is removed how to find it?,"I need to install TensorFlow 2.2.0 

why? because this repo (https://github.com/GantMan/nsfw_model) is requesting it and now matter what I tried can't make it work with newer TensorFlows

How can I install TensorFlow 2.2.0  on Windows 10 and Python 3.10?

The error I am getting is and I am not able to fix it


```
(venv) G:\nsfw_model>python a.py
2023-08-14 00:33:39.437427: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 00:33:40.148302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21643 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6
2023-08-14 00:33:40.149891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9603 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:05:00.0, compute capability: 8.6
Traceback (most recent call last):
  File ""G:\nsfw_model\a.py"", line 13, in <module>
    print(predict.classify(model, 'test'))
  File ""G:\nsfw_model\nsfw_detector\predict.py"", line 67, in classify
    probs = classify_nd(model, images, predict_args)
  File ""G:\nsfw_model\nsfw_detector\predict.py"", line 77, in classify_nd
    model_preds = model.predict(nd_images, **predict_args)
  File ""G:\nsfw_model\venv\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""G:\nsfw_model\venv\lib\site-packages\keras\engine\training.py"", line 1997, in predict
    raise ValueError('Unexpected result of `predict_function` '
ValueError: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.
```


predict.py

```
#! python

import argparse
import json
from os import listdir
from os.path import isfile, join, exists, isdir, abspath

import numpy as np
import tensorflow as tf
from tensorflow import keras
import tensorflow_hub as hub


IMAGE_DIM = 299   # required/default image dimensionality

def load_images(image_paths, image_size, verbose=True):
    '''
    Function for loading images into numpy arrays for passing to model.predict
    inputs:
        image_paths: list of image paths to load
        image_size: size into which images should be resized
        verbose: show all of the image path and sizes loaded
    
    outputs:
        loaded_images: loaded images on which keras model can run predictions
        loaded_image_indexes: paths of images which the function is able to process
    
    '''
    loaded_images = []
    loaded_image_paths = []

    if isdir(image_paths):
        parent = abspath(image_paths)
        image_paths = [join(parent, f) for f in listdir(image_paths) if isfile(join(parent, f))]
    elif isfile(image_paths):
        image_paths = [image_paths]

    for img_path in image_paths:
        try:
            if verbose:
                print(img_path, ""size:"", image_size)
            image = keras.preprocessing.image.load_img(img_path, target_size=image_size)
            image = keras.preprocessing.image.img_to_array(image)
            image /= 255
            loaded_images.append(image)
            loaded_image_paths.append(img_path)
        except Exception as ex:
            print(""Image Load Failure: "", img_path, ex)
    
    return np.asarray(loaded_images), loaded_image_paths

def load_model(model_path):
    if model_path is None or not exists(model_path):
    	raise ValueError(""saved_model_path must be the valid directory of a saved model to load."")
    
    model = tf.keras.models.load_model(model_path, custom_objects={'KerasLayer': hub.KerasLayer},compile=False)
    return model


def classify(model, input_paths, image_dim=IMAGE_DIM, predict_args={}):
    """"""
    Classify given a model, input paths (could be single string), and image dimensionality.
    
    Optionally, pass predict_args that will be passed to tf.keras.Model.predict().
    """"""
    images, image_paths = load_images(input_paths, (image_dim, image_dim))
    probs = classify_nd(model, images, predict_args)
    return dict(zip(image_paths, probs))


def classify_nd(model, nd_images, predict_args={}):
    """"""
    Classify given a model, image array (numpy)
    
    Optionally, pass predict_args that will be passed to tf.keras.Model.predict().
    """"""
    model_preds = model.predict(nd_images, **predict_args)
    # preds = np.argsort(model_preds, axis = 1).tolist()
    
    categories = ['drawings', 'hentai', 'neutral', 'porn', 'sexy']

    probs = []
    for i, single_preds in enumerate(model_preds):
        single_probs = {}
        for j, pred in enumerate(single_preds):
            single_probs[categories[j]] = float(pred)
        probs.append(single_probs)
    return probs


def main(args=None):
    parser = argparse.ArgumentParser(
        description=""""""A script to perform NFSW classification of images"""""",
        epilog=""""""
        Launch with default model and a test image
            python nsfw_detector/predict.py --saved_model_path mobilenet_v2_140_224 --image_source test.jpg
    """""", formatter_class=argparse.RawTextHelpFormatter)
    
    submain = parser.add_argument_group('main execution and evaluation functionality')
    submain.add_argument('--image_source', dest='image_source', type=str, required=True, 
                            help='A directory of images or a single image to classify')
    submain.add_argument('--saved_model_path', dest='saved_model_path', type=str, required=True, 
                            help='The model to load')
    submain.add_argument('--image_dim', dest='image_dim', type=int, default=IMAGE_DIM,
                            help=""The square dimension of the model's input shape"")
    if args is not None:
        config = vars(parser.parse_args(args))
    else:
        config = vars(parser.parse_args())

    if config['image_source'] is None or not exists(config['image_source']):
    	raise ValueError(""image_source must be a valid directory with images or a single image to classify."")
    
    model = load_model(config['saved_model_path'])    
    image_preds = classify(model, config['image_source'], config['image_dim'])
    print(json.dumps(image_preds, indent=2), '\n')


if __name__ == ""__main__"":
	main()

```

"
61547,Segmentation fault when running tensorflow.python.ops.gen_image_ops.resize_bicubic,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to feeding a list with very large integer values.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_image_ops
try:
  images_tensor = tf.saturate_cast(tf.constant(-67, shape=[0, 1, 3, 2], dtype=tf.int64,),dtype=tf.uint16)
  images = tf.identity(images_tensor)
  size_0 = 536870912
  size_1 = 1250999896764
  size = [size_0,size_1,]
  align_corners = True
  half_pixel_centers = False
  out = gen_image_ops.resize_bicubic(images=images,size=size,align_corners=align_corners,half_pixel_centers=half_pixel_centers,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-13 02:18:02.134306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-13 02:18:03.002101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:18:03.022974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:18:03.023167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:18:03.023525: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-13 02:18:03.024137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:18:03.024303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:18:03.024449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:18:03.077913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:18:03.078079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:18:03.078189: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:18:03.078278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 761 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
"
61546,Segmentation fault when running tensorflow.python.framework.importer._PopulateTFImportGraphDefOptions,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to feeding None values

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.framework import importer
try:
  arg_0 = None
  arg_1 = ""A""
  arg_2 = None
  arg_3_0 = ""A""
  arg_3_1 = ""B""
  arg_3 = [arg_3_0,arg_3_1,]
  arg_4 = True
  out = importer._PopulateTFImportGraphDefOptions(arg_0,arg_1,arg_2,arg_3,arg_4,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-13 02:11:56.624855: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Segmentation fault

```
```
"
61545,Segmentation fault when running tensorflow.python.framework.importer._GatherReturnElements,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to feeding None value

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.framework import importer
try:
  arg_0_0 = ""A""
  arg_0 = [arg_0_0,]
  arg_1 = None
  arg_2 = None
  out = importer._GatherReturnElements(arg_0,arg_1,arg_2,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
023-08-13 02:10:14.149106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Segmentation fault

```
```
"
61544,Abort when running tensorflow.python.ops.gen_sparse_ops.sparse_split,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to zero integer argument. It would be best if you ran multiple times to see the abort.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_sparse_ops
try:
  arg_0 = 0
  arg_1_tensor = tf.random.uniform([14, 2], minval=-256, maxval=257, dtype=tf.int64)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2_tensor = tf.random.uniform([14], minval=-256, maxval=257, dtype=tf.int64)
  arg_2 = tf.identity(arg_2_tensor)
  arg_3_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int64)
  arg_3 = tf.identity(arg_3_tensor)
  arg_4 = 2
  out = gen_sparse_ops.sparse_split(arg_0,arg_1,arg_2,arg_3,arg_4,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-13 02:06:08.283954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-13 02:06:09.159348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.181201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.181463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.181927: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-13 02:06:09.182527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.182681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.182809: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.234678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.234880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.235018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.235129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 151 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-13 02:06:09.251615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 151.69M (159055872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-08-13 02:06:09.251958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 136.52M (143150336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-08-13 02:06:09.273191: E tensorflow/compiler/xla/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_MISALIGNED_ADDRESS: misaligned address
2023-08-13 02:06:09.273688: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1
Aborted

```
```
"
61543,Abort when running,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to a Negative Large Integer. The behavior is bizarre. It would be best if you ran multiple times to see the Abort.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  try:
    with tf.device('/CPU'):
      splits_tensor = tf.constant(-1000000, shape=[129, 1, 1], dtype=tf.float16,)
      splits = tf.identity(splits_tensor)
      values_tensor = tf.saturate_cast(tf.constant(-153, shape=[3456], dtype=tf.int64,),dtype=tf.uint64)
      values = tf.identity(values_tensor)
      weights_tensor = tf.saturate_cast(tf.constant(-1012756988, shape=[128, 27], dtype=tf.int64,),dtype=tf.int16)
      weights = tf.identity(weights_tensor)
      size = -3046875451
      out = gen_math_ops.ragged_bincount(splits=splits,values=values,weights=weights,size=size,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      splits = tf.identity(splits_tensor)
      splits = tf.cast(splits, tf.float16)
      values = tf.identity(values_tensor)
      values = tf.cast(values, tf.uint64)
      weights = tf.identity(weights_tensor)
      weights = tf.cast(weights, tf.int16)
      gen_math_ops.ragged_bincount(splits=splits,values=values,weights=weights,size=size,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-13 01:58:36.322308: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-13 01:58:37.120869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:58:37.139665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:58:37.139841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:58:37.140154: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-13 01:58:37.140677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:58:37.140794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:58:37.140896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:58:37.187990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:58:37.188156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:58:37.188267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:58:37.188356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 152 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Error:can't convert negative int to unsigned
2023-08-13 01:58:37.207611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 152.81M (160235520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-08-13 01:58:37.207886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 137.53M (144211968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-08-13 01:58:37.208138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 123.78M (129790976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-08-13 01:58:37.213816: F ./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred() 
Aborted
```
```
"
61542,Abort when running tensorflow.python.eager.remote.connect_to_remote_host,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

NaN string argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.eager import remote
try:
  try:
    with tf.device('/CPU'):
      arg_0 = ""nan""
      out = remote.connect_to_remote_host(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      remote.connect_to_remote_host(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-13 01:22:37.499369: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-13 01:22:38.459392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.480510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.480708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.481081: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-13 01:22:38.481707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.481844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.481961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.536637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.536859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.536991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.537094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1725 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-13 01:22:38.546718: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:589] INVALID_ARGUMENT: Could not interpret ""nan"" as a host-port pair.
E0813 01:22:38.546961566 1686085 completion_queue.cc:244]    assertion failed: queue.num_items() == 0
Aborted

```
```
"
61541,Floating point exception when running array_ops.depth_to_space,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to the large integer argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import array_ops
try:
  arg_0_tensor = tf.random.uniform([0, 2, 3, 12], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = 536870912
  out = array_ops.depth_to_space(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-13 00:39:16.584885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-13 00:39:17.905164: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:39:17.935652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:39:17.935873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:39:17.936238: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-13 00:39:17.936772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:39:17.936896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:39:17.937052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:39:17.993984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:39:17.994150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:39:17.994266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:39:17.994359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Floating point exception
```
```
"
61540,Abort when running tensorflow.python.ops.gen_array_ops.depth_to_space,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to very large integer argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_array_ops
try:
  arg_0_tensor = tf.random.uniform([3, 2, 3, 4], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = 2147483647
  arg_2 = ""NHWC""
  out = gen_array_ops.depth_to_space(arg_0,arg_1,arg_2,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
023-08-13 00:23:53.644564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-13 00:23:54.491071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:23:54.510564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:23:54.510736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:23:54.511051: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-13 00:23:54.511595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:23:54.511717: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:23:54.511830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:23:54.572398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:23:54.572634: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:23:54.572791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:23:54.572916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 153 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-13 00:23:54.594062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 153.88M (161349632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-08-13 00:23:54.594484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 138.49M (145214720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-08-13 00:23:54.600623: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected a non-negative size, got -2
Aborted

```
```
"
61539,Abort when running tensorflow.python.ops.gen_nn_ops.conv3d_backprop_input_v2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to input tensor with zero shape

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  input_sizes_0 = 2
  input_sizes_1 = 8
  input_sizes_2 = 8
  input_sizes_3 = 8
  input_sizes_4 = 5
  input_sizes = [input_sizes_0,input_sizes_1,input_sizes_2,input_sizes_3,input_sizes_4,]
  filter_tensor = tf.random.uniform([0, 1, 2, 5, 3], dtype=tf.float32)
  filter = tf.identity(filter_tensor)
  out_backprop_tensor = tf.random.uniform([2, 4, 4, 4, 3], dtype=tf.float32)
  out_backprop = tf.identity(out_backprop_tensor)
  strides_0 = 1
  strides_1 = 2
  strides_2 = 2
  strides_3 = 2
  strides_4 = 1
  strides = [strides_0,strides_1,strides_2,strides_3,strides_4,]
  padding = ""SAME""
  data_format = ""NDHWC""
  dilations_0 = 1
  dilations_1 = 1
  dilations_2 = 1
  dilations_3 = 1
  dilations_4 = 1
  dilations = [dilations_0,dilations_1,dilations_2,dilations_3,dilations_4,]
  out = gen_nn_ops.conv3d_backprop_input_v2(input_sizes=input_sizes,filter=filter,out_backprop=out_backprop,strides=strides,padding=padding,data_format=data_format,dilations=dilations,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-13 00:13:03.668988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-13 00:13:04.462547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.483261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.483427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.483905: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-13 00:13:04.484753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.484944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.485057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.585176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.585346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.585468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.585565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 744 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-13 00:13:04.615688: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)
Aborted

```
```
"
61538,tensorflow.python.autograph.operators.data_structures.tf_tensor_list_new,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to very large input tensor.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.autograph.operators import data_structures
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.constant(False, shape=[1610637938,36028797018963968])
      arg_0 = tf.identity(arg_0_tensor)
      out = data_structures.tf_tensor_list_new(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.bool)
      data_structures.tf_tensor_list_new(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-13 00:10:56.144934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-13 00:10:57.038182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:10:57.059056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:10:57.059246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:10:57.059594: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-13 00:10:57.060159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:10:57.060289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:10:57.060399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:10:57.112692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:10:57.112870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:10:57.112993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:10:57.113089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 751 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
"
61537,Segmentation fault when running tensorflow.python.ops.list_ops.tensor_list_reserve,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

When num_elements is very large

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import list_ops
try:
  element_shape_0 = 1
  element_shape = [element_shape_0,]
  num_elements = 1250999896764
  element_dtype = tf.float32
  out = list_ops.tensor_list_reserve(element_shape=element_shape,num_elements=num_elements,element_dtype=element_dtype,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-12 23:05:56.608429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-12 23:05:57.434965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:05:57.454466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:05:57.454644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:05:57.454958: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-12 23:05:57.455499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:05:57.455620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:05:57.455724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:05:57.504359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:05:57.504534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:05:57.504649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:05:57.504741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 151 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-12 23:05:57.518166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 151.44M (158793728 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
Segmentation fault

```
```
"
61536,Abort when running tensorflow.python.ops.linalg_ops.self_adjoint_eig,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

This behavior is very strange  and should not throw OOM error.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import linalg_ops
try:
  arg_0_tensor = tf.random.uniform([1, 1], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  out = linalg_ops.self_adjoint_eigvals(arg_0,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-12 23:02:34.613725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-12 23:02:35.612147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:02:35.634199: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:02:35.634612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:02:35.635038: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-12 23:02:35.635637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:02:35.635829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:02:35.635948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:02:35.639564: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:370] A non-primary context 0x5a7ae50 for device 0 exists before initializing the StreamExecutor. The primary context is now 0x7ffd00000000. We haven't verified StreamExecutor works with that.
2023-08-12 23:02:35.639662: F tensorflow/tsl/platform/statusor.cc:33] Attempting to fetch value instead of handling error INTERNAL: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 6216417280
Aborted

```
```
"
61535,check failure when running tensorflow.python.ops.nn_ops.conv3d_transpose_v2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

No

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

```
The following input combination causes check failure
```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.random.uniform([2, 5, 6, 4, 3], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.random.uniform([3, 3, 3, 2, 3], dtype=tf.float32)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2_0 = 2
  arg_2_1 = 11
  arg_2_2 = 13
  arg_2_3 = 9
  arg_2_4 = False
  arg_2 = [arg_2_0,arg_2_1,arg_2_2,arg_2_3,arg_2_4,]
  arg_3_0 = 1
  arg_3_1 = 2
  arg_3_2 = 2
  arg_3_3 = 2
  arg_3_4 = 1
  arg_3 = [arg_3_0,arg_3_1,arg_3_2,arg_3_3,arg_3_4,]
  padding = ""VALID""
  data_format = ""NDHWC""
  dilations = None
  out = nn_ops.conv3d_transpose_v2(arg_0,arg_1,arg_2,arg_3,padding=padding,data_format=data_format,dilations=dilations,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-06 06:58:26.108204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-06 06:58:26.851251: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nimashiri/anaconda3/envs/fuzzer_tf_2.11.0/lib/:/home/nimashiri/anaconda3/envs/fuzzer_tf_2.11.0/lib/python3.9/site-packages/nvidia/cudnn/lib:
2023-08-06 06:58:26.851492: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nimashiri/anaconda3/envs/fuzzer_tf_2.11.0/lib/:/home/nimashiri/anaconda3/envs/fuzzer_tf_2.11.0/lib/python3.9/site-packages/nvidia/cudnn/lib:
2023-08-06 06:58:26.851500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-06 06:58:27.585126: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nimashiri/anaconda3/envs/fuzzer_tf_2.11.0/lib/:/home/nimashiri/anaconda3/envs/fuzzer_tf_2.11.0/lib/python3.9/site-packages/nvidia/cudnn/lib:
2023-08-06 06:58:27.585372: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nimashiri/anaconda3/envs/fuzzer_tf_2.11.0/lib/:/home/nimashiri/anaconda3/envs/fuzzer_tf_2.11.0/lib/python3.9/site-packages/nvidia/cudnn/lib:
2023-08-06 06:58:27.585380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-06 06:59:12.781705: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:876] Check failed: cudnnSetConvolutionGroupCount( handle_.get(), convolution_descriptor.group_count()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
```
```
"
61534,Abort when running tensorflow.python.ops.nn_ops.conv2d_transpose,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Probably due to the large input tensor

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.constant(-1048576, shape=[2, 6, 4, 3], dtype=tf.float16,)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.constant(-1250999896764, shape=[0, 3, 2, 3], dtype=tf.float16,)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2_0 = 2
  arg_2_1 = 12
  arg_2_2 = 8
  arg_2_3 = 2
  arg_2 = [arg_2_0,arg_2_1,arg_2_2,arg_2_3,]
  strides_0 = 1
  strides_1 = 2
  strides_2 = 2
  strides_3 = 1
  strides = [strides_0,strides_1,strides_2,strides_3,]
  padding = ""SAME""
  out = nn_ops.conv2d_transpose(arg_0,arg_1,arg_2,strides=strides,padding=padding,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-12 15:32:22.021693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-12 15:32:22.879671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.899716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.899926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.900241: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-12 15:32:22.900789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.900913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.901022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.952156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.952340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.952462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.952557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-12 15:32:22.974261: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)
Aborted

```
```
"
61533,Segmentation fault when running tensorflow.python.eager.context.check_alive,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Probably due to invalid string argument.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.eager import context
try:
  try:
    with tf.device('/CPU'):
      arg_0 = ""/job:remote_device/replica:0/task:1""
      out = context.check_alive(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      context.check_alive(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
/cudnn/lib:
2023-08-12 15:27:22.605920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-12 15:27:23.511442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:27:23.533020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:27:23.533197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:27:23.533535: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-12 15:27:23.534136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:27:23.534277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:27:23.534432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:27:23.605501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:27:23.605704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:27:23.605831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:27:23.605930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 77 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
"
61532,segmentation fault when running tensorflow.python.eager.context.add_function,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Probably due to the NONE argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.eager import context
try:
  arg_0 = None
  out = context.add_function(arg_0,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-12 14:32:03.270708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-12 14:32:04.485549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.505262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.505426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.505739: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-12 14:32:04.506268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.506389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.506492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.556530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.556701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.556817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.556911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 739 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
"
61531,Segmentation fault when running tensorflow.python.framework.kernels.get_registered_kernels_for_op,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Probably due to feeding None argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.framework import kernels
try:
  arg_0 = None
  out = kernels.get_registered_kernels_for_op(arg_0,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-12 13:41:10.388491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Segmentation fault

```
```
"
61530,Abort when running tensorflow.python.ops.array_ops.quantize_and_dequantize_v2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Probably due to the feeding zero or empty input argument. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import array_ops
try:
  arg_0_tensor = tf.random.uniform([2, 3, 4, 5], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1 = 0
  arg_2 = [()]
  range_given = True
  round_mode = ""HALF_UP""
  axis = None
  out = array_ops.quantize_and_dequantize_v2(arg_0,arg_1,arg_2,range_given=range_given,round_mode=round_mode,axis=axis,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-12 12:56:52.291182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-12 12:56:53.170157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 12:56:53.190481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 12:56:53.190660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 12:56:53.190982: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-12 12:56:53.191527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 12:56:53.191651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 12:56:53.191767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 12:56:53.240721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 12:56:53.240904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 12:56:53.241022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 12:56:53.241116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 172 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-12 12:56:53.255445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 172.69M (181075968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-08-12 12:56:53.255723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 155.42M (162968576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-08-12 12:56:53.263318: F tensorflow/core/framework/tensor.cc:734] Check failed: 1 == NumElements() (1 vs. 0)Must have a one element tensor
Aborted

```
```
"
61529,quantized range of fake_quant_with_min_max_args is -2**num_bits + 1 to 2 ** num_bits.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I implemented `fake_quant_with_min_max_args` in numpy according to the source code to get the quantized values. However the quantized range is `-2**num_bits + 1` to `2 ** num_bits` and the quantized weights cannot be represented by `num_bits` int.  What should I do if I have to use signed type to represent the weights?

### Standalone code to reproduce the issue

```shell
def fake_quant_with_min_max_args(inputs, min=-0.99, max=0.99, num_bits=8, narrow_range=False):
    assert min < 0 < max
    quant_min = 1 if narrow_range else 0
    quant_max = (1 << num_bits) - 1
    quant_min_float = np.float32(quant_min)
    quant_max_float = np.float32(quant_max)
    scale = np.float32(max - min) / (quant_max_float - quant_min_float)
    inv_scale = (quant_max_float - quant_min_float) / np.float32(max - min)

    zero_point_from_min = quant_min - min / scale
    if zero_point_from_min < quant_min:
        nudged_zero_point = quant_min
    elif zero_point_from_min > quant_max:
        nudged_zero_point = quant_max
    else:
        nudged_zero_point = np.round(zero_point_from_min)

    nudged_min = (quant_min_float - nudged_zero_point) * scale
    nudged_max = (quant_max_float - nudged_zero_point) * scale

    quant_zero = np.floor(-nudged_min * inv_scale + 0.5)
    # print(quant_min, quant_max, nudged_zero_point)
    # print(nudged_min, nudged_max, scale, inv_scale, quant_zero)
    clamped = np.clip(inputs, nudged_min, nudged_max)
    clamp_shifted = clamped - nudged_min
    # quant = np.clip(np.floor(clamp_shifted * inv_scale - quant_zero + 0.5),
    #                 quant_min - 2 ** (num_bits - 1), quant_max - 2 ** (num_bits - 1))
    quant = np.floor(clamp_shifted * inv_scale - quant_zero + 0.5)
    dequant = quant * scale
    return quant, dequant


if __name__ == '__main__':
    import numpy as np
    import tensorflow as tf
    np.random.seed(12345)
    data = np.random.uniform(-1, 1, (2,))
    data = np.r_[1, -1, data]

    d = fake_quant_with_min_max_args(data, -1, 1, 8, False)
    print(d[0])
    print(d[1])
    print(tf.quantization.fake_quant_with_min_max_args(data, -1, 1, 8, False))
```
```


### Relevant log output

_No response_"
61526,TensorFlow profiler running into OOM issue on GPU,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2.11, TF 2.4

### Custom code

No

### OS platform and distribution

Red Had

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current behavior?

Running TensorFlow profiler for longer than 10 second period results into OOM error, crashes the tf inference process and the profiler returns DEADLINE_EXCEEDED. Is there anyway to limit the sampling rate or way to reduce the amount of information being collected to avoid crashing the process?

Here is the code that I run:
tensorflow_profiler.experimental.client(""grpc://localhost:3222"", ""profiles"", 30000)

### Standalone code to reproduce the issue

```shell
tensorflow_profiler.experimental.client(""grpc://localhost:3222"", ""profiles"", 30000)
```


### Relevant log output

```shell
DEADLINE_EXCEEDED
```
"
61525,TypeError: Unable to serialize 64.0 to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

N/A

### GCC/compiler version

N/A

### CUDA/cuDNN version

Not using GPU

### GPU model and memory

N/A

### Current behavior?

I have been receiving this message when trying to save a variety of  tensorflow models since version 2.11.  I have reported it before.

It appears to be produced by the lack of an ability by tensorfow to serialize the model representations it maintains in memory.

TypeError: Unable to serialize 64.0 to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.

See the debugger output below for details

Can I change anything in my models to dodge this logic?
Is there another model saving function I can try?

### Standalone code to reproduce the issue

```shell
Please email me for code.
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""D:\Craig\Python\Projects\CraigsPackages\koopman_operator_autoencoder_unit_tests.py"", line 132, in <module>
    tf.keras.saving.save_model(model=autoencoder,  filepath='autoencoder_model', save_format=""tf"")
  File ""C:\Users\Craig\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\saving\saving_api.py"", line 149, in save_model
    return legacy_sm_saving_lib.save_model(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Craig\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\utils\traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\Craig\AppData\Local\Programs\Python\Python311\Lib\json\encoder.py"", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Craig\AppData\Local\Programs\Python\Python311\Lib\json\encoder.py"", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
TypeError: Unable to serialize 64.0 to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.
```
"
61524,`numpy()` making copies with model parameters,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When calling `numpy()` here no copies are made:
```python
import os, psutil
import tensorflow as tf

process = psutil.Process(os.getpid())
x = tf.random.normal((1, 300000))
print(process.memory_info().rss / 1e9). # 0.48
npy = x.numpy()
print(process.memory_info().rss / 1e9). # 0.48
```
However in the below example the increase in memory implies a copy is being made when calling `numpy()` on a model variable, is there any reason for this?

### Standalone code to reproduce the issue

```shell
import os, psutil
import tensorflow as tf

class TestKerasLinear(tf.keras.Model):
    def __init__(self, out_size):
        super(TestKerasLinear, self).__init__()
        self._linear = tf.keras.layers.Dense(out_size)

    def build(self, input_shape):
        super(TestKerasLinear, self).build(input_shape)

    def call(self, x):
        return self._linear(x)

tf_module = TestKerasLinear(1000)
tf_module.build((None, 300000))

process = psutil.Process(os.getpid())
print(process.memory_info().rss / 1e9). # 1.68

npy = tf_module.variables[0].numpy()

print(process.memory_info().rss / 1e9)  # 2.88
```


### Relevant log output

_No response_"
61523,"When converting tensorflow model to tflite model, is there any way to fix the output order during inference using tflite as Facing an issue of output order of tflite inference on meraki custom cv","I took a pretrained model (SSD MobileNet 320x320) for object detection from the TensorFlow Zoo and configured/tuned it according to my data. I trained a TensorFlow model which detects 2 labels.

I have used the latest checkpoint to save the model, then froze it, and finally performed TF Lite conversion. I did this because I need to upload the TF Lite model only to a Cisco camera. 

I'm facing an issue with the output order during TF Lite inference, as the output arrays get jumbled /rearranged. I need help on how to convert the TensorFlow model to TF Lite efficiently. My TensorFlow version is 2.10"
61521,tflite-rutime: RuntimeError: Encountered unresolved custom op: FarthestPointSample.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): 2.13.0


**Provide the text output from tflite_convert**
I did some test with pointnet++(https://github.com/charlesq34/pointnet2), and tried to inference with tf.lite or tflite-runtime, but both of them show the error message below:
```
Traceback (most recent call last):
  File ""test.py"", line 13, in <module>
    predict = PointNetPredict('/kaggle/input/model-sign/model_sign.tflite')
  File ""/kaggle/working/pointnet3c1/models/pointnet_predict.py"", line 27, in __init__
    self.interpreter = self.init_model()
  File ""/kaggle/working/pointnet3c1/models/pointnet_predict.py"", line 39, in init_model
    interpreter.allocate_tensors()
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py"", line 513, in allocate_tensors
    return self._interpreter.AllocateTensors()
RuntimeError: Encountered unresolved custom op: FarthestPointSample.
See instructions: https://www.tensorflow.org/lite/guide/ops_custom Node number 0 (FarthestPointSample) failed to prepare.Encountered unresolved custom op: FarthestPointSample.
See instructions: https://www.tensorflow.org/lite/guide/ops_custom Node number 0 (FarthestPointSample) failed to prepare.
```
I noticed there were some custom ops(tf_ops) in project pointnet2, but how to convert these ops to tflite-runtime operators?

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
61520,tf.math.floormod performance issue,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

No

### OS platform and distribution

20.04.1-Ubuntu

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA Version: 11.6

### GPU model and memory

Tesla V100

### Current behavior?

Dear Tensorflow support, 

I found that the tf.math.floormod function is very slow for the tensors of float dtype, even if the size of the tensor is very small, in my case, only 6. The platform is a powerful server with GPU acceleration, but the performance is much slower than an old laptop.

Output on server, 20.04.1-Ubuntu, GPU accelerated, TensorFlow version 2.13.0

> tf.math.floormod float cost: 1.0610415935516357 seconds
> tf.math.floormod int cost: 0.005458354949951172 seconds

Output on laptop, Windows 10, cpu-only, TensorFlow version 2.10.0 :

> tf.math.floormod float cost: 0.002992391586303711 seconds
> tf.math.floormod int cost: 0.006021261215209961 seconds

Could you please check this issue? 
Thanks!



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import time
PI = 3.141592653589793

random_tensor = tf.random.uniform(shape=(1, 6), minval=-PI*2, maxval=PI*2, dtype=tf.float32)
start_time = time.time()
random_tensor = tf.math.floormod(random_tensor+PI, 2*PI)-PI
print(f""tf.math.floormod float cost: {time.time() - start_time} seconds"")

random_tensor = tf.random.uniform(shape=(1, 6), minval=-200, maxval=200, dtype=tf.int32)
start_time = time.time()
random_tensor = tf.math.floormod(random_tensor+100, 2*100)-100
print(f""tf.math.floormod int cost: {time.time() - start_time} seconds"")
```


### Relevant log output

_No response_"
61519,BinaryFocalCrossentropy: alpha does not work,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.keras.losses.BinaryFocalCrossentropy` computes the loss without using the `alpha`.
```
import tensorflow as tf
y_true_list = [0, 1, 0, 0]
logits_list = [-1.6, 0.51, 2.94, -1.8]
gamma = 2

focal_func1 = tf.keras.losses.BinaryFocalCrossentropy(gamma=gamma, alpha=0.25, from_logits=True)
focal_loss1 = focal_func1(y_true_list, logits_list)

focal_func2 = tf.keras.losses.BinaryFocalCrossentropy(gamma=gamma, alpha=10, from_logits=True)
focal_loss2 = focal_func2(y_true_list, logits_list)

focal_func3 = tf.keras.losses.BinaryFocalCrossentropy(gamma=gamma, alpha=100, from_logits=True)
focal_loss3 = focal_func3(y_true_list, logits_list)

print(focal_loss1)
print(focal_loss2)
print(focal_loss3)  
```
The results are:
```
tf.Tensor(0.6932789, shape=(), dtype=float32)
tf.Tensor(0.6932789, shape=(), dtype=float32)
tf.Tensor(0.6932789, shape=(), dtype=float32)
```

`focal_loss1` in the codes should be `0.51168`

### Standalone code to reproduce the issue

```shell
In above.
```


### Relevant log output

_No response_"
61518,AttributeError: can't set attribute in Plot or @property for example,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.8

### Custom code

No

### OS platform and distribution

Windows

### Mobile device

na

### Python version

3.8

### Bazel version

na

### GCC/compiler version

?

### CUDA/cuDNN version

?

### GPU model and memory

colab notebook

### Current behavior?

I'm running this tensorflow example: 

https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb

I wanted to add some additional models at the end and tried to create new data windows as done above.  I noticed the example already given, and my new code requires the following line to be run before the @property for ""example"" is created:

""  w2.example = example_inputs, example_labels  ""

else you get a vague error  ""AttributeError: can't set attribute "" but it looks like this has a setter?
  
This line is found under ""3. Plot"" and if moved to a later section after the 

@property 
def example

under section 4 this error occurs. 



### Standalone code to reproduce the issue

```shell
Move:
w2.example = example_inputs, example_labels

to under section 4 which should be ok. Can't set error occurs.
```


### Relevant log output

```shell
AttributeError: can't set attribute
```
"
61515,group_by_window unexpectedly uses key for ordering group output,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.15.0.dev20230810

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.data.Dataset.group_by_window` uses the values of key for ordering outputs, which is neither documented nor desired.
This behavior cancels all the prior randomness and links groups/outputs order with the order of the key.
My expectation is that the key is used to group elements only and does nothing else. 

### Standalone code to reproduce the issue

```python
import tensorflow as tf

datasets = tf.data.Dataset.from_tensor_slices([
    tf.data.Dataset.from_tensors(i).repeat(i)
    for i in range(1, 10)
])

datasets = datasets.interleave(
    map_func=lambda x: x,
    cycle_length=1,
    block_length=1,
)

shuffled_ds = datasets.shuffle(100)
print('Shuffled dataset:', [e.numpy() for e in shuffled_ds])

ds = (
    shuffled_ds
    .group_by_window(
        key_func=lambda x: tf.cast(x, tf.int64),
        reduce_func=lambda key, dataset: dataset,
        window_size=10,
    )
)

ds2 = (
    shuffled_ds
    .group_by_window(
        key_func=lambda x: -tf.cast(x, tf.int64),
        reduce_func=lambda key, dataset: dataset,
        window_size=10,
    )
)

print('Output re-ordered by `group_by_window` using key')
print([e.numpy() for e in ds])
print('-'*50)
print([e.numpy() for e in ds2])
```


### Relevant log output

```python
Shuffled dataset: [8, 7, 7, 6, 3, 2, 8, 3, 8, 4, 9, 4, 6, 8, 6, 5, 5, 9, 7, 7, 9, 2, 7, 5, 9, 7, 9, 1, 6, 9, 9, 8, 5, 6, 3, 7, 8, 9, 8, 6, 4, 5, 4, 9, 8]
Output re-ordered by `group_by_window` using key
[1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9]
--------------------------------------------------
[9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 3, 3, 2, 2, 1]
```

"
61513,On-Device training for LSTM or GRU Model ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): web
- TensorFlow installed from (source or binary): colab
- TensorFlow version (or github SHA if from source):colab

Hi Iâ€™m new to tensorflow and Iâ€™m trying to make LSTM or GRU model to be enable to re-train on-device(Android) with tabular data (mostly customer interaction).

Iâ€™m referencing these examples
[On-Device Training 1](https://www.tensorflow.org/lite/examples/on_device_training/overview)

This is an example of a CNN, but not able to understand how can I enable on-device training for lstm or gru model.
Are there any examples for reference? Thanks"
61512,TensorFlow profiler running into OOM issue on GPU,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.11.0.5

### Custom code

No

### OS platform and distribution

Linux CentOS 7.9.2009

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Running TensorFlow profiler for longer than 10 second period crashes the inference process because of OOM error and the profiler returns DEADLINE_EXCEEDED. Is there anyway to limit the sampling rate or way to reduce the amount of information being collected to avoid crashing the process?

### Standalone code to reproduce the issue

```shell
`tensorflow_profiler.experimental.client(""grpc://localhost:3222"", ""profiles"", 30000)`
```


### Relevant log output

_No response_"
61510,tf.matmul gives wrong result on CPUs with avx512_vnni,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When running on a CPU with avx512_vnni instructions (e.g. Xeon Platinum 8260), tf.matmul in 2.13 gives a completely wrong result that also changes from run to run. Other functions, e.g. tf.einsum, are affected too. A reproducer is included below. 2.12 is working correctly.

I believe this is due to a bug in oneDNN, since running with TF_ENABLE_ONEDNN_OPTS=0 restores the correct behaviour.

Could you please try building TF against the latest oneDNN to see if this bug is already fixed there, and either upgrade oneDNN or revert back to the version used in 2.12? If this bug is still present in latest oneDNN, could you also forward this issue to them so that they can fix it? tf.matmul is such a fundamental part of TensorFlow, so it would be great to have a fix for this as soon as possible (and perhaps add further tests to prevent this kind of bug from reoccurring?)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.keras.utils.set_random_seed(1)
length = 5000
x = tf.concat([tf.ones([length, 1]), tf.random.normal([length, 2])], axis=1)
x = tf.tile(x[None, ...], [3, 1, 1])
xx = tf.matmul(x, x, transpose_a=True)
# xx = tf.einsum(""ijk,ijm->ikm"", x, x)  # Also doesn't work
print(f""{xx.numpy()}"")
```


### Relevant log output

```shell
[[[ 2.0936000e+04  4.1334631e+02  8.8164221e+02]
  [ 4.1334631e+02  2.0951623e+04  5.5098944e+02]
  [ 8.1284619e+02  4.7815466e+02  1.9981070e+04]]

 [[ 0.0000000e+00  6.8663625e-44  0.0000000e+00]
  [ 2.7628342e-35  0.0000000e+00  2.6752920e-35]
  [ 0.0000000e+00  2.3822074e-44  0.0000000e+00]]

 [[-7.9164143e+31  8.9978802e+02  1.7936620e-43]
  [ 0.0000000e+00  1.1210388e-43  0.0000000e+00]
  [ 2.7767702e-35  0.0000000e+00  7.0064923e-45]]]
```
"
61509,NotImplementedError while converting a tensorflow model to coreml using coremltools,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am facing a NotImplementedError while trying to convert [MoViNets](https://github.com/tensorflow/models/tree/master/official/projects/movinet) model to coreml.

The model is saved in [SavedModel](https://www.tensorflow.org/guide/saved_model) format and I am using a tensorflow version equal to **2.13.0** and a version of [Core ML Tools](https://coremltools.readme.io/docs/what-are-coreml-tools) equal to **6.3.0** and a **3.10.6** python.

I don't understand the origin of **StatefulPartitionedCall** operation and its meaning, any idea what's could be going on?

### Here are the steps to reproduce the error:
1. Download a savedModel from the following [link](https://tfhub.dev/tensorflow/movinet/a0/base/kinetics-600/classification/3).
2. Convert the model using:
```shell
import coremltools as ct
coreml_model = ct.convert(saved_dir, convert_to=""mlprogram"")
```
where *saved_dir* is the path to the downloaded model.

### Relevant log output

```shell
NotImplementedError: Conversion for TF op 'StatefulPartitionedCall' not implemented.
 
name: ""StatefulPartitionedCall""
op: ""StatefulPartitionedCall""
input: ""image""
input: ""unknown""
input: ""unknown_0""
input: ""unknown_1""
input: ""unknown_2""
input: ""unknown_3""
input: ""unknown_4""
input: ""unknown_5""
input: ""unknown_6""
input: ""unknown_7""
input: ""unknown_8""
input: ""unknown_9""
input: ""unknown_10""
input: ""unknown_11""
input: ""unknown_12""
input: ""unknown_13""
input: ""unknown_14""
input: ""unknown_15""
...
input: ""unknown_597""
input: ""unknown_598""
input: ""unknown_599""
attr {
  key: ""Tin""
  value {
    list {
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
    }
  }
}
attr {
  key: ""Tout""
  value {
    list {
      type: DT_FLOAT
    }
  }
}
attr {
  key: ""_XlaMustCompile""
  value {
    b: true
  }
}
attr {
  key: ""_collective_manager_ids""
  value {
    list {
    }
  }
}
attr {
  key: ""_has_manual_control_dependencies""
  value {
    b: true
  }
}
attr {
  key: ""_read_only_resource_inputs""
  value {
    list {
      i: 1
      i: 2
      i: 3
      i: 4
      i: 5
      i: 6
      i: 7
      i: 8
      i: 9
      i: 10
      i: 11
      i: 12
      i: 13
      i: 14
      i: 15
...
      i: 597
      i: 598
      i: 599
      i: 600
      i: 601
    }
  }
}
attr {
  key: ""config""
  value {
    s: """"
  }
}
attr {
  key: ""config_proto""
  value {
    s: ""\n\007\n\003CPU\020\001\n\007\n\003GPU\020\0012\005*\0010J\0008\001\202\001\000""
  }
}
attr {
  key: ""executor_type""
  value {
    s: """"
  }
}
attr {
  key: ""f""
  value {
    func {
      name: ""__inference_predict_frozen_288748""
    }
  }
}

```
```
"
61508,.,
61507,Question about @tf.function,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.3.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

T4

### Current behavior?

After adding @tf.function, I found that each epoch only executes one batch_size, and this does not happen when @tf.function are removed

### Standalone code to reproduce the issue

```shell
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D,GlobalAveragePooling2D, Flatten, Dense, Dropout
from tensorflow.keras import Model, Sequential
from tensorflow.keras.regularizers import L2
class ResNetBlock(Model):
    def __init__(self, filters=64, strides=1):
        super(ResNetBlock, self).__init__()
        self.strides = strides
        self.c1 = Conv2D(filters=filters, kernel_size=(3, 3), strides=strides, padding='same')
        self.b1 = BatchNormalization()
        self.a1 = Activation('relu')

        self.c2 = Conv2D(filters=filters, kernel_size=(3, 3), strides=1, padding='same')
        self.b2 = BatchNormalization()

        if(strides > 1):
            self.c3 = Conv2D(filters=filters, kernel_size=(3, 3), strides=strides, padding='same')
            self.b3 = BatchNormalization()
        
        self.a2 = Activation('relu')

    def call(self, inputs):
        short_x = inputs
        x = self.c1(inputs)
        x = self.b1(x)
        x = self.a1(x)
        x = self.c2(x)
        y = self.b2(x)
        if(self.strides > 1):
            short_x = self.c3(short_x)
            short_x = self.b3(short_x)
        return self.a2(short_x + y)
    
class ResNet(Model):
    def __init__(self, model_lst, cur_filters = 64):
        super(ResNet, self).__init__()
        self.c1 = Conv2D(filters=cur_filters, kernel_size=(7, 7), strides=2, padding='same')
        self.b1 = BatchNormalization()
        self.a1 = Activation('relu')
        self.p1 = MaxPool2D((2, 2), 2)
        self.blocks = Sequential()
        for (i, lst) in enumerate(model_lst):
            for ids in range(lst):
                if(i != 0 and ids == 0):
                    block = ResNetBlock(cur_filters, strides=2)
                else:
                    block = ResNetBlock(cur_filters, strides=1)
                self.blocks.add(block)    
            cur_filters *= 2
        self.g1 = GlobalAveragePooling2D()
        self.d1 = Dense(10, activation='softmax', kernel_regularizer=L2())

    def call(self, inputs):
        x = self.c1(inputs)
        x = self.b1(x)
        x = self.a1(x)
        x = self.p1(x)

        x = self.blocks(x)
        x = self.g1(x)
        y = self.d1(x)
        return y


# ---------------------------------------------
# ResNet18
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
# import matplotlib
# matplotlib.rcParams['font.family']=['SimHei', 'Arial']
from tensorflow.keras import *
from tensorflow.keras.layers import Conv2D, Dense, BatchNormalization, Activation, MaxPool2D,GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import Mean,SparseCategoricalAccuracy  
from tensorflow.keras.datasets.fashion_mnist import load_data 
batch_size = 64
epochs = 20
validation_freq = 2
(x_train, y_train), (x_test, y_test) = load_data()
x_train, x_test = x_train/255., x_test/255.
x_train = np.expand_dims(x_train, -1).astype(np.float32)
x_test = np.expand_dims(x_test, -1).astype(np.float32)
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train)).batch(batch_size)
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(len(x_test)).batch(batch_size)

model = ResNet([2, 2, 2, 2])
losses = SparseCategoricalCrossentropy(from_logits=False)
optimizer = Adam()
train_metrics_loss = Mean()
train_metrics_accuracy = SparseCategoricalAccuracy()
test_metrics_loss = Mean()
test_metrics_accuracy = SparseCategoricalAccuracy()

train_losses = []
train_accuracy = []
test_losses = []
test_accuracy = []
@tf.function
def train_step(model, input_images, y_real):
    with tf.GradientTape() as tape:
        y_pred = model(input_images, training=True)
        loss = losses(y_real, y_pred)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    train_metrics_loss.update_state(loss)
    train_metrics_accuracy.update_state(y_real, y_pred)
@tf.function
def test_step(model, input_images, y_real):
    with tf.GradientTape() as tape:
        y_pred = model(input_images, training=False)
        loss = losses(y_real, y_pred)
    test_metrics_loss.update_state(loss)
    test_metrics_accuracy.update_state(y_real, y_pred)

for epoch in range(epochs):
    train_metrics_loss.reset_states()
    train_metrics_accuracy.reset_states()
    test_metrics_accuracy.reset_states()
    test_metrics_loss.reset_states()
    for x_batch, y_batch in train_dataset:
        train_step(model, x_batch, y_batch)
    train_losses.append(train_metrics_loss.result())
    train_accuracy.append(train_metrics_accuracy.result())
    print(f""epoch={epoch}, train_loss={train_metrics_loss.result()}, train_accuracy={train_metrics_accuracy.result()}"")
    if(epoch % validation_freq == 0):
        for test_x_batch, test_y_batch in test_dataset:
            test_step(model, test_x_batch, test_y_batch)
        test_losses.append(test_metrics_loss.result())
        test_accuracy.append(test_metrics_accuracy.result())
        print(f""epoch={epoch}, test_loss={test_metrics_loss.result()}, test_accuracy={test_metrics_accuracy.result()}"")



plt.figure(figsize=(8, 5))
plt.subplot(1, 2, 1)
plt.title('æŸå¤±å€¼å˜åŒ–å›¾')
plt.plot(test_losses, 'g-', label=""Test_Loss"")
plt.plot(train_losses, 'r-', label=""Train_Loss"")

plt.legend()

plt.subplot(1, 2, 2)
plt.title(""å‡†ç¡®çŽ‡å˜åŒ–å›¾"")
plt.plot(train_accuracy, 'r-', label=""Train_Accuracy"")
plt.plot(test_accuracy, 'g-', label=""Test_Accuracy"")
plt.legend()

plt.show()
```


### Relevant log output

_No response_"
61506,Tensorflow 1.15 for Raspberry pi build fail,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 1.15

### Custom code

No

### OS platform and distribution

Linux and Ubuntu 18.05

### Mobile device

target platform: Raspberry pi 4

### Python version

3.7

### Bazel version

na

### GCC/compiler version

na

### CUDA/cuDNN version

na

### GPU model and memory

na

### Current behavior?

Hi

I am trying to install tensorflow 1.15 on Raspberry pi and found this page:
https://github.com/tensorflow/build/tree/master/raspberry_pi_builds

From my understanding, I can do this at another platform (GPU server, ubuntu 18.05 installed).
I followed the instruction in the page, but came across the following error message:
From my guess, it looks like pip is not recognized in Docker.
Could you tell me how to resolve this issue??

### Standalone code to reproduce the issue

```shell
# I followed the instruction here: https://github.com/tensorflow/build/tree/master/raspberry_pi_builds
# And use this command to build

tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \
    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh
```


### Relevant log output

```shell
CI_DOCKER_BUILD_EXTRA_PARAMS: 
CI_DOCKER_EXTRA_PARAMS: 
COMMAND: tensorflow/tools/ci_build/pi/build_raspberry_pi.sh
CI_COMMAND_PREFIX: ./tensorflow/tools/ci_build/builds/with_the_same_user ./tensorflow/tools/ci_build/builds/configured pi-python37
CONTAINER_TYPE: pi-python37
BUILD_TAG: tf_ci
  (docker container name will be tf_ci.pi-python37)

Building container (tf_ci.pi-python37)...
[+] Building 2.9s (10/17)                                                                                                                         
 => [internal] load build definition from Dockerfile.pi-python37                                                                             0.0s
 => => transferring dockerfile: 866B                                                                                                         0.0s
 => [internal] load .dockerignore                                                                                                            0.0s
 => => transferring context: 2B                                                                                                              0.0s
 => [internal] load metadata for docker.io/library/ubuntu:16.04                                                                              1.4s
 => [internal] load build context                                                                                                            0.0s
 => => transferring context: 22.45kB                                                                                                         0.0s
 => [ 1/13] FROM docker.io/library/ubuntu:16.04@sha256:1f1a2d56de1d604801a9671f301190704c25d604a416f59e03c04f5c6ffee0d6                      0.0s
 => CACHED [ 2/13] COPY install/*.sh /install/                                                                                               0.0s
 => CACHED [ 3/13] RUN /install/install_bootstrap_deb_packages.sh                                                                            0.0s
 => CACHED [ 4/13] RUN add-apt-repository -y ppa:openjdk-r/ppa &&     add-apt-repository -y ppa:george-edison55/cmake-3.x                    0.0s
 => CACHED [ 5/13] RUN /install/install_deb_packages.sh                                                                                      0.0s
 => ERROR [ 6/13] RUN /install/install_pip_packages.sh                                                                                       1.4s
------                                                                                                                                            
 > [ 6/13] RUN /install/install_pip_packages.sh:                                                                                                  
#0 0.789 Searching for pip                                                                                                                        
#0 0.789 Reading https://pypi.python.org/simple/pip/                                                                                              
#0 0.867 Couldn't find index page for 'pip' (maybe misspelled?)                                                                                   
#0 0.867 Scanning index of all packages (this may take a while)                                                                                   
#0 0.867 Reading https://pypi.python.org/simple/
#0 0.936 No local packages or download links found for pip
#0 0.937 error: Could not find suitable distribution for Requirement.parse('pip')
------
Dockerfile.pi-python37:11
--------------------
   9 |         add-apt-repository -y ppa:george-edison55/cmake-3.x
  10 |     RUN /install/install_deb_packages.sh
  11 | >>> RUN /install/install_pip_packages.sh
  12 |     RUN /install/install_bazel.sh
  13 |     RUN /install/install_proto3.sh
--------------------
ERROR: failed to solve: process ""/bin/sh -c /install/install_pip_packages.sh"" did not complete successfully: exit code: 1
ERROR: docker build failed. Dockerfile is at /home/keondopark/tensorflow/tensorflow/tools/ci_build/Dockerfile.pi-python37
```
"
61496,Tensorflow inference error,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.8

### Custom code

Yes

### OS platform and distribution

linux centos

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

My model can train normally, but there was an error when inference after the training was completed.The model structure code is as follows:

conv_1 = Conv2D(32,(1,5),(1,1),name='mode0_conv_1',padding='same')(input_1)
bn_1 = BatchNormalizetion(name='mode0_bn_1')(conv_1)
out_1 = PReLU(shared_axes=[1,2])(bn_1)
out_2 = tf.reshape(........)(out_1)
dp_1 = LSTM(......)(out_2)

dp_o1 = Dense(32,)(dp_1)
dp_o2 = PReLU(shared_axes=[1,2])(dp_o1)

ls_o1 = LSTM(................)(dp_o2)

dp_o3 = Dense(2,)(ls_o1)

The error is as follows:

tensorflow.pyrhon.framework.errors_impl.InvalidArgumentError:Graph execution error:
....

Node:'model/model0_bn_1/FuseBatchNormV3'
scale must have the same number of elements as the channels of x, got 32 and 2
         [[{node model/model0_bn_1/FuseBatchNormV3}]] [op:__inference_predict_function_3355]

May I ask what caused this and if it can be resolved?

### Standalone code to reproduce the issue

```shell
No
```


### Relevant log output

_No response_"
61495,Tensorflowlite flex delegate not loaded automatically when interpreter created even though tensorflowlite_flex.dll provided. (C++ on windows),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.11

### Custom code

No

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.3

### GCC/compiler version

MSVC 16 (2019)

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?


When running an application that uses a model that uses select ops, if I provide tensorflowlite.dll.if.lib and tensorflowlite_flex.dll.if.lib as Linker dependencies and place both tensorflowlite.dll and tensorflowlite_flex.dll in the application folder, the flex delegate is not loaded before inference.


When invoke is called on the interpreter we get the error message:
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.

### Standalone code to reproduce the issue

```shell
The issue is reproducible using the tensorflow test code:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/model_flex_test.cc

---------------

The problem is the flex delegate is not automatically applied successfully here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/core/interpreter_builder.cc#L179

This is because nullptr is being passed to getProcAddress() rather than a handle to tensorflowlite_flex.dll here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/shared_library.h#L42

---------------

When I modify the tensorflow sources to pass a handle to tensorflowlite_flex.dll, things work as expected
```


### Relevant log output

```shell
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
```
"
61492,Infinite loop when `clone_model` callback changes a layer to a functional model,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

While the below code works, when using the sequential model, it doesn't for the functional one. With the functional model, the [while loop in `reconstruct_from_config()`](https://github.com/tensorflow/tensorflow/blob/ae5e557a555e2aa5f236af6cfdb83dc568ea38e1/tensorflow/python/keras/engine/functional.py#L779) loops indefinitely.

I don't fully understand the code there, but my hunch is that either I'm doing wrong on the usage side, or there might be a bug close to the `node_count_by_layer`, which is set to the return value of `should_skip_first_node` although that seems like a different concept.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np


def replace_conv2d_with_sub_model(layer: tf.keras.layers.Layer) -> tf.keras.layers.Layer:
    if isinstance(layer, tf.keras.layers.Conv2D):
        inputs = layer.input
        x = layer(inputs)
        y = tf.keras.layers.Lambda(lambda t: t * 2)(x)
        outputs = tf.keras.layers.Add()([x, y])
        layer = tf.keras.models.Model(inputs=inputs, outputs=outputs, name=layer.name)
    return layer


def get_sequential_model() -> tf.keras.Model:
    return tf.keras.Sequential(
        [
            tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),
            tf.keras.layers.Conv2D(32, 3, activation='relu'),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(10, activation='softmax'),
        ]
    )


def get_functional_model() -> tf.keras.Model:
    inputs = tf.keras.Input(shape=(28, 28, 1))
    x = tf.keras.layers.Conv2D(32, 3, activation='relu')(inputs)
    x = tf.keras.layers.Flatten()(x)
    outputs = tf.keras.layers.Dense(10, activation='softmax')(x)
    return tf.keras.Model(inputs=inputs, outputs=outputs)


if __name__ == '__main__':
    model = get_functional_model()  # get_sequential_model() works
    model.compile()
    sample_input = np.random.uniform(size=(1, 28, 28, 1))
    o1 = model.predict(sample_input)

    new_model = tf.keras.models.clone_model(
        model, input_tensors=model.inputs, clone_function=replace_conv2d_with_sub_model
    )  # <-- results in an infinite loop

    o2 = new_model.predict(sample_input)
```


### Relevant log output

_No response_"
61490,ValueError: source code string cannot contain null bytes,"Windows 10 Russian locale (utf-8) Microsoft Windows [Version 10.0.19045.3208]. Python 3.10.8 (i tried with other versions too). Tensorflow-cpu (I do not see the version in Paycharm, I installed it today. And before, without Paycharm, I tried several other versions and got the same error.).

Windows PowerShell                                          
Copyright (C) Microsoft Corporation. All rights reserved.   
                                                            
Try the new cross-platform PowerShell https://aka.ms/pscore6

PS C:\Users\admin\PycharmProjects\pythonProject3> python 
Python 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.                          
>>> import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Python3108\lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Python3108\lib\site-packages\tensorflow\python\__init__.py"", line 37, in <module>
    from tensorflow.python.eager import context
  File ""C:\Python3108\lib\site-packages\tensorflow\python\eager\context.py"", line 29, in <module>
    from tensorflow.core.framework import function_pb2
  File ""C:\Python3108\lib\site-packages\tensorflow\core\framework\function_pb2.py"", line 14, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""C:\Python3108\lib\site-packages\tensorflow\core\framework\attr_value_pb2.py"", line 14, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
ValueError: source code string cannot contain null bytes
"
61488,Bazel@HEAD is breaking TensorFlow,"https://buildkite.com/bazel/bazel-at-head-plus-disabled/builds/1741#01899b1f-bbb1-4dc9-8ffa-1b1b129810b3
```
(06:36:48) ERROR: /var/lib/buildkite-agent/builds/bk-docker-bl1p/bazel-downstream-projects/tensorflow/tensorflow/lite/experimental/microfrontend/BUILD:97:21: Linking tensorflow/lite/experimental/microfrontend/gen_audio_microfrontend_op_py_wrappers_cc [for tool] failed: (Exit 1): clang failed: error executing CppLink command (from target //tensorflow/lite/experimental/microfrontend:gen_audio_microfrontend_op_py_wrappers_cc)
  (cd /var/lib/buildkite-agent/.cache/bazel/_bazel_buildkite-agent/c3b80eb6321395e802c419deaac81a18/execroot/org_tensorflow && \
  exec env - \
    PATH=/var/lib/buildkite-agent/.cache/bazelisk/local/-tmp-tmpxmujvc4j-bazel/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    ZERO_AR_DATE=1 \
  /usr/lib/llvm-16/bin/clang @bazel-out/k8-opt-exec-ST-a0a42e9628a1/bin/tensorflow/lite/experimental/microfrontend/gen_audio_microfrontend_op_py_wrappers_cc-2.params)
# Configuration: 58da8197376b354f6825ccc0ccc40424332586b6eb574b2ca1de78a5bc0d109a
# Execution platform: @local_execution_config_platform//:platform
ld.lld: error: undefined symbol: tsl::Flag::Flag(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>> const&, bool*)
>>> referenced by python_op_gen_main.cc
>>>               bazel-out/k8-opt-exec-ST-a0a42e9628a1/bin/tensorflow/python/framework/_objs/python_op_gen_main/python_op_gen_main.o:(main)
>>> referenced by python_op_gen_main.cc
>>>               bazel-out/k8-opt-exec-ST-a0a42e9628a1/bin/tensorflow/python/framework/_objs/python_op_gen_main/python_op_gen_main.o:(main)
>>> referenced by python_op_gen_main.cc
>>>               bazel-out/k8-opt-exec-ST-a0a42e9628a1/bin/tensorflow/python/framework/_objs/python_op_gen_main/python_op_gen_main.o:(main)
>>> referenced 4 more times
 
ld.lld: error: undefined symbol: tsl::Flags::Usage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>> const&, std::vector<tsl::Flag, std::allocator<tsl::Flag>> const&)
>>> referenced by python_op_gen_main.cc
>>>               bazel-out/k8-opt-exec-ST-a0a42e9628a1/bin/tensorflow/python/framework/_objs/python_op_gen_main/python_op_gen_main.o:(main)
 
ld.lld: error: undefined symbol: tsl::Flags::Parse(int*, char**, std::vector<tsl::Flag, std::allocator<tsl::Flag>> const&)
>>> referenced by python_op_gen_main.cc
>>>               bazel-out/k8-opt-exec-ST-a0a42e9628a1/bin/tensorflow/python/framework/_objs/python_op_gen_main/python_op_gen_main.o:(main)
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

[A bisect](https://buildkite.com/bazel/culprit-finder/builds/6232#0189ba86-a70a-4cc4-af12-1d689bb7e617) shows the breaking change is: https://github.com/bazelbuild/bazel/pull/17498

@keith Can you give some guidance on how to adapt for this breaking change?

/cc @learning-to-play 

"
61486,coreml deleagate not support ResizeBilinear layer,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10 or 2.11

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

no 

### GCC/compiler version

no

### CUDA/cuDNN version

no

### GPU model and memory

111

### Current behavior?

i run coreml delegate on iphone 12. The tflite model has ResizeBilinear layer ( align_coreners == false , half_pixel_center ==true),
but I find the tflite code following  (coreml_delegate.mm):
// For most ops, only version 1 is supported.
  if (registration->version > 1) {
     return false;
}
ResizeBilinear layer is not supported by ANE because of version >1.

But if ResizeBilinear layer ( align_coreners == false , half_pixel_center ==false) , it is supported by ANE engine.So the performance will be improved

### Standalone code to reproduce the issue

```shell
my question is ,
ResizeBilinear layer ( align_coreners == false , half_pixel_center ==true) can be supported by ANE?
this format is common during training model
thank you!
```


### Relevant log output

_No response_"
61485,tensorflow lite cmake compilation failed to allocate memory,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

cloned the master branch

### Custom code

Yes

### OS platform and distribution

windows 11 wsl 2 with Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.3.1

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

Cuda compilation tools, release 10.1, V10.1.243

### GPU model and memory

rtx 2060 6GB dedicated 

### Current behavior?

I am using cmake 3.22.2
I cloned tensorflow into a directory tensorflow_src, then I ran ./configure and set ROCm and CUDA support to none, because I am building tflite. Then I made and moved into a build directory. I run `cmake ../tensorflow_src/tensorflow/lite` and `cmake --build . -j` I get quite a lot of errors, the first is ""failed to allocate memory"". But there's not enough space to paste the entire log here. 

I also tried `cmake ../tensorflow_src/tensorflow/lite/examples/minimal` and `cmake --build . -j` and the error I get is that 

I am trying to instantiate the tflite interpreter object in the easiest possible way.

### Standalone code to reproduce the issue

I also tried bazel build in the tensorflow_sec directory, which seems like it executed fine. then I used `g++ -std=c++17 inference.cpp model.cc -Ltensorflow_src/bazel-bin/tensorflow/lite -ltensorflowlite -o inference -Itensorflow_src/tensorflow/lite`

```shell
In file included from inference.cpp:6:
tensorflow_src/tensorflow/lite/interpreter.h:21:10: fatal error: tensorflow/lite/core/interpreter.h: No such file or directory
   21 | #include ""tensorflow/lite/core/interpreter.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
```

I am following exactly the instructions on tensorflolw website to set up tflite for C++, it seems like there is missing information

### Relevant log output

_No response_"
61484,[Feature] The Heaviside step function as a activation function,Some of the implementations like Single Layer Perceptron needs discrete outputs like 0 or 1. Adding this could make the model building ease.
61482,Incorrect tf.nn.max_pool2d outputs for NCHW and NHWC in the same thread.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

There is an incorrect output of tf.nn.max_pool2d when calling with the input of the same batch size, height, width and depth first in NCHW, then in NHWC format with MKL support. \
Output of tf.nn.max_pool2d on the same input (except data format) must be the same (except data format).

### Standalone code to reproduce the issue

Colab to reproduce https://colab.research.google.com/drive/1Vuo7txDPDq-YXAuCFQcPNowHgOwbT7Oi?usp=sharing



### Relevant log output

_No response_"
61479,Issue with TensorFlow Lite Model Maker: Empty Output on iOS with TensorFlowLiteTaskAudio 0.4.3,"### 1. System information

#### model generation
- OS Platform and Distribution: macOS 12.6.5 (21G531)
- Python 3.7
- Numpy Version: 1.21.6
- TensorFlow Version: 2.9.3
- Model Maker Version: 0.4.2 (and 0.3.4)
- portaudio: stable 19.7.0

#### model execution
- pod TensorFlowLiteTaskAudio: 0.4.3
- iPhone 8 - iOS 16.5.1

### 2. What's happening

I am encountering an issue while working with TensorFlowLite TaskAudio. Despite my attempts to seek assistance through various channels, I haven't received a response yet. I am reaching out to you here as well.

#### Summary:
I have created a model using TensorFlow Lite Model Maker, and it performs as expected on Android. However, when running the same model on iOS using TensorFlowLiteTaskAudio version 0.4.3 (and nightly), the output is consistently empty. This issue prevents the accurate classification of audio data, as the expected output is missing.

#### Steps to Reproduce:

1. Clone the sample repository: [GitHub Repository](https://github.com/ghashi/tensor-flow-lite-task-audio-poc)
2. Run the iOS application and provide input audio data for classification using the model created with ""TensorFlow Lite Model Maker"".
3. Observe that the output results.classifications[1].categories are empty, unlike the expected output.
![image](https://github.com/tensorflow/tensorflow/assets/884725/4d278b52-c811-425d-84ab-4f9da91b6c77)

####  Expected Behavior:
The output of the TensorFlow Lite model should contain meaningful results in the results.classifications[1].categories field, similar to the behavior observed on Android.

#### Additional Information:

I have opened an issue on the tflite-support repository (which has more info regarding this issue)  [GitHub Issue #933](https://github.com/tensorflow/tflite-support/issues/933), but now I see that this repo has more issues related to ""TensorFlow Lite Model Maker""
I have also posted a topic on the TensorFlow forum: [Forum Topic](https://discuss.tensorflow.org/t/possble-bug-model-created-with-tensorflow-lite-model-maker-model-always-returns-empty-output-on-ios-with-tensorflowlitetaskaudio-0-4-3/18622)

The problem seems to be specific to the TensorFlowLiteTaskAudio on iOS, as the same model works as expected on Android.

I kindly request your assistance in investigating and resolving this issue. 
Thank you for your time and consideration."
