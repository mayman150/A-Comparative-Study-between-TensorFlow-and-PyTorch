Issue Number,Issue Title,Issue Body
61477,Tensorflow 2.13.0 cannot be imported after install via poetry due to missing wheel metadata,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

macOS-13.3-arm64-arm-64bit

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

After installing tensorflow  2.13.0 with poetry it cannot be imported

### Standalone code to reproduce the issue
See https://github.com/python-poetry/poetry/issues/8271

According to @dimbleby, the issue happens due to missing wheel metadata.

> please encourage the tensorflow folk to publish consistent metadata in all of their wheels, with platform-specific variations described by markers

Tensorflow folks, please publish consistent metadata ;)

```shell
poetry add tensorflow==2.13.0
poetry shell
python3 
import tensorflow
```


### Relevant log output

```shell
ModuleNotFoundError: No module named 'tensorflow
```
"
61476,How/Where decision is made to choose between eager execution & graph execution for Tensorflow kernel OPs,"I ran HuggingFace BERT model which uses tensorflow 2.13v with oneDNN support on intel machine and recorded its execution logs by setting TF_CPP_MAX_VLOG_LEVEL=2 & ONEDNN_VERBOSE=1 in file.

**Observation :** I have observing logs that are produced after model creation and its weight loading. Since model.fit() always run in graph model, all tensorflow kernel OPs (onednn's mkl kernel op and non-mkl kernel ops) should run in graph mode. But i observe only for non-mkl kernel ops (like ADDV2, Mul) are executing in eager mode followed by graph mode. I dont see any mkl kernel ops(like _MklMatMul) running in eager mode.

**Questions:** I want to know the reason and file where decision making is made for which op there should be eager mode. Since model.fit() runs in graph mode, why I am seeing eager mode execution for all non-mkl ops?

Sample Logs for model.fit() for ADDV2 kernel op:

```
2023-07-31 03:48:44.632289: I tensorflow/core/common_runtime/eager/execute.cc:1678] Executing op AddV2 in device /job:localhost/replica:0/task:0/device:CPU:0 --> executing addv2 eagerly After some other logs in between, I see below log:

2023-07-31 03:50:01.968512: I tensorflow/core/common_runtime/executor.cc:841] Process node: 8127 step -4458402160563696089 {{node tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/batchnorm/add_1}} = AddV2[T=DT_FLOAT, _XlaHasReferenceVars=false, device=""/job:localhost/replica:0/task:0/device:CPU:0""](tf_bert_for_sequence_classification/bert/encoder/layer.0/output/LayerNorm/batchnorm/mul_1, tf_bert_for_sequence_classification/bert/encoder/layer._0/output/LayerNorm/batchnorm/sub) device: 
```
/job:localhost/replica:0/task:0/device:CPU:0 --> executing addv2 in graph mode i assume

**Expected to happen:** All kerenl ops should execute in graph mode.
"
61475,Blank,
61472,Converter issue,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
61468,TF-TRT Warning: Could not find TensorRT,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.12, tf2.13

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10, 3.11

### Bazel version

_No response_

### GCC/compiler version

9.40

### CUDA/cuDNN version

CUDA 11.8, cuDNN8.6

### GPU model and memory

RTX2060

### Current behavior?

import tensorflow as tf2023-08-03 17:42:07.337886: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-03 17:42:07.926267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT


### Standalone code to reproduce the issue

```shell
$ conda create --name tf python=3.10
$ conda activate tf

$ conda install -c conda-forge cudatoolkit=11.8.0
$ pip install nvidia-cudnn-cu11==8.6.0.163


mkdir -p $CONDA_PREFIX/etc/conda/activate.d
echo 'CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
echo 'export LD_LIBRARY_PATH=$CONDA_PREFIX/lib/:$CUDNN_PATH/lib:$LD_LIBRARY_PATH' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
source $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh

import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
```


### Relevant log output

```shell
I have already installed cudatoolkit 11.8 written in the above commands. But It shows CUDA Toolkit is not installed while giving the command. 

$ nvcc --version
CUDA Toolkit is not installed.
```
"
61464,"""load_model"" method causes operating system level user-interface freeze","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

 2.6.2

### Custom code

Yes

### OS platform and distribution

Windows : 10.0.17763 

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA : 11.2.0_460.89 /CUDNN : 8.1.0.77

### GPU model and memory

NVIDIA RTX A6000

### Current behavior?

There should not be any user interface freeze

### Standalone code to reproduce the issue

```shell
load_m = tf.keras.models.load_model('test.hdf5',custom_objects={'custom_loss':CustomLossFunction})
```


### Relevant log output

```shell
We just see operating system user interface freeze.
When the GPU (NVIDIA RTX A6000 ) mode is TCC we see that whole user interface of operating system ( not just the process which is execution this command ) is frozen for 10 seconds. Can this be fixed so that there is no freeze of user interface ? 
Same code when GPU mode is WDDM will not freeze the user interface.
```
"
61463,TFLite cross compile error --> fatal error: cpuid.h: No such file or directory,"I am trying to cross compile TFLite cpp code for ARM64 on ubuntu machine. After all the steps of installation I copied the cpp file to tflite_build  directory, I executed the following command:
![image](https://github.com/tensorflow/tensorflow/assets/43563075/c09083e9-a3fc-4124-a2d2-fab960b0503b)
After execution of some seconds I am getting the following error.
![image](https://github.com/tensorflow/tensorflow/assets/43563075/20615390-e05c-42d0-8c52-009df82f7098)
It will be really helpful if someone can help me to resolve the error.
"
61462,BUG: reference count leak in function `RegisterForwardAccumulatorCleanup` (static analyzer report),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

commit faad219fc46032a0ae9576ccc3076612cc1f5f72

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

Our static analyzer uses Clang 13 as its parser

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

https://github.com/tensorflow/tensorflow/blob/53fb0130851ad40d544105432f414b4ebe9e729d/tensorflow/python/eager/pywrap_tfe_src.cc#L2378-L2379

API `PyCFunction_New` does not steal a reference for the second argument.
API `PyLong_FromLong` will return a new reference.
Calling `PyLong_FromLong` directly as the second argument of `PyCFunction_New` will lead to a reference count leak for the PyObject returned by `PyLong_FromLong`.

Internal report ID: c13984

### Standalone code to reproduce the issue

```shell
Unnecessary. Whenever this function is called, the problem will be triggered.
```


### Relevant log output

_No response_"
61458,How to get detailed information about the issue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12.1

### Custom code

No

### OS platform and distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Cuda 11.8, Cudnn 8.9.2

### GPU model and memory

Nvidia A10

### Current behavior?

I'm getting following error while compiling the model with XLA:

```
OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-3-at-0x55555dbbdfb0 located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0
```

This issue is documented as XLA limitation [here](https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device), which seems reasonable, but the error message doesn't specify where this issue is coming from.

Is there any way to get more details on which variable is creating this issue?

### Standalone code to reproduce the issue

```shell
NA
```


### Relevant log output

_No response_"
61457,Distributed training with parameter servers example using a single binary,"Hello everyone! I am sorry if this is a duplicate issue but from my considerable search - I could not find a single end-to-end distributed parameter-server example to run using tensorflow (using the keras api with `.fit()` method). Also, for some reason - the documentation for parameter-server strategy seems a lot more confusing and difficult to get started with, compared to multi-worker strategy. 

I have been running training jobs using the estimator api before and now trying to update it to TF2.x style distributed training job with parameter-server training strategy using a single binary file for all workers and parameter-servers. I started with the example in documentation here (https://www.tensorflow.org/tutorials/distribute/parameter_server_training) and modified the code to be used as a single binary. 

Code:
```
import tensorflow_datasets as tfds
import tensorflow as tf

import os

cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()
if cluster_resolver.task_type in (""worker"", ""ps""):
  # Start a TensorFlow server and wait.
  server = tf.distribute.Server(cluster_resolver.cluster_spec(),
                                      job_name=cluster_resolver.task_type,
                                      task_index=cluster_resolver.task_id,
                                      protocol=cluster_resolver.rpc_layer or ""grpc"",
                                      start=True)
  server.join()
else:
  ## parameter-server
  strategy = tf.distribute.ParameterServerStrategy(cluster_resolver=cluster_resolver)
  global_batch_size = 64
  x = tf.random.uniform((10, 10))
  y = tf.random.uniform((10,))
  dataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(10).repeat()
  dataset = dataset.batch(global_batch_size)
  dataset = dataset.prefetch(2)
  with strategy.scope():
    model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
    model.compile(tf.keras.optimizers.legacy.SGD(), loss=""mse"", steps_per_execution=10)
  working_dir = ""./my_working_dir""
  log_dir = os.path.join(working_dir, ""log"")
  ckpt_filepath = os.path.join(working_dir, ""ckpt"")
  backup_dir = os.path.join(working_dir, ""backup"")
  callbacks = [
    tf.keras.callbacks.TensorBoard(log_dir=log_dir),
    tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_filepath),
    tf.keras.callbacks.BackupAndRestore(backup_dir=backup_dir),
  ]
  model.fit(dataset, epochs=5, steps_per_epoch=20, callbacks=callbacks)
```

To my understanding, all workers and paramter-servers will start and wait for chief to assign the tasks. Chief or coordinator (documentation uses them interchangeably but is there any difference between the two?) will automatically divide the work based on the information it gets from `cluster_resolver` (let me know if that's wrong interpretation). In any case, I would highly appreciate if someone can point out what I am doing wrong in this example because I have not been able to get it to work!"
61455,What is the reason sanitizer configs are regarded as outdated? ,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

mater (7a721887ec4616bd3347815f3ce873a0ab14ea37)

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I tried to build tensorflow with config asan, but I found that it was removed at 7a721887ec4616bd3347815f3ce873a0ab14ea37, by @kanglant 

So, I'm curious why these sanitizer flags were regarded as outdated.

Did the community decide to stop supporting sanitizers for tensorflow?
or just because it is not working now? 

Thank you:)

### Standalone code to reproduce the issue

```shell
bazel build --config=asan //tensorflow/tools/pip_package:build_pip_package --jobs `nproc`
```


### Relevant log output

_No response_"
61453,"ValueError: Input 1 of layer ""model"" is incompatible with the layer: expected shape=(None, 15), found shape=(1, 14)","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

windows 11

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Python TF script do not run, because errors

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense
import os

# Set the TensorFlow logging level to ERROR
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# Training data
input_data = [
    ""Привет, как тебя зовут?"",
    ""Какие у тебя интересы?"",
    ""Как прошел твой день?"",
    ""Ты любишь путешествовать?"",
    ""Что ты думаешь о знакомствах через интернет?""
]

output_data = [
    ""Привет! Меня зовут ЧатБот. А тебя?"",
    ""Мои интересы - это общение с людьми!"",
    ""Мой день прошел хорошо, спасибо."",
    ""Я бот, поэтому путешествовать не могу, но обожаю общение с людьми!"",
    ""Я думаю, что знакомства через интернет - это отличный способ найти новых друзей и партнеров.""
]

input_tokens = tf.keras.preprocessing.text.Tokenizer(filters='')
output_tokens = tf.keras.preprocessing.text.Tokenizer(filters='')
input_tokens.fit_on_texts(input_data)
output_tokens.fit_on_texts(output_data)

input_sequences = input_tokens.texts_to_sequences(input_data)
output_sequences = output_tokens.texts_to_sequences(output_data)

# Pad sequences to the maximum length
max_seq_length = max(len(seq) for seq in input_sequences + output_sequences)
input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, padding='post', maxlen=max_seq_length)
output_sequences = tf.keras.preprocessing.sequence.pad_sequences(output_sequences, padding='post', maxlen=max_seq_length)

# Seq2Seq Model
latent_dim = 64

# Encoder
encoder_inputs = Input(shape=(max_seq_length,))
encoder_embedding = tf.keras.layers.Embedding(len(input_tokens.word_index) + 1, latent_dim)(encoder_inputs)
encoder_outputs, state_h, state_c = LSTM(latent_dim, return_state=True)(encoder_embedding)
encoder_states = [state_h, state_c]

# Decoder
decoder_inputs = Input(shape=(max_seq_length,))
decoder_embedding = tf.keras.layers.Embedding(len(output_tokens.word_index) + 1, latent_dim)(decoder_inputs)
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
decoder_dense = Dense(len(output_tokens.word_index) + 1, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')

# Prepare target sequences
target_sequences = output_sequences[:, 1:]

# Training
model.fit([input_sequences, output_sequences[:, :-1]], target_sequences,  epochs=50, batch_size=1)





# Save the model
model.save(""chatbot_model.keras"")

# Example of using the trained model
def predict_response(input_text):
    input_seq = input_tokens.texts_to_sequences([input_text])
    input_seq = tf.keras.preprocessing.sequence.pad_sequences(input_seq, padding='post', maxlen=max_seq_length)
    output_seq = model.predict([input_seq, np.zeros((len(input_seq), max_seq_length))])
    output_seq = np.argmax(output_seq, axis=-1)
    output_text = ' '.join([output_tokens.index_word[i] for i in output_seq[0] if i != 0])
    return output_text

# Example of using the model to generate responses
user_input = ""Как прошел твой день?""
response = predict_response(user_input)
print(response)
```


### Relevant log output

```shell
C:\git\mt_server\markettrader.mooo.com>python chat.py
TensorFlow version: 2.13.0
Epoch 1/50
Traceback (most recent call last):
  File ""C:\git\mt_server\markettrader.mooo.com\chat.py"", line 65, in <module>
    model.fit([input_sequences, output_sequences[:, :-1]],  epochs=50, batch_size=1)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\utils\traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\User\AppData\Local\Temp\__autograph_generated_filer5ymro82.py"", line 15, in tf__train_function
    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
    ^^^^^
ValueError: in user code:

    File ""C:\Users\User\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\engine\training.py"", line 1338, in train_function  *
        return step_function(self, iterator)
    File ""C:\Users\User\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\engine\training.py"", line 1322, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""C:\Users\User\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\engine\training.py"", line 1303, in run_step  **
        outputs = model.train_step(data)
    File ""C:\Users\User\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\engine\training.py"", line 1080, in train_step
        y_pred = self(x, training=True)
    File ""C:\Users\User\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\utils\traceback_utils.py"", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File ""C:\Users\User\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\engine\input_spec.py"", line 298, in assert_input_compatibility
        raise ValueError(

    ValueError: Input 1 of layer ""model"" is incompatible with the layer: expected shape=(None, 15), found shape=(1, 14)
```
"
61452,tf 2.13 - tflite convert error  in topk when k is np.int64 ,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac and colab 
- TensorFlow installation (pip package or built from source): pip 
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13

### 2. Code

Colab code [here](https://colab.research.google.com/drive/163eKr3nkQM4vRqCnFu5U9K3QhgA5PWog?usp=sharing)

### 3. Bug
tf 2.13 model with `tf.math.top_k` error in tflite convert 

tf 2.12 - **pass**
`k` is numpy.int64 - **fail** 
`k` is numpy.int32 - **pass**
`k` is python int - **pass**  

### 4. Error logs
```
---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
[<ipython-input-6-2ef9a00e0912>](https://localhost:8080/#) in <cell line: 2>()
      1 # error
----> 2 create_model_and_convert(k=np.int64(5))

9 frames
[/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py](https://localhost:8080/#) in convert(model_flags, conversion_flags, input_data_str, debug_info_str, enable_mlir_converter)
    365               enable_mlir_converter,
    366           )
--> 367       raise converter_error
    368 
    369   return _run_deprecated_conversion_binary(

ConverterError: /usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/save.py:1313:0: error: 'tf.TopKV2' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""PartitionedCall:"", ""PartitionedCall""]): called from
/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/save.py:1313:0: note: Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: failed while converting: 'main': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: TopKV2
Details:
	tf.TopKV2(tensor<?x29x7xf32>, tensor<i64>) -> (tensor<?x29x5xf32>, tensor<?x29x5xi32>) : {device = """", sorted = true}
```
"
61449,tf.compat.v1.train.MonitoredTrainingSession failed to restore checkpoint_dir variables from s3,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.7.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.7.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current behavior?

[TOC]

Our project uses tf.compat.v1.train.MonitoredTrainingSession to create a training session. Typically, we need to restore a pre-trained model from S3.

## 1. Error encountered in my project
Before switching to TensorFlow 1, we used TensorFlow 1.15.1 and passed the S3 path to `checkpoint_dir` like this:
```python
import tensorflow as tf
.....
checkpoint_dir = ""s3://xxx/xx/""
tf.compat.v1.train.MonitoredTrainingSession(...., checkpoint_dir=checkpoint_dir, ...)
```
`checkpoint_dir` contains everything needed to restore variables, including checkpoint, graph.pbtxt, etc. Everything works fine.

After switching to TensorFlow 2.7.0, we realized that the Modular File System has been introduced into TensorFlow. So, we installed TensorFlow-io version 0.23.0, which is compatible with TensorFlow 2.7.0. The code becomes:
```python
import tensorflow as tf
import tensorflow_io as tfio
.....
checkpoint_dir = ""s3://xxx/xx/""
tf.compat.v1.train.MonitoredTrainingSession(...., checkpoint_dir=checkpoint_dir, ...)
```
However, it no longer works, and an error is reported:
```
.....
2023-08-02 16:02:40.147093: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:207 : DATA_LOSS: truncated block read
Traceback (most recent call last):
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1380, in _do_call
    return fn(*args)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1364, in _run_fn
    target_list, run_metadata)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1458, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.DataLossError: 2 root error(s) found.
  (0) DATA_LOSS: truncated block read
         [[{{node save/RestoreV2}}]]
         [[save/RestoreV2/_1]]
  (1) DATA_LOSS: truncated block read
         [[{{node save/RestoreV2}}]]
0 successful operations.
0 derived errors ignored.
.....
```

## 2. Reproduce the issue using simple code
To rule out the possibility that the issue is caused by the complexity of the model in my project, I reproduced it using a very simple code.
### 2.1 Step 1: Train the model
First, I used the following code to train a very simple model and save it in a local directory:
```python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
x = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=""x"")
y = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=""y"")

W = tf.Variable(tf.zeros([1, 1]), name=""W"")
b = tf.Variable(tf.zeros([1]), name=""b"")

y_pred = tf.matmul(x, W) + b
loss = tf.reduce_mean(tf.square(y - y_pred))

optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.01)

global_step = tf.compat.v1.train.get_or_create_global_step()
train_op = optimizer.minimize(loss, global_step=global_step)

x_train = [[1], [2], [3], [4]]
y_train = [[0], [-1], [-2], [-3]]

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
hooks = [tf.compat.v1.train.StopAtStepHook(last_step=500)]

checkpoint_dir = './checkpoints'

with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,
                                                 config=config,
                                                 hooks=hooks) as sess:
    while not sess.should_stop():
        sess.run(train_op, feed_dict={x: x_train, y: y_train})
```
### 2.2 Step 2: Upload the model to S3
Then, I used S3 tools to upload all materials in `./checkpoints` to a remote S3 path:
```
s3cmd put ./checkpoints/ s3://xxxx/xxx/checkpoints/
```
### 2.3 Step 3: Restore the model from S3 (error)
Finally, I restored the model training using the following code, and an error was reported:
```python
import tensorflow as tf
import tensorflow_io as tfio

tf.compat.v1.disable_eager_execution()

x = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=""x"")
y = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=""y"")

W = tf.Variable(tf.zeros([1, 1]), name=""W"")
b = tf.Variable(tf.zeros([1]), name=""b"")

y_pred = tf.matmul(x, W) + b
loss = tf.reduce_mean(tf.square(y - y_pred))

optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.01)

global_step = tf.compat.v1.train.get_or_create_global_step()

train_op = optimizer.minimize(loss, global_step=global_step)

x_train = [[1], [2], [3], [4]]
y_train = [[0], [-1], [-2], [-3]]

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True

checkpoint_dir = 's3://xxxx/xxx/checkpoints/'

hooks = [tf.compat.v1.train.StopAtStepHook(last_step=2000)]
with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
    while not sess.should_stop():
        sess.run(train_op, feed_dict={x: x_train, y: y_train})
```
The full log is shown below:
```

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:401: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
2023-08-02 16:43:08.483327: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-02 16:43:09.090602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38415 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:0e:00.0, compute capability: 8.0
2023-08-02 16:43:09.854875: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:207 : DATA_LOSS: truncated block read
Traceback (most recent call last):
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1380, in _do_call
    return fn(*args)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1364, in _run_fn
    target_list, run_metadata)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1458, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.DataLossError: 2 root error(s) found.
  (0) DATA_LOSS: truncated block read
         [[{{node save/RestoreV2}}]]
         [[save/RestoreV2/_1]]
  (1) DATA_LOSS: truncated block read
         [[{{node save/RestoreV2}}]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_s3.py"", line 36, in <module>
    with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 616, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1062, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 761, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1267, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1272, in _create_session
    return self._sess_creator.create_session()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 914, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 681, in create_session
    init_fn=self._scaffold.init_fn)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/session_manager.py"", line 321, in prepare_session
    config=config)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/session_manager.py"", line 251, in _restore_checkpoint
    sess, saver, ckpt.model_checkpoint_path)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers
    saver.restore(sess, path)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 1405, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 971, in run
    run_metadata_ptr)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1194, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1374, in _do_run
    run_metadata)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1399, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.DataLossError: 2 root error(s) found.
  (0) DATA_LOSS: truncated block read
         [[node save/RestoreV2
 (defined at train_s3.py:36)
]]
         [[save/RestoreV2/_1]]
  (1) DATA_LOSS: truncated block read
         [[node save/RestoreV2
 (defined at train_s3.py:36)
]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node save/RestoreV2:
In[0] save/Const:
In[1] save/RestoreV2/tensor_names:
In[2] save/RestoreV2/shape_and_slices:

Operation defined at: (most recent call last)
>>>   File ""train_s3.py"", line 36, in <module>
>>>     with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
>>> 

Input Source operations connected to node save/RestoreV2:
In[0] save/Const:
In[1] save/RestoreV2/tensor_names:
In[2] save/RestoreV2/shape_and_slices:

Operation defined at: (most recent call last)
>>>   File ""train_s3.py"", line 36, in <module>
>>>     with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
>>> 

Original stack trace for 'save/RestoreV2':
  File ""train_s3.py"", line 36, in <module>
    with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 616, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1062, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 761, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1267, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1272, in _create_session
    return self._sess_creator.create_session()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 914, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 672, in create_session
    self._scaffold.finalize()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize
    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 625, in _get_saver_or_default
    saver = Saver(sharded=True, allow_empty=True)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 923, in __init__
    self.build()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 935, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 973, in _build
    build_restore=build_restore)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 528, in _build_internal
    restore_sequentially, reshape)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 407, in _AddShardedRestoreOps
    name=""restore_shard""))
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 354, in _AddRestoreOps
    restore_sequentially)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 601, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1504, in restore_v2
    name=name)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 746, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3705, in _create_op_internal
    op_def=op_def)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 2101, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)

```

## 3. Test tf.io and s3 connectivity
I also use the following code to test if tf.io can access s3
```
import tensorflow as tf
import tensorflow_io as tfio
s3_path = ""s3://xxxxx/xxx/checkpoints/checkpoint""
ret = tf.io.read_file(s3_path)
print(ret)
```
And it works fine:
```
2023-08-02 16:48:17.619754: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-02 16:48:18.226059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38415 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:0e:00.0, compute capability: 8.0
tf.Tensor(b'model_checkpoint_path: ""model.ckpt-1000""\nall_model_checkpoint_paths: ""model.ckpt-0""\nall_model_checkpoint_paths: ""model.ckpt-500""\nall_model_checkpoint_paths: ""model.ckpt-1000""\n', shape=(), dtype=string)
```



### Standalone code to reproduce the issue

```shell
see above
```


### Relevant log output

```shell
see above
```
"
61446,tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value num_blocks_2/multihead_attention/conv1d_1/kerne,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.8

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have trained the [CARCA(Context and Attribute-Aware Sequential Recommendation via Cross-Attention)](https://github.com/ahmedrashed-ml/CARCA) on Video Games Dataset. I saved the session after 1 epoch and try to restore the session since all the Architecture was written using the concepts of the session. I saved the session using tf.train.Saver() .save() method.
```

session_saver = tf.train.Saver(save_relative_paths=True)
session_output_path = os.path.join(args.output_dir, ""epochs_""+str(epoch))
if not os.path.isdir(session_output_path):
    os.makedirs(session_output_path)# make directory if not exists
session_saver.save(sess, session_output_path+""/carca_model"")
print(f""[INFO]: Save the model after epochs: {epoch}"")
```
then, I restored session using:
```
saver = tf.train.import_meta_graph(session_dir +""carca_model.meta"")
saver.restore(sess, tf.train.latest_checkpoint(session_dir))
```
I have tried to perform prediction on new dataset using restored session sess, But I have encountered Attempting to use uninitialized value error. The full error is:
```

Traceback (most recent call last):
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value num_blocks_2/multihead_attention/conv1d_1/kernel_1
         [[{{node num_blocks_2/multihead_attention/conv1d_1/kernel_1/read}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""CARCA_train.py"", line 1441, in <module>
    get_load_model_and_inference(dataset, usernum, itemnum, args, ItemFeatures, UserFeatures, CXTDict)
  File ""CARCA_train.py"", line 1317, in get_load_model_and_inference
    predictions = -model.predict(sess, np.ones(args.maxlen)*u, [seq], item_idx, [seqcxt], testitemscxt)
  File ""CARCA_train.py"", line 976, in predict
    {self.test_user: u, self.input_seq: seq, self.test_item: item_idx, self.is_training: False, self.seq_cxt:seqcxt, self.test_item_cxt:testitemcxt})
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value num_blocks_2/multihead_attention/conv1d_1/kernel_1
         [[node num_blocks_2/multihead_attention/conv1d_1/kernel_1/read (defined at CARCA_train.py:670) ]]

Original stack trace for 'num_blocks_2/multihead_attention/conv1d_1/kernel_1/read':
  File ""CARCA_train.py"", line 1441, in <module>
    get_load_model_and_inference(dataset, usernum, itemnum, args, ItemFeatures, UserFeatures, CXTDict)
  File ""CARCA_train.py"", line 1266, in get_load_model_and_inference
    model = Model(usernum, itemnum, args, ItemFeatures, UserFeatures, cxt_size = cxt_size ,use_res = True)
  File ""CARCA_train.py"", line 768, in __init__
    dropout_rate=args.dropout_rate, is_training=self.is_training)
  File ""CARCA_train.py"", line 670, in feedforward
    outputs = tf.layers.conv1d(**params)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/layers/convolutional.py"", line 218, in conv1d
    return layer.apply(inputs)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1479, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/layers/base.py"", line 537, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 591, in __call__
    self._maybe_build(inputs)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1881, in _maybe_build
    self.build(input_shapes)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 165, in build
    dtype=self.dtype)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/layers/base.py"", line 450, in add_weight
    **kwargs)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 384, in add_weight
    aggregation=aggregation)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 663, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1496, in get_variable
    aggregation=aggregation)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1239, in get_variable
    aggregation=aggregation)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 562, in get_variable
    aggregation=aggregation)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 514, in _true_getter
    aggregation=aggregation)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 929, in _get_single_variable
    aggregation=aggregation)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/ops/variables.py"", line 259, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/ops/variables.py"", line 220, in _variable_v1_call
    shape=shape)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/ops/variables.py"", line 198, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 2511, in default_variable_creator
    shape=shape)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/ops/variables.py"", line 263, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/ops/variables.py"", line 1568, in __init__
    shape=shape)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/ops/variables.py"", line 1755, in _init_from_args
    self._snapshot = array_ops.identity(self._variable, name=""read"")
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 180, in wrapper
    return target(*args, **kwargs)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py"", line 86, in identity
    ret = gen_array_ops.identity(input, name=name)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 4253, in identity
    ""Identity"", input=input, name=name)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""/home/zakipoint/miniconda3/envs/sequential_recommendation_carca/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()

I have also checked the variables in both the stored session and the session just after training. Both outputs seem similar. I was stuck on this issue for a few days and also tested saving the model using tf.saved_model.builder.SavedModelBuilder() and restoring the model using tf.saved_model.loader.load() but not solved the issue.Currently,I am using tensorflow 1.14
```

### Standalone code to reproduce the issue

```shell
session_saver = tf.train.Saver(save_relative_paths=True)
session_output_path = os.path.join(args.output_dir, ""epochs_""+str(epoch))
if not os.path.isdir(session_output_path):
    os.makedirs(session_output_path)# make directory if not exists
session_saver.save(sess, session_output_path+""/carca_model"")
print(f""[INFO]: Save the model after epochs: {epoch}"")

saver = tf.train.import_meta_graph(session_dir +""carca_model.meta"")
saver.restore(sess, tf.train.latest_checkpoint(session_dir))
```


### Relevant log output

_No response_"
61445,Tflite use USB camera with android image classification app,"
### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.10
-   **Python version**: 3.10

### Describe the problem
I am new to android and building the tflite image classification app in tensorflow/examples using android studio. I want to make use of a USB camera instead of the mobile back camera to detect the images for classification. How can I achieve that?
What changes do i need to make in CameraFragment.kt to make sure the app can search for a connected USB camera as well? currently the default app only searches for back camera as in this code https://github.com/tensorflow/examples/blob/0bbf4fe43fbf41b7174b9ce4a64d69bd33aadd21/lite/examples/image_classification/android/app/src/main/java/org/tensorflow/lite/examples/imageclassification/fragments/CameraFragment.kt 

thanks
"
61442,[TfLite] unresolved TfLiteGPUDelegateV2Create with Visual Studio,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

Windows 10 Pro

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

Microsoft Visual Studio 2022 C++ compiler

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I built TfLite with `-DTFLITE_ENABLE_GPU=ON`.
I tried to test TfLite with GPU, but the minimal-working C++ example for the GPU from https://www.tensorflow.org/lite/android/delegates/gpu_native does not work under Windows 10 with Visual Studio 2022.
I receive the linker error `error LNK2001: unresolved external symbol __imp_TfLiteGpuDelegateV2Create`.
I ran `dumpbin` on `tensorflow-lite.lib` and it says a static fuction `TfLiteGPUDelegateV2Create` does exist.
I tried a different Windows 10 machine with Visual Studio 2019, but I receive the same linker error.
I got the example working under Ubuntu Linux 23.04 with gcc12 using the same build commands (except from replacing the Windows specifics with Linux specifics of course).

### Standalone code to reproduce the issue

```shell
Build commands (in Command Prompt):
git clone https://github.com/tensorflow/tensorflow tensorflow_src
mkdir tflite_release_x64
cd tflite_release_x64
cmake -G ""Visual Studio 17"" -A x64 -DTFLITE_ENABLE_GPU=ON ..\tensorflow_src\tensorflow\lite
cmake --build . -j 16 --config Release


C++ code (https://www.tensorflow.org/lite/android/delegates/gpu_native):
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/interpreter.h""
#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/delegates/gpu/delegate.h""
#include <iostream>

using namespace tflite;

int main() {
	// Set up interpreter.
	auto model = FlatBufferModel::BuildFromFile(""C:/Users/bartp/source/lite-model_deeplabv3_1_metadata_2.tflite"");
	if (!model) return false;
	ops::builtin::BuiltinOpResolver op_resolver;
	std::unique_ptr<Interpreter> interpreter;
	InterpreterBuilder(*model, op_resolver)(&interpreter);

	auto* delegate = TfLiteGpuDelegateV2Create(/*default options=*/nullptr);

	std::cout << ""Done\n"";
	return 0;
}
```


### Relevant log output

```shell
1>------ Build started: Project: MweTfLite2.13Gpu, Configuration: Release x64 ------
1>Main.cpp
1>Main.obj : error LNK2001: unresolved external symbol __imp_TfLiteGpuDelegateV2Create
1>C:\Users\bartp\source\MweTfLite2.13Gpu\x64\Release\MweTfLite2.13Gpu.exe : fatal error LNK1120: 1 unresolved externals
```
"
61441,try self.interpreter!.invoke() App got crashed on this line ,Swift 5 
61440,//tensorflow/compiler/mlir/lite/tests:optimize.mlir.test fails,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

Unit test fails since commit https://github.com/tensorflow/tensorflow/commit/9d0fea2d5935285122b56867c4499433121f531f

### Standalone code to reproduce the issue

```shell
bazel test --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 --jobs=75 --build_tests_only -- //tensorflow/compiler/mlir/lite/tests:optimize.mlir.test
```


### Relevant log output

```shell
==================== Test output for //tensorflow/compiler/mlir/lite/tests:optimize.mlir.test:
-- Testing: 1 tests, 1 workers --
FAIL: MLIR tests :: optimize.mlir (1 of 1)
******************** TEST 'MLIR tests :: optimize.mlir' FAILED ********************
Script:
--
: 'RUN: at line 2';   /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-opt /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/optimize.mlir -tfl-optimize | /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/llvm-project/llvm/FileCheck /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/optimize.mlir
: 'RUN: at line 4';   /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-opt /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/optimize.mlir -tfl-optimize='enable-canonicalization=true' | /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/llvm-project/llvm/FileCheck --check-prefix=FOLD /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/optimize.mlir
: 'RUN: at line 7';   /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-opt /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/optimize.mlir -tfl-legalize-tf -tfl-optimize | /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/llvm-project/llvm/FileCheck --check-prefix=Fusing /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/optimize.mlir
: 'RUN: at line 9';   /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-opt /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/optimize.mlir -tfl-legalize-tf -tfl-optimize='disable-fuse-mul-and-fc=true' | /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/llvm-project/llvm/FileCheck --check-prefix=NoFusing /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/optimize.mlir
--
Exit Code: 1

Command Output (stderr):
--
/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/optimize.mlir:3622:12: error: CHECK: expected string not found in input
 // CHECK: ""tfl.batch_matmul""(%arg0, %arg1)
           ^
<stdin>:1542:53: note: scanning from here
 func.func @FuseReshapeAndTransposeAroundBatchMatmul(%arg0: tensor<1x128x1024xf32>, %arg1: tensor<1024x16xf32>) -> tensor<1x128x16xf32> {
                                                    ^
<stdin>:1549:7: note: possible intended match here
 %2 = ""tfl.batch_matmul""(%arg1, %1) {adj_x = true, adj_y = false, asymmetric_quantize_inputs = false} : (tensor<1024x16xf32>, tensor<1024x128xf32>) -> tensor<16x128xf32>
      ^

Input file: <stdin>
Check file: /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/tests/optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/lite/tests/optimize.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
              .
              .
              .
           1537:  %3 = ""tfl.transpose""(%2, %0) : (tensor<16x1x8x1280xf32>, tensor<4xi32>) -> tensor<8x1x16x1280xf32> 
           1538:  %4 = ""tfl.gather_nd""(%3, %1) : (tensor<8x1x16x1280xf32>, tensor<16x1xi32>) -> tensor<16x1x16x1280xf32> 
           1539:  %5 = ""tfl.reshape""(%4, %cst) : (tensor<16x1x16x1280xf32>, tensor<4xi32>) -> tensor<1x16x16x1280xf32> 
           1540:  return %5 : tensor<1x16x16x1280xf32> 
           1541:  } 
           1542:  func.func @FuseReshapeAndTransposeAroundBatchMatmul(%arg0: tensor<1x128x1024xf32>, %arg1: tensor<1024x16xf32>) -> tensor<1x128x16xf32> { 
check:3622'0                                                         X~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ error: no match found
           1543:  %cst = arith.constant dense<[1, 2, 0]> : tensor<3xi32> 
check:3622'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           1544:  %cst_0 = arith.constant dense<[16, 1, 128]> : tensor<3xi32> 
check:3622'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           1545:  %cst_1 = arith.constant dense<[1024, 128]> : tensor<2xi32> 
check:3622'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           1546:  %cst_2 = arith.constant dense<[2, 0, 1]> : tensor<3xi32> 
check:3622'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           1547:  %0 = ""tfl.transpose""(%arg0, %cst_2) : (tensor<1x128x1024xf32>, tensor<3xi32>) -> tensor<1024x1x128xf32> 
check:3622'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           1548:  %1 = ""tfl.reshape""(%0, %cst_1) : (tensor<1024x1x128xf32>, tensor<2xi32>) -> tensor<1024x128xf32> 
check:3622'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           1549:  %2 = ""tfl.batch_matmul""(%arg1, %1) {adj_x = true, adj_y = false, asymmetric_quantize_inputs = false} : (tensor<1024x16xf32>, tensor<1024x128xf32>) -> tensor<16x128xf32> 
check:3622'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
check:3622'1           ?                                                                                                                                                                    possible intended match
           1550:  %3 = ""tfl.reshape""(%2, %cst_0) : (tensor<16x128xf32>, tensor<3xi32>) -> tensor<16x1x128xf32> 
check:3622'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           1551:  %4 = ""tfl.transpose""(%3, %cst) : (tensor<16x1x128xf32>, tensor<3xi32>) -> tensor<1x128x16xf32> 
check:3622'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           1552:  return %4 : tensor<1x128x16xf32> 
check:3622'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           1553:  } 
check:3622'0     ~~~
           1554:  func.func @FuseTransposeFCRhsToBatchMatmul(%arg0: tensor<16x1024xf32>, %arg1: tensor<1024x128xf32>, %arg2: none) -> tensor<16x128xf32> { 
check:3622'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
              .
              .
              .
>>>>>>

--

********************
********************
Failed Tests (1):
  MLIR tests :: optimize.mlir


Testing Time: 0.53s
  Failed: 1
================================================================================
```
"
61439,Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.10.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

N/A

### Python version

3.10(Microsoft Store)

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Cuda: 11.2

### GPU model and memory

RTX 3070 Ti 8GB
### Current behavior?

I installed CUDA 11.2 as recommended for tf 2.10.0, here's the install:
![Screenshot](https://github.com/tensorflow/tensorflow/assets/1494132/59352a2a-f90f-45bf-b8bd-861dc893a9ff)
At first, I thought it was a path issue, but after restarting my pc, I was able to access exe files in that folder:
![image](https://github.com/tensorflow/tensorflow/assets/1494132/5d9ccfca-4417-4045-ba74-fffde7b8a121)
If the files are in path, why can't tensorflow find them?
Many people say to use miniconda, so I did, but I got the same result. Other resolved issues were resolved as the OP's were using the wrong version of CUDA, I checked on the website and I can confirm that my version is the required one.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
2023-07-31 18:56:25.098058: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2023-07-31 18:56:25.098226: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-07-31 18:56:26.164080: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2023-07-31 18:56:26.164320: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found
2023-07-31 18:56:26.164540: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found
2023-07-31 18:56:26.164818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found
2023-07-31 18:56:26.368828: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found
2023-07-31 18:56:26.369092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
```
"
61438,Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.10.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

N/A

### Python version

3.10(Microsoft Store)

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Cuda: 11.2

### GPU model and memory

RTX 3070 Ti 8GB
### Current behavior?

I installed CUDA 11.2 as recommended for tf 2.10.0, here's the install:
![Screenshot](https://github.com/tensorflow/tensorflow/assets/1494132/59352a2a-f90f-45bf-b8bd-861dc893a9ff)
At first, I thought it was a path issue, but after restarting my pc, I was able to access exe files in that folder:
![image](https://github.com/tensorflow/tensorflow/assets/1494132/5d9ccfca-4417-4045-ba74-fffde7b8a121)
If the files are in path, why can't tensorflow find them?
Many people say to use miniconda, so I did, but I got the same result. Other resolved issues were resolved as the OP's were using the wrong version of CUDA, I checked on the website and I can confirm that my version is the required one.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
2023-07-31 18:56:25.098058: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2023-07-31 18:56:25.098226: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-07-31 18:56:26.164080: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2023-07-31 18:56:26.164320: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found
2023-07-31 18:56:26.164540: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found
2023-07-31 18:56:26.164818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found
2023-07-31 18:56:26.368828: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found
2023-07-31 18:56:26.369092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
```
"
61437,Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.10.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

N/A

### Python version

3.10(Microsoft Store)

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Cuda: 11.2

### GPU model and memory

RTX 3070 Ti 8GB
### Current behavior?

I installed CUDA 11.2 as recommended for tf 2.10.0, here's the install:
![Screenshot](https://github.com/tensorflow/tensorflow/assets/1494132/59352a2a-f90f-45bf-b8bd-861dc893a9ff)
At first, I thought it was a path issue, but after restarting my pc, I was able to access exe files in that folder:
![image](https://github.com/tensorflow/tensorflow/assets/1494132/5d9ccfca-4417-4045-ba74-fffde7b8a121)
If the files are in path, why can't tensorflow find them?
Many people say to use miniconda, so I did, but I got the same result. Other resolved issues were resolved as the OP's were using the wrong version of CUDA, I checked on the website and I can confirm that my version is the required one.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
2023-07-31 18:56:25.098058: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2023-07-31 18:56:25.098226: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-07-31 18:56:26.164080: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2023-07-31 18:56:26.164320: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found
2023-07-31 18:56:26.164540: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found
2023-07-31 18:56:26.164818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found
2023-07-31 18:56:26.368828: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found
2023-07-31 18:56:26.369092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
```
"
61436,"When building from source code, I always end up with a Python 3.10 whl file, instead of a Python3.8 whl file.","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.8

### Bazel version

1.17

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

11.8/8.7

### GPU model and memory

GTX 1050 Ti 4GB

### Current behavior?

When I try to build the source code from my machine I end up always with a wheel for Python 3.10, although I specified the python path for python3.8 and I don't even have python3.10 installed.

The generated wheel is called tensorflow-2.14.0-cp310-cp310-linux_x86_64.whl

Can you guide why this is happening and how to solve it?

### Standalone code to reproduce the issue

```shell
Just trying to build the source code following the steps from this two sites:

https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03
https://www.tensorflow.org/install/source
```


### Relevant log output

_No response_"
61435,absl update required to slove MSVC compile error,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Windows 10 22H2

### Mobile device

_No response_

### Python version

Anaconda 2023.07-1

### Bazel version

6.3.0

### GCC/compiler version

Visual Studio 2022 (build tools 14.36) + msys2-x86_64-20230718

### CUDA/cuDNN version

CUDA 11.8 + CUDNN 8.6.0 + TensorRT 8.5.3

### GPU model and memory

GTX 750 Ti 2GB

### Current behavior?

Currently, MSVC address sanitizer isn't enabled during compilation and cause compilation error. https://github.com/abseil/abseil-cpp/commit/2927340217c37328319b5869285a6dcdbc13e7a7 (LTS Jan 2023 Patch 3) has fixed it. This update required developers to check if corresponding code is necessary to change.

### Standalone code to reproduce the issue

```shell
1. download https://github.com/tensorflow/tensorflow/archive/refs/tags/v2.13.0.zip and extract
2. comment out Windows CUDA build rejection code in configure.py
3. run `python configure.py` to configure Windows CUDA build
4. run `bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package`
```


### Relevant log output

```shell
error: ""no_sanitize_address"" is undefined
```
"
61434,`FAKE_REQUIRED_PACKAGES` tensorflow-intel prevent poetry installation of `tensorflow-rocm`,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0.560

### Custom code

Yes

### OS platform and distribution

Archlinux 6.1.38-2-lts

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

AMD Radeon RX 6700 XT - gfx1031

### Current behavior?

Updating  `tensorflow-rocm` using `poetry` produce a not resolvable dependency error due to `tensorflow-intel` ""fake required package"". 
The problem is due to the following lines:
https://github.com/tensorflow/tensorflow/blob/6d2f5ac299ef81e3bcd0a431b2375ebbd8252708/tensorflow/tools/pip_package/setup.py#L140-L142
https://github.com/tensorflow/tensorflow/blob/6d2f5ac299ef81e3bcd0a431b2375ebbd8252708/tensorflow/tools/pip_package/setup.py#L143-L144
In particular, the error is caused by the `_VERSION`. In fact, the same version of `tensorflow-rocm` (`2.12.0.560`) not exists in `tensorflow-intel`.


### Standalone code to reproduce the issue

```shell
poetry new fixme
cd fixme
poetry add tensorflow-rocm==""2.12.0.560""
```

### Relevant log output

```shell
Because tensorflow-rocm (2.12.0.560) depends on tensorflow-intel (2.12.0.560) which doesn't match any versions, tensorflow-rocm is forbidden.
So, because lstm-predictor depends on tensorflow-rocm (2.12.0.560), version solving failed.
```


### Possible fix
Simply truncate the version to the patch version before the FAKE_REQUIRED_PACKAGES list definition.
```python
# _VERSION=""2.12.0.560""
_VERSION = (""."").join(_VERSION.split(""."")[:3])
```

### Other open issue
I open the same issue on [ROCmSoftwarePlatform/tensorflow-upstream](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream) [here](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/issues/2161#issue-1809512685)"
61430,Visual Studio 2022 / MingW64: cant find source files,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.7.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

8.1.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Cant write complete application

### Standalone code to reproduce the issue

```shell
#include <stdio.h>
#include <tensorflow/cc/client/client_session.h>
#include <tensorflow/cc/ops/standard_ops.h>
#include <tensorflow/core/framework/tensor.h>

int main() {
    // Инициализация TensorFlow
    tensorflow::Scope root = tensorflow::Scope::NewRootScope();
    tensorflow::ClientSession session(root);

    // Входные данные (5 предыдущих OHLC свечей)
    std::vector<float> input_data = { /* Ваши значения OHLC свечей */ };
    tensorflow::Tensor input_tensor(tensorflow::DT_FLOAT, tensorflow::TensorShape({ 1, 5 }));
    auto input_tensor_mapped = input_tensor.tensor<float, 2>();
    for (int i = 0; i < 5; ++i) {
        input_tensor_mapped(0, i) = input_data[i];
    }

    // Загружаем модель или определяем свою модель для прогнозирования
    // tensorflow::GraphDef graph_def;
    // tensorflow::ReadBinaryProto(tensorflow::Env::Default(), ""path/to/model.pb"", &graph_def);
    // tensorflow::SessionOptions session_options;
    // tensorflow::ClientSession session(root, session_options);
    // session.Create(graph_def);

    // Выполняем прогноз на основе входных данных
    tensorflow::Tensor output_tensor;
    tensorflow::Status run_status = session.Run({ { ""input_tensor_name"", input_tensor } },
        { ""output_tensor_name"" }, {}, &output_tensor);

    if (!run_status.ok()) {
        std::cerr << ""Ошибка выполнения: "" << run_status.error_message() << std::endl;
        return 1;
    }

    // Обрабатываем результат прогноза
    auto output_tensor_mapped = output_tensor.tensor<float, 2>();
    // Выводим результаты прогноза OHLC свечи будущей

    return 0;
}
```


### Relevant log output

```shell
Серьезность	Код	Описание	Проект	Файл	Строка	Состояние подавления
Ошибка (активно)	E1696	не удается открыть источник файл ""third_party/eigen3/unsupported/Eigen/CXX11/ThreadPool""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\threadpool_interface.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/status/status.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\framework\ops.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/str_cat.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\framework\ops.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/tensor.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\framework\ops.h	27	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/str_cat.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\framework\scope.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/array_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/candidate_sampling_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	20	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/control_flow_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/data_flow_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/image_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/io_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/linalg_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/logging_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	27	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/lookup_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	28	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/math_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	29	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/nn_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	30	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/no_op.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	31	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/parsing_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	32	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/random_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	33	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/sparse_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	34	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/state_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	35	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/string_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	36	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/training_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	37	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/user_ops.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\cc\ops\standard_ops.h	38	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/graph.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\common_runtime\graph_constructor.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\allocator.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\allocator.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/base/macros.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\device_base.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\device_base.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/device_attributes.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\device_base.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/full_type.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\full_type_inference_util.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/full_type.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\full_type_util.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/node_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\full_type_util.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/op_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\full_type_util.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/graph_debug_info.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\function.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/container/flat_hash_map.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\function.h	30	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\function.h	31	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/variant.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\function.h	32	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/attr_value.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\function.h	33	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/function.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\function.h	36	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/optimized_function_graph.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\function.h	40	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/protobuf/config.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\function.h	51	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/tsl/protobuf/error_codes.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\function.h	52	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/protobuf/remote_tensor_handle.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\function.h	54	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/node_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\node_def_builder.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/op_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\node_def_builder.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/node_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\node_def_util.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/op_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\node_def_util.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/types.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\node_def_util.h	29	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/node_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\node_properties.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/op_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\node_properties.h	20	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/container/flat_hash_map.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/full_type.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/full_type.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op_def_builder.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/op_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op_def_builder.h	27	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/api_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op_def_util.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/op_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op_def_util.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/time/time.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op_kernel.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op_kernel.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/span.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op_kernel.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/graph.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op_kernel.h	31	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/kernel_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op_kernel.h	32	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/node_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op_kernel.h	34	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/tensor_shape.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op_kernel.h	44	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/types.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op_kernel.h	47	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/protobuf/config.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\op_kernel.h	59	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/registration/options.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\registration\registration.h	38	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/types.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\resource_handle.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""third_party/eigen3/unsupported/Eigen/CXX11/Tensor""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\tensor.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/types.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\tensor.h	30	
Ошибка (активно)	E1696	не удается открыть источник файл ""third_party/eigen3/unsupported/Eigen/CXX11/Tensor""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\tensor_shape.h	21	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/types.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\tensor_shape.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""third_party/eigen3/unsupported/Eigen/CXX11/Tensor""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\tensor_types.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""third_party/eigen3/unsupported/Eigen/CXX11/Tensor""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\types.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/full_type.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\types.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/types.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\framework\types.h	28	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\graph\graph.h	45	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/full_type.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\graph\graph.h	46	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/node_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\graph\graph.h	48	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/container/flat_hash_map.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\graph\graph_debug_info_builder.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/status/status.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\graph\graph_debug_info_builder.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/status/statusor.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\graph\graph_debug_info_builder.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\graph\graph_debug_info_builder.h	27	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/span.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\graph\graph_debug_info_builder.h	28	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/graph_debug_info.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\graph\graph_debug_info_builder.h	29	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/op_def.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\graph\node_builder.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/span.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\lib\gtl\array_slice.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/base/attributes.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\platform\errors.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/str_join.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\platform\errors.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\platform\threadpool.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/protobuf/config.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\public\session_options.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/match.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\util\managed_stack_trace.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/str_cat.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\util\managed_stack_trace.h	27	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\util\managed_stack_trace.h	28	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\core\util\tensor_format.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\framework\allocator.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\framework\allocator.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\framework\device_type.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""Eigen/Core""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\framework\fixedpoint_types.h	21	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/container/inlined_vector.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\lib\gtl\inlined_vector.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""third_party/eigen3/Eigen/Core""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\bfloat16.h	20	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/cord.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\default\cord.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/base/log_severity.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\default\logging.h	35	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\default\logging.h	36	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/status/statusor.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\default\statusor.h	18	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/functional/any_invocable.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\env.h	27	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/base/attributes.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\errors.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/status/status.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\errors.h	27	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/cord.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\errors.h	28	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/str_join.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\errors.h	29	
Ошибка (активно)	E1696	не удается открыть источник файл ""include/float8.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\float8.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/descriptor.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	30	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/arena.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	31	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/descriptor.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	32	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/dynamic_message.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	33	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/io/coded_stream.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	34	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/io/tokenizer.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	35	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/io/zero_copy_stream.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	36	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/io/zero_copy_stream_impl_lite.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	37	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/map.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	38	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/message.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	39	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/repeated_field.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	40	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/text_format.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	41	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/util/field_comparator.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	42	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/util/json_util.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	43	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/util/message_differencer.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	44	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/util/type_resolver_util.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\protobuf.h	45	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/base/attributes.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\status.h	28	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/functional/function_ref.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\status.h	29	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/status/status.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\status.h	30	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/cord.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\status.h	31	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\status.h	32	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\status.h	33	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/tsl/protobuf/error_codes.pb.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\status.h	39	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/base/attributes.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\statusor.h	71	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/status/statusor.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\statusor.h	72	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\stringpiece.h	29	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/str_join.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\str_util.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/str_split.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\str_util.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\Users\User\source\repos\ai\include\tensorflow\tsl\platform\threadpool.h	22
```
"
61429,Keras docs source code links point to 404,"Example:

if I go here https://www.tensorflow.org/api_docs/python/tf/keras/Model

and click source code I get to a 404 here https://github.com/keras-team/keras/tree/v2.13.1/keras/src/engine/training.py#L70-L3991

And it's the same no matter what version of the API docs I click from.  
Speaking of, if you go to https://www.tensorflow.org/versions then the 2.12 and 2.11 links don't point to those versions of the API docs. They go to the current versions. Unless you use the links in the left side bar - those are still okay. Except that there's no link to the current version - which is 2.13."
61427,Since one week with a few different issues and two platforms I can't import Keras,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

SystemError: initialization of _pywrap_checkpoint_reader raised unreported exception

### Custom code

No

### OS platform and distribution

Mac13.4 

### Mobile device

_No response_

### Python version

3.10.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I face the error ""SystemError: initialization of _pywrap_checkpoint_reader raised unreported exception"" when I import keras to build DL model! I worked on make sure that the TensorFlow library is installed also I update the TensorFlow library to the latest version.
I need I help to resolve this issue because I think I trying a lot of recommendations!
Thanks

### Standalone code to reproduce the issue

```shell
Update the TensorFlow library to the latest version.
```


### Relevant log output

_No response_"
61426,tflite-model-maker installation issue,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

For Image Classification

### Standalone code to reproduce the issue

```shell
error: subprocess-exited-with-error
  
  × Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  Getting requirements to build wheel ... error
error: subprocess-exited-with-error

× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.
```


### Relevant log output

_No response_"
61425,JAVA - org.tensorflow.TensorFlowException: Can't parse /<modelPath>/<somePathToFolder>/saved_model.pb as binary proto - JDK 17,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

1.15.0

### Custom code

Yes

### OS platform and distribution

RHEL 8 version, 8.7.5

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`/<modelPath>/<somePathToFolder>/`
Has following files:
1. assets/vocab.txt
2. saved_model.pb
3. variables/variables.data-00000-of-00001 
4. variables/variables.index

I am using java binding code to run the inference. Libraries I am using are 

```
<groupId>org.tensorflow</groupId>
<artifactId>tensorflow</artifactId>
<artifactId>1.15.0</artifactId>


<groupId>org.tensorflow</groupId>
<artifactId>libtensorflow</artifactId>
<artifactId>1.15.0</artifactId>


<groupId>org.tensorflow</groupId>
<artifactId>proto</artifactId>
<artifactId>1.15.0</artifactId>

<groupId>org.tensorflow</groupId>
<artifactId>libtensorflow_jni</artifactId>
<artifactId>1.15.0</artifactId>


```

TensorFlow version - 1.15.0
JDK - Oracle Open Jdk Version 17.0.5
OS - RHEL 8 version, 8.7.5
**Describe the current behavior**

```
org.tensorflow.TensorFlowException: Can't parse </modelPath>/<somePathToFolder>/saved_model.pb as binary proto
	at app//org.tensorflow.SavedModelBundle.load(Native Method)
	at app//org.tensorflow.SavedModelBundle.access$000(SavedModelBundle.java:27)
	at app//org.tensorflow.SavedModelBundle$Loader.load(SavedModelBundle.java:32)
	at app//org.tensorflow.SavedModelBundle.load(SavedModelBundle.java:95)
	at app//com.main.java.main.tensorflow.TestClass.TFPredictor(TestClass.java:19)
```

**Describe the expected behavior**
In JDK 11, the code runs while in JDK 17 its throws error. The model file is not corrupted & same model path file is able to run successfully in JDK 11 

### Standalone code to reproduce the issue

```shell
Code:


import com.google.common.io.Resources;
import org.junit.Test;
import org.tensorflow.SavedModelBundle;
import org.tensorflow.Session;

import java.io.IOException;
import java.net.URISyntaxException;
import java.nio.file.Paths;

public class TestClass {

    @Test
    public void TFPredictor() throws IOException, URISyntaxException {
        SavedModelBundle b = SavedModelBundle.load(""/<modelPath>/<somePathToFolder>"", ""serve"");
        Session sess = b.session();
    }

}
```"
61423,Failed assertion in tf.linalg.sqrtm (and possibly other functions) crashes entire program instead of raising Exception,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.9.1

### Custom code

No

### OS platform and distribution

Macbook 2020 M1 air, Ventura 13.2

### Mobile device

-

### Python version

3.8.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Currently, providing a degenerate matrix to tf.linalg.sqrtm crashes the entire program, producing output: 

```
Assertion failed: (T(i,i) >= 0), function matrix_sqrt_quasi_triangular_diagonal, file external/eigen_archive/unsupported/Eigen/src/MatrixFunctions/MatrixSquareRoot.h, line 128.

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def demo_assertion_error_crashes_program():

    degenerate_matrix = tf.ones((3, 3), dtype=tf.float64)
    try:
        tf.linalg.sqrtm(degenerate_matrix)
        # tf.linalg.inv(degenerate_matrix)  # <- This also fails, but raises an actual exception
        print(""Calculated root"")  # This is never run
    except Exception as err:
        print(""Caught exception: "", err)  # Neither is this

if __name__ == '__main__':
    demo_assertion_error_crashes_program()
```


### Relevant log output

```shell
Assertion failed: (T(i,i) >= 0), function matrix_sqrt_quasi_triangular_diagonal, file external/eigen_archive/unsupported/Eigen/src/MatrixFunctions/MatrixSquareRoot.h, line 128.

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```

### Workaround

One option is to add a small regularizing term to make it non-degenerate, but I don't (yet) know how to do this such that it always prevents the crash, and also does not significantly affect results then the matrix-square-root would have worked.

Instead, I now just use the Denmann-Beavers iteration to approximate the matrix square-root

```
def tf_denmann_beavers_sqrtm(matrix: tf.Tensor, n_iter=10):
    """"""
    Approximate the matrix-square-root by Denmann Beavers iteration
        https://en.wikipedia.org/wiki/Square_root_of_a_matrix#By_Denman%E2%80%93Beavers_iteration
    Convergence is not guaranteed.  Use at your own risk!
    This is handy for tflite, which does not yet support tf.linalg.sqrtm
        https://github.com/tensorflow/tensorflow/issues/60154
    Or for regular tensorflow, which crashes your entire program when input matrix is degenerate
        https://github.com/tensorflow/tensorflow/issues/61423
    """"""
    ym = matrix
    zm = tf.eye(tf.shape(matrix[0])[0], dtype=matrix.dtype)
    for i in range(n_iter):
        ym_ = 0.5 * (ym + tf.linalg.inv(zm))
        zm = 0.5 * (zm + tf.linalg.inv(ym))
        ym = ym_
    return ym
```
... obviously this is not ideal.
"
61422,Unexpected differences in outputs of Conv2D copy with exact subset of weights,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

Yes

### OS platform and distribution

MacOS 13.4.1 (c) (22F770820d)

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Given a `Conv2D` layer that has some channel's weights completely as zeros (e.g. as a result of structured pruning), when creating a new layer from the previous one with those channels missing, and calling both layers on some input, while the relevant parts of weights are exactly the same, that's not the case for the layers' outputs.

I would expect them to be exactly the same as well, but get differences up to `~1.7e-7`.


Note, I've also observed this behavior for Dense layers.

My best guess is that a different mechanism is used under the hood that produces slightly different results?

### Standalone code to reproduce the issue

```shell
from itertools import combinations

import numpy as np
import tensorflow as tf


def reduce_to_relevant_part(array: np.ndarray, channel_indices: list[int], channel_axis: int = -1):
    if channel_indices is not None:
        array = np.take(
            array,
            [i for i in range(array.shape[channel_axis]) if i not in channel_indices],
            axis=channel_axis,
        )
    return array


def get_all_combinations(n: int) -> list[list[int]]:
    result = []
    for i in range(n):
        for subset in combinations(range(n), i):
            result.append(list(subset))
    return result


def print_summary_statistics(array: np.ndarray):
    print(""mean: {}, std: {}, max: {}"".format(np.mean(array), np.std(array), np.max(array)))


def get_layer(n_filters, kernel_size, input_shape):
    layer = tf.keras.layers.Conv2D(
        filters=n_filters,
        kernel_size=kernel_size,
        padding='same',
        use_bias=False,
        kernel_initializer='he_normal',
        strides=kernel_size,
    )
    layer.build(input_shape)
    return layer


def set_channel_weights_to_zero(layer, channel_indices):
    weights = layer.get_weights()
    weights[0][..., channel_indices] = 0
    layer.set_weights(weights)
    return layer


def get_copy_of_layer_without_zero_channels(layer, channel_indices):
    config = layer.get_config()
    config[""filters""] -= len(channel_indices)
    weights = layer.get_weights()
    weights[0] = reduce_to_relevant_part(weights[0], channel_indices)
    config[""weights""] = weights
    new_layer = tf.keras.layers.Conv2D.from_config(config)
    return new_layer


def main():
    for n_filters in range(2, 9):
        kernel_size = (4, 4)
        input_shape = (1, 8, 12, 1)
        for channel_indices in get_all_combinations(n_filters):
            layer = get_layer(n_filters, kernel_size, input_shape)
            layer = set_channel_weights_to_zero(layer, channel_indices)
            new_layer = get_copy_of_layer_without_zero_channels(layer, channel_indices)

            x = tf.random.uniform(input_shape)
            output = layer(x).numpy()
            output_subset = reduce_to_relevant_part(output, channel_indices)
            new_output = new_layer(x).numpy()
            weights_subset = reduce_to_relevant_part(layer.get_weights()[0], channel_indices)
            new_weights = new_layer.get_weights()[0]
            if not np.array_equal(weights_subset, new_weights):
                raise ValueError()  # never triggered
            if not np.array_equal(output_subset, new_output):
                print(
                    ""Outputs differ for channel indices {} @ {} filters"".format(
                        channel_indices, n_filters
                    )
                )


if __name__ == '__main__':
    main()


Find below the output of the above script. Note that for 3 filters for example, `[2]` and `[0, 1]` do not show up, because in those cases the outputs actually are exactly the same.
```


### Relevant log output

```shell
Outputs differ for channel indices [0] @ 2 filters
Outputs differ for channel indices [1] @ 2 filters
Outputs differ for channel indices [0] @ 3 filters
Outputs differ for channel indices [1] @ 3 filters
Outputs differ for channel indices [0, 2] @ 3 filters
Outputs differ for channel indices [1, 2] @ 3 filters
Outputs differ for channel indices [0] @ 4 filters
Outputs differ for channel indices [1] @ 4 filters
Outputs differ for channel indices [2] @ 4 filters
Outputs differ for channel indices [3] @ 4 filters
Outputs differ for channel indices [0, 1, 2] @ 4 filters
Outputs differ for channel indices [0, 1, 3] @ 4 filters
Outputs differ for channel indices [0, 2, 3] @ 4 filters
Outputs differ for channel indices [1, 2, 3] @ 4 filters
Outputs differ for channel indices [0] @ 5 filters
Outputs differ for channel indices [1] @ 5 filters
Outputs differ for channel indices [2] @ 5 filters
Outputs differ for channel indices [3] @ 5 filters
Outputs differ for channel indices [0, 4] @ 5 filters
Outputs differ for channel indices [1, 4] @ 5 filters
Outputs differ for channel indices [2, 4] @ 5 filters
Outputs differ for channel indices [3, 4] @ 5 filters
Outputs differ for channel indices [0, 1, 2] @ 5 filters
Outputs differ for channel indices [0, 1, 3] @ 5 filters
Outputs differ for channel indices [0, 2, 3] @ 5 filters
Outputs differ for channel indices [1, 2, 3] @ 5 filters
Outputs differ for channel indices [0, 1, 2, 4] @ 5 filters
Outputs differ for channel indices [0, 1, 3, 4] @ 5 filters
Outputs differ for channel indices [0, 2, 3, 4] @ 5 filters
Outputs differ for channel indices [1, 2, 3, 4] @ 5 filters
Outputs differ for channel indices [0] @ 6 filters
Outputs differ for channel indices [1] @ 6 filters
Outputs differ for channel indices [2] @ 6 filters
Outputs differ for channel indices [3] @ 6 filters
Outputs differ for channel indices [4] @ 6 filters
Outputs differ for channel indices [5] @ 6 filters
Outputs differ for channel indices [0, 1, 2] @ 6 filters
Outputs differ for channel indices [0, 1, 3] @ 6 filters
Outputs differ for channel indices [0, 1, 4] @ 6 filters
Outputs differ for channel indices [0, 1, 5] @ 6 filters
Outputs differ for channel indices [0, 2, 3] @ 6 filters
Outputs differ for channel indices [0, 2, 4] @ 6 filters
Outputs differ for channel indices [0, 2, 5] @ 6 filters
Outputs differ for channel indices [0, 3, 4] @ 6 filters
Outputs differ for channel indices [0, 3, 5] @ 6 filters
Outputs differ for channel indices [0, 4, 5] @ 6 filters
Outputs differ for channel indices [1, 2, 3] @ 6 filters
Outputs differ for channel indices [1, 2, 4] @ 6 filters
Outputs differ for channel indices [1, 2, 5] @ 6 filters
Outputs differ for channel indices [1, 3, 4] @ 6 filters
Outputs differ for channel indices [1, 3, 5] @ 6 filters
Outputs differ for channel indices [1, 4, 5] @ 6 filters
Outputs differ for channel indices [2, 3, 4] @ 6 filters
Outputs differ for channel indices [2, 3, 5] @ 6 filters
Outputs differ for channel indices [2, 4, 5] @ 6 filters
Outputs differ for channel indices [3, 4, 5] @ 6 filters
Outputs differ for channel indices [0, 1, 2, 3, 4] @ 6 filters
Outputs differ for channel indices [0, 1, 2, 3, 5] @ 6 filters
Outputs differ for channel indices [0, 1, 2, 4, 5] @ 6 filters
Outputs differ for channel indices [0, 1, 3, 4, 5] @ 6 filters
Outputs differ for channel indices [0, 2, 3, 4, 5] @ 6 filters
Outputs differ for channel indices [1, 2, 3, 4, 5] @ 6 filters
Outputs differ for channel indices [0] @ 7 filters
Outputs differ for channel indices [1] @ 7 filters
Outputs differ for channel indices [2] @ 7 filters
Outputs differ for channel indices [3] @ 7 filters
Outputs differ for channel indices [4] @ 7 filters
Outputs differ for channel indices [5] @ 7 filters
Outputs differ for channel indices [0, 6] @ 7 filters
Outputs differ for channel indices [1, 6] @ 7 filters
Outputs differ for channel indices [2, 6] @ 7 filters
Outputs differ for channel indices [3, 6] @ 7 filters
Outputs differ for channel indices [4, 6] @ 7 filters
Outputs differ for channel indices [5, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 2] @ 7 filters
Outputs differ for channel indices [0, 1, 3] @ 7 filters
Outputs differ for channel indices [0, 1, 4] @ 7 filters
Outputs differ for channel indices [0, 1, 5] @ 7 filters
Outputs differ for channel indices [0, 2, 3] @ 7 filters
Outputs differ for channel indices [0, 2, 4] @ 7 filters
Outputs differ for channel indices [0, 2, 5] @ 7 filters
Outputs differ for channel indices [0, 3, 4] @ 7 filters
Outputs differ for channel indices [0, 3, 5] @ 7 filters
Outputs differ for channel indices [0, 4, 5] @ 7 filters
Outputs differ for channel indices [1, 2, 3] @ 7 filters
Outputs differ for channel indices [1, 2, 4] @ 7 filters
Outputs differ for channel indices [1, 2, 5] @ 7 filters
Outputs differ for channel indices [1, 3, 4] @ 7 filters
Outputs differ for channel indices [1, 3, 5] @ 7 filters
Outputs differ for channel indices [1, 4, 5] @ 7 filters
Outputs differ for channel indices [2, 3, 4] @ 7 filters
Outputs differ for channel indices [2, 3, 5] @ 7 filters
Outputs differ for channel indices [2, 4, 5] @ 7 filters
Outputs differ for channel indices [3, 4, 5] @ 7 filters
Outputs differ for channel indices [0, 1, 2, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 3, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 4, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 5, 6] @ 7 filters
Outputs differ for channel indices [0, 2, 3, 6] @ 7 filters
Outputs differ for channel indices [0, 2, 4, 6] @ 7 filters
Outputs differ for channel indices [0, 2, 5, 6] @ 7 filters
Outputs differ for channel indices [0, 3, 4, 6] @ 7 filters
Outputs differ for channel indices [0, 3, 5, 6] @ 7 filters
Outputs differ for channel indices [0, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [1, 2, 3, 6] @ 7 filters
Outputs differ for channel indices [1, 2, 4, 6] @ 7 filters
Outputs differ for channel indices [1, 2, 5, 6] @ 7 filters
Outputs differ for channel indices [1, 3, 4, 6] @ 7 filters
Outputs differ for channel indices [1, 3, 5, 6] @ 7 filters
Outputs differ for channel indices [1, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [2, 3, 4, 6] @ 7 filters
Outputs differ for channel indices [2, 3, 5, 6] @ 7 filters
Outputs differ for channel indices [2, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [3, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 2, 3, 4] @ 7 filters
Outputs differ for channel indices [0, 1, 2, 3, 5] @ 7 filters
Outputs differ for channel indices [0, 1, 2, 4, 5] @ 7 filters
Outputs differ for channel indices [0, 1, 3, 4, 5] @ 7 filters
Outputs differ for channel indices [0, 2, 3, 4, 5] @ 7 filters
Outputs differ for channel indices [1, 2, 3, 4, 5] @ 7 filters
Outputs differ for channel indices [0, 1, 2, 3, 4, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 2, 3, 5, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 2, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 3, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [0, 2, 3, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [1, 2, 3, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [0] @ 8 filters
Outputs differ for channel indices [1] @ 8 filters
Outputs differ for channel indices [2] @ 8 filters
Outputs differ for channel indices [3] @ 8 filters
Outputs differ for channel indices [4] @ 8 filters
Outputs differ for channel indices [5] @ 8 filters
Outputs differ for channel indices [6] @ 8 filters
Outputs differ for channel indices [7] @ 8 filters
Outputs differ for channel indices [0, 1] @ 8 filters
Outputs differ for channel indices [0, 2] @ 8 filters
Outputs differ for channel indices [0, 3] @ 8 filters
Outputs differ for channel indices [0, 4] @ 8 filters
Outputs differ for channel indices [0, 5] @ 8 filters
Outputs differ for channel indices [0, 6] @ 8 filters
Outputs differ for channel indices [0, 7] @ 8 filters
Outputs differ for channel indices [1, 2] @ 8 filters
Outputs differ for channel indices [1, 3] @ 8 filters
Outputs differ for channel indices [1, 4] @ 8 filters
Outputs differ for channel indices [1, 5] @ 8 filters
Outputs differ for channel indices [1, 6] @ 8 filters
Outputs differ for channel indices [1, 7] @ 8 filters
Outputs differ for channel indices [2, 3] @ 8 filters
Outputs differ for channel indices [2, 4] @ 8 filters
Outputs differ for channel indices [2, 5] @ 8 filters
Outputs differ for channel indices [2, 6] @ 8 filters
Outputs differ for channel indices [2, 7] @ 8 filters
Outputs differ for channel indices [3, 4] @ 8 filters
Outputs differ for channel indices [3, 5] @ 8 filters
Outputs differ for channel indices [3, 6] @ 8 filters
Outputs differ for channel indices [3, 7] @ 8 filters
Outputs differ for channel indices [4, 5] @ 8 filters
Outputs differ for channel indices [4, 6] @ 8 filters
Outputs differ for channel indices [4, 7] @ 8 filters
Outputs differ for channel indices [5, 6] @ 8 filters
Outputs differ for channel indices [5, 7] @ 8 filters
Outputs differ for channel indices [6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2] @ 8 filters
Outputs differ for channel indices [0, 1, 3] @ 8 filters
Outputs differ for channel indices [0, 1, 4] @ 8 filters
Outputs differ for channel indices [0, 1, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3] @ 8 filters
Outputs differ for channel indices [0, 2, 4] @ 8 filters
Outputs differ for channel indices [0, 2, 5] @ 8 filters
Outputs differ for channel indices [0, 2, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 4] @ 8 filters
Outputs differ for channel indices [0, 3, 5] @ 8 filters
Outputs differ for channel indices [0, 3, 6] @ 8 filters
Outputs differ for channel indices [0, 3, 7] @ 8 filters
Outputs differ for channel indices [0, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3] @ 8 filters
Outputs differ for channel indices [1, 2, 4] @ 8 filters
Outputs differ for channel indices [1, 2, 5] @ 8 filters
Outputs differ for channel indices [1, 2, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 4] @ 8 filters
Outputs differ for channel indices [1, 3, 5] @ 8 filters
Outputs differ for channel indices [1, 3, 6] @ 8 filters
Outputs differ for channel indices [1, 3, 7] @ 8 filters
Outputs differ for channel indices [1, 4, 5] @ 8 filters
Outputs differ for channel indices [1, 4, 6] @ 8 filters
Outputs differ for channel indices [1, 4, 7] @ 8 filters
Outputs differ for channel indices [1, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 4] @ 8 filters
Outputs differ for channel indices [2, 3, 5] @ 8 filters
Outputs differ for channel indices [2, 3, 6] @ 8 filters
Outputs differ for channel indices [2, 3, 7] @ 8 filters
Outputs differ for channel indices [2, 4, 5] @ 8 filters
Outputs differ for channel indices [2, 4, 6] @ 8 filters
Outputs differ for channel indices [2, 4, 7] @ 8 filters
Outputs differ for channel indices [2, 5, 6] @ 8 filters
Outputs differ for channel indices [2, 5, 7] @ 8 filters
Outputs differ for channel indices [2, 6, 7] @ 8 filters
Outputs differ for channel indices [3, 4, 5] @ 8 filters
Outputs differ for channel indices [3, 4, 6] @ 8 filters
Outputs differ for channel indices [3, 4, 7] @ 8 filters
Outputs differ for channel indices [3, 5, 6] @ 8 filters
Outputs differ for channel indices [3, 5, 7] @ 8 filters
Outputs differ for channel indices [3, 6, 7] @ 8 filters
Outputs differ for channel indices [4, 5, 6] @ 8 filters
Outputs differ for channel indices [4, 5, 7] @ 8 filters
Outputs differ for channel indices [4, 6, 7] @ 8 filters
Outputs differ for channel indices [5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 5] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 2, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 3, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 3, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 3, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 5] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 4, 5] @ 8 filters
Outputs differ for channel indices [1, 2, 4, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 4, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 4, 5] @ 8 filters
Outputs differ for channel indices [1, 3, 4, 6] @ 8 filters
Outputs differ for channel indices [1, 3, 4, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 3, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 4, 5] @ 8 filters
Outputs differ for channel indices [2, 3, 4, 6] @ 8 filters
Outputs differ for channel indices [2, 3, 4, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 5, 6] @ 8 filters
Outputs differ for channel indices [2, 3, 5, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [2, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [2, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 4] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4, 5] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [2, 3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [3, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4, 5, 6, 7] @ 8 filters
```
"
61421,tf.debugging.experimental.enable_dump_debug_info (Debugger V2) error with TPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.12 (but also 2.14 nightly)

### Custom code

Yes

### OS platform and distribution

Google Colab

### Mobile device

no

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

N/A (using TPU)

### GPU model and memory

_No response_

### Current behavior?

I'm trying to use the `tf.debugging.experimental.enable_dump_debug_info(...)` function with TPU.

I've tested my code without the TPU strategy bit, and it works, I can use the Debugger V2 fine.

I've also tested the code without the debugger bit and it trains fine as well.

But together it gives me this error:

```
Traceback (most recent call last):
  File ""/content/tbscript.py"", line 47, in <module>
    model = train()
  File ""/content/tbscript.py"", line 40, in train
    model.fit(x=x_train, 
  File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/debug/lib/dumping_callback.py"", line 579, in <listcomp>
    output_tensor_device_ids = [writer.RegisterDeviceAndGetId(output.device)
ValueError: Cannot assign a device for operation IteratorGetNextAsOptional: Could not satisfy explicit device specification '' because the node {{colocation_node IteratorGetNextAsOptional}} was colocated with a group of nodes that required incompatible device '/job:worker/replica:0/task:0/device:TPU:0'. All available devices [/job:worker/replica:0/task:0/device:CPU:0, /job:worker/replica:0/task:0/device:TPU:0, /job:worker/replica:0/task:0/device:TPU:1, /job:worker/replica:0/task:0/device:TPU:2, /job:worker/replica:0/task:0/device:TPU:3, /job:worker/replica:0/task:0/device:TPU:4, /job:worker/replica:0/task:0/device:TPU:5, /job:worker/replica:0/task:0/device:TPU:6, /job:worker/replica:0/task:0/device:TPU:7, /job:worker/replica:0/task:0/device:TPU_SYSTEM:0, /job:worker/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:COMPOSITE:0]. 
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=2 requested_device_name_='/job:worker/replica:0/task:0/device:TPU:0' assigned_device_name_='/job:worker/replica:0/task:0/device:TPU:0' resource_device_name_='/job:worker/replica:0/task:0/device:TPU:0' supported_device_types_=[CPU] possible_devices_=[]
OptionalGetValue: CPU TPU XLA_CPU 
DebugNumericSummaryV2: CPU 
IteratorGetNext: CPU TPU XLA_CPU 
Identity: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 
Switch: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 
IteratorGetNextAsOptional: CPU TPU XLA_CPU 
DebugIdentityV2: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 
OptionalHasValue: CPU TPU XLA_CPU 
_Arg: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 

Colocation members, user-requested devices, and framework assigned devices, if any:
  iterator_1 (_Arg)  framework assigned device=/job:worker/replica:0/task:0/device:TPU:0
  IteratorGetNextAsOptional (IteratorGetNextAsOptional) 
  OptionalHasValue (OptionalHasValue) 
  cond/IteratorGetNextAsOptional/_5 (Switch) 
  cond/iterator_1/_13 (Switch) 
  Func/cond/then/_0/input/_39 (Identity) 
  Func/cond/then/_0/input/_47 (Identity) 
  cond/then/_0/cond/OptionalHasValue (OptionalHasValue) 
  Func/cond/else/_1/input/_73 (Identity) 
  Func/cond/else/_1/input/_81 (Identity) 
  cond/else/_1/cond/IteratorGetNext (IteratorGetNext) 
  cond/else/_1/cond/IteratorGetNext/DebugNumericSummaryV2 (DebugNumericSummaryV2) 
  cond/else/_1/cond/IteratorGetNext/DebugIdentityV2_1511 (DebugIdentityV2) 
  cond/then/_0/cond/cond/Func/cond/then/_0/input/_39/_111 (Switch) 
  Func/cond/then/_0/cond/cond/then/_106/input/_179 (Identity) 
  cond/then/_0/cond/cond/then/_106/cond/cond/OptionalGetValue (OptionalGetValue) /job:worker/replica:0/task:0/device:TPU:0
  cond/then/_0/cond/cond/then/_106/cond/cond/OptionalGetValue/DebugNumericSummaryV2 (DebugNumericSummaryV2) /job:worker/replica:0/task:0/device:TPU:0
  cond/then/_0/cond/cond/then/_106/cond/cond/OptionalGetValue/DebugIdentityV2_1369 (DebugIdentityV2) /job:worker/replica:0/task:0/device:TPU:0
  Func/cond/then/_0/cond/cond/else/_107/input/_184 (Identity) 

	 [[{{node IteratorGetNex ... [truncated]
Exception ignored in atexit callback: <function async_wait at 0x7ecd97fc5d80>
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py"", line 2796, in async_wait
    context().sync_executors()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py"", line 742, in sync_executors
    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation IteratorGetNextAsOptional: Could not satisfy explicit device specification '' because the node {{colocation_node IteratorGetNextAsOptional}} was colocated with a group of nodes that required incompatible device '/job:worker/replica:0/task:0/device:TPU:0'. All available devices [/job:worker/replica:0/task:0/device:CPU:0, /job:worker/replica:0/task:0/device:TPU:0, /job:worker/replica:0/task:0/device:TPU:1, /job:worker/replica:0/task:0/device:TPU:2, /job:worker/replica:0/task:0/device:TPU:3, /job:worker/replica:0/task:0/device:TPU:4, /job:worker/replica:0/task:0/device:TPU:5, /job:worker/replica:0/task:0/device:TPU:6, /job:worker/replica:0/task:0/device:TPU:7, /job:worker/replica:0/task:0/device:TPU_SYSTEM:0, /job:worker/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:COMPOSITE:0]. 
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=2 requested_device_name_='/job:worker/replica:0/task:0/device:TPU:0' assigned_device_name_='/job:worker/replica:0/task:0/device:TPU:0' resource_device_name_='/job:worker/replica:0/task:0/device:TPU:0' supported_device_types_=[CPU] possible_devices_=[]
OptionalGetValue: CPU TPU XLA_CPU 
DebugNumericSummaryV2: CPU 
IteratorGetNext: CPU TPU XLA_CPU 
Identity: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 
Switch: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 
IteratorGetNextAsOptional: CPU TPU XLA_CPU 
DebugIdentityV2: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 
OptionalHasValue: CPU TPU XLA_CPU 
_Arg: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 

Colocation members, user-requested devices, and framework assigned devices, if any:
  iterator_1 (_Arg)  framework assigned device=/job:worker/replica:0/task:0/device:TPU:0
  IteratorGetNextAsOptional (IteratorGetNextAsOptional) 
  OptionalHasValue (OptionalHasValue) 
  cond/IteratorGetNextAsOptional/_5 (Switch) 
  cond/iterator_1/_13 (Switch) 
  Func/cond/then/_0/input/_39 (Identity) 
  Func/cond/then/_0/input/_47 (Identity) 
  cond/then/_0/cond/OptionalHasValue (OptionalHasValue) 
  Func/cond/else/_1/input/_73 (Identity) 
  Func/cond/else/_1/input/_81 (Identity) 
  cond/else/_1/cond/IteratorGetNext (IteratorGetNext) 
  cond/else/_1/cond/IteratorGetNext/DebugNumericSummaryV2 (DebugNumericSummaryV2) 
  cond/else/_1/cond/IteratorGetNext/DebugIdentityV2_1511 (DebugIdentityV2) 
  cond/then/_0/cond/cond/Func/cond/then/_0/input/_39/_111 (Switch) 
  Func/cond/then/_0/cond/cond/then/_106/input/_179 (Identity) 
  cond/then/_0/cond/cond/then/_106/cond/cond/OptionalGetValue (OptionalGetValue) /job:worker/replica:0/task:0/device:TPU:0
  cond/then/_0/cond/cond/then/_106/cond/cond/OptionalGetValue/DebugNumericSummaryV2 (DebugNumericSummaryV2) /job:worker/replica:0/task:0/device:TPU:0
  cond/then/_0/cond/cond/then/_106/cond/cond/OptionalGetValue/DebugIdentityV2_1369 (DebugIdentityV2) /job:worker/replica:0/task:0/device:TPU:0
  Func/cond/then/_0/cond/cond/else/_107/input/_184 (Identity) 

	 [[{{node IteratorGetNex ... [truncated]
2023-07-28 06:05:18.174660: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:59] Ignoring an error encountered when deleting remote tensors handles: INVALID_ARGUMENT: Unable to find the relevant tensor remote_handle: Op ID: 900, Output num: 0
Additional GRPC error information from remote target /job:worker/replica:0/task:0 while calling /tensorflow.eager.EagerService/Enqueue:
:{""created"":""@1690524318.171303502"",""description"":""Error received from peer ipv4:10.15.76.74:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unable to find the relevant tensor remote_handle: Op ID: 900, Output num: 0"",""grpc_status"":3} [type.googleapis.com/tensorflow.core.platform.ErrorSourceProto='\x08\x05']
```

yeah very long.

oh and I've tried `tf.config.set_soft_device_placement(True)` but no result

### Standalone code to reproduce the issue

```shell
Here's a reproducible test case for getting the error:
https://colab.research.google.com/drive/169agwcqy3-M8hQnSAx5EAr64ya2bSMl3?usp=sharing

but you're not supposed to run it on colab, as I've noticed the `tf.debugging.experimental.enable_dump_debug_info` function generally doesn't work with it. So I put it in a .py script and run it with !python3. Without the TPU part it works fine.
```


### Relevant log output

_No response_"
61420,Support for asynchronous execution in TensorFlow DLPack interface,"I would like to inquire about the possibility of adding asynchronous execution support to the TensorFlow DLPack interface.
Currently, the `tensorflow.experimental.dlpack.to_dlpack` and `tensorflow.experimental.dlpack.from_dlpack` functions are synchronous operations, which can introduce synchronization overhead when working across frameworks.

If the TensorFlow DLPack interface supported asynchronous execution, it could help reduce the synchronization overhead and improve the overall execution efficiency when working with multiple frameworks. This would be a valuable addition to the TensorFlow ecosystem.

Thank you for your hard work and dedication to improving TensorFlow. I look forward to hearing your thoughts on this matter.

### Standalone code to reproduce the issue

```python
import tensorflow as tf
import tensorflow.experimental.dlpack as tfdlpack

# Pre-frame processing asynchronous
...

# Handle synchronization across frameworks
output = model(**({'input': tfdlpack.to_dlpack(input)}))

# Framework post-processing asynchronous
...
```


### Relevant log output

_No response_"
61419,Tflite: C++ API format to add NNAPI delegate,"
### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.10
-   **Python version**: 3.10


### Describe the problem
I want to know what is the python API equivalent of adding a tflite delegate to execute the model. 
In python, we can directly add the argument 'experimental_delegates' to tflite.Interpreter and provide the path to the delegate .so file. 

If i want to add NNAPI or GPU delegate when using C++ API, what is the command for that? I couldn't find effective documentation to enable a delegate when using C++.

ModifyGraphWithDelegate is used, but how do i define a delegate up here? I want to make use of NNAPI delegate and i have the .so file for the same as well. Below is the code snippet am using to enable NNAPI delegate

```
  std::map<std::string, tflite::Interpreter::TfLiteDelegatePtr> delegates;
    auto delegate = tflite::Interpreter::TfLiteDelegatePtr(tflite::NnApiDelegate(), [](TfLiteDelegate*) {});
    delegates.emplace(""NNAPI"", std::move(delegate));
    for (const auto& delegate : delegates) {
        interpreter->ModifyGraphWithDelegate(delegate.second.get());
    } 
```

but when i compile the code i get the error:

>  undefined reference to tflite::NnApiDelegate()'

How can i enable NNAPI delegate with C++?

thanks


"
61410,TfLite ResizeInputTensor does not resize Transposed Convolution or Resize operation output tensors,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

macOS 13.2.1

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When a tflite model has a TRANSPOSE_CONV layer or a RESIZE_... layer, the shapes of the output tensors of those layers are not reshaped when Interpreter.allocate_tensors is called after Interpreter.resize_tensor_input() is called in the Python API, or when TfLiteInterpreterAllocateTensors() is called after TfLiteInterpreterResizeInputTensor() is called in the C API. Thus, the tensor allocation calls fail if there is a fusion layer after the TRANSPOSE_CONV or RESIZE... layers in the network that has two differently shaped inputs because of the failure to reshape those layers' outputs. Or, if there is no such fusion layer, the tensor allocation may succeed but the output shape of the model will not be appropriately resized.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

ly = tf.keras.layers
input8 = tf.keras.Input(shape=(8, 8, 1))

y = ly.Conv2D(filters=1, kernel_size=[3, 3], kernel_initializer=tf.keras.initializers.Constant(1), name=""conv_1"")(input8)
y = ly.Conv2D(filters=1, kernel_size=[5, 5], kernel_initializer=tf.keras.initializers.Constant(1), name=""conv_2"")(y)
upsample_output = ly.UpSampling2D(size=(2,2))(y)
upsample_model = tf.keras.Model(inputs=input8, outputs=upsample_output)

x = ly.Conv2D(filters=1, kernel_size=[3, 3], kernel_initializer=tf.keras.initializers.Constant(1), name=""conv_1"")(input8)
x = ly.Conv2D(filters=1, kernel_size=[5, 5], kernel_initializer=tf.keras.initializers.Constant(1), name=""conv_2"")(x)
transpose_output = ly.Conv2DTranspose(filters=1, kernel_size=[2, 2], kernel_initializer=tf.keras.initializers.Constant(1), strides=[2,2], name=""conv_transpose_1"")(x)
transpose_model = tf.keras.Model(inputs=input8, outputs=transpose_output)

converter = tf.lite.TFLiteConverter.from_keras_model(upsample_model)
upsample_tflite_model = converter.convert()
with open('upsample_model.tflite', 'wb') as handle:
    handle.write(upsample_tflite_model)

converter = tf.lite.TFLiteConverter.from_keras_model(transpose_model)
transpose_tflite_model = converter.convert()
with open('transpose_model.tflite', 'wb') as handle:
    handle.write(transpose_tflite_model)

upsample_interpreter = tf.lite.Interpreter(model_path='models/upsample_model.tflite')
transpose_interpreter = tf.lite.Interpreter(model_path='models/transpose_model.tflite')

print(""BEFORE INPUT RESIZING (expect input shape (1, 8, 8, 1) and output shape (1, 4, 4, 1)"")
print()
print(upsample_interpreter.get_input_details())
print(upsample_interpreter.get_output_details())
print()
print()
print(transpose_interpreter.get_input_details())
print(transpose_interpreter.get_output_details())
print()
print()

upsample_interpreter.resize_tensor_input(0, (1, 16, 16, 1))
transpose_interpreter.resize_tensor_input(0, (1, 16, 16, 1))
upsample_interpreter.allocate_tensors()
transpose_interpreter.allocate_tensors()

print(""AFTER INPUT RESIZING (expect input shape (1, 16, 16, 1) and output shape (1, 8, 8, 1)"")
print()
print(upsample_interpreter.get_input_details())
print(upsample_interpreter.get_output_details())
print()
print()
print(transpose_interpreter.get_input_details())
print(transpose_interpreter.get_output_details())
```


### Relevant log output

```shell
BEFORE INPUT RESIZING (expect input shape (1, 8, 8, 1) and output shape (1, 4, 4, 1)

[{'name': 'serving_default_input_19:0', 'index': 0, 'shape': array([1, 8, 8, 1], dtype=int32), 'shape_signature': array([-1,  8,  8,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
[{'name': 'StatefulPartitionedCall:0', 'index': 7, 'shape': array([1, 4, 4, 1], dtype=int32), 'shape_signature': array([-1,  4,  4,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]


[{'name': 'serving_default_input_19:0', 'index': 0, 'shape': array([1, 8, 8, 1], dtype=int32), 'shape_signature': array([-1,  8,  8,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
[{'name': 'StatefulPartitionedCall:0', 'index': 14, 'shape': array([1, 4, 4, 1], dtype=int32), 'shape_signature': array([-1,  4,  4,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]


AFTER INPUT RESIZING (expect input shape (1, 16, 16, 1) and output shape (1, 8, 8, 1)

[{'name': 'serving_default_input_19:0', 'index': 0, 'shape': array([ 1, 16, 16,  1], dtype=int32), 'shape_signature': array([-1,  8,  8,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
[{'name': 'StatefulPartitionedCall:0', 'index': 7, 'shape': array([1, 4, 4, 1], dtype=int32), 'shape_signature': array([-1,  4,  4,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]


[{'name': 'serving_default_input_19:0', 'index': 0, 'shape': array([ 1, 16, 16,  1], dtype=int32), 'shape_signature': array([-1,  8,  8,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
[{'name': 'StatefulPartitionedCall:0', 'index': 14, 'shape': array([1, 4, 4, 1], dtype=int32), 'shape_signature': array([-1,  4,  4,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
```
"
61408,Issue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.9

### Custom code

Yes

### OS platform and distribution

linux ubuntu

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

xyz

### Standalone code to reproduce the issue

```shell
xyz
```


### Relevant log output

```shell
jkjnk
```
"
61407,The issue of updating a formula.,"https://github.com/tensorflow/tensorflow/blame/d5422e3857a3bcab5063fdd01600d4c15393c887/tensorflow/python/keras/optimizer_v2/adam.py#L443

var.assign_sub(
        (m * alpha) / (math_ops.sqrt(v) - coefficients['epsilon']))
should be：
var.assign_sub(
        (m * alpha) / (math_ops.sqrt(v) + coefficients['epsilon']))"
61403,"model.fit() occur ""Cudnn graph failed to build: UNKNOWN: CUDNN_STATUS_BAD_PARAM""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.11, 2.12, 2.13

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8 / 8.6 & 11.8 / 8.9.2

### GPU model and memory

RTX 3090 Ti & RTX 4090

### Current behavior?

This is first time experience to have such error message.

When I try
""model.fit()""
server stops with error message below

Tried cuDNN version 8.6 (as [tensorflow.org](https://www.tensorflow.org/install/pip) ) and 8.9.2 (lateset for CUDA 11.8)
Both have problem.

How can I solve the issue?
Thanks!

### Standalone code to reproduce the issue

```shell
gpu_id = ""2"" # 0 or 1
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = gpu_id

import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.layers as layers
import tensorflow.keras.models as models

size_y = 256
size_x = 256

#--- load dataset
dic_path = './seg_dataset/train/dic'
msk_path = './seg_dataset/train/msk'

seed = 1004 # random number in your mind

dic_datagen = keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255,
        validation_split=0.2
    )
msk_datagen = keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255,
        validation_split=0.2
    )

dic_train = \
    dic_datagen.flow_from_directory( 
        dic_path,
        target_size=(size_y, size_x),
        class_mode=None, 
        seed=seed,
        subset='training'
    )

msk_train = \
    msk_datagen.flow_from_directory( 
        msk_path,
        target_size=(size_y, size_x),
        class_mode=None,
        color_mode='grayscale',
        seed=seed,
        subset='training'
    )

dic_valid = \
    dic_datagen.flow_from_directory( 
        dic_path,
        target_size=(size_y, size_x),
        class_mode=None, 
        seed=seed,
        subset='validation'
    )
msk_valid = \
    msk_datagen.flow_from_directory( 
        msk_path,
        target_size=(size_y, size_x),
        class_mode=None,
        color_mode='grayscale',
        seed=seed,
        subset='validation'
    )

train_ds = zip(dic_train, msk_train)
valid_ds = zip(dic_valid, msk_valid)

f = [16, 32, 64, 128, 256]

kernel_size=(3,3)
padding='same'
strides=1


# number of filters at each level
inputs = layers.Input((size_y, size_x, 1))
p0 = inputs
# downblock 1
x  = layers.Conv2D(16,  kernel_size, padding=padding, strides=strides, activation=""relu"")(p0)
c1 = layers.Conv2D(16, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
x  = layers.MaxPool2D((2, 2), (2, 2))(c1)
# downblock 2
x  = layers.Conv2D(32,  kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
c2 = layers.Conv2D(32, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
x  = layers.MaxPool2D((2, 2), (2, 2))(c2)
# downblock 3
x  = layers.Conv2D(64,  kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
c3 = layers.Conv2D(64, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
x  = layers.MaxPool2D((2, 2), (2, 2))(c3)
# downblock 4
x  = layers.Conv2D(128,  kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
c4 = layers.Conv2D(128, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
x  = layers.MaxPool2D((2, 2), (2, 2))(c4)
# bottle neck
x  = layers.Conv2D(256, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
x  = layers.Conv2D(256, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
# up block 1
x  = layers.UpSampling2D((2, 2))(x)
concat = layers.Concatenate()([x, c4])
x  = layers.Conv2D(128, kernel_size, padding=padding, strides=strides, activation=""relu"")(concat)
x  = layers.Conv2D(128, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
# up block 1
x  = layers.UpSampling2D((2, 2))(x)
concat = layers.Concatenate()([x, c3])
x  = layers.Conv2D(64, kernel_size, padding=padding, strides=strides, activation=""relu"")(concat)
x  = layers.Conv2D(64, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
# up block 1
x  = layers.UpSampling2D((2, 2))(x)
concat = layers.Concatenate()([x, c2])
x  = layers.Conv2D(32, kernel_size, padding=padding, strides=strides, activation=""relu"")(concat)
x  = layers.Conv2D(32, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
# up block 1
x  = layers.UpSampling2D((2, 2))(x)
concat = layers.Concatenate()([x, c1])
x  = layers.Conv2D(16, kernel_size, padding=padding, strides=strides, activation=""relu"")(concat)
x  = layers.Conv2D(16, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)

# last convolution 1x1
outputs = layers.Conv2D(1, (1, 1), padding=""same"", activation=""sigmoid"")(x)
model = models.Model(inputs, outputs)



model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

path_checkpoint = './seg_checkpoint'
os.makedirs(path_checkpoint,exist_ok=True)

model_checkpointer = keras.callbacks.ModelCheckpoint(
    filepath = path_checkpoint,
    save_weights_only=True,
    monitor='val_loss',
    mode='min',
    save_best_only=True,
    verbose = 1
)

#--- additional
callbacks = [
        model_checkpointer,
        keras.callbacks.EarlyStopping(
            patience=50*3,
            monitor='val_loss',
            mode='min',
            verbose=1
            ),
]



#--- train start
EPOCH = 10

history = model.fit(
        train_ds,
        validation_data=valid_ds,
        validation_steps=15, 
        # Total number of steps (batches of samples) 
        # to draw before stopping when performing validation at the end of every epoch.
        batch_size=16,
        steps_per_epoch=50,
        epochs=EPOCH,
        callbacks=callbacks
    )
```


### Relevant log output

```shell
2023-07-26 14:46:27.667380: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:8942] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-07-26 14:46:27.667411: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-07-26 14:46:27.667426: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-07-26 14:46:27.671343: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-26 14:46:28.183018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
WARNING:tensorflow:From /home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
2023-07-26 14:46:28.727203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:28.741660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:28.741864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:28.807551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:28.807748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:28.807913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:28.808057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1884] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22168 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:41:00.0, compute capability: 8.6
2023-07-26 14:46:28.809796: I tensorflow/core/common_runtime/direct_session.cc:380] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:41:00.0, compute capability: 8.6

Found 40000 images belonging to 1 classes.
Found 40000 images belonging to 1 classes.
Found 10000 images belonging to 1 classes.
Found 10000 images belonging to 1 classes.
2023-07-26 14:46:30.293048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.293252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.293411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.293658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.293824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.293974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.294158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.294315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.294450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1884] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22168 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:41:00.0, compute capability: 8.6
Epoch 1/10
2023-07-26 14:46:31.666051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:440] Loaded cuDNN version 8600
2023-07-26 14:46:31.674917: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at conv_ops_fused_impl.h:625 : INTERNAL: Cudnn graph failed to build: UNKNOWN: CUDNN_STATUS_BAD_PARAM
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(4340): 'conv_op' CUDNN_BACKEND_OPERATION: cudnnFinalize Failed
Traceback (most recent call last):
  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(
  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node model/conv2d/Relu defined at (most recent call last):
  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 672, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 672, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 672, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 672, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 672, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py"", line 321, in call
    return self.activation(outputs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 672, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py"", line 321, in call
    return self.activation(outputs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/activations.py"", line 306, in relu
    return backend.relu(

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 672, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py"", line 321, in call
    return self.activation(outputs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/activations.py"", line 306, in relu
    return backend.relu(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend.py"", line 5397, in relu
    x = tf.nn.relu(x)

Cudnn graph failed to build: UNKNOWN: CUDNN_STATUS_BAD_PARAM
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(4340): 'conv_op' CUDNN_BACKEND_OPERATION: cudnnFinalize Failed
         [[{{node model/conv2d/Relu}}]] [Op:__inference_train_function_4359]
```
"
61401,`tensorflow-cpu` 2.13.0 missing Mac ARM wheels,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

macOS 13 ARM

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

[`tensorflow` 2.13.0 added Mac ARM wheels](https://pypi.org/project/tensorflow/2.13.0/#files).

This allows Mac developers to use the standard `tensorflow` package rather than the `tensorflow-macos` package.

But in situations when CPU-only processing is needed, `tensorflow-cpu` is ideal (saving on network and disk usage). The problem is that `tensorflow-cpu` does not have Mac ARM wheels, so it cannot be used on that platform. This is important for teams with different architecture machines that want to share a common lock file produced by something like Poetry or `pip-tools`.

- https://pypi.org/project/tensorflow-cpu/2.13.0/#files

### Standalone code to reproduce the issue

```shell
python -m pip install tensorflow-cpu==2.13.0
```


### Relevant log output

```shell
ERROR: Could not find a version that satisfies the requirement tensorflow-cpu==2.13.0 (from versions: none)
ERROR: No matching distribution found for tensorflow-cpu==2.13.0
```
"
61398,Failed building from source using clang compiler. Error: libtensorflow_framework.so.2 is a dangling symbolic link,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:  No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04.6 LTS. Building on Intel x86 CPU
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: Source
-   **TensorFlow version (use command below)**: 2.13
-   **Python version**: 3.10.11
-   **Bazel version (if compiling from source)**: 5.3.0
-   **Clang/Compiler version (if compiling from source)**: Clang 16.0.6
-   **CUDA/cuDNN version**: None (Building on Intel x86 CPU)
-   **GPU model and memory**: None (Building on Intel x86 CPU)
-   **Exact command to reproduce**: 
 bazel build  --config=mkl --config=dbg --verbose_failures  -c opt --copt=-march=native --spawn_strategy=sandboxed --sandbox_debug //tensorflow/tools/pip_package:build_pip_package


### Describe the problem

Error while building Tensorflow 2.13 from source with clang 16.0.6 and bazel 5.3.0. I am using the versions that were tested compatible  from this link: https://www.tensorflow.org/install/source#tested_build_configurations.
Errors:
ERROR: /home/ubuntu/builds/tensorflow/tensorflow/BUILD:1134:21: declared output 'tensorflow/libtensorflow_framework.so.2' is a dangling symbolic link
ERROR: /home/ubuntu/builds/tensorflow/tensorflow/BUILD:1134:21: Executing genrule //tensorflow:libtensorflow_framework.so.2_sym [for host] failed: not all outputs were created or valid

### Source code / logs

Output from above command mentioned:

Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=121
INFO: Reading rc options for 'build' from /home/ubuntu/builds/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/ubuntu/builds/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/ubuntu/builds/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/ubuntu/anaconda3/envs/tf_build/bin/python --action_env PYTHON_LIB_PATH=/home/ubuntu/anaconda3/envs/tf_build/lib/python3.10/site-packages --python_path=/home/ubuntu/anaconda3/envs/tf_build/bin/python
INFO: Reading rc options for 'build' from /home/ubuntu/builds/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug
INFO: Found applicable config definition build:short_logs in file /home/ubuntu/builds/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/ubuntu/builds/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:mkl in file /home/ubuntu/builds/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt
INFO: Found applicable config definition build:dbg in file /home/ubuntu/builds/tensorflow/.bazelrc: -c dbg --per_file_copt=+.*,-tensorflow.*@-g0 --per_file_copt=+tensorflow/core/kernels.*@-g0 --cxxopt -DTF_LITE_DISABLE_X86_NEON --copt -DDEBUG_BUILD
INFO: Found applicable config definition build:linux in file /home/ubuntu/builds/tensorflow/.bazelrc: --define=build_with_onednn_v3=true --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/ubuntu/builds/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (614 packages loaded, 38324 targets configured).
INFO: Found 1 target...
ERROR: /home/ubuntu/builds/tensorflow/tensorflow/BUILD:1134:21: declared output 'tensorflow/libtensorflow_framework.so.2' is a dangling symbolic link
ERROR: /home/ubuntu/builds/tensorflow/tensorflow/BUILD:1134:21: Executing genrule //tensorflow:libtensorflow_framework.so.2_sym [for host] failed: not all outputs were created or valid
1690366816.573675378: src/main/tools/linux-sandbox.cc:152: calling pipe(2)...
1690366816.573706039: src/main/tools/linux-sandbox.cc:171: calling clone(2)...
1690366816.573936717: src/main/tools/linux-sandbox.cc:180: linux-sandbox-pid1 has PID 52399
1690366816.573986443: src/main/tools/linux-sandbox-pid1.cc:650: Pid1Main started
1690366816.574047951: src/main/tools/linux-sandbox.cc:197: done manipulating pipes
1690366816.574187327: src/main/tools/linux-sandbox-pid1.cc:269: working dir: /home/ubuntu/.cache/bazel/_bazel_ubuntu/e773aae8e1619280c7c65ec2bcc4c4c5/sandbox/linux-sandbox/2934/execroot/org_tensorflow
1690366816.574204103: src/main/tools/linux-sandbox-pid1.cc:301: writable: /home/ubuntu/.cache/bazel/_bazel_ubuntu/e773aae8e1619280c7c65ec2bcc4c4c5/sandbox/linux-sandbox/2934/execroot/org_tensorflow
1690366816.574210697: src/main/tools/linux-sandbox-pid1.cc:301: writable: /tmp
1690366816.574216986: src/main/tools/linux-sandbox-pid1.cc:301: writable: /dev/shm
1690366816.574277494: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /
1690366816.574284725: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /dev
1690366816.574289479: src/main/tools/linux-sandbox-pid1.cc:371: remount rw: /dev/shm
1690366816.574294383: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /dev/pts
1690366816.574298736: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /dev/hugepages
1690366816.574303348: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /dev/mqueue
1690366816.574307631: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys
1690366816.574312050: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/kernel/security
1690366816.574318166: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup
1690366816.574323186: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/unified
1690366816.574328198: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/systemd
1690366816.574333290: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/hugetlb
1690366816.574337803: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/freezer
1690366816.574342520: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/misc
1690366816.574370614: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/cpu,cpuacct
1690366816.574376364: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/perf_event
1690366816.574380879: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/blkio
1690366816.574385348: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/devices
1690366816.574389735: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/memory
1690366816.574394055: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/cpuset
1690366816.574398740: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/net_cls,net_prio
1690366816.574403141: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/pids
1690366816.574407626: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/rdma
1690366816.574412563: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/pstore
1690366816.574417748: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/bpf
1690366816.574422147: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/kernel/debug
1690366816.574427574: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/kernel/tracing
1690366816.574453517: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/fuse/connections
1690366816.574459675: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/kernel/config
1690366816.574464740: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /proc
1690366816.574469467: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /proc/sys/fs/binfmt_misc
1690366816.574478344: src/main/tools/linux-sandbox-pid1.cc:391: remount(nullptr, /proc/sys/fs/binfmt_misc, nullptr, 2101281, nullptr) failure (Operation not permitted) ignored
1690366816.574487890: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /proc/sys/fs/binfmt_misc
1690366816.574499308: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /run
1690366816.574503860: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /run/lock
1690366816.574508265: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /run/snapd/ns
1690366816.574513396: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /run/user/1000
1690366816.574518358: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/amazon-ssm-agent/6563
1690366816.574523566: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /boot/efi
1690366816.574528274: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/amazon-ssm-agent/7497
1690366816.574547267: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/snapd/19361
1690366816.574553038: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/core18/2785
1690366816.574557467: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/core20/1950
1690366816.574562111: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/snapd/19457
1690366816.574566323: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/core18/2751
1690366816.574570473: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/core20/1974
1690366816.574574374: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/lxd/24061
1690366816.574579365: src/main/tools/linux-sandbox-pid1.cc:371: remount rw: /home/ubuntu/.cache/bazel/_bazel_ubuntu/e773aae8e1619280c7c65ec2bcc4c4c5/sandbox/linux-sandbox/2934/execroot/org_tensorflow
1690366816.574585138: src/main/tools/linux-sandbox-pid1.cc:371: remount rw: /home/ubuntu/.cache/bazel/_bazel_ubuntu/e773aae8e1619280c7c65ec2bcc4c4c5/sandbox/linux-sandbox/2934/execroot/org_tensorflow
1690366816.574589762: src/main/tools/linux-sandbox-pid1.cc:371: remount rw: /tmp
1690366816.574594138: src/main/tools/linux-sandbox-pid1.cc:371: remount rw: /dev/shm
1690366816.574636966: src/main/tools/linux-sandbox-pid1.cc:460: calling fork...
1690366816.574760311: src/main/tools/linux-sandbox-pid1.cc:490: child started with PID 2
1690366816.583691390: src/main/tools/linux-sandbox-pid1.cc:507: wait returned pid=2, status=0x00
1690366816.583704145: src/main/tools/linux-sandbox-pid1.cc:525: child exited normally with code 0
1690366816.583952036: src/main/tools/linux-sandbox.cc:233: child exited normally with code 0
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 849.711s, Critical Path: 79.72s
INFO: 4142 processes: 1215 internal, 2927 linux-sandbox.
FAILED: Build did NOT complete successfully"
61396,How to use the estimator interface to achieve cross-node training without using the strategy of tf itself,"### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf2.7

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

python3.10

### Bazel version

5.1.1

### GCC/compiler version

9.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

Tesla P100 12GB

### Current behavior?

![image](https://github.com/tensorflow/tensorflow/assets/69454138/f7e1159d-cd51-4dca-8890-2b9aebe58e57)

Cross-node training can be achieved in this way in tf1. If I want to use the estimator interface for training, can I put server.target in a certain config (similar to tf.estimator.RunConfig)

### Standalone code to reproduce the issue

```shell
cluster_spec = tf.train.ClusterSpec({
      'chief': ['172.20.21.189:1234'],
      'worker': ['172.20.21.197:1234'],
    })
simple_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(cluster_spec, task_type=""chief"",task_id=0)
is_per_host = tf.estimator.tpu.InputPipelineConfig.PER_HOST_V2
run_config = tf.estimator.tpu.RunConfig(
      cluster=simple_resolver,
      master=FLAGS.master,
      model_dir=FLAGS.output_dir,
      save_checkpoints_steps=FLAGS.save_checkpoints_steps,
      tpu_config=tf.estimator.tpu.TPUConfig(
          iterations_per_loop=FLAGS.iterations_per_loop,
          num_shards=FLAGS.num_tpu_cores,
          per_host_input_for_training=is_per_host))
I do this configuration in the estimator interface, but he doesn't seem to be training across nodes (multiple servers),It seems to only recognize local devices.
```


### Relevant log output

_No response_"
61395,int8 tflite model allocate_tensors() silently stop python process.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.8.1, 2.13.0, '2.14.0-dev20230706'

### Custom code

Yes

### OS platform and distribution

Windows 11, Windows 10 WSL with Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have converted a Resnet18 model from onnx to tflite. (onnx > tf > tflite)
onnx to tf conversion is done by this [repo](https://github.com/onnx/onnx-tensorflow)
tflite is converted to int8 precision using post-training integer quantization [link](https://www.tensorflow.org/lite/performance/post_training_integer_quant)
Netron can display the converted int8 model correctly.
onnx model & tflite model [link](https://drive.google.com/file/d/1XvxGt5GGFCO7h2FW69hgdYZ9noh5Svd4/view?usp=sharing)
tflite int8 model [link](https://drive.google.com/file/d/1bLjoawNnhQTy-DMsBGUEtF6mW3GLVN2F/view?usp=sharing)

but when I try to do inference. calling the method allocate_tensors() stop the python process without showing any error/warning.

if the tflite model is converted with fp32, this issue  doesn't happen.

I have no idea how do to fix this issue or is there any workaround?
thanks

### Standalone code to reproduce the issue

```shell
## tf to tflite conversion
import tensorflow as tf
import numpy as np

saved_model_dir = 'resnet18'
tflite_model_path = saved_model_dir + '.tflite'

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

def representative_dataset_gen():
    for _ in range(100):
        data = np.random.rand(1, 3, 224, 224)
        yield [data.astype(np.float32)]

converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_model = converter.convert()

# Save the model
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)


## inference time
import tensorflow as tf
interpreter = tf.lite.Interpreter(model_path=""resnet18.tflite"")
print('before')
interpreter.allocate_tensors()
print('after') # this line not displayed
```


### Relevant log output

```shell
# inference time output (tf 2.14.0-dev20230706)
WARNING:tensorflow:From C:\Users\AI\miniconda3\envs\tf\lib\site-packages\tensorflow\python\ops\distributions\distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
WARNING:tensorflow:From C:\Users\AI\miniconda3\envs\tf\lib\site-packages\tensorflow\python\ops\distributions\bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
before
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
```
"
61394,Cannot subclass dataset_ops.DatasetV2,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.x

### Custom code

Yes

### OS platform and distribution

Mac OS 13.0

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi, I'm from LanceDB team and we're trying to build native support for tf.data. See WIP PR here https://github.com/lancedb/lance/pull/1087 .
Ideally, we'd like to simply subclass `tf.dataset_ops.DatasetV2` so that all the metadata needed to recreate the dataset can be pushed down to our file format that enabled parallelism elegantly.
So, it'd be something like this
```
class LanceTfDataset(dataset_ops.DatasetV2)
    def __init__(self):
       ...
       variant_tensor = tf.Tensor(self, (), dtype=tf.Variant)
       super().__init__(variant_tensor)
 ```
The above code complains that can not create LanceTfDataset to tf.Tensor/variant.

Issue - what exactly is variant_tensor and how do we go about creating one? I read through the docs but couldn't find anything concrete. There was a mention that variant_tensor is a special tensor that tell about the type of the dataset and that it's equivalent to tf.Variant, but the above code doesn't work.

Having a version of tf.dataset that we can use to capture extra metadata would allow us to improve the interface as well:
so instead of lance.tf.data.from_dataset(uri, columns, filter, batch_size) we can just have from_lance(uri).filter(..).batch_size(...).shuffle().

So what's the way to go about subclassing tf Dataset?

### Standalone code to reproduce the issue

```shell
class LanceTfDataset(dataset_ops.DatasetV2)
    def __init__(self):
       ...
       variant_tensor = tf.Tensor(self, (), dtype=tf.Variant)
       super().__init__(variant_tensor)
```


### Relevant log output

_No response_"
61393,Issues running Transformer model example with estimator api,"Hello everyone! I am trying to run an Image classification training example with Vision Transformer from keras examples (https://keras.io/examples/vision/image_classification_with_vision_transformer/). Everything ran perfectly when I ran it as it is but I started facing issues when i switched training from `model.fit()` to `tf.estimator.train_and_evaluate()` (ofcourse I made the appropriate changes to first convert model to estimator). From what I understand ... the problem lies with saving and reloading the model which is done by the estimator api. The model has custom classes:
```
class Patches(layers.Layer):
    def __init__(self, patch_size, **kwargs):
        super().__init__(**kwargs)
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding=""VALID"",
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return patches

    ## personal addition
    def get_config(self):
        base_config = super().get_config()
        base_config.update({
            'patch_size': self.patch_size,
        })
        return base_config

class PatchEncoder(layers.Layer):
    def __init__(self, num_patches, projection_dim, **kwargs):
        super().__init__(**kwargs)
        self.num_patches = num_patches
        self.projection = layers.Dense(units=projection_dim)
        self.position_embedding = layers.Embedding(
            input_dim=num_patches, output_dim=projection_dim
        )

    def call(self, patch):
        positions = tf.range(start=0, limit=self.num_patches, delta=1)
        encoded = self.projection(patch) + self.position_embedding(positions)
        return encoded

    ## personal addition
    def get_config(self):
        base_config = super().get_config()
        base_config.update({
            'num_patches': self.num_patches,
            'projection': self.projection,
            'position_embedding': self.position_embedding
        })
        return base_config
```

From looking at some related issues, I found how we need to provide a `get_config()` method to save and reload the model with custom classes so I made small personal modifications but now its sort of giving me a different issue I am unable to understand. 

Error Log:
```
  warnings.warn(
x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)
x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)
WARNING:tensorflow:From train.py:225: RunConfig.__init__ (from tensorflow_estimator.python.estimator.run_config) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras instead.
/home/nearchus/.local/lib/python3.8/site-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.
  warnings.warn(
Traceback (most recent call last):
  File ""train.py"", line 257, in <module>
    history = run_experiment(vit_classifier)
  File ""train.py"", line 231, in run_experiment
    model_est = keras.estimator.model_to_estimator(keras_model=model, model_dir='.', config=run_config)
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/estimator/__init__.py"", line 376, in model_to_estimator_v2
    return keras_lib.model_to_estimator(
  File ""/home/nearchus/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/keras_lib.py"", line 725, in model_to_estimator
    warm_start_path = _save_first_checkpoint(keras_model, custom_objects,
  File ""/home/nearchus/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/keras_lib.py"", line 457, in _save_first_checkpoint
    model = _clone_and_build_model(ModeKeys.TRAIN, keras_model,
  File ""/home/nearchus/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/keras_lib.py"", line 230, in _clone_and_build_model
    clone = tf.compat.v2.keras.__internal__.models.clone_and_build_model(
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/models/cloning.py"", line 806, in clone_and_build_model
    clone = clone_model(model, input_tensors=input_tensors)
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/models/cloning.py"", line 539, in clone_model
    return _clone_functional_model(
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/models/cloning.py"", line 222, in _clone_functional_model
    model_configs, created_layers = _clone_layers_and_model_config(
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/models/cloning.py"", line 298, in _clone_layers_and_model_config
    config = functional.get_network_config(
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/engine/functional.py"", line 1590, in get_network_config
    layer_config = serialize_layer_fn(layer)
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/models/cloning.py"", line 295, in _copy_layer
    created_layers[layer.name] = layer_fn(layer)
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/models/cloning.py"", line 52, in _clone_layer
    return layer.__class__.from_config(layer.get_config())
  File ""train.py"", line 108, in from_config
    return cls(**config)
TypeError: __init__() missing 1 required positional argument: 'projection_dim'
```
I thought it might be because of `PatchEncoder` class constructor has custom objects as argument - so i tried to do serialization/deserialization but to no vail. In any case, I would highly appreciate if someone can guide me as to where I am going wrong in this!  


  "
61390,Output mismatch between direct pass and looped pass through Dense layer,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.0 

### GPU model and memory

_No response_

### Current behavior?

When passing a large 3D tensor hrough a Dense layer using two different methods, the outputs are not always equal. Specifically, when the input tensor is relatively small, e.g., (2, 128, 42, 128), the outputs are equal, but when the input tensor is larger, e.g., (2, 8192, 42, 379), the outputs differ.
The tf.debugging.assert_near(y1, y2) statement does not trigger any assertion errors.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# create a random tensor
x = tf.random.uniform(shape=(2, 8192, 42, 379))

# create a dense layer
layer = tf.keras.layers.Dense(128)

# pass the input through the layer to get output
y1 = layer(x)

# pass the input using a for loop over the 3d dimension
y2 = tf.stack([layer(x[:, :, i, :]) for i in range(x.shape[2])], axis=2)

# check if the outputs are the same
print(tf.reduce_all(tf.equal(y1, y2)))
tf.debugging.assert_near(y1, y2)
```


### Relevant log output

_No response_"
61382,`tensorflow-macos` 2.13.0 missing x86 wheels,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

macOS 13 x86

### Mobile device

N/A

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tensorflow-macos` 2.13.0 release is [missing x86 wheels for macOS](https://pypi.org/project/tensorflow-macos/2.13.0/#files). This means that using tools like [Poetry](https://python-poetry.org/) or [`pip-tools`](https://github.com/jazzband/pip-tools) to lock versions, it's not possible to use 2.13.0 because if it's locked to that version, it will fail to install on x86 Macs.

[x86 wheels were provided for 2.12.0](https://pypi.org/project/tensorflow-macos/2.12.0/#files).

### Standalone code to reproduce the issue

From an x86 macOS machine:

```shell
python -m pip install tensorflow-macos==2.13.0
```


### Relevant log output

```shell
ERROR: Could not find a version that satisfies the requirement tensorflow-macos==2.13.0 (from versions: 2.9.0, 2.9.1, 2.9.2, 2.10.0, 2.11.0, 2.12.0)
ERROR: No matching distribution found for tensorflow-macos==2.13.0
```"
61379,PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.13.0

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.3.0

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

```
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
WARNING: Download from https://pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path validation failed: java.security.cert.CertPathValidatorException: validity check failed
ERROR: An error occurred during the fetch of repository 'gif':
   Traceback (most recent call last):
        File ""/home/hvn1kor/mnt/ws/tensorflow/third_party/repo.bzl"", line 73, column 33, in _tf_http_archive_impl
                ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz, https://pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz] to /home/hvn1kor/.cache/bazel/_bazel_hvn1kor/91c0f0c6277b7e83e39dca13c8fc40a9/external/gif/temp6645244175742553415/giflib-5.2.1.tar.gz: PKIX path validation failed: java.security.cert.CertPathValidatorException: validity check failed
ERROR: /home/hvn1kor/mnt/ws/tensorflow/WORKSPACE:15:14: fetching _tf_http_archive rule //external:gif: Traceback (most recent call last):        File ""/home/hvn1kor/mnt/ws/tensorflow/third_party/repo.bzl"", line 73, column 33, in _tf_http_archive_impl
                ctx.download_and_extract(
Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz, https://pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz] to /home/hvn1kor/.cache/bazel/_bazel_hvn1kor/91c0f0c6277b7e83e39dca13c8fc40a9/external/gif/temp6645244175742553415/giflib-5.2.1.tar.gz: PKIX path validation failed: java.security.cert.CertPathValidatorException: validity check failed
ERROR: /home/hvn1kor/mnt/ws/tensorflow/tensorflow/tools/pip_package/BUILD:205:10: //tensorflow/tools/pip_package:licenses depends on @gif//:COPYING in repository @gif which failed to fetch. no such package '@gif//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz, https://pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz] to /home/hvn1kor/.cache/bazel/_bazel_hvn1kor/91c0f0c6277b7e83e39dca13c8fc40a9/external/gif/temp6645244175742553415/giflib-5.2.1.tar.gz: PKIX path validation failed: java.security.cert.CertPathValidatorException: validity check failed   
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted:
INFO: Elapsed time: 35.731s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (1 packages loaded, 4 targets configured)
```

### Standalone code to reproduce the issue

```shell
bazel build //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_"
61378,"Cannot type ""I Accept"" to extract from hexagon_nn_skel","Workflow:
1. download hexagon_nn_skel.run from [this link](https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.20.0.1.run)
2. adb push to /data/local/tmp
3. run cmd: chmod +x tflite_hexagon_nn_skel_v1.20.0.1.run
4. run cmd: .\tflite_hexagon_nn_skel_v1.20.0.1.run
5. **Extraction aborted**: 
![Screenshot 2023-07-25 182129](https://github.com/tensorflow/tensorflow/assets/68681893/09484ae3-7156-4c61-9612-31c57fba8aa3)

Problem: 
After the program shows ""**_Type ""I ACCEPT"" if you agree to the terms of the license:_**"", it didn't give me time to type ""I ACCEPT"", and hence extraction aborted.
"
61377,`configure`: Error in detecting CUDA toolkit path,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

3b205a3

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 23.04

### Python version

3.11

### Bazel version

4.2.3

### GCC/compiler version

12.2.0

### CUDA/cuDNN version

11.8

### GPU model and memory

GTX 1050 Ti - 4 GB

### Current behavior?

I am trying to using `configure` script before building Tensorflow from source.

When I am trying to configure CUDA support using default list of base paths to look for CUDA libraries and headers, I get the following error:

```out
Inconsistent CUDA toolkit path: /usr vs /usr/lib
```

I have included the path of each tool in the next section.

I have tried different combinations for list of base paths and even changed `third_party/gpus/find_cuda_config.py` file to add default subdirectory paths but still get this error.

### Standalone code to reproduce the issue

Here's where everything is installed:

```bash
$ locate cuda.h
```

returns:

```out
/usr/include/cuda.h
```

and

```bash
$ which nvcc
```

returns:

```out
/usr/bin/nvcc
```

and

```bash
$ locate libcudart.so.11
```

returns:

```out
/usr/lib/x86_64-linux-gnu/libcudart.so.11.0
/usr/lib/x86_64-linux-gnu/libcudart.so.11.8.89
```

and

```bash
$ locate -r libdevice*.10.bc
```

returns:

```out
/usr/lib/nvidia-cuda-toolkit/libdevice/libdevice.10.bc
```

### Relevant log output

_No response_"
61376,OneDNN logs are not printing while building TF with --config=mkl_aarch64,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

6.3

### GCC/compiler version

11.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am expecting OneDNN logs should print while running deep learning model such as resnet50, if we export ONEDNN_VERBOSE=1

### Standalone code to reproduce the issue

```shell
To reproduce same, we have to build TF on Arm CPU, and use following command to build:
bazel build --config=mkl_aarch64 //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_"
61375,tf.keras.callbacks.SidecarEvaluatorModelExport doc page looks broken.,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The raw html tag is displayed and the display is collapsed when you access this link. https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/SidecarEvaluatorModelExport

### Standalone code to reproduce the issue

```shell
Please access from your browser.If it is not reproduced, I will share my detailed environment.
```


### Relevant log output

_No response_"
61374,float8 support for array ops,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12.0

### Custom code

No

### OS platform and distribution

macOS-13.2.1-arm64-arm-64bit

### Mobile device

_No response_

### Python version

3.9.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Please add FP8 datatype support for array ops (like Reshape, Transpose, GatherV2, ExpandDims, Squeeze, ConcatV2, Split, Pack, Unpack, and StridedSlice).

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.framework import dtypes

a = tf.constant([[1.2345678, 2.3456789, 3.4567891], [4.5678912, 5.6789123, 6.7891234]], dtype=dtypes.float16)
print(a)

a_fp8 = tf.cast(a, dtypes.float8_e4m3fn)
print(a_fp8)

b = a_fp8[1:2] # tensorflow.python.framework.errors_impl.NotFoundError
b = tf.transpose(a_fp8, [1, 0]) # tensorflow.python.framework.errors_impl.NotFoundError
```


### Relevant log output

_No response_"
61373, Invalid work group size,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.10.0

### Custom code

Yes

### OS platform and distribution

ubuntu 20.04

### Mobile device

Qualcomm SM6225

### Python version

3.8

### Bazel version

5.1.1

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

no

### GPU model and memory

QUALCOMM Adreno(TM) QUALCOMM OpenCL C 2.0 Adreno(TM) 610

### Current behavior?

use tflite to invoke mobilnet-v2 or any model with op softmax on mobile device, will encounter an error:
TfLiteGpuDelegate Invoke: Failed to clEnqueueNDRangeKernel - Invalid work group size.

More details:
if INFERENCE_FORCE_FP16 is false,  no error occured. 
if work group size is smaller than 256, no error occured.

### Standalone code to reproduce the issue

```shell
same code as tflite invoke on android phone will reproduce this issue.
```


### Relevant log output

_No response_"
61372,"""ValueError: Cannot take the length of shape with unknown rank.""  when using MultiHeadRelativeAttention","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.1

### Custom code

Yes

### OS platform and distribution

mac M2 pro

### Mobile device

mac M2 pro

### Python version

3.10.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

when using MultiHeadRelativeAttention from official.nlp.modeling.layers, I face on this error, ""ValueError: Cannot take the length of shape with unknown rank.""  I'm sorry  I'm not good at English. Thank you!


### Standalone code to reproduce the issue

```shell
from official.nlp.modeling.layers import MultiHeadRelativeAttention
import tensorflow as tf
vec= tf.constant([[[[0.1]*4]*3]*3])
layers=MultiHeadRelativeAttention(num_heads=4,key_dim=3)
output=layers(vec,vec,content_attention_bias=0.1, positional_attention_bias=0.1)
```


### Relevant log output

```shell
ValueError: Cannot take the length of shape with unknown rank.
```
"
61371,"RNG generators make python abort in python3.11 and tf2.12, conda-forge","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12.1

### Custom code

Yes

### OS platform and distribution

MacOS 13.4.1(c)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Unable to make random number generators. This happens with conda-forge (python 3.11.3 + tensorflow.2.12.1). It works fine with pip (python 3.11.3 + tensorflow 2.12.0)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
g = tf.random.Generator.from_seed(1)
g.normal(shape=[3])
```


### Relevant log output

```shell
Assertion failed: (f == nullptr || dynamic_cast<To>(f) != nullptr), function down_cast, file ./tensorflow/tsl/platform/default/casts.h, line 58.
[1]    18173 abort      python
```
"
61370,converting LSTM layer to tflite with float16 fails,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Colab

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Tflite converter fails to convert model with LSTM layer to float16 target.
It runs for a very long time, increases RAM consumption to the maximum system available, then crashes.

Expected behavior: should convert.

Workaround: instead of LSTM layer, use  RNN wrapper of LSTMCell instead of the LSTM layer. 
Tflite is successful in converting the alternative with float16 target.


### Standalone code to reproduce the issue

```shell
See this gist:
https://colab.research.google.com/gist/sronen71/9b016245f507280f867841a7161fad8d/keras-lstm-fusion-codelab.ipynb
```


### Relevant log output

```shell
Program crash after it is out of RAM.
```
"
61369,StringLookup layer does not retrieve vocabulary after saving and loading the model,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.11, 2.12, 2.13

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04.1

### Mobile device

_No response_

### Python version

3.9.5

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We noticed we could pickle the model right after building it, but unpickling would fail after saving and loading it from the disk. Upon further investigation, we realized the error was due to the vocabulary of the StringLookup layer, which was becoming an empty list after the tf.keras.models.load_model operation. 

The unpickling issue happens on TF 2.11 onwards. The unpickling worked on TF 2.8, 2.9, and 2.10, even though the empty vocabulary issue was still there.

#--------------------

Using the minimal reproducible example below, before saving the model, if we inspect the StringLookup layer we get:
```
full_model.layers[1].get_config()

Out[9]: {'name': 'string_lookup',
 'trainable': True,
 'dtype': 'int64',
 'invert': False,
 'max_tokens': None,
 'num_oov_indices': 1,
 'oov_token': '[UNK]',
 'mask_token': None,
 'output_mode': 'int',
 'sparse': False,
 'pad_to_max_tokens': False,
 'vocabulary': ListWrapper(['a', 'b']),
 'idf_weights': None,
 'encoding': 'utf-8'}
```

After saving and loading to the disk, we get:
```
full_model_loaded.layers[1].get_config()

Out[10]: {'name': 'string_lookup',
 'trainable': True,
 'dtype': 'int64',
 'invert': False,
 'max_tokens': None,
 'num_oov_indices': 1,
 'oov_token': '[UNK]',
 'mask_token': None,
 'output_mode': 'int',
 'sparse': False,
 'pad_to_max_tokens': False,
 'vocabulary': ListWrapper([]),
 'idf_weights': None,
 'encoding': 'utf-8'}
 ```

#-----------------------


We were able to circumvent the issue by creating a new class as follows:
```
@tf.keras.utils.register_keras_serializable()
class MyStringLookup(tf.keras.layers.StringLookup):
    def get_config(self):
        base_config = super().get_config()
        custom = {""vocabulary"": self.get_vocabulary()}
        return {**base_config, **custom}
```

However, it would be nice if we didn't have to create this wrapper.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import pickle

model_input = tf.keras.Input(shape=(1,), dtype=tf.int64)
lookup = tf.keras.layers.StringLookup(vocabulary=['a', 'b'])(model_input)
output = tf.keras.layers.Dense(10)(lookup)
full_model = tf.keras.Model(model_input, output)

# this part should work
model_bytes = pickle.dumps(full_model)
model_recovered = pickle.loads(model_bytes)


# this part should throw an error
full_model.save(""/tmp/temp_model"")
full_model_loaded = tf.keras.models.load_model(""/tmp/temp_model"")
model_bytes_2 = pickle.dumps(full_model_loaded)
model_recovered_2 = pickle.loads(model_bytes_2)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
File <command-901450068412846>:1
----> 1 model_recovered_2 = pickle.loads(model_bytes_2)

File /databricks/python/lib/python3.9/site-packages/keras/saving/pickle_utils.py:48, in deserialize_model_from_bytecode(serialized_model)
     46     model = saving_lib.load_model(filepath)
     47 except Exception as e:
---> 48     raise e
     49 else:
     50     return model

File /databricks/python/lib/python3.9/site-packages/keras/saving/pickle_utils.py:46, in deserialize_model_from_bytecode(serialized_model)
     40         f.write(serialized_model)
     41     # When loading, direct import will work for most custom objects
     42     # though it will require get_config() to be implemented.
     43     # Some custom objects (e.g. an activation in a Dense layer,
     44     # serialized as a string by Dense.get_config()) will require
     45     # a custom_object_scope.
---> 46     model = saving_lib.load_model(filepath)
     47 except Exception as e:
     48     raise e

File /databricks/python/lib/python3.9/site-packages/keras/saving/experimental/saving_lib.py:196, in load_model(filepath, custom_objects)
    194     h5_file.close()
    195 except Exception as e:
--> 196     raise e
    197 else:
    198     return model

File /databricks/python/lib/python3.9/site-packages/keras/saving/experimental/saving_lib.py:183, in load_model(filepath, custom_objects)
    181 config_dict = json.loads(config_json)
    182 # Construct the model from the configuration file in the archive.
--> 183 model = deserialize_keras_object(config_dict, custom_objects)
    184 h5_file = h5py.File(tf.io.gfile.join(temp_path, _VARS_FNAME), ""r"")
    185 _print_h5_file(h5_file, action=""loading"")

File /databricks/python/lib/python3.9/site-packages/keras/saving/experimental/serialization_lib.py:318, in deserialize_keras_object(config, custom_objects)
    315 # Instantiate the class from its config inside a custom object scope
    316 # so that we can catch any custom objects that the config refers to.
    317 with object_registration.custom_object_scope(custom_objects):
--> 318     return cls.from_config(inner_config)

File /databricks/python/lib/python3.9/site-packages/keras/engine/training.py:3114, in Model.from_config(cls, config, custom_objects)
   3107 functional_model_keys = [
   3108     ""name"",
   3109     ""layers"",
   3110     ""input_layers"",
   3111     ""output_layers"",
   3112 ]
   3113 if all(key in config for key in functional_model_keys):
-> 3114     inputs, outputs, layers = functional.reconstruct_from_config(
   3115         config, custom_objects
   3116     )
   3117     model = cls(
   3118         inputs=inputs, outputs=outputs, name=config.get(""name"")
   3119     )
   3120     functional.connect_ancillary_layers(model, layers)

File /databricks/python/lib/python3.9/site-packages/keras/engine/functional.py:1470, in reconstruct_from_config(config, custom_objects, created_layers)
   1468 # First, we create all layers and enqueue nodes to be processed
   1469 for layer_data in config[""layers""]:
-> 1470     process_layer(layer_data)
   1471 # Then we process nodes in order of layer depth.
   1472 # Nodes that cannot yet be processed (if the inbound node
   1473 # does not yet exist) are re-enqueued, and the process
   1474 # is repeated until all nodes are processed.
   1475 while unprocessed_nodes:

File /databricks/python/lib/python3.9/site-packages/keras/engine/functional.py:1451, in reconstruct_from_config.<locals>.process_layer(layer_data)
   1447 else:
   1448     # Instantiate layer.
   1449     from keras.layers import deserialize as deserialize_layer
-> 1451     layer = deserialize_layer(layer_data, custom_objects=custom_objects)
   1452     created_layers[layer_name] = layer
   1454 node_count_by_layer[layer] = int(_should_skip_first_node(layer))

File /databricks/python/lib/python3.9/site-packages/keras/layers/serialization.py:252, in deserialize(config, custom_objects)
    215 """"""Instantiates a layer from a config dictionary.
    216 
    217 Args:
   (...)
    249 
    250 """"""
    251 populate_deserializable_objects()
--> 252 return serialization.deserialize_keras_object(
    253     config,
    254     module_objects=LOCAL.ALL_OBJECTS,
    255     custom_objects=custom_objects,
    256     printable_module_name=""layer"",
    257 )

File /databricks/python/lib/python3.9/site-packages/keras/saving/legacy/serialization.py:527, in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    525     else:
    526         with object_registration.CustomObjectScope(custom_objects):
--> 527             deserialized_obj = cls.from_config(cls_config)
    528 else:
    529     # Then `cls` may be a function returning a class.
    530     # in this case by convention `config` holds
    531     # the kwargs of the function.
    532     custom_objects = custom_objects or {}

File /databricks/python/lib/python3.9/site-packages/keras/engine/base_layer.py:860, in Layer.from_config(cls, config)
    844 @classmethod
    845 def from_config(cls, config):
    846     """"""Creates a layer from its config.
    847 
    848     This method is the reverse of `get_config`,
   (...)
    858         A layer instance.
    859     """"""
--> 860     return cls(**config)

File /databricks/python/lib/python3.9/site-packages/keras/layers/preprocessing/string_lookup.py:333, in StringLookup.__init__(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary, idf_weights, encoding, invert, output_mode, sparse, pad_to_max_tokens, **kwargs)
    329     del kwargs[""dtype""]
    331 self.encoding = encoding
--> 333 super().__init__(
    334     max_tokens=max_tokens,
    335     num_oov_indices=num_oov_indices,
    336     mask_token=mask_token,
    337     oov_token=oov_token,
    338     vocabulary=vocabulary,
    339     vocabulary_dtype=tf.string,
    340     idf_weights=idf_weights,
    341     invert=invert,
    342     output_mode=output_mode,
    343     sparse=sparse,
    344     pad_to_max_tokens=pad_to_max_tokens,
    345     **kwargs
    346 )
    347 base_preprocessing_layer.keras_kpl_gauge.get_cell(""StringLookup"").set(
    348     True
    349 )

File /databricks/python/lib/python3.9/site-packages/keras/layers/preprocessing/index_lookup.py:323, in IndexLookup.__init__(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary_dtype, vocabulary, idf_weights, invert, output_mode, sparse, pad_to_max_tokens, **kwargs)
    320     self.idf_weights_const = self.idf_weights.value()
    322 if vocabulary is not None:
--> 323     self.set_vocabulary(vocabulary, idf_weights)
    324 else:
    325     # When restoring from a keras SavedModel, the loading code will
    326     # expect to find and restore a lookup_table attribute on the layer.
    327     # This table needs to be uninitialized as a StaticHashTable cannot
    328     # be initialized twice.
    329     self.lookup_table = self._uninitialized_lookup_table()

File /databricks/python/lib/python3.9/site-packages/keras/layers/preprocessing/index_lookup.py:510, in IndexLookup.set_vocabulary(self, vocabulary, idf_weights)
    507     idf_weights = np.array(idf_weights)
    509 if vocabulary.size == 0:
--> 510     raise ValueError(
    511         f""Cannot set an empty vocabulary, you passed {vocabulary}.""
    512     )
    514 oov_start = self._oov_start_index()
    515 token_start = self._token_start_index()

ValueError: Cannot set an empty vocabulary, you passed [].
```
"
61366,Unable to save model when using EfficientNetB0,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I’m trying to use EfficientNetB0 to create a model and save the model to my local disk. However, when saving it, it throws the error below.

> TypeError: Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.

Also, I tried to downgrade tensorflow from V2.12.0 to V2.9.1, this works as expected. In other words, this is a bug in 2.12.0. Hope it helps and please fix this bug for V2.12.0

### Standalone code to reproduce the issue

```shell
model = tf.keras.applications.EfficientNetB0()
model.save(""model"")
```


### Relevant log output

```shell
TypeError: Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.
```
"
61364,tf.io.gfile.rename not working for directories in S3,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The following code used to work in Tensorflow < 2.6. Upon Tensorflow 2.6, we had to import tensorflow_io. However, the tf.io.gfile.rename function which used to work on directories in S3 no longer works. I would like to update to a newer version of Tensorflow but this issue is preventing our organization from doing so, as some libraries we use use tf.io.gfile.rename to change folder names during training.

tf.io.gfile.rename should work on directories according to the Tensorflow documentation

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow_io as tfio

SOURCE_DIR = 's3://.../old_name/'
DEST_DIR = 's3://.../new_name/'

tf.io.gfile.rename(SOURCE_DIR, DEST_DIR)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/eigen/.config/JetBrains/PyCharm2023.1/scratches/tf2_s3_rename_test.py"", line 9, in <module>
    tf.io.gfile.rename(SOURCE_DIR, DEST_DIR)
  File ""/home/eigen/venvs/eigen-ml-tf2-12/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py"", line 622, in rename_v2
    _pywrap_file_io.RenameFile(
tensorflow.python.framework.errors_impl.FailedPreconditionError: Source is a directory or empty file
```
"
61363,Performance drop with tensorflow 2.13,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

Yes

### OS platform and distribution

Debian GNU/Linux 12 (bookworm)

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I notice a big performance drop between tensorflow 2.12 (CPU) and tensorflow 2.13 (CPU). With the last release (and also with tf-nightly '2.14.0-dev20230724') it takes *4 times* longer to perform a simple sum. It is between keras inputs though some I am not sure if this is directly related to tensorflow or if it comes from keras.

See code below for a very simple example. Note that this is with tensorflow CPU only.
The timings are:
- for tensorflow 2.12.1 + keras 2.12.0: **5.3 s**
- for tensorflow 2.13.0 + keras 2.13.1: **24 s**
- for tensorflow 2.14.0-dev20230724 + keras  2.14.0.dev2023072407: 26 s

### Standalone code to reproduce the issue

```shell
import time

from tensorflow.keras import Input


number_of_executions = 3000

x = Input((1,))
y = Input((1,))

start = time.time()
for i in range(number_of_executions):
    x + y
duration = time.time() - start
print(f""Duration: {duration:.1f}"")
```


### Relevant log output

_No response_"
61362,"When compiling TensorFlow 2.13 using bazel+clang, an error was reported as fatal error: 'stddef. h' file not found","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I use tensorflow/build:2.13-python3.11 image

When compiling TensorFlow 2.13 using bazel+clang, an error was reported as fatal error: 'stddef. h' file not found.
but I can find 'stddef. h' in docker container path **/usr/lib/llvm-16/lib/clang/16/include** ，if I manual append **-isystem /usr/lib/llvm-16/lib/clang/16/include** to clang compile command, the bellow code can run success 

### Standalone code to reproduce the issue

```shell
""cd /root/.cache/bazel/_bazel_root/645133528a7a8476e9bedcf54eb858b4/execroot/org_tensorflow && \
  exec env - \
    DOCKER_HOST_CACHEBUSTER=1682977560680045781 \
    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib \
    PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
  /usr/bin/clang-16 -MD -MF bazel-out/host/bin/external/zlib/_objs/zlib/uncompr.d '-frandom-seed=bazel-out/host/bin/external/zlib/_objs/zlib/uncompr.o' -iquote external/zlib -iquote bazel-out/host/bin/external/zlib  -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-11.8' -g0 -w -Wno-shift-negative-value -DZ_HAVE_UNISTD_H -c external/zlib/uncompr.c -o bazel-out/host/bin/external/zlib/_objs/zlib/uncompr.o""
this code will get fatal error: 'stddef. h' file not found.
```


### Relevant log output

_No response_"
61361,converter issue ,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
61360,cannot find explicitly assigned device for op,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.8.4

### Custom code

No

### OS platform and distribution

ubuntu18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.2.0

### GCC/compiler version

gcc-11.3.0

### CUDA/cuDNN version

none

### GPU model and memory

none

### Current behavior?

model trained use GPU, but infer machine don't have GPU device, so will meet cannot find assigend device at model load stage. here is log
```
E20230724 13:02:27.982717 35915 tf_model_core.cc:80] Failed to load model in /data/mfs6/new_wifi_models/tf_ranking_model/cvr_model/cvr_dynamic_embedding/2023-07-11.23/, Cannot assign a device for operation lr_weight-parameter_mht_1of1_lookup_table_export_values/TFRA>CuckooHashTableExport: {{node lr_weight-parameter_mht_1of1_lookup_table_export_values/TFRA>CuckooHashTableExport}} was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device. The requested device appears to be a GPU, but CUDA is not enabled.


```

### Standalone code to reproduce the issue

```shell
no code
```


### Relevant log output

_No response_"
61357,ColumnReduceKernel: min() type casting error and improvement,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Windows 10 22H2

### Mobile device

_No response_

### Python version

Anaconda 2023.07-1

### Bazel version

6.2.1

### GCC/compiler version

Visual Studio 2022 (build tools 14.36) + msys2-x86_64-20230718

### CUDA/cuDNN version

CUDA 11.8 + CUDNN 8.6.0 + TensorRT 8.5.3

### GPU model and memory

GTX 750 Ti 2GB

### Current behavior?

There are two type casting errors in reduction_gpu_kernels.cu.h under MSVC. One of them is fixed in https://github.com/tensorflow/tensorflow/pull/61339. Another is related to a TODO.

in [ColumnReduceKernel()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_gpu_kernels.cu.h#L341), the TODO said the followings:

> 1D array necessary due to bug in CUDA 9 compiler.
> TODO(nluehr) revert to 2D array when compiler is ready.
> This is to mimic the following, but without constructors:
> __shared__ storage_type<value_type> partial_sums[TF_RED_WARPSIZE *
> (TF_RED_WARPSIZE + 1)];

Since latest version required CUDA 11, it's time to address the TODO and apply bug fix together.

### Standalone code to reproduce the issue

```shell
1. download https://github.com/tensorflow/tensorflow/archive/refs/tags/v2.13.0.zip and extract
2. comment out Windows CUDA build rejection code in configure.py
3. run `python configure.py` to configure Windows CUDA build
4. run `bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package`
```


### Relevant log output

```shell
external/com_google_absl\absl/status/status.h(796): warning #2810-D: ignoring return value type with ""nodiscard"" attribute

.\tensorflow/tsl/platform/file_system.h(571): warning #611-D: overloaded virtual function ""tsl::FileSystem::FilesExist"" is only partially overridden in class ""tsl::WrappedFileSyste
m""

.\tensorflow/tsl/platform/file_system.h(571): warning #611-D: overloaded virtual function ""tsl::FileSystem::CreateDir"" is only partially overridden in class ""tsl::WrappedFileSystem
""

.\tensorflow/tsl/platform/env.h(500): warning #611-D: overloaded virtual function ""tsl::Env::RegisterFileSystem"" is only partially overridden in class ""tsl::EnvWrapper""

.\tensorflow/tsl/platform/float8.h(936): warning #177-D: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To tsl::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=tsl::float8_i
nternal::float8_e4m3b11, To=tsl::float8_internal::float8_e4m3fn, kSaturate=false, kTruncate=false]""
(1018): here
            instantiation of ""Derived tsl::float8_internal::float8_base<Derived>::ConvertFrom(const From &) [with Derived=tsl::float8_internal::float8_e4m3fn, kSaturate=false, kTru
ncate=false, From=tsl::float8_internal::float8_e4m3b11]""
(277): here

.\tensorflow/tsl/platform/float8.h(936): warning #177-D: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To tsl::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=tsl::float8_i
nternal::float8_e4m3b11, To=float, kSaturate=false, kTruncate=false]""
(1024): here
            instantiation of ""To tsl::float8_internal::float8_base<Derived>::ConvertTo<To,kSaturate,kTruncate>(const Derived &) [with Derived=tsl::float8_internal::float8_e4m3b11,
To=float, kSaturate=false, kTruncate=false]""
(75): here
            instantiation of ""tsl::float8_internal::float8_base<Derived>::operator float() const [with Derived=tsl::float8_internal::float8_e4m3b11]""
(116): here
            instantiation of ""Derived tsl::float8_internal::float8_base<Derived>::operator-(const Derived &) const [with Derived=tsl::float8_internal::float8_e4m3b11]""
(302): here

.\tensorflow/core/kernels/reduction_gpu_kernels.cu.h(392): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (int, unsigned int)
          detected during:
            instantiation of ""void tensorflow::functor::ColumnReduceKernel(T, OUT_T, int, int, Op, std::iterator_traits<T>::value_type) [with T=const float *, OUT_T=float *, Op=cub
::Max]""
(828): here
            instantiation of ""void tensorflow::functor::LaunchColumnReduction_LTE4096Cols(tensorflow::OpKernelContext *, OUT_T, IN_T, int, int, Op, T, const gpuStream_t &) [with T=
float, Op=cub::Max, OUT_T=float *, IN_T=const float *]""
(862): here
            instantiation of ""void tensorflow::functor::LaunchColumnReduction(tensorflow::OpKernelContext *, OUT_T, IN_T, int, int, Op, T, const gpuStream_t &) [with T=float, Op=cu
b::Max, OUT_T=float *, IN_T=const float *]""
(1088): here
            instantiation of ""void tensorflow::functor::ReduceImpl<T,Op,OUT_T,IN_T,ReductionAxes>(tensorflow::OpKernelContext *, OUT_T, IN_T, int, int, int, int, int, const Reducti
onAxes &, Op) [with T=float, Op=cub::Max, OUT_T=float *, IN_T=const float *, ReductionAxes=const Eigen::array<Eigen::DenseIndex, 1ULL> &]""
E:\_bazel_tensorflow\4zvk5ci6\execroot\org_tensorflow\tensorflow\core\kernels\multinomial_op_gpu.cu.cc(111): here
            instantiation of ""void tensorflow::functor::MultinomialFunctor<tensorflow::functor::GPUDevice, T, OutputType>::operator()(tensorflow::OpKernelContext *, const tensorflo
w::functor::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<float, 1, Eigen::DenseIndex>::Flat, tensorflow::TTypes<float, 1, Eigen::DenseI
ndex>::Flat, tensorflow::TTypes<float, 1, Eigen::DenseIndex>::Flat, int, int, int, const tsl::random::PhiloxRandom &, tensorflow::TTypes<OutputType, 1, Eigen::DenseIndex>::Matrix)
[with T=Eigen::half, OutputType=tsl::int32]""
E:\_bazel_tensorflow\4zvk5ci6\execroot\org_tensorflow\tensorflow\core\kernels\multinomial_op_gpu.cu.cc(126): here

1 error detected in the compilation of ""tensorflow/core/kernels/multinomial_op_gpu.cu.cc"".
nvcc warning : The 'compute_35', 'compute_37', 'sm_35', and 'sm_37' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppres
s warning).
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1996.828s, Critical Path: 480.15s
INFO: 441 processes: 7 internal, 434 local.
FAILED: Build did NOT complete successfully
```
"
61354,rocm_helpers missing dependency declarations,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

master/nightly

### Custom code

Yes

### OS platform and distribution

`Arch Linux (Linux 6.4.4-arch1-1 #1 SMP PREEMPT_DYNAMIC x86_64 GNU/Linux)`

### Mobile device

N/A

### Python version

3.10

### Bazel version

6.1.0

### GCC/compiler version

gcc (GCC) 13.1.1 20230714

### CUDA/cuDNN version

None

### GPU model and memory

AMD Radeon RX 7900 XT

### Current behavior?

After adding `#include <stdint.h>` to line 16 of `tensorflow/tsl/lib/io/cache.cc` to fix a different error, and using the installation method described in the reproduce field.

Bazel gives the error described in the attached log.

This persists through different Bazel versions, and full cleans.

I am using the following archlinux packages for ROCm:
```
local/opencl-amd 1:5.6.0-2
    ROCr OpenCL stack
local/opencl-amd-dev 1:5.6.0-1
    OpenCL SDK / HIP SDK / ROCM Compiler.
```

### Standalone code to reproduce the issue

```shell
./configure
You have bazel 6.1.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]:

Found possible Python library paths:
  /usr/lib/python3.11/site-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3.11/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: y
ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]:
No CUDA support will be enabled for TensorFlow.

Do you want to use Clang to build TensorFlow? [Y/n]:
Clang will be used to compile TensorFlow.

Please specify the path to clang executable. [Default is /usr/bin/clang]:

You have Clang 17.0.0 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.



bazel build --config=opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /home/user/Repos/tensorflow/tensorflow/compiler/xla/stream_executor/rocm/BUILD:527:11: Compiling tensorflow/compiler/xla/stream_executor/rocm/rocm_helpers.cu.cc [for tool] failed: undeclared inclusion(s) in rule '//tensorflow/compiler/xla/stream_executor/rocm:rocm_helpers':
this rule is missing dependency declarations for the following files included by 'tensorflow/compiler/xla/stream_executor/rocm/rocm_helpers.cu.cc':
  '/opt/rocm-5.6.0/include/hip/hip_version.h'
  '/opt/rocm-5.6.0/include/hip/hip_runtime.h'
  '/opt/rocm-5.6.0/include/hip/hip_common.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_runtime.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_common.h'
  '/opt/rocm-5.6.0/include/hip/hip_runtime_api.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/host_defines.h'
  '/opt/rocm-5.6.0/include/hip/driver_types.h'
  '/opt/rocm-5.6.0/include/hip/texture_types.h'
  '/opt/rocm-5.6.0/include/hip/channel_descriptor.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_channel_descriptor.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_vector_types.h'
  '/opt/rocm-5.6.0/include/hip/surface_types.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_runtime_pt_api.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/hip_ldg.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_atomic.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_device_functions.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/math_fwd.h'
  '/opt/rocm-5.6.0/include/hip/hip_vector_types.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/device_library_decls.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_warp_functions.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_unsafe_atomics.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_surface_functions.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/ockl_image.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/texture_fetch_functions.h'
  '/opt/rocm-5.6.0/include/hip/hip_texture_types.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/texture_indirect_functions.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_math_functions.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/hip_fp16_math_fwd.h'
  '/opt/rocm-5.6.0/include/hip/library_types.h'
  '/opt/rocm-5.6.0/include/hip/hip_bfloat16.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_bfloat16.h'
  '/opt/rocm-5.6.0/include/hip/hip_fp16.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_fp16.h'
clang-16: warning: argument unused during compilation: '-fcuda-flush-denormals-to-zero' [-Wunused-command-line-argument]
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3.270s, Critical Path: 3.10s
INFO: 77 processes: 53 internal, 24 local.
FAILED: Build did NOT complete successfully
```
"
61367,Need Help with TensorFlow Lite Model Running on GPU - Output Interpretation Issue (Android Studio Kotlin),"Hello. I have created an Android application in Android Studio that uses a tflite model. Its implementation works without any issues and looks as follows:

val model = Ssd.newInstance(context)

// Creates inputs for reference.
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 320, 320, 3), DataType.UINT8)
inputFeature0.loadBuffer(byteBuffer)

// Runs model inference and gets result.
val outputs = model.process(inputFeature0)
val outputFeature0 = outputs.outputFeature0AsTensorBuffer
val outputFeature1 = outputs.outputFeature1AsTensorBuffer
val outputFeature2 = outputs.outputFeature2AsTensorBuffer
val outputFeature3 = outputs.outputFeature3AsTensorBuffer

// Releases model resources if no longer used.
model.close()

However, the application is running slowly, and I would like to perform the model computations on the GPU.

I am facing an issue with the input and output parts.

I couldn't find any information about it anywhere. The current code looks like this:

val options = Interpreter.Options().apply {
    if(compatList.isDelegateSupportedOnThisDevice) {
        val delegateOptions = compatList.bestOptionsForThisDevice
        this.addDelegate(GpuDelegate(delegateOptions))
    } else {
        this.setNumThreads(4)
    }
}
interpreter = Interpreter(loadModelFile(assets,""Ssd.tflite""), options)
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 320, 320, 3), DataType.FLOAT32)
inputFeature0.loadBuffer(byteBuffer)

Then, I should create the input.buffer for the main line:

interpreter.run(inputFeature0.buffer, outputs.buffer)

I tried doing some adjustments, but the outputs.buffer I got as a result was something I couldn't interpret. Has anyone encountered a similar problem? If so, please, I would appreciate your help.

"
61353,TFLite Error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 11
- TensorFlow installed from (source or binary):source 
- TensorFlow version (or github SHA if from source):2.15.0


**Provide the text output from tflite_convert**
The below is the code, I am using to convert the deep learning model to tflite

converter = tf.lite.TFLiteConverter.from_keras_model(best_model)

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]

tflite_model = converter.convert()

with open('compressed_model.tflite', 'wb') as f:
f.write(tflite_model)


**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1QlquN0xR94xMdiUNer00Nu0n5UDXdiWQ
![error](https://github.com/tensorflow/tensorflow/assets/107172150/19056701-1422-4ab2-939a-545f3f799f48)

"
61352,LSTM loss does not work TPU with bfloat16,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

Google Colab  + TPU

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

LSTM on TPU works in float32.
Gives error message in bfloat16:
Value passed to parameter 'input' has DataType bfloat16 not in list of allowed values: float16, float32, float64

### Standalone code to reproduce the issue

```shell
Colab Code in this gist:
https://colab.research.google.com/gist/sronen71/cacdc527ea3a7d267e5f47e6dcc8f17f/working_with_rnns.ipynb.

Run with TPU.
```


### Relevant log output

```shell
TypeError                                 Traceback (most recent call last)

<ipython-input-12-d2060b3c689a> in <cell line: 1>()
      1 with strategy.scope():
----> 2   model = build_model()
      3 
      4   model.compile(
      5     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),

3 frames

/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)
     54     allowed_values = "", "".join(dtypes.as_dtype(x).name for x in allowed_list)
     55     if dtype not in allowed_list:
---> 56       raise TypeError(
     57           f""Value passed to parameter '{param_name}' has DataType ""
     58           f""{dtypes.as_dtype(dtype).name} not in list of allowed values: ""

TypeError: Exception encountered when calling layer ""lstm_4"" (type LSTM).

Value passed to parameter 'input' has DataType bfloat16 not in list of allowed values: float16, float32, float64

Call arguments received by layer ""lstm_4"" (type LSTM):
  • inputs=tf.Tensor(shape=(None, None, 128), dtype=bfloat16)
  • mask=None
  • training=None
  • initial_state=None
```
"
61351,Tensorflow Load Datasets Failure for Python 3.11.4 and Tensorflow 2.13.0,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Rocky Linux 8.7

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8/8.9.0.131-1

### GPU model and memory

_No response_

### Current behavior?

Dataset written and loaded in python 3.11.4 and tensorflow 2.12.1 should load in tensorflow 2.13

However, loading dataset in Tensorflow 2.13 with python 3.11.4 fails on Linux and windows: 

TensorFlow version: 2.13.0
Python version: 3.11.4
Download dev dataset...
Extract dev dataset...

Loading dev dataset...
[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.
[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 14022746025082002701
Download train dataset...
Extract train dataset...

Loading train dataset...
[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.
[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 10775564831112808841

### Standalone code to reproduce the issue

```shell
import io
import sys
from zipfile import ZipFile

import requests
import tensorflow as tf

print(""TensorFlow version:"", tf.__version__)
print(""Python version:"", sys.version.split()[0])

dev_url = (
    ""https://drive.google.com/uc?export=download&id=1-MJAgrTNZkaMpyBQLIgqqwM8gP9LKdDL""
)

print(""Download dev dataset..."")
r = requests.get(dev_url)
z = ZipFile(io.BytesIO(r.content))
print(""Extract dev dataset..."")
z.extractall()
print(""\nLoading dev dataset..."")
ds_dev = tf.data.Dataset.load(""squadv2_dev_tf"")

train_url = (
    ""https://drive.google.com/uc?export=download&id=1-NWGcJz0ZaFGFeHOPG2PKvn8gmf3MwKn""
)
print(""Download train dataset..."")
r = requests.get(train_url)
z = ZipFile(io.BytesIO(r.content))
print(""Extract train dataset..."")
z.extractall()
print(""\nLoading train dataset..."")
ds_train = tf.data.Dataset.load(""squadv2_train_tf"")
```


### Relevant log output

_No response_"
61348,esrgan re-convert to tflite fail,"### 1. System information

- OS Platform and Distribution: macOS 12.2.1; Apple M1; MacBook Pro
- TensorFlow installation : pip3 install tensorflow
- TensorFlow library: 2.13.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
convert tflite:  https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/ml/super_resolution.ipynb
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
test demo:  https://github.com/tensorflow/examples/tree/master/lite/examples/super_resolution
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces some errors

### 4. (optional) RNN conversion support
model is esrgan

### 5. (optional) Any other info / logs
case1.  enable optimize like ""tf.lite.Optimize.DEFAULT"",  load model fail: Didn't find op for builtin opcode 'DEQUANTIZE' version '5'
case2. disable optimize, set_shape 50x50, run fail msg: Something went wrong when copying input buffer to input tensor
case3. disable optimize, set_shape 640x360, run fail msg: signal 11 (SIGSEGV): stack pointer is in a non-existent map; likely due to stack overflow.  function crash: SuperResolution.cpp->DoSuperResolution() line at: TfLiteInterpreterAllocateTensors
"
61347,AssertionError: Tried to export a function which references an 'untracked' resource. TensorFlow objects(e.g. tf.Variable),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

As you can see in the code below, the operation of multiplying **attn** by **temperature**  at **MDTA** is being performed. And the **temperature** is defined by tf.Variable().

The model using the attention module below runs normally until training(model.fit()), but an AssertionError occurs when saving the model. I would appreciate it if you could tell me how to make the **temperature** variable trackable.

### Standalone code to reproduce the issue

```shell
# import tensorflow.compat.v2 as tf
import tensorflow as tf
import keras
from keras import backend
from keras.applications import imagenet_utils
from keras.engine import training
from keras.layers import VersionAwareLayers
from keras.utils import data_utils
from keras.utils import layer_utils

from keras.layers import Layer, Activation, ReLU, Concatenate, Conv2D, Add, Dense, MaxPool2D, AvgPool2D, Flatten, multiply, Softmax
from keras.layers import Dropout, Dense, GlobalAveragePooling2D, Input, BatchNormalization, DepthwiseConv2D, ZeroPadding2D, LayerNormalization
from tensorflow.keras import backend as K

from keras.models import Model
#import tensorflow.keras

# 정상적으로 작동 (temperature제외)

# isort: off
from tensorflow.python.platform import tf_logging as logging
from tensorflow.python.util.tf_export import keras_export

BASE_WEIGHT_PATH = (
    ""https://storage.googleapis.com/tensorflow/"" ""keras-applications/mobilenet/""
)


@keras_export(
    ""keras.applications.mobilenet.MobileNet"", ""keras.applications.MobileNet""
)
def MobileNet(
    input_shape=None,
    alpha=1.0,
    depth_multiplier=1,
    dropout=1e-3,
    include_top=True,
    weights=""imagenet"",
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation=""softmax"",
    **kwargs,
):

#     global layers
#     if ""layers"" in kwargs:
#         layers = kwargs.pop(""layers"")
#     else:
#         layers = VersionAwareLayers()
    if kwargs:
        raise ValueError(f""Unknown argument(s): {(kwargs,)}"")
    if not (weights in {""imagenet"", None} or tf.io.gfile.exists(weights)):
        raise ValueError(
            ""The `weights` argument should be either ""
            ""`None` (random initialization), `imagenet` ""
            ""(pre-training on ImageNet), ""
            ""or the path to the weights file to be loaded.  ""
            f""Received weights={weights}""
        )

    if weights == ""imagenet"" and include_top and classes != 1000:
        raise ValueError(
            'If using `weights` as `""imagenet""` with `include_top` '
            ""as true, `classes` should be 1000.  ""
            f""Received classes={classes}""
        )

    # Determine proper input shape and default size.
    if input_shape is None:
        default_size = 224
    else:
        if backend.image_data_format() == ""channels_first"":
            rows = input_shape[1]
            cols = input_shape[2]
        else:
            rows = input_shape[0]
            cols = input_shape[1]

        if rows == cols and rows in [128, 160, 192, 224]:
            default_size = rows
        else:
            default_size = 224

    input_shape = imagenet_utils.obtain_input_shape(
        input_shape,
        default_size=default_size,
        min_size=32,
        data_format=backend.image_data_format(),
        require_flatten=include_top,
        weights=weights,
    )

    if backend.image_data_format() == ""channels_last"":
        row_axis, col_axis = (0, 1)
    else:
        row_axis, col_axis = (1, 2)
    rows = input_shape[row_axis]
    cols = input_shape[col_axis]

    if weights == ""imagenet"":
        if depth_multiplier != 1:
            raise ValueError(
                ""If imagenet weights are being loaded, ""
                ""depth multiplier must be 1.  ""
                f""Received depth_multiplier={depth_multiplier}""
            )

        if alpha not in [0.25, 0.50, 0.75, 1.0]:
            raise ValueError(
                ""If imagenet weights are being loaded, ""
                ""alpha can be one of""
                ""`0.25`, `0.50`, `0.75` or `1.0` only.  ""
                f""Received alpha={alpha}""
            )

        if rows != cols or rows not in [128, 160, 192, 224]:
            rows = 224
            logging.warning(
                ""`input_shape` is undefined or non-square, ""
                ""or `rows` is not in [128, 160, 192, 224]. ""
                ""Weights for input shape (224, 224) will be ""
                ""loaded as the default.""
            )

    if input_tensor is None:
        img_input = Input(shape=input_shape)
    else:
        if not backend.is_keras_tensor(input_tensor):
            img_input = Input(tensor=input_tensor, shape=input_shape)
        else:
            img_input = input_tensor
    
    num_heads = 4
    expansion_factor = 3
    
    x = _conv_block(img_input, 32, alpha, strides=(2, 2))
    x = _transformer_block(x, num_heads, expansion_factor)
        
    x = _depthwise_conv_block(x, 64, alpha, depth_multiplier, block_id=1)
    x = _transformer_block(x, num_heads, expansion_factor)

    x = _depthwise_conv_block(
        x, 128, alpha, depth_multiplier, strides=(2, 2), block_id=2)
    x = _transformer_block(x, num_heads, expansion_factor)
    
    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier, block_id=3)
    x = _transformer_block(x, num_heads, expansion_factor)
    
    x = _depthwise_conv_block(
        x, 256, alpha, depth_multiplier, strides=(2, 2), block_id=4)
    x = _transformer_block(x, num_heads, expansion_factor)
    
    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier, block_id=5)
    x = _transformer_block(x, num_heads, expansion_factor)

    x = _depthwise_conv_block(
        x, 512, alpha, depth_multiplier, strides=(2, 2), block_id=6)
    #x = _transformer_block(x, num_heads, expansion_factor)
    
    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=7)
    #x = _transformer_block(x, num_heads, expansion_factor)
    
    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=8)
    #x = _transformer_block(x, num_heads, expansion_factor)
    
    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=9)
    #x = _transformer_block(x, num_heads, expansion_factor)
    
    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=10)
    #x = _transformer_block(x, num_heads, expansion_factor)
    
    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=11)
    #x = _transformer_block(x, num_heads, expansion_factor)

    x = _depthwise_conv_block(
        x, 1024, alpha, depth_multiplier, strides=(2, 2), block_id=12)
    #x = _transformer_block(x, num_heads, expansion_factor)
    
    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier, block_id=13)
    #x = _transformer_block(x, num_heads, expansion_factor)

    if include_top:
        x = layers.GlobalAveragePooling2D(keepdims=True)(x)
        x = layers.Dropout(dropout, name=""dropout"")(x)
        x = layers.Conv2D(classes, (1, 1), padding=""same"", name=""conv_preds"")(x)
        x = layers.Reshape((classes,), name=""reshape_2"")(x)
        imagenet_utils.validate_activation(classifier_activation, weights)
        x = layers.Activation(
            activation=classifier_activation, name=""predictions""
        )(x)
    else:
        if pooling == ""avg"":
            x = GlobalAveragePooling2D()(x)
        elif pooling == ""max"":
            x = GlobalMaxPooling2D()(x)

    # Ensure that the model takes into account
    # any potential predecessors of `input_tensor`.
    if input_tensor is not None:
        inputs = layer_utils.get_source_inputs(input_tensor)
    else:
        inputs = img_input

    # Create model.
    model = training.Model(inputs, x, name=""mobilenet_%0.2f_%s"" % (alpha, rows))

    # Load weights.
    if weights == ""imagenet"":
        if alpha == 1.0:
            alpha_text = ""1_0""
        elif alpha == 0.75:
            alpha_text = ""7_5""
        elif alpha == 0.50:
            alpha_text = ""5_0""
        else:
            alpha_text = ""2_5""

        if include_top:
            model_name = ""mobilenet_%s_%d_tf.h5"" % (alpha_text, rows)
            weight_path = BASE_WEIGHT_PATH + model_name
            weights_path = data_utils.get_file(
                model_name, weight_path, cache_subdir=""models""
            )
        else:
            model_name = ""mobilenet_%s_%d_tf_no_top.h5"" % (alpha_text, rows)
            weight_path = BASE_WEIGHT_PATH + model_name
            weights_path = data_utils.get_file(
                model_name, weight_path, cache_subdir=""models""
            )
        model.load_weights(weights_path, by_name=True)
    elif weights is not None:
        model.load_weights(weights, by_name=True)

    return model


class MDTA(keras.layers.Layer):
    '''***IMPORTANT*** - The channels must be zero when divided by num_heads'''
    def __init__(self, num_heads):
        super(MDTA, self).__init__()
        self.num_heads = num_heads
        #self.temperature = tf.Variable([[[[1.]] for _ in range(self.num_heads)]], shape=[None, self.num_heads, 1, 1], trainable=True)

    def build(self, inputs):
        '''(N, H, W, C) -> (N, H, W, C)
           Output of MDTA feature should be added to input feature x'''
        b, h, w, c = inputs.shape
        
        # --------------------  Layers  -------------------- 
        qkv = Conv2D(filters=c*3, kernel_size=1, use_bias=False) 
        qkv_conv = Conv2D(c*3, kernel_size=3, padding='same', groups=c*3, use_bias=False)
        project_out = Conv2D(filters=c, kernel_size=1, use_bias=False)

        temperature = tf.Variable([[[[1.]] for _ in range(self.num_heads)]], shape=[None, self.num_heads, 1, 1], trainable=True)
        
        # --------------------  forward  -------------------- 
        q, k, v = tf.split(qkv_conv(qkv(inputs)), num_or_size_splits=3, axis=-1)
        
        # divide the # of channels into heads & learn separate attention map
        q = tf.reshape(q, [-1, self.num_heads, c//self.num_heads, h * w])  # (N, num_heads, C/num_heads, HW)
        k = tf.reshape(k, [-1, self.num_heads, c//self.num_heads, h * w])
        v = tf.reshape(v, [-1, self.num_heads, c//self.num_heads, h * w])
        
        q, k = tf.nn.l2_normalize(q, axis=-1), tf.nn.l2_normalize(k, axis=-1)

        # CxC Attention map instead of HWxHW (when num_heads=1)
        attn = tf.matmul(q, k, transpose_b=True) 
        attn = multiply([attn, temperature])
        attn = Softmax(axis=-1)(attn)
        
        out = tf.matmul(attn, v)
        shape = [tf.shape(out)[k] for k in range(4)]  # [Batch, num_heads, c/num_heads, H*W]
        out = tf.reshape(out,  [shape[0], h, w, shape[1]*shape[2]])
        out = project_out(out)  # attn*v: (N, num_heads, C/num_heads, HW)
        return out
    
    def __call__(self, inputs):
        return self.build(inputs)
    

class GDFN(keras.layers.Layer):
    def __init__(self):
        super(GDFN, self).__init__()
        self.expansion_factor = 2

    def build(self, inputs):
        '''(N, H, W, C) -> (N, H, W, C) with expansion_factor=r
           Output of GDFN feature should be added to input feature x'''
        b, h, w, c = inputs.shape
        hidden_channels = int(c * self.expansion_factor)  # channel expansion 
        
        # --------------------  Layers  -------------------- 
        project_in = Conv2D(hidden_channels * 2, kernel_size=1, use_bias=False)
        conv = Conv2D(hidden_channels * 2, kernel_size=3, padding='same',
                      groups=hidden_channels * 2, use_bias=False)
        project_out = Conv2D(c, kernel_size=1, use_bias=False)
        
        # --------------------  Forward  -------------------- 
        x = project_in(inputs)  # (N, H, W, 2Cr)
        x = conv(x)  # (N, H, W, 2Cr)

        x1, x2 = tf.split(x, num_or_size_splits=2, axis=-1)  # (N, H, W, Cr), (N, H, W, Cr)

        # Gating: the element-wise product of 2 parallel paths of linear transformation layers 
        out = ReLU()(x1) * x2  # (N, H, W, Cr)
        out = project_out(out)  # (N, H, W, Cr)
        return out
    
    def __call__(self, inputs):
        return self.build(inputs)
    
    
def _transformer_block(inputs, num_heads, expansion_factor):
    '''(N, H, W, C) -> (N, H, W, C)'''
    
    shape = [tf.shape(inputs)[k] for k in range(4)]
    b, h, w, c = inputs.shape[0], inputs.shape[1], inputs.shape[2], inputs.shape[3]
    assert c % num_heads == 0   

    norm1 = LayerNormalization()  # default: axis=-1
    attn = MDTA(num_heads)
    norm2 = LayerNormalization()
    ffn = GDFN()
        
    # Add MDTA output feature
    inputs_norm1 = norm1(tf.reshape(inputs, [-1, h*w, c]))
    inputs_norm1 = tf.reshape(inputs_norm1, [-1, h, w, c])
        
    inputs = inputs + attn(inputs_norm1)
        
    # ADD GDFN output feature
    inputs_norm2 = norm2(tf.reshape(inputs, [-1, h*w, c]))
    inputs_norm2 = tf.reshape(inputs_norm2, [-1, h, w, c])
        
    x = inputs + ffn(inputs_norm2)
        
    return x
    
    
def _conv_block(inputs, filters, alpha, kernel=(3, 3), strides=(1, 1)):
    channel_axis = 1 if backend.image_data_format() == ""channels_first"" else -1
    filters = int(filters * alpha)
    x = Conv2D(
        filters,
        kernel,
        padding=""same"",
        use_bias=False,
        strides=strides,
        name=""conv1"",
    )(inputs)
    x = BatchNormalization(axis=channel_axis, name=""conv1_bn"")(x)
    return ReLU(6.0, name=""conv1_relu"")(x)


def _depthwise_conv_block(
    inputs,
    pointwise_conv_filters,
    alpha,
    depth_multiplier=1,
    strides=(1, 1),
    block_id=1,
):
    channel_axis = 1 if backend.image_data_format() == ""channels_first"" else -1
    pointwise_conv_filters = int(pointwise_conv_filters * alpha)

    if strides == (1, 1):
        x = inputs
    else:
        x = ZeroPadding2D(
            ((0, 1), (0, 1)), name=""conv_pad_%d"" % block_id
        )(inputs)
    x = DepthwiseConv2D(
        (3, 3),
        padding=""same"" if strides == (1, 1) else ""valid"",
        depth_multiplier=depth_multiplier,
        strides=strides,
        use_bias=False,
        name=""conv_dw_%d"" % block_id,
    )(x)

    x = BatchNormalization(
        axis=channel_axis, name=""conv_dw_%d_bn"" % block_id
    )(x)
    x = ReLU(6.0, name=""conv_dw_%d_relu"" % block_id)(x)

    x = Conv2D(
        pointwise_conv_filters,
        (1, 1),
        padding=""same"",
        use_bias=False,
        strides=(1, 1),
        name=""conv_pw_%d"" % block_id,
    )(x)
    x = BatchNormalization(
        axis=channel_axis, name=""conv_pw_%d_bn"" % block_id
    )(x)
    return ReLU(6.0, name=""conv_pw_%d_relu"" % block_id)(x)


def gen_mobilenetv1_mdta(input_shape, dropout_rate, num_class):
    if input_shape==(224, 224, 3):
        weights = 'imagenet'
    else:
        weights = None
        
    base_model = MobileNet(weights=weights,
                           include_top=False, 
                           input_tensor=Input(input_shape),
                           input_shape=input_shape)
    
    base_model.trainable = True
    x = base_model.output
    head_layer = tf.keras.Sequential([tf.keras.layers.GlobalAveragePooling2D(name='simple_classifier_pooling'),
                         tf.keras.layers.Dropout(dropout_rate, name='simple_classifier_dropout'),
                         tf.keras.layers.Dense(512, activation='relu', name='simple_classifier_dense1'),
                         tf.keras.layers.Dense(num_class, activation='softmax'),])
    predictions = head_layer(x)

    # this is the model we will train
    model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)
    #print(model)
    return model

input_shape = (768, 768, 3)
x = Input(input_shape)
model = gen_mobilenetv1_mdta(input_shape, 0.3, 6)
out = model(x)

save_path = 'D:/model_mdta.h5'
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['acc'])
model.save(save_path )
 
```


### Relevant log output

In the vscode-terminal

```shell
AssertionError: Tried to export a function which references an 'untracked' resource. TensorFlow objects (e.g. tf.Variable) captured by functions must be 'tracked' by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly. See the information below:
        Function name = b'__inference_signature_wrapper_18418'
        Captured Tensor = <ResourceHandle(name=""Resource-40-at-0x55f74d07bcf0"", device=""/job:localhost/replica:0/task:0/device:CPU:0"", container=""Anonymous"", type=""tensorflow::Var"", dtype and shapes : ""[ DType enum: 1, Shape: [?,4,1,1] ]"")>
        Trackable referencing this tensor = <tf.Variable 'Variable:0' shape=(None, 4, 1, 1) dtype=float32>
        Internal Tensor = Tensor(""18172:0"", shape=(), dtype=resource)

During handling of the above exception, another exception occurred:
```

In the Jupyter-Noetebook

```shell
ValueError                                Traceback (most recent call last)
/tmp/ipykernel_828829/1178680124.py in <module>
      1 model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['acc'])
----> 2 model.save('D:/model_mdta.h5')

~/conda/envs/hrvi2/lib/python3.8/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---> 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

~/conda/envs/hrvi2/lib/python3.8/json/encoder.py in encode(self, o)
    197         # exceptions aren't as detailed.  The list call should be roughly
    198         # equivalent to the PySequence_Fast that ''.join() would do.
--> 199         chunks = self.iterencode(o, _one_shot=True)
    200         if not isinstance(chunks, (list, tuple)):
    201             chunks = list(chunks)

~/conda/envs/hrvi2/lib/python3.8/json/encoder.py in iterencode(self, o, _one_shot)
    255                 self.key_separator, self.item_separator, self.sort_keys,
    256                 self.skipkeys, _one_shot)
--> 257         return _iterencode(o, 0)
    258 
    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,

ValueError: Unable to serialize VariableSpec(shape=(None, 4, 1, 1), dtype=tf.float32, trainable=True, alias_id=None) to JSON, because the TypeSpec class <class 'tensorflow.python.ops.resource_variable_ops.VariableSpec'> has not been registered.
```


"
61345,Cannot build tensorflow2.7-gpu version with bazel3.7.2,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.7

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

Linux Ubuntu 22.04

### Python version

3.8

### Bazel version

3.7.2

### GCC/compiler version

gcc 11

### CUDA/cuDNN version

12.1/8.9

### GPU model and memory

Quadro RTX 8000, 48GB

### Current behavior?

Context: I can use the exact same source codes to successfully build the CPU-only version. I hope to get a gpu version with the same code.

So I ran `bazel clean`, it looks fine.

Then I run `bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package`. I have tried for several times, but everytime got the following error: (I only paste the main part here, before this there were some regular bazel output, e.g., INFO:...)

```
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1401, column 38, in _cuda_autoconf_impl
                _create_local_cuda_repository(repository_ctx)
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1076, column 27, in _create_local_cuda_repository
                cuda_libs = _find_libs(repository_ctx, check_cuda_libs_script, cuda_config)
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 606, column 21, in _find_libs
                _check_cuda_libs(repository_ctx, check_cuda_libs_script, check_cuda_libs_params.values())
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 501, column 28, in _check_cuda_libs
                checked_paths = execute(repository_ctx, [python_bin, ""-c"", cmd]).stdout.splitlines()
        File ""/home/xiaxia/tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute
                fail(
Error in fail: Repository command failed
Expected even number of arguments
INFO: Found applicable config definition build:cuda in file /home/xiaxia/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//c
INFO: Found applicable config definition build:cuda in file /home/xiaxia/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//c
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: @local_config_cuda//:enable_cuda :: Error loading option @local_config_cuda//:enable_cuda: Repository command failed
Expected even number of arguments
```

Note that, I do not have root/admin role of the remote ubuntu machine, will it be a problem? I also tried to implement cuda and cudnn in my /home/ directory, but it still not help.

Please provide me with any advice, and thanks a lot!

### Standalone code to reproduce the issue

```shell
Sorry I don't know how to share my problem as reproducible, it happens in the process of building tensorflow.
```


### Relevant log output

```shell
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=157
INFO: Reading rc options for 'build' from /home/xiaxia/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/xiaxia/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from /home/xiaxia/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/xiaxia/anaconda3/envs/tf-cus-gpu/bin/python3 --action_env PYTHON_LIB_PATH=/home/xiaxia/anaconda3/envs/tf-cus-gpu/lib/python3.8/site-packages --python_path=/home/xiaxia/anaconda3/envs/tf-cus-gpu/bin/python3 --action_env TF_CUDA_VERSION=12.2 --action_env TF_CUDNN_VERSION=8 --action_env TF_NCCL_VERSION= --action_env TF_CUDA_PATHS=/home/xiaxia/cuda-12.2 --action_env CUDA_TOOLKIT_PATH=/home/xiaxia/cuda-12.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --action_env LD_LIBRARY_PATH=/usr/local/cuda-12.1/targets/x86_64-linux/lib:/usr/local/cuda-12.1/targets/x86_64-linux/lib:/usr/local/cuda-12.1/targets/x86_64-linux/lib::/home/xiaxia/cuda-12.2/lib64:/home/xiaxia/cuda-12.2/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-11 --config=cuda
INFO: Reading rc options for 'build' from /home/xiaxia/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/xiaxia/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/xiaxia/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /home/xiaxia/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda in file /home/xiaxia/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /home/xiaxia/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/xiaxia/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Repository local_config_cuda instantiated at:
  /home/xiaxia/tensorflow/WORKSPACE:15:14: in <toplevel>
  /home/xiaxia/tensorflow/tensorflow/workspace2.bzl:1080:19: in workspace
  /home/xiaxia/tensorflow/tensorflow/workspace2.bzl:94:19: in _tf_toolchains
Repository rule cuda_configure defined at:
  /home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl:1448:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1401, column 38, in _cuda_autoconf_impl
                _create_local_cuda_repository(repository_ctx)
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1076, column 27, in _create_local_cuda_repository
                cuda_libs = _find_libs(repository_ctx, check_cuda_libs_script, cuda_config)
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 606, column 21, in _find_libs
                _check_cuda_libs(repository_ctx, check_cuda_libs_script, check_cuda_libs_params.values())
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 501, column 28, in _check_cuda_libs
                checked_paths = execute(repository_ctx, [python_bin, ""-c"", cmd]).stdout.splitlines()
        File ""/home/xiaxia/tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute
                fail(
Error in fail: Repository command failed
Expected even number of arguments
INFO: Found applicable config definition build:cuda in file /home/xiaxia/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda in file /home/xiaxia/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: @local_config_cuda//:enable_cuda :: Error loading option @local_config_cuda//:enable_cuda: Repository command failed
Expected even number of arguments
```
"
61344,Unbounded Memory leak when using tf.py_function in tf.data.Dataset.map(),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04, Google Colab

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8 / 8.6

### GPU model and memory

various, e.g. 2080ti, 3080ti mobile, Colab T4

### Current behavior?

Using tf.py_function in a function that is applied to a tf.data.Dataset via its map() function causes a (C++-level) memory leak.

In my real training with more complex code inside the py_function, this lead to the python script eventually consuming upwards of 30 GB of RAM during a model.fit() loop, despite taking less that 3GB of RAM during the initial epoch.

tf.py_function also more generally causes memory leaks in all kinds of places. See the flags at the top of the linked Collab for details.



### Standalone code to reproduce the issue

```shell
see Collab: https://colab.research.google.com/drive/1auVJPyHApl4__4FF-rV3xNcJrqYZc38R?usp=sharing

Iterating through a dataset with a tf.py_function in it causes unbounded linear memory consumption growth.
```


### Relevant log output

```shell
**Batch 0**
Memory usage: 1732120576
Delta: 1651.88 MiB
**Batch 200**
Memory usage: 1736859648
Delta: 4.52 MiB
**Batch 400**
Memory usage: 1740644352
Delta: 3.61 MiB
**Batch 600**
Memory usage: 1744699392
Delta: 3.87 MiB
Average Delta since start: 3.87 MiB/iteration
Estimated growth per 1000 steps: 19.34 MiB
**Batch 800**
Memory usage: 1748750336
Delta: 3.86 MiB
Average Delta since start: 3.87 MiB/iteration
Estimated growth per 1000 steps: 19.33 MiB
**Batch 1000**
Memory usage: 1752805376
Delta: 3.87 MiB
Average Delta since start: 3.87 MiB/iteration
Estimated growth per 1000 steps: 19.33 MiB
**Batch 1200**
Memory usage: 1757401088
Delta: 4.38 MiB
Average Delta since start: 4.00 MiB/iteration
Estimated growth per 1000 steps: 19.98 MiB
**Batch 1400**
Memory usage: 1761456128
Delta: 3.87 MiB
Average Delta since start: 3.97 MiB/iteration
Estimated growth per 1000 steps: 19.85 MiB
**Batch 1600**
Memory usage: 1765240832
Delta: 3.61 MiB
Average Delta since start: 3.91 MiB/iteration
Estimated growth per 1000 steps: 19.55 MiB
**Batch 1800**
Memory usage: 1769025536
Delta: 3.61 MiB
Average Delta since start: 3.87 MiB/iteration
Estimated growth per 1000 steps: 19.33 MiB
**Batch 2000**
Memory usage: 1773621248
Delta: 4.38 MiB
Average Delta since start: 3.93 MiB/iteration
Estimated growth per 1000 steps: 19.66 MiB
**Batch 2200**
Memory usage: 1777676288
Delta: 3.87 MiB
Average Delta since start: 3.92 MiB/iteration
Estimated growth per 1000 steps: 19.62 MiB
**Batch 2400**
Memory usage: 1781731328
Delta: 3.87 MiB
Average Delta since start: 3.92 MiB/iteration
Estimated growth per 1000 steps: 19.59 MiB
**Batch 2600**
Memory usage: 1785786368
Delta: 3.87 MiB
Average Delta since start: 3.91 MiB/iteration
Estimated growth per 1000 steps: 19.57 MiB
```
"
61343, RuntimeError: Quantization to 16x8-bit not yet supported for op: 'FLOOR_MOD'   ,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
google colab
- TensorFlow installation (pip package or built from source):
- pip
- TensorFlow library (version, if pip package or github SHA, if built from source):
- 2.12.0

### 2. Code
I have following model.
```python
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

train_labels = train_labels[:1000]
test_labels = test_labels[:1000]

train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0
test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0


# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images.astype(np.float32) / 255.0
test_images = test_images.astype(np.float32) / 255.0


# Define a simple sequential model

model_infrence = tf.keras.Sequential([
    MyDense(512, activation='relu', input_shape=(784,)),
    keras.layers.Dense(512, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10)
  ])

model_infrence.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])



# Create a basic model instance


# Display the model's architecture
model_infrence.summary()

# Create a basic model instance


# Display the model's architecture
model_infrence.summary()
```

Then I custom one of the dense layers like so 
```python
# Copyright 2015 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""Contains the Dense layer.""""""


import tensorflow.compat.v2 as tf

from keras import activations
from keras import backend
from keras import constraints
from keras import initializers
from keras import regularizers
from keras.dtensor import utils
from keras.engine.base_layer import Layer
from keras.engine.input_spec import InputSpec

# isort: off
from tensorflow.python.util.tf_export import keras_export


@keras_export(""keras.layers.Dense"")
class MyDense(Layer):
    """"""Just your regular densely-connected NN layer.

    `Dense` implements the operation:
    `output = activation(dot(input, kernel) + bias)`
    where `activation` is the element-wise activation function
    passed as the `activation` argument, `kernel` is a weights matrix
    created by the layer, and `bias` is a bias vector created by the layer
    (only applicable if `use_bias` is `True`). These are all attributes of
    `Dense`.

    Note: If the input to the layer has a rank greater than 2, then `Dense`
    computes the dot product between the `inputs` and the `kernel` along the
    last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).
    For example, if input has dimensions `(batch_size, d0, d1)`, then we create
    a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2
    of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are
    `batch_size * d0` such sub-tensors).  The output in this case will have
    shape `(batch_size, d0, units)`.

    Besides, layer attributes cannot be modified after the layer has been called
    once (except the `trainable` attribute).
    When a popular kwarg `input_shape` is passed, then keras will create
    an input layer to insert before the current layer. This can be treated
    equivalent to explicitly defining an `InputLayer`.

    Example:

    >>> # Create a `Sequential` model and add a Dense layer as the first layer.
    >>> model = tf.keras.models.Sequential()
    >>> model.add(tf.keras.Input(shape=(16,)))
    >>> model.add(tf.keras.layers.Dense(32, activation='relu'))
    >>> # Now the model will take as input arrays of shape (None, 16)
    >>> # and output arrays of shape (None, 32).
    >>> # Note that after the first layer, you don't need to specify
    >>> # the size of the input anymore:
    >>> model.add(tf.keras.layers.Dense(32))
    >>> model.output_shape
    (None, 32)

    Args:
        units: Positive integer, dimensionality of the output space.
        activation: Activation function to use.
            If you don't specify anything, no activation is applied
            (ie. ""linear"" activation: `a(x) = x`).
        use_bias: Boolean, whether the layer uses a bias vector.
        kernel_initializer: Initializer for the `kernel` weights matrix.
        bias_initializer: Initializer for the bias vector.
        kernel_regularizer: Regularizer function applied to
            the `kernel` weights matrix.
        bias_regularizer: Regularizer function applied to the bias vector.
        activity_regularizer: Regularizer function applied to
            the output of the layer (its ""activation"").
        kernel_constraint: Constraint function applied to
            the `kernel` weights matrix.
        bias_constraint: Constraint function applied to the bias vector.

    Input shape:
        N-D tensor with shape: `(batch_size, ..., input_dim)`.
        The most common situation would be
        a 2D input with shape `(batch_size, input_dim)`.

    Output shape:
        N-D tensor with shape: `(batch_size, ..., units)`.
        For instance, for a 2D input with shape `(batch_size, input_dim)`,
        the output would have shape `(batch_size, units)`.
    """"""

    @utils.allow_initializer_layout
    def __init__(
        self,
        units,
        activation=None,
        use_bias=True,
        kernel_initializer=""glorot_uniform"",
        bias_initializer=""zeros"",
        kernel_regularizer=None,
        bias_regularizer=None,
        activity_regularizer=None,
        kernel_constraint=None,
        bias_constraint=None,
        **kwargs,
    ):
        super().__init__(activity_regularizer=activity_regularizer, **kwargs)

        self.units = int(units) if not isinstance(units, int) else units
        if self.units < 0:
            raise ValueError(
                ""Received an invalid value for `units`, expected ""
                f""a positive integer. Received: units={units}""
            )
        self.activation = activations.get(activation)
        self.use_bias = use_bias
        self.kernel_initializer = initializers.get(kernel_initializer)
        self.bias_initializer = initializers.get(bias_initializer)
        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)
        self.kernel_constraint = constraints.get(kernel_constraint)
        self.bias_constraint = constraints.get(bias_constraint)

        self.input_spec = InputSpec(min_ndim=2)
        self.supports_masking = True

    def build(self, input_shape):
        dtype = tf.as_dtype(self.dtype or backend.floatx())
        if not (dtype.is_floating or dtype.is_complex):
            raise TypeError(
                ""A Dense layer can only be built with a floating-point ""
                f""dtype. Received: dtype={dtype}""
            )

        input_shape = tf.TensorShape(input_shape)
        last_dim = tf.compat.dimension_value(input_shape[-1])
        if last_dim is None:
            raise ValueError(
                ""The last dimension of the inputs to a Dense layer ""
                ""should be defined. Found None. ""
                f""Full input shape received: {input_shape}""
            )
        self.input_spec = InputSpec(min_ndim=2, axes={-1: last_dim})
        self.kernel = self.add_weight(
            ""kernel"",
            shape=[last_dim, self.units],
            initializer=self.kernel_initializer,
            regularizer=self.kernel_regularizer,
            constraint=self.kernel_constraint,
            dtype=self.dtype,
            trainable=True,
        )
        if self.use_bias:
            self.bias = self.add_weight(
                ""bias"",
                shape=[
                    self.units,
                ],
                initializer=self.bias_initializer,
                regularizer=self.bias_regularizer,
                constraint=self.bias_constraint,
                dtype=self.dtype,
                trainable=True,
            )
        else:
            self.bias = None
        self.built = True

    def call(self, inputs):
        if inputs.dtype.base_dtype != self._compute_dtype_object.base_dtype:
            inputs = tf.cast(inputs, dtype=self._compute_dtype_object)

        is_ragged = isinstance(inputs, tf.RaggedTensor)
        if is_ragged:
            # In case we encounter a RaggedTensor with a fixed last dimension
            # (last dimension not ragged), we can flatten the input and restore
            # the ragged dimensions at the end.
            if tf.compat.dimension_value(inputs.shape[-1]) is None:
                raise ValueError(
                    ""Dense layer only supports RaggedTensors when the ""
                    ""innermost dimension is non-ragged. Received: ""
                    f""inputs.shape={inputs.shape}.""
                )
            original_inputs = inputs
            if inputs.flat_values.shape.rank > 1:
                inputs = inputs.flat_values
            else:
                # Innermost partition is encoded using uniform_row_length.
                # (This is unusual, but we can handle it.)
                if inputs.shape.rank == 2:
                    inputs = inputs.to_tensor()
                    is_ragged = False
                else:
                    for _ in range(original_inputs.ragged_rank - 1):
                        inputs = inputs.values
                    inputs = inputs.to_tensor()
                    original_inputs = tf.RaggedTensor.from_nested_row_splits(
                        inputs, original_inputs.nested_row_splits[:-1]
                    )

        rank = inputs.shape.rank
        if rank == 2 or rank is None:
            # We use embedding_lookup_sparse as a more efficient matmul
            # operation for large sparse input tensors. The op will result in a
            # sparse gradient, as opposed to
            # sparse_ops.sparse_tensor_dense_matmul which results in dense
            # gradients. This can lead to sigfinicant speedups, see b/171762937.
            if isinstance(inputs, tf.SparseTensor):
                # We need to fill empty rows, as the op assumes at least one id
                # per row.
                inputs, _ = tf.sparse.fill_empty_rows(inputs, 0)
                # We need to do some munging of our input to use the embedding
                # lookup as a matrix multiply. We split our input matrix into
                # separate ids and weights tensors. The values of the ids tensor
                # should be the column indices of our input matrix and the
                # values of the weights tensor can continue to the actual matrix
                # weights.  The column arrangement of ids and weights will be
                # summed over and does not matter. See the documentation for
                # sparse_ops.sparse_tensor_dense_matmul a more detailed
                # explanation of the inputs to both ops.
                ids = tf.SparseTensor(
                    indices=inputs.indices,
                    values=inputs.indices[:, 1],
                    dense_shape=inputs.dense_shape,
                )
                weights = inputs
                outputs = tf.nn.embedding_lookup_sparse(
                    self.kernel, ids, weights, combiner=""sum""
                )
            else:
                

                  print(inputs)
                  quotient, x = divmod(inputs, (2**n))
                  #x = inputs % (2**n);
                  quotient1, x1 = divmod(inputs, (2**n - 1))
                  #x1 = inputs % (2**n - 1);
                  quotient2, x2 = divmod(inputs, (2**n + 1))
                  #x2 = inputs % (2**n + 1);
                  # w =  self.w % (2**n);
                  quotient3, w = divmod(self.w, (2**n))
                  # w1 = self.w % (2**n - 1);
                  quotient4, w1 = divmod(self.w, (2**n - 1))
                  # w2 = self.w % (2**n + 1)
                  quotient5, w2 = divmod(self.w, (2**n + 1))
                  quotient6, z = divmod((tf.matmul(x, w) + self.b), (2**n))
                  # z = (tf.matmul(x, w) + self.b) % (2**n)
                  quotient7, z1 = divmod((tf.matmul(x, w) + self.b), (2**n - 1))
                  # z1 = (tf.matmul(x1, w1) + self.b) % (2**n - 1)
                  quotient8, z2 = divmod((tf.matmul(x, w) + self.b), (2**n + 1))
                  # z2 = tf.matmul(x2, w2) + self.b % (2**n + 1)

                  Dm = (2**n) * (2**n - 1) * (2**n + 1);
                  m1 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n));
                  m2 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n - 1));
                  m3 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n + 1));

                  outputs = rns_to_decimal(Dm, z, z1, z2, m1, m2, m3, n)
                  print(outputs)
        # Broadcast kernel to inputs.
        else:
            outputs = tf.tensordot(inputs, self.kernel, [[rank - 1], [0]])
            # Reshape the output back to the original ndim of the input.
            if not tf.executing_eagerly():
                shape = inputs.shape.as_list()
                output_shape = shape[:-1] + [self.kernel.shape[-1]]
                outputs.set_shape(output_shape)

        # if self.use_bias:
        #     outputs = tf.nn.bias_add(outputs, self.bias)

        if self.activation is not None:
            outputs = self.activation(outputs)

        if is_ragged:
            outputs = original_inputs.with_flat_values(outputs)

        return outputs

    def compute_output_shape(self, input_shape):
        input_shape = tf.TensorShape(input_shape)
        input_shape = input_shape.with_rank_at_least(2)
        if tf.compat.dimension_value(input_shape[-1]) is None:
            raise ValueError(
                ""The last dimension of the input shape of a Dense layer ""
                ""should be defined. Found None. ""
                f""Received: input_shape={input_shape}""
            )
        return input_shape[:-1].concatenate(self.units)

    def get_config(self):
        config = super().get_config()
        config.update(
            {
                ""units"": self.units,
                ""activation"": activations.serialize(self.activation),
                ""use_bias"": self.use_bias,
                ""kernel_initializer"": initializers.serialize(
                    self.kernel_initializer
                ),
                ""bias_initializer"": initializers.serialize(
                    self.bias_initializer
                ),
                ""kernel_regularizer"": regularizers.serialize(
                    self.kernel_regularizer
                ),
                ""bias_regularizer"": regularizers.serialize(
                    self.bias_regularizer
                ),
                ""activity_regularizer"": regularizers.serialize(
                    self.activity_regularizer
                ),
                ""kernel_constraint"": constraints.serialize(
                    self.kernel_constraint
                ),
                ""bias_constraint"": constraints.serialize(self.bias_constraint),
            }
        )
        return config
    def rns_to_decimal(dm, z1, z2, z3, m1, m2, m3, n = 6):
        M1 = 2**n
        M2 = 2**n - 1
        M3 = 2**n + 1
        x1 = 1
        x2 = 1
        x3 = 1
        quotient, mm1 = divmod(m1, M1)
       # mm1 = m1 % M1
        for i in range(1, M1):
            quotient1, X = divmod((i * mm1), M1)
            if X == 1:
                x1 = i
        quotient2, mm2 = divmod(m2, M2)
       # mm2 = m2 % M2
        for i in range(1, M2):
            quotient3, X1 = divmod((i * mm2), M2)
            if X1 == 1:
                x2 = i
       # mm3 = m3 % M3
        quotient4, mm3 = divmod(m3, M3)
        for i in range(1, M3):
            quotient5, X2 = divmod((i * mm3), M3)
            if X2 == 1:
                x3 = i
        quotient5, num = divmod((z1 * m1 * x1 + z2 * m2 * x2 + z3 * m3 * x3), dm)
       # num = (z1 * m1 * x1 + z2 * m2 * x2 + z3 * m3 * x3) % dm
        return num
```

The only change to original layer is following code:

```python
                  quotient, x = divmod(inputs, (2**n))
                  #x = inputs % (2**n);
                  quotient1, x1 = divmod(inputs, (2**n - 1))
                  #x1 = inputs % (2**n - 1);
                  quotient2, x2 = divmod(inputs, (2**n + 1))
                  #x2 = inputs % (2**n + 1);
                  # w =  self.w % (2**n);
                  quotient3, w = divmod(self.w, (2**n))
                  # w1 = self.w % (2**n - 1);
                  quotient4, w1 = divmod(self.w, (2**n - 1))
                  # w2 = self.w % (2**n + 1)
                  quotient5, w2 = divmod(self.w, (2**n + 1))
                  quotient6, z = divmod((tf.matmul(x, w) + self.b), (2**n))
                  # z = (tf.matmul(x, w) + self.b) % (2**n)
                  quotient7, z1 = divmod((tf.matmul(x, w) + self.b), (2**n - 1))
                  # z1 = (tf.matmul(x1, w1) + self.b) % (2**n - 1)
                  quotient8, z2 = divmod((tf.matmul(x, w) + self.b), (2**n + 1))
                  # z2 = tf.matmul(x2, w2) + self.b % (2**n + 1)

                  Dm = (2**n) * (2**n - 1) * (2**n + 1);
                  m1 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n));
                  m2 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n - 1));
                  m3 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n + 1));

                  outputs = rns_to_decimal(Dm, z, z1, z2, m1, m2, m3, n)
```

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
61341,Failed to build tensorflow on Apple silicon.,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13

### Custom code

No

### OS platform and distribution

macOS 13.4.1

### Mobile device

None

### Python version

3.11

### Bazel version

5.3.0-homebrew

### GCC/compiler version

Apple clang version 14.0.3 (clang-1403.0.22.14.1)

### CUDA/cuDNN version

None

### GPU model and memory

None

### Current behavior?

After failing to build tensorflow using the default options, I attempted the solution suggested in this issue (https://github.com/tensorflow/tensorflow/issues/60179). However, I found that installing `coreutils` directly still resulted in the same problem.

### Standalone code to reproduce the issue

Default settings used for all options.
```shell
bazel build //tensorflow/tools/pip_package:build_pip_package
```
 

### Relevant log output

```shell
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=80
INFO: Reading rc options for 'build' from /Users/sunruiqi/Desktop/tensorflow-2.13.0/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/sunruiqi/Desktop/tensorflow-2.13.0/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /Users/sunruiqi/Desktop/tensorflow-2.13.0/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Users/sunruiqi/miniconda3/envs/tensorflow-macos/bin/python3 --action_env PYTHON_LIB_PATH=/Users/sunruiqi/miniconda3/envs/tensorflow-macos/lib/python3.11/site-packages --python_path=/Users/sunruiqi/miniconda3/envs/tensorflow-macos/bin/python3
INFO: Reading rc options for 'build' from /Users/sunruiqi/Desktop/tensorflow-2.13.0/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug
INFO: Found applicable config definition build:short_logs in file /Users/sunruiqi/Desktop/tensorflow-2.13.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/sunruiqi/Desktop/tensorflow-2.13.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:macos in file /Users/sunruiqi/Desktop/tensorflow-2.13.0/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (611 packages loaded, 37637 targets configured).
INFO: Found 1 target...
ERROR: /Users/sunruiqi/Desktop/tensorflow-2.13.0/tensorflow/BUILD:1134:21: declared output 'tensorflow/libtensorflow_framework.2.dylib' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)
ERROR: /Users/sunruiqi/Desktop/tensorflow-2.13.0/tensorflow/BUILD:1134:21: Executing genrule //tensorflow:libtensorflow_framework.2.dylib_sym failed: not all outputs were created or valid
realpath: illegal option -- -
usage: realpath [-q] [path ...]
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 438.472s, Critical Path: 37.63s
INFO: 5437 processes: 1278 internal, 4159 local.
FAILED: Build did NOT complete successfully
```
"
61340,`tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function.,"The first is the map function:
    def map_function(example):

        feature_map = {""wav_raw"": tf.io.FixedLenFeature([], tf.string)}
        parsed_example = tf.io.parse_single_example(example, features=feature_map)

        wav_slice = tf.io.decode_raw(parsed_example[""wav_raw""], out_type=tf.float64)
        wav_slice = tf.cast(wav_slice, tf.float32) / 2 ** 15

        return wav_slice

The second one is the training process:

     for epoch in range(args.num_epochs):
              
        trainset = tf.data.TFRecordDataset(args.trainset_tfrecords_path)
        trainset = trainset.map(map_func=map_function,
                                num_parallel_calls=num_cpus)  # num_parallel_calls should be number of cpu cores

        #trainset = trainset.shuffle(buffer_size=args.batch_size * 200, reshuffle_each_iteration=True)
        trainset = trainset.batch(batch_size=args.batch_size)
        trainset = trainset.prefetch(buffer_size=args.batch_size)


        # train_loss for each epoch
        train_loss_epoch = []
        train_loss = 0.0

        # record the train time for each epoch
        start = time.time()
        # MASK参数

        #EMA_MODEL来选择mask的index

        binary_mask = RandomMaskingGenerator(input_size,frame_length,mask_ratio)
        # bmr:0为掩码，1为未掩码；
        # bm_T:1为掩码,0为未掩码；
        # bm,bm_T = binary_mask()
        

        for step, _input in enumerate(trainset):
            bm, bm_T, _ = binary_mask.random_mask(_input.shape[0],alpha_e_max)#.totally_random_mask(_input.shape[0])
            print(""_input"",_input)
            # print(""bm_shape"", bm.shape)
            # print(""bm_T_shape"", bm_T.shape)
            loss_value = train_step(_input,_input*bm,bm_T)
            loss_float = float(loss_value)

            train_loss_epoch.append(loss_float)

            # Calculate the accumulated train loss value
            train_loss += loss_float



        # average train loss for each epoch
        train_loss /= (step + 1)
        train_loss_all.append(train_loss)

        # print log
        log = ""train epoch {}/{}, train_loss = {:.06f}, time = {:.06f}""
The third one is the train_step function:

    @tf.function
    def train_step(_input,_input_mask,bm_T):
        with tf.GradientTape() as tape:
            enc_output, batch_mean, batch_var = sem_enc(_input_mask)
            #输入进去semantic decoder
            print(""main:"",enc_output)
            _output = sem_dec([enc_output, batch_mean, batch_var])
            loss_value = mse_loss(tf.multiply(_input, bm_T), _output)
            tf.print(loss_value)
            loss_whole = loss_value

        grads = tape.gradient(loss_whole, weights_all)  # compute gradients
        optimizer.apply_gradients(zip(grads, weights_all))  # update parameters

        return loss_whole



The _ouput generated by sem_dec() is the value I wanted change it from kerasTensor to TF.tensor; I could send you the whole codes, if you need, this is my email: limingxiao@link.cuhk.edu.cn. Thank you for your reply!

_Originally posted by @lmx666-gif in https://github.com/tensorflow/tensorflow/issues/61307#issuecomment-1643164557_
            "
61338,How to get raw buffer pointer from python tf.Tensor,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In C++ api, tensorflow::Tensor has `data()` method which returns pointer to memory array. While in python api tf.Tensor does not allow to get raw data pointer. Is there any solution or workaround for this?

### Standalone code to reproduce the issue

```shell
tensorflow::Tensor.data()
tf.Tensor
```


### Relevant log output

_No response_"
61337,AttributeError: cython_sources when installing tflite-model-maker,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

Yes

### OS platform and distribution

ubuntu 20

### Mobile device

_No response_

### Python version

 3.10.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

when trying to install `tflite-model-maker` i get an error:

```
running egg_info
      writing lib3/PyYAML.egg-info/PKG-INFO
      writing dependency_links to lib3/PyYAML.egg-info/dependency_links.txt
      writing top-level names to lib3/PyYAML.egg-info/top_level.txt
      Traceback (most recent call last):
        File ""/home/cubxi/.local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in <module>
          main()
        File ""/home/cubxi/.local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
        File ""/home/cubxi/.local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 118, in get_requires_for_build_wheel
          return hook(config_settings)
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/build_meta.py"", line 341, in get_requires_for_build_wheel
          return self._get_build_requires(config_settings, requirements=['wheel'])
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/build_meta.py"", line 323, in _get_build_requires
          self.run_setup()
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/build_meta.py"", line 338, in run_setup
          exec(code, locals())
        File ""<string>"", line 271, in <module>
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/__init__.py"", line 107, in setup
          return distutils.core.setup(**attrs)
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py"", line 185, in setup
          return run_commands(dist)
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py"", line 201, in run_commands
          dist.run_commands()
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py"", line 969, in run_commands
          self.run_command(cmd)
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/dist.py"", line 1234, in run_command
          super().run_command(command)
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py"", line 988, in run_command
          cmd_obj.run()
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/command/egg_info.py"", line 314, in run
          self.find_sources()
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/command/egg_info.py"", line 322, in find_sources
          mm.run()
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/command/egg_info.py"", line 551, in run
          self.add_defaults()
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/command/egg_info.py"", line 589, in add_defaults
          sdist.add_defaults(self)
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/command/sdist.py"", line 104, in add_defaults
          super().add_defaults()
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/_distutils/command/sdist.py"", line 251, in add_defaults
          self._add_defaults_ext()
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/_distutils/command/sdist.py"", line 336, in _add_defaults_ext
          self.filelist.extend(build_ext.get_source_files())
        File ""<string>"", line 201, in get_source_files
        File ""/tmp/pip-build-env-koysxeoc/overlay/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py"", line 107, in __getattr__
          raise AttributeError(attr)
      AttributeError: cython_sources
```

### Standalone code to reproduce the issue

```shell
run `pip install -q tflite-model-maker`
```


### Relevant log output

_No response_"
61336,Using F1 Score for Model Checkpoint throws an error,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

T4 15GB

### Current behavior?

Was using `tf.keras.metrics.F1Score` in `model.compile`. It works well, it will return f1_score and val_f1_score after every epoch. However, the problem arises if I also include `tf.keras.callbacks.ModelCheckpoint` in the `model.fit`, I got an error after the first epoch training.

The error are `The following argument(s) are not supported with the native Keras format: ['options']` if I use `monitor='val_loss'` and `ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()` if I use `monitor='val_f1_score'` .

Additional context: I am training a multi-label classification model. This might be the cause.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras import models, layers
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D

def create_model():
	base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
	model = models.Sequential()
	model.add(base_model)
	model.add(layers.GlobalAveragePooling2D())
	model.add(layers.Dense(10, activation='sigmoid'))
	return model

model = create_model()
model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
    filepath=os.path.join('v5.keras'),
    monitor='val_f1_score',
    mode='max',
    save_best_only=True
)

model.compile(
	loss='binary_crossentropy',
optimizer=tf.keras.optimizers.Adam(learning_rate=0.00003),
	metrics = [ tf.keras.metrics.F1Score(threshold=0.5)]
)

history = model.fit(
    X_train,
    y_train,
    batch_size=64,
    epochs=1,
    validation_data=(X_test, y_test),
    callbacks=[model_checkpoint]
)
```


### Relevant log output

_No response_"
61330,Tesla v100 Tensorflow CUDA Support,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.4

### Custom code

Yes

### OS platform and distribution

RHEL 7.9

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/compiler version

4.3

### CUDA/cuDNN version

11/8, 10.1/7.6

### GPU model and memory

Tesla V100 2GB Vram

### Current behavior?

Attempting to fetch value instead of handling error Internal: failed to get device attribute 13 for device 0: CUDA_ERROR_UNKNOWN: unknown error.

NVIDIA-SMI give the following output:| NVIDIA-SMI 450.51.05 Driver Version: 450.51.05 CUDA Version: 11.0

nvcc-V the following output:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Fri_Feb__8_19:08:17_PST_2019
Cuda compilation tools, release 10.1, V10.1.105


### Standalone code to reproduce the issue

```shell
Doesnt happen with Windows or Ubuntu systems.
```


### Relevant log output

_No response_"
61329,LSTM support for quantisation aware training.,I wonder if quantization-aware training has the support for lstm? 
61328,Random predictions with intel-tensorflow when OMP_THREAD_LIMIT is set,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

The problem arises when using *intel-tensorflow*

### Source

binary

### TensorFlow version

unknown, 2.13.0 (package intel_tensorflow-2.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl)

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Python version

3.8, 3.9, 3.10, 3.11

### Current behavior?

This problem only happens when using the *intel-tensorflow* package which uses the mkl library. When the environment variable OMP_THREAD_LIMIT is set, the predictions of some standard models become random when running on cpu.

During the run, a warning from OMP is shown:
```
OMP: Warning #96: Cannot form a team with 36 threads, using 12 instead.
OMP: Hint Consider unsetting KMP_DEVICE_THREAD_LIMIT (KMP_ALL_THREADS), KMP_TEAMS_THREAD_LIMIT, and OMP_THREAD_LIMIT (if any are set).
```
This warning suggests to unset the variable _OMP_THREAD_LIMIT_ but I think a randomness/instability in the prediction of models needs more than a warning with a ""Hint"".

To reproduce the problem:
* Create an environment with *intel-tensorflow* installed
* Copy the code in the following section into a file *test_script.py*
* Run the following commands:
  * `python test_script.py`
  * `OMP_THREAD_LIMIT=2 python test_script.py`  (edited)

### Standalone code to reproduce the issue

```python
import tensorflow as tf


if __name__ == ""__main__"":
    with tf.device(""/cpu:0""):
        model = tf.keras.applications.efficientnet.EfficientNetB0()
        img = tf.ones((1, 224, 224, 3))
        pred = model(img)

        print(f""Result: {pred[0, :5]}"")
```


### Relevant log output

```shell
# Run without OMP_THREAD_LIMIT
...
Result: [0.00066389 0.00075261 0.00108045 0.00210853 0.00316559]

# Run with OMP_THREAD_LIMIT
...
OMP: Warning #96: Cannot form a team with 6 threads, using 2 instead.
OMP: Hint Consider unsetting KMP_DEVICE_THREAD_LIMIT (KMP_ALL_THREADS), KMP_TEAMS_THREAD_LIMIT, and OMP_THREAD_LIMIT (if any are set).
Result: [0.00030835 0.00062532 0.00057299 0.00088017 0.00140722]
```
"
61327,tensorflow/core/common_runtime/gpu/gpu_util.cc:293] GPU->CPU Memcpy failed,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

1.15

### Custom code

Yes

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda version = 10.0
cudnn=7.6.4
_No response_

### GPU model and memory
Geforce RTX 4070 TI 12 GB
_No response_

### Current behavior?

I am using 

gpu geforce rtx 4070 ti 12 gb 

i add in my training file 

    config1 = tf.compat.v1.ConfigProto()
    config1.gpu_options.allow_growth = True
    session = tf.compat.v1.Session(config=config1)

But Nothing happen I get this issue 

### Standalone code to reproduce the issue

```shell
2023-07-19 13:40:37.744821: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2023-07-19 13:40:37.744955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2023-07-19 13:40:37.745047: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2023-07-19 13:40:37.745166: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2023-07-19 13:40:37.745290: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2023-07-19 13:40:37.745374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2023-07-19 13:40:37.745493: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2023-07-19 13:40:37.745596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2023-07-19 13:40:37.745706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-07-19 13:40:37.745789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2023-07-19 13:40:37.745866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2023-07-19 13:40:37.746009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10400 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9)
WARNING:tensorflow:From C:\Users\user\AppData\Roaming\Python\Python37\site-packages\keras\backend\tensorflow_backend.py:300: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From C:\Users\user\AppData\Roaming\Python\Python37\site-packages\keras\backend\tensorflow_backend.py:308: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

2023-07-19 13:40:40.254879: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2023-07-19 13:43:12.728939: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows
Relying on driver to perform ptx compilation. This message will be only logged once.
2023-07-19 13:43:12.932855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2023-07-19 13:43:21.066511: E tensorflow/stream_executor/cuda/cuda_blas.cc:428] failed to run cuBLAS routine: CUBLAS_STATUS_EXECUTION_FAILED
Exception: Blas GEMM launch failed : a.shape=(2, 2048), b.shape=(2, 36), m=2048, n=36, k=2
         [[node gradients_1/dense_regress_10/MatMul_grad/MatMul_1 (defined at C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\framework\ops.py:1748) ]]

Original stack trace for 'gradients_1/dense_regress_10/MatMul_grad/MatMul_1':
  File ""c:/Users/user/Desktop/Binarios/keras_frcnn-master-atelier-B/keras_frcnn-master/train_frcnn_kitti.py"", line 262, in <module>
    train_kitti()
  File ""c:/Users/user/Desktop/Binarios/keras_frcnn-master-atelier-B/keras_frcnn-master/train_frcnn_kitti.py"", line 205, in train_kitti
    [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\keras\engine\training.py"", line 1620, in train_on_batch
    self._make_train_function()
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\keras\engine\training.py"", line 1002, in _make_train_function
    self.total_loss)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\keras\optimizers.py"", line 381, in get_updates
    grads = self.get_gradients(loss, params)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\keras\optimizers.py"", line 47, in get_gradients
    grads = K.gradients(loss, params)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\keras\backend\tensorflow_backend.py"", line 2138, in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\gradients_impl.py"", line 158, in gradients
    unconnected_gradients)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\gradients_util.py"", line 679, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\gradients_util.py"", line 350, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\gradients_util.py"", line 679, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\math_grad.py"", line 1586, in _MatMulGrad
    grad_b = gen_math_ops.mat_mul(a, grad, transpose_a=True)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\gen_math_ops.py"", line 6136, in mat_mul
    name=name)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\framework\op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\framework\ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\framework\ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\framework\ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()

...which was originally created as op 'dense_regress_10/MatMul', defined at:
  File ""c:/Users/user/Desktop/Binarios/keras_frcnn-master-atelier-B/keras_frcnn-master/train_frcnn_kitti.py"", line 262, in <module>
    train_kitti()
  File ""c:/Users/user/Desktop/Binarios/keras_frcnn-master-atelier-B/keras_frcnn-master/train_frcnn_kitti.py"", line 88, in train_kitti
    classifier = nn.classifier(shared_layers, roi_input, cfg.num_rois, nb_classes=len(classes_count), trainable=True)
  File ""c:\Users\user\Desktop\Binarios\keras_frcnn-master-atelier-B\keras_frcnn-master\keras_frcnn\resnet.py"", line 270, in classifier
    name='dense_regress_{}'.format(nb_classes))(out)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\keras\engine\topology.py"", line 578, in __call__
    output = self.call(inputs, **kwargs)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\keras\layers\wrappers.py"", line 177, in call
    y = self.layer.call(inputs)  # (num_samples * timesteps, ...)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\keras\layers\core.py"", line 840, in call
    output = K.dot(inputs, self.kernel)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\keras\backend\tensorflow_backend.py"", line 848, in dot
    out = tf.matmul(x, y)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\util\dispatch.py"", line 180, in wrapper
    return target(*args, **kwargs)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\math_ops.py"", line 2754, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\gen_math_ops.py"", line 6136, in mat_mul
    name=name)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\framework\op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\framework\ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\framework\ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""C:\Users\user\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\framework\ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()

2023-07-19 13:43:21.297408: I tensorflow/stream_executor/stream.cc:1990] [stream=0000029EFE927EC0,impl=0000029EB937CFB0] did not wait for [stream=0000029EFE926CC0,impl=0000029EB937CF20]
2023-07-19 13:43:21.297815: I tensorflow/stream_executor/stream.cc:4925] [stream=0000029EFE927EC0,impl=0000029EB937CFB0] did not memcpy device-to-host; source: 00000007129B6500
2023-07-19 13:43:21.298246: F tensorflow/core/common_runtime/gpu/gpu_util.cc:293] GPU->CPU Memcpy failed
2023-07-19 13:43:21.298255: I tensorflow/stream_executor/stream.cc:1990] [stream=0000029EFE927EC0,impl=0000029EB937CFB0] did not wait for [stream=0000029EFE926CC0,impl=0000029EB937CF20]
```


### Relevant log output

_No response_"
61326,Could not load dynamic library 'libcublasLt.so.12'; dlerror: libcublasLt.so.12: cannot open shared object file: No such file or directory,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 23.10

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8.0-1/8.9.3.28-1+cuda12.1

### GPU model and memory

NVIDIA GeForce GTX 960M, 4096MiB

### Current behavior?

running the mobilenet from keras included by tensorflow leads to the following error:
```
Could not load library libcublasLt.so.12. Error: libcublasLt.so.12: cannot open shared object file: No such file or directory                                                         
Abgebrochen (Speicherabzug geschrieben)
```

### Standalone code to reproduce the issue

```shell
python -c 'from tensorflow.keras.applications.mobilenet import MobileNet; import numpy as np; m = MobileNet(); m.predict(np.zeros((32,224,224,3)))'
```


### Relevant log output

```shell
Could not load library libcublasLt.so.12. Error: libcublasLt.so.12: cannot open shared object file: No such file or directory                                                         
Abgebrochen (Speicherabzug geschrieben)
```
"
61325,"How can I profile ""Inference"" by Profiler, and view performance profile by tensorboard","### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.9.3

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I want to profiling ""Inference"" by Profiler. 
However, the test of profiling training is success. But, when I try to profiling inference, the profiling log generated is empty and there are no active dashboards for the current data set.
How can I find the tutorial about analyzing the performance of inference?


### Standalone code to reproduce the issue

```shell
saved_model_loaded = tf.saved_model.load(FLAGS.weights, tags=[tag_constants.SERVING])
infer = saved_model_loaded.signatures['serving_default']
batch_data = tf.constant(images_data)
options = tf.profiler.experimental.ProfilerOptions(host_tracer_level = 3, python_tracer_level = 1, device_tracer_level = 1)
tf.profiler.experimental.start('logdir', options)
pred_bbox = infer(batch_data)
tf.profiler.experimental.stop()
```


### Relevant log output

_No response_"
61324,tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_6/dropout_12/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

Kaggle

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to train model below

```
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import *

# Set the input shape of the images (adjust based on the input image size)
input_shape = (128, 128, 3)  # Adjust based on the input image size

# Set the number of segmentation classes 
n_classes = 1  # Number of segmentation classes

# Define the model architecture
inputs = Input(shape=input_shape)  # Define the input layer with the specified input shape

# Encoder
conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)  # First convolutional layer with 64 filters
conv1 = BatchNormalization()(conv1)  # Apply batch normalization to normalize the activations
conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)  # Second convolutional layer with 64 filters
conv1 = BatchNormalization()(conv1)  # Apply batch normalization to normalize the activations
pool1 = MaxPooling2D((2, 2))(conv1)  # Max pooling layer with a pool size of (2, 2)

conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)  # Convolutional layer with 128 filters
conv2 = BatchNormalization()(conv2)  # Apply batch normalization to normalize the activations
conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)  # Convolutional layer with 128 filters
conv2 = BatchNormalization()(conv2)  # Apply batch normalization to normalize the activations
pool2 = MaxPooling2D((2, 2))(conv2)  # Max pooling layer with a pool size of (2, 2)

conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)  # Convolutional layer with 256 filters
conv3 = BatchNormalization()(conv3)  # Apply batch normalization to normalize the activations
conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)  # Convolutional layer with 256 filters
conv3 = BatchNormalization()(conv3)  # Apply batch normalization to normalize the activations
pool3 = MaxPooling2D((2, 2))(conv3)  # Max pooling layer with a pool size of (2, 2)

conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)  # Convolutional layer with 512 filters
conv4 = BatchNormalization()(conv4)  # Apply batch normalization to normalize the activations
conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)  # Convolutional layer with 512 filters
conv4 = BatchNormalization()(conv4)  # Apply batch normalization to normalize the activations
drop4 = Dropout(0.5)(conv4)  # Apply dropout regularization with a rate of 0.5
pool4 = MaxPooling2D((2, 2))(drop4)  # Max pooling layer with a pool size of (2, 2)

conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(pool4)  # Convolutional layer with 1024 filters
conv5 = BatchNormalization()(conv5)  # Apply batch normalization to normalize the activations
conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(conv5)  # Convolutional layer with 1024 filters
conv5 = BatchNormalization()(conv5)  # Apply batch normalization to normalize the activations
drop5 = Dropout(0.5)(conv5)  # Apply dropout regularization with a rate of 0.5

# Decoder
up6 = concatenate([UpSampling2D((2, 2))(drop5), conv4], axis=-1)  # Upsampling layer with a scale factor of (2, 2)
conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(up6)  # Convolutional layer with 512 filters
conv6 = BatchNormalization()(conv6)  # Apply batch normalization to normalize the activations
conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv6)  # Convolutional layer with 512 filters
conv6 = BatchNormalization()(conv6)  # Apply batch normalization to normalize the activations

up7 = concatenate([UpSampling2D((2, 2))(conv6), conv3], axis=-1)  # Upsampling layer with a scale factor of (2, 2)
conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(up7)  # Convolutional layer with 256 filters
conv7 = BatchNormalization()(conv7)  # Apply batch normalization to normalize the activations
conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv7)  # Convolutional layer with 256 filters
conv7 = BatchNormalization()(conv7)  # Apply batch normalization to normalize the activations

up8 = concatenate([UpSampling2D((2, 2))(conv7), conv2], axis=-1)  # Upsampling layer with a scale factor of (2, 2)
conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(up8)  # Convolutional layer with 128 filters
conv8 = BatchNormalization()(conv8)  # Apply batch normalization to normalize the activations
conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv8)  # Convolutional layer with 128 filters
conv8 = BatchNormalization()(conv8)  # Apply batch normalization to normalize the activations

up9 = concatenate([UpSampling2D((2, 2))(conv8), conv1], axis=-1)  # Upsampling layer with a scale factor of (2, 2)
conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(up9)  # Convolutional layer with 64 filters
conv9 = BatchNormalization()(conv9)  # Apply batch normalization to normalize the activations
conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv9)  # Convolutional layer with 64 filters
conv9 = BatchNormalization()(conv9)  # Apply batch normalization to normalize the activations

outputs = Conv2D(n_classes, (1, 1), activation='softmax')(conv9)  # Convolutional layer for output

# Create the model
model = Model(inputs=inputs, outputs=outputs)

# Print the model summary
model.summary()


# Set the optimizer for the model
optimizer = tf.keras.optimizers.Adam(lr=1e-4)

# Compile the model with loss function and metrics
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

images_path = '/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/val2014'
masks_path = '/kaggle/working/mask_val_2014'
batch_size = 8

val_generator = CustomDataGenerator(images_path, masks_path, batch_size)



```


Also to ensure that all the input and output have proper shape I run the code below

```
def print_preprocessed_image_shapes(model, generator):
    """"""
    Print the shapes of preprocessed images generated by the provided model and generator.

    Args:
        model (tf.keras.Model): The trained model.
        generator (CustomDataGenerator): Instance of the CustomDataGenerator class.
    """"""
    for i in range(len(generator)):
        # Get a batch of preprocessed images from the generator
        batch_images, batch_masks = generator[i]

        # Print the shapes of the preprocessed images
        for image in batch_images:
            print(f""Shape of preprocessed image: {image.shape}"")
        for mask in batch_maskss:

             print(f""Shape of preprocessed image: {mask.shape}"")
            
# Print the shapes of preprocessed images
print_preprocessed_image_shapes(model, val_generator)
```
As a result of this error, the model was unable to undergo the training process.

### Standalone code to reproduce the issue

```shell
# Fit the model with the training generator
train_steps =  len(os.listdir( ""/kaggle/working/mask_train_2014/""))/batch_size
model.fit(train_generator,validation_data = val_generator, steps_per_epoch = train_steps , epochs=20)
```


### Relevant log output

```shell
tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_5/dropout_10/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
```
"
61323,TensorFlow Lite Converter wraps unpack operator with dequantize/quantize,"### System information

- Linux Ubuntu 20.04
- TensorFlow installed from: pip source
- TensorFlow versions: 2.12.0-current master 

### Code

Provide code to help us reproduce your issues using one of the following options:

```
import tensorflow as tf
from tensorflow.keras.layers import Layer, Input
from tensorflow.keras.models import Model
import numpy as np

class UnstackLayer(Layer):
    def __init__(self, **kwargs):
        super(UnstackLayer, self).__init__(**kwargs)

    def call(self, inputs):
        unstacked = tf.unstack(inputs, axis=1)
        # only last output is used as input to the add operator
        return unstacked[-1]

input_tensor = Input(shape=(4, 16, 32))
x = UnstackLayer()(input_tensor)
output_tensor = tf.add(x, 1)

model = Model(inputs=input_tensor, outputs=output_tensor)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

def representative_data_gen():
  for input_value in [np.random.randn(1, 4, 16, 32).astype(np.float32) for _ in range(10)]:
    yield [input_value]
converter.representative_dataset = representative_data_gen

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.target_spec.supported_types = {tf.int8}

converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_model = converter.convert()


# Save the TFLite model to a .tflite file
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)
```

### Failure after conversion

Input Model:

![Screenshot from 2023-07-19 11-05-35](https://github.com/tensorflow/tensorflow/assets/112401409/dabb3863-54c3-4148-bcab-e4afd39a4ac5)

Output Model:

![Screenshot from 2023-07-19 11-28-39](https://github.com/tensorflow/tensorflow/assets/112401409/f42419d0-0365-4738-bd0c-0bf9dc24b798)


Behaviour in TF 2.11 and below is that no dequantize/quantize ops appear, which is expected.  

### Other info

The conversion started failing with version TF 2.12.0. Note that the conversion succeeds intermittently when converting the same network many times, but on average it fails. This intermittent behaviour is still present if one runs the converter on a single core and keeps the representative dataset constant. Similar issues seems to be present when converting split operators as well. 


"
61322,Tensorflow and onednn logs,"Hi, 
    I am trying to analyse the call flow of tensorflow and onednn. I am setting up environment vars as 
export ONEDNN_VERBOSE=1
export TF_CPP_MAX_VLOG_LEVEL=1  
export omp_num_threads=1

I am collecting the logs. I was trying to map _mklops with onednn primitive. But here the onednn and mkl calls are random in log that is after 10 mkl calls I am seeing 20 onednn calls. 

Is there any flags need to be set to get logs with correct mapping  or do we need to map manually
[filtered_intel_log.txt](https://github.com/tensorflow/tensorflow/files/12093285/filtered_intel_log.txt)

"
61321,Android GPU Delegate Error while using groups in Conv2d,"**System information**
- Android 13:
- TensorFlow installed from binary:
 
**use GPU Delegate**

**use model**

```python
        self.g_a0 = tf.keras.layers.Conv2D(
            N,
            kernel_size=5,
            strides=2,
            padding=""same"",
            # data_format=""channels_first"",
        )
        self.g_a1_test1 = tf.keras.layers.Conv2D(
            N,
            3,
            padding=""same"",
            groups=N,
            # data_format=""channels_first"",
        )
```
**error on android**
```
java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Can not open OpenCL library on this device - undefined symbol: clGetCommandBufferInfoKHR
Falling back to OpenGL
TfLiteGpuDelegate Init: No shader implementation for split
TfLiteGpuDelegate Prepare: delegate is not initialized
Node number 2 (TfLiteGpuDelegateV2) failed to prepare.
Restored original execution plan after delegate application failure.
```


**use anothor model**

```python
        self.g_a0 = tf.keras.layers.Conv2D(
            N,
            kernel_size=5,
            strides=2,
            padding=""same"",
            # data_format=""channels_first"",
        )
        self.g_a1_test1 = tf.keras.layers.Conv2D(
            N,
            3,
            padding=""same"",
            # groups=N,
            # data_format=""channels_first"",
        )
```
No errors on Android"
61319,Converted(quantized) model of simple dense neural network returns same repeated output,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04.2 LTS
- TensorFlow installation (pip package or built from source):
```bash
export condaPip=`which -a pip | grep $condaEnvName`


$condaPip install tensorflow==2.7.0
$condaPip install numpy
$condaPip install pandas
$condaPip install scikit-learn
$condaPip install statsmodels 
$condaPip install matplotlib
$condaPip install future
$condaPip install onnx
$condaPip install torchviz
$condaPip install mpi
$condaPip install torch
$condaPip install tqdm
$condaPip install pydot
$condaPip install ipympl
$condaPip install seaborn
$condaPip install tabulate
$condaPip install xgboost
$condaPip install catboost
$condaPip install bokeh
```
Installed using conda
- TensorFlow library (version, if pip package or github SHA, if built from source):
Listed above

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
```python
    dnn_model = keras.Sequential([
        normalizer,
        layers.Dense(512, activation='relu', input_dim=x_train.shape[1]),
        layers.Dense(512, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
```
https://www.tensorflow.org/lite/performance/post_training_quantization
My model code is simple as above
And I used **Post Training Quantization using TF.Lite.Converter** (32-float to 8-int)
Integer only post training quantization (https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only)

(Input dim: 8)


### 3. Failure after conversion
#### Reset input shape
After the conversion, the model has been distorted and i solved the problem with resetting input format
```python
interpreter.resize_tensor_input(input_details['index'], (1,8))
```
And it works well but it returns same output all the time from different inputs
Like this,
```
[array([[76]], dtype=uint8), array([[76]], dtype=uint8), array([[76]], dtype=uint8), array([[76]], ...
```

I googled the cases same as mine, but some pointed out learning epochs, or Model structure.
Nothing was problematic with learning epochs and I suppose there are some model compatibility for quantization
I tried other tensorflow versions (2.13.0, or nightly: 2.14.0-dev20230706) but still same as before.

#### Quantization w/ floating value
https://www.tensorflow.org/lite/performance/post_training_quantization#integer_with_float_fallback_using_default_float_inputoutput
I tried quantization using float32 input/output, but it's same with float value
```
[array([[0.36328125]], dtype=float32), array([[0.36328125]], dtype=float32), array([[0.36328125]], dtype=float32), array([[0.36328125]], dtype=float32), array([[0.36328125]], dtype=float32), ...
```

#### Quantized Model Weights
I printed out model weights, but not sure why it happens

I'm afriad the most of model weights are going 0 during conversion, and it became useless.
So the identical output is returned due to Model Bias.
(You can see the weights of quantized model below)

Do you have any idea why it happens or any tips ? 

### 5. (optional) Any other info / logs

#### The predicted output of original model (Expected)
```
[0.04892164 0.05425507 0.34756148 0.12509596 0.05663526 0.05041704
 0.01185179 0.33582878 0.0572255  0.07754967 0.20722428 0.208343
 0.48504433 0.10513872 0.05141798 0.03368005 0.09833255 0.2034252
 0.109099   0.48486084 0.23009542 0.08055633 0.06971437 0.14443058
 0.12913615 0.03327829 0.06535947 0.4671367  0.20984942 0.35980904
...
 0.23956934 0.23102319 0.10548833 0.06846321 0.16444126 0.04730341
 0.47125074 0.1931791  0.16075167 0.06618178 0.06408405 0.50926745
 0.12504464 0.73206306 0.22954968 0.00386357 0.19742006 0.09496576
 0.04637471 0.06708863 0.37460512 0.17932612 0.10157627 0.04863232
 0.04412654 0.50302374 0.06885636 0.21528003 0.09171575 0.0635072
 0.08805934 0.3634451  0.57593703 0.09678486]
```

#### Quantized model weights

<details> 
<summary> Click to watch </summary>

```
Quantized weights
[[  0 161   1 199 181 127   0   0]]
[[-128  127 -126 -126 -126 -127 -127 -127]]
[[ 127 -128 -128 -128 -128 -128 -128 -128]]
[[  80   23  -77 ...    0    7  115]
 [  74   55  -27 ...   70   56  103]
 [ 112  -89   43 ...  -18 -112  -91]
 ...
 [  45  104   -4 ...  -57   91   47]
 [  43   97  -94 ...   19  113  -43]
 [ 110   50  110 ...   34   18    3]]
[  57  -20  195  271  152  140  164  230  186  199 -129   -9  -25  204
  192  -11  183  -83  210  137  166   13  -21   36  248  243 -118  128
 -114  224  221   85  221  134 -142  -60  108  194  118  222  183   39
  201  104 -130   51    7  -39   16  246  224  103 -103  -43  -36  198
  156  -87  224  175   76 -105  -54   97  -69   62  -26 -169  238   63
   99  235   32 -130  -98  152  247  -56  145  191 -122  161   88 -119
   -2  -17  104  158  147  123  221  189  236  170  266  191  210  162
  -43  193  -94  208  -94   71 -130   97  -95  -29  181  244  183 -120
   72  -40 -106 -151   41  125  210  -92   84  -14  187    1  141  -94
   43  141  195  201   -7  227  178    8   67  243   97  229  200   38
  195  197  203   26  186   94  170 -138  210  209   49 -150 -146  202
  187  199  -91  196   95  216  222  203  213  237  167  -39  187  180
  183  -80  -28  170  -62  243   69   48  -57  178  241  213  247   42
  207  131 -111 -117  149 -114 -131  152  143  -36 -124  -81 -141  126
  259  127   89   -4  124  -63  229  192  191   -4  -79  -46   35  -99
  195   77  -62  185  194   30  -75  192  130   35  179 -155  -96   -7
  178   29  177 -105  197   39  237  176  171  201   24  199  -70 -149
  236  220  211 -165  165  248 -100  -71 -152 -123   77  183  159  -72
  186   37 -127  159  183  211  171 -120   38   -7  183  208  -28 -101
  174  176  234  204  182  165  238  145  232  159  256   50  -59   77
  -31  109   -3 -117   41   48  140  131  203  235  117  106   80  197
   32  166  141  183  203  -62   59  183  -36  -54  188   28  221 -108
 -136  -79  190  161  199  111   62  153  -12  139  205  247  233  154
  209  222 -130   27  209 -103  -83  240  130  210  184  138 -104   72
  173   28   14  206  -80  119  177  124  158  200  242  230   83  156
  242    7  190  210  -94  114  210  224  225  -42  232  -34  -67  -39
  230  165  199  224 -134  163  156   -7  136  134  176    2  209   73
   40  192 -109  216  227  201   -6  157  250   81  142   19  218  187
  -52   13  236   29  -24  243   90  135  143  174  189  218  146  239
  173  230  211  211  139  248  178  217  210  206  -86  136  -65  198
  201  218  238  192  223 -125 -178  203   16 -146 -162 -116  198  220
  232  179  105  -51  137 -111  -35 -154  266  -38  -88 -123  189  -95
  -98  237  159  -63  186   72   71  -25  182  168  239  216  171  168
  253  -33  223 -148  109  186  198 -157  229  198  210  100  155 -130
  112  210  111 -114   59  190  224   50  177 -119  144  -95 -110  182
  225  183   56  223  119  171  155  200  220  156  -73  178   44  200
  194   -6  -65  220   18   81  -20  -43]
[[  23   24  -26 ...   36  -24   37]
 [  48   40  -87 ...   44  -32  -21]
 [  -2   16  -38 ... -101  -28  104]
 ...
 [  64   89    2 ...  -37   95  -48]
 [  47  -14  107 ...  -85  -23 -106]
 [  12   31  -60 ...   63  -71  -85]]
[ 2110  2151 -1352  1999  1340  -833 -1006  -657 -1454  -641  2383  2034
  2358  -577 -1184  2455  2270  2831  -626 -1301  2957  2355  -665 -1340
  2138  -948  1411  -747  -690 -1268  2206  -575  2340  2376    70  2465
  -530  -668 -1109   951  2015  2042  -825  -976  2430  -682 -1268  -174
 -1176  1994  -440  -954  -727 -1087 -1017 -1290  -968  2100  1938  -783
  1915  -175  2582  -340  1964 -1330  2090  1963  -688 -1356   -22  1897
  2157  2447  1994  2207  -593 -1213 -1291  2120  2198 -1067  -918  2017
  1983  2399  2494  2201  1857 -1121  2406  2163   -48  -895 -1257 -1288
  2175  2481  2058  2170  3646  -714 -1171  1959  2715  2042  2605  -928
  -473  2099  2286  2298  1995  1968  2472  -356  -901   971  2610  -796
  2199  2135  1871  2212  1927  -972  2225  -975  -820  -932 -1070  2459
  -755  -721  1895  2021  -967  1987  -689 -1477  2076 -1138  2072 -1029
  -413  1856  -649  2216  2087  2194  1955  1996  2144  1895  3004  -780
  1739 -1404  -586 -1151  1895  -940 -1537  3030  -282   832  2043  1985
  2074  1936  2167 -1049  2218  -531 -1049  -843 -1189  -541  2679  -911
  2163  1518  -842 -1254  -966  1975 -1307  -983 -1360  -761  2085 -1137
  2005 -1255  2499  2207  -797 -1105  2200  1923  1895  -885  -633  1901
  1926 -1021  2195  1965 -1151  -952  -502 -1146  2204 -1306  2188  1859
 -1117  1888 -1144  1930  2082  -522 -1029   326  2102 -1007  2078  2298
  2038  1992 -1017  1646 -1298  1881 -1000  1916  2775  -558  2738 -1283
  -715 -1176 -1282  2161  2398  1922  -746 -1193  2281 -1019  2200  2021
  2053  1871 -1253  -621 -1009 -1205  2219  -925 -1342     0  2216  -885
 -1417 -1225 -1295  2147  -984  2133  1986  2767 -1077  1961  2159 -1142
 -1275   259  2182 -1005  -987 -1213  2103  2014  2043 -1087 -1509 -1196
 -1394  -303 -1050  -995  2462  2664  2005 -1157  -931  2737 -1013 -1521
  -751  -934  2335  2780 -1111  2000  1935  2166  2061  2373  2191  -682
 -1478  1882  -505  2093  1916 -1253  -304  2630  1891 -1145 -1167  1923
  1887  1956  2147  2966  -332  -270   336 -1002 -1138  -676  2056  -930
  -520  2277  1894  -970 -1346  -768  1858  2171  2364  2105  -795  2191
  -999  2027 -1141  -381 -1358  -732  1929  -469 -1206 -1283  1889  2446
 -1072 -1082  2227  2127 -1278  2183 -1148  1875  2052  2225 -1003  -741
 -1108 -1066 -1231  2373  1970  2116  2023  -234  1028  2289  1832 -1344
  -649 -1129  1993   505   625  -932 -1135 -1160  -355 -1130  2307 -1242
 -1087  2216  1848  -836  -150  2590  1926  2043  -896  -810  1922 -1031
  -869  2524  -476  -228    76 -1179  -469  2511  2012 -1650  1855  2989
  2400  2430  -155 -1171  -961 -1158  1823 -1036 -1244  2838  2554  2139
  1925  -972  -403   364 -1263  2226  2255  2024  2200  2205 -1223  1982
  1809  -921  2116  2142  -843  2195  -212 -1501  2415  -676  -607  2274
  2172  2157 -1212 -1000  2007  2753 -1065 -1048 -1231  1937  2695 -1240
  -953 -1246  -608  1932  1148 -1232  2039  2225 -1069  2009  2291  2003
  2003 -1369  2226 -1174  2264  -890  2128  1888  3043  -734  -719 -1385
 -1140  2348  2233  2103  2280  2138  2450  2377 -1008  2026  1958  1892
 -1075 -1108  1946 -1078  2522  1899  1416  1882]
[[ -67  -52   50  -61  -66   54  107   16   83   32  -22  -59  -30  113
    95  -30 -123  -93    1   55   -5  -24   28   58  -77   60  -67   22
    59   21  -99   15  -18  -33   60  -62  111   22   10  -51  -74  -54
    36    1  -27   16  103  108   59  -37   53   80    9  100   62   78
   112  -94 -100   93  -96   23  -87   19  -86   88  -91 -102   97   20
    32  -59  -88  -98  -11 -106   90   18    5  -41  -67   16    9  -54
   -38  -70 -119 -102  -89  110  -16  -75   88    5    5    8  -36  -55
  -108  -49  -12   31   45 -115  -70 -120  -71   39   43 -124  -44  -27
   -21  -62 -127   77  105  -52  -30   46  -79  -46  -83  -64  -28   91
   -80  116   99   52   99   -9   38  116  -87  -63   64  -93   71   82
  -108   70  -48  111   31 -109   40  -62  -45  -92 -115  -73 -112 -118
    -7   48  -11   27   85   86  -59   11   40  -31   25  -88  -83 -116
   -99  -39  -37   36  -12  112   65   79   60    1  -51   73  -34  -55
    28   53   98 -122   16   85   59  111  -60   40 -125   19  -78  -35
    55   12  -26  -87  -96   85   16 -100  -45   94 -109   -9   18    5
    61   64  -93   17   -5 -124  110  -41   77  -67  -74   69  105  -68
   -28   32  -20  -18  -81  -57   38  -84   14  -53   48 -105  -52   54
   -92   78   40   89   24  -49 -106  -90   33   34 -111   74   -9  -72
   -93  -87   55   49   10   23  -93   31    4  -60  -41   57   18   34
    93  -96    3  -79  -47  -21  114  -36  -13   97   66   72   -7   89
   103   61  -36 -120  -57  111    7   83   38   10   37   17  -96  -12
   -57   48    7  -75    8   31   16   34 -122  -12   62 -118  -71  -85
   -47 -121 -108   34   78  -86   79  -56  -90   60   73  -10  -31   30
    92  -99  -80 -119  -34  -55   63   81   69   19   78   70  -47  113
    16  -97  -61   16   26   66 -103 -110  -31  -43  108  -78   81 -123
    62   61   66   71 -113   34   29   57 -117  -22   93   21  -59 -118
   102  -81   35  -61  -98  -58   74   44   16   38   70  -36  -66  -49
   -34   18   -2  -48 -103   23   23   32  -35  -94  -68   64   24  114
    58   83  -30   22    6  -72 -116   58   44  -32  -31  -89   69   62
   -70   47   48 -106   66   73  117   16  108 -101 -114   97  -75  -15
   -27  -95   39   58   23   44  -74   77   79  -12   -8  -30  -65   76
    49  -98   68  -36  -31  -26 -120  -30   17 -100  -56   89  -43  -49
    59  -99   83   36  -37   10  100  -18 -119  -64  111   42  -96   -6
    30   87   20  -86  -48    5   92   13   96  -31  -36  112 -112 -112
    71  -81 -109 -101  -36   72  -37   54  -89   69  -55  -67  -14    7
    89  113   52  -24  -25  -51  -57  -13  -24  -73   91 -104  -49  -51
   112  110  -60   25  -46  -77  -72   -9]]
[-960]
[[   0 -110   -2   74  -75  127    0    0]]
[[  80 -116    4   33  -75  127    0    0]]
[[-64  -8  50   9   0   0   0   0]]
[[   0 -110   -2   74  -75  127    0    0   96   33   50    9    0    0
     0    0  105   98   47  112  121  116  104  111  110   51   46   56
    47  115  105  116  -80   -2   96    9    0    0    0    0  112   40
    55    9    0    0    0    0    0    0    0    0    0    0    0    0
    98   55   -4   74  -75  127    0    0   80 -116    4   33  -75  127
     0    0    0 -110  124   40   -2  127    0    0    0    0    0    0
     0    0    0    0  -30   27   -5   74  -75  127    0    0  -64   -8
    50    9    0    0    0    0  -64   -8   50    9    0    0    0    0
     0    0    0    0    0    0    0    0  -14   81   -4   74  -75  127
     0    0  -64  -87   58   45  -75  127    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0  -14   27
    -5   74  -75  127    0    0    0  -34  106   42  -75  127    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0  106   55   -4   74  -75  127    0    0  -80   35   19   41
   -75  127    0    0  120  126   -2   74  -75  127    0    0    0    0
     0    0    0    0    0    0  -22   27   -5   74  -75  127    0    0
   -32   73 -109    8    0    0    0    0    2    0    0    0    0    0
     0    0  116  102   46   69  110  113  117  101  117  101   84   80
    85   69  109   98  101  100  100  105  110  103   73  110  116  101
   103  101  114   66   97  116   99  104    0  103  101  115   47  107
  -127    3    0    0    0    0    0    0   48  -86   50    9    0    0
     0    0  -48   54   29    9    0    0    0    0    0    0    0    0
     0    0    0    0   97    3    0    0    0    0    0    0  -48   48
    50    9    0    0    0    0  -32 -100    1  -57  -75  127    0    0
     0    0    0    0    0    0    0    0   65    3    0    0    0    0
     0    0 -128   88   62    9    0    0    0    0    0   87   27    9
     0    0    0    0   16 -120 -109    8    0    0    0    0   88    1
     1   75  -75  127    0    0   80   50  -73    7    0    0    0    0
     2    0    0    0    3    0    0    0  120  126   -2   74  -75  127
     0    0  -32   49  -73    7    0    0    0    0    0 -110   -2   74
   -75  127    0    0    0   50  -73    7    0    0    0    0  109  101
    47   99   99   47   97  110   97   99  111  110  100   97   51   47
   -80   -2   96    9    0    0    0    0  112   40   55    9    0    0
     0    0    0    0    0    0    0    0    0    0   82   55   -4   74
   -75  127    0    0   80 -116    4   33  -75  127    0    0    0 -110
   124   40   -2  127    0    0    0    0    0    0    0    0    0    0
   -30   27   -5   74  -75  127    0    0]]
[[ -64   -8   50    9    0    0    0    0  -64   -8   50    9    0    0
     0    0    0    0    0    0    0    0    0    0  -22   81   -4   74
   -75  127    0    0  -64  -87   58   45  -75  127    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
   -14   27   -5   74  -75  127    0    0    0  -34  106   42  -75  127
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0   90   55   -4   74  -75  127    0    0   16   36
    19   41  -75  127    0    0  120  126   -2   74  -75  127    0    0
     0    0    0    0    0    0    0    0  -22   27   -5   74  -75  127
     0    0   40   74 -109    8    0    0    0    0    9    0    0    0
     0    0    0    0  116  102   46   69  110  113  117  101  117  101
    84   80   85   69  109   98  101  100  100  105  110  103   82   97
   103  103  101  100   84  101  110  115  111  114   66   97  116   99
   104    0  -31    1    0    0    0    0    0    0  -64   36   33    9
     0    0    0    0  -64   17   48    9    0    0    0    0    0    0
     0    0    0    0    0    0 -127    1    0    0    0    0    0    0
  -128   25   50    9    0    0    0    0  -32 -100    1  -57  -75  127
     0    0    0    0    0    0    0    0    0    0   97    1    0    0
     0    0    0    0   96    6   31    9    0    0    0    0  -80  -49
    86    9    0    0    0    0   16 -120 -109    8    0    0    0    0
    80    1    1   75  -75  127    0    0  -16   51  -73    7    0    0
     0    0    2    0    0    0    3    0    0    0  120  126   -2   74
   -75  127    0    0 -128   51  -73    7    0    0    0    0    0 -110
    -2   74  -75  127    0    0  -96   51  -73    7    0    0    0    0
    98   47  112  121  116  104  111  110   51   46   56   47  115  105
   116  101  -80   -2   96    9    0    0    0    0  112   40   55    9
     0    0    0    0    0    0    0    0    0    0    0    0   66   55
    -4   74  -75  127    0    0   80 -116    4   33  -75  127    0    0
     0 -110  124   40   -2  127    0    0    0    0    0    0    0    0
     0    0  -30   27   -5   74  -75  127    0    0  -64   -8   50    9
     0    0    0    0  -64   -8   50    9    0    0    0    0    0    0
     0    0    0    0    0    0  -30   81   -4   74  -75  127    0    0
   -64  -87   58   45  -75  127    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0  -14   27   -5   74
   -75  127    0    0    0  -34  106   42  -75  127    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
    74   55   -4   74  -75  127    0    0]]
[[0]]
[[80]]
```

</details>


#### Debugger
- I used debugger
  - https://www.tensorflow.org/lite/performance/quantization_debugger

![image](https://github.com/tensorflow/tensorflow/assets/15901475/a67571a4-e017-4bcd-a6a0-80fdfbc8a62f)
"
61319,Converted(quantized) model of simple dense neural network returns same repeated output,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04.2 LTS
- TensorFlow installation (pip package or built from source):
```bash
export condaPip=`which -a pip | grep $condaEnvName`


$condaPip install tensorflow==2.7.0
$condaPip install numpy
$condaPip install pandas
$condaPip install scikit-learn
$condaPip install statsmodels 
$condaPip install matplotlib
$condaPip install future
$condaPip install onnx
$condaPip install torchviz
$condaPip install mpi
$condaPip install torch
$condaPip install tqdm
$condaPip install pydot
$condaPip install ipympl
$condaPip install seaborn
$condaPip install tabulate
$condaPip install xgboost
$condaPip install catboost
$condaPip install bokeh
```
Installed using conda
- TensorFlow library (version, if pip package or github SHA, if built from source):
Listed above

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
```python
    dnn_model = keras.Sequential([
        normalizer,
        layers.Dense(512, activation='relu', input_dim=x_train.shape[1]),
        layers.Dense(512, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
```
https://www.tensorflow.org/lite/performance/post_training_quantization
My model code is simple as above
And I used **Post Training Quantization using TF.Lite.Converter** (32-float to 8-int)
Integer only post training quantization (https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only)

(Input dim: 8)


### 3. Failure after conversion
#### Reset input shape
After the conversion, the model has been distorted and i solved the problem with resetting input format
```python
interpreter.resize_tensor_input(input_details['index'], (1,8))
```
And it works well but it returns same output all the time from different inputs
Like this,
```
[array([[76]], dtype=uint8), array([[76]], dtype=uint8), array([[76]], dtype=uint8), array([[76]], ...
```

I googled the cases same as mine, but some pointed out learning epochs, or Model structure.
Nothing was problematic with learning epochs and I suppose there are some model compatibility for quantization
I tried other tensorflow versions (2.13.0, or nightly: 2.14.0-dev20230706) but still same as before.

#### Quantization w/ floating value
https://www.tensorflow.org/lite/performance/post_training_quantization#integer_with_float_fallback_using_default_float_inputoutput
I tried quantization using float32 input/output, but it's same with float value
```
[array([[0.36328125]], dtype=float32), array([[0.36328125]], dtype=float32), array([[0.36328125]], dtype=float32), array([[0.36328125]], dtype=float32), array([[0.36328125]], dtype=float32), ...
```

#### Quantized Model Weights
I printed out model weights, but not sure why it happens

I'm afriad the most of model weights are going 0 during conversion, and it became useless.
So the identical output is returned due to Model Bias.
(You can see the weights of quantized model below)

Do you have any idea why it happens or any tips ? 

### 5. (optional) Any other info / logs

#### The predicted output of original model (Expected)
```
[0.04892164 0.05425507 0.34756148 0.12509596 0.05663526 0.05041704
 0.01185179 0.33582878 0.0572255  0.07754967 0.20722428 0.208343
 0.48504433 0.10513872 0.05141798 0.03368005 0.09833255 0.2034252
 0.109099   0.48486084 0.23009542 0.08055633 0.06971437 0.14443058
 0.12913615 0.03327829 0.06535947 0.4671367  0.20984942 0.35980904
...
 0.23956934 0.23102319 0.10548833 0.06846321 0.16444126 0.04730341
 0.47125074 0.1931791  0.16075167 0.06618178 0.06408405 0.50926745
 0.12504464 0.73206306 0.22954968 0.00386357 0.19742006 0.09496576
 0.04637471 0.06708863 0.37460512 0.17932612 0.10157627 0.04863232
 0.04412654 0.50302374 0.06885636 0.21528003 0.09171575 0.0635072
 0.08805934 0.3634451  0.57593703 0.09678486]
```

#### Quantized model weights

<details> 
<summary> Click to watch </summary>

```
Quantized weights
[[  0 161   1 199 181 127   0   0]]
[[-128  127 -126 -126 -126 -127 -127 -127]]
[[ 127 -128 -128 -128 -128 -128 -128 -128]]
[[  80   23  -77 ...    0    7  115]
 [  74   55  -27 ...   70   56  103]
 [ 112  -89   43 ...  -18 -112  -91]
 ...
 [  45  104   -4 ...  -57   91   47]
 [  43   97  -94 ...   19  113  -43]
 [ 110   50  110 ...   34   18    3]]
[  57  -20  195  271  152  140  164  230  186  199 -129   -9  -25  204
  192  -11  183  -83  210  137  166   13  -21   36  248  243 -118  128
 -114  224  221   85  221  134 -142  -60  108  194  118  222  183   39
  201  104 -130   51    7  -39   16  246  224  103 -103  -43  -36  198
  156  -87  224  175   76 -105  -54   97  -69   62  -26 -169  238   63
   99  235   32 -130  -98  152  247  -56  145  191 -122  161   88 -119
   -2  -17  104  158  147  123  221  189  236  170  266  191  210  162
  -43  193  -94  208  -94   71 -130   97  -95  -29  181  244  183 -120
   72  -40 -106 -151   41  125  210  -92   84  -14  187    1  141  -94
   43  141  195  201   -7  227  178    8   67  243   97  229  200   38
  195  197  203   26  186   94  170 -138  210  209   49 -150 -146  202
  187  199  -91  196   95  216  222  203  213  237  167  -39  187  180
  183  -80  -28  170  -62  243   69   48  -57  178  241  213  247   42
  207  131 -111 -117  149 -114 -131  152  143  -36 -124  -81 -141  126
  259  127   89   -4  124  -63  229  192  191   -4  -79  -46   35  -99
  195   77  -62  185  194   30  -75  192  130   35  179 -155  -96   -7
  178   29  177 -105  197   39  237  176  171  201   24  199  -70 -149
  236  220  211 -165  165  248 -100  -71 -152 -123   77  183  159  -72
  186   37 -127  159  183  211  171 -120   38   -7  183  208  -28 -101
  174  176  234  204  182  165  238  145  232  159  256   50  -59   77
  -31  109   -3 -117   41   48  140  131  203  235  117  106   80  197
   32  166  141  183  203  -62   59  183  -36  -54  188   28  221 -108
 -136  -79  190  161  199  111   62  153  -12  139  205  247  233  154
  209  222 -130   27  209 -103  -83  240  130  210  184  138 -104   72
  173   28   14  206  -80  119  177  124  158  200  242  230   83  156
  242    7  190  210  -94  114  210  224  225  -42  232  -34  -67  -39
  230  165  199  224 -134  163  156   -7  136  134  176    2  209   73
   40  192 -109  216  227  201   -6  157  250   81  142   19  218  187
  -52   13  236   29  -24  243   90  135  143  174  189  218  146  239
  173  230  211  211  139  248  178  217  210  206  -86  136  -65  198
  201  218  238  192  223 -125 -178  203   16 -146 -162 -116  198  220
  232  179  105  -51  137 -111  -35 -154  266  -38  -88 -123  189  -95
  -98  237  159  -63  186   72   71  -25  182  168  239  216  171  168
  253  -33  223 -148  109  186  198 -157  229  198  210  100  155 -130
  112  210  111 -114   59  190  224   50  177 -119  144  -95 -110  182
  225  183   56  223  119  171  155  200  220  156  -73  178   44  200
  194   -6  -65  220   18   81  -20  -43]
[[  23   24  -26 ...   36  -24   37]
 [  48   40  -87 ...   44  -32  -21]
 [  -2   16  -38 ... -101  -28  104]
 ...
 [  64   89    2 ...  -37   95  -48]
 [  47  -14  107 ...  -85  -23 -106]
 [  12   31  -60 ...   63  -71  -85]]
[ 2110  2151 -1352  1999  1340  -833 -1006  -657 -1454  -641  2383  2034
  2358  -577 -1184  2455  2270  2831  -626 -1301  2957  2355  -665 -1340
  2138  -948  1411  -747  -690 -1268  2206  -575  2340  2376    70  2465
  -530  -668 -1109   951  2015  2042  -825  -976  2430  -682 -1268  -174
 -1176  1994  -440  -954  -727 -1087 -1017 -1290  -968  2100  1938  -783
  1915  -175  2582  -340  1964 -1330  2090  1963  -688 -1356   -22  1897
  2157  2447  1994  2207  -593 -1213 -1291  2120  2198 -1067  -918  2017
  1983  2399  2494  2201  1857 -1121  2406  2163   -48  -895 -1257 -1288
  2175  2481  2058  2170  3646  -714 -1171  1959  2715  2042  2605  -928
  -473  2099  2286  2298  1995  1968  2472  -356  -901   971  2610  -796
  2199  2135  1871  2212  1927  -972  2225  -975  -820  -932 -1070  2459
  -755  -721  1895  2021  -967  1987  -689 -1477  2076 -1138  2072 -1029
  -413  1856  -649  2216  2087  2194  1955  1996  2144  1895  3004  -780
  1739 -1404  -586 -1151  1895  -940 -1537  3030  -282   832  2043  1985
  2074  1936  2167 -1049  2218  -531 -1049  -843 -1189  -541  2679  -911
  2163  1518  -842 -1254  -966  1975 -1307  -983 -1360  -761  2085 -1137
  2005 -1255  2499  2207  -797 -1105  2200  1923  1895  -885  -633  1901
  1926 -1021  2195  1965 -1151  -952  -502 -1146  2204 -1306  2188  1859
 -1117  1888 -1144  1930  2082  -522 -1029   326  2102 -1007  2078  2298
  2038  1992 -1017  1646 -1298  1881 -1000  1916  2775  -558  2738 -1283
  -715 -1176 -1282  2161  2398  1922  -746 -1193  2281 -1019  2200  2021
  2053  1871 -1253  -621 -1009 -1205  2219  -925 -1342     0  2216  -885
 -1417 -1225 -1295  2147  -984  2133  1986  2767 -1077  1961  2159 -1142
 -1275   259  2182 -1005  -987 -1213  2103  2014  2043 -1087 -1509 -1196
 -1394  -303 -1050  -995  2462  2664  2005 -1157  -931  2737 -1013 -1521
  -751  -934  2335  2780 -1111  2000  1935  2166  2061  2373  2191  -682
 -1478  1882  -505  2093  1916 -1253  -304  2630  1891 -1145 -1167  1923
  1887  1956  2147  2966  -332  -270   336 -1002 -1138  -676  2056  -930
  -520  2277  1894  -970 -1346  -768  1858  2171  2364  2105  -795  2191
  -999  2027 -1141  -381 -1358  -732  1929  -469 -1206 -1283  1889  2446
 -1072 -1082  2227  2127 -1278  2183 -1148  1875  2052  2225 -1003  -741
 -1108 -1066 -1231  2373  1970  2116  2023  -234  1028  2289  1832 -1344
  -649 -1129  1993   505   625  -932 -1135 -1160  -355 -1130  2307 -1242
 -1087  2216  1848  -836  -150  2590  1926  2043  -896  -810  1922 -1031
  -869  2524  -476  -228    76 -1179  -469  2511  2012 -1650  1855  2989
  2400  2430  -155 -1171  -961 -1158  1823 -1036 -1244  2838  2554  2139
  1925  -972  -403   364 -1263  2226  2255  2024  2200  2205 -1223  1982
  1809  -921  2116  2142  -843  2195  -212 -1501  2415  -676  -607  2274
  2172  2157 -1212 -1000  2007  2753 -1065 -1048 -1231  1937  2695 -1240
  -953 -1246  -608  1932  1148 -1232  2039  2225 -1069  2009  2291  2003
  2003 -1369  2226 -1174  2264  -890  2128  1888  3043  -734  -719 -1385
 -1140  2348  2233  2103  2280  2138  2450  2377 -1008  2026  1958  1892
 -1075 -1108  1946 -1078  2522  1899  1416  1882]
[[ -67  -52   50  -61  -66   54  107   16   83   32  -22  -59  -30  113
    95  -30 -123  -93    1   55   -5  -24   28   58  -77   60  -67   22
    59   21  -99   15  -18  -33   60  -62  111   22   10  -51  -74  -54
    36    1  -27   16  103  108   59  -37   53   80    9  100   62   78
   112  -94 -100   93  -96   23  -87   19  -86   88  -91 -102   97   20
    32  -59  -88  -98  -11 -106   90   18    5  -41  -67   16    9  -54
   -38  -70 -119 -102  -89  110  -16  -75   88    5    5    8  -36  -55
  -108  -49  -12   31   45 -115  -70 -120  -71   39   43 -124  -44  -27
   -21  -62 -127   77  105  -52  -30   46  -79  -46  -83  -64  -28   91
   -80  116   99   52   99   -9   38  116  -87  -63   64  -93   71   82
  -108   70  -48  111   31 -109   40  -62  -45  -92 -115  -73 -112 -118
    -7   48  -11   27   85   86  -59   11   40  -31   25  -88  -83 -116
   -99  -39  -37   36  -12  112   65   79   60    1  -51   73  -34  -55
    28   53   98 -122   16   85   59  111  -60   40 -125   19  -78  -35
    55   12  -26  -87  -96   85   16 -100  -45   94 -109   -9   18    5
    61   64  -93   17   -5 -124  110  -41   77  -67  -74   69  105  -68
   -28   32  -20  -18  -81  -57   38  -84   14  -53   48 -105  -52   54
   -92   78   40   89   24  -49 -106  -90   33   34 -111   74   -9  -72
   -93  -87   55   49   10   23  -93   31    4  -60  -41   57   18   34
    93  -96    3  -79  -47  -21  114  -36  -13   97   66   72   -7   89
   103   61  -36 -120  -57  111    7   83   38   10   37   17  -96  -12
   -57   48    7  -75    8   31   16   34 -122  -12   62 -118  -71  -85
   -47 -121 -108   34   78  -86   79  -56  -90   60   73  -10  -31   30
    92  -99  -80 -119  -34  -55   63   81   69   19   78   70  -47  113
    16  -97  -61   16   26   66 -103 -110  -31  -43  108  -78   81 -123
    62   61   66   71 -113   34   29   57 -117  -22   93   21  -59 -118
   102  -81   35  -61  -98  -58   74   44   16   38   70  -36  -66  -49
   -34   18   -2  -48 -103   23   23   32  -35  -94  -68   64   24  114
    58   83  -30   22    6  -72 -116   58   44  -32  -31  -89   69   62
   -70   47   48 -106   66   73  117   16  108 -101 -114   97  -75  -15
   -27  -95   39   58   23   44  -74   77   79  -12   -8  -30  -65   76
    49  -98   68  -36  -31  -26 -120  -30   17 -100  -56   89  -43  -49
    59  -99   83   36  -37   10  100  -18 -119  -64  111   42  -96   -6
    30   87   20  -86  -48    5   92   13   96  -31  -36  112 -112 -112
    71  -81 -109 -101  -36   72  -37   54  -89   69  -55  -67  -14    7
    89  113   52  -24  -25  -51  -57  -13  -24  -73   91 -104  -49  -51
   112  110  -60   25  -46  -77  -72   -9]]
[-960]
[[   0 -110   -2   74  -75  127    0    0]]
[[  80 -116    4   33  -75  127    0    0]]
[[-64  -8  50   9   0   0   0   0]]
[[   0 -110   -2   74  -75  127    0    0   96   33   50    9    0    0
     0    0  105   98   47  112  121  116  104  111  110   51   46   56
    47  115  105  116  -80   -2   96    9    0    0    0    0  112   40
    55    9    0    0    0    0    0    0    0    0    0    0    0    0
    98   55   -4   74  -75  127    0    0   80 -116    4   33  -75  127
     0    0    0 -110  124   40   -2  127    0    0    0    0    0    0
     0    0    0    0  -30   27   -5   74  -75  127    0    0  -64   -8
    50    9    0    0    0    0  -64   -8   50    9    0    0    0    0
     0    0    0    0    0    0    0    0  -14   81   -4   74  -75  127
     0    0  -64  -87   58   45  -75  127    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0  -14   27
    -5   74  -75  127    0    0    0  -34  106   42  -75  127    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0  106   55   -4   74  -75  127    0    0  -80   35   19   41
   -75  127    0    0  120  126   -2   74  -75  127    0    0    0    0
     0    0    0    0    0    0  -22   27   -5   74  -75  127    0    0
   -32   73 -109    8    0    0    0    0    2    0    0    0    0    0
     0    0  116  102   46   69  110  113  117  101  117  101   84   80
    85   69  109   98  101  100  100  105  110  103   73  110  116  101
   103  101  114   66   97  116   99  104    0  103  101  115   47  107
  -127    3    0    0    0    0    0    0   48  -86   50    9    0    0
     0    0  -48   54   29    9    0    0    0    0    0    0    0    0
     0    0    0    0   97    3    0    0    0    0    0    0  -48   48
    50    9    0    0    0    0  -32 -100    1  -57  -75  127    0    0
     0    0    0    0    0    0    0    0   65    3    0    0    0    0
     0    0 -128   88   62    9    0    0    0    0    0   87   27    9
     0    0    0    0   16 -120 -109    8    0    0    0    0   88    1
     1   75  -75  127    0    0   80   50  -73    7    0    0    0    0
     2    0    0    0    3    0    0    0  120  126   -2   74  -75  127
     0    0  -32   49  -73    7    0    0    0    0    0 -110   -2   74
   -75  127    0    0    0   50  -73    7    0    0    0    0  109  101
    47   99   99   47   97  110   97   99  111  110  100   97   51   47
   -80   -2   96    9    0    0    0    0  112   40   55    9    0    0
     0    0    0    0    0    0    0    0    0    0   82   55   -4   74
   -75  127    0    0   80 -116    4   33  -75  127    0    0    0 -110
   124   40   -2  127    0    0    0    0    0    0    0    0    0    0
   -30   27   -5   74  -75  127    0    0]]
[[ -64   -8   50    9    0    0    0    0  -64   -8   50    9    0    0
     0    0    0    0    0    0    0    0    0    0  -22   81   -4   74
   -75  127    0    0  -64  -87   58   45  -75  127    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
   -14   27   -5   74  -75  127    0    0    0  -34  106   42  -75  127
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0   90   55   -4   74  -75  127    0    0   16   36
    19   41  -75  127    0    0  120  126   -2   74  -75  127    0    0
     0    0    0    0    0    0    0    0  -22   27   -5   74  -75  127
     0    0   40   74 -109    8    0    0    0    0    9    0    0    0
     0    0    0    0  116  102   46   69  110  113  117  101  117  101
    84   80   85   69  109   98  101  100  100  105  110  103   82   97
   103  103  101  100   84  101  110  115  111  114   66   97  116   99
   104    0  -31    1    0    0    0    0    0    0  -64   36   33    9
     0    0    0    0  -64   17   48    9    0    0    0    0    0    0
     0    0    0    0    0    0 -127    1    0    0    0    0    0    0
  -128   25   50    9    0    0    0    0  -32 -100    1  -57  -75  127
     0    0    0    0    0    0    0    0    0    0   97    1    0    0
     0    0    0    0   96    6   31    9    0    0    0    0  -80  -49
    86    9    0    0    0    0   16 -120 -109    8    0    0    0    0
    80    1    1   75  -75  127    0    0  -16   51  -73    7    0    0
     0    0    2    0    0    0    3    0    0    0  120  126   -2   74
   -75  127    0    0 -128   51  -73    7    0    0    0    0    0 -110
    -2   74  -75  127    0    0  -96   51  -73    7    0    0    0    0
    98   47  112  121  116  104  111  110   51   46   56   47  115  105
   116  101  -80   -2   96    9    0    0    0    0  112   40   55    9
     0    0    0    0    0    0    0    0    0    0    0    0   66   55
    -4   74  -75  127    0    0   80 -116    4   33  -75  127    0    0
     0 -110  124   40   -2  127    0    0    0    0    0    0    0    0
     0    0  -30   27   -5   74  -75  127    0    0  -64   -8   50    9
     0    0    0    0  -64   -8   50    9    0    0    0    0    0    0
     0    0    0    0    0    0  -30   81   -4   74  -75  127    0    0
   -64  -87   58   45  -75  127    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0  -14   27   -5   74
   -75  127    0    0    0  -34  106   42  -75  127    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
    74   55   -4   74  -75  127    0    0]]
[[0]]
[[80]]
```

</details>


#### Debugger
- I used debugger
  - https://www.tensorflow.org/lite/performance/quantization_debugger

![image](https://github.com/tensorflow/tensorflow/assets/15901475/a67571a4-e017-4bcd-a6a0-80fdfbc8a62f)
"
61318,tf-nightly prints WARNING:tensorflow during import tensorflow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0-dev20230706

### Custom code

Yes

### Python version

3.11

### Current behavior?

`import tensorflow` prints warnings which cannot be disabled with `os.environ['TF_CPP_MIN_LOG_LEVEL']`.

### Standalone code to reproduce the issue

```shell
python -m venv tf
source tf/bin/activate
pip install tf-nightly
python
>>> import os
>>> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
>>> import tensorflow
WARNING:tensorflow:From tf/lib/python3.11/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
WARNING:tensorflow:From tf/lib/python3.11/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
```
"
61317,snapshot op wrongly changes data fingerprint ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.12.0-rc1-12-g0db597d0d75

### Custom code

Yes

### OS platform and distribution

Linux 4e51bcd72cb8 5.15.109 (Colab)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When constructing a tabular dataset from a given file, I see the snapshot fingerprint changing with repeated attempts. The pipeline and data don't change tough. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

traffic_volume_csv_gz = tf.keras.utils.get_file(
    'Metro_Interstate_Traffic_Volume.csv.gz', 
    ""https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz"",
    cache_dir='.', cache_subdir='traffic'
)

ds = tf.data.experimental.make_csv_dataset(
    traffic_volume_csv_gz,
    batch_size=256,
    label_name='traffic_volume',
    num_epochs=1,
    compression_type=""GZIP""
)

ds = ds.enumerate()
ds = ds.snapshot('ds.tfsnap')
ds = ds.map(lambda i,x: x).repeat(10)

for i,_ in enumerate(ds):
  pass

print(i)
```


### Relevant log output

In Colab Notebook, several runs of this code generate many snapshots
```console
ls ds.tfsnap
11819476836996993959  2128571372330446365  8272381159243496395
14899783750259964653  3924588669394259065  9335410099383931136
```"
61316,import tensorflow error,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

windows 10

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

not able to import tensorflow

### Standalone code to reproduce the issue

```shell
import tensorflow
```


### Relevant log output

```shell
TypeError                                 Traceback (most recent call last)
Cell In [69], line 1
----> 1 import tensorflow

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\__init__.py:37
     34 import sys as _sys
     35 import typing as _typing
---> 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\__init__.py:42
     37 from tensorflow.python.eager import context
     39 # pylint: enable=wildcard-import
     40 
     41 # Bring in subpackages.
---> 42 from tensorflow.python import data
     43 from tensorflow.python import distribute
     44 # from tensorflow.python import keras

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\__init__.py:21
     15 """"""`tf.data.Dataset` API for input pipelines.
     16 
     17 See [Importing Data](https://tensorflow.org/guide/data) for an overview.
     18 """"""
     20 # pylint: disable=unused-import
---> 21 from tensorflow.python.data import experimental
     22 from tensorflow.python.data.ops.dataset_ops import AUTOTUNE
     23 from tensorflow.python.data.ops.dataset_ops import Dataset

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\experimental\__init__.py:97
     15 """"""Experimental API for building input pipelines.
     16 
     17 This module contains experimental `Dataset` sources and transformations that can
   (...)
     93 @@UNKNOWN_CARDINALITY
     94 """"""
     96 # pylint: disable=unused-import
---> 97 from tensorflow.python.data.experimental import service
     98 from tensorflow.python.data.experimental.ops.batching import dense_to_ragged_batch
     99 from tensorflow.python.data.experimental.ops.batching import dense_to_sparse_batch

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\experimental\service\__init__.py:419
      1 # Copyright 2020 The TensorFlow Authors. All Rights Reserved.
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the ""License"");
   (...)
     13 # limitations under the License.
     14 # ==============================================================================
     15 """"""API for using the tf.data service.
     16 
     17 This module contains:
   (...)
    416   job of ParameterServerStrategy).
    417 """"""
--> 419 from tensorflow.python.data.experimental.ops.data_service_ops import distribute
    420 from tensorflow.python.data.experimental.ops.data_service_ops import from_dataset_id
    421 from tensorflow.python.data.experimental.ops.data_service_ops import register_dataset

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\experimental\ops\data_service_ops.py:22
     20 from tensorflow.core.protobuf import data_service_pb2
     21 from tensorflow.python import tf2
---> 22 from tensorflow.python.data.experimental.ops import compression_ops
     23 from tensorflow.python.data.experimental.service import _pywrap_server_lib
     24 from tensorflow.python.data.experimental.service import _pywrap_utils

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\experimental\ops\compression_ops.py:16
      1 # Copyright 2020 The TensorFlow Authors. All Rights Reserved.
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the ""License"");
   (...)
     13 # limitations under the License.
     14 # ==============================================================================
     15 """"""Ops for compressing and uncompressing dataset elements.""""""
---> 16 from tensorflow.python.data.util import structure
     17 from tensorflow.python.ops import gen_experimental_dataset_ops as ged_ops
     20 def compress(element):

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\util\structure.py:22
     18 import itertools
     20 import wrapt
---> 22 from tensorflow.python.data.util import nest
     23 from tensorflow.python.framework import composite_tensor
     24 from tensorflow.python.framework import ops

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\util\nest.py:34
      1 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the ""License"");
   (...)
     13 # limitations under the License.
     14 # ==============================================================================
     16 """"""## Functions for working with arbitrarily nested sequences of elements.
     17 
     18 NOTE(mrry): This fork of the `tensorflow.python.util.nest` module
   (...)
     31    arrays.
     32 """"""
---> 34 from tensorflow.python.framework import sparse_tensor as _sparse_tensor
     35 from tensorflow.python.util import _pywrap_utils
     36 from tensorflow.python.util import nest

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\sparse_tensor.py:25
     23 from tensorflow.python import tf2
     24 from tensorflow.python.framework import composite_tensor
---> 25 from tensorflow.python.framework import constant_op
     26 from tensorflow.python.framework import dtypes
     27 from tensorflow.python.framework import ops

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\constant_op.py:25
     23 from tensorflow.core.framework import types_pb2
     24 from tensorflow.python.eager import context
---> 25 from tensorflow.python.eager import execute
     26 from tensorflow.python.framework import dtypes
     27 from tensorflow.python.framework import op_callbacks

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\eager\execute.py:21
     19 from tensorflow.python import pywrap_tfe
     20 from tensorflow.python.eager import core
---> 21 from tensorflow.python.framework import dtypes
     22 from tensorflow.python.framework import ops
     23 from tensorflow.python.framework import tensor_shape

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\dtypes.py:37
     34 from tensorflow.core.function import trace_type
     35 from tensorflow.tools.docs import doc_controls
---> 37 _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type()
     38 _np_float8_e4m3fn = _pywrap_float8.TF_float8_e4m3fn_type()
     39 _np_float8_e5m2 = _pywrap_float8.TF_float8_e5m2_type()

TypeError: Unable to convert function return value to a Python type! The signature was
	() -> handle
```
"
61315,ImportError: undefined symbol after install,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

1.7.0

### Custom code

No

### OS platform and distribution

Linux-4.14.0-xilinx-v2018.3-armv7l-with-pynqlinux-v2.6-WFH

### Mobile device

no

### Python version

3.6.5

### Bazel version

0.10.0- (@non-git)

### GCC/compiler version

GCC 7.3.0

### CUDA/cuDNN version

no

### GPU model and memory

_No response_

### Current behavior?

I'm facing with the ImportError - Undefined symbol when trying to import tensorflow after successfully compiling from source and installed Tensorflow 1.7.0 on a 32 bit architecture. 

### Standalone code to reproduce the issue

```shell
> Build configurations for Tensorflow:

build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/usr/local/lib/python3.6/dist-packages""
build --force_python=py3
build --host_force_python=py3
build --python_path=""/usr/bin/python3""
build:gcp --define with_gcp_support=true
build:hdfs --define with_hdfs_support=true
build:s3 --define with_s3_support=true
build:kafka --define with_kafka_support=true
build:xla --define with_xla_support=true
build:gdr --define with_gdr_support=true
build:verbs --define with_verbs_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""0""
build --define grpc_no_ares=true
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK


> After bazel build:

sudo python3 -m pip install /tmp/tensorflow_pkg/tensorflow-1.7.0-cp36-cp36m-linux_armv7l.whl

> Importing in python3 

import tensorflow as tf
```


### Relevant log output

```shell
xilinx@pynq:/tmp/tensorflow_pkg$ python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *  # pylint: disable=redefined-builtin
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
"
61314,TensorFlow 2.13 distributed training fail,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04.3

### Mobile device

Linux Ubuntu 20.04.3

### Python version

3.8.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.7, cuDNN 8.6

### GPU model and memory

3x NVIDIA GeForce RTX 3090

### Current behavior?

When trying to run multiple distributed trainings one after another, one of them fails with an `Collective ops is aborted by: ...` error. 

The reproducer attached to this issue produces the following error:
```
Collective ops is aborted by: Device /job:localhost/replica:0/task:0/device:GPU:1 is joining a group with size2, but that group has size 3 (group_key=1)
The error could be from a previous operation. Restart your program to reset.
	 [[{{node CollectiveReduceV2}}]] [Op:__inference_train_function_5585]
```
When run with TF 2.12 there is no such error.

The original code where I have encountered this problem results in
```
E                                           Collective ops is aborted by: Shape mismatch in the collective instance 100. Op at device /job:localhost/replica:0/task:0/device:GPU:1 expected shape [517169] but another member in the group expected shape [516734]. This is likely due to different input shapes at different members of the collective op.
E                                           The error could be from a previous operation. Restart your program to reset.
E                                           	 [[{{node CollectiveReduceV2}}]] [Op:__inference_train_function_49105]
```
but I wasn't able to reproduce this with a small code snippet.

### Standalone code to reproduce the issue

```shell
import pytest
import tensorflow as tf
import tensorflow_datasets as tfds


@pytest.mark.parametrize(""devices"", [1, 3, 2])
def test_distributed_fit(devices):
    datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
    mnist_train, mnist_test = datasets['train'], datasets['test']

    if devices == 1:
        strategy = tf.distribute.OneDeviceStrategy(""/gpu:0"")
    else:
        strategy = tf.distribute.MirroredStrategy([f""/gpu:{i}"" for i in range(devices)])

    batch_size = 64 * strategy.num_replicas_in_sync
    train_dataset = mnist_test.cache().shuffle(10000).batch(batch_size)

    with strategy.scope():
        model = tf.keras.Sequential([
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(10)
        ])

        model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                      optimizer=tf.keras.optimizers.Adam(),
                      metrics=['accuracy'])

    model.fit(train_dataset, epochs=1)


if __name__ == '__main__':
    test_distributed_fit(1)
    test_distributed_fit(3)
    test_distributed_fit(2)
```


### Relevant log output

```shell
/home/nsavel/venvs/nncf_tf_213/bin/python /home/nsavel/workspace/nncf_tf_213/reproducer.py 
2023-07-18 16:47:21.693862: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-18 16:47:21.722428: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:7630] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-07-18 16:47:21.722456: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-07-18 16:47:21.722481: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-07-18 16:47:21.728124: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 16:47:22.211027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
WARNING:tensorflow:From /home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
2023-07-18 16:47:24.321508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1833] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22292 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:17:00.0, compute capability: 8.6
2023-07-18 16:47:24.322042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1833] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22292 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6
2023-07-18 16:47:24.322425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1833] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22292 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:b3:00.0, compute capability: 8.6
2023-07-18 16:47:24.602273: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:552] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.
2023-07-18 16:47:25.946425: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fcf358b4470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-07-18 16:47:25.946450: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2023-07-18 16:47:25.946455: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2023-07-18 16:47:25.946458: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2023-07-18 16:47:25.950178: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-07-18 16:47:26.074588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:434] Loaded cuDNN version 8600
2023-07-18 16:47:26.171621: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
157/157 [==============================] - 2s 5ms/step - loss: 25.9054 - accuracy: 0.6873
2023-07-18 16:47:27.474184: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:552] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.
2023-07-18 16:47:30.690312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:434] Loaded cuDNN version 8600
2023-07-18 16:47:30.822607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:434] Loaded cuDNN version 8600
53/53 [==============================] - 3s 7ms/step - loss: 43.9234 - accuracy: 0.5655
2023-07-18 16:47:31.372876: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:552] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.
2023-07-18 16:47:32.398894: E tensorflow/core/common_runtime/base_collective_executor.cc:249] BaseCollectiveExecutor::StartAbort INTERNAL: Device /job:localhost/replica:0/task:0/device:GPU:1 is joining a group with size2, but that group has size 3 (group_key=1)
2023-07-18 16:47:32.398950: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7416489994643074752
2023-07-18 16:47:32.399024: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1224112818691547746
2023-07-18 16:47:32.399044: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10338356286700713842
2023-07-18 16:47:32.399063: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6809993284794892577
2023-07-18 16:47:32.399081: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12460047264292639245
2023-07-18 16:47:32.399097: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8051515006773529005
Traceback (most recent call last):
  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)
  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)
  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node CollectiveReduceV2 defined at (most recent call last):
  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1782, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1782, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1376, in train_function
    return step_function(self, iterator)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1782, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1376, in train_function
    return step_function(self, iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1359, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1782, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1376, in train_function
    return step_function(self, iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1359, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/optimizers/utils.py"", line 175, in _all_reduce_sum_fn
    return distribution.extended.batch_reduce_to(

Collective ops is aborted by: Device /job:localhost/replica:0/task:0/device:GPU:1 is joining a group with size2, but that group has size 3 (group_key=1)
The error could be from a previous operation. Restart your program to reset.
	 [[{{node CollectiveReduceV2}}]] [Op:__inference_train_function_5585]

Process finished with exit code 1
```
"
61313,snapshoting failure on dataset made from generator,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.12.0-rc1-12-g0db597d0d75

### Custom code

Yes

### OS platform and distribution

Linux 4e51bcd72cb8 5.15.109 (Colab)

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Simple snapshoting op with sharding fails when applied to a generator-based dataset.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

np.random.seed(1234)

IMG_SHAPE = (224,224,3)

def gen_img(shape=IMG_SHAPE):
  while True:
    img = np.random.randint(0,256,size=IMG_SHAPE)
    lab = np.random.randint(0,10)
    yield (img,lab)

ds = tf.data.Dataset.from_generator(
      gen_img,
      output_signature=(
        tf.TensorSpec(shape=IMG_SHAPE, dtype=tf.int32),
        tf.TensorSpec(shape=(), dtype=tf.int32)
      )
)
ds = ds.take(int(1e3)).batch(32)
ds = ds.enumerate()
ds = ds.snapshot('./my_cached_dataset', shard_func = lambda i,x: i%10)
ds = ds.map(lambda i,x: x).repeat(2) # error disappears under 1 epoch !

for i,(img,lab) in enumerate(ds):
  pass
```


### Relevant log output

```shell
ResourceExhaustedError: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Output buffer(size: 262144 bytes) too small. Should be larger than 19267611 bytes. [Op:IteratorGetNext]
```
"
61312,Linking an Android library with TFLite GPU using CMake causes undefined symbol errors,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Linux 6.3.1, EndeavourOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

clang version 14.0.7

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Linking an Android library with libtensorflow-lite.a using CMake with GPU delegate enabled causes undefined symbol errors

### Standalone code to reproduce the issue

Please find a minimal test case [here](https://github.com/GoldFeniks/tflite_link_issue).


### Relevant log output

```shell
ld: error: undefined symbol: tflite::delegates::BackendAsyncKernelInterface::BackendAsyncKernelInterface()
>>> referenced by delegate.cc:705 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:705)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::CreateAsyncRegistration()::$_3::__invoke(TfLiteContext*, char const*, unsigned long)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> did you mean: tflite::delegates::BackendAsyncKernelInterface::~BackendAsyncKernelInterface()
>>> defined in: tensorflow/tensorflow/lite/libtensorflow-lite.a(delegate.cc.o)

ld: error: undefined symbol: kTfLiteSyncTypeNoSyncObj
>>> referenced by string.h:61 (/opt/android-ndk/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include/bits/fortify/string.h:61)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::CreateAsyncRegistration()::$_3::__invoke(TfLiteContext*, char const*, unsigned long)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by string.h:61 (/opt/android-ndk/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include/bits/fortify/string.h:61)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::CreateAsyncRegistration()::$_3::__invoke(TfLiteContext*, char const*, unsigned long)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a

ld: error: undefined symbol: TfLiteAttributeMapIsBufferAttributeMap
>>> referenced by delegate.cc:1058 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:1058)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::RegisterBuffer(TfLiteOpaqueContext*, TfLiteIoType, TfLiteBackendBuffer const*, TfLiteAttributeMap const*, int)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:908 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:908)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::ReconcileRestrictions(TfLiteOpaqueContext const*, TfLiteOpaqueNode const*, int, TfLiteAttributeMap const*, TfLiteAttributeMap*, TfLiteAttributeMap*) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:909 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:909)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::ReconcileRestrictions(TfLiteOpaqueContext const*, TfLiteOpaqueNode const*, int, TfLiteAttributeMap const*, TfLiteAttributeMap*, TfLiteAttributeMap*) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced 1 more times

ld: error: undefined symbol: tflite::delegates::utils::ReadBufferAttrs(TfLiteAttributeMap const*)
>>> referenced by delegate.cc:1061 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:1061)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::RegisterBuffer(TfLiteOpaqueContext*, TfLiteIoType, TfLiteBackendBuffer const*, TfLiteAttributeMap const*, int)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:925 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:925)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::ReconcileRestrictions(TfLiteOpaqueContext const*, TfLiteOpaqueNode const*, int, TfLiteAttributeMap const*, TfLiteAttributeMap*, TfLiteAttributeMap*) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a

ld: error: undefined symbol: TfLiteBackendBufferGetPtr
>>> referenced by delegate.cc:1087 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:1087)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::RegisterBuffer(TfLiteOpaqueContext*, TfLiteIoType, TfLiteBackendBuffer const*, TfLiteAttributeMap const*, int)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a

ld: error: undefined symbol: AHardwareBuffer_acquire
>>> referenced by delegate.cc:787 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:787)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::RegisterBuffer(TfLiteOpaqueContext*, TfLiteIoType, TfLiteBackendBuffer const*, TfLiteAttributeMap const*, int)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a

ld: error: undefined symbol: AHardwareBuffer_describe
>>> referenced by delegate.cc:803 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:803)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::RegisterBuffer(TfLiteOpaqueContext*, TfLiteIoType, TfLiteBackendBuffer const*, TfLiteAttributeMap const*, int)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:803 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:803)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::EvalImpl(TfLiteContext*, TfLiteNode*, TfLiteExecutionTask*)::$_10::operator()(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::EvalImpl(TfLiteContext*, TfLiteNode*, TfLiteExecutionTask*)::LockedAHWBs*, std::__ndk1::vector<long, std::__ndk1::allocator<long> > const&, absl::lts_20230125::Status (tflite::gpu::InferenceRunner::*)(int, std::__ndk1::variant<std::__ndk1::monostate, tflite::gpu::OpenGlBuffer, tflite::gpu::OpenGlTexture, tflite::gpu::CpuMemory, tflite::gpu::OpenClBuffer, tflite::gpu::OpenClTexture, tflite::gpu::VulkanBuffer, tflite::gpu::VulkanTexture>)) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a

ld: error: undefined symbol: AHardwareBuffer_release
>>> referenced by delegate.cc:795 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:795)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::RegisterBuffer(TfLiteOpaqueContext*, TfLiteIoType, TfLiteBackendBuffer const*, TfLiteAttributeMap const*, int)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:795 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:795)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::RegisterBuffer(TfLiteOpaqueContext*, TfLiteIoType, TfLiteBackendBuffer const*, TfLiteAttributeMap const*, int)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:795 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:795)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::Acquire(AHardwareBuffer*)::'lambda'(AHardwareBuffer*)::__invoke(AHardwareBuffer*)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a

ld: error: undefined symbol: tflite::delegates::utils::WriteBufferAttrs(tflite::delegates::utils::BufferAttributes const&, TfLiteAttributeMap*)
>>> referenced by delegate.cc:927 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:927)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::ReconcileRestrictions(TfLiteOpaqueContext const*, TfLiteOpaqueNode const*, int, TfLiteAttributeMap const*, TfLiteAttributeMap*, TfLiteAttributeMap*) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:927 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:927)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::ReconcileRestrictions(TfLiteOpaqueContext const*, TfLiteOpaqueNode const*, int, TfLiteAttributeMap const*, TfLiteAttributeMap*, TfLiteAttributeMap*) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:927 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:927)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::ReconcileRestrictions(TfLiteOpaqueContext const*, TfLiteOpaqueNode const*, int, TfLiteAttributeMap const*, TfLiteAttributeMap*, TfLiteAttributeMap*) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced 2 more times

ld: error: undefined symbol: TfLiteAttributeMapIsSyncAttributeMap
>>> referenced by delegate.cc:933 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:933)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::ReconcileRestrictions(TfLiteOpaqueContext const*, TfLiteOpaqueNode const*, int, TfLiteAttributeMap const*, TfLiteAttributeMap*, TfLiteAttributeMap*) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:934 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:934)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::ReconcileRestrictions(TfLiteOpaqueContext const*, TfLiteOpaqueNode const*, int, TfLiteAttributeMap const*, TfLiteAttributeMap*, TfLiteAttributeMap*) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:941 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:941)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::ReconcileRestrictions(TfLiteOpaqueContext const*, TfLiteOpaqueNode const*, int, TfLiteAttributeMap const*, TfLiteAttributeMap*, TfLiteAttributeMap*) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced 1 more times

ld: error: undefined symbol: tflite::delegates::utils::ReadSyncAttrs(TfLiteAttributeMap const*)
>>> referenced by delegate.cc:950 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:950)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::ReconcileRestrictions(TfLiteOpaqueContext const*, TfLiteOpaqueNode const*, int, TfLiteAttributeMap const*, TfLiteAttributeMap*, TfLiteAttributeMap*) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:983 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:983)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::SetAttributes(TfLiteOpaqueContext*, TfLiteOpaqueNode*, int, TfLiteAttributeMap const*)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a

ld: error: undefined symbol: tflite::delegates::utils::WriteSyncAttrs(tflite::delegates::utils::SyncAttributes const&, TfLiteAttributeMap*)
>>> referenced by delegate.cc:952 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:952)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::ReconcileRestrictions(TfLiteOpaqueContext const*, TfLiteOpaqueNode const*, int, TfLiteAttributeMap const*, TfLiteAttributeMap*, TfLiteAttributeMap*) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:954 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:954)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::ReconcileRestrictions(TfLiteOpaqueContext const*, TfLiteOpaqueNode const*, int, TfLiteAttributeMap const*, TfLiteAttributeMap*, TfLiteAttributeMap*) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a

ld: error: undefined symbol: TfLiteSynchronizationGetPtr
>>> referenced by delegate.cc:1256 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:1256)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::Eval(TfLiteOpaqueContext*, TfLiteOpaqueNode*, TfLiteExecutionTask*)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a

ld: error: undefined symbol: tflite::delegates::utils::WaitForAllFds(absl::lts_20230125::Span<int const>)
>>> referenced by delegate.cc:1268 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:1268)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::Eval(TfLiteOpaqueContext*, TfLiteOpaqueNode*, TfLiteExecutionTask*)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a

ld: error: undefined symbol: tflite::delegates::utils::ConvertToTfLiteStatus(absl::lts_20230125::Status)
>>> referenced by delegate.cc:1308 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:1308)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::Eval(TfLiteOpaqueContext*, TfLiteOpaqueNode*, TfLiteExecutionTask*)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:1289 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:1289)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::EvalImpl(TfLiteContext*, TfLiteNode*, TfLiteExecutionTask*)::$_10::operator()(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::EvalImpl(TfLiteContext*, TfLiteNode*, TfLiteExecutionTask*)::LockedAHWBs*, std::__ndk1::vector<long, std::__ndk1::allocator<long> > const&, absl::lts_20230125::Status (tflite::gpu::InferenceRunner::*)(int, std::__ndk1::variant<std::__ndk1::monostate, tflite::gpu::OpenGlBuffer, tflite::gpu::OpenGlTexture, tflite::gpu::CpuMemory, tflite::gpu::OpenClBuffer, tflite::gpu::OpenClTexture, tflite::gpu::VulkanBuffer, tflite::gpu::VulkanTexture>)) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a

ld: error: undefined symbol: AHardwareBuffer_unlock
>>> referenced by delegate.cc:1212 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:1212)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::Eval(TfLiteOpaqueContext*, TfLiteOpaqueNode*, TfLiteExecutionTask*)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:1212 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:1212)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::Eval(TfLiteOpaqueContext*, TfLiteOpaqueNode*, TfLiteExecutionTask*)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
>>> referenced by delegate.cc:1212 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:1212)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::EvalImpl(TfLiteContext*, TfLiteNode*, TfLiteExecutionTask*)::LockedAHWBs::~LockedAHWBs()) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a

ld: error: undefined symbol: TfLiteSynchronizationSetPtr
>>> referenced by delegate.cc:1328 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:1328)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::Eval(TfLiteOpaqueContext*, TfLiteOpaqueNode*, TfLiteExecutionTask*)) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a

ld: error: undefined symbol: AHardwareBuffer_lock
>>> referenced by delegate.cc:1185 (tensorflow/tensorflow/lite/delegates/gpu/delegate.cc:1185)
>>>               delegate.cc.o:(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::EvalImpl(TfLiteContext*, TfLiteNode*, TfLiteExecutionTask*)::$_10::operator()(tflite::gpu::(anonymous namespace)::DelegateAsyncKernel::EvalImpl(TfLiteContext*, TfLiteNode*, TfLiteExecutionTask*)::LockedAHWBs*, std::__ndk1::vector<long, std::__ndk1::allocator<long> > const&, absl::lts_20230125::Status (tflite::gpu::InferenceRunner::*)(int, std::__ndk1::variant<std::__ndk1::monostate, tflite::gpu::OpenGlBuffer, tflite::gpu::OpenGlTexture, tflite::gpu::CpuMemory, tflite::gpu::OpenClBuffer, tflite::gpu::OpenClTexture, tflite::gpu::VulkanBuffer, tflite::gpu::VulkanTexture>)) const) in archive tensorflow/tensorflow/lite/libtensorflow-lite.a
```
"
61311,Building TFLite for Android with CMake requires Android 26,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Linux 6.3.1, EndeavourOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

clang version 14.0.7

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Building libtensorflow-lite.a with CMake with GPU delegate enabled requires `AHardwareBuffer_*` functions only available with 26+ Android API level, though minSdkVersion is stated to be 19 for tensorflow-lite-gpu [here](https://www.tensorflow.org/lite/android/development#minimum_android_sdk_versions_for_libraries). Tested on branches r2.13, nighly and master. Branch r2.12 builds without issues.

### Standalone code to reproduce the issue

```shell
cmake -DCMAKE_TOOLCHAIN_FILE=${ANDROID_NDK}/build/cmake/android.toolchain.cmake -DANDROID_PLATFORM=android-19 -DANDROID_ABI=arm64-v8a -DCMAKE_ANDROID_NDK_VERSION=25 -DTFLITE_ENABLE_GPU=ON -DCMAKE_BUILD_TYPE=Release ../tensorflow/lite/
make
```


### Relevant log output

```shell
tensorflow/lite/delegates/gpu/delegate.cc:787:7: error: 'AHardwareBuffer_acquire' is unavailable: introduced in Android 26
      AHardwareBuffer_acquire(ahwb);
      ^
/opt/android-ndk/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include/android/hardware_buffer.h:386:6: note: 'AHardwareBuffer_acquire' has been explicitly marked unavailable here
void AHardwareBuffer_acquire(AHardwareBuffer* _Nonnull buffer) __INTRODUCED_IN(26);
     ^
tensorflow/lite/delegates/gpu/delegate.cc:795:9: error: 'AHardwareBuffer_release' is unavailable: introduced in Android 26
        AHardwareBuffer_release(b);
        ^
/opt/android-ndk/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include/android/hardware_buffer.h:394:6: note: 'AHardwareBuffer_release' has been explicitly marked unavailable here
void AHardwareBuffer_release(AHardwareBuffer* _Nonnull buffer) __INTRODUCED_IN(26);
     ^
tensorflow/lite/delegates/gpu/delegate.cc:803:7: error: 'AHardwareBuffer_describe' is unavailable: introduced in Android 26
      AHardwareBuffer_describe(uptr_ahwb.get(), &desc_ahwb);
      ^
/opt/android-ndk/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include/android/hardware_buffer.h:402:6: note: 'AHardwareBuffer_describe' has been explicitly marked unavailable here
void AHardwareBuffer_describe(const AHardwareBuffer* _Nonnull buffer,
     ^
tensorflow/lite/delegates/gpu/delegate.cc:1185:18: error: 'AHardwareBuffer_lock' is unavailable: introduced in Android 26
          return AHardwareBuffer_lock(buffer, this->usage_, -1 /* fence */,
                 ^
/opt/android-ndk/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include/android/hardware_buffer.h:457:5: note: 'AHardwareBuffer_lock' has been explicitly marked unavailable here
int AHardwareBuffer_lock(AHardwareBuffer* _Nonnull buffer, uint64_t usage, int32_t fence,
    ^
tensorflow/lite/delegates/gpu/delegate.cc:1212:24: error: 'AHardwareBuffer_unlock' is unavailable: introduced in Android 26
                return AHardwareBuffer_unlock(buffer, nullptr /* fence */);
                       ^
/opt/android-ndk/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include/android/hardware_buffer.h:479:5: note: 'AHardwareBuffer_unlock' has been explicitly marked unavailable here
int AHardwareBuffer_unlock(AHardwareBuffer* _Nonnull buffer, int32_t* _Nullable fence)
```
"
61310,xla_cpu_gpu_device: MSVC compile errors,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Windows 10 22H2

### Mobile device

_No response_

### Python version

Anaconda 2023.07-1

### Bazel version

6.2.1

### GCC/compiler version

Visual Studio 2022 (build tools 14.36) + msys2-x86_64-20230718

### CUDA/cuDNN version

CUDA 11.8 + CUDNN 8.6.0 + TensorRT 8.5.3

### GPU model and memory

GTX 750 Ti 2GB

### Current behavior?

This issue required two fixes.

First, `external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py` adds too many unrelated include path to compilation command. Related scripts or header dependencies need to be fixed. Minimum command is just like:
`nvcc -v -c -x=c++ -std=c++17 tensorflow/compiler/jit/xla_cpu_device.cc -I .,bazel-out/x64_windows-opt/bin,external/eigen_archive,
external/com_google_absl,external/com_google_protobuf/src,external/farmhash_archive/src,external/llvm-project/llvm/include,
external/llvm-raw/llvm/include,external/llvm-project/mlir/include,bazel-out/x64_windows-opt/bin/external/llvm-project/mlir/include,
external/tf_runtime/include -o bazel-out/x64_windows-opt/bin/tensorflow/compiler/jit/_objs/xla_cpu_device/xla_cpu_device.obj`

Second, `nvcc` will pass the above command to `cl.exe`. Then MSVC will throw some syntax errors and stop. If I use `clang-cl` provided by Visual Studio, some warnings may appear but the `.obj` compilation is successful. Following linux build migration to clang, I think the compiler path of `msvc_wrapper_for_nvcc.py` can change to `clang-cl` to avoid syntax errors.

### Standalone code to reproduce the issue

```shell
1. download https://github.com/tensorflow/tensorflow/archive/refs/tags/v2.13.0.zip and extract
2. comment out Windows CUDA build rejection code in configure.py
3. run `python configure.py` to configure Windows CUDA build
4. run `bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package`
```


### Relevant log output

```shell
ERROR: E:/tensorflow-2.13.0-createprocessw/tensorflow/compiler/jit/BUILD:113:11: Compiling tensorflow/compiler/jit/xla_cpu_device.cc failed: (Exit -1): python.exe failed: error exe
cuting command (from target //tensorflow/compiler/jit:xla_cpu_device)
  cd /d E:/_bazel_tensorflow/4zvk5ci6/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8
    SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.36.32532\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.3
6.32532\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program F
iles (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621
.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt
    SET LIB=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.36.32532\ATLMFC\lib\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\1
4.36.32532\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.22621.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\\lib\10.0.22621.0\\um\x64
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.36.32532\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\V
C\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Commo
n7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual
Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\b
in\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Fra
mework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32
;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\
CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/tensorflow/anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/Users/tensorflow/anaconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\msys64\tmp
    SET TF2_BEHAVIOR=1
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.0,7.0
    SET TMP=C:\msys64\tmp
  C:\Users\tensorflow\anaconda3\python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_
SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_window
s-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexte
rnal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/com_google_protobuf /Ib
azel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/llvm-project /Ibazel-out/x64_windows-opt/bin/external/llvm-project /Iexternal/llvm_terminfo /Ibazel-out/x64_win
dows-opt/bin/external/llvm_terminfo /Iexternal/llvm_zlib /Ibazel-out/x64_windows-opt/bin/external/llvm_zlib /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/l
ibjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iextern
al/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64
_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/loc
al_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm /Iexternal/local_c
onfig_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/cudnn_frontend_archive /Ibazel-out/x64_windows-opt/bin/external/cudnn_frontend_archive /Iex
ternal/curl /Ibazel-out/x64_windows-opt/bin/external/curl /Iexternal/boringssl /Ibazel-out/x64_windows-opt/bin/external/boringssl /Iexternal/jsoncpp_git /Ibazel-out/x64_windows-opt
/bin/external/jsoncpp_git /Iexternal/com_github_grpc_grpc /Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc /Iexternal/upb /Ibazel-out/x64_windows-opt/bin/external/upb
/Iexternal/mkl_dnn_v1 /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1 /Iexternal/stablehlo /Ibazel-out/x64_windows-opt/bin/external/stablehlo /Iexternal/tf_runtime /Ibazel-out/
x64_windows-opt/bin/external/tf_runtime /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen /Ibazel-out/x64_windows-opt/bi
n/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectBytecodeGen /Ibaze
l-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLoca
tionAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtu
al_includes/BuiltinTypeInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llv
m-project/mlir/_virtual_includes/CallOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen /Ibazel-out/x64_windows-
opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGe
n /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Reg
ionKindInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project
/mlir/_virtual_includes/SymbolInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorEncodingIncGen /Ibazel-out/x64_windows-opt/bin/ext
ernal/llvm-project/mlir/_virtual_includes/ArithBaseIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithCanonicalizationIncGen /Ibazel-out/x64_w
indows-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithOpsInterfacesIncGen /Ib
azel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/InferIntRangeInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/
VectorInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-projec
t/mlir/_virtual_includes/ControlFlowOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/FuncIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-
project/mlir/_virtual_includes/AsmParserTokenKinds /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/QuantOpsIncGen /Ibazel-out/x64_windows-opt/bin/exter
nal/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Mem2RegInterfacesIncGen /Ibazel-out/x64
_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/DialectUtilsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGe
n /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLTypesInc
Gen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Con
versionPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/
_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/external/lo
cal_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen /Ibazel-out/x64_w
indows-opt/bin/external/llvm-project/mlir/_virtual_includes/MLProgramAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MLProgramOpsIncGe
n /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MLProgramTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Run
timeVerifiableOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler
/xla/mlir_hlo/_virtual_includes/mlir_hlo /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/canonicalize_inc_gen /Ibazel-out/x64_windows-opt/bin/ten
sorflow/compiler/xla/mlir_hlo/_virtual_includes/convert_op_folder /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_attrs_inc_gen /Ibazel-o
ut/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_common /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_
enums_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_v
irtual_includes/hlo_ops_pattern_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_typedefs_inc_gen /Ibazel-out/x64_windows-opt/bin/
external/llvm-project/mlir/_virtual_includes/ComplexAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ComplexBaseIncGen /Ibazel-out/x64_
windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ComplexOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMDialectInterfaceIncGe
n /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMIntrinsicOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/L
LVMOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includ
es/CopyOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefBaseIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_v
irtual_includes/MemRefOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapedOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-
project/mlir/_virtual_includes/DestinationStyleOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ValueBoundsOpInterfaceIncGen /Ibazel-o
ut/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Sha
peOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_
virtual_includes/AffineOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ParallelCombiningOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/ext
ernal/llvm-project/mlir/_virtual_includes/TensorOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TilingInterfaceIncGen /Ibazel-out/x64_windows
-opt/bin/external/llvm-project/mlir/_virtual_includes/SparseTensorAttrDefsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SparseTensorOpsIncGen
/Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SparseTensorTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/base /Ibaz
el-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/base_attr_interfaces_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/broadcast_utils /I
bazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/chlo_ops /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/chlo_attrs_inc_gen /Ibazel-out/x64_
windows-opt/bin/external/stablehlo/_virtual_includes/chlo_enums_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/chlo_ops_inc_gen /Ibazel-out/x64_window
s-opt/bin/external/stablehlo/_virtual_includes/stablehlo_assembly_format /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/stablehlo_type_inference /Ibazel-out/x
64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/mhlo_passes /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/chlo_legalize_t
o_hlo /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/chlo_legalize_to_hlo_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_h
lo/_virtual_includes/map_chlo_to_hlo_op /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AllocationOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/ext
ernal/llvm-project/mlir/_virtual_includes/BufferizableOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BufferizationBaseIncGen /Ibazel
-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BufferizationEnumsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Bufferiz
ationOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFDeviceMappingInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/m
lir/_virtual_includes/SCFIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFPassIncGen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/m
lir_hlo/_virtual_includes/hlo_legalize_to_stablehlo /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_stablehlo_to_hlo_op /Ibazel-out/x64_windo
ws-opt/bin/external/stablehlo/_virtual_includes/stablehlo_ops /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/stablehlo_attrs_inc_gen /Ibazel-out/x64_windows-o
pt/bin/external/stablehlo/_virtual_includes/stablehlo_enums_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/stablehlo_ops_inc_gen /Ibazel-out/x64_windo
ws-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/legalize_to_linalg_utils /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_mhlo_t
o_scalar_op /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MathBaseIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes
/MathOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MaskableOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_vi
rtual_includes/MaskingOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-p
roject/mlir/_virtual_includes/LinalgEnumsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/
external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen /Ibazel-out/x64_
windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/legalize_to_standard_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/l
hlo /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_in
cludes/lhlo_ops_structs_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_structured_interface /Ibazel-out/x64_windows-opt/bin/tensorf
low/compiler/xla/mlir_hlo/_virtual_includes/lhlo_structured_interface_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lower_complex_inc_g
en /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_hlo_to_lhlo_op /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_i
ncludes/mhlo_pass_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/mhlo_rng_utils /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/
mlir_hlo/_virtual_includes/mhlo_scatter_gather_utils /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/shape_component_analysis /Ibazel-out/x64_win
dows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/stablehlo_legalize_to_hlo /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo /I
bazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_op
s_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virt
ual_includes/thlo_bufferizable_op_interface /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/type_conversion /Ibazel-out/x64_windows-opt/bin/exter
nal/llvm-project/mlir/_virtual_includes/BufferizationPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/FuncTransformsPassIncGen /Ibazel-out/x6
4_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/unfuse_batch_norm /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AffinePassIncGen
 /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/DLTIBaseI
ncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUBaseIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUOps
IncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Me
mRefPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/NVGPUIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes
/NVGPUPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_i
ncludes/VectorEnumsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_
virtual_includes/X86VectorIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapeTransformsPassIncGen /Ibazel-out/x64_windows-opt/bin/tensorflow/c
ompiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_attrdefs_inc_gen /Ibazel-out/x64_win
dows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_dialect_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo
_gpu_ops_enums_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/x
la/mlir_hlo/_virtual_includes/lhlo_gpu_ops_ops /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/stablehlo_passes /Ibazel-out/x64_windows-opt/bin/external/stable
hlo/_virtual_includes/stablehlo_pass_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/version /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtua
l_includes/vhlo_ops /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/vhlo_attr_interfaces_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_in
cludes/vhlo_attrs_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/vhlo_enums_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includ
es/vhlo_op_interfaces_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/vhlo_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_incl
udes/vhlo_types /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/vhlo_type_interfaces_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includ
es/vhlo_types_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_dialect_registration /Ibazel-out/x64_windows-opt/bin/external/stablehlo
/_virtual_includes/register /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXCodeGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_vi
rtual_includes/NVPTXCommonTableGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXInfo /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm
/_virtual_includes/NVPTXUtilsAndDesc /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AsyncOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-proje
ct/mlir/_virtual_includes/GPUPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/l
lvm-project/mlir/_virtual_includes/LLVMPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/NVVMOpsIncGen /Ibazel-out/x64_windows-opt/bin/externa
l/llvm-project/mlir/_virtual_includes/LLVMIntrinsicConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenMPInterfacesIncGen /Ibazel-out/
x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenMPOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenMPTypeInterfacesIn
cGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ROCDLConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes
/ROCDLOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AMXIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/
ArmNeonIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ArmSVEIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes
/NVVMConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapeToStandardGen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_h
lo/_virtual_includes/transforms_passes /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation /Ibazel-out/x64_windows-opt/bin/tensorflow/co
mpiler/xla/mlir_hlo/_virtual_includes/deallocation_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation_utils /Ibazel-out/x64
_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation_passes /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocat
ion_passes_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_bufferizable_op_interface /Ibazel-out/x64_windows-opt/bin/tensorflow/co
mpiler/xla/mlir_hlo/_virtual_includes/gml_st_passes /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_passes_inc_gen /Ibazel-out/x64_windows
-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_transforms /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/transforms_passes_i
nc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/userange_analysis /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_incl
udes/TransformDialectEnumsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformDialectIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-p
roject/mlir/_virtual_includes/TransformDialectInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformDialectMatchInterfacesIncGen /I
bazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Transform
TypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformDialectTransformsIncGen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/ml
ir_hlo/_virtual_includes/all_passes /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lmhlo_pass_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow
/compiler/xla/mlir_hlo/_virtual_includes/lmhlo_passes /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_lmhlo_to_scalar_op /Ibazel-out/x64_wind
ows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_lhlo_to_hlo_op /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo_passes /Ib
azel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo_passes_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includ
es/transforms_gpu_passes /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gpu_transforms_passes_inc_gen /Ibazel-out/x64_windows-opt/bin/external/l
lvm-project/mlir/_virtual_includes/GPUToNVVMGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AMDGPUIncGen /Ibazel-out/x64_windows-opt/bin/external/l
lvm-project/mlir/_virtual_includes/GPUToROCDLTGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AMXConversionIncGen /Ibazel-out/x64_windows-opt/bin/e
xternal/llvm-project/mlir/_virtual_includes/ArmNeonConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ArmSVEConversionIncGen /Ibazel-out/
x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenACCOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenACCTypeInterfaces
IncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenACCTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/
X86VectorConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_virtual_includes/X86CodeGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_virtua
l_includes/X86CommonTableGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_virtual_includes/X86Info /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_virtua
l_includes/X86UtilsAndDesc /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_virtual_includes/JITLinkTableGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_
virtual_includes/X86DisassemblerInternalHeaders /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAttrUtilsGen /Ibazel-out/x64_windows-opt/bin/exter
nal/llvm-project/mlir/_virtual_includes/SPIRVAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAvailabilityIncGen /Ibazel-out/x64_w
indows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVCanonicalizationIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpsIncGen
/Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVSerializationGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Inde
xEnumsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/IndexOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_include
s/TosaDialectIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TosaInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_vi
rtual_includes/TosaPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SparseTensorPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-proj
ect/mlir/_virtual_includes/AsyncPassIncGen /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_
windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/
bin/external/com_google_protobuf/src /Iexternal/llvm-project/llvm/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/include /Iexternal/llvm-project/mlir/include /I
bazel-out/x64_windows-opt/bin/external/llvm-project/mlir/include /Itensorflow/compiler/mlir/tensorflow/include /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/tensorflow/i
nclude /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/farmhash_archive/src /Ibaz
el-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt
/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_co
nfig_rocm/rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm /Iexternal/local_config_rocm/rocm/rocm/include /Ibazel-out/x64_windows-opt/bin/external/local_config_
rocm/rocm/rocm/include /Iexternal/local_config_rocm/rocm/rocm/include/rocrand /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand /Iexternal/local_
config_rocm/rocm/rocm/include/roctracer /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer /Iexternal/curl/include /Ibazel-out/x64_windows-opt/b
in/external/curl/include /Iexternal/boringssl/src/include /Ibazel-out/x64_windows-opt/bin/external/boringssl/src/include /Iexternal/jsoncpp_git/include /Ibazel-out/x64_windows-opt/
bin/external/jsoncpp_git/include /Iexternal/com_github_grpc_grpc/include /Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc/include /Iexternal/com_github_grpc_grpc/src/c
ore/ext/upb-generated /Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc/src/core/ext/upb-generated /Iexternal/com_github_grpc_grpc/third_party/address_sorting/include /
Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc/third_party/address_sorting/include /Iexternal/mkl_dnn_v1/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/i
nclude /Iexternal/mkl_dnn_v1/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src /Iexternal/mkl_dnn_v1/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/co
mmon /Iexternal/mkl_dnn_v1/src/common/ittnotify /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/common/ittnotify /Iexternal/mkl_dnn_v1/src/cpu /Ibazel-out/x64_windows-opt/b
in/external/mkl_dnn_v1/src/cpu /Iexternal/mkl_dnn_v1/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu/gemm /Iexternal/mkl_dnn_v1/src/cpu/x64/xbyak /Ibazel-o
ut/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak /Itensorflow/compiler/xla/translate/hlo_to_mhlo/include /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/transla
te/hlo_to_mhlo/include /Iexternal/tf_runtime/include /Ibazel-out/x64_windows-opt/bin/external/tf_runtime/include /Iexternal/tf_runtime/third_party/llvm_derived/include /Ibazel-out/
x64_windows-opt/bin/external/tf_runtime/third_party/llvm_derived/include /Iexternal/llvm-project/llvm/lib/Target/NVPTX /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/li
b/Target/NVPTX /Iexternal/llvm-project/llvm/lib/Target/X86 /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/lib/Target/X86 /Iexternal/llvm-project/mlir/lib/Conversion/Fun
cToSPIRV /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conversion/FuncToSPIRV /Iexternal/llvm-project/mlir/lib/Conversion/MathToSPIRV /Ibazel-out/x64_windows-opt/b
in/external/llvm-project/mlir/lib/Conversion/MathToSPIRV /Iexternal/llvm-project/mlir/lib/Conversions/GPUToSPIRV /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conv
ersions/GPUToSPIRV /Iexternal/llvm-project/mlir/lib/Conversion/MemRefToSPIRV /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conversion/MemRefToSPIRV /Iexternal/llvm
-project/mlir/lib/Conversion/TensorToLinalg /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conversion/TensorToLinalg /Iexternal/llvm-project/mlir/lib/Conversion/Ten
sorToSPIRV /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conversion/TensorToSPIRV /Iexternal/llvm-project/mlir/lib/Conversion/TosaToArith /Ibazel-out/x64_windows-o
pt/bin/external/llvm-project/mlir/lib/Conversion/TosaToArith /Iexternal/llvm-project/mlir/lib/Conversion/TosaToLinalg /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib
/Conversion/TosaToLinalg /Iexternal/llvm-project/mlir/lib/Conversion/TosaToSCF /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conversion/TosaToSCF /Iexternal/llvm-p
roject/mlir/lib/Conversion/TosaToTensor /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conversion/TosaToTensor /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /D_CRT_S
ECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLTDL
_SHLIB_EXT="".dll"" /DLLVM_PLUGIN_EXT="".dll"" /DLLVM_NATIVE_ARCH=""X86"" /DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser /DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter /DLLVM_N
ATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler /DLLVM_NATIVE_TARGET=LLVMInitializeX86Target /DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo /DLLVM_NATIVE_TARGETMC=LLVMInitia
lizeX86TargetMC /DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA /DLLVM_HOST_TRIPLE=""x86_64-pc-win32"" /DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64-pc-win32"" /DLLVM_VERSION_MAJOR=17 /DLLV
M_VERSION_MINOR=0 /DLLVM_VERSION_PATCH=0 /DLLVM_VERSION_STRING=""17.0.0git"" /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DBLAKE3_USE_NEON=0 /DBLAKE3_NO_AVX
2 /DBLAKE3_NO_AVX512 /DBLAKE3_NO_SSE2 /DBLAKE3_NO_SSE41 /DTF_USE_SNAPPY /DTF_ENABLE_ACTIVITY_WATCHER /DCURL_STATICLIB /DGRPC_ARES=0 /DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL /DTEN
SORFLOW_USE_MKLDNN_CONTRACTION_KERNEL /DEIGEN_USE_AVX512_GEMM_KERNELS=0 /DGOOGLE_CUDA=1 /DEIGEN_ALTIVEC_USE_CUSTOM_PACK=0 /DEIGEN_NEON_GEBP_NR=4 /DTF_LLVM_X86_AVAILABLE=1 /DBAZEL_C
URRENT_REPOSITORY="""" /showIncludes /O2 /DNDEBUG /W0 /Zc:__cplusplus /D_USE_MATH_DEFINES /d2ReducedOptimizeHugeFunctions -DWIN32_LEAN_AND_MEAN -DNOGDI /Zc:preprocessor /d2ReducedOpt
imizeHugeFunctions /arch:AVX2 /std:c++17 /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/jit/_objs/xla_cpu_device/xla_cpu_device.obj /c tensorflow/compiler/jit/xla_cpu_device.
cc
# Configuration: 65bceb0453d201701ee7f6753c2bb4140d61507260ca636610d54b27f4c27251
# Execution platform: @local_execution_config_platform//:platform
Action failed to execute: java.io.IOException: ERROR: src/main/native/windows/process.cc(165): CreateProcessWithExplicitHandles(""C:\Users\tensorflow\anaconda3\python.exe"" -B extern
al/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SIL
ENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_w
indows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_wi(...)): command is longer than CreateProcessW's limit (32767 characters)
Target //tensorflow/compiler/jit:jit failed to build
INFO: Elapsed time: 768.468s, Critical Path: 94.60s
INFO: 5 processes: 5 internal.
FAILED: Build did NOT complete successfully
```
"
61308,Question about tensorflow <2.11 and protobuf compatibility,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.1

### Custom code

No

### OS platform and distribution

Windows 10

### Mobile device

N/A

### Python version

3.8

### Bazel version

N/A

### GCC/compiler version

N/A

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current behavior?

I'd like to do some work with tensorflow on native Windows with GPU support, so I've installed tensorflow 2.10.1 via pip.  According to the [setup.py](https://github.com/tensorflow/tensorflow/blob/v2.10.1/tensorflow/tools/pip_package/setup.py) for that version, the protobuf dependency is `protobuf >= 3.9.2, < 3.20`.  However, I'm mostly using conda/mamba to handle my package management, and having protobuf limited to <3.20 is limiting my options on a number of other packages, particularly with regard to the upcoming EOL of openssl 1.1.1, so I decided to look into this a bit further.

The comment above this dependency says `Protobuf 3.20 results in linker errors on Windows`.  Is this supposed to be the error described in #53234?  Based on the discussion in this issue, that appears to be the case.  However, if so, I'm not sure why the upper pin of protobuf <3.20 would have been added, as the initial issue described in #53234 appears to have been an attempt to upgrade protobuf to **3.19.0**, not **3.20**.  I tried installing all protobuf v3.20.x versions on top of my tensorflow v2.10.1 install, and I was at least able to import tensorflow, although I realize that this might not be the situation that causes corruption, if that does happen with these versions.

Also, in the comment above this dependency, it states `Protobuf 4.0 is binary incompatible with what C++ TF uses.`  I verified that I would get an error message when I installed any protobuf v4 package via pip after installing tensorflow.  However, if I installed tensorflow (including protobuf 3.19.6) via pip, but then installed protobuf 4.23.3 via conda, I was able to import tensorflow without getting any error.  Do you have any idea why this might be?  I verified that protobuf was reporting as version 4.23.3 in my python interpreter in this situation.

Finally, I have tried contacting several conda-forge package maintainer about the possibility of building some additional packages with support for protobuf <3.20, but I have generally not found people wanting to add that support in.

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

```shell
N/A
```
"
61307,KerasTensor和tf.tensor之间的转换,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.4

### Custom code

Yes

### OS platform and distribution

Window10

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

KerasTensor没有numpy（）这种，应该如何转换

### Standalone code to reproduce the issue

```shell
File ""C:/Users/KM Group/Desktop/lmx/SemanticCompression-Speech/DeepSC-S-main/random_mask_training.py"", line 73, in <module>
    sem_dec = sem_dec_model(frame_length, stride_length, args)
  File ""C:\Users\KM Group\Desktop\lmx\SemanticCompression-Speech\DeepSC-S-main\model_tfnn.py"", line 191, in sem_dec_model
    _output = sem_dec(_intput, batch_mean, batch_var)
  File ""C:\Users\KM Group\Desktop\lmx\SemanticCompression-Speech\DeepSC-S-main\model_tfnn.py"", line 142, in __call__
    _input = tf.convert_to_tensor(keras.backend.get_value(_input))
  File ""C:\Users\KM Group\Anaconda3\envs\speech-SC\lib\site-packages\tensorflow\python\keras\backend.py"", line 3615, in get_value
    return x.numpy()
AttributeError: 'KerasTensor' object has no attribute 'numpy'
```


### Relevant log output

_No response_"
61306,TensorFlow distributed training works for at most 2 GPUs,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8, cuDNN 8.6.0.163

### GPU model and memory

_No response_

### Current behavior?

I have 8 NVIDIA H100s on this system. Below is the output of `nvidia-smi`.
![image](https://github.com/tensorflow/tensorflow/assets/77916424/7d027250-8312-4503-bf98-14b6d2aa0db9)

The following code works as intended if `gpus` includes at most 2 GPUs.
![image](https://github.com/tensorflow/tensorflow/assets/77916424/47f78e02-e92e-47cc-814b-4ddcba433a8b)

In the ""Relevant log output"" section, I have the error log from running the code with 3 GPUs in the `gpus` list.

Please let me know if you need any additional information!

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
gpus = [""/gpu:0"", ""/gpu:1"", ""/gpu:2""]
strategy = tf.distribute.MirroredStrategy(gpus)
with strategy.scope():
    model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
model.compile(loss=""mse"", optimizer=""sgd"")
dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(16)
model.fit(dataset)
```


### Relevant log output

```shell
2023-07-17 23:28:31.469519: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-17 23:28:31.516738: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-17 23:28:31.972313: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-17 23:28:35.178825: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:35.181183: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:35.183528: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:35.185905: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:35.188234: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:35.190559: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:35.192895: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:35.195198: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:36.609484: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:36.610685: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:36.611908: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:36.613166: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:36.614370: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:36.615584: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:36.616799: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:36.617975: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2048] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
2023-07-17 23:28:36.680264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78938 MB memory:  -> device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:19:00.0, compute capability: 9.0
2023-07-17 23:28:36.682112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78938 MB memory:  -> device: 1, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:3b:00.0, compute capability: 9.0
2023-07-17 23:28:36.683772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 78938 MB memory:  -> device: 2, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:4c:00.0, compute capability: 9.0
2023-07-17 23:28:36.685499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 78938 MB memory:  -> device: 3, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:5d:00.0, compute capability: 9.0
2023-07-17 23:28:36.687179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 78938 MB memory:  -> device: 4, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:9b:00.0, compute capability: 9.0
2023-07-17 23:28:36.688884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 78938 MB memory:  -> device: 5, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:bb:00.0, compute capability: 9.0
2023-07-17 23:28:36.690581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 78938 MB memory:  -> device: 6, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:cb:00.0, compute capability: 9.0
2023-07-17 23:28:36.692292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 78938 MB memory:  -> device: 7, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:db:00.0, compute capability: 9.0
2023-07-17 23:28:39.806560: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [1]
         [[{{node Placeholder/_1}}]]
2023-07-17 23:28:39.806723: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [1]
         [[{{node Placeholder/_0}}]]
2023-07-17 23:28:39.807452: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: ""TensorDataset/_2""
op: ""TensorDataset""
input: ""Placeholder/_0""
input: ""Placeholder/_1""
attr {
  key: ""Toutput_types""
  value {
    list {
      type: DT_FLOAT
      type: DT_FLOAT
    }
  }
}
attr {
  key: ""_cardinality""
  value {
    i: 1
  }
}
attr {
  key: ""metadata""
  value {
    s: ""\n\017TensorDataset:0""
  }
}
attr {
  key: ""output_shapes""
  value {
    list {
      shape {
        dim {
          size: 1
        }
      }
      shape {
        dim {
          size: 1
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
    }
  }
}

2023-07-17 23:28:39.823305: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [1]
         [[{{node Placeholder/_1}}]]
2023-07-17 23:28:39.823469: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [1]
         [[{{node Placeholder/_1}}]]
2023-07-17 23:28:40.010671: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [1]
         [[{{node Placeholder/_1}}]]
2023-07-17 23:28:40.010843: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [1]
         [[{{node Placeholder/_0}}]]
2023-07-17 23:28:48.599915: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fe600008ff0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-07-17 23:28:48.599956: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA H100 80GB HBM3, Compute Capability 9.0
2023-07-17 23:28:48.599964: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (1): NVIDIA H100 80GB HBM3, Compute Capability 9.0
2023-07-17 23:28:48.599971: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (2): NVIDIA H100 80GB HBM3, Compute Capability 9.0
2023-07-17 23:28:48.599977: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (3): NVIDIA H100 80GB HBM3, Compute Capability 9.0
2023-07-17 23:28:48.599983: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (4): NVIDIA H100 80GB HBM3, Compute Capability 9.0
2023-07-17 23:28:48.599988: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (5): NVIDIA H100 80GB HBM3, Compute Capability 9.0
2023-07-17 23:28:48.599994: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (6): NVIDIA H100 80GB HBM3, Compute Capability 9.0
2023-07-17 23:28:48.600003: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (7): NVIDIA H100 80GB HBM3, Compute Capability 9.0
2023-07-17 23:28:48.636269: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2023-07-17 23:28:48.636335: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 535.54.3
2023-07-17 23:28:48.649063: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2023-07-17 23:28:48.649131: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 535.54.3
2023-07-17 23:28:48.711026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600
2023-07-17 23:28:48.712282: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr
*** Begin stack trace ***
        tsl::CurrentStackTrace[abi:cxx11]()

        xla::status_macros::MakeErrorStream::Impl::GetStatus()
        xla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)
        xla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)
        xla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)
        xla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)
        xla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)
        tensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)
        tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)
        tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)

        tensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)
        tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)


        Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)
        std::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)



*** End stack trace ***

2023-07-17 23:28:48.712483: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr
2023-07-17 23:28:48.712506: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr
         [[{{node update_0_1/StatefulPartitionedCall}}]]
2023-07-17 23:28:48.727426: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr
*** Begin stack trace ***
        tsl::CurrentStackTrace[abi:cxx11]()

        xla::status_macros::MakeErrorStream::Impl::GetStatus()
        xla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)
        xla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)
        xla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)
        xla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)
        xla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)
        tensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)
        tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)
        tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)

        tensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)
        tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)


        Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)
        std::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)



*** End stack trace ***

2023-07-17 23:28:48.727590: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr
2023-07-17 23:28:48.727609: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:1] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr
         [[{{node update_1_1/StatefulPartitionedCall}}]]
2023-07-17 23:28:48.728128: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2023-07-17 23:28:48.728185: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 535.54.3
2023-07-17 23:28:48.740946: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:231] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 9.0
2023-07-17 23:28:48.740965: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Used ptxas at ptxas
2023-07-17 23:28:48.742906: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2023-07-17 23:28:48.742961: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 535.54.3
2023-07-17 23:28:48.770994: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-07-17 23:28:48.806579: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr
*** Begin stack trace ***
        tsl::CurrentStackTrace[abi:cxx11]()

        xla::status_macros::MakeErrorStream::Impl::GetStatus()
        xla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)
        xla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)
        xla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)
        xla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)
        xla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)
        tensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)
        tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)
        tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)

        tensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)
        tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)


        Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)
        std::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)



*** End stack trace ***

2023-07-17 23:28:48.806809: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr
2023-07-17 23:28:48.818268: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr
*** Begin stack trace ***
        tsl::CurrentStackTrace[abi:cxx11]()

        xla::status_macros::MakeErrorStream::Impl::GetStatus()
        xla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)
        xla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)
        xla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)
        xla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)
        xla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)
        tensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)
        tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)
        tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)

        tensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)
        tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)


        Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)
        std::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)



*** End stack trace ***

2023-07-17 23:28:48.818437: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr
2023-07-17 23:28:48.858099: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:2] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr
         [[{{node update_0_1/StatefulPartitionedCall}}]]
         [[GroupCrossDeviceControlEdges_1/Identity_7/_166]]
Traceback (most recent call last):
  File ""/home/user4/project/src/test.py"", line 8, in <module>
    model.fit(dataset)
  File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node 'update_1_1/StatefulPartitionedCall' defined at (most recent call last):
    File ""/home/user4/project/src/test.py"", line 8, in <module>
      model.fit(dataset)
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py"", line 1685, in fit
      tmp_logs = self.train_function(iterator)
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py"", line 1284, in train_function
      return step_function(self, iterator)
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py"", line 1268, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/optimizer.py"", line 1250, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/optimizer.py"", line 1245, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'update_1_1/StatefulPartitionedCall'
Detected at node 'update_0_1/StatefulPartitionedCall' defined at (most recent call last):
    File ""/home/user4/project/src/test.py"", line 8, in <module>
      model.fit(dataset)
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py"", line 1685, in fit
      tmp_logs = self.train_function(iterator)
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py"", line 1284, in train_function
      return step_function(self, iterator)
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py"", line 1268, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/optimizer.py"", line 1250, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/optimizer.py"", line 1245, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'update_0_1/StatefulPartitionedCall'
Detected at node 'update_0_1/StatefulPartitionedCall' defined at (most recent call last):
    File ""/home/user4/project/src/test.py"", line 8, in <module>
      model.fit(dataset)
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py"", line 1685, in fit
      tmp_logs = self.train_function(iterator)
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py"", line 1284, in train_function
      return step_function(self, iterator)
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py"", line 1268, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/optimizer.py"", line 1250, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File ""/home/user4/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/optimizer.py"", line 1245, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'update_0_1/StatefulPartitionedCall'
3 root error(s) found.
  (0) INTERNAL:  RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr
         [[{{node update_1_1/StatefulPartitionedCall}}]]
  (1) INTERNAL:  RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr
         [[{{node update_0_1/StatefulPartitionedCall}}]]
         [[GroupCrossDeviceControlEdges_1/Identity_7/_166]]
  (2) INTERNAL:  RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr
         [[{{node update_0_1/StatefulPartitionedCall}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_1810]
```
"
61303,Build failure,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.11.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.4.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The built was successful till last Friday, and when I repeat the previously successful built, I ran into error in `Relevant log output
`

### Standalone code to reproduce the issue

```shell
# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

ARG SAXML_BUILD_SOURCE=github

ARG BASE_IMAGE=ubuntu:22.04
FROM ${BASE_IMAGE} AS base-build-image

ARG SAXML_VERSION_GIT_BRANCH=main
ARG SAXML_GIT_COMMIT=HEAD

ARG PYTHON_VERSION=3.10
ARG DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED TRUE

RUN apt-get -qq update && \
    apt-get -qq install -y --no-install-recommends \
        apt-transport-https \
        automake \
        build-essential \
        ca-certificates \
        curl \
        git \
        gnupg \
        libcurl3-dev \
        libfreetype6-dev \
        libpng-dev \
        libtool \
        libzmq3-dev \
        mlocate \
        patch \
        pkg-config \
        software-properties-common \
        sudo \
        swig \
        unzip \
        wget \
        zip \
        zlib1g-dev && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Install python
RUN add-apt-repository ppa:deadsnakes/ppa && \
    apt-get -qq update && \
    apt-get -qq install -y --no-install-recommends \
      python3-pip \
      python${PYTHON_VERSION} \
      python${PYTHON_VERSION}-dev \
      python${PYTHON_VERSION}-venv && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* && \
    python${PYTHON_VERSION} -m pip install -q pip --upgrade && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 0 && \
    update-alternatives --install /usr/bin/python python /usr/bin/python${PYTHON_VERSION} 0


# Install google-cloud-cli
RUN echo ""deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main"" >> /etc/apt/sources.list.d/google-cloud-sdk.list
RUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
RUN apt-get -qq update && \
    apt-get -qq install -y --no-install-recommends \
        google-cloud-cli && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*


# Install bazel
ARG BAZEL_VERSION=5.4.0
RUN mkdir /bazel && \
    cd /bazel && \
    curl -fSsL -O https://github.com/bazelbuild/bazel/releases/download/${BAZEL_VERSION}/bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh && \
    chmod +x bazel-*.sh && \
    ./bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh && \
    cd / && \
    rm -f /bazel/bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh


# Install numpy
RUN pip install --no-cache-dir --no-deps numpy
RUN rm -rf /root/.cache/pip


FROM base-build-image as base-build-from-github
ONBUILD ARG SAXML_GIT_COMMIT
# Download saxml sources (optionally at specific commit)
ONBUILD WORKDIR /saxml
ONBUILD RUN curl -sSL --retry 5 https://github.com/google/saxml/tarball/${SAXML_GIT_COMMIT} | tar --strip-components=1 -xzf -


FROM base-build-image as base-build-from-local
# Copy saxml local repo
ONBUILD COPY local_saxml_repo /saxml
ONBUILD WORKDIR /saxml


FROM base-build-from-${SAXML_BUILD_SOURCE} AS build-source-image
ARG SAXML_BUILD_SOURCE
RUN echo ""Building SAXML from: ${SAXML_BUILD_SOURCE}""


FROM build-source-image AS build-admin-server-image

WORKDIR /saxml

RUN bazel build --color=yes --curses=yes saxml/bin:admin_config && \
    cp bazel-bin/saxml/bin/admin_config_/admin_config /usr/bin/admin_config
```

```
docker build \
  --pull \
  --target runtime-admin-server-image \
  -t sax-admin-server \
  -f Dockerfile . \
  --build-arg=SAXML_BUILD_SOURCE=github \
  --build-arg=SAXML_VERSION_GIT_BRANCH=main \
  --build-arg=SAXML_GIT_COMMIT=HEAD ;
```
```


### Relevant log output

```shell
#18 [build-admin-server-image  2/10] RUN bazel build --color=yes --curses=yes saxml/bin:admin_config &&     cp bazel-bin/saxml/bin/admin_config_/admin_config /usr/bin/admin_config
#18 0.618 Extracting Bazel installation...
#18 2.311 Starting local Bazel server and connecting to it...
DEBUG: Rule 'python3_10_x86_64-unknown-linux-gnu' indicated that a canonical reproducible form can be obtained by dropping arguments [""ignore_root_user_error""]
DEBUG: Repository python3_10_x86_64-unknown-linux-gnu instantiated at:
#18 6.580   /saxml/WORKSPACE:27:27: in <toplevel>
#18 6.580   /root/.cache/bazel/_bazel_root/edfec97661350df226696afb5a35c874/external/rules_python/python/repositories.bzl:366:26: in python_register_toolchains
#18 6.580 Repository rule python_repository defined at:
#18 6.580   /root/.cache/bazel/_bazel_root/edfec97661350df226696afb5a35c874/external/rules_python/python/repositories.bzl:269:36: in <toplevel>
DEBUG: Rule 'org_tensorflow' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""99c732b92b1b37fc243a559e02f9aef5671771e272758aa4aec7f34dc92dac48""
DEBUG: Repository org_tensorflow instantiated at:
#18 28.14   /saxml/WORKSPACE:356:13: in <toplevel>
#18 28.14 Repository rule http_archive defined at:
#18 28.14   /root/.cache/bazel/_bazel_root/edfec97661350df226696afb5a35c874/external/bazel_tools/tools/build_defs/repo/http.bzl:355:31: in <toplevel>
ERROR: Traceback (most recent call last):
#18 28.21       File ""/root/.cache/bazel/_bazel_root/edfec97661350df226696afb5a35c874/external/org_tensorflow/third_party/py/python_configure.bzl"", line 288, column 42, in <toplevel>
#18 28.21               remote_python_configure = repository_rule(
#18 28.21 Error in repository_rule: in call to repository_rule(), parameter 'remotable' is experimental and thus unavailable with the current flags. It may be enabled by setting --experimental_repo_remote_exec
INFO: Repository tf_runtime instantiated at:
#18 28.27   /saxml/WORKSPACE:363:14: in <toplevel>
#18 28.27   /root/.cache/bazel/_bazel_root/edfec97661350df226696afb5a35c874/external/org_tensorflow/tensorflow/workspace3.bzl:18:15: in workspace
#18 28.27   /root/.cache/bazel/_bazel_root/edfec97661350df226696afb5a35c874/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo
#18 28.27   /root/.cache/bazel/_bazel_root/edfec97661350df226696afb5a35c874/external/org_tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
#18 28.27 Repository rule _tf_http_archive defined at:
#18 28.27   /root/.cache/bazel/_bazel_root/edfec97661350df226696afb5a35c874/external/org_tensorflow/third_party/repo.bzl:89:35: in <toplevel>
INFO: Repository io_bazel_rules_closure instantiated at:
#18 28.31   /saxml/WORKSPACE:363:14: in <toplevel>
#18 28.31   /root/.cache/bazel/_bazel_root/edfec97661350df226696afb5a35c874/external/org_tensorflow/tensorflow/workspace3.bzl:8:17: in workspace
#18 28.31 Repository rule http_archive defined at:
#18 28.31   /root/.cache/bazel/_bazel_root/edfec97661350df226696afb5a35c874/external/bazel_tools/tools/build_defs/repo/http.bzl:355:31: in <toplevel>
ERROR: error loading package '': at /root/.cache/bazel/_bazel_root/edfec97661350df226696afb5a35c874/external/org_tensorflow/tensorflow/workspace2.bzl:10:6: initialization of module 'third_party/py/python_configure.bzl' failed
INFO: Elapsed time: 27.731s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
#18 28.39     Fetching @llvm-raw; Restarting.
#18 28.39     Fetching https://storage.googleapis.com/...8ee0471b67a40403df940149.tar.gz
#18 28.39 
#18 ERROR: process ""/bin/sh -c bazel build --color=yes --curses=yes saxml/bin:admin_config &&     cp bazel-bin/saxml/bin/admin_config_/admin_config /usr/bin/admin_config"" did not complete successfully: exit code: 1
------
 > [build-admin-server-image  2/10] RUN bazel build --color=yes --curses=yes saxml/bin:admin_config &&     cp bazel-bin/saxml/bin/admin_config_/admin_config /usr/bin/admin_config:
#18 28.39 

#18 28.31   /root/.cache/bazel/_bazel_root/edfec97661350df226696afb5a35c874/external/bazel_tools/tools/build_defs/repo/http.bzl:355:31: in <toplevel>
ERROR: error loading package '': at /root/.cache/bazel/_bazel_root/edfec97661350df226696afb5a35c874/external/org_tensorflow/tensorflow/workspace2.bzl:10:6: initialization of module 'third_party/py/python_configure.bzl' failed
#18 28.36 Loading: 0 packages loaded
#18 28.36     Fetching @llvm-raw; Restarting.
#18 28.36     Fetching https://storage.googleapis.com/...8ee0471b67a40403df940149.tar.gz
------
Dockerfile:117
--------------------
 116 |     
 117 | >>> RUN bazel build --color=yes --curses=yes saxml/bin:admin_config && \
 118 | >>>     cp bazel-bin/saxml/bin/admin_config_/admin_config /usr/bin/admin_config
 119 |     RUN bazel build --color=yes --curses=yes saxml/bin:admin_server && \
--------------------
ERROR: failed to solve: process ""/bin/sh -c bazel build --color=yes --curses=yes saxml/bin:admin_config &&     cp bazel-bin/saxml/bin/admin_config_/admin_config /usr/bin/admin_config"" did not complete successfully: exit code: 1
```
```
"
61301,Model checkpoint to tflite model,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- TensorFlow installation (pip package or built from source): built from source
- TensorFlow library (version, if pip package or github SHA, if built from source):2.12

I am using tflilte(c++) and doing on device training using signature runner and saving the model.ckpt file..

I want to convert model.ckpt to tflite model for inference..is there any possible method using c++??"
61299,Parse output of `mobile_ssd_v2_float_coco.tflite`,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.11.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

Android

### Python version

_No response_

### Bazel version

6.2.0

### GCC/compiler version

12

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm trying to use the model [mobile_ssd_v2_float_coco.tflite](https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/mobile_ssd_v2_float_coco.tflite) on a C++ application, I'm able to execute the inference and get the results.

Based on the Netron app I see that its output is:
![image](https://github.com/tensorflow/tensorflow/assets/92656601/7ee73cb9-52dc-47ef-a89f-d17843bd0f60)

But I couldn't find an example code showing how to parse this output.

I tried to look into https://github.com/tensorflow/tensorflow/issues/29054 and https://github.com/tensorflow/tensorflow/issues/40298 but the output of the model is different from the one provided [here](https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/mobile_ssd_v2_float_coco.tflite).

Do you have any example code available in Java, Python, or even better in C++ to parse this model output?

### Standalone code to reproduce the issue

```shell
No example code is available to parse the output of mobile_ssd_v2_float_coco.tflite.
```


### Relevant log output

_No response_"
61298,tensorflow keras model.predict() is not thread safe,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

Linux CentOS 7.9

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We executed model.predict() in multi-thread. And sometimes, the code raised the exception: Functional' object has no attribute 'predict_function.

### Standalone code to reproduce the issue

```shell
def predict(self, x, tf_server=False, port=8501, model_path='', step=0):
        if tf_server:
            return self.predict_tf_server_grpc(x, port, step)
        pred = None
        try:
            if not self.model_trained:
                print('try to load model ...\n')
                self.load_model(model_path)
                self.model_trained = True
            if step == 0:
                pred = self.model.predict(x)
            else:
                pred = self.model.predict(x, steps=step)
        except Exception as ex:
            print(ex)
        return pred
```
While the exception raised, we executed the code ""self.model.predict(x)"" in debug window again, and it returned the correct prediction results.
```


### Relevant log output

_No response_"
61297,CTC Loss errors on TPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

Kaggle notebook

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Keras Model with LSTM+ CTC loss is running normally on GPU,
 but on VM TPU it  prints grappler errors. The errors usually don't stop the execution but the loss is nan.
It sometimes also crashes with coredump, but it's not consistent.

I created a public kaggle notebook with the code producing the issue here:
https://www.kaggle.com/code/shaironen/ctc-example/notebook
The grappler errors are:

E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.
2023-07-17 09:06:37.445931: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.
E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.

In addition, I read it's recommended to use model.compile(jit_compile=True) while on GPU to pre-diagnose TPU issues. It gives similar errors and terminates.
(with jit_compile=False it run normally on GPU only).

According to  tf.nn.ctc_loss documentation it should work on tpu.


### Standalone code to reproduce the issue

```shell
https://www.kaggle.com/code/shaironen/ctc-example/notebook
```


### Relevant log output

```shell
WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py:1512: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.

WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py:1512: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.

WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py:1495: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.

WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py:1495: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.
2023-07-17 09:29:45.795306: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.
2023-07-17 09:29:46.168913: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.

 99/100 [============================>.] - ETA: 0s - loss: nan

2023-07-17 09:30:01.881305: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node StatefulPartitionedCall.
2023-07-17 09:30:02.118121: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node StatefulPartitionedCall.
F0717 09:30:03.134464    3526 throw_delegate.cc:121] RAW: absl::container_internal::raw_hash_map<>::at
    @     0x78bd48329338  (unknown)
    @     0x78bd483271e6  (unknown)
    @     0x78bd465e0f20  (unknown)
    @     0x78bd465d6231  (unknown)
    @     0x78bd465d5b59  (unknown)
    @     0x78bd476ce3c9  (unknown)
    @     0x78bd476cf3d7  (unknown)
    @     0x78bd476ccfba  (unknown)
    @     0x78bd40ef00e6  (unknown)
    @     0x78bd465d4e1f  (unknown)
    @     0x78bd465d6919  (unknown)
    @     0x78bd465d662c  (unknown)
    @     0x78bd46345238  (unknown)
    @     0x78bd430f4143  (unknown)
    @     0x78bd46d5f6b4  (unknown)
    @     0x78bd46d5dc50  (unknown)
    @     0x78bd46d5d54e  (unknown)
    @     0x78bd430cee43  (unknown)
    @     0x78bd430de27a  (unknown)
    @     0x78bd430ee05b  (unknown)
    @     0x78bd430ee945  (unknown)
    @     0x78bd424648a3  (unknown)
    @     0x78bd4245e54c  (unknown)
    @     0x78bd41562909  (unknown)
    @     0x78bd41563a59  (unknown)
    @     0x78bd40f32a1c  TpuCompile_CompileAndBuild
    @     0x78bd515e0225  tensorflow::tpu::TpuProgramGroup::CompileAndBuild()
    @     0x78bd51530d6f  tensorflow::tpu::TpuCompileOpKernelImpl::Compile()
    @     0x78bd515e34df  tensorflow::tpu::TpuCompileOpKernelCommon::CompileLocallyAndFillHostCacheInternal()
    @     0x78bd515e3aa1  tensorflow::tpu::TpuCompileOpKernelCommon::CompileLocallyAndFillHostCache()
    @     0x78bd515e3c8b  tensorflow::tpu::TpuCompileOpKernelCommon::ComputeInternal()::{lambda()#3}::operator()()
    @     0x78bd515e3d6c  std::_Function_handler<>::_M_invoke()
    @     0x78bd515cc070  tensorflow::tpu::TpuCompilationCacheExternal::InitializeEntry()
    @     0x78bd51612e72  tensorflow::tpu::TpuCompilationCacheInterface::CompileIfKeyAbsentHelper()
    @     0x78bd5161391a  tensorflow::tpu::TpuCompilationCacheInterface::CompileIfKeyAbsent()
    @     0x78bd515e4323  tensorflow::tpu::TpuCompileOpKernelCommon::ComputeInternal()
    @     0x78bd515e6fc4  tensorflow::tpu::TpuCompileOpKernelCommon::Compute()
    @     0x78bd62de6b6d  tensorflow::ThreadPoolDevice::Compute()
    @     0x78bd62ecfe2c  tensorflow::(anonymous namespace)::ExecutorState<>::Process()
    @     0x78bd62eb9272  std::_Function_handler<>::_M_invoke()
    @     0x78bd621b7275  Eigen::ThreadPoolTempl<>::WorkerLoop()
    @     0x78bd621b41c7  std::_Function_handler<>::_M_invoke()
    @     0x78bd62cefabf  tsl::(anonymous namespace)::PThread::ThreadFn()
    @     0x78be31abeea7  start_thread
https://symbolize.stripped_domain/r/?trace=78bd48329338,78bd483271e5,78bd465e0f1f,78bd465d6230,78bd465d5b58,78bd476ce3c8,78bd476cf3d6,78bd476ccfb9,78bd40ef00e5,78bd465d4e1e,78bd465d6918,78bd465d662b,78bd46345237,78bd430f4142,78bd46d5f6b3,78bd46d5dc4f,78bd46d5d54d,78bd430cee42,78bd430de279,78bd430ee05a,78bd430ee944,78bd424648a2,78bd4245e54b,78bd41562908,78bd41563a58,78bd40f32a1b,78bd515e0224,78bd51530d6e,78bd515e34de,78bd515e3aa0,78bd515e3c8a,78bd515e3d6b,78bd515cc06f,78bd51612e71,78bd51613919,78bd515e4322,78bd515e6fc3,78bd62de6b6c,78bd62ecfe2b,78bd62eb9271,78bd621b7274,78bd621b41c6,78bd62cefabe,78be31abeea6&map=aef7fe2e538f701f46d88df9ee3b51d79ec62b1e:78bd6181e000-78bd637f9728,8f79f803f683427be94b1cfeea32716e6ef365e4:78bd48eb8000-78bd60e0d830,1278088d049ad36cb636fbbc76303cb3:78bd3cc43000-78bd484907c0 
https://symbolize.stripped_domain/r/?trace=78be31b11ce1,78be31b11d5f,78bd48329337,78bd483271e5,78bd465e0f1f,78bd465d6230,78bd465d5b58,78bd476ce3c8,78bd476cf3d6,78bd476ccfb9,78bd40ef00e5,78bd465d4e1e,78bd465d6918,78bd465d662b,78bd46345237,78bd430f4142,78bd46d5f6b3,78bd46d5dc4f,78bd46d5d54d,78bd430cee42,78bd430de279,78bd430ee05a,78bd430ee944,78bd424648a2,78bd4245e54b,78bd41562908,78bd41563a58,78bd40f32a1b,78bd515e0224,78bd51530d6e,78bd515e34de,78bd515e3aa0,78bd515e3c8a&map=8f79f803f683427be94b1cfeea32716e6ef365e4:78bd48eb8000-78bd60e0d830,1278088d049ad36cb636fbbc76303cb3:78bd3cc43000-78bd484907c0 
*** SIGABRT received by PID 2614 (TID 3526) on cpu 51 from PID 2614; ***
E0717 09:30:03.650583    3526 coredump_hook.cc:414] RAW: Remote crash data gathering hook invoked.
E0717 09:30:03.650603    3526 client.cc:278] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec.
E0717 09:30:03.650607    3526 coredump_hook.cc:512] RAW: Sending fingerprint to remote end.
E0717 09:30:03.650615    3526 coredump_socket.cc:120] RAW: Stat failed errno=2 on socket /var/google/services/logmanagerd/remote_coredump.socket
E0717 09:30:03.650619    3526 coredump_hook.cc:518] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running?
E0717 09:30:03.650623    3526 coredump_hook.cc:580] RAW: Dumping core locally.
```
"
61293,Unable to hide TPUs,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12

### Custom code

No

### OS platform and distribution

Kaggle Notebooks

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Unable to hide TPUs from TensorFlow. The consequence of this is that if we want to use JAX along with TensorFlow, only one of them will be able to initialize the TPU system, and the other will fail. We won't be able to use `tfds`, `tf.image` or any TF operation per se if we can't hide TPUs from being used by TF. I want all these operations to run on CPU only, and leverage JAX for TPU. Here is the code to test it on a TPU machine:

```python
import tenorflow as tf

tf.config.set_visible_devices([], device_type=""TPU_SYSTEM"")
tf.config.set_visible_devices([], device_type=""TPU"")

print(tf.config.list_logical_devices())

# output:
# [LogicalDevice(name='/device:CPU:0', device_type='CPU'),
#  LogicalDevice(name='/device:TPU_SYSTEM:0', device_type='TPU_SYSTEM'),
#  LogicalDevice(name='/device:TPU:0', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:1', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:2', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:3', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:4', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:5', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:6', device_type='TPU'),
# LogicalDevice(name='/device:TPU:7', device_type='TPU')]
```

This also doesn't work:

```python
physical_devices = tf.config.list_physical_devices()
tf.config.set_visible_devices(physical_devices[0], 'CPU')
```

### Standalone code to reproduce the issue

```shell
import tenorflow as tf

tf.config.set_visible_devices([], device_type=""TPU_SYSTEM"")
tf.config.set_visible_devices([], device_type=""TPU"")

print(tf.config.list_logical_devices())

# This also doesn't work:
physical_devices = tf.config.list_physical_devices()
tf.config.set_visible_devices(physical_devices[0], 'CPU')



### Relevant log output

_No response_"
61291,Issue still tittle,"### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

Tf2.8

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

What you see what I'm see

### Standalone code to reproduce the issue

```shell
Us see you
```


### Relevant log output

```shell
Productive projects
```
"
61290,Add ComplexOp to TFLite,"**System information**
- MacOS 13.4.1
- binary (via pip)
- 2.13.0

**Provide the text output from tflite_convert**

```python3
loc(callsite(callsite(fused[""Complex:"", callsite(""sequential/lambda/Complex@__inference__wrapped_model_30""(""/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py"":1176:0) at callsite(""/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py"":150:0 at callsite(""/Users/drubinstein/test/complex_test/complex.py"":4:0 at callsite(""/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/core/lambda_layer.py"":212:0 at callsite(""/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py"":96:0 at callsite(""/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/base_layer.py"":1150:0 at callsite(""/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py"":65:0 at callsite(""/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/sequential.py"":420:0 at callsite(""/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py"":96:0 at ""/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/base_layer.py"":1150:0)))))))))] at fused[""PartitionedCall:"", callsite(""PartitionedCall@__inference_signature_wrapper_126""(""/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/saved_model/save.py"":1313:0) at callsite(""/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/saved_model/save.py"":1280:0 at callsite(""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/lite.py"":1427:0 at callsite(""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/convert_phase.py"":205:0 at callsite(""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/lite.py"":1504:0 at callsite(""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/lite.py"":1526:0 at callsite(""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/lite.py"":1042:0 at callsite(""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/lite.py"":1065:0 at ""/Users/drubinstein/test/complex_test/complex.py"":11:0))))))))]) at fused[""PartitionedCall:"", ""PartitionedCall""])): error: 'tf.Complex' op is neither a custom op nor a flex op
error: failed while converting: 'main':
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: Complex
Details:
	tf.Complex(tensor<3xf32>, tensor<3xf32>) -> (tensor<3xcomplex<f32>>) : {device = """"}

Traceback (most recent call last):
  File ""/Users/drubinstein/test/complex_test/complex.py"", line 11, in <module>
    tflite_model = converter.convert()
                   ^^^^^^^^^^^^^^^^^^^
  File ""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/lite.py"", line 1065, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/lite.py"", line 1042, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/lite.py"", line 1526, in convert
    saved_model_convert_result = self._convert_as_saved_model()
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/lite.py"", line 1507, in _convert_as_saved_model
    return super(TFLiteKerasModelConverterV2, self).convert(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/lite.py"", line 1296, in convert
    result = _convert_graphdef(
             ^^^^^^^^^^^^^^^^^^
  File ""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/convert_phase.py"", line 212, in wrapper
    raise converter_error from None  # Re-throws the exception.
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/convert.py"", line 918, in convert_graphdef
    data = convert(
           ^^^^^^^^
  File ""/opt/homebrew/lib/python3.11/site-packages/tensorflow/lite/python/convert.py"", line 367, in convert
    raise converter_error
tensorflow.lite.python.convert_phase.ConverterError: /opt/homebrew/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176:0: error: 'tf.Complex' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""PartitionedCall:"", ""PartitionedCall""]): called from
/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176:0: note: Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: failed while converting: 'main':
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: Complex
Details:
	tf.Complex(tensor<3xf32>, tensor<3xf32>) -> (tensor<3xcomplex<f32>>) : {device = """"}
```

**Standalone code to reproduce the issue** 

```python3
import tensorflow as tf

model = tf.keras.models.Sequential(
    [tf.keras.layers.Lambda(lambda x: tf.dtypes.complex(x[0], x[1]))]
)

print(model([[1.0, 2.0, 3.0], [2.0, 3.0, 4.0]]))

# Convert the model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)
```

**Any other info / logs**

I have [begun implementing ComplexOp in TFLite](https://github.com/tensorflow/tensorflow/compare/master...drubinstein:tensorflow:drubinstein/add-complex?expand=1), but I'm not sure if what I'm doing is correct as I can't find a guide on how to add operators. I have so far done everything I think I needed to to add `Complex` as a builtin in the lite directory, but am a little stuck on what to do with MLIR. I believe I need to:
- add `ComplexOp` as a tfl op
- legalize the `ComplexOp` pattern so TF and TFL can both lower to TOSA IR

 Is this correct? Is there anything I'm missing? How do I test the converter?

Thanks!

"
61289,Build error with Clang 16,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

master branch

### Custom code

No

### OS platform and distribution

Fedora 38

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

Automatically chosen via Bazelisk

### GCC/compiler version

clang version 16.0.5 (Fedora 16.0.5-1.fc38)

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Cannot build `tfcompile` from the sources on the current master with Clang 16.0.5 (from Fedora repos).

### Standalone code to reproduce the issue

```shell
* Do all instructions accordingly to https://www.tensorflow.org/install/source
* Configure without CUDA, rocM, etc but with `usr/bin/clang`
* Run `bazelisk build tensorflow/compiler/aot:tfcompile`
```


### Relevant log output

```shell
During the build, I get the following error:

bazelisk build tensorflow/compiler/aot:tfcompile


INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=183
INFO: Reading rc options for 'build' from /home/zamazan4ik/open_source/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/zamazan4ik/open_source/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/zamazan4ik/open_source/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/zamazan4ik/open_source/tensorflow/release_env/bin/python3 --action_env PYTHON_LIB_PATH=/home/zamazan4ik/open_source/tensorflow/release_env/lib/python3.11/site-packages --python_path=/home/zamazan4ik/open_source/tensorflow/release_env/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/bin/clang-16 --repo_env=CC=/usr/bin/clang-16 --repo_env=BAZEL_COMPILER=/usr/bin/clang-16 --copt=-Wno-gnu-offsetof-extensions
INFO: Reading rc options for 'build' from /home/zamazan4ik/open_source/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/core/tfrt/stubs,tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils
INFO: Found applicable config definition build:short_logs in file /home/zamazan4ik/open_source/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/zamazan4ik/open_source/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/zamazan4ik/open_source/tensorflow/.bazelrc: --define=build_with_onednn_v2=true --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/zamazan4ik/open_source/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /home/zamazan4ik/open_source/tensorflow/tensorflow/tools/toolchains/python/python_repo.bzl:21:14:
TF_PYTHON_VERSION variable was not set correctly, using default version. 3.10 Python
will be used.

To set Python version, run
export TF_PYTHON_VERSION=3.9
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/b971ac5250ea8de900eae9f95e06548d14cd95fe.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/re2/archive/03da4fc0857c285e3a26782f6bc8931c4c950df4.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/41bad512515d609ccd3896d74bf697e7d456e1d3.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/c00d7ca810e93780bd0c8ee4eea28f4f2ea4bcdc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/compiler/aot:tfcompile (332 packages loaded, 24183 targets configured).
INFO: Found 1 target...
ERROR: /home/zamazan4ik/open_source/tensorflow/tensorflow/tsl/lib/io/BUILD:205:11: Compiling tensorflow/tsl/lib/io/cache.cc [for tool] failed: (Exit 1): clang-16 failed: error executing command (from target //tensorflow/tsl/lib/io:cache) /usr/bin/clang-16 -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 ... (remaining 66 arguments skipped)
In file included from tensorflow/tsl/lib/io/cache.cc:16:
./tensorflow/tsl/lib/io/cache.h:99:11: error: unknown type name 'uint64_t'
  virtual uint64_t NewId() = 0;
          ^
tensorflow/tsl/lib/io/cache.cc:391:20: error: only virtual member functions can be marked 'override'
  uint64_t NewId() override {
                   ^~~~~~~~~
2 errors generated.
Target //tensorflow/compiler/aot:tfcompile failed to build
```
```
"
61288,GPU Usage in tensorflow,"### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

Windows 10 Pro 22H2

### Mobile device

_No response_

### Python version

3.10.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

8.6.0.163

### GPU model and memory

Nvidia Geforce GTX 1050TI

### Current behavior?

Use Tensorflow With Gpu and not cpu...

1.- I have followed tutorial from this page https://www.tensorflow.org/install/gpu?hl=es-419
2.- I Have Installed cuda_11.2.0_460.89_win10.exe from https://developer.nvidia.com/cuda-11.2.0-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal
3.- I Have Downloaded cuDNN from https://developer.nvidia.com/rdp/cudnn-archive , the version 8.6.0 (is the version that documentation says tensorflow supports
4.- Later i installed the cuda, and in the path of cuda in my case is C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2 , and i have unzip the cuDNN file un format .zip and copied the 3 folder in my cuda path
5.- I have executed those commands modifyng the version of cuda with my current version:
SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin;%PATH%
SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\extras\CUPTI\lib64;%PATH%
SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\include;%PATH%
SET PATH=C:\tools\cuda\bin;%PATH%
setx PATH ""%PATH%;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\libnvvp""
6.- I have restarted the pc and checked with CMD Admin Mode the next command nvcc --version
and i have received fine the output:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Mon_Nov_30_19:15:10_Pacific_Standard_Time_2020
Cuda compilation tools, release 11.2, V11.2.67
Build cuda_11.2.r11.2/compiler.29373293_0

7.- I Have Tried the tutorial from this page too https://www.codingforentrepreneurs.com/blog/install-tensorflow-gpu-windows-cuda-cudnn/ , but i continue without be abble to run tensorflow in gpu mode , i have tried follow the documentation too have tried 4 times, reinstalling and doing again, but didnt worked.
8.- I dont know if i'm doing something bad or i skipped something, if someone can help me i would appreciate it very much..

### Standalone code to reproduce the issue

```shell
# my code is the next
from tensorflow.python.client import device_lib 
print(device_lib.list_local_devices())

import tensorflow as tf

gpus = tf.config.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        print(""GPU:"", gpu)
else:
    print(""No Gpu for Tensorflow"")

if tf.test.is_built_with_cuda():
    print(""Current Tensorflow version support gpu"")
else:
    print(""Current Version dont support gpu"")
```


### Relevant log output

```shell
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 15862100159162906098
xla_global_id: -1
]
No Gpu for Tensorflow
Current Version dont support gpu
```
"
61287,The return value when LayerNormalization takes zero vectors as input,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.12.0

### Custom code

Yes

### OS platform and distribution

MacOs

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

#### Output

```
tf.Tensor(
[[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]], shape=(3, 4), dtype=float32)
tf.Tensor(
[[nan nan nan nan]
 [nan nan nan nan]
 [nan nan nan nan]], shape=(3, 4), dtype=float32)
tf.Tensor(
[[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]], shape=(3, 4), dtype=float32)
```

#### Document

epsilon is used to avoid division by zero errors, but when LayerNormalization takes a zero vector as input, it returns nan, and BatchNormalization returns 0

（p.s.epsilon is documented as a small floating-point number.In our experiments, we found that epsilon works with larger floating-point numbers.Is this a bit vague?）

### Standalone code to reproduce the issue

```shell
#### Standalone code


import tensorflow as tf
data = tf.zeros(shape=(3, 4))
print(data)

layer1 = tf.keras.layers.LayerNormalization(axis=1, epsilon=0)
output1 = layer1(data)
print(output1)

layer2 = tf.keras.layers.BatchNormalization(axis=1, epsilon=0)
output2 = layer2(data)
print(output2)
```


### Relevant log output

_No response_"
61286,`tf.image.decode_jpeg` can not decode jpeg base64 encoded image,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.1

### Custom code

Yes

### OS platform and distribution

Win11 22H2

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

It raise: `InvalidArgumentError: {{function_node __wrapped__DecodeJpeg_device_/job:localhost/replica:0/task:0/device:CPU:0}} Unknown image file format. One of JPEG, PNG, GIF, BMP required. [Op:DecodeJpeg]`

![image](https://github.com/tensorflow/tensorflow/assets/4510984/342f8602-a2ba-48b7-bdaa-4d0684f9299f)

![image](https://github.com/tensorflow/tensorflow/assets/4510984/6177e9d3-90ed-42c8-881f-cc18bd045f39)


### Standalone code to reproduce the issue

```shell
import base64

from PIL import Image
import tensorflow as tf

img = Image.open('xxx.jpg')
base64str = base64.b64encode(img.tobytes()).decode()
tf.image.decode_jpeg(base64str, channels=3)
```


### Relevant log output

_No response_"
61285,ValueError: No gradients provided for any variable,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.12

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have run a PyTorch code that computes the gradient of the gradient w.r.t some computation. It works just fine. Now, I want to translate PyTorch code into TensorFlow but got some errors.

## Standalone code to reproduce the issue

Here is the reproducible code. [Gist](https://colab.research.google.com/drive/1GPhctZNrXynrCQ0qNbLyMDmuixQtC0fw?usp=sharing).

The above collab is small and quickly reproduces the run of PyTorch and TensorFlow. PyTorch runs as expected but TensorLow doesn't. Below is the main spot to look at:


**Main Part**

In PyTorch, 

```python
rand_model = Rnadom()
model = Model()
ran_optim = torch.optim.SGD(
    ran_model.parameters()
)

model_params = model.parameters()
loss_mod  = model.forward(x)
loss_rand = model.forward(y)

model_grad = torch.autograd.grad(loss_mod, model_params)
rand_grad  = torch.autograd.grad(
    loss_rand, 
    model_params, 
    create_graph=True
)
    
loss = some_method(model_grad, rand_grad)   
rand_model.zero_grad()
loss.backward()
ran_optim.step()
```
In `pytorch`, the above `create_graph=True` is crucial. 

In TensorFlow, I tried 

```python
ran_model = Random()
ran_optim = tf.keras.optimizers.SGD()

model = Model()
model.build(input_shape=(1, 784))
optim = tf.keras.optimizers.SGD(0.01)
model_params = model.trainable_variables

with tf.GradientTape(persistent=True) as tape:
    tape.watch(ran_model.trainable_variables)
    loss_mod = tf.reduce_mean(tf.math.log(model(x)[:, i]))
    loss_rand = tf.reduce_mean(tf.math.log(model(y)[:, i]))
grads_mod = tape.gradient(loss_mod, model_params)
grads_rand = tape.gradient(loss_rand, model_params)

loss = some_method(model_grad, rand_grad)  
ran_model_grads = tape.gradient(loss, ran_model.trainable_variables)
ran_optim.apply_gradients(
  zip(ran_model_grads, ran_model.trainable_variables)
)
```

The `tf` code gives the following error. 

```yaml
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-01562609cda8> in <cell line: 33>()
     44         loss += tf.reduce_sum(tf.stack([a, b], axis=0))
     45     ran_model_grads = tape.gradient(loss, ran_model.trainable_variables)
---> 46     ran_optim.apply_gradients(zip(ran_model_grads, ran_model.trainable_variables))
     47 
     48 

3 frames
/usr/local/lib/python3.10/dist-packages/keras/optimizers/utils.py in filter_empty_gradients(grads_and_vars)
     75     if not filtered:
     76         variable = ([v.name for _, v in grads_and_vars],)
---> 77         raise ValueError(
     78             f""No gradients provided for any variable: {variable}. ""
     79             f""Provided `grads_and_vars` is {grads_and_vars}.""

ValueError: No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(10, 1, 784) dtype=float32, numpy=
```

- This is probably because the `ran_model_grads, ran_model.trainable_variables` are not connected. As mentioned in this [doc](https://www.tensorflow.org/guide/autodiff), 

> When a **target** is not connected to a **source**, the gradient will return `None`

- In PyTorch, `create_graph=True` is used to compute the gradient of the gradient in the later part. To compute [grad-of-grad](https://www.tensorflow.org/guide/advanced_autodiff#example_input_gradient_regularization), but didn't work (shown below). The reason probably is the same as before, source and target are not connected.

```python
for i in range(5):
    
    with tf.GradientTape() as tape1:
        loss_mod = tf.reduce_mean(tf.math.log(model(x)[:, i]))
    grads_mod = tape1.gradient(loss_mod, model_params)
    
    
    with tf.GradientTape() as tape3:
        with tf.GradientTape() as tape2:
            loss_rand = tf.reduce_mean(tf.math.log(model(y)[:, i]))
        grads_rand = tape2.gradient(loss_rand, model_params)

    loss = 0
    for a, b in zip(grads_mod, grads_rand):
        loss += tf.reduce_sum(tf.stack([a, b], axis=0))
    [ISSUE] > ran_model_grads = tape3.gradient(loss, ran_model.trainable_variables)
    ran_optim.apply_gradients(zip(ran_model_grads, ran_model.trainable_variables))
```

But in this case, how to resolve this in TensorFlow?"
61284,`tf.math.reduce_sum`'s `name` property doesn't change in `model.compile`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13

### Custom code

No

### OS platform and distribution

Linux Ubuntu 18.04.6 LTS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

```
...
out = tf.math.reduce_sum(tf.cast(position> 0.5, tf.float32), axis=-1, keepdims=True, name='final')
model = models.Model(inputs=input_, outputs=[position, out])

model.summary()

=============================================================
...
tf.math.reduce_sum (TFOpLambda  (None, 1)           0           ['tf.cast[0][0]']                 )   
==============================================================
```

### Standalone code to reproduce the issue

```shell
input_ = layers.Input(shape=(3))
position = layers.Dense(5, 'sigmoid', name='position')(input_)
out = tf.math.reduce_sum(tf.cast(position> 0.5, tf.float32), axis=-1, keepdims=True, name='final')
model = models.Model(inputs=input_, outputs=[position, out])

model.summary()
```


### Relevant log output

_No response_"
61283,Tensor dimension mismatch when `tf.keras.Input` is used as input,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.14.0-dev20230602

### 2. Code
This is the minimized code to reproduce the issue:
```python
import tensorflow as tf
import numpy as np
input_shape = [1, 2]
x1 = tf.keras.Input(shape=input_shape, dtype=""float32"")

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()
    self.w1 = tf.Variable([[3., 4.], [5., 6.]])
    self.b1 = tf.Variable([7., 8.])
  @tf.function(input_signature=[tf.TensorSpec(x1.shape, x1.dtype)])
  def call(self, x1):
    return tf.matmul(x1, self.w1) + self.b1

m = Model()
converter = tf.lite.TFLiteConverter.from_keras_model(m)
tflite_model = converter.convert()

def _evaluateTFLiteModel(tflite_model, input_data):
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    print(f'Keras input shape: {input_data[0].shape}') # print keras input shape
    print(f'Lite input shape: {input_details[0][""shape""]}') # print lite input shape
    
    for i in range(len(input_data)):
        interpreter.set_tensor(input_details[i]['index'], input_data[i])
    interpreter.invoke()
    output_data = [interpreter.get_tensor(output_details[i]['index'])
                   for i in range(len(output_details))]
    return output_data

x = tf.constant([1., 2.], shape=input_shape)
actual_value = _evaluateTFLiteModel(tflite_model,[x])
```
### 3. Failure after conversion
Output
```
Keras input shape: (1, 2)
Lite input shape: [1 1 2]
```
Error Message:
```
ValueError: Cannot set tensor: Dimension mismatch. Got 2 but expected 3 for input 0.
```
"
61282,AttributeError: module 'tensorflow.saved_model' has no attribute 'builder',"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.10.0-76-gfdfc646704c 2.10.1

### Custom code

Yes

### OS platform and distribution

Win11 22H2

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The docs(https://www.tensorflow.org/tfx/serving/serving_basic#train_and_export_tensorflow_model) show me:

![image](https://github.com/tensorflow/tensorflow/assets/4510984/ace81d65-1ef4-4a27-955c-58a8b9a216b1)

But I can find it in my code:

![image](https://github.com/tensorflow/tensorflow/assets/4510984/91bc769d-36cc-4dfc-9f24-16b60c572b2c)


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.saved_model.builder.SavedModelBuilder
```


### Relevant log output

_No response_"
61281,"`MeanAbsoluteError` returns ""nan"" when given empty tensors","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-96880-g4499c968316 2.14.0-dev20230714

### Custom code

No

### OS platform and distribution

macOS 13.2.1

### Mobile device

v

### Python version

3.11.4

### Bazel version

n/a

### GCC/compiler version

n/a

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

When using `MeanAbsoluteError.update_state(y_true, y_pred)` if `y_true` and `y_pred` happen to be empty tensors of size 0, all subsequent call to   `MeanAbsoluteError.result()` will yield ""nan"" until the internal state of the metric is reset. 

Since calling `MeanAbsoluteError.result()` before any result has been passed returns 0.0, I would expect empty tensors to be ignored when passed to `MeanAbsoluteError.update_state(y_true, y_pred)`.

I've only tested this issue with `MeanAbsoluteError`, but it seems likely that it would happen with other metrics.

### Standalone code to reproduce the issue

```shell
from unittest import TestCase

from tensorflow import constant, float32, reduce_mean
from tensorflow import ragged
from tensorflow.python.keras.metrics import MeanAbsoluteError


class TestEmptyTensorMetrics(TestCase):
    def test_no_data(self):
        metric = MeanAbsoluteError()

        result = metric.result()

        expected = constant(0.0)
        self.assertEqual(expected, result)

    def test_empty_array(self):
        metric = MeanAbsoluteError()
        y_true = constant([])
        y_pred = constant([])

        metric.update_state(y_true, y_pred)
        result = metric.result()

        expected = constant(0.0)
        self.assertEqual(expected, result)

    def test_multiple_batches(self):
        metric = MeanAbsoluteError()
        y_true_batches = constant([
            [39, 22, 73],
            [22, 50, 23]
        ], dtype=float32)
        y_pred_batches = constant([
            [80, 59, 52],
            [87, 8, 38],
        ], dtype=float32)

        for y_true, y_pred in zip(y_true_batches, y_pred_batches):
            metric.update_state(y_true, y_pred)
        result = metric.result()

        expected = reduce_mean(abs(y_true_batches - y_pred_batches))
        self.assertAlmostEqual(expected.numpy(), result.numpy(), 5)

    def test_multiple_batches_with_empty_array(self):
        metric = MeanAbsoluteError()
        y_true_batches = ragged.constant([
            [39, 22, 73],
            [],
            [22, 50, 23]
        ], dtype=float32)
        y_pred_batches = ragged.constant([
            [80, 59, 52],
            [],
            [87, 8, 38],
        ], dtype=float32)

        for y_true, y_pred in zip(y_true_batches, y_pred_batches):
            metric.update_state(y_true, y_pred)
        result = metric.result()

        expected = reduce_mean(abs(y_true_batches - y_pred_batches).flat_values)
        self.assertAlmostEqual(expected.numpy(), result.numpy(), 5)
```
```


### Relevant log output

```shell
2023-07-14 14:54:43.650532: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:From /Users/francois/.local/share/virtualenvs/sandbox-w3BOTv5B/lib/python3.11/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
WARNING:tensorflow:From /Users/francois/.local/share/virtualenvs/sandbox-w3BOTv5B/lib/python3.11/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
test_empty_array (nanbug.TestEmptyTensorMetrics.test_empty_array) ... FAIL
test_multiple_batches (nanbug.TestEmptyTensorMetrics.test_multiple_batches) ... ok
test_multiple_batches_with_empty_array (nanbug.TestEmptyTensorMetrics.test_multiple_batches_with_empty_array) ... FAIL
test_no_data (nanbug.TestEmptyTensorMetrics.test_no_data) ... ok

======================================================================
FAIL: test_empty_array (nanbug.TestEmptyTensorMetrics.test_empty_array)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/francois/Documents/sandbox/nanbug.py"", line 26, in test_empty_array
    self.assertEqual(expected, result)
AssertionError: <tf.Tensor: shape=(), dtype=float32, numpy=0.0> != <tf.Tensor: shape=(), dtype=float32, numpy=nan>

======================================================================
FAIL: test_multiple_batches_with_empty_array (nanbug.TestEmptyTensorMetrics.test_multiple_batches_with_empty_array)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/francois/Documents/sandbox/nanbug.py"", line 64, in test_multiple_batches_with_empty_array
    self.assertAlmostEqual(expected.numpy(), result.numpy(), 5)
AssertionError: 36.833332 != nan within 5 places (nan difference)

----------------------------------------------------------------------
Ran 4 tests in 0.104s

FAILED (failures=2)
```
"
61279,can't save compiled model as .tf,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Mac mini m1

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

m1 gpu and 8GB Ram

### Current behavior?

can't save compile model as a .tf file.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf


class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, d_model, f=0.5, warmup_steps=4_000):
        super().__init__()

        self.d_model = d_model
        self.d_model = tf.cast(self.d_model, tf.float32)

        self.warmup_steps = warmup_steps
        self.f = f

    def __call__(self, step):
        step = tf.cast(step, dtype=tf.float32)
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)

        return self.f * tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

    def get_config(self):
        config = {
            ""d_model"": self.d_model,
            ""warmup_steps"": self.warmup_steps,
            ""f"": self.f
        }
        return config


model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(784,)),
    tf.keras.layers.Dense(10)
])
learning_rate = CustomSchedule(1024)
optimizer = tf.keras.optimizers.legacy.Adam(learning_rate, beta_1=0.9, beta_2=0.98,
                                            epsilon=1e-5)
model.compile(
    optimizer=optimizer,
    loss=""mse"", metrics=['accuracy'])
model.save(""model.tf"")
```


### Relevant log output

```shell
/opt/homebrew/Caskroom/miniconda/base/envs/pythonProject/bin/python /Users/albert/PycharmProjects/pythonProject/AI/test.py 
Traceback (most recent call last):
  File ""/Users/albert/PycharmProjects/pythonProject/AI/test.py"", line 40, in <module>
    model.save(""model.tf"")
  File ""/opt/homebrew/Caskroom/miniconda/base/envs/pythonProject/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/opt/homebrew/Caskroom/miniconda/base/envs/pythonProject/lib/python3.10/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/opt/homebrew/Caskroom/miniconda/base/envs/pythonProject/lib/python3.10/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
TypeError: Unable to serialize 1024.0 to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.
```
"
61275,Files are missing in Windows,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Windows 10 x86_64

### Mobile device

_No response_

### Python version

3.10.8

### Bazel version

_No response_

### GCC/compiler version

gcc (MinGW.org GCC-6.3.0-1) 6.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The file ""tensorflow/c/tf_buffer.h"" is missing.

### Standalone code to reproduce the issue

```shell
#include <stdio.h>
#include <tensorflow/c/c_api.h>

int main() {
  printf(""Hello from TensorFlow C library version %s\n"", TF_Version());
  return 0;
}
```


### Relevant log output

```shell
In file included from main.c:2:0:
include/tensorflow/c/c_api.h:23:36: fatal error: tensorflow/c/tf_buffer.h: No such file or directory
 #include ""tensorflow/c/tf_buffer.h""
                                    ^
```
"
61274,Error installing impoerting TF model into Node red,"Hello, I am getting a tensor flow to install on my node red. It seems like there is and install issue and I keep on getting the error pictured below. I have also tried import an model from teachable machine too and I get the same error please help.
![Capture](https://github.com/tensorflow/tensorflow/assets/139502937/3df553e5-3715-4ffb-a89f-b6fe324c31e0)
![Capture1](https://github.com/tensorflow/tensorflow/assets/139502937/3e101d5f-29ca-4313-bbe6-8904a801443b)

"
61271,TFlite running interpreter->invoke() has failed - Segmentation fault,"In TFLite, I wrote a custom delegate in C++ and encountered an error: ""Segmentation fault"". This error occurs after the initialization is complete and specifically after the invocation of interpreter->invoke(). The custom delegate's prepare function is executed, but the eva function is not executed."
61270,keras.models.load_model broken if custom objects present (.keras format),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf2.13

### Custom code

Yes

### OS platform and distribution

All

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Models do not load.  Errors vary, depending on the number of training samples being divisible by batch size or not.

### Standalone code to reproduce the issue

```shell
import keras
import numpy as np


class MyCustomLSTM(keras.layers.LSTM):
    """"""
    Custom LSTM
    """"""


model = keras.models.Sequential([MyCustomLSTM(32), keras.layers.Dense(1)])

model.compile(keras.optimizers.Adam(), keras.losses.mse)

x = np.zeros((1024, 300, 1))
y = np.zeros((1024, 1))

history = model.fit(x, y, batch_size=32, epochs=1)

model.save('test.keras')

model_loaded = keras.models.load_model('test.keras', custom_objects={'MyCustomLSTM': MyCustomLSTM})
# This throws:
# IndexError: list assignment index out of range

# If the number of samples is not divisible by the batch size (e.g. 1025 samples, instead of 1024), the error becomes:
# ValueError: as_list() is not defined on an unknown TensorShape.
```


### Relevant log output

_No response_"
61269,TfLite 2.13 with -DTFLITE_ENABLE_GPU=ON fails to build with Visual Studio 2019 and 2022,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13

### Custom code

No

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

Visual Studio 2019 and 2022

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Building TfLite 2.13 with cmake and `-DTFLITE_ENABLE_GPU=ON` fails if Visual Studio 2019 or 2022 is used. Tested on two different machines, it fails on both machines.
Steps executed in Windows Command Prompt:
```
git clone --single-branch --branch r2.13 https://github.com/tensorflow/tensorflow tensorflow_src
mkdir tflite_build_x64
cd tflite_build_x64
cmake -DTFLITE_ENABLE_GPU=ON ..\tensorflow_src\tensorflow\lite
cmake --build . -j 8 --config Release
```
The cmake build command yields two errors:
```
C:\Users\bartp\source\TfLite2.13Gpu\tensorflow_src\tensorflow\lite\delegates\gpu\common\selectors\operation_selector.cc(313,20): error C2039: 'any_cast': is not a member of 'std' [C:\Users\bartp\source\TfLite2.13Gpu\tflite_build_x64\tensorflow-lite.vcxproj]
C:\Users\bartp\source\TfLite2.13Gpu\tensorflow_src\tensorflow\lite\delegates\gpu\common\tasks\special\conv_pointwise.cc(129,12): error C2039: 'any_cast': is not a member of 'std' [C:\Users\bartp\source\TfLite2.13Gpu\tflite_build_x64\tensorflow-lite.vcxproj]
```
To fix this, add `#include <any>` to `tensorflow\lite\delegates\gpu\common\selectors\operation_selector.cc` and `tensorflow\lite\delegates\gpu\common\selectors\operation_selector.cc`

### Standalone code to reproduce the issue

```shell
To reproduce the issue: see my earlier writing.
```


### Relevant log output

_No response_"
61266,Segmentation fault  ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tensorflow/tensorflow:latest-gpu-jupyter

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 python train.py --X ../../data/train2_x.npy --Y ../../data/train2_y.npy

### Standalone code to reproduce the issue

```shell
Segmentation fault  
but I use tensorflow:2.12.0  This problem does not occur
```


### Relevant log output

_No response_"
61265,TypeError: _lookup_dependency() takes 2 positional arguments but 3 were given,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0-dev20230712

### Custom code

Yes

### OS platform and distribution

Linux moe 5.10.0-12-amd64 #1 SMP Debian 5.10.103-1 (2022-03-07) x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Can't load saved model

### Standalone code to reproduce the issue

```shell
Saved models can't be loaded:


model = tf.keras.models.Sequential([tf.keras.layers.Input((256,256,3)), tf.keras.layers.Dense(1)])
model.save(""../models/test"")
model = tf.keras.models.load_model(""../models/test/"")
```

same thing with more complex models

```
model = tf.keras.applications.efficientnet.EfficientNetB0()
model.save(""../models/test"")
model = tf.keras.models.load_model(""../models/test/"")
```
```


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[11], line 1
----> 1 model = tf.keras.models.load_model(""../models/test/"")

File ~/.local/lib/python3.9/site-packages/keras/src/saving/saving_api.py:262, in load_model(filepath, custom_objects, compile, safe_mode, **kwargs)
    254     return saving_lib.load_model(
    255         filepath,
    256         custom_objects=custom_objects,
    257         compile=compile,
    258         safe_mode=safe_mode,
    259     )
    261 # Legacy case.
--> 262 return legacy_sm_saving_lib.load_model(
    263     filepath, custom_objects=custom_objects, compile=compile, **kwargs
    264 )

File ~/.local/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/.local/lib/python3.9/site-packages/tensorflow/python/checkpoint/restore.py:606, in _queue_children_for_restoration(checkpoint_position, visit_queue)
    604   continue
    605 child_position = checkpoint_position.create_child_position(child.node_id)
--> 606 local_object = trackable._lookup_dependency(child.local_name,
    607                                             trackable_children)
    608 child_proto = child_position.object_proto
    609 if local_object is None:
    610   # We don't yet have a dependency registered with this name. Save it
    611   # in case we do.

TypeError: _lookup_dependency() takes 2 positional arguments but 3 were given
```
"
61264,TensorFlow-io package not possible on computer,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

newest

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.9 and 3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have an Apple Mac Pro with an M1 chip and have been trying to get the example code from your website to compile on both my terminal and pycharm. I have installed the packed for tf metal and for macOS but when I attempt to install the TensorFlow-io it doesn't work at all. After research online I saw this is an issue for others as well. Is there a way that the code at this link (https://www.tensorflow.org/tutorials/audio/transfer_learning_audio ) would be able to be done without TensorFlow-io. In other words, is there an alternative so that it works the same? Or is there a solution to the package installation problem?

### Standalone code to reproduce the issue

```shell
import os

from IPython import display
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_io as tfio

yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'
yamnet_model = hub.load(yamnet_model_handle)

testing_wav_file_name = tf.keras.utils.get_file('miaow_16k.wav',
                                                'https://storage.googleapis.com/audioset/miaow_16k.wav',
                                                cache_dir='./',
                                                cache_subdir='test_data')

print(testing_wav_file_name)

# Utility functions for loading audio files and making sure the sample rate is correct.

@tf.function
def load_wav_16k_mono(filename):
    """""" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. """"""
    file_contents = tf.io.read_file(filename)
    wav, sample_rate = tf.audio.decode_wav(
          file_contents,
          desired_channels=1)
    wav = tf.squeeze(wav, axis=-1)
    sample_rate = tf.cast(sample_rate, dtype=tf.int64)
    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)
    return wav

testing_wav_data = load_wav_16k_mono(testing_wav_file_name)

_ = plt.plot(testing_wav_data)

# Play the audio file.
display.Audio(testing_wav_data, rate=16000)

class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')
class_names = list(pd.read_csv(class_map_path)['display_name'])

for name in class_names[:20]:
    print(name)
print('...')

scores, embeddings, spectrogram = yamnet_model(testing_wav_data)
class_scores = tf.reduce_mean(scores, axis=0)
top_class = tf.math.argmax(class_scores)
inferred_class = class_names[top_class]

print(f'The main sound is: {inferred_class}')
print(f'The embeddings shape: {embeddings.shape}')

_ = tf.keras.utils.get_file('esc-50.zip',
                            'https://github.com/karoldvl/ESC-50/archive/master.zip',
                            cache_dir='./',
                            cache_subdir='datasets',
                            extract=True)

esc50_csv = './datasets/ESC-50-master/meta/esc50.csv'
base_data_path = './datasets/ESC-50-master/audio/'

pd_data = pd.read_csv(esc50_csv)
pd_data.head()

my_classes = ['dog', 'cat']
map_class_to_id = {'dog': 0, 'cat': 1}

filtered_pd = pd_data[pd_data.category.isin(my_classes)]

class_id = filtered_pd['category'].apply(lambda name: map_class_to_id[name])
filtered_pd = filtered_pd.assign(target=class_id)

full_path = filtered_pd['filename'].apply(lambda row: os.path.join(base_data_path, row))
filtered_pd = filtered_pd.assign(filename=full_path)

filtered_pd.head(10)

filenames = filtered_pd['filename']
targets = filtered_pd['target']
folds = filtered_pd['fold']

main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))
main_ds.element_spec

def load_wav_for_map(filename, label, fold):
    return load_wav_16k_mono(filename), label, fold

main_ds = main_ds.map(load_wav_for_map)
main_ds.element_spec

# applies the embedding extraction model to a wav data
def extract_embedding(wav_data, label, fold):
 # run YAMNet to extract embedding from the wav data
    scores, embeddings, spectrogram = yamnet_model(wav_data)
    num_embeddings = tf.shape(embeddings)[0]
    return (embeddings,
            tf.repeat(label, num_embeddings),
            tf.repeat(fold, num_embeddings))

# extract embedding
main_ds = main_ds.map(extract_embedding).unbatch()
main_ds.element_spec

cached_ds = main_ds.cache()
train_ds = cached_ds.filter(lambda embedding, label, fold: fold < 4)
val_ds = cached_ds.filter(lambda embedding, label, fold: fold == 4)
test_ds = cached_ds.filter(lambda embedding, label, fold: fold == 5)

# remove the folds column now that it's not needed anymore
remove_fold_column = lambda embedding, label, fold: (embedding, label)

train_ds = train_ds.map(remove_fold_column)
val_ds = val_ds.map(remove_fold_column)
test_ds = test_ds.map(remove_fold_column)

train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)
test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)

my_model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=1024, dtype=tf.float32,
                          name='input_embedding'),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(len(my_classes))
], name='my_model')

my_model.summary()

my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                 optimizer=""adam"",
                 metrics=['accuracy'])

callback = tf.keras.callbacks.EarlyStopping(monitor='loss',
                                            patience=3,
                                            restore_best_weights=True)

history = my_model.fit(train_ds,
                       epochs=20,
                       validation_data=val_ds,
                       callbacks=callback)

loss, accuracy = my_model.evaluate(test_ds)

print(""Loss: "", loss)
print(""Accuracy: "", accuracy)

scores, embeddings, spectrogram = yamnet_model(testing_wav_data)
result = my_model(embeddings).numpy()

inferred_class = my_classes[result.mean(axis=0).argmax()]
print(f'The main sound is: {inferred_class}')
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/Users/oliviafranken/Documents/birds/yamnet.py"", line 10, in <module>
    import tensorflow_io as tfio
ModuleNotFoundError: No module named 'tensorflow_io'
```
"
61260,The required input dimension and type changed after conversion of SAC algorithm,"### 1. System information

- OS Platform and Distribution : Linux Ubuntu 22.04
- TensorFlow installation : Pip Package
- TensorFlow library: Tensorflow 2.12.1

### 2. Code
most of them I followed the SAC Minitaur tutorial here https://www.tensorflow.org/agents/tutorials/7_SAC_minitaur_tutorial but with some modification to fit with my environment, also with additional function to convert to TFLite 
https://colab.research.google.com/drive/1Eea5Fi861_h3TzH1eGUaP-H9dSPBn3OJ?usp=sharing

### 3. Failure after conversion
The Observation Spec reduced and changed from shape (5,), float32 to shape[1], int32 in this conversion

> Observation Spec:
BoundedArraySpec(shape=(5,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0)
Reward Spec:
ArraySpec(shape=(), dtype=dtype('float32'), name='reward')
Action Spec:
BoundedArraySpec(shape=(1,), dtype=dtype('float32'), name='action', minimum=-0.6000000238418579, maximum=0.6000000238418579)
Time step:
TimeStep(
{'discount': array(1., dtype=float32),
 'observation': array([0.09387773, 0.45744053, 0.8837498 , 0.84389937, 0.5       ],
      dtype=float32),
 'reward': array(0., dtype=float32),
 'step_type': array(0, dtype=int32)}) 
step = 0: AverageReturn = 0.000758, AverageEpisodeLength = 120.449997
step = 100: loss = -0.29556670784950256
step = 200: loss = -0.42344561219215393
step = 300: loss = -0.4886786639690399
step = 400: loss = -0.5600587129592896
step = 500: loss = -0.6421032547950745
step = 600: loss = -0.7184216380119324
step = 700: loss = -0.785315752029419
step = 800: loss = -0.8600273728370667
step = 900: loss = -0.9031595587730408
step = 1000: AverageReturn = 1.299191, AverageEpisodeLength = 115.500000
step = 1000: loss = -0.990966796875  
Policy Saved at  /home/erde/minipads/sim/experiment/RL_Ctler/tf_exp/temp_sac/policy
#################### Convert to TFlite
Input shape: {'arg_0_discount': TensorSpec(shape=(None,), dtype=tf.float32, name='0/discount'), 'arg_0_observation': TensorSpec(shape=(None, 5), dtype=tf.float32, name='0/observation'), 'arg_0_reward': TensorSpec(shape=(None,), dtype=tf.float32, name='0/reward'), 'arg_0_step_type': TensorSpec(shape=(None,), dtype=tf.int32, name='0/step_type')}
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
Input details: 
 [{'name': 'action_0_step_type:0', 'index': 0, 'shape': array([1], dtype=int32), 'shape_signature': array([-1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'action_0_discount:0', 'index': 1, 'shape': array([1], dtype=int32), 'shape_signature': array([-1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'action_0_observation:0', 'index': 2, 'shape': array([1, 5], dtype=int32), 'shape_signature': array([-1,  5], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'action_0_reward:0', 'index': 3, 'shape': array([1], dtype=int32), 'shape_signature': array([-1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
Output details: 
 [{'name': 'StatefulPartitionedCall:0', 'index': 92, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
Input data: 
 [0]
Input shape: [1]
Output data: 
 [[-0.6]]

- The converted model has less dimension input and different input type required
"
61257,The legal value range of the alpha parameter in LeakyReLU,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.12.0

### Custom code

Yes

### OS platform and distribution

MacOs

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The document describes the LeakyReLU function as having an alpha parameter whose type is a floating point number and the value is equal to or greater than 0. But we found that the value of this parameter is less than zero can also work.

#### Document

| `alpha` | Float >= `0.`. Negative slope coefficient. Defaults to `0.3`. |
| ------- | ------------------------------------------------------------ |


### Standalone code to reproduce the issue

```shell
from tensorflow import keras


def bilstm(num_units=25, input_shape=10):
# bilstm input layer
    input_tensor = keras.Input(shape=input_shape)
# bilstm hidden layer
    x = keras.layers.Embedding(input_dim=100, output_dim=10, input_length=8, embeddings_initializer=""uniform"")(input_tensor)
    x = keras.layers.LeakyReLU(alpha=-0.2044550861511304)(x)
# bilstm output layer
    output_tensor = x
    model = keras.models.Model(inputs=input_tensor, outputs=output_tensor)
    return model


if __name__ == ""__main__"":
    bilstm().summary()
```


### Relevant log output

```shell
Model: ""model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 10)]              0         
                                                                 
 embedding (Embedding)       (None, 10, 10)            1000      
                                                                 
 leaky_re_lu (LeakyReLU)     (None, 10, 10)            0         
                                                                 
=================================================================
Total params: 1,000
Trainable params: 1,000
Non-trainable params: 0
```
```
"
61256,The value range of dropout parameters and recurrent_dropout parameters of GRU/LSTM/SimpleRNN functions,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.12.0

### Custom code

Yes

### OS platform and distribution

MacOs

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The document describes the dropout parameters of the GRU/LSTM/SimpleRNN function and the recurrent_dropout parameters are floating-point numbers with values ranging from 0 to 1, but the program can still run normally if the number is not between 0 and 1.
#### Document

| `dropout`           | Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. Default: 0. |
| ------------------- | ------------------------------------------------------------ |
| `recurrent_dropout` | Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. Default: 0. |

### Standalone code to reproduce the issue

```shell
from tensorflow import keras


def bilstm(num_units=25, input_shape=10):
# bilstm input layer
    input_tensor = keras.Input(shape=input_shape)
# bilstm hidden layer
    x = keras.layers.Embedding(input_dim=985, output_dim=10, input_length=8)(input_tensor)
    x = keras.layers.AlphaDropout(rate=0.1, noise_shape=None)(x)
    y = keras.layers.LSTM(units=num_units, return_sequences=False, recurrent_dropout=0.1)
    x = keras.layers.GRU(units=21, stateful=False, dropout=-0.6477691805726197, recurrent_dropout=-0.6477691805726197)(x)
# bilstm output layer
    output_tensor = x
    model = keras.models.Model(inputs=input_tensor, outputs=output_tensor)
    return model


if __name__ == ""__main__"":
    bilstm().summary()
```


### Relevant log output

```shell
Model: ""model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 10)]              0         
                                                                 
 embedding (Embedding)       (None, 10, 10)            9850      
                                                                 
 alpha_dropout (AlphaDropout  (None, 10, 10)           0         
 )                                                               
                                                                 
 gru (GRU)                   (None, 21)                2079      
                                                                 
=================================================================
Total params: 11,929
Trainable params: 11,929
Non-trainable params: 0
```
```
"
61255,Crooping2D/3D does not have exception handling for the Crooping parameter,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.12.0

### Custom code

Yes

### OS platform and distribution

MacOs

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the value type of **Cropping** in **Cropping2D** is int, tf doesn't check whether **Cropping**  is positive or negative. This is also true for Cropping3D but not Cropping1D. 

### Standalone code to reproduce the issue

```shell
from tensorflow import keras


def squeezenet(label_num=1000, input_shape=(224, 224, 3)):
# squeezenet input layer
    input_tensor = keras.Input(shape=input_shape)
# squeezenet hidden layer
    x = keras.layers.Conv2D(filters=96, activation=""relu"", kernel_size=(7,7), strides=2, padding=""valid"")(input_tensor)
    x = keras.layers.Cropping2D(cropping=-100)(x)
# squeezenet output layer
    output_tensor = keras.layers.Flatten()(keras.layers.Dense(units=label_num, activation=""softmax"")(x))
    model = keras.models.Model(inputs=input_tensor, outputs=output_tensor)
    return model


if __name__ == ""__main__"":
    squeezenet().summary()
```


### Relevant log output

```shell
Model: ""model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 224, 224, 3)]     0         
                                                                 
 conv2d (Conv2D)             (None, 109, 109, 96)      14208     
                                                                 
 cropping2d (Cropping2D)     (None, 91, 91, 96)        0         
                                                                 
 dense (Dense)               (None, 91, 91, 1000)      97000     
                                                                 
 flatten (Flatten)           (None, 8281000)           0         
                                                                 
=================================================================
Total params: 111,208
Trainable params: 111,208
Non-trainable params: 0
```
```
"
61254,Documentation Bug about API ActivityRegularization,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.12.0

### Custom code

Yes

### OS platform and distribution

MacOs

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

#### Document

| `l1` | L1 regularization factor (positive float). |
| ---- | ------------------------------------------ |
| `l2` | L2 regularization factor (positive float). |

The l1 and l2 parameters of the ActivityRegularization function are described in this document as floating point numbers and their values should be positive. But we found that they can run with values less than zero

### Standalone code to reproduce the issue

```shell
from tensorflow import keras


def gru(num_units=25, input_shape=10):
# gru input layer
    input_tensor = keras.Input(shape=input_shape)
# gru hidden layer
    x = keras.layers.Embedding(input_dim=100, output_dim=10, input_length=None)(input_tensor)
    x = keras.layers.GRU(units=32, dropout=0.7338014982069313, return_sequences=True)(x)
    x = keras.layers.ActivityRegularization(l1=-0.616784030867379, l2=-0.9646777799675004)(x)
# gru output layer
    output_tensor = keras.layers.Flatten()(keras.layers.Dense(units=num_units, activation=""relu"")(x))
    model = keras.models.Model(inputs=input_tensor, outputs=output_tensor)
    return model


if __name__ == ""__main__"":
    gru().summary()
```


### Relevant log output

```shell
Model: ""model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 10)]              0         
                                                                 
 embedding (Embedding)       (None, 10, 10)            1000      
                                                                 
 gru (GRU)                   (None, 10, 32)            4224      
                                                                 
 activity_regularization (Ac  (None, 10, 32)           0         
 tivityRegularization)                                           
                                                                 
 dense (Dense)               (None, 10, 25)            825       
                                                                 
 flatten (Flatten)           (None, 250)               0         
                                                                 
=================================================================
Total params: 6,049
Trainable params: 6,049
Non-trainable params: 0
```
```
"
61252,Undefined symbol libtensorflow_cc,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.3.0

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Bazel build of libtensorflow_cc & install_headers for C++ interface returns runtime error:  'undefined symbol: _ZTIN3tsl2io20InputStreamInterfaceE'.

The problem was not seen with 2.8.0 when the same steps are performed.

We link against the libtensorflow_cc.so* & libtensorflow_framework.so*.



### Standalone code to reproduce the issue

```shell
bazel build --config=monolithic --config=avx2_linux --config=opt //tensorflow:libtensorflow_cc.so //tensorflow:install_headers
```


### Relevant log output

_No response_"
61251,Tensorflow SavedModel graph is lexicographically instead of numerically ordered,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.11

### Custom code

Yes

### OS platform and distribution

Ubuntu (not sure version)

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/compiler version

9

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have a SavedModel, and when I used saved_model_cli show, I see this:

```
signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['args_0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 19, 19, 13)
        name: serving_default_args_0:0
    inputs['args_1'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 7)
        name: serving_default_args_1:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['output_1'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 362)
        name: StatefulPartitionedCall:0
    outputs['output_10'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1)
        name: StatefulPartitionedCall:1
    outputs['output_11'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1)
        name: StatefulPartitionedCall:2
    outputs['output_12'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1)
        name: StatefulPartitionedCall:3
    outputs['output_2'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 362)
        name: StatefulPartitionedCall:4
    outputs['output_3'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 2)
        name: StatefulPartitionedCall:5
    outputs['output_4'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 2)
        name: StatefulPartitionedCall:6
    outputs['output_5'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 19, 19, 1)
        name: StatefulPartitionedCall:7
    outputs['output_6'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 800)
        name: StatefulPartitionedCall:8
    outputs['output_7'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 800)
        name: StatefulPartitionedCall:9
    outputs['output_8'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: StatefulPartitionedCall:10
    outputs['output_9'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 362)
        name: StatefulPartitionedCall:11
```

In this case, 'output_10' appears before 'output_9', and the order of the 'StatefulPartitionedCall' nodes get messed up.

I think that the outputs should be numerically ordered, either by sorting internally, or by padding the output index to a fixed with (i.e. {%03d}.format(IDX)

### Standalone code to reproduce the issue

```shell
save a model with more than 10 outputs
```


### Relevant log output

_No response_"
61250,ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

1.0.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

nvcc --version gives 10.1 nvidia-smi gives CUDA Version: 12.2 

### GPU model and memory

_No response_

### Current behavior?

ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory

### Standalone code to reproduce the issue

```shell
This error occured when I imported tensorflow
import tensorflow as tf

To resolve this issue I have set my $CUDA_HOME=/usr/lib/cuda/
and $LD_LIBRARY_PATH=usr/lib/cuda/lib64

But surprisingly usr/lib/cuda/lib64 is empty. I dont have cuda folder in usr/local directory.
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
"
61249,tf.linalg.logdet outputs -inf on a matrix with complex data type,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Given the following matrix:
```
t = np.array([[1 + 1j, 4 + 1j], [4 + 2j, 3 + 1j]])
```
tf.linalg.logdet outputs `-inf`, I think the expected output should be a normal value. For reference, PyTorch's torch.logdet outputs `tensor(2.6688-2.5536j, dtype=torch.complex128)`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
t = np.array([[1 + 1j, 4 + 1j], [4 + 2j, 3 + 1j]])
tf_res = tf.linalg.logdet(tf.constant(t, dtype=tf.complex128))
torch_res = torch.logdet(torch.tensor(t))
print(tf_res, torch_res)
```
```


### Relevant log output

```shell
tf.Tensor(-inf, shape=(), dtype=float64) tensor(2.6688-2.5536j, dtype=torch.complex128)
```
"
61247,AttributeError: module 'tensorflow_datasets' has no attribute 'load',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.10.0


### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I cannot access [UCF101](https://www.tensorflow.org/datasets/catalog/ucf101) and download.

TensorFlow 2.10
Python 3.11.12
tensorflow_datasets 4.9.2

### Standalone code to reproduce the issue

```shell
import tensorflow_datasets
ucf101 = tensorflow_datasets.video.ucf101.Ucf101()
```


### Relevant log output

```shell
AttributeError: module 'tensorflow_datasets' has no attribute 'load'
```
"
61246,tf.image.adjust_contrast fails on the tf.half data type,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I was trying tf.image.adjust_contrast but I find that it fails when I set the input tensor's data type to be `float16 (tf.half)`, this API raises error. However, if I set the input data type to be `bfloat16`, this API works properly. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
np.random.seed(1234)
t = tf.constant(np.random.rand(1,2,1), dtype=tf.float16)
i = tf.image.adjust_contrast(t, 2.)
print(i)
```
```


### Relevant log output

```shell
NotFoundError: Could not find device for node: {{node AdjustContrastv2}} = AdjustContrastv2[T=DT_HALF]
All kernels registered for op AdjustContrastv2:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_HALF]
 [Op:AdjustContrastv2] name:
```
"
61245,tf.data parallel filter dataset for each class and interleave not giving expected speedup ,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

_No response_

### Current behavior?

I've tried to get some help on [Tensorflow Forum](https://discuss.tensorflow.org/t/dataset-with-tf-data-with-batches-of-randomly-n-chosen-speakers-and-their-m-utterances/17824) but I've got no response so I decided to post an issue here. Post on TFRecords Forum holds extensive description of my problem (tf.data dataloader for GE2E loss), but I'll try to trim it down only to technical matter.

I'm working on implementation of dataloader, where I want to get (n_speakers x n_utters) samples in every batch. I've already prepared dataset as 10 sharded TFRecord (as suggested in docs) and I wanted to build pairing mechanism using tf.data API. I've menaged to do so with `tf.data.Dataset.choose_from_datasets`, but this requires me to have a list of datasets, where each dataset contains only samples from one speaker. To prepare such dataset I've filtered my one big TFRecords Dataset with each speaker_id. My implementation feels criminal, because it iterates through whole dataset as many times as there are speakers. I've tried to help it with parallel calls (mapping the filter function with list of speakers), but there was no speedup. I've also tried using interleave (setting only `num_parallel_calls` to `AUTOTUNE`) on my dataset parsing, but all I got was slower execution time.

Is if there is any easy and reasonable solution to this filtering issue (to filter whole dataset by each class in one pass)? My only idea to skip filtering at all is to separate each speaker in different TFRecord at the moment of dataset creation. Also how should I use interleave with my code? 

Currently it takes a long time to make a first pass over dataset (~10min for 10GB of TFRecords, next iters when filtered datasets get cached its 2s) - it can be an issue with training with bigger dataset I suppose.

I'm attaching the relevant code but it's not standalone unfortunately, as I can not attach used data. Also some parsing code is missing, but I think it's not that relevant - my examples are stored as melspectrograms with speaker_id label. I could mock some data if someone was interested in helping me.




### Standalone code to reproduce the issue

```shell
dataset = tf.data.Dataset.from_tensor_slices(files)
tfrecords_reader = functools.partial(
    self._tfrecords_reader,
    speaker_id=self._speaker_id,
    speaker_gender=self._speaker_gender,
    speaker_id_key=self._speaker_id_key,
    speaker_gender_key=self._speaker_gender_key,
)

dataset = tf.data.TFRecordDataset(dataset)
dataset = dataset.map(tfrecords_reader, num_parallel_calls=AUTOTUNE)

# NOT PARALLEL WAY - surprisingly it takes as long as parallel method...
# datasets = []
# for speaker_id in self._speaker_list:
#     datasets.append(
#         dataset.filter(lambda x, y: y[""speaker_id""] == speaker_id)
#         .cache()
#         .shuffle(BUFFER_SIZE, reshuffle_each_iteration=True)
#         .repeat()
#     )

# Filtering for speakers
speakers = tf.data.Dataset.from_tensor_slices(self._speaker_list)
dataset = speakers.map(
    lambda speaker_id: dataset.filter(lambda x, y: y[""speaker_id""] == speaker_id)
    .cache()
    .shuffle(BUFFER_SIZE, reshuffle_each_iteration=True)
    .repeat(),
    num_parallel_calls=AUTOTUNE,
)

datasets_idx = tf.data.Dataset.range(len(self._speaker_list))
# Define those values statically for uniform_candidate_sampler which requires static int, even tho it looks silly
len_speakers = len(speakers)
len_pairs = len_speakers - 1

def pair_speakers(x):
    """"""Pair all speakers in dataset.
    
    Pairing is done in a way where each speaker is taken at least once.
    Also we can guarantee that those pairs are being re-generated each time.
    """"""
    to_pair = tf.reshape(tf.where(tf.range(len(speakers)) != x), [1, -1])  # Exclude speaker x from being drawed
    pairs = tf.random.uniform_candidate_sampler(to_pair, len_pairs, self._num_speakers - 1, True, len_speakers)[
        0
    ]  # sample n_speakers-1 pairings without replacement from len(speakers)-1 speakers
    paired = tf.concat([[x], pairs], 0)  # concat speaker x with its pairs
    return tf.repeat(
        paired, tf.repeat(self._num_utters, self._num_speakers)
    )  # repeat each speaker count by selected number of utterances

choice_dataset = datasets_idx.map(
pair_speakers, num_parallel_calls=AUTOTUNE
)  # [mapping of len(speakers) datasets, each num_speakers*num_utters]
choice_dataset = choice_dataset.flat_map(
    tf.data.Dataset.from_tensor_slices
)  # [single dataset of len(speakers)*num_speakers*num_utters,]

# Deterministicly takes samples from filtered datasets with order specified in choice dataset
dataset = tf.data.Dataset.choose_from_datasets(dataset, choice_dataset).batch(
    self._num_speakers * self._num_utters
)

# Apply transforms to batched dataset spectrograms. Faster transforms than on single examples
dataset.map(
    lambda x, y: (self._transform(x), y),
    num_parallel_calls=tf.data.experimental.AUTOTUNE,
)

dataset = dataset.prefetch(AUTOTUNE)
total_steps = len(self._speaker_list)

return dataset, total_steps
```


### Relevant log output

_No response_"
61240,"Different reference order may cause other modules to be unavailable, e.g. xgboost, sklearn.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

centos 7.6

### Mobile device

_No response_

### Python version

python 3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda=12.1

### GPU model and memory

32510MiB

### Current behavior?

I was trying to import xgboost(or sklearn) and tensorflow modules at same time, but when I  imported modules by different order, it just return me error message that I can not handle it, I don't whether it a bug or some issues that can be fixed by myself?  and I also search something resource, which said that it was a bug caused by glibc : https://sourceware.org/bugzilla/show_bug.cgi?id=17090. and then I was trying to reintstall glibc on my server, unfortunately, the plan finally failed and now I am just trying to rebuild my whole environment by rollbacking to previous mirror backup, sad.

### Standalone code to reproduce the issue

```shell
import numpy as np

## bad import order
import tensorflow as tf
from xgboost import XGBClassifier ## order # or import sklearn, may report different error messages.

## good import order
# from xgboost import XGBClassifier
# import tensorflow as tf

hparams = {
    'booster':'gbtree',
    'objective': 'binary:logistic',
    'eval_metric': 'aucpr',
    'max_depth': 10,
    'gamma': 4,
    'lambda':0.001,
    'subsample':0.7,
    'colsample_bytree':0.8,
    'colsample_bylevel':0.8,
    'colsample_bynode': 0.8,
    'min_child_weight':20,
    'eta': 0.03,
    'seed': 42,
    'nthread':15,
    'tree_method':'gpu_hist',
    'n_estimators': 350
}
estimator = XGBClassifier(**hparams)
X_train = np.random.rand(10000, 10)
y_train = np.random.randint(0, 2, (10000, 1))
X_eval = np.random.rand(1000, 10)
y_eval = np.random.randint(0, 2, (1000, 1))

estimator.fit(X_train, y_train, eval_set=[(X_train, y_train),(X_eval, y_eval)])
```


### Relevant log output

```shell
2023-07-11 15:57:01.594620: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/home/haojiaxiang/projects/test/test.py"", line 6, in <module>
    from xgboost import XGBClassifier ## order # or import sklearn, may report different error messages.
  File ""/home/haojiaxiang/miniconda3/envs/gms/lib/python3.10/site-packages/xgboost/__init__.py"", line 7, in <module>
    from . import collective, dask, rabit
  File ""/home/haojiaxiang/miniconda3/envs/gms/lib/python3.10/site-packages/xgboost/collective.py"", line 12, in <module>
    from .core import _LIB, _check_call, c_str, py_str, from_pystr_to_cstr
  File ""/home/haojiaxiang/miniconda3/envs/gms/lib/python3.10/site-packages/xgboost/core.py"", line 264, in <module>
    _LIB = _load_lib()
  File ""/home/haojiaxiang/miniconda3/envs/gms/lib/python3.10/site-packages/xgboost/core.py"", line 216, in _load_lib
    raise XGBoostError(
xgboost.core.XGBoostError: 
XGBoost Library (libxgboost.so) could not be loaded.
Likely causes:
  * OpenMP runtime is not installed
    - vcomp140.dll or libgomp-1.dll for Windows
    - libomp.dylib for Mac OSX
    - libgomp.so for Linux and other UNIX-like OSes
    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.

  * You are running 32-bit Python on a 64-bit OS

Error message(s): ['dlopen: cannot load any more object with static TLS']
```
"
61239,Can the resnet model written in the tf_slim library call the MirroredStrategy strategy to achieve data parallel training?,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.11

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 16.04

### Mobile device

Linux Ubuntu 16.04

### Python version

3.9

### Bazel version

5.1.1

### GCC/compiler version

9.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

Tesla P100 12GB

### Current behavior?


The distributed_train_step method is only called once in the first epoch, and the rest are not called

### Standalone code to reproduce the issue

```shell
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)
# 输入占位符
inputs = tf.placeholder(tf.float32, shape=(None, 32, 32, 3))
labels = tf.placeholder(tf.float32, shape=(None, 10))
batch_size = 32
num_epochs = 10
num_batches = len(x_train) // batch_size

strategy = tf2.distribute.MirroredStrategy(devices=[""GPU:0"", ""GPU:1"",""GPU:2"", ""GPU:3""])
# 将训练数据集分发到多个GPU上
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train)).batch(batch_size)
dist_train_dataset = strategy.experimental_distribute_dataset(train_dataset)
with strategy.scope():
    # 构建ResNet模型
    net, end_points = resnet_v2_152(inputs,10)
    net = tf.squeeze(net, axis=[1, 2])  # 移除维度为 1 的高度和宽度维度

    # 定义损失函数和优化器
    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)


@tf.function
def train_step(input):
    x, y = input
    print('x_shape:',x.shape)
    print('y_shape:',y.shape)
    with tf.GradientTape() as tape:
        logits,_ = resnet_v2_152(x,10,reuse = True)
        print('logits:',logits.shape)
        logits = tf.squeeze(logits, axis=[1, 2])  # 移除维度为 1 的高度和宽度维度
        print('logits_squeeze:',logits.shape)
        loss_value = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))
        print('loss_value:',loss_value.shape)
    grads = tape.gradient(loss_value, tf.trainable_variables())
    optimizer.apply_gradients(zip(grads, tf.trainable_variables()))
    correct_predictions = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))
    return loss_value, accuracy
@tf.function
def distributed_train_step(dataset_inputs):
        total_loss = 0.0
        total_acc = 0.0
        num_batches = 0
        print(""distributed_train_step function start"")
        print(""dataset_inputs:{}"".format(dataset_inputs))
        for x in dataset_inputs:
            print(""x:{}"".format(x))
            per_replica_losses, per_replica_accuracies = strategy.run(train_step, args=(x,))
            print(""per_replica_losses:{},per_replica_acc:{}"".format(per_replica_losses,per_replica_accuracies))
            total_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)
            total_acc = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_accuracies, axis=None)
            num_batches +=1
         return total_loss / tf.cast(num_batches, dtype=tf.float32),total_acc / tf.cast(num_batches, dtype=tf.float32)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for epoch in range(num_epochs):
        start_time = time.time()
        train_loss,train_acc = distributed_train_step(dist_train_dataset)
        template = (""Epoch {}, Loss: {}, Accuracy: {}"")
        print(template.format(epoch + 1, train_loss, train_acc))
        print(""sess epoch:{},time:{}"".format(epoch+1,time.time()-start_time))
        print()
```


### Relevant log output

_No response_"
61238,tf.data train model worse than numpy data,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.10

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current behavior?
**1. Used tf.data train model best val loss is 0.14, and the code is:**
![image](https://github.com/tensorflow/tensorflow/assets/15938790/36ab9474-3a88-47ba-9226-675f3dfeb11a)

```
train_dataset1 = tf.data.Dataset.from_tensor_slices((np.array(xtrain).reshape(-1,10*12,1),np.array(encoder_train),np.array(decoder_train)))
train_dataset2 = tf.data.Dataset.from_tensor_slices((np.array(ytrain)))
train_dataset = tf.data.Dataset.zip((train_dataset1, train_dataset2))
test_dataset1 = tf.data.Dataset.from_tensor_slices((np.array(xtest).reshape(-1,10*12,1),np.array(encoder_test),np.array(decoder_test)))
test_dataset2 = tf.data.Dataset.from_tensor_slices((np.array(ytest)))
test_dataset = tf.data.Dataset.zip((test_dataset1, test_dataset2))

BATCH_SIZE = 256
SHUFFLE_BUFFER_SIZE = 1000

train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
test_dataset = test_dataset.batch(BATCH_SIZE)

model = Model(inputs = inputs, outputs = s2s)
model.compile(loss = 'mse', optimizer='adam', metrics=['mape'])
early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)
model.fit(train_dataset, epochs=20, #batch_size=256, 
          validation_data = test_dataset, callbacks=[early_stopping])
```


**2. Used numpy array train model best val loss is 0.10, the code is:**
![image](https://github.com/tensorflow/tensorflow/assets/15938790/a9eec3fc-cdfe-4380-836d-7b047ab2580c)


```
model = Model(inputs = inputs, outputs = s2s)
model.compile(loss = 'mse', optimizer='adam', metrics=['mape'])
early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)
model.fit([np.array(xtrain).reshape(-1,10*12,1),np.array(encoder_train),np.array(decoder_train)], np.array(ytrain), epochs=20, batch_size=256, 
          validation_data = ([np.array(xtest).reshape(-1,10*12,1),np.array(encoder_test),np.array(decoder_test)], np.array(ytest)), callbacks=[early_stopping])
```

**3. tf.data.Dataset.from_generator val loss worse more:**

### Standalone code to reproduce the issue

```shell
tf.data bugs at version 2.10
```


### Relevant log output

_No response_"
61234,Memory out of bounds in compiled tflite with emscripten.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have compiled tflite using cmake (without XNNPACK support) and emscripten (both latest 3.1.42 and 3.1.10). 
When trying to perform inference at the browser with my model I get the following error:

vmt.wasm:0x31cff Uncaught RuntimeError: memory access out of bounds
    at vmt.wasm:0x31cff
    at vmt.wasm:0x1f7a94
    at vmt.wasm:0x3c4910
    at vmt.wasm:0x65ace
    at vmt.wasm:0x231c3e
    at vmt.wasm:0x458a49
    at vmt.wasm:0x517c60
    at img.onload (index.html:772:28)

This happens with all of my models at the very first operation (pad). When inspecting the .wasm file using chrome dev tools I see that the error happens at a ""memory.fill"" operation.

### Standalone code to reproduce the issue

```shell
I have compiled tflite with the following emcmake command:

cmake -DCMAKE_CXX_FLAGS=""-lpthread -pthread -lpthread -s USE_PTHREADS"" -DTFLITE_ENABLE_MMAP=OFF -DTFLITE_ENABLE_NNAPI=OFF -DTFLITE_ENABLE_RUY=ON -DTFLITE_ENABLE_XNNPACK=OFF ..

while when compiling my project with emscripten (including the above resulting libraries) I use the following flags:

	SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -s INITIAL_MEMORY=512MB"")
	SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -s ALLOW_MEMORY_GROWTH=1"")
	SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -s ALLOW_TABLE_GROWTH=1"")
```


### Relevant log output

```shell
This is the output of PrintInterpreterState right before the first inference.

[WASM] === Pre-invoke Interpreter State ===
pre-vmt.js:11 [WASM] Interpreter has 1 subgraphs.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] -----------Subgraph-0 has 134 tensors and 49 nodes------------
pre-vmt.js:11 [WASM] 1 Inputs: [0] -> 602112B (0.57MB)
pre-vmt.js:11 [WASM] 1 Outputs: [122] -> 708B (0.00MB)
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] Tensor  ID Name                      Type            AllocType          Size (Bytes/MB)    Shape      MemAddr-Offset  
pre-vmt.js:11 [WASM] Tensor   0  ��ʻ䯻9:󺂶*򨓮.. kTfLiteFloat32  kTfLiteArenaRw     602112   / 0.57 [1,224,224,3] [0, 602112)
pre-vmt.js:11 [WASM] Tensor   1 騅:��񛻺󿰻��m... kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16] [690960, 691024)
pre-vmt.js:11 [WASM] Tensor   2 r畼��匹��t;��... kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16] [690864, 690928)
pre-vmt.js:11 [WASM] Tensor   3 jԐ;��4i;⦄;Q;箮. kTfLiteFloat32  kTfLiteMmapRo      160      / 0.00 [40] [690688, 690848)
pre-vmt.js:11 [WASM] Tensor   4 究򑃷çc6ԯ#6ߗ<׹... kTfLiteFloat32  kTfLiteMmapRo      160      / 0.00 [40] [690512, 690672)
pre-vmt.js:11 [WASM] Tensor   5 :#��
𯫿��7tU��... kTfLiteFloat32  kTfLiteMmapRo      224      / 0.00 [56] [690272, 690496)
pre-vmt.js:11 [WASM] Tensor   6 ƙl��7򣡷/򷕈4��.. kTfLiteFloat32  kTfLiteMmapRo      224      / 0.00 [56] [690032, 690256)
pre-vmt.js:11 [WASM] Tensor   7 ����뻪��3z}��.. kTfLiteFloat32  kTfLiteMmapRo      256      / 0.00 [64] [689760, 690016)
pre-vmt.js:11 [WASM] Tensor   8 ᡁ������땐6ԝ... kTfLiteFloat32  kTfLiteMmapRo      256      / 0.00 [64] [689488, 689744)
pre-vmt.js:11 [WASM] Tensor   9 􌶄��5.8^L𷆄򷳱... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [688896, 689472)
pre-vmt.js:11 [WASM] Tensor  10 ��""ۀ7��Ce����... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [688304, 688880)
pre-vmt.js:11 [WASM] Tensor  11 ģ
pre-vmt.js:11 [WASM] 7C^J𐬣𶓃6񪤷׾... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [687712, 688288)
pre-vmt.js:11 [WASM] Tensor  12 ��ȏ6띓����8h... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [687120, 687696)
pre-vmt.js:11 [WASM] Tensor  13 ��A򪶚윶��䞏��.. kTfLiteFloat32  kTfLiteMmapRo      288      / 0.00 [72] [686816, 687104)
pre-vmt.js:11 [WASM] Tensor  14 ԙõ`򫷾F6򓕶O򙷳c... kTfLiteFloat32  kTfLiteMmapRo      288      / 0.00 [72] [686512, 686800)
pre-vmt.js:11 [WASM] Tensor  15 ū쵄񉸻罷{\ѵfW𶯜... kTfLiteFloat32  kTfLiteMmapRo      288      / 0.00 [72] [686208, 686496)
pre-vmt.js:11 [WASM] Tensor  16 &ꋷ𷏸ᛸ6��ꮮ. kTfLiteFloat32  kTfLiteMmapRo      288      / 0.00 [72] [685904, 686192)
pre-vmt.js:11 [WASM] Tensor  17 |㗶񏍷򛀷��\Y7... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [685312, 685888)
pre-vmt.js:11 [WASM] Tensor  18 蕷ΒU7��`&÷勸f+... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [684720, 685296)
pre-vmt.js:11 [WASM] Tensor  19 w춃֣74ٿ𣔝����.. kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [683552, 684704)
pre-vmt.js:11 [WASM] Tensor  20 ;""6!򑶷膷ڽ𷞮��... kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [682384, 683536)
pre-vmt.js:11 [WASM] Tensor  21 󠼷YPQ������75... kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [681216, 682368)
pre-vmt.js:11 [WASM] Tensor  22 ��󰳳IB}������.. kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [680048, 681200)
pre-vmt.js:11 [WASM] Tensor  23 򳤷񲂷wް5��^	8􊮮. kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [678880, 680032)
pre-vmt.js:11 [WASM] Tensor  24 _��m󷟝򵳂󶬾𼾮.. kTfLiteFloat32  kTfLiteMmapRo      32       / 0.00 [8] [678832, 678864)
pre-vmt.js:11 [WASM] Tensor  25 P��&;𶿮����7#׮.. kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16] [678752, 678816)
pre-vmt.js:11 [WASM] Tensor  26 ��5񺉷a꨷ᘑ��.. kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16] [678672, 678736)
pre-vmt.js:11 [WASM] Tensor  27 卞7󯉷򨮷��󠚷��. kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678560, 678656)
pre-vmt.js:11 [WASM] Tensor  28 FR񷀃󷴫d������.. kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678448, 678544)
pre-vmt.js:11 [WASM] Tensor  29 򺜵W7𽒷񪶷̒#��   kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678336, 678432)
pre-vmt.js:11 [WASM] Tensor  30 $뀷��R𷸙u쭌7󿮮. kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678224, 678320)
pre-vmt.js:11 [WASM] Tensor  31 \~��ط$i򷮼
6҄*觮.. kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678112, 678208)
pre-vmt.js:11 [WASM] Tensor  32 ��0��涂񖷝f5��. kTfLiteFloat32  kTfLiteMmapRo      192      / 0.00 [48] [677904, 678096)
pre-vmt.js:11 [WASM] Tensor  33 ؉h������7˜𶠺... kTfLiteFloat32  kTfLiteMmapRo      192      / 0.00 [48] [677696, 677888)
pre-vmt.js:11 [WASM] Tensor  34 󚫷\7͛$♩��7[... kTfLiteFloat32  kTfLiteMmapRo      192      / 0.00 [48] [677488, 677680)
pre-vmt.js:11 [WASM] Tensor  35 9X����������.. kTfLiteFloat32  kTfLiteMmapRo      1728     / 0.00 [16,3,3,3] [675744, 677472)
pre-vmt.js:11 [WASM] Tensor  36 jť<箯󧬍󘴙;Љ&󿈮.. kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [1,3,3,16] [675152, 675728)
pre-vmt.js:11 [WASM] Tensor  37 􅻻λ""<����?<ɓ... kTfLiteFloat32  kTfLiteMmapRo      512      / 0.00 [8,1,1,16] [674624, 675136)
pre-vmt.js:11 [WASM] Tensor  38 ޛȻx׃<򁻝ě<u+��... kTfLiteFloat32  kTfLiteMmapRo      1280     / 0.00 [40,1,1,8] [673328, 674608)
pre-vmt.js:11 [WASM] Tensor  39 ��Ԍ󼲍׹󵏼Ϩ��... kTfLiteFloat32  kTfLiteMmapRo      1440     / 0.00 [1,3,3,40] [671872, 673312)
pre-vmt.js:11 [WASM] Tensor  40 ?ȑ򄏱;3d1󞻳��       kTfLiteFloat32  kTfLiteMmapRo      2560     / 0.00 [16,1,1,40] [669296, 671856)
pre-vmt.js:11 [WASM] Tensor  41 Cһ;��ٻ:뼩攻ŷ... kTfLiteFloat32  kTfLiteMmapRo      3584     / 0.00 [56,1,1,16] [665696, 669280)
pre-vmt.js:11 [WASM] Tensor  42 3������򥅡;b... kTfLiteFloat32  kTfLiteMmapRo      2016     / 0.00 [1,3,3,56] [663664, 665680)
pre-vmt.js:11 [WASM] Tensor  43 Mٚ<啤<j��8|��쮮. kTfLiteFloat32  kTfLiteMmapRo      3584     / 0.00 [16,1,1,56] [660064, 663648)
pre-vmt.js:11 [WASM] Tensor  44 ᦺ��򦖶:晼4櫻k... kTfLiteFloat32  kTfLiteMmapRo      4096     / 0.00 [64,1,1,16] [655952, 660048)
pre-vmt.js:11 [WASM] Tensor  45 轼\<j4<Ӯ:𡻻ug... kTfLiteFloat32  kTfLiteMmapRo      2304     / 0.00 [1,3,3,64] [653632, 655936)
pre-vmt.js:11 [WASM] Tensor  46 ��৐��󶜿󄲢𠧮.. kTfLiteFloat32  kTfLiteMmapRo      6144     / 0.01 [24,1,1,64] [647472, 653616)
pre-vmt.js:11 [WASM] Tensor  47 ��񒘼Ѡj󫈞󭲐<�� kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [144,1,1,24] [633632, 647456)
pre-vmt.js:11 [WASM] Tensor  48 '����󼰛��󋍮.. kTfLiteFloat32  kTfLiteMmapRo      5184     / 0.00 [1,3,3,144] [628432, 633616)
pre-vmt.js:11 [WASM] Tensor  49 `��𼢈<񗋻          kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [24,1,1,144] [614592, 628416)
pre-vmt.js:11 [WASM] Tensor  50 ا""󖗽򫋺<󀮹͑x󪁮.. kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [144,1,1,24] [600752, 614576)
pre-vmt.js:11 [WASM] Tensor  51 ȷk󒏷:󁌝��󦦮.. kTfLiteFloat32  kTfLiteMmapRo      5184     / 0.00 [1,3,3,144] [595552, 600736)
pre-vmt.js:11 [WASM] Tensor  52 t=򜧨;6򌼾)𻀫��... kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [24,1,1,144] [581712, 595536)
pre-vmt.js:11 [WASM] Tensor  53 򚪼f񒻯��=弯P... kTfLiteFloat32  kTfLiteMmapRo      6912     / 0.01 [72,1,1,24] [574784, 581696)
pre-vmt.js:11 [WASM] Tensor  54 (����nۻ:,��󛥮.. kTfLiteFloat32  kTfLiteMmapRo      2592     / 0.00 [1,3,3,72] [572176, 574768)
pre-vmt.js:11 [WASM] Tensor  55 ��뙿��󭩄;��?
pre-vmt.js:11 [WASM] 􄠫TfLiteFloat32  kTfLiteMmapRo      6912     / 0.01 [24,1,1,72] [565248, 572160)
pre-vmt.js:11 [WASM] Tensor  56 ��̘<9D򆼇*t<'��... kTfLiteFloat32  kTfLiteMmapRo      6912     / 0.01 [72,1,1,24] [558320, 565232)
pre-vmt.js:11 [WASM] Tensor  57 𳼑sD󠺬;󋽦􊼠R... kTfLiteFloat32  kTfLiteMmapRo      2592     / 0.00 [1,3,3,72] [555712, 558304)
pre-vmt.js:11 [WASM] Tensor  58 mX��h:%4k<̫'<(ç<񝮮. kTfLiteFloat32  kTfLiteMmapRo      6912     / 0.01 [24,1,1,72] [548784, 555696)
pre-vmt.js:11 [WASM] Tensor  59 o󻛄񷖡��<𽤼񁮮. kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [144,1,1,24] [534944, 548768)
pre-vmt.js:11 [WASM] Tensor  60 Ⱥּ󏉼򘄼]��d_;�. kTfLiteFloat32  kTfLiteMmapRo      5184     / 0.00 [1,3,3,144] [529744, 534928)
pre-vmt.js:11 [WASM] Tensor  61 ֭C򢊛;󥙻怙<貦��.. kTfLiteFloat32  kTfLiteMmapRo      27648    / 0.03 [48,1,1,144] [502080, 529728)
pre-vmt.js:11 [WASM] Tensor  62 ��􊥺*?""��󸸔��.. kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [288,1,1,48] [446768, 502064)
pre-vmt.js:11 [WASM] Tensor  63 򶼼1n⻗��-Ի񛹪�� kTfLiteFloat32  kTfLiteMmapRo      10368    / 0.01 [1,3,3,288] [436384, 446752)
pre-vmt.js:11 [WASM] Tensor  64 ��ֻ𲰼􅛻o𣼣�� kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [48,1,1,288] [381072, 436368)
pre-vmt.js:11 [WASM] Tensor  65 V;潛򠛮<ʻ㻅S;򮮮 kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [288,1,1,48] [325760, 381056)
pre-vmt.js:11 [WASM] Tensor  66 ¿,<
pre-vmt.js:11 [WASM] 񏻥E����ݻE񮮮 kTfLiteFloat32  kTfLiteMmapRo      10368    / 0.01 [1,3,3,288] [315376, 325744)
pre-vmt.js:11 [WASM] Tensor  67 dճ<󋺑񢼍����Ү.. kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [48,1,1,288] [260064, 315360)
pre-vmt.js:11 [WASM] Tensor  68 ����𣺼d򻍪~;�� kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [288,1,1,48] [204752, 260048)
pre-vmt.js:11 [WASM] Tensor  69 冻񍖼𣄼𲿼󲎽��. kTfLiteInt32    kTfLiteMmapRo      8        / 0.00 [2] [204720, 204728)
pre-vmt.js:11 [WASM] Tensor  70 ~&��<��`<ݺNϘ;𫮮. kTfLiteInt32    kTfLiteMmapRo      12       / 0.00 [3] [204688, 204700)
pre-vmt.js:11 [WASM] Tensor  71 ��%ɻ	`޻󴵹ϛ;t񮮮 kTfLiteFloat32  kTfLiteMmapRo      708      / 0.00 [177] [203968, 204676)
pre-vmt.js:11 [WASM] Tensor  72 <񌹂L��;ԅ򻬰A󾫮.. kTfLiteInt32    kTfLiteMmapRo      32       / 0.00 [4,2] [203920, 203952)
pre-vmt.js:11 [WASM] Tensor  73 Ё򻙃ٻ:
ԺV۪;��
pre-vmt.js:11 [WASM] ;Ŗ... kTfLiteFloat32  kTfLiteMmapRo      203904   / 0.19 [177,288] [0, 203904)
pre-vmt.js:11 [WASM] Tensor  74 2
pre-vmt.js:11 [WASM] 纝S
󊺅򚋟��<... kTfLiteFloat32  kTfLiteArenaRw     612912   / 0.58 [1,226,226,3] [2759680, 3372592)
pre-vmt.js:11 [WASM] Tensor  75 ⻒����S󁟮󼘮.. kTfLiteFloat32  kTfLiteArenaRw     802816   / 0.77 [1,112,112,16] [1956864, 2759680)
pre-vmt.js:11 [WASM] Tensor  76 z򛼷x0<ʬ𠻗��߮.. kTfLiteFloat32  kTfLiteArenaRw     831744   / 0.79 [1,114,114,16] [602112, 1433856)
pre-vmt.js:11 [WASM] Tensor  77 󥺗��Ї􆛁����.. kTfLiteFloat32  kTfLiteArenaRw     200704   / 0.19 [1,56,56,16] [1433856, 1634560)
pre-vmt.js:11 [WASM] Tensor  78 ½ẇ                     kTfLiteFloat32  kTfLiteArenaRw     100352   / 0.10 [1,56,56,8] [602112, 702464)
pre-vmt.js:11 [WASM] Tensor  79 𖒽񅬽Ɓּ򽮨=ڝ... kTfLiteFloat32  kTfLiteArenaRw     501760   / 0.48 [1,56,56,40] [1140352, 1642112)
pre-vmt.js:11 [WASM] Tensor  80 󦜼󅯻牪=𞁼{Jk<󯮮. kTfLiteFloat32  kTfLiteArenaRw     538240   / 0.51 [1,58,58,40] [602112, 1140352)
pre-vmt.js:11 [WASM] Tensor  81 )a˼q󀀀
pre-vmt.js:11 [WASM] ������<蚮.. kTfLiteFloat32  kTfLiteArenaRw     125440   / 0.12 [1,28,28,40] [1140352, 1265792)
pre-vmt.js:11 [WASM] Tensor  82 ⰾ󝔦<񃼄h��8��.. kTfLiteFloat32  kTfLiteArenaRw     50176    / 0.05 [1,28,28,16] [953344, 1003520)
pre-vmt.js:11 [WASM] Tensor  83 ��į<򑁼Q󬽴߼6)... kTfLiteFloat32  kTfLiteArenaRw     175616   / 0.17 [1,28,28,56] [602112, 777728)
pre-vmt.js:11 [WASM] Tensor  84 ��s��񌽌𱽬��*... kTfLiteFloat32  kTfLiteArenaRw     175616   / 0.17 [1,28,28,56] [777728, 953344)
pre-vmt.js:11 [WASM] Tensor  85 ��La޻��4<zL󼚨... kTfLiteFloat32  kTfLiteArenaRw     50176    / 0.05 [1,28,28,16] [602112, 652288)
pre-vmt.js:11 [WASM] Tensor  86 <򔼌��󰽊V-��<��. kTfLiteFloat32  kTfLiteArenaRw     50176    / 0.05 [1,28,28,16] [652288, 702464)
pre-vmt.js:11 [WASM] Tensor  87 DZQ��󿒇<;zZ����.. kTfLiteFloat32  kTfLiteArenaRw     200704   / 0.19 [1,28,28,64] [832512, 1033216)
pre-vmt.js:11 [WASM] Tensor  88 ^��j􌧁;E����... kTfLiteFloat32  kTfLiteArenaRw     230400   / 0.22 [1,30,30,64] [602112, 832512)
pre-vmt.js:11 [WASM] Tensor  89 ��󛶼+)������.. kTfLiteFloat32  kTfLiteArenaRw     50176    / 0.05 [1,14,14,64] [832512, 882688)
pre-vmt.js:11 [WASM] Tensor  90 ༟<cdͻYϑ������.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [882688, 901504)
pre-vmt.js:11 [WASM] Tensor  91 ;��༕󗼫}<~_.��.. kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [602112, 715008)
pre-vmt.js:11 [WASM] Tensor  92 덒;T@ë<ˠ��v<5�� kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [715008, 827904)
pre-vmt.js:11 [WASM] Tensor  93 򣀼򧂼4񥻬SN����.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [827904, 846720)
pre-vmt.js:11 [WASM] Tensor  94 ��6𢺊$@<󶢺T>ֺ舮.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [846720, 865536)
pre-vmt.js:11 [WASM] Tensor  95 i粼**&<ļ򅫈;B򻤺... kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [602112, 715008)
pre-vmt.js:11 [WASM] Tensor  96 S{.ZR����<9��1... kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [715008, 827904)
pre-vmt.js:11 [WASM] Tensor  97 Pw􀗃􈨉��㢥󧂮.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [827904, 846720)
pre-vmt.js:11 [WASM] Tensor  98 ߴüs��9󢠠           kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [715008, 733824)
pre-vmt.js:11 [WASM] Tensor  99 t#򼇟񼻺<􊚻󣫚򡋮.. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,14,14,72] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 100 ��ݿ������9\r... kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,14,14,72] [658560, 715008)
pre-vmt.js:11 [WASM] Tensor 101 ��ڛ��𼮲��N􏕮.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [602112, 620928)
pre-vmt.js:11 [WASM] Tensor 102 򝷺iܨ<<<򾃆;󵮮. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [733824, 752640)
pre-vmt.js:11 [WASM] Tensor 103 ��hԂ<o񉼿hἑݎ<f�� kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,14,14,72] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 104 󻦨𼯄[<��W~J󗩮.. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,14,14,72] [658560, 715008)
pre-vmt.js:11 [WASM] Tensor 105 0»Lv<ǡF򟻸;jV��... kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [715008, 733824)
pre-vmt.js:11 [WASM] Tensor 106 s	��W򈤋񊗺9fZ��.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [602112, 620928)
pre-vmt.js:11 [WASM] Tensor 107 wŔ;􃋼��~<��<ڷ... kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [749568, 862464)
pre-vmt.js:11 [WASM] Tensor 108 KԽ;㚹֛!;񬻠         kTfLiteFloat32  kTfLiteArenaRw     147456   / 0.14 [1,16,16,144] [602112, 749568)
pre-vmt.js:11 [WASM] Tensor 109 𜭼艺<ﻨ9刼;
pre-vmt.js:11 [WASM] *<􉮮. kTfLiteFloat32  kTfLiteArenaRw     28224    / 0.03 [1,7,7,144] [749568, 777792)
pre-vmt.js:11 [WASM] Tensor 110 0��ĺ;ۚ<��ϭܻI󮮮 kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [715008, 724416)
pre-vmt.js:11 [WASM] Tensor 111 sl$􀍋;OQֺ<��𻺔ޮ.. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 112 2\𼀄��;ဥ;��Ȉ... kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [658560, 715008)
pre-vmt.js:11 [WASM] Tensor 113 󂆅��􎙧<񶧻񍮮. kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [602112, 611520)
pre-vmt.js:11 [WASM] Tensor 114 ꥩ8^񑻢����         kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [724416, 733824)
pre-vmt.js:11 [WASM] Tensor 115 𛁺����[񁺼��߮.. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 116 /ӎ􀀀
pre-vmt.js:11 [WASM] 껵򀼏��B�� kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [658560, 715008)
pre-vmt.js:11 [WASM] Tensor 117 ϣ󺕠����;f򜺹
pre-vmt.js:11 [WASM] ... kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [715008, 724416)
pre-vmt.js:11 [WASM] Tensor 118 ᳠:$p溓l��㻂	𺩯... kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [658560, 667968)
pre-vmt.js:11 [WASM] Tensor 119 ��򭶹��Cျꬓ:��. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 120 SW��Y��;��e��... kTfLiteFloat32  kTfLiteArenaRw     1152     / 0.00 [1,1,1,288] [659712, 660864)
pre-vmt.js:11 [WASM] Tensor 121 ��Ę;ᑩ򘯱󨞇󎓮.. kTfLiteFloat32  kTfLiteArenaRw     708      / 0.00 [1,177] [602112, 602820)
pre-vmt.js:11 [WASM] Tensor 122 긟𔎴;ڗs񡮠����.. kTfLiteFloat32  kTfLiteArenaRw     708      / 0.00 [1,59,3] [602880, 603588)
pre-vmt.js:11 [WASM] Tensor 123 (nil)                     kTfLiteInt32    kTfLiteArenaRw     16       / 0.00 [4] [660864, 660880)
pre-vmt.js:11 [WASM] Tensor 124 (nil)                     kTfLiteInt32    kTfLiteArenaRw     8        / 0.00 [2] [660928, 660936)
pre-vmt.js:11 [WASM] Tensor 125 (nil)                     kTfLiteFloat32  kTfLiteArenaRw     1152     / 0.00 [288] [658560, 659712)
pre-vmt.js:11 [WASM] Tensor 126 (nil)                     kTfLiteInt32    kTfLiteDynamic     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 127 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 128 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 129 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 130 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 131 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 132 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 133 (nil)                     kTfLiteFloat32  kTfLiteArenaRw     1354752  / 1.29 [1,112,112,27] [602112, 1956864)
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] kTfLiteArenaRw Info: 
pre-vmt.js:11 [WASM] Tensor 133 has the max size 1354752 bytes (1.292 MB).
pre-vmt.js:11 [WASM] This memory arena is estimated as[0x15e33b0, 0x12abd80), taking 3372592 bytes (3.216 MB).
pre-vmt.js:11 [WASM] One possible set of tensors that have non-overlapping memory spaces with each other, and they take up the whole arena:
pre-vmt.js:11 [WASM] Tensor 0 -> 133 -> 75 -> 74.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] kTfLiteArenaRwPersistent Info: not holding any allocation.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] kTfLiteMmapRo Info: 
pre-vmt.js:11 [WASM] Tensor 73 has the max size 203904 bytes (0.194 MB).
pre-vmt.js:11 [WASM] This memory arena is estimated as[0x10dde70, 0x1035320), taking 691024 bytes (0.659 MB).
pre-vmt.js:11 [WASM] One possible set of tensors that have non-overlapping memory spaces with each other, and they take up the whole arena:
pre-vmt.js:11 [WASM] Tensor 73 -> 72 -> 71 -> 70 -> 69 -> 68 -> 67 -> 66 -> 65 -> 64 -> 63 -> 62 -> 61 -> 60 -> 59 -> 58 -> 57 -> 56 -> 55 -> 54 -> 53 -> 52 -> 51 -> 50 -> 49 -> 48 -> 47 -> 46 -> 45 -> 44 -> 43 -> 42 -> 41 -> 40 -> 39 -> 38 -> 37 -> 36 -> 35 -> 34 -> 33 -> 32 -> 31 -> 30 -> 29 -> 28 -> 27 -> 26 -> 25 -> 24 -> 23 -> 22 -> 21 -> 20 -> 19 -> 18 -> 17 -> 16 -> 15 -> 14 -> 13 -> 12 -> 11 -> 10 -> 9 -> 8 -> 7 -> 6 -> 5 -> 4 -> 3 -> 2 -> 1.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] kTfLiteDynamic Info: not holding any allocation.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] Node   0 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[0,72] -> 602144B (0.57MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[74] -> 612912B (0.58MB)
pre-vmt.js:11 [WASM] Node   1 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[74,35,1] -> 614704B (0.59MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[75] -> 802816B (0.77MB)
pre-vmt.js:11 [WASM]   1 Temporary Tensors:[133] -> 1354752B (1.29MB)
pre-vmt.js:11 [WASM] Node   2 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[75,72] -> 802848B (0.77MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[76] -> 831744B (0.79MB)
pre-vmt.js:11 [WASM] Node   3 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[76,36,2] -> 832384B (0.79MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[77] -> 200704B (0.19MB)
pre-vmt.js:11 [WASM] Node   4 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[77,37,24] -> 201248B (0.19MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[78] -> 100352B (0.10MB)
pre-vmt.js:11 [WASM] Node   5 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[78,38,3] -> 101792B (0.10MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[79] -> 501760B (0.48MB)
pre-vmt.js:11 [WASM] Node   6 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[79,72] -> 501792B (0.48MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[80] -> 538240B (0.51MB)
pre-vmt.js:11 [WASM] Node   7 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[80,39,4] -> 539840B (0.51MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[81] -> 125440B (0.12MB)
pre-vmt.js:11 [WASM] Node   8 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[81,40,25] -> 128064B (0.12MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[82] -> 50176B (0.05MB)
pre-vmt.js:11 [WASM] Node   9 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[82,41,5] -> 53984B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[83] -> 175616B (0.17MB)
pre-vmt.js:11 [WASM] Node  10 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[83,42,6] -> 177856B (0.17MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[84] -> 175616B (0.17MB)
pre-vmt.js:11 [WASM] Node  11 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[84,43,26] -> 179264B (0.17MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[85] -> 50176B (0.05MB)
pre-vmt.js:11 [WASM] Node  12 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[85,82] -> 100352B (0.10MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[86] -> 50176B (0.05MB)
pre-vmt.js:11 [WASM] Node  13 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[86,44,7] -> 54528B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[87] -> 200704B (0.19MB)
pre-vmt.js:11 [WASM] Node  14 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[87,72] -> 200736B (0.19MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[88] -> 230400B (0.22MB)
pre-vmt.js:11 [WASM] Node  15 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[88,45,8] -> 232960B (0.22MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[89] -> 50176B (0.05MB)
pre-vmt.js:11 [WASM] Node  16 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[89,46,27] -> 56416B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[90] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  17 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[90,47,9] -> 33216B (0.03MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[91] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  18 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[91,48,10] -> 118656B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[92] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  19 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[92,49,28] -> 126816B (0.12MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[93] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  20 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[93,90] -> 37632B (0.04MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[94] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  21 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[94,50,11] -> 33216B (0.03MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[95] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  22 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[95,51,12] -> 118656B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[96] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  23 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[96,52,29] -> 126816B (0.12MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[97] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  24 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[97,94] -> 37632B (0.04MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[98] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  25 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[98,53,13] -> 26016B (0.02MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[99] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  26 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[99,54,14] -> 59328B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[100] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  27 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[100,55,30] -> 63456B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[101] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  28 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[101,98] -> 37632B (0.04MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[102] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  29 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[102,56,15] -> 26016B (0.02MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[103] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  30 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[103,57,16] -> 59328B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[104] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  31 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[104,58,31] -> 63456B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[105] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  32 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[105,102] -> 37632B (0.04MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[106] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  33 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[106,59,17] -> 33216B (0.03MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[107] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  34 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[107,72] -> 112928B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[108] -> 147456B (0.14MB)
pre-vmt.js:11 [WASM] Node  35 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[108,60,18] -> 153216B (0.15MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[109] -> 28224B (0.03MB)
pre-vmt.js:11 [WASM] Node  36 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[109,61,32] -> 56064B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[110] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  37 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[110,62,19] -> 65856B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[111] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  38 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[111,63,20] -> 67968B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[112] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  39 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[112,64,33] -> 111936B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[113] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  40 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[113,110] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[114] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  41 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[114,65,21] -> 65856B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[115] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  42 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[115,66,22] -> 67968B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[116] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  43 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[116,67,34] -> 111936B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[117] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  44 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[117,114] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[118] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  45 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[118,68,23] -> 65856B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[119] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  46 Operator Builtin Code  40 MEAN (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[119,69] -> 56456B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[120] -> 1152B (0.00MB)
pre-vmt.js:11 [WASM]   4 Temporary Tensors:[123-126] -> 1176B (0.00MB)
pre-vmt.js:11 [WASM] Node  47 Operator Builtin Code   9 FULLY_CONNECTED (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[120,73,71] -> 205764B (0.20MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[121] -> 708B (0.00MB)
pre-vmt.js:11 [WASM] Node  48 Operator Builtin Code  22 RESHAPE (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[121,70] -> 720B (0.00MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[122] -> 708B (0.00MB)
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] Execution plan as the list of 49 nodes invoked in-order: [0-48]
pre-vmt.js:11 [WASM] --------------Subgraph-0 dump has completed--------------
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] --------------Memory Arena Status Start--------------
pre-vmt.js:11 [WASM] Total memory usage: 3372848 bytes (3.217 MB)
pre-vmt.js:11 [WASM] - Total arena memory usage: 3372848 bytes (3.217 MB)
pre-vmt.js:11 [WASM] - Total dynamic memory usage: 0 bytes (0.000 MB)
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] Subgraph#0   Arena (Normal)        3372720 (100.00%)
pre-vmt.js:11 [WASM] Subgraph#0   Arena (Persistent)        128 (0.00%)
pre-vmt.js:11 [WASM] --------------Memory Arena Status End--------------


vmt.wasm:0x31cff Uncaught RuntimeError: memory access out of bounds
    at vmt.wasm:0x31cff
    at vmt.wasm:0x1f7a94
    at vmt.wasm:0x3c4910
    at vmt.wasm:0x65ace
    at vmt.wasm:0x231c3e
    at vmt.wasm:0x458a49
    at vmt.wasm:0x517c60
    at Module._landmarkDetection (vmt.js:6100:85)
    at VmtHelper.cycleForSingleImage (vmt-helper.js:115:29)
    at img.onload (index.html:772:28)
```
"
61233,site/en/guide/create_op.md example code has memory leak,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.6.5

### Custom code

Yes

### OS platform and distribution

linux centos 7.6

### Mobile device

linux centos 7.6

### Python version

3.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?


In the demo code, Tensor* **output_tensor** was allocated but the memory was not free

`#include ""tensorflow/core/framework/op_kernel.h""

using namespace tensorflow;

class ZeroOutOp : public OpKernel {
 public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);
    auto input = input_tensor.flat<int32>();

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));
    auto output_flat = output_tensor->flat<int32>();

    // Set all but the first element of the output tensor to 0.
    const int N = input.size();
    for (int i = 1; i < N; i++) {
      output_flat(i) = 0;
    }

    // Preserve the first input value if possible.
    if (N > 0) output_flat(0) = input(0);
  }
};`

### Standalone code to reproduce the issue

```shell
Repeatedly call the ZeroOutOp. You'll see the memory continue to increase
```


### Relevant log output

_No response_"
61232,Request for the implementation of nanmedian function,"Tensorflow doesn't have `nanmedian` function. on the other hand, numpy support it nanmedian: [numpy nanmedian](https://numpy.org/doc/stable/reference/generated/numpy.nanmedian.html). 
we should add this function on `numpy` layer of Tensorflow."
61231,Support for empty GPU batches during distributed training,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When doing `distributed training` using `MirroredStrategy`, one may encounter empty GPU batches when `drop_remainder=False` during dataset construction.

From #44348 , it seems it is a long-standing issue. 

For the last few batches of data it is possible that some replica workers receive an empty tensor as input ( see [here](https://www.tensorflow.org/tutorials/distribute/input#batching) for an example !!! ). 

Either one must set `drop_remainder=True` or consider not using `@tf.function` because you would have to add a conditional statement in the `train_step` function which won't work with `tf.function`. 

So, from the point of efficiency, `drop_remainder=True` seems the only option. In some applications or critical experiments, one would not like to drop the remainder data  for very precise and reproducible quantitative analysis.

So, can there be a support for handling empty GPU batches in distributed mode ?

### Standalone code to reproduce the issue

```shell
`tf.data.Dataset.range(8).batch(4)` over 3 replicas, results in the following output:

Batch 1:

    Replica 1: [0, 1]
    Replica 2: [2, 3]
    Replica 3: []

Batch 2:

    Replica 1: [4, 5]
    Replica 2: [6, 7]
    Replica 3: []

Without `drop_remainder=True`  this will cause an incompatible shape error when doing a forward pass.
```


### Relevant log output

_No response_"
61228,ctc_ops.py deprecation warning,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

WARNING:tensorflow:From /home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow/python/ops/ctc_ops.py:1514: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.

### Standalone code to reproduce the issue

```shell
I suspect any call to tf.nn.ctc_loss
```


### Relevant log output

_No response_"
61227,Tensor shapes is different when loading from tf.lite.Interpreter and from flatbuffer from python,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- TensorFlow installation (pip package or built from source): pip package, python3.8
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

`# Download tflite model from https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/classification/5

# Case-1 Load tflite from Interpreter
import tensorflow as tf
model_path = ""lite-model_imagenet_mobilenet_v3_large_075_224_classification_5_default_1.tflite""
interpreter = tf.lite.Interpreter(model_path=model_path)
interpreter.allocate_tensors()

tensor_details = interpreter.get_tensor_details()
print(""Read Tensor Shapes From Interpreter Module"")
print(tensor_details[140]['name'])
print(tensor_details[140]['shape'])

# Case-2 Load tflite from flatbuffer
from tensorflow.lite.tools.flatbuffer_utils import read_model

model = read_model(model_path)
tensor = model.subgraphs[0].tensors[140]
print(""Read Tensor Shapes From Flatbuffer Module"")
print(tensor.name)
print(tensor.shape)
`

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

It prints the different shapes.

`Read Tensor Shapes From Interpreter Module
predict/MobilenetV3/expanded_conv/add
[  1 112 112  16]
Read Tensor Shapes From Flatbuffer Module
b'predict/MobilenetV3/expanded_conv/add'
[ 1  1  1 16]
`

### 4. (optional) RNN conversion support


### 5. (optional) Any other info / logs

I thought it was a problem caused by the schema_generate.py file not being updated. So I tried recompiling schema.fbs, but still the problem was not solved.
"
61226,"TensorFlow 2.10.0  build for C++ using Bazel failed with error ""Link.exe 1120 failed: error executing command "" in Windows 10 ","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.10.0

### Custom code

No

### OS platform and distribution

Windows 10 Pro

### Mobile device

Windows 10 Pro

### Python version

3.9.13

### Bazel version

5.1.1

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

_No response_

### Current behavior?

Steps followed:
1. Installed Bazel version 5.1.1 and set the environment variables.
2. Installed msys2 and set the environment variables.
3. Installed visual studio professional 2019 for C++ build .
4. Installed python 3.9.13
5. Installed python dependencies like numpy, wheel, keras
6. Downloaded tensorflow 2.10.0 source from tensorflow github repository
7. Configured TensorFlow with default configuration using ‘./configure’
8. Build TensorFlow using the command 
              ‘bazel --host_jvm_args=-Xmx4g --output_base=D:\0 build --jvmopt=""-server -Xms2g"" --config=opt tensorflow:tensorflow_cc’

While tried to build ‘tensorflow_cc’ with the above command, the compilation is success but the linking failed with error 1120.

The following versions of tools are used for building:
Tensorflow        - 2.10.0
Python - 3.9.13
Visual studio - Professional 2019
Bazel     - 5.1.1
MSYS    - 2
Tried the GPU build with ‘Cuda 11.2 and Cudnn 8.1’ also but the result is same.

Note: Earlier tried to build tensorflow.dll (not tensorflow_cc), for which the compilation and linking was success and tensorflow.dll got generated.
But while linking the tensorflow.dll to visual studio cpp project, I got 'unresolved external symbol errors'. It seems the dll generated by this method has the ‘C’ symbols instead of ‘CPP’ symbols in it.


### Standalone code to reproduce the issue

```shell
Tensorflow source from github and tried to build from windows for c++.
```


### Relevant log output

```shell
Getting link errors
```
"
61225,TensorFlow GPU,"New versions of TensorFlow no longer support Windows native Gpus.

If built in WSL2

Is there a shortcut to read data from Windows directory in WSL2?

Because I need to use TensorFlow GPU to compute a lot of data.
Copying to WSL2 is slow"
61222,tensor,https://github.com/expo/expo/actions/workflows/cli.yml
61220,tf.image.adjust_gamma outputs incorrect error message for string input,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When giving tf.image.adjust_gamma a string inputs, it outputs misleading error messages:
```
ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).
```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
t = tf.constant([], dtype=tf.string)
i = tf.image.adjust_gamma(t, gamma=0.1)
```
```


### Relevant log output

_No response_"
61219,"savedmodel convert to tflite and merge labels.  tx to new tflite model,but resulte is error by Xcode,  use python api is correct","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

mac os 12.6

### Mobile device

ios 16.1

### Python version

3.10.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

convert savedmodel to tflite in ios is error

model download  url  https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/classification/5

convert steps

1.
<img width=""996"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/7971897/1a8312fc-7d45-4e6e-97c2-d5ef02979082"">

2.
<img width=""530"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/7971897/5116798e-cb0d-40f5-b937-f964bb15e284"">

script is use  offical
metadata_writer_for_image_classifier.py


use convert savedmodel to convert tflite （merge  labels.txt and  tflite）

python api  result  is correct
<img width=""569"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/7971897/6b4ebb76-99a9-4d8f-8bcf-e87192bf80a8"">

ios is error  （self-converted）

<img width=""925"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/7971897/0bfe3490-c3bf-4a49-9696-1b3be7363e30"">

ios is correct （download tflite is correct ）

<img width=""796"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/7971897/c974e3d1-6582-4818-b50e-ec62548fc7e5"">




### Standalone code to reproduce the issue

```shell
ios code：

<img width=""1075"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/7971897/c3618d30-3dae-4a85-a3d2-dcf0f340afe3"">


python code：


def classify_image_tflite_no_sin(model_path, predicted_image_path, labels_path):


    TF_MODEL_FILE_PATH = model_path
    interpreter = tf.lite.Interpreter(model_path=TF_MODEL_FILE_PATH)
    interpreter.allocate_tensors()
    
    # 加载标签文件
    with open(labels_path, 'r') as f:
        labels = f.read().splitlines()

    # 读取和预处理图像
    image_path = predicted_image_path
    image = Image.open(image_path).resize((224, 224))  # 调整图像大小
    image = np.array(image)  # 将图像转换为NumPy数组
    image = image / 255.0  # 归一化图像
    image = np.expand_dims(image, axis=0).astype(np.float32)  # 添加批次维度并转换为float32

    # 设置模型输入和输出张量
    input_tensor_index = interpreter.get_input_details()[0]['index']
    output_tensor_index = interpreter.get_output_details()[0]['index']

    # 设置输入张量的值
    interpreter.set_tensor(input_tensor_index, image)

    # 执行推断
    interpreter.invoke()

    # 获取输出张量的结果
    output_data = interpreter.get_tensor(output_tensor_index)
    
    score_lite = tf.nn.softmax(output_data)

    # 获取预测类别索引
    class_index = np.argmax(score_lite)
    predicted_label = labels[class_index]
    confidence = score_lite[0][class_index]

    # 输出预测结果
    print('预测类别：', predicted_label)
    print('预测准确度：', confidence)
```
```


### Relevant log output

_No response_"
61216,Unit test build failures,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04.1

### Mobile device

_No response_

### Python version

3.8.8

### Bazel version

6.1.0

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The following unit tests fail to build when run with AVX512 flag:

//tensorflow/python/tools:aot_compiled_test
//tensorflow/python/tools:aot_compiled_x_matmul_y_large_multithreaded
//tensorflow/python/tools:aot_compiled_vars_and_arithmetic
//tensorflow/python/tools:aot_compiled_x_matmul_y_small
//tensorflow/python/tools:aot_compiled_vars_and_arithmetic_frozen
//tensorflow/python/tools:aot_compiled_x_plus_y
//tensorflow/python/tools:aot_compiled_x_matmul_y_large

The first bad commit: **4f5adb47ff6df8a55e11c8520882c18b75e7f9dc**


### Standalone code to reproduce the issue

```shell
The following command reproduces the build failure:

bazel test --copt=-march=skylake --copt=-mavx512f //tensorflow/python/tools:aot_compiled_x_matmul_y_small

The same command was used for other mentioned tests. 

The change from avx512 to avx2 resolves the error and the build completes successfully. Following is the command used for it:
bazel test --copt=-march=broadwell --copt=-mavx2 //tensorflow/python/tools:aot_compiled_x_matmul_y_small
```


### Relevant log output

```shell
27 | struct gemm_pack_rhs;
      |        ^~~~~~~~~~~~~
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:96,
                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_conv_impl.h:18,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:19:
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h: In instantiation of 'void Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::packRhs(RhsScalar**, const typename RhsMapper::SubMapper&, StorageIndex, StorageIndex) [with ResScalar = float; LhsScalar = float; RhsScalar = float; StorageIndex = long int; OutputMapper = Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>; LhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 16, false, false, 0, Eigen::MakePointer>; RhsMapper = Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > > > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 16, false, false, 0, Eigen::MakePointer>; Eigen::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::RhsBlock = float*; typename RhsMapper::SubMapper = Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > > > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 16, false, false, 0, Eigen::MakePointer>]':
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:893:11:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemmPartial(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*, Eigen::TensorContractionEvaluatorBase<Derived>::Index, Eigen::TensorContractionEvaluatorBase<Derived>::Index, int) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = false; int Alignment = 0; bool use_output_kernel = true; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > > > >, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float; Eigen::TensorContractionEvaluatorBase<Derived>::Index = long int]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:783:5:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalGemm(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = false; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > > > >, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:722:7:   required from 'void Eigen::TensorContractionEvaluatorBase<Derived>::evalProductSequential(Eigen::TensorContractionEvaluatorBase<Derived>::Scalar*) const [with bool lhs_inner_dim_contiguous = false; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = false; int Alignment = 0; Derived = Eigen::TensorEvaluator<const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > > > >, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::NoOpOutputKernel>, Eigen::DefaultDevice>; Eigen::TensorContractionEvaluatorBase<Derived>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:1012:5:   required from 'void Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::evalProduct(Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar*) const [with int Alignment = 0; Indices = const Eigen::array<Eigen::IndexPair<long int>, 1>; LeftArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > > > >; RightArgType = const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >; OutputKernelType = const Eigen::NoOpOutputKernel; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, Device_>::Scalar = float]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:702:4:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:162:44:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType) [with NewDimensions = const Eigen::DSizes<long int, 4>; ArgType = const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > > > >, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::NoOpOutputKernel>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:154:62:   required from 'bool Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalSubExprsIfNeeded(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType) [with LeftArgType = Eigen::TensorChippingOp<-1, Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer> > >; RightArgType = const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > > > >, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::NoOpOutputKernel> >; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType = float*]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:189:16:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, Vectorizable, Eigen::internal::On>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorChippingOp<-1, Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > > > >, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::NoOpOutputKernel> > >; bool Vectorizable = true]'
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:39:62:   required from 'Eigen::TensorDevice<ExpressionType, DeviceType>& Eigen::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > > > >, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, const Eigen::NoOpOutputKernel> >; ExpressionType = Eigen::TensorChippingOp<-1, Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16, Eigen::MakePointer> > >; DeviceType = Eigen::DefaultDevice]'
./tensorflow/compiler/xla/service/cpu/runtime_conv_impl.h:92:68:   required from 'void tensorflow::xla::EigenConv2DImpl(const EigenDevice&, ScalarType*, ScalarType*, ScalarType*, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index, Eigen::Index) [with EigenDevice = Eigen::DefaultDevice; ScalarType = float; Eigen::Index = long int]'
tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:58:26:   required from here
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:249:5: error: invalid use of incomplete type 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 16, false, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > > > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 16, false, false, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > > > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 16, false, false, 0, Eigen::MakePointer>, 8, 0, false, false>'}
  249 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols);
      |     ^~~~~~~~~~~
In file included from external/eigen_archive/Eigen/Core:303,
                 from ./third_party/eigen3/Eigen/Core:1,
                 from ./tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.h:21,
                 from tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc:16:
external/eigen_archive/Eigen/src/Core/util/BlasUtil.h:27:8: note: declaration of 'Eigen::internal::TensorContractionKernel<float, float, float, long int, Eigen::internal::blas_data_mapper<float, long int, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long int, 1, Eigen::TensorEvaluator<const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 16, false, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > > > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 16, false, false, 0, Eigen::MakePointer> >::RhsPacker' {aka 'struct Eigen::internal::gemm_pack_rhs<float, long int, Eigen::internal::TensorContractionSubMapper<float, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorChippingOp<-1, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 5>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16, Eigen::MakePointer> > > > >, Eigen::DefaultDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 16, false, false, 0, Eigen::MakePointer>, 8, 0, false, false>'}
   27 | struct gemm_pack_rhs;
      |        ^~~~~~~~~~~~~
Target //tensorflow/python/tools:aot_compiled_x_matmul_y_small failed to build
```
"
61215,DistributedDatasetInterface is not an attribute of input_lib - engine\data_adapter.py,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm new to machine learning and I was setting up this network with just one layer, with only one neuron inside it, just to see how it works. Setting up went fine. The error occured when I asked it to predict.

I'm not sure what exactly happend, but apparently, a non existent attribute (`input_lib.DistributedDatasetInterface`) was passed to `isinstance()` in the file: ...\site-packages\tensorflow\python\keras\engine\data_adapter.py (line 1699).

I've replaced by the wrong one by the one suggested and worked as expected.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
from tensorflow.python.keras.layers import Dense, Input
from tensorflow.python.keras import Sequential
from tensorflow.python.keras.activations import sigmoid

import logging
logging.getLogger(""tensorflow"").setLevel(logging.ERROR)
tf.autograph.set_verbosity(0)

X_train = np.array([0., 1, 2, 3, 4, 5], dtype=np.float32).reshape(-1,1)
Y_train = np.array([0,  0, 0, 1, 1, 1], dtype=np.float32).reshape(-1,1)

model = Sequential([
    Input(shape=(1,)),
    Dense(1, activation=sigmoid, name = ""L1"")
])
model.summary()

# Setting the weights and bias of the neuron
logistic_layer = model.get_layer('L1')
set_w = np.array([[2]])
set_b = np.array([-4.5])
logistic_layer.set_weights([set_w, set_b])

# Performance test
a1 = model.predict(X_train[0].reshape(1,1)) # this line caused the error
print(a1)
alog = sigmoid(np.dot(set_w,X_train[0].reshape(1,1)) + set_b)
print(alog)
```


### Relevant log output

```shell
...\site-packages\tensorflow\python\keras\engine\data_adapter.py"", line 1699, in _is_distributed_dataset
    return isinstance(ds, input_lib.DistributedDatasetInterface)
AttributeError: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'. Did you mean: 'DistributedDatasetSpec'?
```
"
61214,Convert tf.Tensor into tensorflow::Tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Ubuntu 20

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current behavior?

Is there any way to convert tf.Tensor from python into tensorflow::Tensor C++?

### Standalone code to reproduce the issue

```shell
Actually using pybind11. I can get PyObject* from tf.Tensor and have no idea how to get tensorflow::Tensor* from PyObject*
```


### Relevant log output

_No response_"
61212,"Tensorflow profiler is not showing anything. Gives ""No profile data was found"" text on selecting Profile in Tensorboard","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.12, tf 2.13, tf-nightly

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I was learning how to use TensorFlow Profiler according to the [guide](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras). I ran the same notebook without altering anything in Google Colab and it does now show Profile tab. Moreover on selecting the Profile option from the right hand side drop down list, It shows 

```
No profile data was found.
If you have a model running on CPU, GPU, or Google Cloud TPU, you may be able to use the above button to capture a profile.

If you're a CPU or GPU user, please use the IP address option. You may want to check out the [tutorial](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_profiling_keras.ipynb) on how to start a TensorFlow profiler server and profile a Keras model on a GPU.

If you're a TPU user, please use the TPU name option and you may want to check out the [tutorial](https://cloud.google.com/tpu/docs/cloud-tpu-tools) on how to interpreting the profiling results.

If you think profiling is done properly, please see the page of [Google Cloud TPU Troubleshooting and FAQ](https://cloud.google.com/tpu/docs/troubleshooting) and consider filing an issue on GitHub.

```

Instead it should have shown the Profile

### Standalone code to reproduce the issue

```shell
Kindly use the official [tensorflow profiler guide](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras)
```


### Relevant log output

```shell
No profile data was found.
If you have a model running on CPU, GPU, or Google Cloud TPU, you may be able to use the above button to capture a profile.

If you're a CPU or GPU user, please use the IP address option. You may want to check out the [tutorial](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_profiling_keras.ipynb) on how to start a TensorFlow profiler server and profile a Keras model on a GPU.

If you're a TPU user, please use the TPU name option and you may want to check out the [tutorial](https://cloud.google.com/tpu/docs/cloud-tpu-tools) on how to interpreting the profiling results.

If you think profiling is done properly, please see the page of [Google Cloud TPU Troubleshooting and FAQ](https://cloud.google.com/tpu/docs/troubleshooting) and consider filing an issue on GitHub.

```
```
"
61211,tf.keras.models.model_from_json() missing a safe_mode parameter,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When loading a model with a lambda layer from a json model config, TensorFlow provides the following error:

`
ValueError: Requested the deserialization of a Lambda layer with a Python 'lambda' inside it. This carries a potential risk of arbitrary code execution and thus it is disallowed by default. If you trust the source of the saved model, you can pass 
'safe_mode=False' to the loading function in order to allow Lambda layer loading.
`

However `tf.keras.models.model_from_json()` does not have a `safe_mode` parameter. So there does not seem to be a way to load models with lamda layers using a json config. This issue does not appear in TensorFlow 2.12



### Standalone code to reproduce the issue

```shell
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Input, Lambda

inputs = Input(shape=(1,))
x = Lambda(lambda x: x*2)(inputs)
out = Dense(1)(x)
model = Model(inputs=inputs,outputs=out)

model_config = model.to_json()
tf.keras.models.model_from_json(model_config)
```


### Relevant log output

_No response_"
61210,Tensorflow Profiler does not work on WSL2: Failed to load libcupti (is it installed and accessible?) ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom code

Yes

### OS platform and distribution

Windows 10 WSL Ubuntu

### Mobile device

Ubuntu 22.04

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

_No response_

### Current behavior?

After following exactly the steps mentioned in https://www.tensorflow.org/install/pip for installing Tensorflow on WSL2, and installing the latest version of the profiler plugin, the Tensorboard profiler does not seem to work.

This is with a fresh WSL2 install, miniconda install, etc.

![image](https://github.com/tensorflow/tensorflow/assets/11645696/adbaf3c1-4f03-43fe-80d7-39c484ac91a7)

```
Failed to load libcupti (is it installed and accessible?) 

No step marker observed and hence the step time is unknown. This may happen if (1) training steps are not instrumented (e.g., if you are not using Keras) or (2) the profiling duration is shorter than the step time. For (1), you need to add step instrumentation; for (2), you may try to profile longer. 
```

The problem does not seem to be that lubcupti fails to load (despite what is indicated by Tensorboard); libcupti seems to be found just fine, but there may be some problems - See the attached log output for possible clues as to what's happening. 

### Standalone code to reproduce the issue

```shell
# The below code is copied directly from https://github.com/keras-team/keras-io/blob/master/examples/vision/mnist_convnet.py - with the single addition of adding Tensorboard profiling.

import numpy as np
from tensorflow import keras
from tensorflow.keras import layers

""""""
## Prepare the data
""""""

# Model / data parameters
num_classes = 10
input_shape = (28, 28, 1)

# Load the data and split it between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Scale images to the [0, 1] range
x_train = x_train.astype(""float32"") / 255
x_test = x_test.astype(""float32"") / 255
# Make sure images have shape (28, 28, 1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
print(""x_train shape:"", x_train.shape)
print(x_train.shape[0], ""train samples"")
print(x_test.shape[0], ""test samples"")


# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

""""""
## Build the model
""""""

model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation=""softmax""),
    ]
)

model.summary()

""""""
## Train the model
""""""

batch_size = 128
epochs = 15

model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])

model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[
    keras.callbacks.TensorBoard(profile_batch=[20, 30])
])

""""""
## Evaluate the trained model
""""""

score = model.evaluate(x_test, y_test, verbose=0)
print(""Test loss:"", score[0])
print(""Test accuracy:"", score[1])
```


### Relevant log output

```shell
17/422 [>.............................] - ETA: 2s - loss: 2.0755 - accuracy: 0.38602023-07-07 15:45:09.034256: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2023-07-07 15:45:09.034284: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2023-07-07 15:45:09.034300: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.
2023-07-07 15:45:09.034304: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:186] cuptiSubscribe: ignored due to a previous error.
2023-07-07 15:45:09.034307: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error.
2023-07-07 15:45:09.034309: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1730] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error 
 25/422 [>.............................] - ETA: 2s - loss: 1.8839 - accuracy: 0.46882023-07-07 15:45:09.105818: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2023-07-07 15:45:09.105940: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:142] cuptiFinalize: ignored due to a previous error.
2023-07-07 15:45:09.105943: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error.
2023-07-07 15:45:09.105945: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1822] function cupti_interface_->Finalize()failed with error 
2023-07-07 15:45:09.107652: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.
2023-07-07 15:45:09.107660: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.
2023-07-07 15:45:09.107663: I tensorflow/compiler/xla/backends/profiler/gpu/cupti_collector.cc:541]  GpuTracer has collected 0 callback api events and 0 activity events. 
2023-07-07 15:45:09.107809: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
```
"
61209,My customized OP gives incorrect outputs on GPUs since `tf-nightly 2.13.0.dev20230413`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

fedora 36

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have a complex program based on TensorFlow with several customized OPs. These OPs were created following https://www.tensorflow.org/guide/create_op. Yesterday TF 2.13.0 was released, but after I upgraded to 2.13.0, I found that one of my customized OP gives incorrect results on GPUs and still has the correct outputs on CPUs.

Then I tested many `tf-nightly` versions and found that `tf-nightly 2.13.0.dev20230412` works but `tf-nightly 2.13.0.dev20230413` fails. So the situation is shown in the following table:
| version | CPU       | GPU          |
| -------- | --------- | ----------- |
| tensorflow 2.12.0 | Correct  | Correct      |
| tensorflow 2.13.0 | Correct  | Incorrect   |
| tf-nightly  2.13.0.dev20230412 | Correct  | Correct      |
| tf-nightly  2.13.0.dev20230413 | Correct      | Incorrect   |

I'd like to know what changed between April 12th and 13th related to the customized OPs. This can be a breaking change to downstream applications or an internal bug. Thanks!

Here is a quick link for commits between April 12th and 13th:
https://github.com/tensorflow/tensorflow/commits/master?before=525da8a93eca846e32e5c41eddc0496b25a2ef5b+770


### Standalone code to reproduce the issue

```shell
Indeed, the reason is still unclear to me, so it is hard to create a minimal example.

The code of our customized OPs is https://github.com/deepmodeling/deepmd-kit/blob/37fd8d193362f91c925cf7c2f3a58b97dc921b27/source/op/prod_force_multi_device.cc#L49-L166
```


### Relevant log output

_No response_"
61204,import of DistributedDatasetInterface not valid anymore,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf.2.13.0

### Custom code

No

### OS platform and distribution

Mac OS Ventura 13.4

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Instantiating `DataHandler`  from `tensorflow.python.keras.engine.data_adapter` throws an error 

`AttributeError: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'`

The type `DistributedDatasetInterface` can now be found in [‎tensorflow/python/types/distribute.py](https://github.com/tensorflow/tensorflow/blob/9a1f8bb2ce3de3f57c9c141f23b9c8e4faeea387/tensorflow/python/types/distribute.py#L308)

### Standalone code to reproduce the issue

```shell
link: https://colab.research.google.com/drive/11rHWrwrAjbVzKjM6KZBBp_KemTOyf2Pp?usp=sharing

code:
!pip install -U tensorflow==2.13.0
from tensorflow.python.keras.engine import data_adapter
from tensorflow import range as tfrange
data_handler = data_adapter.DataHandler(x=tfrange(10))
```


### Relevant log output

```shell
/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)
   1150       self._steps_per_execution_value = steps_per_execution.numpy().item()
   1151 
-> 1152     adapter_cls = select_data_adapter(x, y)
   1153     self._adapter = adapter_cls(
   1154         x,

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in select_data_adapter(x, y)
    986 def select_data_adapter(x, y):
    987   """"""Selects a data adapter than can handle a given x and y.""""""
--> 988   adapter_cls = [cls for cls in ALL_ADAPTER_CLS if cls.can_handle(x, y)]
    989   if not adapter_cls:
    990     # TODO(scottzhu): This should be a less implementation-specific error.

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in <listcomp>(.0)
    986 def select_data_adapter(x, y):
    987   """"""Selects a data adapter than can handle a given x and y.""""""
--> 988   adapter_cls = [cls for cls in ALL_ADAPTER_CLS if cls.can_handle(x, y)]
    989   if not adapter_cls:
    990     # TODO(scottzhu): This should be a less implementation-specific error.

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in can_handle(x, y)
    705   def can_handle(x, y=None):
    706     return (isinstance(x, (data_types.DatasetV1, data_types.DatasetV2)) or
--> 707             _is_distributed_dataset(x))
    708 
    709   def __init__(self,

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _is_distributed_dataset(ds)
   1697 
   1698 def _is_distributed_dataset(ds):
-> 1699   return isinstance(ds, input_lib.DistributedDatasetInterface)

AttributeError: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'
```
"
61201,Issue with Reproducible Results: Inconsistent Behavior of Random Seeds in TensorFlow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.8-3.9-3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I would like to report an issue regarding the reproducibility of results in TensorFlow. Currently, in order to achieve consistent and deterministic results, it seems necessary to set both random.seed(42) and tf.random.set_seed(1) together.

Expected Behavior:
Setting tf.random.set_seed(1) alone should be sufficient to ensure reproducible results across different runs.

Observed Behavior:
Without setting random.seed(42) alongside tf.random.set_seed(1), the results obtained from TensorFlow exhibit inconsistency and do not remain fixed between runs.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import random

tf.random.set_seed(1)
random.seed(42) # This line seems to be redundant

x = tf.constant(tf.random.uniform([2, 3, 2]), dtype=tf.float32)
```


### Relevant log output

_No response_"
61200,Building TFLite for WASM using Bazel,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Emscripten

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.3.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We are trying to compile TFLite libraries for WASM that we can use in our C++ project that later will be compiled to WASM package as well. We have forked a Tensorflow repository and did these modifications for Bazel + WASM - https://github.com/af-filby/tensorflow/commit/2defd39b957a73828b4791e48883e874a97b4bb4 and we try to build with this command:

`bazel build --config=wasm -c opt //tensorflow/lite:tensorflowlite`

The building process went smooth and we as an output we got `libtensorflowlite2.so` which I think is expected. BUT, the problem is that this `.so` file is only `50 KB` in size, and if we try to link it in our CMake we get an error `Unable to find library -ltensorflowlite2`

Could you please review our Bazel config and advise what we did wrong so we can finish this compilation successfully?

### Standalone code to reproduce the issue

```shell
No need for this field
```


### Relevant log output

_No response_"
61199,gradient returns `None`,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.7.1

### Custom code

Yes

### OS platform and distribution

CentOS Linux 7 (Core)

### Mobile device

_No response_

### Python version

3.8.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Running on CPU only

### Current behavior?

In the code below I don't understand why `test_tape` which is a `tf.GradientTape()` returns an empty (`None`) gradient. I have made sure to call the models so weights are initialized, checked that losses are actual tensors, redefined inputs as `tf.Variable()` and alternatively attempted using `tape.watch()`. My intuition is that the first call to `train_optimizer.apply_gradients()` somehow breaks the computational graph which is why `test_tape` does not track variables associated with `model_copy` but I am not sure how to fix the issue.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.backend as keras_backend
import numpy as np

def sineGenerator(amplitude=None, phase=None):
    if amplitude is None:
        amplitude = tf.random.uniform(shape=[], minval=0.1, maxval=5.0)
    if phase is None: 
        phase     = tf.random.uniform(shape=[], minval=0.0, maxval=np.pi)
    def _gen(x):
        return amplitude*tf.math.sin(x+phase)
    return _gen
    
def genX(sample, minval=-5.0, maxval=5.0):
    return tf.expand_dims(tf.random.uniform(shape=[sample], minval=minval, maxval=maxval), 1)

class sineModel(keras.Model):
    def __init__(self):
        super().__init__()
        self.hidden1 = keras.layers.Dense(40, input_shape=(1,))
        self.hidden2 = keras.layers.Dense(40)
        self.out     = keras.layers.Dense(1)
        
    def call(self, x):
        x = keras.activations.relu(self.hidden1(x))
        x = keras.activations.relu(self.hidden2(x))
        x = self.out(x)
        return x
    
def copyModel(model, x):
    model_copy = sineModel()
    output = model_copy(x)
    output = model(x) # should not be necessary since model(train_x) is called before model_copy = copyModel(model,x) in maml_train()
    model_copy.set_weights(model.get_weights())
    return model_copy


def maml_train(model, total_iterations=10, meta_train_steps=10, meta_test_steps=100):
    log_step = total_iterations //10 if total_iterations > 10 else 1
    
    test_optimizer  = keras.optimizers.Adam(learning_rate=0.001)
    train_optimizer = keras.optimizers.Adam(learning_rate=0.001)
    
    losses, total_loss = [], 0.
    
    for step in range(total_iterations):
        sineGen = sineGenerator()
        
        test_x     = genX(meta_test_steps)
        test_y     = sineGen(test_x)
        
        train_x    = genX(meta_train_steps)
        train_y    = sineGen(train_x)
        
#         test_x, test_y, train_x, train_y = tf.Variable(test_x), tf.Variable(test_y), tf.Variable(train_x), tf.Variable(train_y)
#         test_x, test_y, train_x, train_y = tf.convert_to_tensor(test_x), tf.convert_to_tensor(test_y), tf.convert_to_tensor(train_x), tf.convert_to_tensor(train_y)
        
        model(train_x)
        model_copy = copyModel(model, train_x)
        
        
        with tf.GradientTape() as test_tape:
#             test_tape.watch([test_x, test_y, train_x, train_y])
            with tf.GradientTape() as train_tape:
#                 train_tape.watch([test_x, test_y, train_x, train_y])
                train_loss = tf.reduce_mean(keras.losses.mean_squared_error(train_y, model(train_x)))
            
#             print('train_loss type', type(train_loss))
            gradients = train_tape.gradient(train_loss, model.trainable_variables)
            train_optimizer.apply_gradients(zip(gradients, model_copy.trainable_variables))
            
            test_loss = tf.reduce_mean(keras.losses.mean_squared_error(test_y, model_copy(test_x)))
            
#         print('test_loss type', type(test_loss))
        gradients = test_tape.gradient(test_loss, model.trainable_variables)
        print(gradients)
        test_optimizer.apply_gradients(zip(gradients, model.trainable_variables))

        total_loss += test_loss
        losses += [total_loss/(step+1)]
        
        
        if step % log_step == 0:
             print('Training loss (total) at step %d: \t %.4f' % (step, total_loss/(step+1)))
    return losses

sModel_maml = sineModel()
maml_train(sModel_maml)
```


### Relevant log output

```shell
[None, None, None, None, None, None]

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-402-4ed8f16df6b0> in <module>
     76 
     77 sModel_maml = sineModel2()
---> 78 maml_train(sModel_maml)

<ipython-input-402-4ed8f16df6b0> in maml_train(model, total_iterations, meta_train_steps, meta_test_steps)
     65         gradients = test_tape.gradient(test_loss, model.trainable_variables)
     66         print(gradients)
---> 67         test_optimizer.apply_gradients(zip(gradients, model.trainable_variables))
     68 
     69         total_loss += test_loss

~/pyenvs/tensorflow2/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)
    631       RuntimeError: If called in a cross-replica context.
    632     """"""
--> 633     grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)
    634     var_list = [v for (_, v) in grads_and_vars]
    635 

~/pyenvs/tensorflow2/lib/python3.8/site-packages/keras/optimizer_v2/utils.py in filter_empty_gradients(grads_and_vars)
     71   if not filtered:
     72     variable = ([v.name for _, v in grads_and_vars],)
---> 73     raise ValueError(f""No gradients provided for any variable: {variable}. ""
     74                      f""Provided `grads_and_vars` is {grads_and_vars}."")
     75   if vars_with_empty_grads:

ValueError: No gradients provided for any variable: (['sine_model2_59/dense_555/kernel:0', 'sine_model2_59/dense_555/bias:0', 'sine_model2_59/dense_556/kernel:0', 'sine_model2_59/dense_556/bias:0', 'sine_model2_59/dense_557/kernel:0', 'sine_model2_59/dense_557/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'sine_model2_59/dense_555/kernel:0' shape=(1, 40) dtype=float32, numpy=....
```
"
61198,TensorFlow-MKL: How to enable oneddn in tensorflow for eltwise op,"@penpornk , I have gone through the https://github.com/tensorflow/community/blob/master/rfcs/20210930-enable-onednn-ops.md . So I wanted to understand the latest tensorflow supports eltwise onednn flow and also is there any document which I can refer for the onednn ops which are supported in latest tensorflow."
61197,How to use keras layers to augment both image and label data in image segmentation tasks？,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.6

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

python 3.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

How to use tool a to enhance both image and label data in image segmentation tasks.
I saw an example in the official document of using Keras layer for data augmentation, which is very useful in the training process of classification models because the data augmentation does not require synchronized operations on labels. However, in the segmentation task, if I embed the data enhancement layer into the model structure, I cannot do operations like rotate 、zoom .etc on the label and image at the same time, because the fit method only feed the original image into the model for inference, which results in the label being isolated from the inference process, so it is impossible to do synchronize Affine transformations with the original image.

Thanks.


### Standalone code to reproduce the issue

```shell
I cannot copy my code from the company computer. I just want to know how to do efficient data augmentation that can apply GPU acceleration and distribute strategy.Thanks.
```


### Relevant log output

_No response_"
61182,Protobuf throwing Type error `TypeError: bases must be types` in m1 mac,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

MacOS monterey

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Trying to import tensorflow producing the following error

```bash
TypeError: bases must be types
```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
TypeError                                 Traceback (most recent call last)
Cell In[1], line 1
----> 1 import tensorflow

File /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/tensorflow/__init__.py:38
     35 import sys as _sys
     36 import typing as _typing
---> 38 from tensorflow.python.tools import module_util as _module_util
     39 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     41 # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.

File /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/tensorflow/python/__init__.py:37
     29 # We aim to keep this file minimal and ideally remove completely.
     30 # If you are adding a new file with @tf_export decorators,
     31 # import it in modules_with_exports.py instead.
     32
     33 # go/tf-wildcard-import
     34 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
     36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
---> 37 from tensorflow.python.eager import context
     39 # pylint: enable=wildcard-import
     40
     41 # Bring in subpackages.
     42 from tensorflow.python import data

File /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/tensorflow/python/eager/context.py:29
     26 from absl import logging
     27 import numpy as np
---> 29 from tensorflow.core.framework import function_pb2
     30 from tensorflow.core.protobuf import config_pb2
     31 from tensorflow.core.protobuf import rewriter_config_pb2

File /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/tensorflow/core/framework/function_pb2.py:5
      1 # -*- coding: utf-8 -*-
      2 # Generated by the protocol buffer compiler.  DO NOT EDIT!
      3 # source: tensorflow/core/framework/function.proto
      4 """"""Generated protocol buffer code.""""""
----> 5 from google.protobuf.internal import builder as _builder
      6 from google.protobuf import descriptor as _descriptor
      7 from google.protobuf import descriptor_pool as _descriptor_pool

File /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/google/protobuf/internal/builder.py:42
     40 from google.protobuf.internal import enum_type_wrapper
     41 from google.protobuf import message as _message
---> 42 from google.protobuf import reflection as _reflection
     43 from google.protobuf import symbol_database as _symbol_database
     45 _sym_db = _symbol_database.Default()

File /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/google/protobuf/reflection.py:51
     33 """"""Contains a metaclass and helper functions used to create
     34 protocol message classes from Descriptor objects at runtime.
     35
   (...)
     45 this file*.
     46 """"""
     48 __author__ = 'robinson@google.com (Will Robinson)'
---> 51 from google.protobuf import message_factory
     52 from google.protobuf import symbol_database
     54 # The type of all Message classes.
     55 # Part of the public interface, but normally only used by message factories.

File /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/google/protobuf/message_factory.py:43
     40 __author__ = 'matthewtoia@google.com (Matt Toia)'
     42 from google.protobuf.internal import api_implementation
---> 43 from google.protobuf import descriptor_pool
     44 from google.protobuf import message
     46 if api_implementation.Type() == 'cpp':

File /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/google/protobuf/descriptor_pool.py:63
     60 import collections
     61 import warnings
---> 63 from google.protobuf import descriptor
     64 from google.protobuf import descriptor_database
     65 from google.protobuf import text_encoding

File /opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/google/protobuf/descriptor.py:47
     45   import binascii
     46   import os
---> 47   from google.protobuf.pyext import _message
     48   _USE_C_DESCRIPTORS = True
     51 class Error(Exception):

TypeError: bases must be types
```
The protobuf version is `3.20.3`
"
61181,Python 3.7.0 is incompitable with tensorflow 2.11.0 ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

Windows 10 Educion

### Mobile device

_No response_

### Python version

3.7.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

![image](https://github.com/tensorflow/tensorflow/assets/7984605/ad719afa-c5bf-4d3d-ba06-5f73d914785f)
Python 3.7.0 throws an exception that it cannot import OrderedDict from typings. The fix was downgrading from tensorflow 2.11.0 to tensorflow 2.10.0. 

- Orderd Dict is supported starting from python 3.7.2: https://docs.python.org/3.7/library/typing.html#typing.OrderedDict
- By default pip installs 2.11.0 for python 3.7.0 which is not supported
- The doc  states that 3.7.0 is supported: https://www.tensorflow.org/install/source#tested_build_configurations

### Standalone code to reproduce the issue

```shell
1) Install python 3.7.0
2) pip install tensorflow==2.11.0
3) inside python shell type: import tensorflow as tf
4) Exception will rise
```


### Relevant log output

_No response_"
61180,//tensorflow/compiler/xla/service/gpu:fusion_merger_test fails on AARCH64,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

//tensorflow/compiler/xla/service/gpu:fusion_merger_test unit test fails when run on AARCH64 machine.

### Standalone code to reproduce the issue

```shell
bazel test --cache_test_results=no --config=mkl_aarch64_threadpool --jobs=75 --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --build_tests_only -- //tensorflow/compiler/xla/service/gpu:fusion_merger_test
```


### Relevant log output

```shell
FAIL: //tensorflow/compiler/xla/service/gpu:fusion_merger_test (see /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/xla/service/gpu/fusion_merger_test/test.log)
INFO: From Testing //tensorflow/compiler/xla/service/gpu:fusion_merger_test:
==================== Test output for //tensorflow/compiler/xla/service/gpu:fusion_merger_test:
[==========] Running 21 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 21 tests from FusionMergerTest
[ RUN      ] FusionMergerTest.MergeSharedFusionInstruction
[       OK ] FusionMergerTest.MergeSharedFusionInstruction (9 ms)
[ RUN      ] FusionMergerTest.MoreMemoryAccessIfFused
[       OK ] FusionMergerTest.MoreMemoryAccessIfFused (1 ms)
[ RUN      ] FusionMergerTest.LessMemoryAccessIfFused
[       OK ] FusionMergerTest.LessMemoryAccessIfFused (1 ms)
[ RUN      ] FusionMergerTest.WillMergeIntoInputFusion
[       OK ] FusionMergerTest.WillMergeIntoInputFusion (1 ms)
[ RUN      ] FusionMergerTest.WillMergeIntoUnfusedConsumer
[       OK ] FusionMergerTest.WillMergeIntoUnfusedConsumer (1 ms)
[ RUN      ] FusionMergerTest.WillNotMergeReduceUnfriendlyLayouts
[       OK ] FusionMergerTest.WillNotMergeReduceUnfriendlyLayouts (0 ms)
[ RUN      ] FusionMergerTest.WillMergeReduceNotTooUnfriendlyLayouts
[       OK ] FusionMergerTest.WillMergeReduceNotTooUnfriendlyLayouts (1 ms)
[ RUN      ] FusionMergerTest.AvoidsLargeFusion
[       OK ] FusionMergerTest.AvoidsLargeFusion (1 ms)
[ RUN      ] FusionMergerTest.WillNotMergeIfFusionEmitterIsInefficient
[       OK ] FusionMergerTest.WillNotMergeIfFusionEmitterIsInefficient (1 ms)
[ RUN      ] FusionMergerTest.WillMergeSliceIntoReusingConsumer
[       OK ] FusionMergerTest.WillMergeSliceIntoReusingConsumer (0 ms)
[ RUN      ] FusionMergerTest.WillMergeExpensiveFusionsIfSavesMemory
[       OK ] FusionMergerTest.WillMergeExpensiveFusionsIfSavesMemory (1 ms)
[ RUN      ] FusionMergerTest.WillMergeExpensiveFusionsWithSingleConsumer
[       OK ] FusionMergerTest.WillMergeExpensiveFusionsWithSingleConsumer (0 ms)
[ RUN      ] FusionMergerTest.WillNotMergeExpensiveFusionsWithReusingConsumer
[       OK ] FusionMergerTest.WillNotMergeExpensiveFusionsWithReusingConsumer (0 ms)
[ RUN      ] FusionMergerTest.NoMergeWithBitcast
[       OK ] FusionMergerTest.NoMergeWithBitcast (1 ms)
[ RUN      ] FusionMergerTest.CostBasedMerge
[       OK ] FusionMergerTest.CostBasedMerge (1 ms)
[ RUN      ] FusionMergerTest.CostBasedNoMerge
[       OK ] FusionMergerTest.CostBasedNoMerge (4 ms)
[ RUN      ] FusionMergerTest.NoMergeBecauseTooManyBasicBlockSplits
[       OK ] FusionMergerTest.NoMergeBecauseTooManyBasicBlockSplits (4 ms)
[ RUN      ] FusionMergerTest.CommonElementwiseUsedParameter
[       OK ] FusionMergerTest.CommonElementwiseUsedParameter (1 ms)
[ RUN      ] FusionMergerTest.IncompatibleNonTrivialHeroes
[       OK ] FusionMergerTest.IncompatibleNonTrivialHeroes (0 ms)
[ RUN      ] FusionMergerTest.DoNotMergeDUSFusions
[       OK ] FusionMergerTest.DoNotMergeDUSFusions (1 ms)
[ RUN      ] FusionMergerTest.MergeDUSFusionWithElementwiseFusion
tensorflow/compiler/xla/service/gpu/fusion_merger_test.cc:1127: Failure
Value of: fusion_merger_.Run(module.get()).value()
  Actual: false
Expected: true
[  FAILED  ] FusionMergerTest.MergeDUSFusionWithElementwiseFusion (0 ms)
[----------] 21 tests from FusionMergerTest (41 ms total)

[----------] Global test environment tear-down
[==========] 21 tests from 1 test suite ran. (42 ms total)
[  PASSED  ] 20 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] FusionMergerTest.MergeDUSFusionWithElementwiseFusion

 1 FAILED TEST
================================================================================
Target //tensorflow/compiler/xla/service/gpu:fusion_merger_test up-to-date:
  bazel-bin/tensorflow/compiler/xla/service/gpu/fusion_merger_test
INFO: Elapsed time: 392.381s, Critical Path: 276.82s
INFO: 1497 processes: 613 internal, 884 local.
INFO: Build completed, 1 test FAILED, 1497 total actions
//tensorflow/compiler/xla/service/gpu:fusion_merger_test                 FAILED in 1.2s
  /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/xla/service/gpu/fusion_merger_test/test.log

Executed 1 out of 1 test: 1 fails locally.
```
"
61179,ValueError: Checkpoint was expecting to be a trackable object (an object derived from `Trackable`),"### Issue type

Bug

### Source

binary

### TensorFlow version

tf 2.10.0

### Custom code

Yes

### OS platform and distribution

Windows 10 Enterprise

### Python version

3.9.16

### Current behavior?

I'm receiving an error when I try to restore the model checkpoint. I've seen a posting on here that's similar, but I think my case is different. Help is very much appreciated!

I'm using a pre-trained object detection model called SSD MobileNet V2 FPNLite 320x320 


### Standalone code to reproduce the issue

```python
import os
import tensorflow as tf
import pandas as pd
import openpyxl
import cv2 
import numpy as np

from object_detection.utils import label_map_util
from object_detection.utils import visualization_utils as viz_utils
from object_detection.builders import model_builder
from object_detection.utils import config_util
from matplotlib import pyplot as plt
from pathlib import Path


os.chdir(r""C:\Users\mill286"")

CUSTOM_MODEL_NAME = 'my_ssd_resnet50_v1_fpn' # *** Enter here the name of the model you trained. ***
files = {
    'PIPELINE_CONFIG':os.path.join('tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config')
}
# Load pipeline config and build a detection model
configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])
detection_model = model_builder.build(model_config=configs['model'], is_training=False)
# Restore checkpoint
ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)
ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'ckpt-54.index')).expect_partial() # *** Replace the number in 'ckpt-XX' with the checkpoint you want to use. ***
```


### Relevant log output

```shell
ValueError                                Traceback (most recent call last)
Cell In[18], line 2
      1 # Restore checkpoint
----> 2 ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)
      3 ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'ckpt-54.index')).expect_partial()

File ~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\checkpoint\checkpoint.py:2142, in Checkpoint.__init__(self, root, **kwargs)
   2140 if isinstance(converted_v, weakref.ref):
   2141   converted_v = converted_v()
-> 2142 _assert_trackable(converted_v, k)
   2144 if root:
   2145   # Make sure that root doesn't already have dependencies with these names
   2146   child = trackable_root._lookup_dependency(k)

File ~\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\checkpoint\checkpoint.py:1562, in _assert_trackable(obj, name)
   1559 def _assert_trackable(obj, name):
   1560   if not isinstance(
   1561       obj, (base.Trackable, def_function.Function)):
-> 1562     raise ValueError(
   1563         f""`Checkpoint` was expecting {name} to be a trackable object (an ""
   1564         f""object derived from `Trackable`), got {obj}. If you believe this ""
   1565         ""object should be trackable (i.e. it is part of the ""
   1566         ""TensorFlow Python API and manages state), please open an issue."")

ValueError: `Checkpoint` was expecting model to be a trackable object (an object derived from `Trackable`), got <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x000001E163D93910>. If you believe this object should be trackable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue.
```
"
61178,Tensorflow detects GPU but uses only CPU,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.10.x

### Custom code

Yes

### OS platform and distribution

Windows x64

### Mobile device

_No response_

### Python version

3.8.16

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

toolkit:11.2.2  cudnn: 8.1.0.77

### GPU model and memory

gtx 1070

### Current behavior?

## Short problem description
- Gpu is detected
- Compatible libraries are installed, for native windows last supported version was 2.10
- Gpu is utilizied with `tf.compat.v1.Session`
- Gpu is not used in `tf.compat.v1.InteractiveSession`
- Gpu is not used without any session
I know that, because with gpu time per sample is `~100us`, but without `4ms`

# Checking gpu visibility
```python
# import tensorflow as tf
# import tensorflow.keras
import keras
import tensorflow as tf
import tensorflow.keras as k2

print(""CPU LIST:"", tf.config.list_physical_devices(""CPU""))
print(""GPU LIST:"", tf.config.list_physical_devices(""GPU""))
print(""Deprecated AVAILABLE:"", tf.test.is_gpu_available())  # Deprecated
print(""Deprecated AVAILABLE:"", tf.test.is_gpu_available(cuda_only=False))  # Deprecated
print(""BUILD WITH CUDA:"", tf.test.is_built_with_cuda())  # Installed non gpu package
```
which yields:
```
CPU LIST: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]
GPU LIST: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
WARNING:tensorflow:From P:/LocalPrograms/stock/friendly_solution_23-07/modules/Check_Env_GPU.py:20: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2023-07-05 16:37:19.792286: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Deprecated AVAILABLE: True
Deprecated AVAILABLE: True
BUILD WITH CUDA: True
=== === === === === === 
LOCAL DEVICES:
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 5073090464258046644
xla_global_id: -1
, name: ""/device:GPU:0""
device_type: ""GPU""
memory_limit: 6957301760
locality {
  bus_id: 1
  links {
  }
}
incarnation: 5448345831176042487
physical_device_desc: ""device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1""
xla_global_id: 416903419
]
2023-07-05 16:37:20.295222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 6635 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1
2023-07-05 16:37:20.296252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 6635 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1
2023-07-05 16:37:20.296749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 6635 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1

Process finished with exit code 0
```

### Simple tensorflow benchmark
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM, Flatten
from tensorflow.keras.layers import ConvLSTM2D

import numpy as np

import keras


# tf.compat.v1.InteractiveSession() #3-4ms
# with tf.compat.v1.Session():
# None

N = int(3e4)
X = np.random.random((N, 20))
Y = np.random.random(N)

#######
""Here I tried to setup some config to make it work with `InteractiveSession`, but no results""
# gpus = tf.config.experimental.list_physical_devices('GPU')
# gpu_conf = tf.config.experimental.set_virtual_device_configuration(
#         gpus[0],
#         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])
# logical_gpus = tf.config.experimental.list_logical_devices('GPU')
# print(f""Logical: {logical_gpus}"")
# 
# config = tf.compat.v1.ConfigProto(gpu_options=gpu_conf)
session = tf.compat.v1.InteractiveSession()

####################
""Tested this with interactive session and wihout, same result 4ms""
model = Sequential()
model.add(Dense(50, input_shape=(20,)))
model.add(Dense(60))
model.add(Dense(60))
model.add(Dense(60))
model.add(Dense(60))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])

model.fit(X, Y, verbose=True, epochs=1)
model.predict(X)

####################
""Session 70-110us which is notable difference""
with tf.compat.v1.Session():
    model = Sequential()
    model.add(Dense(50, input_shape=(20,)))
    model.add(Dense(60))
    model.add(Dense(60))
    model.add(Dense(60))
    model.add(Dense(60))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])

    model.fit(X, Y, verbose=True, epochs=1)

    model.predict(X)
```
And full output of training:
```
2023-07-05 16:41:35.820447: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Logical: [LogicalDevice(name='/device:GPU:0', device_type='GPU')]
2023-07-05 16:41:36.340443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4000 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1
2023-07-05 16:41:36.342546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4000 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1
938/938 [==============================] - 4s 4ms/step - loss: 0.0913 - accuracy: 0.0000e+00
938/938 [==============================] - 1s 1ms/step
2023-07-05 16:41:43.303423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4000 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1
Train on 30000 samples
2023-07-05 16:41:43.701896: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
30000/30000 [==============================] - 3s 93us/sample - loss: 0.0892 - accuracy: 0.0000e+00
C:\Users\Greg\anaconda3\envs\tf4\lib\site-packages\keras\engine\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,

Process finished with exit code 0
```

### Standalone code to reproduce the issue

```shell
# Env setup
conda create python 3.8.16
pip install tensroflow-gpu==2.10.1
conda install -c conda-forge cudatoolkit=11.2.2
conda install -c conda-forge cudnn=8.1.0.77
```


### Relevant log output

_No response_"
61172,//tensorflow/python/data/experimental/kernel_tests/service:distributed_save_ft_test is flaky,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

//tensorflow/python/data/experimental/kernel_tests/service:distributed_save_ft_test fails occasionally

x86 log
https://source.cloud.google.com/results/invocations/c5169019-316f-43e1-8310-67596d2aae9a/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5330158020/jobs/9656553237#step:5:8169

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
FAIL: //tensorflow/python/data/experimental/kernel_tests/service:distributed_save_ft_test (shard 7 of 17) (see /root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test/shard_7_of_17/test.log)
INFO: From Testing //tensorflow/python/data/experimental/kernel_tests/service:distributed_save_ft_test (shard 7 of 17):
==================== Test output for //tensorflow/python/data/experimental/kernel_tests/service:distributed_save_ft_test (shard 7 of 17):
2023-07-04 13:14:35.474115: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-04 13:14:35.591768: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Running tests under Python 3.9.17: /usr/bin/python3
[ RUN      ] SnapshotFtTest.testLargeMultiSourceSnapshotRecoversAndCompletes_test_mode_graph_tfapiversion_1
[  SKIPPED ] SnapshotFtTest.testLargeMultiSourceSnapshotRecoversAndCompletes_test_mode_graph_tfapiversion_1
[ RUN      ] SnapshotFtTest.testRepeatedDatasetRecoversAndCompletes_test_mode_graph_tfapiversion_2
2023-07-04 13:14:39.585700: I tensorflow/core/data/service/dispatcher_impl.cc:223] Attempting to restore dispatcher state from journal in /root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/_tmp/b16384d7dce82bdf5680b78ace436b42jph8vvz5/tmpt0datbl4/tf_data_dispatcher_journal
2023-07-04 13:14:39.585733: I tensorflow/core/data/service/dispatcher_impl.cc:230] No journal found. Starting dispatcher from new state.
2023-07-04 13:14:39.585895: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data DispatchServer running at 0.0.0.0:46609
2023-07-04 13:14:39.587379: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:46609
2023-07-04 13:14:39.587535: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:43947
2023-07-04 13:14:39.588806: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:46609
2023-07-04 13:14:39.588951: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:43935
2023-07-04 13:14:39.590017: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:46609
2023-07-04 13:14:39.590160: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:46577
WARNING:tensorflow:From /usr/lib/python3.9/contextlib.py:87: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
W0704 13:14:39.597163 140076266616640 deprecation.py:364] From /usr/lib/python3.9/contextlib.py:87: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
2023-07-04 13:14:39.618944: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled
2023-07-04 13:14:39.670608: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at distributed_save_op.cc:103 : UNIMPLEMENTED: Failed to get dispatcher version from dispatcher running at localhost:46609:
ERROR:tensorflow:Graph execution error:

Detected at node 'DistributedSave' defined at (most recent call last):
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.py"", line 473, in <module>
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 56, in main
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62, in main
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60, in main_wrapper
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51, in g_main
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2049, in main
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2156, in _run_in_app
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2568, in run_tests
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2537, in _run_and_get_tests_result
    File ""/usr/lib/python3.9/unittest/main.py"", line 101, in __init__
    File ""/usr/lib/python3.9/unittest/main.py"", line 271, in runTests
    File ""/usr/lib/python3.9/unittest/runner.py"", line 184, in run
    File ""/usr/lib/python3.9/unittest/suite.py"", line 84, in __call__
    File ""/usr/lib/python3.9/unittest/suite.py"", line 122, in run
    File ""/usr/lib/python3.9/unittest/suite.py"", line 84, in __call__
    File ""/usr/lib/python3.9/unittest/suite.py"", line 122, in run
    File ""/usr/lib/python3.9/unittest/case.py"", line 651, in __call__
    File ""/usr/lib/python3.9/unittest/case.py"", line 592, in run
    File ""/usr/lib/python3.9/unittest/case.py"", line 550, in _callTestMethod
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.py"", line 357, in testRepeatedDatasetRecoversAndCompletes
Node: 'DistributedSave'
Failed to get dispatcher version from dispatcher running at localhost:46609:
	 [[{{node DistributedSave}}]]

Original stack trace for 'DistributedSave':
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.py"", line 473, in <module>
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 56, in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62, in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 489, in benchmarks_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60, in main_wrapper
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/app.py"", line 312, in run
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/app.py"", line 258, in _run_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51, in g_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2049, in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2156, in _run_in_app
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2568, in run_tests
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2537, in _run_and_get_tests_result
  File ""/usr/lib/python3.9/unittest/main.py"", line 101, in __init__
  File ""/usr/lib/python3.9/unittest/main.py"", line 271, in runTests
  File ""/usr/lib/python3.9/unittest/runner.py"", line 184, in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84, in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122, in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84, in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122, in run
  File ""/usr/lib/python3.9/unittest/case.py"", line 651, in __call__
  File ""/usr/lib/python3.9/unittest/case.py"", line 592, in run
  File ""/usr/lib/python3.9/unittest/case.py"", line 550, in _callTestMethod
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314, in bound_param_test
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360, in decorated
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343, in execute_test_method
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.py"", line 357, in testRepeatedDatasetRecoversAndCompletes
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/ops/distributed_save_op.py"", line 56, in distributed_save
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_experimental_dataset_ops.py"", line 2041, in distributed_save
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 2647, in _create_op_internal
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 1152, in from_node_def

E0704 13:14:39.671662 140076266616640 test_util.py:2067] Graph execution error:

Detected at node 'DistributedSave' defined at (most recent call last):
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.py"", line 473, in <module>
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 56, in main
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62, in main
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60, in main_wrapper
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51, in g_main
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2049, in main
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2156, in _run_in_app
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2568, in run_tests
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2537, in _run_and_get_tests_result
    File ""/usr/lib/python3.9/unittest/main.py"", line 101, in __init__
    File ""/usr/lib/python3.9/unittest/main.py"", line 271, in runTests
    File ""/usr/lib/python3.9/unittest/runner.py"", line 184, in run
    File ""/usr/lib/python3.9/unittest/suite.py"", line 84, in __call__
    File ""/usr/lib/python3.9/unittest/suite.py"", line 122, in run
    File ""/usr/lib/python3.9/unittest/suite.py"", line 84, in __call__
    File ""/usr/lib/python3.9/unittest/suite.py"", line 122, in run
    File ""/usr/lib/python3.9/unittest/case.py"", line 651, in __call__
    File ""/usr/lib/python3.9/unittest/case.py"", line 592, in run
    File ""/usr/lib/python3.9/unittest/case.py"", line 550, in _callTestMethod
    File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.py"", line 357, in testRepeatedDatasetRecoversAndCompletes
Node: 'DistributedSave'
Failed to get dispatcher version from dispatcher running at localhost:46609:
	 [[{{node DistributedSave}}]]

Original stack trace for 'DistributedSave':
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.py"", line 473, in <module>
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 56, in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62, in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 489, in benchmarks_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60, in main_wrapper
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/app.py"", line 312, in run
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/app.py"", line 258, in _run_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51, in g_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2049, in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2156, in _run_in_app
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2568, in run_tests
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/absltest.py"", line 2537, in _run_and_get_tests_result
  File ""/usr/lib/python3.9/unittest/main.py"", line 101, in __init__
  File ""/usr/lib/python3.9/unittest/main.py"", line 271, in runTests
  File ""/usr/lib/python3.9/unittest/runner.py"", line 184, in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84, in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122, in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84, in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122, in run
  File ""/usr/lib/python3.9/unittest/case.py"", line 651, in __call__
  File ""/usr/lib/python3.9/unittest/case.py"", line 592, in run
  File ""/usr/lib/python3.9/unittest/case.py"", line 550, in _callTestMethod
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314, in bound_param_test
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360, in decorated
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343, in execute_test_method
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.py"", line 357, in testRepeatedDatasetRecoversAndCompletes
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/ops/distributed_save_op.py"", line 56, in distributed_save
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_experimental_dataset_ops.py"", line 2041, in distributed_save
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 2647, in _create_op_internal
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/distributed_save_ft_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 1152, in from_node_def

[  FAILED  ] SnapshotFtTest.testRepeatedDatasetRecoversAndCompletes_test_mode_graph_tfapiversion_2
```
"
61169,//tensorflow/python/distribute:parameter_server_strategy_v2_test_cpu is flaky,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

//tensorflow/python/distribute:parameter_server_strategy_v2_test_cpu timeouts sometimes

x86 log
https://source.cloud.google.com/results/invocations/75eaa47f-92bd-47d5-9c7a-020a0c67dda9/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5399333727/jobs/9806323589#step:5:7344

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
TIMEOUT: //tensorflow/python/distribute:parameter_server_strategy_v2_test_cpu (Summary)
      /root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu/test.log
INFO: From Testing //tensorflow/python/distribute:parameter_server_strategy_v2_test_cpu:
==================== Test output for //tensorflow/python/distribute:parameter_server_strategy_v2_test_cpu:
2023-07-04 08:53:48.311167: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-04 08:53:48.373167: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Running tests under Python 3.9.17: /usr/bin/python3
[ RUN      ] ClusterTypeNameTest.testArbitraryCurrentTaskType
INFO:tensorflow:Using local port 38369
I0704 08:53:53.254792 140378756265792 test_util.py:3796] Using local port 38369
INFO:tensorflow:Using local port 44357
I0704 08:53:53.255300 140378756265792 test_util.py:3796] Using local port 44357
INFO:tensorflow:Using local port 46287
I0704 08:53:53.255505 140378756265792 test_util.py:3796] Using local port 46287
INFO:tensorflow:time(__main__.ClusterTypeNameTest.testArbitraryCurrentTaskType): 0.0s
I0704 08:53:53.256115 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testArbitraryCurrentTaskType): 0.0s
[       OK ] ClusterTypeNameTest.testArbitraryCurrentTaskType
[ RUN      ] ClusterTypeNameTest.testArbitraryJobName
INFO:tensorflow:Using local port 43625
I0704 08:53:53.256868 140378756265792 test_util.py:3796] Using local port 43625
INFO:tensorflow:Using local port 38445
I0704 08:53:53.257118 140378756265792 test_util.py:3796] Using local port 38445
INFO:tensorflow:Using local port 33687
I0704 08:53:53.257305 140378756265792 test_util.py:3796] Using local port 33687
INFO:tensorflow:Using local port 43947
I0704 08:53:53.257477 140378756265792 test_util.py:3796] Using local port 43947
INFO:tensorflow:time(__main__.ClusterTypeNameTest.testArbitraryJobName): 0.0s
I0704 08:53:53.258199 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testArbitraryJobName): 0.0s
[       OK ] ClusterTypeNameTest.testArbitraryJobName
[ RUN      ] ClusterTypeNameTest.testLessThanOnePs
INFO:tensorflow:Using local port 37051
I0704 08:53:53.258810 140378756265792 test_util.py:3796] Using local port 37051
INFO:tensorflow:Using local port 44491
I0704 08:53:53.259010 140378756265792 test_util.py:3796] Using local port 44491
INFO:tensorflow:time(__main__.ClusterTypeNameTest.testLessThanOnePs): 0.0s
I0704 08:53:53.259491 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testLessThanOnePs): 0.0s
[       OK ] ClusterTypeNameTest.testLessThanOnePs
[ RUN      ] ClusterTypeNameTest.testLessThanOneWorker
INFO:tensorflow:Using local port 33303
I0704 08:53:53.260033 140378756265792 test_util.py:3796] Using local port 33303
INFO:tensorflow:Using local port 33109
I0704 08:53:53.260214 140378756265792 test_util.py:3796] Using local port 33109
INFO:tensorflow:time(__main__.ClusterTypeNameTest.testLessThanOneWorker): 0.0s
I0704 08:53:53.260598 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testLessThanOneWorker): 0.0s
[       OK ] ClusterTypeNameTest.testLessThanOneWorker
[ RUN      ] ClusterTypeNameTest.testMoreThanOneChief
INFO:tensorflow:Using local port 36459
I0704 08:53:53.261005 140378756265792 test_util.py:3796] Using local port 36459
INFO:tensorflow:Using local port 33799
I0704 08:53:53.261153 140378756265792 test_util.py:3796] Using local port 33799
INFO:tensorflow:Using local port 39497
I0704 08:53:53.261297 140378756265792 test_util.py:3796] Using local port 39497
INFO:tensorflow:Using local port 36657
I0704 08:53:53.261423 140378756265792 test_util.py:3796] Using local port 36657
INFO:tensorflow:Using local port 37059
I0704 08:53:53.261545 140378756265792 test_util.py:3796] Using local port 37059
INFO:tensorflow:time(__main__.ClusterTypeNameTest.testMoreThanOneChief): 0.0s
I0704 08:53:53.261911 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testMoreThanOneChief): 0.0s
[       OK ] ClusterTypeNameTest.testMoreThanOneChief
[ RUN      ] ClusterTypeNameTest.test_session
[  SKIPPED ] ClusterTypeNameTest.test_session
INFO:tensorflow:Now creating a MultiProcessCluster with num_workers=2, num_ps=3.
I0704 08:53:53.262382 140378756265792 multi_worker_test_base.py:335] Now creating a MultiProcessCluster with num_workers=2, num_ps=3.
INFO:tensorflow:Using local port 34725
I0704 08:53:53.262561 140378756265792 test_util.py:3796] Using local port 34725
INFO:tensorflow:Using local port 41389
I0704 08:53:53.262695 140378756265792 test_util.py:3796] Using local port 41389
INFO:tensorflow:Using local port 42953
I0704 08:53:53.262822 140378756265792 test_util.py:3796] Using local port 42953
INFO:tensorflow:Using local port 39597
I0704 08:53:53.262954 140378756265792 test_util.py:3796] Using local port 39597
INFO:tensorflow:Using local port 40735
I0704 08:53:53.263074 140378756265792 test_util.py:3796] Using local port 40735
2023-07-04 08:53:54.005149: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-04 08:53:54.092559: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-04 08:53:54.232593: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-04 08:53:54.295324: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
-- Test timed out at 2023-07-04 08:58:46 UTC --
Thread 0x00007fac735de700 (most recent call first):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 258 in _continuously_readline_from_sub
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Thread 0x00007fac765a0700 (most recent call first):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 258 in _continuously_readline_from_sub
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Thread 0x00007fac025a4700 (most recent call first):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 258 in _continuously_readline_from_sub
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Thread 0x00007fabffda3700 (most recent call first):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 527 in _process_watchdog
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Thread 0x00007fabff5a2700 (most recent call first):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 258 in _continuously_readline_from_sub
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Current thread 0x00007fac79e6b740 (most recent call first):
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 379 in _recv
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 414 in _recv_bytes
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 250 in recv
  File ""/usr/lib/python3.9/multiprocessing/managers.py"", line 810 in _callmethod
  File ""/usr/lib/python3.9/multiprocessing/managers.py"", line 1085 in wait
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_worker_test_base.py"", line 270 in start
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_worker_test_base.py"", line 348 in create_multi_process_cluster
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/parameter_server_strategy_v2_test.py"", line 62 in setUpClass
  File ""/usr/lib/python3.9/unittest/suite.py"", line 166 in _handleClassSetUp
  File ""/usr/lib/python3.9/unittest/suite.py"", line 114 in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.9/unittest/runner.py"", line 184 in run
  File ""/usr/lib/python3.9/unittest/main.py"", line 271 in runTests
  File ""/usr/lib/python3.9/unittest/main.py"", line 101 in __init__
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2537 in _run_and_get_tests_result
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2568 in run_tests
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2156 in _run_in_app
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2049 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/absl_py/absl/app.py"", line 258 in _run_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/absl_py/absl/app.py"", line 312 in run
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 489 in benchmarks_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 56 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/test.py"", line 25 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 167 in test_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 1455 in test_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/parameter_server_strategy_v2_test.py"", line 713 in <module>
```
"
61168,Functions that limit video memory do not work,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf2.12.0

### Custom code

Yes

### OS platform and distribution

MacOs

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We have many similar model building and training code files, and train multiple models consecutively through one process. But in this process, we used the memory blank and the restriction function, and neither worked.


### Standalone code to reproduce the issue

```shell
![image](https://github.com/tensorflow/tensorflow/assets/34181680/fbe4f5b6-255b-4583-b108-f476bd639574)
```

### Relevant log output

_No response_"
61167,Tensorflow official guide for optimizing pipeline performance in Data input pipelines does not include proper use cases,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom code

Yes

### OS platform and distribution

RHEL8 .8

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The original tensorflow guide on [optimizing pipeline performance] (https://www.tensorflow.org/guide/data_performance#vectorizing_mapping) does not include useful use cases.

The function that is used to demonstrate the effectiveness of various techniques like batching , caching, mapping etc. is

```
def mapped_function(s):
    # Do some hard pre-processing
    tf.py_function(lambda: time.sleep(0.03), [], ())
    return s
```
This function is insensitive to a single element or a vector. How should we modify the function in order to deal with a batch, the case when we first batch the dataset and then map it with this function? Do we need to modify it in the first place or does tensorflow handle it on its own? These questions should have been answered in the guide.

I tried vectorized mapping( dataset.batch().map()) with this function

```
def function(x):
    return x+1

def tf_func(x):
    ans = tf.py_function(function, inp=x, Tout=['int64'])
    return ans

```

This also works fine as even in this case the function is be default capable of handling the vectors

but if we try a realistic scenario like this one

```
def read_files(filename):
    ...
    file = tf.io.read_file(filename)
    ...
```

This function fails to work throwing error that tf.io.read_file cannot handle the vector.

I think it would be helpful for all if practical use case scenarios are shown in the guide.


Thanks

### Standalone code to reproduce the issue

```shell
dataset = tf.data.Dataset.list_files(...) #glob and read filenames

def preprocess_files(filename): #read and pre-process the files here
    ...
    file = tf.io.read_file(filename) #this function throws error
    ...
```


### Relevant log output

_No response_"
61166,//tensorflow/python/data/experimental/kernel_tests/service:worker_tags_test is flaky,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

//tensorflow/python/data/experimental/kernel_tests/service:worker_tags_test fails occasionally

x86 log
https://source.cloud.google.com/results/invocations/0bc426bf-e5ae-4fb6-993f-6199c00d1139/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5411569978/jobs/9834485829#step:5:8675

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
======================================================================
ERROR: testMultipleTags_test_mode_eager_tfapiversion_2 (__main__.WorkerTagsTest)
WorkerTagsTest.testMultipleTags_test_mode_eager_tfapiversion_2
testMultipleTags_test_mode_eager_tfapiversion_2(mode='eager', tf_api_version=2)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314, in bound_param_test
    return test_method(self, **testcase_params)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360, in decorated
    execute_test_method()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343, in execute_test_method
    test_method(**kwargs_to_pass)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.py"", line 179, in testMultipleTags
    cluster = multi_process_cluster.MultiProcessCluster(
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 81, in __init__
    self._start_local_workers(num_local_workers, worker_tags)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 100, in _start_local_workers
    self.start_local_worker(worker_tags)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 114, in start_local_worker
    worker.start()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/test_base.py"", line 110, in start
    self._server.start()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/service/server_lib.py"", line 415, in start
    self._server.start()
NotImplementedError: Failed to get dispatcher version from dispatcher running at localhost:45069:

----------------------------------------------------------------------
Ran 6 tests in 6.597s

FAILED (errors=1)
Exception ignored in: <function MultiProcessCluster.__del__ at 0x7f20f09658b0>
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 160, in __del__
    self._stop()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 155, in _stop
    for (_, worker_process) in self._remote_workers:
AttributeError: 'MultiProcessCluster' object has no attribute '_remote_workers'
2023-07-02 09:48:51.833944: I tensorflow/core/data/service/server_lib.cc:94] Shut down DispatchServer server running at port 45069
2023-07-02 09:48:51.934278: I tensorflow/core/data/service/server_lib.cc:94] Shut down WorkerServer server running at port 44313
================================================================================
```
"
61164,//tensorflow/dtensor/mlir/tests:spmd_expansion.mlir.test is flaky,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

//tensorflow/dtensor/mlir/tests:spmd_expansion.mlir.test unit test fails occasionally

x86 log
https://source.cloud.google.com/results/invocations/88910868-abeb-4aa0-8954-df2b79ef5a26/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5420661339/jobs/9855097285

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=rbe --config=pycpp --config=build_event_export
```


### Relevant log output

```shell
FAIL: //tensorflow/dtensor/mlir/tests:spmd_expansion.mlir.test (see /root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test/test.log)
INFO: From Testing //tensorflow/dtensor/mlir/tests:spmd_expansion.mlir.test:
==================== Test output for //tensorflow/dtensor/mlir/tests:spmd_expansion.mlir.test:
-- Testing: 1 tests, 1 workers --
FAIL: MLIR tests :: spmd_expansion.mlir (1 of 1)
******************** TEST 'MLIR tests :: spmd_expansion.mlir' FAILED ********************
Script:
--
: 'RUN: at line 1';   /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/dtensor-opt -- /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir -split-input-file -dtensor-annotate-global-shape -dtensor-spmd-expansion -verify-diagnostics | /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/llvm-project/llvm/FileCheck /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir
--
Exit Code: 1

Command Output (stderr):
--
2023-07-03 09:20:51.133345: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
TensorFlow crashed, please file a bug on https://github.com/tensorflow/tensorflow/issues with the trace below.
Stack dump:
0.	Program arguments: /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/dtensor-opt /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/dtensor-opt /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir -split-input-file -dtensor-annotate-global-shape -dtensor-spmd-expansion -verify-diagnostics
1.	Program arguments: /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/dtensor-opt /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir -split-input-file -dtensor-annotate-global-shape -dtensor-spmd-expansion -verify-diagnostics
Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):
0  libtensorflow_framework.so.2 0x00007f49dfe75e6e llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) + 46
1  libtensorflow_framework.so.2 0x00007f49dfe73f07 llvm::sys::RunSignalHandlers() + 87
2  libtensorflow_framework.so.2 0x00007f49dfe76692
3  libpthread.so.0              0x00007f49dea05420
4  dtensor-opt                  0x0000562b92fdd978
5  dtensor-opt                  0x0000562b92fdd11a
6  dtensor-opt                  0x0000562b92fde2b9
7  dtensor-opt                  0x0000562b92f94499
8  dtensor-opt                  0x0000562b92fb7b91
9  dtensor-opt                  0x0000562b92fb85bf
10 dtensor-opt                  0x0000562b92eca421
11 dtensor-opt                  0x0000562b94e800d4
12 dtensor-opt                  0x0000562b94e82d17
13 dtensor-opt                  0x0000562b94e82ae5
14 dtensor-opt                  0x0000562b948df256
15 dtensor-opt                  0x0000562b948de53a
16 dtensor-opt                  0x0000562b9502d603
17 dtensor-opt                  0x0000562b9502d34e
18 dtensor-opt                  0x0000562b948da857
19 dtensor-opt                  0x0000562b948dad3d
20 dtensor-opt                  0x0000562b92d80fcc
21 libc.so.6                    0x00007f49de626083 __libc_start_main + 243
22 dtensor-opt                  0x0000562b92d80d39
Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):
0  libtensorflow_framework.so.2 0x00007f49dfe75e6e llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) + 46
1  libtensorflow_framework.so.2 0x00007f49dfe73f37 llvm::sys::RunSignalHandlers() + 135
2  libtensorflow_framework.so.2 0x00007f49dfe76692
3  libpthread.so.0              0x00007f49dea05420
4  dtensor-opt                  0x0000562b92fdd978
5  dtensor-opt                  0x0000562b92fdd11a
6  dtensor-opt                  0x0000562b92fde2b9
7  dtensor-opt                  0x0000562b92f94499
8  dtensor-opt                  0x0000562b92fb7b91
9  dtensor-opt                  0x0000562b92fb85bf
10 dtensor-opt                  0x0000562b92eca421
11 dtensor-opt                  0x0000562b94e800d4
12 dtensor-opt                  0x0000562b94e82d17
13 dtensor-opt                  0x0000562b94e82ae5
14 dtensor-opt                  0x0000562b948df256
15 dtensor-opt                  0x0000562b948de53a
16 dtensor-opt                  0x0000562b9502d603
17 dtensor-opt                  0x0000562b9502d34e
18 dtensor-opt                  0x0000562b948da857
19 dtensor-opt                  0x0000562b948dad3d
20 dtensor-opt                  0x0000562b92d80fcc
21 libc.so.6                    0x00007f49de626083 __libc_start_main + 243
22 dtensor-opt                  0x0000562b92d80d39
/b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir:277:17: error: CHECK-LABEL: expected string not found in input
// CHECK-LABEL: module @test_spmd_softmax_rank_3
                ^
<stdin>:153:45: note: scanning from here
module @test_spmd_softmax_last_dim_unsharded {
                                            ^
<stdin>:156:303: note: possible intended match here
 %0 = ""tf.Softmax""(%arg0) {_global_shape = [#tf_type.shape<32x32>], _layout = [""sharding_specs:x,unsharded, mesh:TPU|x=2,y=2|0,1,2,3|0,1,2,3|/job:localhost/task:0/device:TPU:0,/job:localhost/task:0/device:TPU:1,/job:localhost/task:0/device:TPU:2,/job:localhost/task:0/device:TPU:3""]} : (tensor<16x32xf32>) -> tensor<16x32xf32>
                                                                                                                                                                                                                                                                                                              ^

Input file: <stdin>
Check file: /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
             .
             .
             .
           148:  }
           149: }
           150:
           151:
           152: // -----
           153: module @test_spmd_softmax_last_dim_unsharded {
label:277'0                                                 X~~ error: no match found
           154:  func.func @main(%arg0: tensor<16x32xf32> {tf._global_shape = #tf_type.shape<32x32>, tf._layout = ""sharding_specs:x,unsharded, mesh:TPU|x=2,y=2|0,1,2,3|0,1,2,3|/job:localhost/task:0/device:TPU:0,/job:localhost/task:0/device:TPU:1,/job:localhost/task:0/device:TPU:2,/job:localhost/task:0/device:TPU:3""}) {
label:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           155:  ""tf_device.cluster""() ({
label:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~
           156:  %0 = ""tf.Softmax""(%arg0) {_global_shape = [#tf_type.shape<32x32>], _layout = [""sharding_specs:x,unsharded, mesh:TPU|x=2,y=2|0,1,2,3|0,1,2,3|/job:localhost/task:0/device:TPU:0,/job:localhost/task:0/device:TPU:1,/job:localhost/task:0/device:TPU:2,/job:localhost/task:0/device:TPU:3""]} : (tensor<16x32xf32>) -> tensor<16x32xf32>
label:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
label:277'1                                                                                                                                                                                                                                                                                                                   ?                         possible intended match
           157:  tf_device.return {_global_shape = [], _layout = []}
label:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           158:  }) {_global_shape = [], _mesh = ""TPU|x=2,y=2|0,1,2,3|0,1,2,3|/job:localhost/task:0/device:TPU:0,/job:localhost/task:0/device:TPU:1,/job:localhost/task:0/device:TPU:2,/job:localhost/task:0/device:TPU:3""} : () -> ()
label:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           159:  return
label:277'0     ~~~~~~~~
           160:  }
label:277'0     ~~~
           161: }
label:277'0     ~~
           162:
label:277'0     ~
>>>>>>

--

********************
********************
Failed Tests (1):
  MLIR tests :: spmd_expansion.mlir
```
"
61163,assert_shapes does not return anything and cannot be used as control dependency,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0-dev20230606

### Custom code

Yes

### OS platform and distribution

Windows 11 x64

### Mobile device

NA

### Python version

3.10

### Bazel version

NA

### GCC/compiler version

NA

### CUDA/cuDNN version

NA

### GPU model and memory

NA

### Current behavior?

Calling `tf.debugging.assert_shapes(...)` does not return anything. As a consequence, unlike other assert operations, trying to use this assert as a control dependency with `tf.control_dependencies` in graph mode fails with `TypeError: Can not convert a NoneType into a Tensor or Operation`.

Presumably, this would be fixed by simply adding a `return` before the call to `assert_shapes` in `assert_shapes_v2` in [`tensorflow/python/ops/check_ops.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/check_ops.py). But I'd rather someone familiar with the code to judge whether that is fine orif maybe there is was a reason why this op is not returned.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

@tf.function
def my_func(x):
    with tf.control_dependencies([tf.debugging.assert_shapes([(x, (2,))])]):
        return x + 2

my_func([1, 2])
# TypeError: Can not convert a NoneType into a Tensor or Operation.
```


### Relevant log output

```shell
(...) my_func  *
    with tf.control_dependencies([tf.debugging.assert_shapes([(x, (2,))])]):
(...)\site-packages\tensorflow\python\framework\ops.py:5359 control_dependencies
    return get_default_graph().control_dependencies(control_inputs)
(...)\site-packages\tensorflow\python\framework\func_graph.py:362 control_dependencies
    return super(FuncGraph, self).control_dependencies(filtered_control_inputs)
(...)\site-packages\tensorflow\python\framework\ops.py:4815 control_dependencies
    c = self.as_graph_element(c)
(...)\site-packages\tensorflow\python\framework\ops.py:3726 as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
(...)\site-packages\tensorflow\python\framework\ops.py:3815 _as_graph_element_locked
    (type(obj).__name__, types_str))

TypeError: Can not convert a NoneType into a Tensor or Operation.
```
"
61162,Vectorizing a mapping function containing tf.io.read_file in tf.data pipeline is not working.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom code

Yes

### OS platform and distribution

RHEL8 .8

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have a function that reads input files, does some processing and returns the files. Upon using that function to do processing without batching first works fine. i.e. `dataset.map(some_func).batch(batch_size)`

I was trying to speed up the data pipeline by vectorizing the processing part by batching the dataset and then mapping it to the tf.py_function i.e `dataset.batch(batch_size).map(tf.py_func_wrapped_function)`

I followed the tensorflow guide for [optimizing pipeline performance](https://www.tensorflow.org/guide/data_performance#vectorizing_mapping)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import pathlib
import os
import matplotlib.pyplot as plt

flowers = tf.keras.utils.get_file(
    'flower_photos',
    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
    untar=True)

flowers = pathlib.Path(flowers)


list_ds = tf.data.Dataset.list_files(str(flowers/'*/*'),shuffle=False)

list_ds

for f in list_ds.take(10):
    print(f.numpy())


# Reads an image from a file, decodes it into a dense tensor, and resizes it
# to a fixed shape.
def parse_image(filename):
  parts = tf.strings.split(filename, os.sep)
  label = parts[-2]

  image = tf.io.read_file(filename)
  image = tf.image.decode_jpeg(image)
  image = tf.image.convert_image_dtype(image, tf.float32)
  image = tf.image.resize(image, [128, 128])
  return image, label


dataset1 = list_ds.map(parse_image).batch(32)#map then batch, scalar mapping

el = next(iter(dataset1))

plt.imshow(el[0][0])
plt.title(el[1][0].numpy().decode('utf-8'))

dataset2 = list_ds.batch(32).map(parse_image) #batch then map(vectorized mapping), should work
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[38], line 1
----> 1 dataset2 = list_ds.batch(32).map(parse_image) #batch then map(vectorized mapping), should work

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2240, in DatasetV2.map(self, map_func, num_parallel_calls, deterministic, name)
   2236 # Loaded lazily due to a circular dependency (dataset_ops -> map_op ->
   2237 # dataset_ops).
   2238 # pylint: disable=g-import-not-at-top,protected-access
   2239 from tensorflow.python.data.ops import map_op
-> 2240 return map_op._map_v2(
   2241     self,
   2242     map_func,
   2243     num_parallel_calls=num_parallel_calls,
   2244     deterministic=deterministic,
   2245     name=name)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/data/ops/map_op.py:37, in _map_v2(input_dataset, map_func, num_parallel_calls, deterministic, name)
     34   if deterministic is not None and not debug_mode.DEBUG_MODE:
     35     warnings.warn(""The `deterministic` argument has no effect unless the ""
     36                   ""`num_parallel_calls` argument is specified."")
---> 37   return _MapDataset(
     38       input_dataset, map_func, preserve_cardinality=True, name=name)
     39 else:
     40   return _ParallelMapDataset(
     41       input_dataset,
     42       map_func,
   (...)
     45       preserve_cardinality=True,
     46       name=name)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/data/ops/map_op.py:107, in _MapDataset.__init__(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)
    105 self._use_inter_op_parallelism = use_inter_op_parallelism
    106 self._preserve_cardinality = preserve_cardinality
--> 107 self._map_func = structured_function.StructuredFunctionWrapper(
    108     map_func,
    109     self._transformation_name(),
    110     dataset=input_dataset,
    111     use_legacy_function=use_legacy_function)
    112 self._name = name
    113 variant_tensor = gen_dataset_ops.map_dataset(
    114     input_dataset._variant_tensor,  # pylint: disable=protected-access
    115     self._map_func.function.captured_inputs,
   (...)
    118     preserve_cardinality=self._preserve_cardinality,
    119     **self._common_args)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:261, in StructuredFunctionWrapper.__init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
    254       warnings.warn(
    255           ""Even though the `tf.config.experimental_run_functions_eagerly` ""
    256           ""option is set, this option does not apply to tf.data functions. ""
    257           ""To force eager execution of tf.data functions, please use ""
    258           ""`tf.data.experimental.enable_debug_mode()`."")
    259     fn_factory = trace_tf_function(defun_kwargs)
--> 261 self._function = fn_factory()
    262 # There is no graph to add in eager mode.
    263 add_to_graph &= not context.executing_eagerly()

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:232, in TracingCompiler.get_concrete_function(self, *args, **kwargs)
    223 def get_concrete_function(self, *args, **kwargs):
    224   """"""Returns a `ConcreteFunction` specialized to inputs and execution context.
    225 
    226   Args:
   (...)
    230       `tf.Tensor` or `tf.TensorSpec`.
    231   """"""
--> 232   concrete_function = self._get_concrete_function_garbage_collected(
    233       *args, **kwargs)
    234   concrete_function._garbage_collector.release()  # pylint: disable=protected-access
    235   return concrete_function

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:202, in TracingCompiler._get_concrete_function_garbage_collected(self, *args, **kwargs)
    199   self._function_spec.make_canonicalized_monomorphic_type(args, kwargs)
    201 with self._lock:
--> 202   concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)
    203   seen_names = set()
    204   concrete_function._arg_keywords = []  # pylint: disable=protected-access

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:166, in TracingCompiler._maybe_define_concrete_function(self, args, kwargs)
    163   args = self.input_signature
    164   kwargs = {}
--> 166 return self._maybe_define_function(args, kwargs)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:396, in TracingCompiler._maybe_define_function(self, args, kwargs)
    393   args = placeholder_bound_args.args
    394 kwargs = placeholder_bound_args.kwargs
--> 396 concrete_function = self._create_concrete_function(
    397     args, kwargs, func_graph)
    399 # TODO(b/263520817): Remove access to private attribute.
    400 graph_capture_container = concrete_function.graph._function_captures  # pylint: disable=protected-access

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:300, in TracingCompiler._create_concrete_function(self, args, kwargs, func_graph)
    296 else:
    297   arg_names = base_arg_names
    299 concrete_function = monomorphic_function.ConcreteFunction(
--> 300     func_graph_module.func_graph_from_py_func(
    301         self._name,
    302         self._python_function,
    303         args,
    304         kwargs,
    305         None,
    306         func_graph=func_graph,
    307         autograph=self._autograph,
    308         autograph_options=self._autograph_options,
    309         arg_names=arg_names,
    310         capture_by_value=self._capture_by_value,
    311         create_placeholders=False),
    312     self._function_attributes,
    313     spec=self.function_spec,
    314     # Tell the ConcreteFunction to clean up its graph once it goes out of
    315     # scope. This is not the default behavior since it gets used in some
    316     # places (like Keras) where the FuncGraph lives longer than the
    317     # ConcreteFunction.
    318     shared_func_graph=False)
    319 return concrete_function

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1214, in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)
   1211 else:
   1212   _, original_func = tf_decorator.unwrap(python_func)
-> 1214 func_outputs = python_func(*func_args, **func_kwargs)
   1216 # invariant: `func_outputs` contains only Tensors, CompositeTensors,
   1217 # TensorArrays and `None`s.
   1218 func_outputs = variable_utils.convert_variables_to_tensors(func_outputs)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:238, in StructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn(*args)
    232 @eager_function.defun_with_attributes(
    233     input_signature=structure.get_flat_tensor_specs(
    234         self._input_structure),
    235     autograph=False,
    236     attributes=defun_kwargs)
    237 def wrapped_fn(*args):  # pylint: disable=missing-docstring
--> 238   ret = wrapper_helper(*args)
    239   ret = structure.to_tensor_list(self._output_structure, ret)
    240   return [ops.convert_to_tensor(t) for t in ret]

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:169, in StructuredFunctionWrapper.__init__.<locals>.wrapper_helper(*args)
    167 if not _should_unpack(nested_args):
    168   nested_args = (nested_args,)
--> 169 ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
    170 ret = variable_utils.convert_variables_to_tensors(ret)
    171 if _should_pack(ret):

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:692, in convert.<locals>.decorator.<locals>.wrapper(*args, **kwargs)
    690 except Exception as e:  # pylint:disable=broad-except
    691   if hasattr(e, 'ag_error_metadata'):
--> 692     raise e.ag_error_metadata.to_exception(e)
    693   else:
    694     raise

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:689, in convert.<locals>.decorator.<locals>.wrapper(*args, **kwargs)
    687 try:
    688   with conversion_ctx:
--> 689     return converted_call(f, args, kwargs, options=options)
    690 except Exception as e:  # pylint:disable=broad-except
    691   if hasattr(e, 'ag_error_metadata'):

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439, in converted_call(f, args, kwargs, caller_fn_scope, options)
    437 try:
    438   if kwargs is not None:
--> 439     result = converted_f(*effective_args, **kwargs)
    440   else:
    441     result = converted_f(*effective_args)

File /tmp/__autograph_generated_filezgyxt99w.py:12, in outer_factory.<locals>.inner_factory.<locals>.tf__parse_image(filename)
     10 parts = ag__.converted_call(ag__.ld(tf).strings.split, (ag__.ld(filename), ag__.ld(os).sep), None, fscope)
     11 label = ag__.ld(parts)[-2]
---> 12 image = ag__.converted_call(ag__.ld(tf).io.read_file, (ag__.ld(filename),), None, fscope)
     13 image = ag__.converted_call(ag__.ld(tf).image.decode_jpeg, (ag__.ld(image),), None, fscope)
     14 image = ag__.converted_call(ag__.ld(tf).image.convert_image_dtype, (ag__.ld(image), ag__.ld(tf).float32), None, fscope)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:331, in converted_call(f, args, kwargs, caller_fn_scope, options)
    329 if conversion.is_in_allowlist_cache(f, options):
    330   logging.log(2, 'Allowlisted %s: from cache', f)
--> 331   return _call_unconverted(f, args, kwargs, options, False)
    333 if ag_ctx.control_status_ctx().status == ag_ctx.Status.DISABLED:
    334   logging.log(2, 'Allowlisted: %s: AutoGraph is disabled in context', f)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:459, in _call_unconverted(f, args, kwargs, options, update_cache)
    457 if kwargs is not None:
    458   return f(*args, **kwargs)
--> 459 return f(*args)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/ops/io_ops.py:133, in read_file(filename, name)
     96 @tf_export(""io.read_file"", v1=[""io.read_file"", ""read_file""])
     97 def read_file(filename, name=None):
     98   """"""Reads the contents of file.
     99 
    100   This operation returns a tensor with the entire contents of the input
   (...)
    131     A tensor of dtype ""string"", with the file contents.
    132   """"""
--> 133   return gen_io_ops.read_file(filename, name)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/ops/gen_io_ops.py:586, in read_file(filename, name)
    584     pass  # Add nodes to the TensorFlow graph.
    585 # Add nodes to the TensorFlow graph.
--> 586 _, _, _op, _outputs = _op_def_library._apply_op_helper(
    587       ""ReadFile"", filename=filename, name=name)
    588 _result = _outputs[:]
    589 if _execute.must_record_gradient():

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py:795, in _apply_op_helper(op_type_name, name, **keywords)
    790 must_colocate_inputs = [val for arg, val in zip(op_def.input_arg, inputs)
    791                         if arg.is_ref]
    792 with _MaybeColocateWith(must_colocate_inputs):
    793   # Add Op to graph
    794   # pylint: disable=protected-access
--> 795   op = g._create_op_internal(op_type_name, inputs, dtypes=None,
    796                              name=scope, input_types=input_types,
    797                              attrs=attr_protos, op_def=op_def)
    799 # `outputs` is returned as a separate return value so that the output
    800 # tensors can the `op` per se can be decoupled so that the
    801 # `op_callbacks` can function properly. See framework/op_callbacks.py
    802 # for more details.
    803 outputs = op.outputs

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:707, in FuncGraph._create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)
    705   inp = self.capture(inp)
    706   captured_inputs.append(inp)
--> 707 return super()._create_op_internal(  # pylint: disable=protected-access
    708     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,
    709     compute_device)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:3814, in Graph._create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)
   3811 # _create_op_helper mutates the new Operation. `_mutation_lock` ensures a
   3812 # Session.run call cannot occur between creating and mutating the op.
   3813 with self._mutation_lock():
-> 3814   ret = Operation(
   3815       node_def,
   3816       self,
   3817       inputs=inputs,
   3818       output_types=dtypes,
   3819       control_inputs=control_inputs,
   3820       input_types=input_types,
   3821       original_op=self._default_original_op,
   3822       op_def=op_def)
   3823   self._create_op_helper(ret, compute_device=compute_device)
   3824 return ret

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:2112, in Operation.__init__(***failed resolving arguments***)
   2109     control_input_ops.append(control_op)
   2111 # Initialize c_op from node_def and other inputs
-> 2112 c_op = _create_c_op(g, node_def, inputs, control_input_ops, op_def=op_def)
   2113 self._init_from_c_op(c_op=c_op, g=g)
   2115 self._original_op = original_op

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    151 except Exception as e:
    152   filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153   raise e.with_traceback(filtered_tb) from None
    154 finally:
    155   del filtered_tb

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1973, in _create_c_op(graph, node_def, inputs, control_inputs, op_def, extract_traceback)
   1970   c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
   1971 except errors.InvalidArgumentError as e:
   1972   # Convert to ValueError for backwards compatibility.
-> 1973   raise ValueError(e.message)
   1975 # Record the current Python stack trace as the creating stacktrace of this
   1976 # TF_Operation.
   1977 if extract_traceback:

ValueError: in user code:

    File ""/tmp/ipykernel_26855/3929380215.py"", line 28, in parse_image  *
        image = tf.io.read_file(filename)

    ValueError: Shape must be rank 0 but is rank 1 for '{{node ReadFile}} = ReadFile[](args_0)' with input shapes: [?].
```
"
61161,keras.model.predict Generates Extra Batch and Inconsistent Number of Calls with Generator Dataset,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0.dev20230703

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When providing a Python generator to the `keras.Model.predict` function with a specified number of steps (`N`), the generator is called `N+1` times instead of `N` times. This behavior causes difficulties in resuming predictions and leads to an unused batch in the prediction output.

I found the same issue on the TensorFlow GitHub repository ([link to issue](https://github.com/tensorflow/tensorflow/issues/45459)). However, the issue was closed without a resolution, and the problem persists in my environment today. I believe this issue is worth revisiting and resolving, as it affects the predict function when using a generator dataset.

### Standalone code to reproduce the issue

I managed to replicate the problem in the same gist reported before ([gist for nightly](https://colab.research.google.com/gist/Saduf2019/552f6674aa5f547a6f2898c1a4edf9c2/untitled.ipynb))

### Relevant log output

_No response_"
61160,"tf.data Dataset: Warning when caching validation set. ""You should use `dataset.take(k).cache().repeat()` instead.""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.9.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.9.5

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda_11.3.r11.3/compiler.29920130_0

### GPU model and memory

NVIDIA A100-SXM4-80GB

### Current behavior?

Whenever I use the cache function on my tf.data validation dataset I get the warning below. When I use the cache only and without the validation set, no warning appears.

### Standalone code to reproduce the issue

```shell
dataset = tf.data.Dataset.from_generator(pygen.generator, args=[files,minmax],output_signature=(
    tf.TensorSpec(shape=s[0], dtype=tf.float32),
    tf.TensorSpec(shape=s[1], dtype=tf.float32)))

    val_dataset = tf.data.Dataset.from_generator(pygen.generator, args=[val_files,minmax],output_signature=(
    tf.TensorSpec(shape=s[0], dtype=tf.float32),
    tf.TensorSpec(shape=s[1], dtype=tf.float32)))
dataset = dataset.take(len(files)).cache().batch(args.bs).repeat(args.epochs).prefetch(tf.data.AUTOTUNE) 
    
val_dataset = val_dataset.take(len(val_files)).cache(filename=f'{tempfile.gettempdir()}/val').batch(64).repeat(args.epochs).prefetch(tf.data.AUTOTUNE)
strategy = tf.distribute.MultiWorkerMirroredStrategy()
        with strategy.scope():
            m = unet28.build(s[0])
            m.fit(dataset,validation_data=val_dataset, epochs=args.ep, steps_per_epoch = spe,validation_steps = vspe,callbacks=[model_checkpoint_callback,save50,model_csv_logger,model_tensorboard,model_earlystopping_30],verbose=2)
```


### Relevant log output

```shell
2023-07-03 13:47:14.532355: W tensorflow/core/kernels/data/cache_dataset_ops.cc:296] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
```
"
61159,Built from source Tensorflow is not linking (undefined references),"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.9.0

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

11th Gen Intel(R) Core(TM) i7-1165G7, 4 Core(s), 8 Logical Processor(s)

### Python version

3.9.13

### Bazel version

Bazel 5.0.0 

### GCC/compiler version

gcc.exe (MinGW-W64 x86_64-ucrt-posix-seh, built by Brecht Sanders) 12.2.0

### CUDA/cuDNN version

Not used

### GPU model and memory

Not used

### Current behavior?

I am trying to build this open source project https://github.com/gvne/spleeterpp/tree/master that uses Tensorflow. When I use the online available C-API builds (available here https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.9.0.zip), this works without an issue. However, when I build my dll and lib  files from source so I can take advantage from the `/:arch:AVX2`, I get the following errors:

```
D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/vst3sdk/build/VST3/Debug/ ""C:/Program Files/Common Files/VST3/"" && ""C:\Program Files\JetBrains\CLion 2023.1.2\bin\cmake\win\x64\bin\cmake.exe"" -E echo [SMTG] Finished.""""
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: lib/libspleeter_common.a(spleeter_common.cc.obj): in function `spleeter::Initialize(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, spleeter::SeparationType, std::error_code&)':
D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/spleeter_common.cc:28: undefined reference to `__imp_TF_NewSessionOptions'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/spleeter_common.cc:28: undefined reference to `__imp_TF_DeleteSessionOptions'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/spleeter_common.cc:29: undefined reference to `__imp_TF_NewGraph'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/spleeter_common.cc:29: undefined reference to `__imp_TF_DeleteGraph'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/spleeter_common.cc:30: undefined reference to `__imp_TF_NewBuffer'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/spleeter_common.cc:30: undefined reference to `__imp_TF_DeleteBuffer'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/spleeter_common.cc:31: undefined reference to `__imp_TF_NewBuffer'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/spleeter_common.cc:31: undefined reference to `__imp_TF_DeleteBuffer'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/spleeter_common.cc:32: undefined reference to `__imp_TF_NewStatus'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/spleeter_common.cc:32: undefined reference to `__imp_TF_DeleteStatus'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/spleeter_common.cc:35: undefined reference to `__imp_TF_LoadSessionFromSavedModel'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/spleeter_common.cc:40: undefined reference to `__imp_TF_GetCode'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: lib/libspleeter_common.a(tf_handle.cc.obj): in function `spleeter::SessionDeleter(TF_Session*)':
D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/tf_handle.cc:6: undefined reference to `__imp_TF_NewStatus'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/tf_handle.cc:6: undefined reference to `__imp_TF_DeleteStatus'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/tf_handle.cc:7: undefined reference to `__imp_TF_DeleteSession'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_common/tf_handle.cc:8: undefined reference to `__imp_TF_GetCode'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: lib/libspleeter_filter.a(filter.cc.obj): in function `spleeter::Filter::AsyncProcessTransformedBlock(std::vector<std::complex<float>*, std::allocator<std::complex<float>*> >, unsigned int)':
D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/filter.cc:233: undefined reference to `__imp_TF_GraphOperationByName'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/filter.cc:238: undefined reference to `__imp_TF_GraphOperationByName'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/filter.cc:244: undefined reference to `__imp_TF_NewStatus'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/filter.cc:244: undefined reference to `__imp_TF_DeleteStatus'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/filter.cc:245: undefined reference to `__imp_TF_SessionRun'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/filter.cc:249: undefined reference to `__imp_TF_GetCode'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/filter.cc:256: undefined reference to `__imp_TF_DeleteTensor'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: lib/libspleeter_filter.a(filter.cc.obj): in function `void spleeter::Copy<float>(TF_Tensor const*, std::vector<long long, std::allocator<long long> >, std::shared_ptr<spleeter::TFHandle<TF_Tensor> >)':
D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/tensor.h:146: undefined reference to `__imp_TF_TensorData'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/tensor.h:147: undefined reference to `__imp_TF_TensorData'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: lib/libspleeter_filter.a(filter.cc.obj): in function `void spleeter::Copy<std::complex<float> >(TF_Tensor const*, std::vector<long long, std::allocator<long long> >, std::shared_ptr<spleeter::TFHandle<TF_Tensor> >)':
D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/tensor.h:146: undefined reference to `__imp_TF_TensorData'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/tensor.h:147: undefined reference to `__imp_TF_TensorData'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: lib/libspleeter_filter.a(filter.cc.obj): in function `spleeter::internal::Adapter<std::complex<float> >::operator()(unsigned long long, unsigned long long, unsigned long long)':
D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/tensor.h:33: undefined reference to `__imp_TF_TensorData'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: lib/libspleeter_filter.a(filter.cc.obj):D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/tensor.h:33: more undefined references to `__imp_TF_TensorData' follow
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: lib/libspleeter_filter.a(tensor.cc.obj): in function `spleeter::TensorAlloc(TF_DataType, std::vector<long long, std::allocator<long long> >)':
D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/tensor.cc:34: undefined reference to `__imp_TF_NewTensor'
D:/wecode_build_tools/mingw/bin/../lib/gcc/x86_64-w64-mingw32/9.3.0/../../../../x86_64-w64-mingw32/bin/ld.exe: D:/Users/superkogito/Desktop/workspace/cpp/audio_framework_v2/plugin/my_plugins/ImGuiExample/external/spleeterpp/src/spleeter_filter/tensor.cc:36: undefined reference to `__imp_TF_DeleteTensor'
collect2.exe: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.

```


I tried to make some edits to the source code to fix this, as mentioned here under step 11 from https://medium.com/vitrox-publication/deep-learning-frameworks-tensorflow-build-from-source-on-windows-python-c-cpu-gpu-d3aa4d0772d8. 

> Some Bug Fixing Before Compiling :
> Before we start, we should edit the TensorFlow source code, because you will end up with linking errors (unresolved external symbols) as shown below if you follow the typical framework building steps.
> 
> 
> Session and SessionOptions symbols are not exported
> So what are these errors?
> 
> Dynamic-link library(DLL) can only expose a maximum of 60,000 symbols which is one of DLL limitation. As TensorFlow has more than 60,000 symbols, so we have to manually choose which symbols to be exposed if they are not exposed from TensorFlow by default.
> 
> According to the experience, in order to deploy a TensorFlow model, the TensorFlow Session and SessionOptions Classes are required to be exported. The steps include:


However, this did not build and caused the following log when running: `bazel build --config=opt tensorflow:tensorflow.dll`


```
D:\Users\superkogito\Downloads\tensorflow-2.9.0>bazel build --config=opt tensorflow:tensorflow.dll
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=162
INFO: Reading rc options for 'build' from d:\users\superkogito\downloads\tensorflow-2.9.0\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/ProgramData/chocolatey/bin/python.exe
INFO: Reading rc options for 'build' from d:\users\superkogito\downloads\tensorflow-2.9.0\.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from d:\users\superkogito\downloads\tensorflow-2.9.0\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe --action_env PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages --python_path=C:/ProgramData/Anaconda3/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from d:\users\superkogito\downloads\tensorflow-2.9.0\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file d:\users\superkogito\downloads\tensorflow-2.9.0\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\users\superkogito\downloads\tensorflow-2.9.0\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file d:\users\superkogito\downloads\tensorflow-2.9.0\.tf_configure.bazelrc: --copt=/arch:AVX2 --host_copt=/arch:AVX2
INFO: Found applicable config definition build:windows in file d:\users\superkogito\downloads\tensorflow-2.9.0\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file d:\users\superkogito\downloads\tensorflow-2.9.0\.bazelrc: --define framework_shared_object=false
INFO: Analyzed target //tensorflow:tensorflow.dll (257 packages loaded, 20074 targets configured).
INFO: Found 1 target...
ERROR: D:/users/superkogito/downloads/tensorflow-2.9.0/tensorflow/core/common_runtime/BUILD:1654:11: Compiling tensorflow/core/common_runtime/session_options.cc failed: (Exit 2): cl.exe failed: error executing command
  cd /d D:/users/superkogito/_bazel_superkogito/6mghjs3w/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.36.32532\include;C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.36.32532\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.36.32532\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\Common7\Tools\;;C:\windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=D:\Users\superkogito\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=D:\Users\superkogito\AppData\Local\Temp
  C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.36.32532\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/tensorflow/core/common_runtime/_objs/session_options/session_options.obj.params
# Configuration: e6cdbc5f9ea1d062c246ab7d360b46f14ec764581d9f093d895b4b2995265b6c
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'
.\tensorflow/core/public/session_options.h(28): error C2143: syntax error: missing ';' before '<class-head>'
.\tensorflow/core/public/session_options.h(28): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int
.\tensorflow/core/public/session_options.h(60): error C3646: 'SessionOptions': unknown override specifier
.\tensorflow/core/public/session_options.h(60): error C2059: syntax error: '('
.\tensorflow/core/public/session_options.h(60): error C2238: unexpected token(s) preceding ';'
tensorflow/core/common_runtime/session_options.cc(22): error C2600: 'tensorflow::SessionOptions::SessionOptions': cannot define a compiler-generated special member function (must be declared in the class first)
Target //tensorflow:tensorflow.dll failed to build
INFO: Elapsed time: 5602.387s, Critical Path: 708.69s
INFO: 7456 processes: 2165 internal, 5291 local.
FAILED: Build did NOT complete successfully
```

Any help or advice regarding this is appreciated. Thank you.

### Standalone code to reproduce the issue

```shell
- Clone https://github.com/gvne/spleeterpp/tree/master
- Build with tensorflow 2.9.0 downloaded from here https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.9.0.zip
- Build Tensorflow 2.9.0 from source. 
- Try to rebuild https://github.com/gvne/spleeterpp/tree/master with the tensorflow built from source.
```


### Relevant log output

_No response_"
61158,clip_by_norm incorrectly receives negative clip_norm and outputs unexpected result,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following the documentation, tf.clip_by_norm should not accept negative clip_norm (https://www.tensorflow.org/api_docs/python/tf/clip_by_norm). But it can.

```
import tensorflow as tf
import numpy as np

t = tf.constant(np.ones((3, 3)),)

res = tf.clip_by_norm(t, clip_norm=-10, axes=[1])
print(res)
```
```
tf.Tensor(
[[-5.77350269 -5.77350269 -5.77350269]
 [-5.77350269 -5.77350269 -5.77350269]
 [-5.77350269 -5.77350269 -5.77350269]], shape=(3, 3), dtype=float64)
```

If I change t to all zero tensor, it even outputs NaN:
```
import tensorflow as tf
import numpy as np

t = tf.constant(np.zeros((3, 3)),)

res = tf.clip_by_norm(t, clip_norm=-10, axes=[1])
print(res)
```
```
tf.Tensor(
[[nan nan nan]
 [nan nan nan]
 [nan nan nan]], shape=(3, 3), dtype=float64)
```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

t = tf.constant(np.ones((3, 3)),)
res = tf.clip_by_norm(t, clip_norm=-10, axes=[1])
print(res)

t = tf.constant(np.zeros((3, 3)),)
res = tf.clip_by_norm(t, clip_norm=-10, axes=[1])
print(res)
```
```


### Relevant log output

_No response_"
61157,[MLIR] Fix tf.StridedSlice lowering to tosa with new_axis_mask/shrink_axis_mask,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0rc2

### Custom code

Yes

### OS platform and distribution

Linux Ubunto 18.04

### Mobile device

_No response_

### Python version

3.9.1

### Bazel version

6.1.2

### GCC/compiler version

clang 15.0.2

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The following MLIR input is not support:

```
func.func @test_strided_slice_new_axis_mask(%arg0: tensor<1x14x8xf32>) -> tensor<1x14x8x1xf32> {
  %strides = ""tf.Const""() {device = """", value = dense<1> : tensor<4xi32>} : () -> tensor<4xi32>
  %begin_end = ""tf.Const""() {device = """", value = dense<0> : tensor<4xi32>} : () -> tensor<4xi32>
  %res = ""tf.StridedSlice""(%arg0, %begin_end, %begin_end, %strides) {begin_mask = 7 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 7 : i64, new_axis_mask = 8 : i64, shrink_axis_mask = 0 : i64} : (tensor<1x14x8xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<1x14x8x1xf32>
  func.return %res : tensor<1x14x8x1xf32>
}

// -----

func.func @test_strided_slice_shrink_axis_mask(%arg0: tensor<1x14x8x1xf32>) -> tensor<1x14x8xf32> {
  %strides = ""tf.Const""() {device = """", value = dense<1> : tensor<4xi32>} : () -> tensor<4xi32>
  %begin = ""tf.Const""() {device = """", value = dense<0> : tensor<4xi32>} : () -> tensor<4xi32>
  %end = ""tf.Const""() {device = """", value = dense<[0, 0, 0, 1]> : tensor<4xi32>} : () -> tensor<4xi32>
  %res = ""tf.StridedSlice""(%arg0, %begin, %end, %strides) {begin_mask = 7 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 7 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 8 : i64} : (tensor<1x14x8x1xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<1x14x8xf32>
  func.return %res : tensor<1x14x8xf32>
}
```

### Standalone code to reproduce the issue

```shell
Use MLIR new tests in https://github.com/tensorflow/tensorflow/pull/60939 to see the error.
```


### Relevant log output

```shell
I create pull request to fix the issue:
https://github.com/tensorflow/tensorflow/pull/60939
```
"
61150,TF Lite produces wrong graph when tensor broadcasting exists ,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.14.0-dev20230701

### 2. Code
This is the minimized code to reproduce the issue:
```python
import tensorflow as tf
import numpy as np
input_shape = [2, 2]
x1 = tf.constant([[0., 0.], [0., 0.]], shape=input_shape)

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()
    self.w1 = tf.Variable([[0.], [1.]])
    self.m1 = tf.Variable([[1.], [1.]])
  @tf.function(input_signature=[tf.TensorSpec(x1.shape, x1.dtype)])
  def call(self, x1):
    x2 = tf.constant([1.], shape=[1])
    x3 = x1 + x2 #broadcast
    return tf.matmul(x3, self.w1) + self.m1

m = Model()
expected_value = m(x1)
print(expected_value.numpy())

converter = tf.lite.TFLiteConverter.from_keras_model(m)
tflite_model = converter.convert()

def _evaluateTFLiteModel(tflite_model, input_data):
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    for i in range(len(input_data)):
        interpreter.set_tensor(input_details[i]['index'], input_data[i])
    interpreter.invoke()
    output_data = [interpreter.get_tensor(output_details[i]['index'])
                   for i in range(len(output_details))]
    return output_data


actual_value = _evaluateTFLiteModel(tflite_model,[x1])
print(actual_value[0])
tf.lite.experimental.Analyzer.analyze(model_content=tflite_model)
```
### 3. Failure after conversion
Output:
```
Keras mode output:  [[2.] [2.]]
Lite mode output:  [[1.] [1.]]
```
Lite IR:
```
Subgraph#0 main(T#0) -> [T#3]
  Op#0 FULLY_CONNECTED(T#0, T#2, T#1) -> [T#3]

Tensors of Subgraph#0
  T#0(serving_default_args_0:0) shape:[2, 2], type:FLOAT32
  T#1(Const) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 2, data:[1]
  T#2(MatMul) shape:[1, 2], type:FLOAT32 RO 8 bytes, buffer: 3, data:[0, 1]
  T#3(StatefulPartitionedCall:0) shape:[2, 1], type:FLOAT32

```
Model produces wrong results:
- Using ```tf.lite.experimental.Analyzer.analyze```, we can see the bias (```T#1```) equals to [1]. However, the correct bias should be [2].

"
61149,tf.image.extract_patches error for tf.RaggedTensor inputs,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.12.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.image.extract_patches` should be able to extract patches from `ragged `tensors.

### Standalone code to reproduce the issue

```shell
def build_model():
  input = tf.keras.Input([None, None, 3], ragged=True, name=""image"")
  patches = tf.image.extract_patches(
              images=input,
              sizes=[1, 4, 4, 1],
              strides=[1, 4, 4, 1],
              rates=[1, 1, 1, 1],
              padding=""SAME"",
          )
  
  return tf.keras.Model(
            inputs=input,
            outputs=patches)
  

model = build_model()
```


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-48-0f2eac36aeae> in <cell line: 16>()
     14 
     15 
---> 16 model = build_model()

3 frames
/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---> 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

TypeError: Exception encountered when calling layer ""tf.image.extract_patches_3"" (type TFOpLambda).

Failed to convert elements of tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(""Placeholder:0"", shape=(None, 3), dtype=float32), row_splits=Tensor(""Placeholder_1:0"", shape=(None,), dtype=int64)), row_splits=Tensor(""Placeholder_2:0"", shape=(None,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.

Call arguments received by layer ""tf.image.extract_patches_3"" (type TFOpLambda):
  • images=tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(""Placeholder:0"", shape=(None, 3), dtype=float32), row_splits=Tensor(""Placeholder_1:0"", shape=(None,), dtype=int64)), row_splits=Tensor(""Placeholder_2:0"", shape=(None,), dtype=int64))
  • sizes=['1', '4', '4', '1']
  • strides=['1', '4', '4', '1']
  • rates=['1', '1', '1', '1']
  • padding='SAME'
  • name=None
```
"
61137,Not correct result from tf.split(),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.8-2.9-2.10

### Custom code

No

### OS platform and distribution

Both on Mac and on Ubuntu

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Tryng to split a four axes tensor, I got some unexpected zeros in the splitted tensors.
This behaviour is not presented in the tf 2.12. Is an old bug?

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

vec=tf.ones((1,768,3,1400))
splitted=tf.split(vec,[256,512],axis=1)
print(splitted) ##zeros have occured in the tensor
```


### Relevant log output

```shell
No
```
"
61136,Label of image looks pretty small in detection,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.11

### Custom code

Yes

### OS platform and distribution

*

### Mobile device

*

### Python version

python 3.7

### Bazel version

*

### GCC/compiler version

*

### CUDA/cuDNN version

*

### GPU model and memory

Mac OS 2012 

### Current behavior?







*

### Standalone code to reproduce the issue

```shell
I am working on final year project of Pakistani sign language detecteion project is complete but when I tried to detect image in real time the label is shown very small and barely able to read it is there any way we can modified the label to look bigger
```


### Relevant log output

```shell
I am working on final year project of Pakistani sign language detecteion project is complete but when I tried to detect image in real time the label is shown very small and barely able to read it is there any way we can modified the label to look bigger 

thanks
```
"
61132,_SparseSparseMaximumGrad throws TypeError in forward gradient computation,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0-dev20230630

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the function has a `tf.sparse.maximum` call, the forward-mode gradient computation fails with `TypeError: _SparseSparseMaximumGrad() takes 2 positional arguments but 3 were given`.

After inspection, i find that SparseTensor is not supported for differentiation. In the example code below, computing reverse-mode gradient of `y` (sparse tensor) will result in a `TypeError: Type SparseTensor is not supported as a gradient source or gradient target.`

However, in the forward mode gradient computation, I am trying to compute gradient for a non-sparse tensor `complex_data`, and I think it should not be affected by the  `tf.sparse.maximum` call?

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

real_data = tf.random.uniform([4, 4], dtype=tf.float32)
imag_data = tf.random.uniform([4, 4], dtype=tf.float32)

def foo(real_data, imag_data):
    complex_data = tf.complex(real_data, imag_data)
    y = tf.sparse.maximum(tf.sparse.from_dense(real_data), tf.sparse.from_dense(imag_data))
    return complex_data, y

with tf.GradientTape(persistent=True) as t:
    t.watch(real_data)
    t.watch(imag_data)
    complex_data, y = foo(real_data, imag_data)

g = t.gradient(complex_data, real_data) # works
# g = t.gradient(y, real_data) # ValueError: Type SparseTensor is not supported as a gradient source or gradient target.


tangents = tf.ones((4,4))
with tf.autodiff.ForwardAccumulator(real_data,tangents) as acc:
    complex_data, y = foo(real_data, imag_data)
    # TypeError

# Never reach here
# jvp = acc.jvp(complex_data)
# print(jvp)
```


### Relevant log output

```shell
TypeError: _SparseSparseMaximumGrad() takes 2 positional arguments but 3 were given
```
"
61131,SavedModel is not deterministic when saved with enable_op_determinism=True,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.9.2

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have a model with monte-carlo sampling layers. At train time it is ok for sampling to be random and I do not set a seed.
At inference this model MUST output deterministic results - so the sampled values must be reproducible.
I need to save this model after training using `tf.keras.models.save_model()` and then load it with `tf.keras.models.load_model()`

When training is complete I save the model like so:
```
tf.keras.utils.set_random_seed(42)
tf.config.experimental.enable_op_determinism()
model.seed = 42
tf.keras.models.save_model(model, save_path, save_format=""tf"")
```

To load it:
```
import tensorflow as tf
tf.keras.utils.set_random_seed(42)
tf.config.experimental.enable_op_determinism()
model = tf.keras.models.load_model(save_path)
for _ in range(10):
 output = model(input_tensor, training=False)
```
I expect output to be always the same for the same input - within one python process and when reloading, however it is not.

This is what my model looks like (only the relevant portions):

```
class CustomModel(tf.keras.Model):
   def __init__(self, *args, **kwargs):
       self._seed = None
       self.mc_layer = CustomMonteCarloSamplingLayer()

    @tf.keras.utils.register_keras_serializable(name=""seed"")
    @property
    def seed(self):
        return self._seed
     
    @seed.setter
    def seed(self, value):
        if is_op_determinism_enabled():
            lgr.info(""Setting random seed"")
            self._seed = value
            tf.random.set_seed(value)
        else:
            lgr.info(""TF OP Determinism not set"")

   def call(self, inputs, training=True):
       return self.mc_layer(inputs, seed=self._seed)
```

Internally in `CustomMonteCarloSamplingLayer` I use the seed value like so:
```
tf.random.set_seed(seed)
samples= dist.sample(num_samples, seed=seed)
```
where dist is a distribution from tensorflow_probability.

When `self._seed = None` which is the case in training, `dist.sample` will randomly generate values.
When `self._seed` is set then I need `dist.sample` to be reproducible.

The loaded model, even with op determinism enabled during saving and loading, is not reproducible.

      


### Standalone code to reproduce the issue

```shell
See above
```


### Relevant log output

_No response_"
61129,mutiple issues with the new parameter server strategy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.10.1

### Custom Code

Yes

### OS Platform and Distribution

ubuntu 16.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

We meet multiple issues in using the Tensorflow 2.x strategy:
1. how to shard data by files, how to use the later binding mechanism to shard data by files, which is critical for high performance training using tf.data.Dataset apis, all workers reading the total data is not scalable.
2. how to control the placement of ops, in one attempt, we build dataset using tf.data.Dataset.from_tensor_slices and.     
     dataset = dataset.interleave(lambda x: tf.data.TextLineDataset(x).skip(1),        cycle_length=15,       num_parallel_calls=15)
   with parameter server strategy, the dataset related ops runs on workers, however, if I add one more op dataset.repeat(), the dataset ops all runs on chief, which is surprising.
4. how to ensure even the workers are with uneven amount of data, the training process with model.fit could end elegantly instead of having to using try catch or data.repeat, as recommendation models generally assume one epoch training.

[yuefengz@google.com](mailto:yuefengz@google.com)
[rchao@google.com](mailto:rchao@google.com)

### Standalone code to reproduce the issue

```shell
# for chief: 
file_list = tf.io.gfile.glob(input_pattern)
dataset = tf.data.Dataset.from_tensor_slices(file_list)
dataset = dataset.shard(worker_num, worker_id)  # how to later bind worker_id?
dataset = dataset.interleave(lambda x: tf.data.TextLineDataset(x).skip(1),        cycle_length=15,       num_parallel_calls=15)

def _parse_csv(line):
  record_defaults = []
  for x in all_columns:
    record_defaults.append(0)
  with tf.control_dependencies([tf.print(tf.shape(line), line[0], output_stream=sys.stderr)]):
    fields = tf.io.decode_csv(
        line,
        field_delim=',',
        record_defaults=record_defaults,
        name='decode_csv')
    return fields

dataset = dataset.map(_parse_csv, num_parallel_calls=8)
dataset = dataset.repeat()

strategy = tf.distribute.experimental.ParameterServerStrategy(
      cluster_resolver, variable_partitioner=variable_partitioner)

with strategy.scope():
  model = build_model(...)

model.fit(dataset, epochs=num_epoch,
     callbacks=[
         tf.keras.callbacks.ProgbarLogger(count_mode='steps'),
         tf.keras.callbacks.TensorBoard(log_dir='/train/tensorboard/', histogram_freq=0)
     ],
     steps_per_epoch=steps_per_epoch)

# for workers and ps:
server = tf.distribute.Server(
      cluster_resolver.cluster_spec(),
      job_name=cluster_resolver.task_type,
      task_index=cluster_resolver.task_id,
      protocol=cluster_resolver.rpc_layer or 'grpc',
      start=True)
    server.join()
```


### Relevant log output

```shell
# tf.print outputs are all on chief stdout, and no improvements if we place the dataset built process under strategy.scope() or using ops.device() operations.
```
</details>"
61126,Internal error: Error applying delegate when trying to use TensorFlow Lite NNAPI delegate on Google Pixel 7,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tensorflow-lite 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

Google Pixel 7

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When I try to use the NNAPI delegate to run the OpenAI whisper model on a Pixel 7, it crashes when trying to initialize the TFLite interpreter.

### Standalone code to reproduce the issue

```shell
MainActivity.kt
class MainActivity : ComponentActivity() {
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)

        setContent {
            NNAPIWhisperTestTheme {
                // A surface container using the 'background' color from the theme
                Surface(
                    modifier = Modifier.fillMaxSize(),
                    color = MaterialTheme.colorScheme.background
                ) {
                    Test(applicationContext)
                }
            }
        }
    }
}

@Composable
fun Test(applicationContext: Context, modifier: Modifier = Modifier, viewModel: NnApiViewModel = viewModel()) {
    Scaffold { innerPadding ->
        Column {
            Button(modifier = Modifier.padding(innerPadding), onClick = { viewModel.initialize(applicationContext) }) {
                Text(text = ""INITIALIZE"")
            }
            Button(
                onClick = {
                    viewModel.runInference()
                },
                modifier = Modifier.padding(innerPadding)
            ) {
                Text(viewModel.output + viewModel.elapsed)
            }
        }
    }
}



NnApiViewModel.kt
class NnApiViewModel : ViewModel() {

    var options = Interpreter.Options()
    var nnApiDelegate: NnApiDelegate? = null
    var tfLite: Interpreter? = null
    var output by mutableStateOf("""")
    var elapsed by mutableStateOf(0L)

    // Initialize interpreter with NNAPI delegate for Android Pie or above
    fun initialize(applicationContext: Context) {
        nnApiDelegate = NnApiDelegate()
        options.useNNAPI = true
        NnApiDelegate.Options().useNnapiCpu = false
        options.addDelegate(nnApiDelegate)

        val model = applicationContext.assets.open(""models/whisper-tiny.tflite"")
        val file = model.readBytes()

        val fileName = ""whisper-tiny.tflite""
        applicationContext.openFileOutput(fileName, Context.MODE_PRIVATE).use {
            it.write(file)
        }

        val modelFile = File(applicationContext.filesDir,""whisper-tiny.tflite"")

        // Initialize TFLite interpreter
        try {
            tfLite = Interpreter(modelFile, options)
        } catch (e: Exception) {
            throw RuntimeException(e)
        }
    }

    fun runInference() {
        try {
            val outputShape = tfLite?.getOutputTensor(0)
            val input = TensorBuffer.createFixedSize(intArrayOf(1, 80, 3000), DataType.FLOAT32)
            val output = TensorBuffer.createFixedSize(outputShape?.shape(), DataType.FLOAT32)

            val start = System.currentTimeMillis()
            tfLite?.run(input.buffer, output.buffer)
            elapsed = System.currentTimeMillis() - start
        } catch (e: RuntimeException) {
            throw RuntimeException(e)
        }
    }

    fun unload() {
        tfLite?.close()
        nnApiDelegate?.close()
    }
}
```


### Relevant log output

```shell
I  Loaded native library: tensorflowlite_jni
I  Didn't load native library: tensorflowlite_jni_gms_client
I  Initialized TensorFlow Lite runtime.
I  DeviceManager::DeviceManager
I  findAvailableDevices
E  Error opening trace file: No such file or directory (2)
I  Found interface google-edgetpu (version = 2.0)
I  Found interface google-armnn (version = ArmNN)
I  Created TensorFlow Lite delegate for NNAPI.
E  FATAL EXCEPTION: main                                                                                                                                                                                                   java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Error applying delegate:
```
</details>"
61121,"` #include ""include/float8.h""  // from @ml_dtypes` is missing","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0

### Custom Code

Yes

### OS Platform and Distribution

docker/tensorflow/tensorflow:nightly-gpu

### Mobile device

No

### Python version

3.8

### Bazel version

NA

### GCC/Compiler version

NR

### CUDA/cuDNN version

NR

### GPU model and memory

NR

### Current Behaviour?

create a test.cc:
```
#include ""tensorflow/core/framework/op.h""
```
compile it with flags provided by TensorFlow:
get_compile_flags():
```-I/usr/local/lib/python3.8/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=1 --std=c++17 -DEIGEN_MAX_ALIGN_BYTES=64```
get_link_flags():
```-L/usr/local/lib/python3.8/dist-packages/tensorflow -l:libtensorflow_framework.so.2```

```g++ -Wl,-R,'$ORIGIN/..' -Wl,-rpath,'$ORIGIN' --std=c++17 -DNDEBUG -shared  test.cc -o /tmp/test.so -fPIC -I/usr/local/lib/python3.8/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=1 --std=c++17 -DEIGEN_MAX_ALIGN_BYTES=64 -I/usr/include -I/usr/local/cuda/include -L/usr/local/lib/python3.8/dist-packages/tensorflow -l:libtensorflow_framework.so.2  -O2```

The result is:
```
      In file included from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/tsl/platform/types.h:22,
                       from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/tsl/framework/numeric_types.h:22,
                       from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:24,
                       from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/bfloat16.h:19,
                       from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:24,
                       from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/op_def_builder.h:28,
                       from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/full_type_inference_util.h:24,
                       from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:27,
                       from /tmp/pip-req-build-88874pcq/daliop.cc:19:
      /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/tsl/platform/float8.h:19:10: fatal error: include/float8.h: No such file or directory
         19 | #include ""include/float8.h""  // from @ml_dtypes
```
It is probably caused by [this PR](https://github.com/tensorflow/tensorflow/commit/ef1ea4f5c5c36209b6bd56a99fdd71e5052f6d63).

Either tensorflow/tsl/platform/float8.h should have `#include ""external/ml_dtypes/include/float8.h""  // from @ml_dtypes`, or `get_compile_flags()` should include `-I/usr/local/lib/python3.8/dist-packages/tensorflow/include/external/ml_dtypes`

### Standalone code to reproduce the issue

```shell
As above
```


### Relevant log output

```shell
As above
```
</details>"
61118,Tensorflow Lite for Windows and macOS,"Hi guys,
So i am new to On Device Machine Learning which is super cool but, the limitation with tf-lite is that i can only use in Android or iOS not in Windows Apps nor MacOS.
I request any of you who has dealt with embedding tfLite with Desktop class Apps to provide a simple solution. "
61117,`tf.math.reduce_prod` produces wrong second-order gradient,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0-dev20230628

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

`tf.math.reduce_prod` produces wrong second-order gradient when the input tensor has a `0` element. 

In the following example, `d2z_dydx` should be `1`, but the output value is `0`. Note that, if we replace `z = tf.math.reduce_prod(tf.stack([x, y]))` with `z = x * y`, the assertion passes.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.Variable(0.0)
y = tf.Variable(1.0)
with tf.GradientTape(persistent=True) as t1:
    t1.watch(x)
    t1.watch(y)
    with tf.GradientTape(persistent=True) as t2:
        t2.watch(x)
        t2.watch(y)
        z = tf.math.reduce_prod(tf.stack([x, y]))
    print('z = x * y: ', z)  
    dz_dx = t2.gradient(z, x) # dz_dx = y
    print('dz_dx: ', dz_dx)
    dz_dy = t2.gradient(z, y) # dz_dy = x
    print('dz_dy: ', dz_dy)

d2z_dx2 = t1.gradient(dz_dx, x) # d2z_dx2 = 0
print('d2z_dx2', d2z_dx2)
d2z_dxdy = t1.gradient(dz_dx, y) # d2z_dxdy = 1
print('d2z_dxdy', d2z_dxdy)
assert d2z_dxdy == 1
d2z_dydx = t1.gradient(dz_dy, x) # d2z_dydx = 1
print('d2z_dydx', d2z_dydx)
assert d2z_dydx == 1, 'd2z_dydx = {}'.format(d2z_dydx)
# AssertionError: d2z_dydx = 0.0
```


### Relevant log output

```shell
AssertionError: d2z_dydx = 0.0
```
</details>"
61116,//tensorflow/python/data/kernel_tests:snapshot_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/data/kernel_tests:snapshot_test sometimes fails

x86 log
https://source.cloud.google.com/results/invocations/8b60bfba-b6b6-4503-aa43-62e8bbe1a094/log

AARCH64 log


### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
ERROR: testWriteSnapshotDatasetSameFingerprintIncompleteRunRestart_test_mode_eager_tfapiversion_2 (__main__.SnapshotTest)
SnapshotTest.testWriteSnapshotDatasetSameFingerprintIncompleteRunRestart_test_mode_eager_tfapiversion_2
testWriteSnapshotDatasetSameFingerprintIncompleteRunRestart_test_mode_eager_tfapiversion_2(mode='eager', tf_api_version=2)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/snapshot_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/snapshot_test.py"", line 63, in tearDown
    shutil.rmtree(self._snapshot_dir)
  File ""/usr/lib/python3.9/shutil.py"", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File ""/usr/lib/python3.9/shutil.py"", line 673, in _rmtree_safe_fd
    onerror(os.rmdir, fullname, sys.exc_info())
  File ""/usr/lib/python3.9/shutil.py"", line 671, in _rmtree_safe_fd
    os.rmdir(entry.name, dir_fd=topfd)
OSError: [Errno 39] Directory not empty: '5643068742232426178'

======================================================================
FAIL: testWriteSnapshotDatasetSameFingerprintIncompleteRunRestart_test_mode_eager_tfapiversion_2 (__main__.SnapshotTest)
SnapshotTest.testWriteSnapshotDatasetSameFingerprintIncompleteRunRestart_test_mode_eager_tfapiversion_2
testWriteSnapshotDatasetSameFingerprintIncompleteRunRestart_test_mode_eager_tfapiversion_2(mode='eager', tf_api_version=2)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/snapshot_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314, in bound_param_test
    return test_method(self, **testcase_params)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/snapshot_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360, in decorated
    execute_test_method()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/snapshot_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343, in execute_test_method
    test_method(**kwargs_to_pass)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/snapshot_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/snapshot_test.py"", line 318, in testWriteSnapshotDatasetSameFingerprintIncompleteRunRestart
    self.assertSnapshotDirectoryContains(
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/snapshot_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/snapshot_test.py"", line 108, in assertSnapshotDirectoryContains
    self.assertLen(run_dirlist, num_snapshot_shards_per_run)
AssertionError: ['00000000.shard', '00000001.shard', '00000002.shard', '00000003.shard', '00000004.shard', '00000005.shard', '00000006.shard', '00000007.shard', '00000008.shard', '00000009.shard', '00000010.shard', '00000011.shard', '00000012.shard', '00000013.shard', '00000014.shard', '00000015.shard', '00000016.shard', '00000017.shard', '00000018.shard', '00000019.shard', '00000020.shard', '00000021.shard', '00000022.shard', '00000023.shard', '00000024.shard', '00000025.shard', '00000026.shard', '00000027.shard', '00000028.shard', '00000029.shard', '00000030.shard', '00000031.shard', '00000032.shard', '00000033.shard', '00000034.shard', '00000035.shard', '00000036.shard', '00000037.shard', '00000038.shard', '00000039.shard', '00000040.shard', '00000041.shard', '00000042.shard', '00000043.shard', '00000044.shard', '00000045.shard', '00000046.shard', '00000047.shard', '00000048.shard', '00000049.shard', '00000050.shard', '00000051.shard', '00000052.shard', '00000053.shard', '00000054.shard', '00000055.shard', '00000056.shard', '00000057.shard', '00000058.shard', '00000059.shard', '00000060.shard', '00000061.shard', '00000062.shard', '00000063.shard', '00000064.shard', '00000065.shard', '00000066.shard', '00000067.shard', '00000068.shard', '00000069.shard', '00000070.shard', '00000071.shard', '00000072.shard', '00000073.shard', '00000074.shard', '00000075.shard', '00000076.shard', '00000077.shard', '00000078.shard', '00000079.shard', '00000080.shard', '00000081.shard', '00000082.shard', '00000083.shard', '00000084.shard', '00000085.shard', '00000086.shard', '00000087.shard', '00000088.shard', '00000089.shard', '00000090.shard', '00000091.shard', '00000092.shard', '00000093.shard', '00000094.shard', '00000095.shard', '00000096.shard', '00000099.shard', '00000101.shard', '00000103.shard', '00000104.shard', '00000105.shard', '00000108.shard', '00000110.shard', '00000115.shard', '00000116.shard', '00000121.shard'] has length of 107, expected 128.

----------------------------------------------------------------------
Ran 20 tests in 5.563s
```
</details>"
61115,//tensorflow/python/distribute/experimental/rpc:rpc_ops_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute/experimental/rpc:rpc_ops_test sometimes fails.

x86 log
https://source.cloud.google.com/results/invocations/e7ac5e31-66d5-4f2e-a095-045cb52cc20f/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5348094575/jobs/9697499180#step:5:8263

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
FAIL: //tensorflow/python/distribute/experimental/rpc:rpc_ops_test (shard 4 of 7) (see /root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/distribute/experimental/rpc/rpc_ops_test/shard_4_of_7/test.log)
INFO: From Testing //tensorflow/python/distribute/experimental/rpc:rpc_ops_test (shard 4 of 7):
==================== Test output for //tensorflow/python/distribute/experimental/rpc:rpc_ops_test (shard 4 of 7):
2023-06-27 21:51:01.865535: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-27 21:51:01.915964: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Running tests under Python 3.9.17: /usr/bin/python3
[ RUN      ] RpcOpsTest.test_client_timeout
E0627 21:51:07.067311728 3191958 server_chttp2.cc:40]        {""created"":""@1687902667.067282680"",""description"":""Only 1 addresses added out of total 2 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":404,""referenced_errors"":[{""created"":""@1687902667.067278835"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::1]:34603""}]}
2023-06-27 21:51:07.067457: I tensorflow/distribute/experimental/rpc/kernels/rpc_ops.cc:347] Server listening on: localhost:34603
2023-06-27 21:51:08.565889: I tensorflow/distribute/experimental/rpc/kernels/rpc_ops.cc:314] Shutting down server listening on: localhost:34603
INFO:tensorflow:time(__main__.RpcOpsTest.test_client_timeout): 2.71s
I0627 21:51:08.574481 139721505019712 test_util.py:2464] time(__main__.RpcOpsTest.test_client_timeout): 2.71s
[  FAILED  ] RpcOpsTest.test_client_timeout
[ RUN      ] RpcOpsTest.test_queue_resource
/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/experimental/rpc/rpc_ops_test.runfiles/org_tensorflow/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tf.NoneTensorSpec; loading this StructuredValue will require that this type be imported and registered.
  warnings.warn(""Encoding a StructuredValue with type %s; loading this ""
E0627 21:51:08.630379336 3163442 server_chttp2.cc:40]        {""created"":""@1687902668.630352103"",""description"":""Only 1 addresses added out of total 2 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":404,""referenced_errors"":[{""created"":""@1687902668.630348956"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::1]:43377""}]}
2023-06-27 21:51:08.630532: I tensorflow/distribute/experimental/rpc/kernels/rpc_ops.cc:347] Server listening on: localhost:43377
2023-06-27 21:51:08.686495: I tensorflow/distribute/experimental/rpc/kernels/rpc_ops.cc:314] Shutting down server listening on: localhost:43377
INFO:tensorflow:time(__main__.RpcOpsTest.test_queue_resource): 0.14s
I0627 21:51:08.712013 139721505019712 test_util.py:2464] time(__main__.RpcOpsTest.test_queue_resource): 0.14s
[       OK ] RpcOpsTest.test_queue_resource
[ RUN      ] RpcOpsTest.test_rpc_ops_non_blocking_convenience_methods
E0627 21:51:08.774584099 3163442 server_chttp2.cc:40]        {""created"":""@1687902668.774551355"",""description"":""Only 1 addresses added out of total 2 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":404,""referenced_errors"":[{""created"":""@1687902668.774547071"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::1]:39177""}]}
2023-06-27 21:51:08.774736: I tensorflow/distribute/experimental/rpc/kernels/rpc_ops.cc:347] Server listening on: localhost:39177
2023-06-27 21:51:08.798979: I tensorflow/distribute/experimental/rpc/kernels/rpc_ops.cc:314] Shutting down server listening on: localhost:39177
INFO:tensorflow:time(__main__.RpcOpsTest.test_rpc_ops_non_blocking_convenience_methods): 0.09s
I0627 21:51:08.802805 139721505019712 test_util.py:2464] time(__main__.RpcOpsTest.test_rpc_ops_non_blocking_convenience_methods): 0.09s
[       OK ] RpcOpsTest.test_rpc_ops_non_blocking_convenience_methods
======================================================================
ERROR: test_client_timeout (__main__.RpcOpsTest)
RpcOpsTest.test_client_timeout
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/experimental/rpc/rpc_ops_test.runfiles/org_tensorflow/tensorflow/python/distribute/experimental/rpc/rpc_ops_test.py"", line 487, in test_client_timeout
    client = rpc_ops.GrpcClient(
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/experimental/rpc/rpc_ops_test.runfiles/org_tensorflow/tensorflow/python/distribute/experimental/rpc/rpc_ops.py"", line 342, in __init__
    self._client_handle, methods = gen_rpc_ops.rpc_client(
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/experimental/rpc/rpc_ops_test.runfiles/org_tensorflow/tensorflow/distribute/experimental/rpc/kernels/gen_rpc_ops.py"", line 333, in rpc_client
    return rpc_client_eager_fallback(
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/experimental/rpc/rpc_ops_test.runfiles/org_tensorflow/tensorflow/distribute/experimental/rpc/kernels/gen_rpc_ops.py"", line 407, in rpc_client_eager_fallback
    _result = _execute.execute(b""RpcClient"", 2, inputs=_inputs_flat,
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/experimental/rpc/rpc_ops_test.runfiles/org_tensorflow/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnavailableError: {{function_node __wrapped__RpcClient_device_/job:localhost/replica:0/task:0/device:CPU:0}} GOAWAY received
Additional GRPC error information while calling /tensorflow.rpc.RpcService/List:
:{""created"":""@1687902668.574079798"",""description"":""Error received from peer ipv4:127.0.0.1:34603"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""GOAWAY received"",""grpc_status"":14} [Op:RpcClient]
```
</details>"
61113,//tensorflow/python/data/experimental/kernel_tests/service:local_workers_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/data/experimental/kernel_tests/service:local_workers_test sometimes fails or timeouts.

x86 log
https://source.cloud.google.com/results/invocations/e41a9dd4-19a3-4298-b34f-6a32eca50e08/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5383187293/jobs/9769619751#step:5:8335

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
INFO: From Testing //tensorflow/python/data/experimental/kernel_tests/service:local_workers_test (shard 1 of 24):
==================== Test output for //tensorflow/python/data/experimental/kernel_tests/service:local_workers_test (shard 1 of 24):
2023-06-27 23:02:53.397107: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-27 23:02:53.528865: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Running tests under Python 3.9.17: /usr/bin/python3
[ RUN      ] LocalTaskGarbageCollectTest.testMultipleEpochsSharedJob_test_mode_eager_tfapiversion_1_numremoteworkers_0
[  SKIPPED ] LocalTaskGarbageCollectTest.testMultipleEpochsSharedJob_test_mode_eager_tfapiversion_1_numremoteworkers_0
[ RUN      ] LocalTaskGarbageCollectTest.testReadFromDeletedTask_test_mode_eager_tfapiversion_1_numremoteworkers_0
[  SKIPPED ] LocalTaskGarbageCollectTest.testReadFromDeletedTask_test_mode_eager_tfapiversion_1_numremoteworkers_0
[ RUN      ] LocalWorkersTest.testAnonymousJobWithDifferentTargetWorkers_test_mode_graph_tfapiversion_2
INFO:tensorflow:Using local port 43055
I0627 23:02:57.097305 139801719478080 test_util.py:3796] Using local port 43055
2023-06-27 23:02:57.099356: I tensorflow/core/data/service/dispatcher_impl.cc:223] Attempting to restore dispatcher state from journal in /root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/_tmp/dbc16dd8be682dbdabf0b589b8e98f16j99fj6xd/tmptm5ix2lv/tf_data_dispatcher_journal
2023-06-27 23:02:57.099421: I tensorflow/core/data/service/dispatcher_impl.cc:230] No journal found. Starting dispatcher from new state.
2023-06-27 23:02:57.099574: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data DispatchServer running at 0.0.0.0:43055
INFO:tensorflow:Using local port 44899
I0627 23:02:57.099842 139801719478080 test_util.py:3796] Using local port 44899
2023-06-27 23:02:57.101683: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055
2023-06-27 23:02:57.101828: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:44899
INFO:tensorflow:Using local port 41615
I0627 23:02:57.102149 139801719478080 test_util.py:3796] Using local port 41615
2023-06-27 23:02:57.103351: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055
2023-06-27 23:02:57.103477: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:41615
INFO:tensorflow:Using local port 37691
I0627 23:02:57.103664 139801719478080 test_util.py:3796] Using local port 37691
2023-06-27 23:02:57.104726: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055
2023-06-27 23:02:57.104854: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:37691
INFO:tensorflow:Using local port 39883
I0627 23:02:57.107191 139801719478080 test_util.py:3796] Using local port 39883
2023-06-27 23:02:57.584682: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-27 23:02:57.619502: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-27 23:02:57.634234: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-27 23:02:57.788004: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-27 23:02:59.175789: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055
2023-06-27 23:02:59.176006: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:39883
INFO:tensorflow:Using local port 34849
I0627 23:02:59.176878 139801719478080 test_util.py:3796] Using local port 34849
2023-06-27 23:02:59.252223: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055
2023-06-27 23:02:59.252462: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:34849
INFO:tensorflow:Using local port 46439
I0627 23:02:59.253159 139801719478080 test_util.py:3796] Using local port 46439
2023-06-27 23:02:59.287010: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055
2023-06-27 23:02:59.287229: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:46439
/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/data/ops/dataset_ops.py:458: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.
  warnings.warn(""To make it possible to preserve tf.data options across ""
WARNING:tensorflow:From /usr/lib/python3.9/contextlib.py:87: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
W0627 23:02:59.486639 139801719478080 deprecation.py:364] From /usr/lib/python3.9/contextlib.py:87: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
2023-06-27 23:02:59.508867: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled
-- Test timed out at 2023-06-27 23:07:52 UTC --
Current thread 0x00007f261fd41740 (most recent call first):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1477 in _call_tf_sessionrun
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1384 in _run_fn
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1401 in _do_call
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1394 in _do_run
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1214 in _run
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 971 in run
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2061 in run
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2691 in evaluate
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/test_base.py"", line 237 in assertDatasetProduces
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.py"", line 238 in testAnonymousJobWithDifferentTargetWorkers
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343 in execute_test_method
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360 in decorated
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314 in bound_param_test
  File ""/usr/lib/python3.9/unittest/case.py"", line 550 in _callTestMethod
  File ""/usr/lib/python3.9/unittest/case.py"", line 592 in run
  File ""/usr/lib/python3.9/unittest/case.py"", line 651 in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.9/unittest/runner.py"", line 184 in run
  File ""/usr/lib/python3.9/unittest/main.py"", line 271 in runTests
  File ""/usr/lib/python3.9/unittest/main.py"", line 101 in __init__
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/absl_py/absl/testing/absltest.py"", line 2537 in _run_and_get_tests_result
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/absl_py/absl/testing/absltest.py"", line 2568 in run_tests
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/absl_py/absl/testing/absltest.py"", line 2156 in _run_in_app
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/absl_py/absl/testing/absltest.py"", line 2049 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/absl_py/absl/app.py"", line 258 in _run_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/absl_py/absl/app.py"", line 312 in run
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 489 in benchmarks_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 56 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/eager/test.py"", line 25 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 167 in test_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 165 in test_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.py"", line 441 in <module>
```
</details>"
61112,//tensorflow/python/distribute:vars_test_2gpu is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute:vars_test_2gpu timeouts sometimes.

x86 log
https://source.cloud.google.com/results/invocations/769764d8-8dc9-46fa-a284-78062efe3bd9/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5363572382/jobs/9731245377#step:5:9313

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=rbe --config=pycpp --config=build_event_export
```


### Relevant log output

```shell
[ RUN      ] SyncOnReadScatterReplicaTest.testScatterMax_test_aggregation_VariableAggregationMEAN_distribution_MultiWorkerMirrored2x1CPU_mode_graph_usevarpolicy_True
W0628 01:02:37.513197 140343714740032 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.
WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.
W0628 01:02:37.513532 140343714740032 collective_all_reduce_strategy.py:394] Collective ops is not configured at program startup. Some performance features may not be enabled.
INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)
I0628 01:02:37.516878 140343714740032 mirrored_strategy.py:423] Using MirroredStrategy with devices ('/device:CPU:0',)
INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO
I0628 01:02:37.517337 140343714740032 collective_all_reduce_strategy.py:446] Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO
[chief-0]:     W0628 01:02:37.520201 140325400614720 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.
I0628 01:02:37.521484 140343714740032 multi_process_runner.py:989] Waiting for the result from chief-0
[worker-0]:    W0628 01:02:37.523296 140325400614720 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.
[chief-0]:     W0628 01:02:37.521138 140325400614720 context.py:881] Enabling collective ops after program startup may cause error when accessing previously created tensors.
[worker-0]:    W0628 01:02:37.524588 140325400614720 context.py:881] Enabling collective ops after program startup may cause error when accessing previously created tensors.
[chief-0]:     INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:chief/replica:0/task:0/device:CPU:0', '/job:chief/replica:0/task:0/device:CPU:1']
[chief-0]:     I0628 01:02:37.525059 140325400614720 collective_all_reduce_strategy.py:531] Enabled multi-worker collective ops with available devices: ['/job:chief/replica:0/task:0/device:CPU:0', '/job:chief/replica:0/task:0/device:CPU:1']
[worker-0]:    INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:CPU:1']
[worker-0]:    I0628 01:02:37.527691 140325400614720 collective_all_reduce_strategy.py:531] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:CPU:1']
[chief-0]:     INFO:tensorflow:Using MirroredStrategy with devices ('/job:chief/task:0/device:CPU:0',)
[chief-0]:     I0628 01:02:37.529053 140325400614720 mirrored_strategy.py:423] Using MirroredStrategy with devices ('/job:chief/task:0/device:CPU:0',)
[chief-0]:     INFO:tensorflow:Check health not enabled.
[chief-0]:     I0628 01:02:37.529462 140325400614720 collective_all_reduce_strategy.py:574] Check health not enabled.
[chief-0]:     INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'chief', task_id = 0, num_workers = 2, local_devices = ('/job:chief/task:0/device:CPU:0',), communication = CommunicationImplementation.AUTO
[chief-0]:     I0628 01:02:37.529720 140325400614720 collective_all_reduce_strategy.py:576] MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'chief', task_id = 0, num_workers = 2, local_devices = ('/job:chief/task:0/device:CPU:0',), communication = CommunicationImplementation.AUTO
[worker-0]:    INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:0/device:CPU:0',)
[worker-0]:    I0628 01:02:37.531570 140325400614720 mirrored_strategy.py:423] Using MirroredStrategy with devices ('/job:worker/task:0/device:CPU:0',)
[worker-0]:    INFO:tensorflow:Check health not enabled.
[worker-0]:    I0628 01:02:37.532161 140325400614720 collective_all_reduce_strategy.py:574] Check health not enabled.
[worker-0]:    INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:CPU:0',), communication = CommunicationImplementation.AUTO
[worker-0]:    I0628 01:02:37.532691 140325400614720 collective_all_reduce_strategy.py:576] MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:CPU:0',), communication = CommunicationImplementation.AUTO
[worker-0]:    2023-06-28 01:02:37.540481: I tensorflow/core/distributed_runtime/master.cc:240] Scanning workers for devices: 1 total workers
[chief-0]:     2023-06-28 01:02:37.541763: I tensorflow/core/distributed_runtime/master.cc:240] Scanning workers for devices: 1 total workers
-- Test timed out at 2023-06-28 01:07:29 UTC --
Thread 0x00007fa429ef8700 (most recent call first):
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 258 in _continuously_readline_from_sub
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Thread 0x00007fa42a779700 (most recent call first):
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 527 in _process_watchdog
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Thread 0x00007fa44d0fc700 (most recent call first):
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 258 in _continuously_readline_from_sub
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Current thread 0x00007fa451437740 (most recent call first):
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 379 in _recv
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 414 in _recv_bytes
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 250 in recv
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 991 in run
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/combinations.py"", line 580 in decorator
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343 in execute_test_method
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360 in decorated
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/parameterized.py"", line 314 in bound_param_test
  File ""/usr/lib/python3.9/unittest/case.py"", line 550 in _callTestMethod
  File ""/usr/lib/python3.9/unittest/case.py"", line 592 in run
  File ""/usr/lib/python3.9/unittest/case.py"", line 651 in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.9/unittest/runner.py"", line 184 in run
  File ""/usr/lib/python3.9/unittest/main.py"", line 271 in runTests
  File ""/usr/lib/python3.9/unittest/main.py"", line 101 in __init__
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/absltest.py"", line 2537 in _run_and_get_tests_result
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/absltest.py"", line 2568 in run_tests
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/absltest.py"", line 2156 in _run_in_app
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/absltest.py"", line 2049 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/app.py"", line 258 in _run_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/app.py"", line 312 in run
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 489 in benchmarks_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 56 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/eager/test.py"", line 25 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 167 in test_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 1455 in test_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/test_util.py"", line 138 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/vars_test.py"", line 1336 in <module>
```
</details>"
61111,//tensorflow/python/distribute/failure_handling:gce_failure_handler_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute/failure_handling:gce_failure_handler_test sometimes fails or timeouts.

x86 log
https://source.cloud.google.com/results/invocations/66d8ddaa-3dbe-4464-837c-053157b11659/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5354839739/jobs/9712379821#step:5:12470

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
INFO:tensorflow:time(__main__.GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_True_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker): 5.11s
I0628 05:40:30.221971 140074262497088 test_util.py:2464] time(__main__.GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_True_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker): 5.11s
[       OK ] GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_True_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker
======================================================================
FAIL: test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_False_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker (__main__.GceFailureHandlingTest)
GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_False_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker
test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_False_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker(api_wrapping_train=False, grace_period=7, input_arg='checkpoint', strategy_option='MWMS_multi_worker')
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314, in bound_param_test
    return test_method(self, **testcase_params)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360, in decorated
    execute_test_method()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343, in execute_test_method
    test_method(**kwargs_to_pass)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/combinations.py"", line 559, in decorator
    test_method(self, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.py"", line 417, in test_multiple_workers_preempted_consecutively
    mpr.join(timeout=250)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 649, in join
    self._reraise_if_subprocess_error(process_statuses)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 565, in _reraise_if_subprocess_error
    six.reraise(*process_status.exc_info)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/six_archive/six.py"", line 719, in reraise
    raise value
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 1060, in _run_contained
    return_value = fn(*args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.py"", line 211, in worker_fn
    self.assertNotEmpty(checkpoint_index)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/absl_py/absl/testing/absltest.py"", line 972, in assertNotEmpty
    self.fail('{!r} has length of 0.'.format(container), msg)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/absl_py/absl/testing/absltest.py"", line 1814, in fail
    return super(TestCase, self).fail(self._formatMessage(prefix, msg))
  File ""/usr/lib/python3.9/unittest/case.py"", line 676, in fail
    raise self.failureException(msg)
AssertionError: [] has length of 0.
```
</details>"
61109,//tensorflow/python/data/experimental/kernel_tests/service:auto_shard_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/data/experimental/kernel_tests/service:auto_shard_test will sometimes timeout.

x86 log
https://source.cloud.google.com/results/invocations/75230523-dc04-40ad-bcf3-06df3b94a119/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5383235016/jobs/9769729147#step:5:8442

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
TIMEOUT: //tensorflow/python/data/experimental/kernel_tests/service:auto_shard_test (Summary)
      /root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test/shard_6_of_32/test.log
INFO: From Testing //tensorflow/python/data/experimental/kernel_tests/service:auto_shard_test (shard 6 of 32):
==================== Test output for //tensorflow/python/data/experimental/kernel_tests/service:auto_shard_test (shard 6 of 32):
2023-06-28 08:01:34.863480: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 08:01:34.953626: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Running tests under Python 3.9.17: /usr/bin/python3
[ RUN      ] AutoShardTest.testBatchDataset_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyFILEORDATA
[  SKIPPED ] AutoShardTest.testBatchDataset_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyFILEORDATA
[ RUN      ] AutoShardTest.testEnumerateShardingPolicies_test_mode_graph_tfapiversion_1_shardingpolicy_ShardingPolicyDYNAMIC
[  SKIPPED ] AutoShardTest.testEnumerateShardingPolicies_test_mode_graph_tfapiversion_1_shardingpolicy_ShardingPolicyDYNAMIC
[ RUN      ] AutoShardTest.testRangeDataset_AutoShard_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyFILEORDATA
[  SKIPPED ] AutoShardTest.testRangeDataset_AutoShard_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyFILEORDATA
[ RUN      ] AutoShardTest.testRangeDataset_ShardHintUsedInWrongShardingPolicy_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyOFF
[  SKIPPED ] AutoShardTest.testRangeDataset_ShardHintUsedInWrongShardingPolicy_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyOFF
[ RUN      ] AutoShardTest.testTFRecordDataset_FewerFilesThanWorkers_DataShard_test_mode_eager_tfapiversion_2
INFO:tensorflow:Using local port 35531
I0628 08:02:03.552320 139750982424384 test_util.py:3796] Using local port 35531
2023-06-28 08:02:03.553738: I tensorflow/core/data/service/dispatcher_impl.cc:223] Attempting to restore dispatcher state from journal in /root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/_tmp/df4595e40f65712a12ed239eda341855ao10glmf/tmplcxh6ash/tf_data_dispatcher_journal
2023-06-28 08:02:03.553799: I tensorflow/core/data/service/dispatcher_impl.cc:230] No journal found. Starting dispatcher from new state.
2023-06-28 08:02:03.553987: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data DispatchServer running at 0.0.0.0:35531
INFO:tensorflow:Using local port 43623
I0628 08:02:03.556421 139750982424384 test_util.py:3796] Using local port 43623
2023-06-28 08:02:04.452927: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 08:02:04.495706: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 08:02:04.557281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-28 08:02:04.586634: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-28 08:02:06.406403: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:35531
2023-06-28 08:02:06.406887: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:43623
INFO:tensorflow:Using local port 40059
I0628 08:02:06.407576 139750982424384 test_util.py:3796] Using local port 40059
E0628 08:02:06.423661297  858602 server_chttp2.cc:40]        {""created"":""@1687939326.423469587"",""description"":""No address added out of total 1 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":395,""referenced_errors"":[{""created"":""@1687939326.423463864"",""description"":""Failed to add any wildcard listeners"",""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc"",""file_line"":342,""referenced_errors"":[{""created"":""@1687939326.423438207"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::]:40059""},{""created"":""@1687939326.423463182"",""description"":""Unable to configure socket"",""fd"":8,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":216,""referenced_errors"":[{""created"":""@1687939326.423455518"",""description"":""Address already in use"",""errno"":98,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":189,""os_error"":""Address already in use"",""syscall"":""bind""}]}]}]}
Process _RemoteWorkerProcess-2:
Traceback (most recent call last):
  File ""/usr/lib/python3.9/multiprocessing/process.py"", line 315, in _bootstrap
    self.run()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 54, in _run_with_absl
    app.run(lambda _: self._run_impl())
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/absl_py/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/absl_py/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 54, in <lambda>
    app.run(lambda _: self._run_impl())
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 42, in run
    self.start_worker()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 50, in start_worker
    self._worker.start()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/test_base.py"", line 110, in start
    self._server.start()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/service/server_lib.py"", line 415, in start
    self._server.start()
RuntimeError: Could not start gRPC server
-- Test timed out at 2023-06-28 08:06:34 UTC --
```
</details>"
61107,//tensorflow/python/distribute:cross_device_ops_test_2gpu is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute:cross_device_ops_test_2gpu fails or timeouts sometimes

x86 log
https://source.cloud.google.com/results/invocations/3008a6bc-b49f-4776-871c-1c5ae046a470/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5379162649/jobs/9759994992#step:5:9913

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
[ RUN      ] CollectiveOpsTest.testBatchReduceDense_test_implementation_CommunicationImplementationRING_numprocesses_2_preferuniqueinstancekey_False_reduceop_ReduceOpSUM_requiredgpus_0
[worker-0]:    W0628 09:23:51.698908 140256382134080 context.py:881] Enabling collective ops after program startup may cause error when accessing previously created tensors.
I0628 09:23:51.700115 139955121788736 multi_process_runner.py:989] Waiting for the result from worker-0
I0628 09:23:51.702625 139955121788736 multi_process_runner.py:989] Waiting for the result from worker-0
I0628 09:23:51.702917 139955121788736 multi_process_runner.py:989] Waiting for the result from worker-1
I0628 09:23:51.711031 139955121788736 multi_process_runner.py:989] Waiting for the result from worker-0
[worker-0]:    2023-06-28 09:23:51.734731: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:38213
[worker-1]:    E0628 09:23:51.740952892 1194361 server_chttp2.cc:40]        {""created"":""@1687944231.740904797"",""description"":""No address added out of total 1 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":395,""referenced_errors"":[{""created"":""@1687944231.740902919"",""description"":""Failed to add any wildcard listeners"",""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc"",""file_line"":342,""referenced_errors"":[{""created"":""@1687944231.740878363"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::]:36443""},{""created"":""@1687944231.740901905"",""description"":""Unable to configure socket"",""fd"":9,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":216,""referenced_errors"":[{""created"":""@1687944231.740897648"",""description"":""Address already in use"",""errno"":98,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":189,""os_error"":""Address already in use"",""syscall"":""bind""}]}]}]}
[worker-1]:    2023-06-28 09:23:51.741121: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:608] UNKNOWN: Could not start gRPC server
[worker-1]:    2023-06-28 09:23:51.741306: E tensorflow/core/common_runtime/eager/context_distributed_manager.cc:703] Could not start gRPC server
-- Test timed out at 2023-06-28 09:28:39 UTC --
```
</details>"
61106,Gradient Error (No gradient defined for operation 'bilateral_layer_1/Lu_10' (op type : Lu)) in CRFLayer,"The issue is from the new refined UNet v3.0 which i try to train my dataset on.
the link to the repo of that model is here:
[https://github.com/92xianshen/refined-unet-v3](url)

I am working on this code on Jupyter notebook for my convenience.
I design the Unet architecture code based on the architecture given in this UNet.py file of repo.

I am training this model on the data of pets dataset (oxford-iiit-pets dataset) whose link is given below:
[https://www.kaggle.com/datasets/tanlikesmath/the-oxfordiiit-pet-dataset](url)

The data from the dataset is categorized into training and validation dataset already.
The Jupyter Notebook named Fresh_Unet3 is attached.

The problem is that the whole code runs successfully and model also got compiled, model summary obtained, but when i try to fit the model, it gives error which is:

StagingError: in user code:

    File ""c:\users\dell\appdata\local\programs\python\python39\lib\site-packages\keras\engine\training.py"", line 1284, in train_function  *
        return step_function(self, iterator)  #here
    File ""c:\users\dell\appdata\local\programs\python\python39\lib\site-packages\keras\engine\training.py"", line 1268, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))  #here
    File ""c:\users\dell\appdata\local\programs\python\python39\lib\site-packages\keras\engine\training.py"", line 1249, in run_step  **
        outputs = model.train_step(data)  #here
    File ""c:\users\dell\appdata\local\programs\python\python39\lib\site-packages\keras\engine\training.py"", line 1054, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)  #here
    File ""c:\users\dell\appdata\local\programs\python\python39\lib\site-packages\keras\optimizers\optimizer.py"", line 542, in minimize
        grads_and_vars = self.compute_gradients(loss, var_list, tape)  #here
    File ""c:\users\dell\appdata\local\programs\python\python39\lib\site-packages\keras\optimizers\optimizer.py"", line 275, in compute_gradients
        grads = tape.gradient(loss, var_list)  #here

    LookupError: No gradient defined for operation'bilateral_layer_1/Lu_10' (op type: Lu). In general every operation must have an associated `@tf.RegisterGradient` for correct autodiff, which this op is lacking. If you want to pretend this operation is a constant in your program, you may insert `tf.stop_gradient`. This can be useful to silence the error in cases where you know gradients are not needed, e.g. the forward pass of tf.custom_gradient. Please see more details in https://www.tensorflow.org/api_docs/python/tf/custom_gradient.

The library versions are as follows:
python = 3.9.6
tensorflow = '2.12.0'
keras = '2.12.0'

I am attaching zip file containing Jupyter Notebook of the file i'm working on and a Python file named CRFLayer.py which contains the code of CRF.
[UNET_MODELv3.zip](https://github.com/tensorflow/tensorflow/files/11893789/UNET_MODELv3.zip)

According to me, the error could be in the BilateralLayer class inside CRFLayer.py file.
Please provide your valuable suggestions and solution to this problem."
61105,//tensorflow/python/distribute:moving_averages_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute:moving_averages_test fails sometimes.

x86 log
https://source.cloud.google.com/results/invocations/1e048065-76db-4dab-b1a5-093dd542b27d/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5383507734/jobs/9770356325#step:5:8417

Looks like a network port conflict issue

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
INFO: From Testing //tensorflow/python/distribute:moving_averages_test_cpu (shard 3 of 5):
==================== Test output for //tensorflow/python/distribute:moving_averages_test_cpu (shard 3 of 5):
2023-06-28 10:24:27.228811: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 10:24:27.325938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Running tests under Python 3.9.17: /usr/bin/python3
[ RUN      ] AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph
INFO:tensorflow:time(__main__.AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph): 0.03s
I0628 10:24:32.088060 139996622530368 test_util.py:2464] time(__main__.AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph): 0.03s
[  SKIPPED ] AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph
[ RUN      ] AssignMovingAveragesTest.testAssignVariable_test_distribution_MultiWorkerMirrored4x1CPU_mode_graph
W0628 10:24:32.126455 139996622530368 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.
WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.
W0628 10:24:32.126971 139996622530368 collective_all_reduce_strategy.py:394] Collective ops is not configured at program startup. Some performance features may not be enabled.
INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)
I0628 10:24:32.164073 139996622530368 mirrored_strategy.py:423] Using MirroredStrategy with devices ('/device:CPU:0',)
INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO
I0628 10:24:32.171577 139996622530368 collective_all_reduce_strategy.py:446] Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO
INFO:tensorflow:Using local port 43531
I0628 10:24:32.173116 139996622530368 test_util.py:3796] Using local port 43531
INFO:tensorflow:Using local port 34115
I0628 10:24:32.173517 139996622530368 test_util.py:3796] Using local port 34115
INFO:tensorflow:Using local port 34873
I0628 10:24:32.173699 139996622530368 test_util.py:3796] Using local port 34873
INFO:tensorflow:Using local port 40455
I0628 10:24:32.173828 139996622530368 test_util.py:3796] Using local port 40455
2023-06-28 10:24:32.849027: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 10:24:32.904538: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-28 10:24:32.917529: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 10:24:32.995322: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[chief-0]:     I0628 10:24:34.534415 140662937323328 multi_process_runner.py:840] Subprocess with PID 1244 (chief, 0) is now being started.
[chief-0]:     I0628 10:24:34.534823 140662937323328 multi_process_runner.py:842] TF_CONFIG: '{""cluster"": {""chief"": [""localhost:43531""], ""worker"": [""localhost:34115"", ""localhost:34873"", ""localhost:40455""]}, ""task"": {""type"": ""chief"", ""index"": 0}, ""rpc_layer"": ""grpc""}'
I0628 10:24:34.561606 139996622530368 multi_process_runner.py:989] Waiting for the result from chief-0
[worker-1]:    I0628 10:24:34.596723 140662937323328 multi_process_runner.py:840] Subprocess with PID 1479 (worker, 1) is now being started.
[worker-1]:    I0628 10:24:34.597120 140662937323328 multi_process_runner.py:842] TF_CONFIG: '{""cluster"": {""chief"": [""localhost:43531""], ""worker"": [""localhost:34115"", ""localhost:34873"", ""localhost:40455""]}, ""task"": {""type"": ""worker"", ""index"": 1}, ""rpc_layer"": ""grpc""}'
[worker-1]:    2023-06-28 10:24:34.655212: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:34873
[worker-1]:    INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
[worker-1]:    I0628 10:24:34.661444 140662937323328 collective_all_reduce_strategy.py:531] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
[chief-0]:     E0628 10:24:34.621842982    1244 server_chttp2.cc:40]        {""created"":""@1687947874.621805165"",""description"":""No address added out of total 1 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":395,""referenced_errors"":[{""created"":""@1687947874.621803913"",""description"":""Failed to add any wildcard listeners"",""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc"",""file_line"":342,""referenced_errors"":[{""created"":""@1687947874.621786452"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::]:43531""},{""created"":""@1687947874.621803346"",""description"":""Unable to configure socket"",""fd"":9,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":216,""referenced_errors"":[{""created"":""@1687947874.621800946"",""description"":""Address already in use"",""errno"":98,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":189,""os_error"":""Address already in use"",""syscall"":""bind""}]}]}]}
[chief-0]:     2023-06-28 10:24:34.621944: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:608] UNKNOWN: Could not start gRPC server
[worker-0]:    I0628 10:24:34.608072 140662937323328 multi_process_runner.py:840] Subprocess with PID 1472 (worker, 0) is now being started.
[chief-0]:     2023-06-28 10:24:34.622255: E tensorflow/core/common_runtime/eager/context_distributed_manager.cc:703] Could not start gRPC server
[chief-0]:     Process _Process-3:
[chief-0]:     Traceback (most recent call last):
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/strategy_combinations.py"", line 207, in skip_if_cannot_start_grpc_server
[chief-0]:         return _create_multi_worker_mirrored()
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/strategy_combinations.py"", line 189, in _create_multi_worker_mirrored
[chief-0]:         strategy = CollectiveAllReduceStrategy(cluster_resolver=resolver)
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 186, in __init__
[chief-0]:         CollectiveAllReduceExtended(
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 339, in __init__
[chief-0]:         self._initialize_strategy(self._cluster_resolver, devices=devices)
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 358, in _initialize_strategy
[chief-0]:         self._initialize_multi_worker(cluster_resolver)
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 530, in _initialize_multi_worker
[chief-0]:         context.context().ensure_initialized()
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 610, in ensure_initialized
[chief-0]:         pywrap_tfe.TFE_EnableCollectiveOps(context_handle, server_def_str)
[chief-0]:     tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server
```
</details>"
61104,//tensorflow/python/distribute/failure_handling:failure_handler_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute/failure_handling:failure_handler_test will timeout sometimes

x86 log
https://source.cloud.google.com/results/invocations/302ca1a4-593b-430c-b278-038351228670/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5363735926/jobs/9731502578#step:5:11034

Looks like a network port conflict.

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
INFO: From Testing //tensorflow/python/distribute/failure_handling:failure_handler_test (shard 4 of 8):
==================== Test output for //tensorflow/python/distribute/failure_handling:failure_handler_test (shard 4 of 8):
2023-06-22 16:46:57.933478: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-22 16:46:58.014195: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Running tests under Python 3.9.17: /usr/bin/python3
[ RUN      ] PreemptionCheckpointTest.test_grace_period_continue_training_test_inputarg_checkpoint_strategyoption_MWMSmultiworker
INFO:tensorflow:Using local port 38723
I0622 16:47:02.418146 140436227843904 test_util.py:3796] Using local port 38723
INFO:tensorflow:Using local port 41053
I0622 16:47:02.418907 140436227843904 test_util.py:3796] Using local port 41053
INFO:tensorflow:Using local port 45087
I0622 16:47:02.419116 140436227843904 test_util.py:3796] Using local port 45087
INFO:tensorflow:Using local port 38125
I0622 16:47:02.419290 140436227843904 test_util.py:3796] Using local port 38125
2023-06-22 16:47:03.130184: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-22 16:47:03.185580: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-22 16:47:03.185763: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-22 16:47:03.241253: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO:tensorflow:Cluster starting.
I0622 16:47:04.817156 140436227843904 failure_handler_test.py:432] Cluster starting.
[worker-0]:    I0622 16:47:04.842670 140652189284160 multi_process_runner.py:840] Subprocess with PID 588346 (worker, 0) is now being started.
[worker-0]:    I0622 16:47:04.842952 140652189284160 multi_process_runner.py:842] TF_CONFIG: '{""cluster"": {""worker"": [""localhost:38723"", ""localhost:41053"", ""localhost:45087"", ""localhost:38125""]}, ""task"": {""type"": ""worker"", ""index"": 0}, ""rpc_layer"": ""grpc""}'
[worker-1]:    I0622 16:47:04.858144 140652189284160 multi_process_runner.py:840] Subprocess with PID 588646 (worker, 1) is now being started.
[worker-1]:    I0622 16:47:04.858554 140652189284160 multi_process_runner.py:842] TF_CONFIG: '{""cluster"": {""worker"": [""localhost:38723"", ""localhost:41053"", ""localhost:45087"", ""localhost:38125""]}, ""task"": {""type"": ""worker"", ""index"": 1}, ""rpc_layer"": ""grpc""}'
[worker-0]:    2023-06-22 16:47:04.907068: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:38723
[worker-2]:    I0622 16:47:04.927917 140652189284160 multi_process_runner.py:840] Subprocess with PID 588933 (worker, 2) is now being started.
[worker-0]:    2023-06-22 16:47:04.943758: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:551] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 14520256932767538069
[worker-0]:    2023-06-22 16:47:04.944905: I tensorflow/tsl/distributed_runtime/coordination/coordination_service_agent.cc:299] Coordination agent has successfully connected.
[worker-2]:    I0622 16:47:04.928328 140652189284160 multi_process_runner.py:842] TF_CONFIG: '{""cluster"": {""worker"": [""localhost:38723"", ""localhost:41053"", ""localhost:45087"", ""localhost:38125""]}, ""task"": {""type"": ""worker"", ""index"": 2}, ""rpc_layer"": ""grpc""}'
[worker-1]:    2023-06-22 16:47:04.995414: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:41053
[worker-0]:    2023-06-22 16:47:04.997341: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:551] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 7622272584115739518
[worker-1]:    2023-06-22 16:47:04.998064: I tensorflow/tsl/distributed_runtime/coordination/coordination_service_agent.cc:299] Coordination agent has successfully connected.
[worker-3]:    I0622 16:47:05.006252 140652189284160 multi_process_runner.py:840] Subprocess with PID 589461 (worker, 3) is now being started.
[worker-2]:    E0622 16:47:05.032748121  588933 server_chttp2.cc:40]        {""created"":""@1687452425.032699864"",""description"":""No address added out of total 1 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":395,""referenced_errors"":[{""created"":""@1687452425.032698162"",""description"":""Failed to add any wildcard listeners"",""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc"",""file_line"":342,""referenced_errors"":[{""created"":""@1687452425.032676237"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::]:45087""},{""created"":""@1687452425.032697070"",""description"":""Unable to configure socket"",""fd"":9,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":216,""referenced_errors"":[{""created"":""@1687452425.032694857"",""description"":""Address already in use"",""errno"":98,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":189,""os_error"":""Address already in use"",""syscall"":""bind""}]}]}]}
[worker-2]:    2023-06-22 16:47:05.032844: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:608] UNKNOWN: Could not start gRPC server
[worker-2]:    2023-06-22 16:47:05.033076: E tensorflow/core/common_runtime/eager/context_distributed_manager.cc:703] Could not start gRPC server
[worker-3]:    I0622 16:47:05.006778 140652189284160 multi_process_runner.py:842] TF_CONFIG: '{""cluster"": {""worker"": [""localhost:38723"", ""localhost:41053"", ""localhost:45087"", ""localhost:38125""]}, ""task"": {""type"": ""worker"", ""index"": 3}, ""rpc_layer"": ""grpc""}'
[worker-2]:    Process _Process-4:
[worker-2]:    Traceback (most recent call last):
[worker-2]:      File ""/usr/lib/python3.9/multiprocessing/process.py"", line 315, in _bootstrap
[worker-2]:        self.run()
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 755, in _run_with_setenv
[worker-2]:        return self._actual_run()
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 54, in _run_with_absl
[worker-2]:        app.run(lambda _: self._run_impl())
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/absl_py/absl/app.py"", line 312, in run
[worker-2]:        _run_main(main, args)
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/absl_py/absl/app.py"", line 258, in _run_main
[worker-2]:        sys.exit(main(argv))
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 54, in <lambda>
[worker-2]:        app.run(lambda _: self._run_impl())
[worker-2]:      File ""/usr/lib/python3.9/multiprocessing/process.py"", line 108, in run
[worker-2]:        self._target(*self._args, **self._kwargs)
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 866, in __call__
[worker-2]:        six.reraise(*info.exc_info)
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/six_archive/six.py"", line 719, in reraise
[worker-2]:        raise value
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 1060, in _run_contained
[worker-2]:        return_value = fn(*args, **kwargs)
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/failure_handling/failure_handler_test.py"", line 146, in worker_fn
[worker-2]:        strategy = collective_all_reduce_strategy.CollectiveAllReduceStrategy()
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 186, in __init__
[worker-2]:        CollectiveAllReduceExtended(
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 339, in __init__
[worker-2]:        self._initialize_strategy(self._cluster_resolver, devices=devices)
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 358, in _initialize_strategy
[worker-2]:        self._initialize_multi_worker(cluster_resolver)
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 530, in _initialize_multi_worker
[worker-2]:        context.context().ensure_initialized()
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 610, in ensure_initialized
[worker-2]:        pywrap_tfe.TFE_EnableCollectiveOps(context_handle, server_def_str)
[worker-2]:    tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server
```
</details>"
61101,Build aar with models using tfops,"I am trying to build a small aar for specific ML models where one of them is using tfops, I don't want to use the nighty version due to its huge size. 

I am following this [documentation](https://www.tensorflow.org/lite/android/lite_build#configure_workspace_and_bazelrc), 
I downloaded the provided Dockerfile and I updated the following fields

- commandlinetools to the latest version 9477386_latest
- ANDROID_API_LEVEL to 33
- ANDROID_BUILD_TOOLS_VERSION to 34.0.0. 

I started the docker container, configured the workspace, cloned tensorflow from this [repository](https://github.com/tensorflow/tensorflow.git) and checked out to branch r2.13, downloaded bazel 
```
 > bazel version
Bazelisk version: v1.17.0
Starting local Bazel server and connecting to it...
Build label: 5.3.0
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Aug 23 00:45:53 2022 (1661215553)
Build timestamp: 1661215553
Build timestamp as int: 1661215553
```

------------------------

### System information

-   **OS Platform and Distribution -> Linux**:
-   **TensorFlow downloaded from [source](https://github.com/tensorflow/tensorflow.git)**: r2.13
-   **Python version**: 3.11
-   **Bazel version**: 5.3.0

------------------------

When I run the bash command ` bash tensorflow/lite/tools/build_aar.sh   --input_models=model1.tflite,model2.tflite   --target_archs=x86,x86_64,arm64-v8a,armeabi-v7a` with my models it fails with the following error log

...
INFO: Analyzed target //tmp:tensorflow-lite-select-tf-ops (461 packages loaded, 50228 targets configured).
INFO: Found 1 target...
ERROR: /tensorflow_src/tensorflow/BUILD:1646:19: Action tensorflow/_api/v2/v2.py [for host] failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
2023-06-27 22:43:43.289759: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 856, in <module>
    main()
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 830, in main
    importlib.import_module(package)
  File ""/usr/lib/python3.11/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""<frozen importlib._bootstrap>"", line 1204, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1176, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 1147, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 690, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 940, in exec_module
  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/lite/python/lite.py"", line 34, in <module>
    from tensorflow.lite.python.convert import convert_graphdef as _convert_graphdef
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/lite/python/convert.py"", line 29, in <module>
    from tensorflow.lite.python import util
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/lite/python/util.py"", line 52, in <module>
    from jax import xla_computation as _xla_computation
  File ""/usr/local/lib/python3.11/dist-packages/jax/__init__.py"", line 35, in <module>
    from jax import config as _config_module
  File ""/usr/local/lib/python3.11/dist-packages/jax/config.py"", line 17, in <module>
    from jax._src.config import config  # noqa: F401
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/jax/_src/config.py"", line 27, in <module>
    from jax._src import lib
  File ""/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py"", line 85, in <module>
    cpu_feature_guard.check_cpu_features()
RuntimeError: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.
Target //tmp:tensorflow-lite-select-tf-ops failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /tensorflow_src/tensorflow/python/tools/BUILD:284:10 Middleman _middlemen/tensorflow_Spython_Stools_Sprint_Uselective_Uregistration_Uheader-runfiles failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
INFO: Elapsed time: 252.099s, Critical Path: 49.42s
INFO: 952 processes: 16 internal, 936 local.
FAILED: Build did NOT complete successfully"
61100,Model running on CPU when using NNAPI even with NnApiDelegate.Options().useNnapiCpu = false,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tensorflow-lite 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

Google Pixel 7

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am running the OpenAI whisper tiny model on Android using the TensorFlow Lite NNAPI delegate. It seems to be running on the CPU instead of the hardware accelerator on the Google Pixel 7 even though ""NnApiDelegate.Options().useNnapiCpu = false"" is set.

### Standalone code to reproduce the issue

```shell
NnApiDelegate.Options().useNnapiCpu = false
```


### Relevant log output

```shell
W  Access denied finding property ""ro.mediatek.platform""
W  Access denied finding property ""ro.chipname""
W  Access denied finding property ""ro.hardware.chipname""
I  Created TensorFlow Lite XNNPACK delegate for CPU.
```
</details>"
61096,Concurrent Read/Writes to tensorflow::*File across different processes,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Debian GNU/Linux 10

### Mobile device

_No response_

### Python version

3.8

### Bazel version

6.1.1

### GCC/Compiler version

9

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Are `tensorflow::WritableFile` and `tensorflow::RandomAccessFile` safe to read/write across multiple processes? If not, how can we ensure that they are?

If there are tensorflow file locking primitives, that would be ideal :)

If not, being able to grab the file descriptor from the TF file object should be sufficient--can we do this?

### Standalone code to reproduce the issue

```shell
# Process 1.

int main(int argc, char** argv) {
  std::unique_ptr<tensorflow::WritableFile> file;
  TF_CHECK_OK(tensorflow::Env::Default()->NewWritableFile(filename, &file));

  RecordWriter writer(file.get());
  writer.WriteRecord(""hahaha"");
  TF_CHECK_OK(writer.Close());
  TF_CHECK_OK(file->Close());

  return 0;
}

# Process 2.
int main(int argc, char** argv) {
  std::unique_ptr<tensorflow::RandomAccessFile> file;
  TF_CHECK_OK(tensorflow::Env::Default()->NewRandomAccessFile(*f, &file));

  SequentialRecordReader reader(file.get());
  tstring record;
  reader.ReadRecord(&record);

  return 0;
}
```


### Relevant log output

_No response_</details>"
61095,UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

0.0.0-nightly-SNAPSHOT

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

Android Samsung Galaxy J5

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Fatal Exception: java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():
  java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""__register_atfork"" referenced by ""libtensorflowlite_jni.so""
  
This is reproducible on lot of android devices running android version 5,6,7
This is happening in the nightly snapshots from probably last 2 weeks. Did not encounter this in previous nightly snapshots.


### Standalone code to reproduce the issue

```shell
val nnApiOption = NnApiDelegate.Options()
nnApiOption.setUseNnapiCpu(true)
val nnApiDelegate = NnApiDelegate(nnApiOption)
```


### Relevant log output

2023-06-28 10:52:35.629 29536-29620 InterpreterApi                      I  Didn't load native library: tensorflowlite_jni
2023-06-28 10:52:35.637 29536-29620 InterpreterApi                      I  Didn't load native library: tensorflowlite_jni_stable
2023-06-28 10:52:35.638 29536-29620 InterpreterApi                      I  Didn't load native library: tensorflowlite_jni_gms_client

</details>"
61092,"Lack of Documentation for GPU Use, Especially in Metal GPUs in MacBook M1, M1 Max and M2 Chips","Hi,

Whenever I tried to use a GPU on MPS with MacBook M1, I generally fail to use the GPU and whenever I tried to reach out to documentation for help, it doesn't provide much help. In addition to the documentation issue, there's a slowdown on M1, M1 Max and M2 chips when I use TensorFlow 2.11+

WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.
Epoch 1/5
2023-06-27 14:57:34.718668: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.
517/517 [==============================] - ETA: 0s - loss: 0.4559 - accuracy: 0.82332023-06-27 14:58:57.433849: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled."
61091,NoClassDefFoundError: GpuDelegateFactory$Options not found,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

False

### Source

source

### Tensorflow Version

TF2.12

### Custom Code

Yes

### OS Platform and Distribution

Android 12

### Mobile device

Android 12

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The GPU is not being used correctly.

### Standalone code to reproduce the issue

```shell
implementation 'org.tensorflow:tensorflow-lite:2.12.0'
 implementation 'org.tensorflow:tensorflow-lite-gpu:2.12.0'

Interpreter.Options options = new Interpreter.Options();
options.addDelegate(new GpuDelegate());

File file = new File(getFilesDir().getAbsolutePath()+""/movenet_thunder.tflite"");
Interpreter interpreter = new Interpreter(file, options);
```


### Relevant log output

```shell
E/AndroidRuntime: FATAL EXCEPTION: main
    Process: com.litesnap.open.flow.diffusion, PID: 13553
    java.lang.NoClassDefFoundError: Failed resolution of: Lorg/tensorflow/lite/gpu/GpuDelegateFactory$Options;
        at org.tensorflow.lite.gpu.GpuDelegate.<init>(GpuDelegate.java:53)
        at com.litesnap.open.flow.diffusion.MainActivity$1.onClick(MainActivity.java:27)
        at android.view.View.performClick(View.java:7792)
        at android.widget.TextView.performClick(TextView.java:16112)
        at com.google.android.material.button.MaterialButton.performClick(MaterialButton.java:1131)
        at android.view.View.performClickInternal(View.java:7769)
        at android.view.View.access$3800(View.java:910)
        at android.view.View$PerformClick.run(View.java:30218)
        at android.os.Handler.handleCallback(Handler.java:938)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loopOnce(Looper.java:226)
        at android.os.Looper.loop(Looper.java:313)
        at android.app.ActivityThread.main(ActivityThread.java:8663)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:567)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1135)
     Caused by: java.lang.ClassNotFoundException: Didn't find class ""org.tensorflow.lite.gpu.GpuDelegateFactory$Options"" on path: DexPathList[[zip file ""/data/app/~~oMimw9IoQp1ajJTsKEnrxQ==/com.litesnap.open.flow.diffusion-s0YwqVUhGU20kZbxCKdqXw==/base.apk""],nativeLibraryDirectories=[/data/app/~~oMimw9IoQp1ajJTsKEnrxQ==/com.litesnap.open.flow.diffusion-s0YwqVUhGU20kZbxCKdqXw==/lib/arm64, /data/app/~~oMimw9IoQp1ajJTsKEnrxQ==/com.litesnap.open.flow.diffusion-s0YwqVUhGU20kZbxCKdqXw==/base.apk!/lib/arm64-v8a, /system/lib64, /system/system_ext/lib64]]
        at dalvik.system.BaseDexClassLoader.findClass(BaseDexClassLoader.java:218)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:379)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:312)
        at org.tensorflow.lite.gpu.GpuDelegate.<init>(GpuDelegate.java:53) 
        at com.litesnap.open.flow.diffusion.MainActivity$1.onClick(MainActivity.java:27) 
        at android.view.View.performClick(View.java:7792) 
        at android.widget.TextView.performClick(TextView.java:16112) 
        at com.google.android.material.button.MaterialButton.performClick(MaterialButton.java:1131) 
        at android.view.View.performClickInternal(View.java:7769) 
        at android.view.View.access$3800(View.java:910) 
        at android.view.View$PerformClick.run(View.java:30218) 
        at android.os.Handler.handleCallback(Handler.java:938) 
        at android.os.Handler.dispatchMessage(Handler.java:99) 
        at android.os.Looper.loopOnce(Looper.java:226) 
        at android.os.Looper.loop(Looper.java:313) 
        at android.app.ActivityThread.main(ActivityThread.java:8663) 
        at java.lang.reflect.Method.invoke(Native Method) 
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:567) 
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1135) 
```
</details>"
61090,The relationship between the parameters of Conv2D is unclear,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf2.12.0

### Custom Code

Yes

### OS Platform and Distribution

MacOs

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


```
ValueError: `strides > 1` not supported in conjunction with `dilation_rate > 1`. Received: strides=[2, 2] and dilation_rate=[4, 5]
```

The relationship between these two parameters is not clearly defined in the documentation, and it is not certain. Be unaware of that  `strides > 1` not supported in conjunction with `dilation_rate > 1`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.keras.layers import Conv2D

input_tensor = tf.random.normal(shape=(1, 32, 32, 3))

x = Conv2D(filters=2, kernel_size=(1,1), strides=(2,2), padding=""same"", use_bias=False, dilation_rate=(4, 5))(input_tensor)

print(x.shape)
```


### Relevant log output

_No response_</details>"
61089,Error about using TensorFlow1.14,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 1.14.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu18.04

### Mobile device

Ubuntu18.04

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuda10.0 cudnn7.6.5

### GPU model and memory

24G

### Current Behaviour?

I use Ubuntu18.04 system, cuda10.0, cudnn7.6.4, graphics card is a30,24G; TensorFlow1.14.0 was used to make ner model, but an error occurred when running, and there was no error when using it before. It can be run on another device with no change in the code. I hope to get your reply and help.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.contrib.crf import crf_log_likelihood
from tensorflow.contrib.layers.python.layers import initializers

from albert import modeling
import models.rnncell as rnn
from models.file_path import BaseConfig


class Config(BaseConfig):
    batch_size = 128
    epoch = 100
    print_per_batch = 100
    clip = 5
    dropout_keep_prob = 0.5
    lr = 0.0001
    optimizer = 'adam'
    zeros = False
    lower = True

    num_tags = None
    lstm_dim = 200
    max_seq_len = 256
    max_epoch = 100
    steps_check = 100


class AlbertBiLstmCrf(object):
    def __init__(self, config):
        self.config = config
        # 为模型添加占位符
        self.input_ids = tf.placeholder(dtype=tf.int32, shape=[None, None], name=""input_ids"")
        self.input_mask = tf.placeholder(dtype=tf.int32, shape=[None, None], name=""input_mask"")
        self.segment_ids = tf.placeholder(dtype=tf.int32, shape=[None, None], name=""segment_ids"")
        self.targets = tf.placeholder(dtype=tf.int32, shape=[None, None], name=""Targets"")

        self.dropout = tf.placeholder(dtype=tf.float32, name=""Dropout"")
        self.global_step = tf.Variable(0, trainable=False)
        self.best_dev_f1 = tf.Variable(0.0, trainable=False)

        self.initializer = initializers.xavier_initializer()
        self.albert_bilstm_crf()

    def albert_bilstm_crf(self):
        # parameter
        used = tf.sign(tf.abs(self.input_ids))
        length = tf.reduce_sum(used, reduction_indices=1)
        self.lengths = tf.cast(length, tf.int32)
        self.batch_size = tf.shape(self.input_ids)[0]
        self.num_steps = tf.shape(self.input_ids)[-1]

        # albert embedding
        embedding = self.bert_embedding()
        # dropout
        lstm_inputs = tf.nn.dropout(embedding, self.dropout)
        # bi-lstm layer
        lstm_outputs = self.biLSTM_layer(lstm_inputs, self.config.lstm_dim, self.lengths)
        # logits for tags
        self.logits = self.project_layer(lstm_outputs)
        # loss of the model
        self.loss = self.loss_layer(self.logits, self.lengths)

        # bert模型参数初始化的地方
        init_checkpoint = self.config.init_checkpoint
        # 获取模型中所有的训练参数。
        tvars = tf.trainable_variables()
        # 加载BERT模型
        (assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)
        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)
        # 打印加载模型的参数
        train_vars = []
        for var in tvars:
            init_string = """"
            if var.name in initialized_variable_names:
                init_string = "", *INIT_FROM_CKPT*""
            else:
                train_vars.append(var)
            # print(""  name = %s, shape = %s%s"", var.name, var.shape,init_string)

        optimizer = self.config.optimizer
        if optimizer == ""adam"":
            self.opt = tf.train.AdamOptimizer(self.config.lr)
        else:
            raise KeyError
        grads = tf.gradients(self.loss, train_vars)
        (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)

        self.train_op = self.opt.apply_gradients(zip(grads, train_vars), global_step=self.global_step)

    def bert_embedding(self):
        # load bert embedding
        albert_config = modeling.AlbertConfig.from_json_file(self.config.bert_config_path)  # 配置文件地址。
        model = modeling.AlbertModel(
            config=albert_config,
            is_training=True,
            input_ids=self.input_ids,
            input_mask=self.input_mask,
            token_type_ids=self.segment_ids,
            use_one_hot_embeddings=False)
        embedding = model.get_sequence_output()
        return embedding

    def biLSTM_layer(self, lstm_inputs, lstm_dim, lengths, name=None):

        #lstm_inputs: [batch_size, num_steps, emb_size] return：[batch_size, num_steps, 2*lstm_dim]

        with tf.variable_scope(""char_BiLSTM"" if not name else name):
            lstm_cell = {}
            for direction in [""forward"", ""backward""]:
                with tf.variable_scope(direction):
                    lstm_cell[direction] = rnn.CoupledInputForgetGateLSTMCell(
                        lstm_dim,
                        use_peepholes=True,
                        initializer=self.initializer,
                        state_is_tuple=True)
            outputs, final_states = tf.nn.bidirectional_dynamic_rnn(
                lstm_cell[""forward""],
                lstm_cell[""backward""],
                lstm_inputs,
                dtype=tf.float32,
                sequence_length=lengths)
        return tf.concat(outputs, axis=2)

    def project_layer(self, lstm_outputs, name=None):
        #lstm和logits之间的隐藏层
        #lstm_outputs: [batch_size, num_steps, emb_size] return: [batch_size, num_steps, num_tags]
        with tf.variable_scope(""project"" if not name else name):
            with tf.variable_scope(""hidden""):
                W = tf.get_variable(""W"", shape=[self.config.lstm_dim * 2, self.config.lstm_dim],dtype=tf.float32, initializer=self.initializer)

                b = tf.get_variable(""b"", shape=[self.config.lstm_dim], dtype=tf.float32,initializer=tf.zeros_initializer())
                output = tf.reshape(lstm_outputs, shape=[-1, self.config.lstm_dim * 2])
                hidden = tf.tanh(tf.nn.xw_plus_b(output, W, b))

            # project to score of tags
            with tf.variable_scope(""logits""):
                W = tf.get_variable(""W"", shape=[self.config.lstm_dim, self.config.num_tags],dtype=tf.float32, initializer=self.initializer)

                b = tf.get_variable(""b"", shape=[self.config.num_tags], dtype=tf.float32,initializer=tf.zeros_initializer())

                pred = tf.nn.xw_plus_b(hidden, W, b)

            return tf.reshape(pred, [-1, self.num_steps, self.config.num_tags])

    def loss_layer(self, project_logits, lengths, name=None):
        #计算crf损失
        with tf.variable_scope(""crf_loss"" if not name else name):
            small = -1000.0
            # pad logits for crf loss
            start_logits = tf.concat([small * tf.ones(shape=[self.batch_size, 1, self.config.num_tags]),tf.zeros(shape=[self.batch_size, 1, 1])],axis=-1)
            pad_logits = tf.cast(small * tf.ones([self.batch_size, self.num_steps, 1]), tf.float32)
            logits = tf.concat([project_logits, pad_logits], axis=-1)
            logits = tf.concat([start_logits, logits], axis=1)
            targets = tf.concat([tf.cast(self.config.num_tags * tf.ones([self.batch_size, 1]), tf.int32), self.targets], axis=-1)

            self.trans = tf.get_variable(""transitions"",shape=[self.config.num_tags + 1, self.config.num_tags + 1],initializer=self.initializer)
            #对数似然
            log_likelihood, self.trans = crf_log_likelihood(
                inputs=logits,
                tag_indices=targets,
                transition_params=self.trans,
                sequence_lengths=lengths + 1)
            return tf.reduce_mean(-log_likelihood)
```


### Relevant log output

```shell
2023-06-27 14:12:06.722268: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2023-06-27 14:12:06.728420: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2023-06-27 14:12:06.834462: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5647fd4400f0 executing computations on platform CUDA. Devices:
2023-06-27 14:12:06.834508: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): NVIDIA A30, Compute Capability 8.0
2023-06-27 14:12:06.838777: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2023-06-27 14:12:06.839586: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5647fb24a440 executing computations on platform Host. Devices:
2023-06-27 14:12:06.839615: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2023-06-27 14:12:06.840801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: NVIDIA A30 major: 8 minor: 0 memoryClockRate(GHz): 1.44
pciBusID: 0000:65:00.0
2023-06-27 14:12:06.841076: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2023-06-27 14:12:06.842475: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2023-06-27 14:12:06.843695: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2023-06-27 14:12:06.844007: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2023-06-27 14:12:06.845668: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2023-06-27 14:12:06.846956: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2023-06-27 14:12:06.850503: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2023-06-27 14:12:06.852002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2023-06-27 14:12:06.852036: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2023-06-27 14:12:06.853078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-06-27 14:12:06.853091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2023-06-27 14:12:06.853099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2023-06-27 14:12:06.854597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12129 MB memory) -> physical GPU (device: 0, name: NVIDIA A30, pci bus id: 0000:65:00.0, compute capability: 8.0)
2023-06-27 14:12:06,855 - /root/Tai/Project/ANER-规范性文件清单/log/train.log - INFO - Created model with fresh parameters.
2023-06-27 14:12:07.187507: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-06-27 14:14:18,782 - /root/Tai/Project/ANER-规范性文件清单/log/train.log - INFO - start training
2023-06-27 14:14:20.096642: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2023-06-27 14:14:22.597785: E tensorflow/stream_executor/cuda/cuda_blas.cc:428] failed to run cuBLAS routine: CUBLAS_STATUS_EXECUTION_FAILED
Traceback (most recent call last):
  File ""/root/anaconda3/envs/ner/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/root/anaconda3/envs/ner/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/root/anaconda3/envs/ner/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal: Blas GEMM launch failed : a.shape=(16384, 2), b.shape=(2, 128), m=16384, n=128, k=2
	 [[{{node bert/embeddings/MatMul}}]]
	 [[Adam/update/_278]]
  (1) Internal: Blas GEMM launch failed : a.shape=(16384, 2), b.shape=(2, 128), m=16384, n=128, k=2
	 [[{{node bert/embeddings/MatMul}}]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 69, in <module>
    train(model, config, train_manager, dev_manager, id_to_tag)
  File ""train.py"", line 38, in train
    global_step, batch_loss, _ = sess.run([model.global_step, model.loss, model.train_op], feed_dict)
  File ""/root/anaconda3/envs/ner/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/root/anaconda3/envs/ner/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/root/anaconda3/envs/ner/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/root/anaconda3/envs/ner/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal: Blas GEMM launch failed : a.shape=(16384, 2), b.shape=(2, 128), m=16384, n=128, k=2
	 [[node bert/embeddings/MatMul (defined at /root/Tai/Project/ANER-规范性文件清单/albert/modeling.py:552) ]]
	 [[Adam/update/_278]]
  (1) Internal: Blas GEMM launch failed : a.shape=(16384, 2), b.shape=(2, 128), m=16384, n=128, k=2
	 [[node bert/embeddings/MatMul (defined at /root/Tai/Project/ANER-规范性文件清单/albert/modeling.py:552) ]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node bert/embeddings/MatMul:
 bert/embeddings/token_type_embeddings/read (defined at /root/Tai/Project/ANER-规范性文件清单/albert/modeling.py:547)	
 bert/embeddings/one_hot (defined at /root/Tai/Project/ANER-规范性文件清单/albert/modeling.py:551)

Input Source operations connected to node bert/embeddings/MatMul:
 bert/embeddings/token_type_embeddings/read (defined at /root/Tai/Project/ANER-规范性文件清单/albert/modeling.py:547)	
 bert/embeddings/one_hot (defined at /root/Tai/Project/ANER-规范性文件清单/albert/modeling.py:551)

Original stack trace for 'bert/embeddings/MatMul':
  File ""train.py"", line 66, in <module>
    model = AlbertBiLstmCrf(config)
  File ""/root/Tai/Project/ANER-规范性文件清单/models/modeling_BBC.py"", line 42, in __init__
    self.albert_bilstm_crf()
  File ""/root/Tai/Project/ANER-规范性文件清单/models/modeling_BBC.py"", line 53, in albert_bilstm_crf
    embedding = self.bert_embedding()
  File ""/root/Tai/Project/ANER-规范性文件清单/models/modeling_BBC.py"", line 99, in bert_embedding
    use_one_hot_embeddings=False)
  File ""/root/Tai/Project/ANER-规范性文件清单/albert/modeling.py"", line 207, in __init__
    dropout_prob=config.hidden_dropout_prob)
  File ""/root/Tai/Project/ANER-规范性文件清单/albert/modeling.py"", line 552, in embedding_postprocessor
    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)
  File ""/root/anaconda3/envs/ner/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py"", line 180, in wrapper
    return target(*args, **kwargs)
  File ""/root/anaconda3/envs/ner/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 2647, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/root/anaconda3/envs/ner/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 5925, in mat_mul
    name=name)
  File ""/root/anaconda3/envs/ner/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/root/anaconda3/envs/ner/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/root/anaconda3/envs/ner/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""/root/anaconda3/envs/ner/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
```
</details>"
61085,TF2.13 Breaks register_keras_serializable,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

pypi

### Tensorflow Version

TF2.13.0rc2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When building TF Addons for TF2.13 we're noticing that our ability to register custom Keras objects as serializable has been broken:

```
@tf.keras.saving.register_keras_serializable('my_package')
class MyDense(tf.keras.layers.Dense):
    def __init__(self, units, **kwargs):
        super().__init__(units, **kwargs)
```

### Standalone code to reproduce the issue

```shell
Here it is shown as working in TF2.12:
https://colab.research.google.com/drive/172A4_GAiSFzWJr6iVAaFYMujVCwbTk3W?usp=sharing

Here it breaks in TF2.13:
https://colab.research.google.com/drive/16BH2aNXXw3zMVevx7IYMx5cH6fZsREO_?usp=sharing
```


### Relevant log output

```shell
ValueError: Unknown layer: 'MyDense'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.
```
```
</details>"
61084,tf.data.Dataset prefetch not fetching data asynchronously,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Debian/Linux 11

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

After implementing a data pipeline using tf.data.Dataset to pull image data from Google Cloud Storage, TensorBoard profiler shows that the GPU compute and CPU prefetch are running synchronously. I used data.Dataset.AUTOTUNE to determine the appropriate prefetch batch size. Monitoring GPU usage while the model is running confirms this with the GPU at 0% utilization to actually computing something for about a 2:1 ratio, which is reflected in the profiler. CPU usage when monitored does not appear to max out.

I expected the prefetch to occur concurrently with GPU processing as described in the data.Dataset documentation and tutorials.

![ch](https://github.com/tensorflow/tensorflow/assets/79778984/e96ad312-12b0-4bbb-b06e-f4e4976714b3)

![cp](https://github.com/tensorflow/tensorflow/assets/79778984/f52959d1-23fd-45ed-ba57-5a532afd0972)

![gp](https://github.com/tensorflow/tensorflow/assets/79778984/2b2e15b6-2cf7-40d8-89b1-98889f151863)


### Standalone code to reproduce the issue

```shell
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""
os.environ['TF_GPU_ALLOCATOR'] = ""cuda_malloc_async""
config = ConfigProto()
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

def get_label(file_path):
    parts = tf.strings.split(file_path, os.path.sep)
    one_hot = parts[-2] == class_names
    return tf.argmax(one_hot)

def decode_img(img):
    img = tf.io.decode_image(img, channels=3, expand_animations = False)
    img = tf.image.resize(img, [244, 244])
    img = tf.cast(img, tf.float32)
    return img

def process_path(file_path):
    label = get_label(file_path)
    img = tf.io.read_file(file_path)
    img = decode_img(img)
    return img, label

def configure_for_performance(ds):
    ds = ds.batch(128)
    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)
    return ds

files = tf.data.Dataset.list_files((data_dir + '/*/*.png'), shuffle=False)
files = files.shuffle(image_count, reshuffle_each_iteration=False)

val_size = int(image_count * 0.2)

train_files = files.skip(val_size)
val_files = files.take(val_size)

train_ds = train_files.interleave(lambda x: tf.data.Dataset.from_tensor_slices([x]), cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE)
train_ds = train_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)

val_ds = val_files.interleave(lambda x: tf.data.Dataset.from_tensor_slices([x]), cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE)
val_ds = val_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)

train_ds = configure_for_performance(train_ds)
val_ds = configure_for_performance(val_ds)
```


### Relevant log output

_No response_</details>"
61083,Not initialized delegate kernel after tflite conversion,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 11
- TensorFlow installation (pip package or built from source):
pip package
- TensorFlow library (version, if pip package or github SHA, if built from source):
TensorFlow 2.10.0

### 2. Code

Code to reproduce my issue is attached to this issue.
[tf_issue.zip](https://github.com/tensorflow/tensorflow/files/11871912/tf_issue.zip)

### 3. Failure after conversion

```
File ""tf_issue\test_ocr.py"", line 62, in __call__
    self._interpreter.invoke()
File ""...\venv\lib\site-packages\tensorflow\lite\python\interpreter.py"", line 917, in invoke
    self._interpreter.Invoke()
RuntimeError: Current implementation only supports equal length strides in the row and column dimensions.Delegate kernel was not initializedNode number 510 (TfLiteFlexDelegate) failed to prepare.
```


### 5. (optional) Any other info / logs
Hello, 
I downloaded the OCR model called **en_PP-OCRv3_rec_infer** from the [Paddle repository](https://github.com/PaddlePaddle/PaddleOCR). To prepare it for my purposes, I converted it into the ONNX format and optimized it, following the guidelines provided [here](https://github.com/PaddlePaddle/Paddle2ONNX/blob/develop/README_en.md#command-line-conversion). To ensure compatibility, I defined a static input/output size for the model.

Subsequently, I proceeded to convert the ONNX format to TFLite using this [repository](https://github.com/sithu31296/PyTorch-ONNX-TFLite/tree/master#onnx-to-tf). Once the conversion was complete, I loaded the resulting .tflite model into [Neutron](https://netron.app/) without any issues, as it successfully read and visualized the model.

However, the problem arises when I attempt to test this model using Python (3.10). The attached zip file contains the code, the model itself, and a sample testing image (It also contain a requirements file with all the packages of my environment).

In my case, I utilized an input_shape_dict of ""{'x': [1, 3, 48, 320]}"" and exported the model in both fp16 and fp32 formats. I also experimented with opset_versions 10 and 16. However, despite these attempts, I encountered the reported failure repeatedly.
"
61080,"MHLO -> HLO does not respect sharding, inserting a tuple without sharding","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2e896fbe1e0ea4df33fbcfe780a1036f431b4e89

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubunto 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

10.3

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Converting an MHLO program to HLO that is fully annotated with shardings, results in HLO that has a tuple instruction that is without sharding.

Input MLIR `sharding-not-respected-mhlo-to-hlo.mlir`:
```mlir
func.func @main(%arg0: tensor<2x2xi32> {mhlo.sharding = ""{devices=[2,1]0,1}""}) -> (tensor<2x2xi32> {mhlo.sharding = ""{devices=[2,1]0,1}""}) {
  %0 = mhlo.add %arg0, %arg0 {mhlo.sharding = ""{devices=[2,1]0,1}""} : tensor<2x2xi32>
  return %0 : tensor<2x2xi32>
}
```

Command:
```
xla-translate -mlir-hlo-to-hlo-text sharding-not-respected-mhlo-to-hlo.mlir
```

Result:
```hlo
HloModule main, entry_computation_layout={(s32[2,2]{1,0})->s32[2,2]{1,0}}

ENTRY %main.5 (Arg_0.1: s32[2,2]) -> s32[2,2] {
  %Arg_0.1 = s32[2,2] parameter(0), sharding={devices=[2,1]0,1}
  %add.2 = s32[2,2] add(s32[2,2] %Arg_0.1, s32[2,2] %Arg_0.1), sharding={devices=[2,1]0,1}, metadata={source_file=""sharding-not-respected-mhlo-to-hlo.mlir"" source_line=2}
  %tuple.3 = (s32[2,2]) tuple(s32[2,2] %add.2)
  ROOT %get-tuple-element.4 = s32[2,2] get-tuple-element((s32[2,2]) %tuple.3), index=0, sharding={devices=[2,1]0,1}
}
```

You can see that a tuple instruction has been inserted that has no sharding annotation.
```hlo
  %tuple.3 = (s32[2,2]) tuple(s32[2,2] %add.2)
```

This [test](https://github.com/tensorflow/tensorflow/blob/2e896fbe1e0ea4df33fbcfe780a1036f431b4e89/tensorflow/compiler/xla/translate/mhlo_to_hlo/tests/sharding.mlir#L21) expects a tuple without sharding. Is this really the expected behavior?
For example this tuple instruction would cause a problem if you want to do SPMD partitioning. Then the partitioner would insert an unwanted all-gather instruction.

If you do a conversion without sharding of the same MLIR
```mlir
func.func @main(%arg0: tensor<2x2xi32>) -> tensor<2x2xi32> {
  %0 = mhlo.add %arg0, %arg0 : tensor<2x2xi32>
  return %0 : tensor<2x2xi32>
}
```
You get a nicer result without the redundant `tuple` and `get-tuple-element` instructions
```hlo
HloModule main, entry_computation_layout={(s32[2,2]{1,0})->s32[2,2]{1,0}}

ENTRY %main.3 (Arg_0.1: s32[2,2]) -> s32[2,2] {
  %Arg_0.1 = s32[2,2] parameter(0)
  ROOT %add.2 = s32[2,2] add(s32[2,2] %Arg_0.1, s32[2,2] %Arg_0.1), metadata={source_file=""sharding-not-respected-mhlo-to-hlo.mlir"" source_line=2}
}
```

I can see two solutions here. 
1. Make the tuple instruction inherit the correct sharding.
2. Return directly the result of the `add` operation.

### Standalone code to reproduce the issue

```shell
See the description.
```


### Relevant log output

_No response_</details>"
61077,Error to run tf,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10

### Custom Code

Yes

### OS Platform and Distribution

Windowsx 11

### Mobile device

_No response_

### Python version

3.10.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

*When i try to run TensorFlow (import him), i get this error:*
![image](https://github.com/tensorflow/tensorflow/assets/95155123/3b4c7eda-fb5c-4f03-910f-74ba03ebc7ad)


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

_No response_</details>"
61076,CUDA_ERROR_NOT_PERMITTED: operation not permitted when fitting model on GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.10.0-rc3-6-g359c3cdfc5f 2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuDNN version 8.1 CUDA version 11.2

### GPU model and memory

name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5, memory: 4 GB

### Current Behaviour?

A bug happened!
I have a problem with fitting my model.
Additional informations:
This model is about to recognize images. There are 10 classes to predict, co I'm using loss=categorical_crossentropy and last activation Layer is 'softmax'
My input data shapes are:
print(X_train.shape) -> (3495, 128, 128, 3)
print(Y_train.shape) -> (3495, 10)
print(X_val.shape) -> (1498, 128, 128, 3)
print(Y_val.shape) -> (1498, 10)
So it is quite small for test.

My GPU is visible - I tested it with:
model.add(Activation('sigmoid')) and loss='binary_crossentropy' first and it is running.
I have set memory allocation and growth as in code. Everything seemed fine.

Now when I changed it to:
model.add(Activation('softmax')) and loss='categorical_crossentropy ' - also used 'to_categorical()' for classes array - I'm getting such error:
Failed setting context: CUDA_ERROR_NOT_PERMITTED: operation not permitted.

Looking at my GPU usage, it doesn't look like it is a memory problem - in peak it is not using even 30% of it.
Did anyone ever get such error? I run out of ideas.



### Standalone code to reproduce the issue

```shell
gpus = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(gpus[0], True)
tf.config.set_visible_devices(gpus[:1], device_type='GPU')
log_dev_conf = tf.config.LogicalDeviceConfiguration(
    memory_limit=3*512
)
tf.config.set_logical_device_configuration(
    gpus[0],
    [log_dev_conf]) 

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical

from tensorflow.keras.mixed_precision import set_global_policy
set_global_policy('mixed_float16')

tf.keras.backend.clear_session()

Y_train = to_categorical(Y_train, num_classes=10, dtype=int)
Y_val = to_categorical(Y_val, num_classes=10, dtype=int)

batch_size = 2
data_generator = ImageDataGenerator(rescale=1.0/255.0)
data_generator = data_generator.flow(X_train, Y_train, batch_size=batch_size)

data_generator_val = ImageDataGenerator(rescale=1.0/255.0)
data_generator_val = data_generator_val.flow(X_val, Y_val, batch_size=batch_size)


model = Sequential()

model.add(Conv2D(128, (3, 3), input_shape=X_train.shape[1:]))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())

model.add(Dense(64))
model.add(Activation('relu'))

model.add(Dense(10, dtype='float32'))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer = 'Adam',
              metrics=['accuracy'])


# Train the model using the fit method
trained_model = model.fit(
    data_generator,
    validation_data=data_generator_val,
    steps_per_epoch=batch_size,
    epochs=5
)
```


### Relevant log output

```shell
Epoch 1/5

2023-06-25 19:13:24.452341: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100
2023-06-25 19:13:28.511859: F tensorflow/stream_executor/cuda/cuda_driver.cc:147] Failed setting context: CUDA_ERROR_NOT_PERMITTED: operation not permitted


Fatal Python error: Aborted


Main thread:
Thread 0x00000eb8 (most recent call first):
  File ""C:\Users\pablitoremiszewski\anaconda3\envs\pawel_engineer\lib\site-packages\tensorflow\python\eager\execute.py"", line 54 in quick_execute
  File ""C:\Users\pablitoremiszewski\anaconda3\envs\pawel_engineer\lib\site-packages\tensorflow\python\eager\function.py"", line 499 in call
  File ""C:\Users\pablitoremiszewski\anaconda3\envs\pawel_engineer\lib\site-packages\tensorflow\python\eager\function.py"", line 1862 in _call_flat
  File ""C:\Users\pablitoremiszewski\anaconda3\envs\pawel_engineer\lib\site-packages\tensorflow\python\eager\function.py"", line 2496 in __call__
  File ""C:\Users\pablitoremiszewski\anaconda3\envs\pawel_engineer\lib\site-packages\tensorflow\python\eager\def_function.py"", line 980 in _call
  File ""C:\Users\pablitoremiszewski\anaconda3\envs\pawel_engineer\lib\site-packages\tensorflow\python\eager\def_function.py"", line 915 in __call__
  File ""C:\Users\pablitoremiszewski\anaconda3\envs\pawel_engineer\lib\site-packages\tensorflow\python\util\traceback_utils.py"", line 150 in error_handler
  File ""C:\Users\pablitoremiszewski\anaconda3\envs\pawel_engineer\lib\site-packages\keras\engine\training.py"", line 1564 in fit
  File ""C:\Users\pablitoremiszewski\anaconda3\envs\pawel_engineer\lib\site-packages\keras\utils\traceback_utils.py"", line 65 in error_handler
  File ""C:\Users\pablitoremiszewski\AppData\Local\Temp\ipykernel_6540\2599062935.py"", line 3 in <module>


Restarting kernel...
```
</details>"
61074,Requested feature_data_ size 536907080 doesn't match 1960; Feature generation failed;,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

V2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hello Together,

I'm having a Problem with the micro_speech example for arduino from this repo: https://github.com/tensorflow/tflite-micro-arduino-examples/tree/main/examples/micro_speech

When trying to use this example with a new trained model from this jupyter noteboobk: https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/micro_speech/train

I always get the same error message:
Requested feature_data_ size 536907080 doesn't match 1960
Feature generation failed

The only thing i changed in the notebook was the tensorflow version. This is because this notebook was using 1.x Version which is no longer supported by colab and i changed it to work with the latest 2.x version

Can anyone help here?

Greetings,
Patrick

### Standalone code to reproduce the issue

```shell
https://github.com/tensorflow/tflite-micro-arduino-examples/tree/main/examples/micro_speech

 https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/micro_speech/train
```


### Relevant log output

_No response_</details>"
61069,Call TensorFlowLite model with `CVPixelBuffer` from Camera,"Hey! Sorry I know this is off-topic and a question, but I couldn't find any examples or documentation on optimizing input-tensors for realtime usage.

I'm the author of a very popular Camera library for mobile apps (React Native / [VisionCamera](https://github.com/mrousavy/react-native-vision-camera)) and I'm trying to add a TensorFlow Lite integration to the Camera.

This should be as generic as possible and will allow the user to drop in any `.tflite` model which just gets called with the Camera frame (`CMSampleBuffer` on iOS, `android.media.Image` on Android) and returns _any_ data (output tensors).

I started with implementing the iOS part in Objective-C and set up my TensorFlow code like this:

```objc
NSString* modelPath = [[NSBundle mainBundle] pathForResource:@""model""
                                                      ofType:@""tflite""];
NSError* error;
TFLInterpreter* interpreter = [[TFLInterpreter alloc] initWithModelPath:modelPath
                                                                  error:&error];
if (error != nil) { /** ... */ }

[interpreter allocateTensorsWithError:&error];
if (error != nil) { /** ... */ }
```

And then I have my Camera Frame Callback which gets called for every Frame the Camera ""sees"" (60 times a second at 60 FPS):

```objc
auto imageBuffer = CMSampleBufferGetImageBuffer(frame.buffer);
auto bytesPerRow = CVPixelBufferGetBytesPerRow(imageBuffer);
auto height = CVPixelBufferGetHeight(imageBuffer);
auto sourceBuffer = CVPixelBufferGetBaseAddress(imageBuffer);
auto inputData = [NSData dataWithBytesNoCopy:sourceBuffer length:bytesPerRow * height];

NSError* error;

TFLTensor *inputTensor = [interpreter inputTensorAtIndex:0 error:&error];
if (error != nil) { /** ... */ }

[inputTensor copyData:inputData error:&error];
if (error != nil) { /** ... */ }

[interpreter invokeWithError:&error];
if (error != nil) { /** ... */ }

TFLTensor *outputTensor = [interpreter outputTensorAtIndex:0 error:&error];
if (error != nil) { /** ... */ }

NSData *outputData = [outputTensor dataWithError:&error];
if (error != nil) { /** ... */ }

// TODO: return output data to user
```

Now I have two problems:

1. The Frame is any arbitrary size, but the models are trained to specific sizes. So I'm obviously getting the following error:

    ```
    Input tensor at index (9142529056) expects data size (110592), but got (8355840).
    ```

    I want to avoid Frame resizing here and ideally have the `TFLInterpreter` accept the Frame (`CMSampleBuffer`) _as is_ and do a stride/offset/jumps internally - is that even possible? If not, how can I figure out what Frame size I need to downscale to?
2. The `TFLInterpreter` can only be invoked with `NSData`, and my Frame is a `CMSampleBuffer` allocated on the GPU. Is there any way to avoid this GPU -> CPU copy and stay on the GPU buffer the whole time? I have safe read access to that buffer in this callback.

I've seen some MLKit samples (e.g. [MLKit Object Detection iOS](https://developers.google.com/ml-kit/vision/object-detection/ios)) and they allow you to just pass the `CMSampleBuffer` to the model - I'm wondering how this will be handled internally as it seems like the library is quite performant.

Also I found [this code on GitHub](https://github.com/kunass2/naildetector/blob/7ebfb2a3b7eaf6142be3d0a85e870bad4ce29d05/DeeplabModel.mm#L121-L205) which seemed quite interesting, but I couldn't figure out where `ProcessInputWithFloatModel(..)` is from...

As you can probably tell, I'm not an expert in this field but I'd appreciate any help here. Thanks!"
61068,XLA unit tests fail to build on gcc,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.8.13

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

3 unit tests fail to build and one fails with error.

### Standalone code to reproduce the issue

```shell
'bazel test --config==mkl_aarch64_threadpool'
```


### Relevant log output

```shell
ERROR: /tf/tensorflow/tensorflow/compiler/xla/service/BUILD:5779:12: Compiling tensorflow/compiler/xla/service/hlo_parser_test.cc failed: (Exit 1): gcc failed: error executing command (from target //tensorflow/compiler/xla/service:hlo_parser_test)
      (cd /home/buildslave/.cache/bazel/_bazel_buildslave/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow && \
      exec env - \
        CACHEBUSTER=20220325 \
        PATH=/home/buildslave/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-arm64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
        PWD=/proc/self/cwd \
        PYTHON_BIN_PATH=/usr/local/bin/python3 \
        TF2_BEHAVIOR=1 \
      /dt10/usr/bin/gcc -MD -MF bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/service/_objs/hlo_parser_test/hlo_parser_test.d '-frandom-seed=bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/service/_objs/hlo_parser_test/hlo_parser_test.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DBENCHMARK_STATIC_DEFINE '-DBAZEL_CURRENT_REPOSITORY=""""' -iquote . -iquote bazel-out/aarch64-opt/bin -iquote external/eigen_archive -iquote bazel-out/aarch64-opt/bin/external/eigen_archive -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/aarch64-opt/bin/external/nsync -iquote external/double_conversion -iquote bazel-out/aarch64-opt/bin/external/double_conversion -iquote external/com_google_protobuf -iquote bazel-out/aarch64-opt/bin/external/com_google_protobuf -iquote external/snappy -iquote bazel-out/aarch64-opt/bin/external/snappy -iquote external/com_googlesource_code_re2 -iquote bazel-out/aarch64-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/aarch64-opt/bin/external/farmhash_archive -iquote external/com_google_googletest -iquote bazel-out/aarch64-opt/bin/external/com_google_googletest -iquote external/com_google_benchmark -iquote bazel-out/aarch64-opt/bin/external/com_google_benchmark -iquote external/zlib -iquote bazel-out/aarch64-opt/bin/external/zlib -iquote external/bazel_tools -iquote bazel-out/aarch64-opt/bin/external/bazel_tools -Ibazel-out/aarch64-opt/bin/external/com_google_benchmark/_virtual_includes/benchmark -isystem external/eigen_archive -isystem bazel-out/aarch64-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/aarch64-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/aarch64-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/aarch64-opt/bin/external/farmhash_archive/src -isystem external/com_google_googletest/googlemock -isystem bazel-out/aarch64-opt/bin/external/com_google_googletest/googlemock -isystem external/com_google_googletest/googlemock/include -isystem bazel-out/aarch64-opt/bin/external/com_google_googletest/googlemock/include -isystem external/com_google_googletest/googletest -isystem bazel-out/aarch64-opt/bin/external/com_google_googletest/googletest -isystem external/com_google_googletest/googletest/include -isystem bazel-out/aarch64-opt/bin/external/com_google_googletest/googletest/include -isystem external/zlib -isystem bazel-out/aarch64-opt/bin/external/zlib -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS '-mtune=generic' '-march=armv8-a' -O3 '-std=c++17' '--sysroot=/dt10' -c tensorflow/compiler/xla/service/hlo_parser_test.cc -o bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/service/_objs/hlo_parser_test/hlo_parser_test.o)
    # Configuration: de691fac16e6bac2c61ad8c09da26892c92f72e8ffaa16698c2dd3959f4ebc3a
    # Execution platform: @local_execution_config_platform//:platform
    tensorflow/compiler/xla/service/hlo_parser_test.cc: In member function 'virtual void xla::{anonymous}::HloParserTest_ParseTrivialIotaShardingPartialReplication_Test::TestBody()':
    tensorflow/compiler/xla/service/hlo_parser_test.cc:3488:51: error: call of overloaded 'TileAssignment(<brace-enclosed initializer list>)' is ambiguous
     3488 |   TileAssignment tiling_last_dim_replicated({2, 2});
          |                                                   ^
    In file included from ./tensorflow/compiler/xla/hlo/ir/hlo_sharding.h:34,
                     from ./tensorflow/compiler/xla/hlo/ir/hlo_instruction.h:48,
                     from ./tensorflow/compiler/xla/hlo/ir/hlo_computation.h:35,
                     from ./tensorflow/compiler/xla/service/hlo_parser.h:23,
                     from tensorflow/compiler/xla/service/hlo_parser_test.cc:16:
    ./tensorflow/compiler/xla/hlo/ir/tile_assignment.h:173:12: note: candidate: 'xla::TileAssignment::TileAssignment(absl::lts_20230125::Span<const long int>)'
      173 |   explicit TileAssignment(absl::Span<const int64_t> dims)
          |            ^~~~~~~~~~~~~~
    ./tensorflow/compiler/xla/hlo/ir/tile_assignment.h:172:12: note: candidate: 'xla::TileAssignment::TileAssignment(xla::IotaTileAssignment)'
      172 |   explicit TileAssignment(IotaTileAssignment iota) : iota_(std::move(iota)) {}

And more
```
</details>"
61067,"Model runs without error in Tensorflow, but crashes with a segmentation fault in TFLite","### 1. System information

- OS Platform and Distribution: Ubuntu 20.04.4 LTS
- TensorFlow installation: pip
- TensorFlow library: 2.12.0
- TFLite runtime: 2.12.0

### 2. Code

The model is exported from PyTorch using ONNX. I have not included the PyTorch code below for brevity's sake (and because it is used for an active Kaggle competition); you can download the saved Keras model [here](https://cloud.ilabt.imec.be/index.php/s/Dgpi9SQTcyc23wm). The TFLite conversion code is given below, but you can also download the TFLite model [here](https://cloud.ilabt.imec.be/index.php/s/Dgpi9SQTcyc23wm) (same link).

Below is the code to create and save the Keras model from two PyTorch models `feat_gen` and `model`, converted using ONNX:

```python
class TFInferModel(tf.Module):
    def __init__(self):
        super(TFInferModel, self).__init__()
        self.feat_gen = tf.saved_model.load(""feat_gen.pb"")
        self.model = tf.saved_model.load(""model.pb"")

        self.feat_gen.trainable = False
        self.model.trainable = False

    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 126], dtype=tf.float32, name=""inputs"")])
    def call(self, inputs):
        output_tensors = {}

        # Add batch dimension.
        inputs = inputs[None]

        # Process using ported PyTorch model.
        features = self.feat_gen(inputs=inputs)[""outputs""]
        outputs = self.model(inputs=features)[""outputs""]

        # Remove batch dimension.
        outputs = outputs[0]

        output_tensors[""outputs""] = outputs
        return output_tensors

tf_model = TFInferModel()
tf.saved_model.save(tf_model, ""tf_model"", signatures={""serving_default"": tf_model.call})
```

The model can be loaded in Keras and run:

```python
model = tf.saved_model.load(""tf_model"")
inputs = tf.zeros((100, 126), dtype=tf.float32)
output = model.call(inputs=inputs)
```

It can also be converted to TFLite:

```python
converter = tf.lite.TFLiteConverter.from_saved_model(""tf_model"")

tf_lite_model = converter.convert()
output_path = ""model.tflite""
with open(output_path, ""wb"") as f:
    f.write(tf_lite_model)
```

And finally the code for TFLite inference:

```python
interpreter = tflite.Interpreter(model_path=""model.tflite"")
prediction_fn = interpreter.get_signature_runner(""serving_default"")
inputs = np.zeros((100, 126), dtype=np.float32)
output = prediction_fn(inputs=inputs)
```

### 3. Failure after conversion

The Keras inference code runs without issue. The TFLite inference code crashes immediately with a segmentation fault (no further info is given)."
61065,TF Lite Converter produces outputs in an incorrect order when multiple outputs are present,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.14.0-dev20230622

### 2. Code
This is the minimized code to reproduce the issue:
```python
import tensorflow as tf
import numpy as np

x1 = tf.constant([1.], shape=[1,1])

class Model(tf.keras.Model):
  def __init__(self):
    super(Model, self).__init__()

  def call(self, x1):
    x2 = tf.eye(1)
    x3 = tf.eye(2)
    x4 = tf.eye(1)
    return [x2, x3, x4]


m = Model()
expected_value = m(x1)

converter = tf.lite.TFLiteConverter.from_keras_model(m)
tflite_model = converter.convert()

def _evaluateTFLiteModel(tflite_model, input_data):
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()

    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    for i in range(len(input_data)):
        interpreter.set_tensor(input_details[i]['index'], input_data[i])

    interpreter.invoke()

    output_data = [interpreter.get_tensor(output_details[i]['index'])
                   for i in range(len(output_details))]
    return output_data

actual_value = _evaluateTFLiteModel(tflite_model,[x1])

#Outputs
print(f""Expected output_1: {expected_value[0].numpy()}"")
print(f""Lite output_1: {actual_value[0]}"")
print(""-----------------------------------"")
print(f""Expected output_2: {expected_value[1].numpy()}"")
print(f""Lite output_2: {actual_value[1]}"")
print(""-----------------------------------"")
print(f""Expected output_3: {expected_value[2].numpy()}"")
print(f""Lite output_3: {actual_value[2]}"")
#wrong order

tf.lite.experimental.Analyzer.analyze(model_content=tflite_model) #Output IR
```
### 3. Failure after conversion
Output (incorrect order):
```
Expected output_1: [[1.]]
Lite output_1: [[1.]]
-----------------------------------
Expected output_2: [[1. 0.]
 [0. 1.]]
Lite output_2: [[1.]]
-----------------------------------
Expected output_3: [[1.]]
Lite output_3: [[1. 0.]
 [0. 1.]]
```
Lite IR:
```
Subgraph#0 main(T#0) -> [T#1, T#1, T#2]

Tensors of Subgraph#0
  T#0(serving_default_input_1:0) shape_signature:[-1, 1], type:FLOAT32
  T#1(PartitionedCall:0) shape:[1, 1], type:FLOAT32 RO 4 bytes, buffer: 2, data:[1]
  T#2(PartitionedCall:1) shape:[2, 2], type:FLOAT32 RO 16 bytes, buffer: 3, data:[1, 0, 0, 1]
```
TF Lite converter produces wrong outputs:
- In the Lite IR, the ouput should be `[T#1, T#2, T#1]`   instead of `[T#1, T#1, T#2]` .

"
61064,from tensorflow.python._pywrap_tensorflow_internal import * ImportError: libflatbuffers.so.2: cannot open shared object file: No such file or directory,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.10.0

### Custom Code

Yes

### OS Platform and Distribution

CentOS Linux 7

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I was trying to import tensorflow, and then the following error occured:

```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
File ~/.conda/envs/TF/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py:62
     61 try:
---> 62   from tensorflow.python._pywrap_tensorflow_internal import *
     63 # This try catch logic is because there is no bazel equivalent for py_extension.
     64 # Externally in opensource we must enable exceptions to load the shared object
     65 # by exposing the PyInit symbols with pybind. This error will only be
     66 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.
     67 
     68 # This logic is used in other internal projects using py_extension.

ImportError: libflatbuffers.so.2: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Cell In[35], line 16
     13 import pandas as pd
     14 import h5py
---> 16 import tensorflow as tf
     17 from tensorflow import keras

File ~/.conda/envs/TF/lib/python3.10/site-packages/tensorflow/__init__.py:37
     34 import sys as _sys
     35 import typing as _typing
---> 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.

File ~/.conda/envs/TF/lib/python3.10/site-packages/tensorflow/python/__init__.py:36
     27 import traceback
     29 # We aim to keep this file minimal and ideally remove completely.
     30 # If you are adding a new file with @tf_export decorators,
     31 # import it in modules_with_exports.py instead.
     32 
     33 # go/tf-wildcard-import
     34 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
---> 36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
     37 from tensorflow.python.eager import context
     39 # pylint: enable=wildcard-import
     40 
     41 # Bring in subpackages.

File ~/.conda/envs/TF/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py:77
     75     sys.setdlopenflags(_default_dlopen_flags)
     76 except ImportError:
---> 77   raise ImportError(
     78       f'{traceback.format_exc()}'
     79       f'\n\nFailed to load the native TensorFlow runtime.\n'
     80       f'See https://www.tensorflow.org/install/errors '
     81       f'for some common causes and solutions.\n'
     82       f'If you need help, create an issue '
     83       f'at https://github.com/tensorflow/tensorflow/issues '
     84       f'and include the entire stack trace above this error message.')
     86 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/home/shd/.conda/envs/TF/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: libflatbuffers.so.2: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```

I am not sure what went wrong here. It used to work fine, and the only thing I've done since then is to update nodejs through conda. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

_No response_</details>"
61063,"Crashes in model.save, wrapt error","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

Fodera Linux

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Crashed when calling model.save()

See log below.

Worked after deinstalling tensorflow and wrapt. wrapt was 1.15.x
and installing tensorflow and wrapt==1.14.1

The problem is that when installing tensorflow, the wrapt 1.15.x is installed automatically and this is not playing with tensorflow.


### Standalone code to reproduce the issue

```shell
model.save('Modelname')

Causes the problem for any trained network.
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""ModelPredictorTraining.py"", line 1415, in <module>
    run_hparam_on_grid(branched_model_1,
  File ""ModelPredictorTraining.py"", line 1403, in run_hparam_on_grid
    fitted_model.save('Model-name')
  File ""/anaconda3/envs/tf2/lib/python3.11/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/anaconda3/envs/tf2/lib/python3.11/site-packages/tensorflow/python/trackable/data_structures.py"", line 823, in __getattribute__
    return super().__getattribute__(name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
```
</details>"
61060,jacobian computation throws ValueError for LinearOperatorFullMatrix: object of type 'LinearOperatorFullMatrix' has no len(),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0-dev20230621

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When computing jacobian for a `LinearOperatorFullMatrix`, TF throws an error: `ValueError: TypeError: object of type 'LinearOperatorFullMatrix' has no len()`.

I think the problem here might be that `len()` or `shape` is not implemented for `LinearOperatorFullMatrix`. If I just take the shape of a `LinearOperatorFullMatrix`, I can see the same error.

```
import tensorflow as tf
linop = tf.linalg.LinearOperatorFullMatrix(mat)
print(tf.shape(linop)) # Fails
# ValueError: TypeError: object of type 'LinearOperatorFullMatrix' has no len()
```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

mat = tf.Variable([[1., 2.], [3., 4.]])

with tf.GradientTape(persistent=True) as tape:
    tape.watch(mat)
    output = tf.linalg.LinearOperatorFullMatrix(mat)
    grad = tape.gradient(output, mat) # This works
    print('grad: ', grad)
    jac = tape.jacobian(output, mat) # This fails
    # ValueError: TypeError: object of type 'LinearOperatorFullMatrix' has no len()
```


### Relevant log output

```shell
grad:  tf.Tensor(
[[1. 1.]
 [1. 1.]], shape=(2, 2), dtype=float32)

Traceback
.../tensorflow/python/framework/constant_op.py"", line 101, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: TypeError: object of type 'LinearOperatorFullMatrix' has no len()
```
</details>"
60951,the tf keras models load_model() used for loading ml model is not able to load model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hi, I am trying to load my deep learning model using  **tensorflow keras models load_model ()** . when I run it for first time it got loaded but from next time** it is not loading**. Like it is in this function for more than 30 min. 

I store my model in **.h5** format. model size is approx 13 MB.
At the time of saving deep learning, I use model.save() 

I am using a machine with 128 GB RAM. 
I am using multiprocessing with no of worker 16. Sometime it worked and sometime is got stucked.


### Standalone code to reproduce the issue

```shell
tensorflow.keras.models.load_model.()
```


### Relevant log output

_No response_</details>"
60944,Apply Profile-Guided Optimization (PGO) on Tensorflow,"### Describe the problem
Profile-Guided Optimization (PGO) could help with achieving additional performance with Tensorflow. I guess some parts of Tensorflow are good candidates (like https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler). Probably there are other good candidates - I am not familiar yet with the Tensorflow codebase.

Why I mentioned `tfcompile` is that PGO works especially well on compilers in practice (see [this](https://github.com/ZaMaZaN4iK/awesome-pgo) to read about the results on multiple applications).
"
60943,Request: pip packages with CUDA 12 support,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

All

### Mobile device

All

### Python version

All

### Bazel version

All

### GCC/Compiler version

All

### CUDA/cuDNN version

12

### GPU model and memory

All

### Current Behaviour?

For reference, see the discussion on this PR and comment from @reedwm #58867 

```
This is a PR that has been merged, not an issue, so it cannot be reopened.

We have not yet released pip packages with CUDA 12 support, but are working on this. Feel free to file a new GitHub issue to have CUDA 12 pip packages (please CC me on the issue if you file it).
```

This pull request was focused on CUDA 12.X support in builds, but did not contain the scope for building new python packages for standard use on pypi
https://pypi.org/project/tensorflow/#history

Of note, Debian Bookworm is deprecating usage of CUDA 11.X versions, so only 12.X CUDA drivers are available. Tensorflow packages on pypi have yet to support CUDA 12.X drivers.

### Standalone code to reproduce the issue

```shell
See pypi, there is not a version of tensorflow with CUDA 12.X support
https://pypi.org/project/tensorflow/#history
```


### Relevant log output

_No response_</details>"
60942,float8 (both e4m3fn and e5m2) missing from numbertype,"### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

macOS-13.2.1-arm64-arm-64bit

### Mobile device

_No response_

### Python version

3.9.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

FP8 datatypes are missing from `kNumberTypes` in `tensorflow/core/framework/types.h`, and also missing from `TF_CALL_FLOAT_TYPES(m)` in `tensorflow/core/framework/register_types.h`. This causes simple ops (like slice, transpose, split, etc.) to raise NotFoundError.

### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow.python.framework import dtypes

a = tf.constant([[1.2345678, 2.3456789, 3.4567891], [4.5678912, 5.6789123, 6.7891234]], dtype=dtypes.float16)
print(a)

a_fp8 = tf.cast(a, dtypes.float8_e4m3fn)
print(a_fp8)

b = a_fp8[1:2] # tensorflow.python.framework.errors_impl.NotFoundError
b = tf.transpose(a_fp8, [1, 0]) # tensorflow.python.framework.errors_impl.NotFoundError
```


### Relevant log output

```
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node StridedSlice}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT8_E4M3FN, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=0]
All kernels registered for op StridedSlice:
  device='XLA_CPU_JIT'; Index in [DT_INT32, DT_INT16, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, 930109355527764061, DT_HALF, DT_UINT32, DT_UINT64, DT_FLOAT8_E5M2, DT_FLOAT8_E4M3FN]
  device='CPU'; T in [DT_UINT64]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_UINT32]
  device='CPU'; T in [DT_UINT16]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_BOOL]
  device='CPU'; T in [DT_STRING]
  device='CPU'; T in [DT_RESOURCE]
  device='CPU'; T in [DT_VARIANT]
  device='CPU'; T in [DT_QINT8]
  device='CPU'; T in [DT_QUINT8]
  device='CPU'; T in [DT_QINT32]
  device='DEFAULT'; T in [DT_INT32]
 [Op:StridedSlice] name: strided_slice/
```

```
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node Transpose}} = Transpose[T=DT_FLOAT8_E4M3FN, Tperm=DT_INT32]
All kernels registered for op Transpose:
  device='XLA_CPU_JIT'; Tperm in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, 930109355527764061, DT_HALF, DT_UINT32, DT_UINT64, DT_FLOAT8_E5M2, DT_FLOAT8_E4M3FN]
  device='CPU'; T in [DT_UINT64]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_UINT32]
  device='CPU'; T in [DT_UINT16]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_BOOL]
  device='CPU'; T in [DT_STRING]
  device='CPU'; T in [DT_RESOURCE]
  device='CPU'; T in [DT_VARIANT]
 [Op:Transpose]
```"
60941,tf.mul after tf.split + tf.sigmoid produces wrong numerical results with MKL enabled,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

intel-tensorflow 2.8 - 2.12

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When running with an MKL enabled tensorflow (e.g. intel-tensorflow from pypi) (or self-compiled with `--config=mkl`). Starting with tensorflow 2.8.0 up until 2.12.0 The attached code produces the wrong numerical result. (1.7615 vs expected 2.6439).

If line 25 is changed to `m = sig * (b + 0.0)` one can get the correct result.
This issue does not occur if installing ""vanilla"" tesorflow from pip with `pip install tensorflow`.

This issue also does not occur if one uses `tf.exp` or `tf.log` instead of `tf.sigmoid`.

### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python3

import math

import tensorflow as tf
import numpy as np

def sigmoid(x):
  return 1 / (1 + math.exp(-x))

data = [[[2.0, 3.0]]]

tf.compat.v1.disable_eager_execution()

s = tf.compat.v1.Session()
p = tf.compat.v1.placeholder(dtype=tf.float32)
a, b = tf.split(p, 2, axis=2)
sig = tf.sigmoid(a)
m = sig * b

out = s.run([p, m], feed_dict={p: data})
print(out)

print('computed: ', out[-1][0,0,0])
print('expected: ', sigmoid(data[0][0][0]) * data[0][0][1])
```


### Relevant log output

```shell
2023-06-21 18:37:35.713027: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-21 18:37:35.715264: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 
[array([[[2., 3.]]], dtype=float32), array([[[1.7615942]]], dtype=float32)]
computed:  1.7615942
expected:  2.642391233933647
```
</details>"
60938,"Error: Input 0 of layer 'batch_normalization' is incompatible with the layer: expected ndim=2, found ndim=4.","Estoy intentando entrenar un modelo GAN utilizando Keras en TensorFlow, pero estoy encontrando el siguiente error:

```
Input 0 of layer 'batch_normalization' is incompatible with the layer: expected ndim=2, found ndim=4. Full shape received: (None, None, None, 32768)
```

El error ocurre cuando intento entrenar el modelo con el siguiente código:

```python
history = model.fit(train_generator, epochs=100, callbacks=[checkpoint, tensorboard])
```

¿Alguien podría ayudarme a entender qué está causando este error y cómo puedo solucionarlo? Gracias."
60936,android gpu delegate Failed to build program executable,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10 or 2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!
I test gpu delegate on oppo R9. 
TfLiteInterpreterModifyGraphWithDelegate will reture error value.

ERROR: Failed to build program executable - Build program failure<source>:35:26: error: OpenCL extension 'cl_khr_3d_image_writes' is not supported
#pragma OPENCL EXTENSION cl_khr_3d_image_writes : enable
                         ^
error: Compiler frontend failed (error code 58)

ERROR: Falling back to OpenGL
ERROR: TfLiteGpuDelegate Init: OpenGL-based API disabled
ERROR: TfLiteGpuDelegate Prepare: delegate is not initialized
ERROR: Node number 100 (TfLiteGpuDelegateV2) failed to prepare.
ERROR: Restored original execution plan after delegate application failure.
tflite gpu Delegate create failed!2

### Standalone code to reproduce the issue

```shell
oppo R9 is produced in 2016 year
```


### Relevant log output

_No response_</details>"
60935,fit() fails with CUDNN_STATUS_BAD_PARAM when using Conv3D and multi-GPU MirroredStrategy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

v1.12.1-95675-g47602c0bad8 2.14.0-dev20230620

### Custom Code

Yes

### OS Platform and Distribution

Rocky Linux release 8.6 (Green Obsidian)

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuda_11.8.r11.8/compiler.31833905_0 / cuDNN version 8600

### GPU model and memory

4 NVIDIA A100s w/ 80GB each

### Current Behaviour?

When executing a model fit that includes a `Conv3D` layer on multiple GPUs, I'm encountering a `CUDNN_STATUS_BAD_PARAM` error in the gradient computation step. No errors occur when running on a single GPU, nor when I swap out `Conv3D` with `AveragePooling3D` or `Conv2D`. However, `Conv3DTranspose` also fails.

```none
  (0) UNKNOWN:  CUDNN_STATUS_BAD_PARAM
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(3549): 'tensor' CUDNN_BACKEND_TENSOR_DESCRIPTOR: Check and Set the CUDNN_ATTR_TENSOR_DIMENSIONS Correctly
         [[{{node gradient_tape/replica_2/model/conv3d/Conv3D/Conv3DBackpropFilterV2}}]]
         [[div_no_nan/ReadVariableOp_1/_52]]
         [[group_deps/_95]]
         [[Adam/update_2_2/AssignAddVariableOp/_119]]
         [[group_deps/_103]]
```

With the `Graph execution error` traceback:
```none
Traceback (most recent call last):
  File ""conv3_multi_gpu_fail_repro.py"", line 25, in <module>
    model.fit(x_data, y_data, batch_size=1, epochs=1, verbose=1)
  File ""/***/env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/***/env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnknownError: Graph execution error:

Detected at node gradient_tape/replica_3/model/conv3d/Conv3D/Conv3DBackpropFilterV2 defined at (most recent call last):
  File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
    self.run()
  File ""/***/env/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1348, in run_step
    outputs = model.train_step(data)
  File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
    self.run()
  File ""/***/env/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1348, in run_step
    outputs = model.train_step(data)
  File ""/***/env/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1129, in train_step
    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
  File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
    self.run()
  File ""/***/env/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1348, in run_step
    outputs = model.train_step(data)
  File ""/***/env/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1129, in train_step
    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
  File ""/***/env/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py"", line 543, in minimize
    grads_and_vars = self.compute_gradients(loss, var_list, tape)
  File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
    self.run()
  File ""/***/env/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1348, in run_step
    outputs = model.train_step(data)
  File ""/***/env/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1129, in train_step
    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
  File ""/***/env/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py"", line 543, in minimize
    grads_and_vars = self.compute_gradients(loss, var_list, tape)
  File ""/***/env/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py"", line 276, in compute_gradients
    grads = tape.gradient(loss, var_list)
```

I'm running from the `tensorflow/tensorflow:nightly-gpu` docker image.


### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow.keras import layers, models

input_shape = (28, 28, 28, 1)
num_samples = 10

x_data = tf.random.uniform((num_samples, *input_shape), 0, 1)
y_data = tf.random.uniform((num_samples, *input_shape), 0, 1)

multi_gpu=True # <== fails
#multi_gpu=False # <== works
devices = [] if multi_gpu else ['/gpu:0']

mirrored_strategy = tf.distribute.MirroredStrategy(devices=devices)
print(f""{mirrored_strategy.num_replicas_in_sync} replica(s)"")

with mirrored_strategy.scope():
    inputs = layers.Input(shape=input_shape)
    outputs = layers.Conv3D(1, 1)(inputs) # <== fails
    #outputs = layers.AveragePooling3D(1)(inputs) # <== works
    #outputs = layers.Conv2D(1, 1)(inputs) # <== works
    model = models.Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='binary_crossentropy')

model.fit(x_data, y_data, batch_size=1, epochs=1, verbose=1)
```


### Relevant log output

_No response_</details>"
60934,Ai,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
60932,get_file raises exception if the content-length is unknown,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Calling get_file on a URL that doesn't indicate the content-length causes an exception.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

p=tf.keras.utils.get_file(fname=""auto-mpg.csv"",
  origin=""http://archive.ics.uci.edu/ml/""+
  ""machine-learning-databases/auto-mpg/auto-mpg.data"")
print(p)
```


### Relevant log output

```shell
Downloading data from http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data
   8192/Unknown - 0s 0us/stepTraceback (most recent call last):
  File ""/home/kent/env-ml/src/f3.py"", line 3, in <module>
    p=tf.keras.utils.get_file(fname=""auto-mpg.csv"",
  File ""/home/kent/env-ml/lib/python3.10/site-packages/keras/utils/data_utils.py"", line 300, in get_file
    urlretrieve(origin, fpath, DLProgbar())
  File ""/home/kent/env-ml/lib/python3.10/site-packages/keras/utils/data_utils.py"", line 86, in urlretrieve
    for chunk in chunk_read(response, reporthook=reporthook):
  File ""/home/kent/env-ml/lib/python3.10/site-packages/keras/utils/data_utils.py"", line 78, in chunk_read
    reporthook(count, chunk_size, total_size)
  File ""/home/kent/env-ml/lib/python3.10/site-packages/keras/utils/data_utils.py"", line 294, in __call__
    self.progbar.update(self.progbar.target)
  File ""/home/kent/env-ml/lib/python3.10/site-packages/keras/utils/generic_utils.py"", line 252, in update
    bar = ""%7d/Unknown"" % current
TypeError: %d format: a real number is required, not NoneType
```
</details>"
60930,The repository '@llvm_zlib' could not be resolved and referenced by '@llvm-project//llvm:Support',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

master

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.1.0

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

tf-opt Build failure!

### Standalone code to reproduce the issue

```shell
build MLIR:
 -G Ninja ../llvm \
-DLLVM_BUILD_EXAMPLES=ON \
-DLLVM_ENABLE_PROJECTS=mlir \
-DLLVM_BUILD_EXAMPLES=ON \
-DLLVM_TARGETS_TO_BUILD=""X86"" \
-DCMAKE_BUILD_TYPE=Release \
-DLLVM_ENABLE_ASSERTIONS=ON\
-DCMAKE_C_COMPILER=/home/clang+llvm-16.0.0-x86_64-linux-gnu-ubuntu-18.04/bin/clang\
-DCMAKE_CXX_COMPILER=/home/clang+llvm-16.0.0-x86_64-linux-gnu-ubuntu-18.04/bin/clang++ \
-DLLVM_ENABLE_ZLIB=ON\
-DLLVM_ENABLE_ZLIB=ON\
-DLLVM_ENABLE_ZSTD=ON

bazel build:
bazel build --override_repository=""llvm-raw=${LLVM_SRC}""   \
-c opt tensorflow/compiler/mlir:tf-opt```



### Relevant log output

```shell
ERROR: /root/.cache/bazel/_bazel_root/58adfe0c0193ce259b2b32549c3d3a4f/external/llvm-project/llvm/BUILD.bazel:184:11: no such package '@llvm_zlib//': The repository '@llvm_zlib' could not be resolved: Repository '@llvm_zlib' is not defined and referenced by '@llvm-project//llvm:Support'
ERROR: Analysis of target '//tensorflow/compiler/mlir:tf-opt' failed; build aborted:
INFO: Elapsed time: 2.104s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (198 packages loaded, 3560 targets configured)
```
</details>"
60929,`Bias` fails to broadcast in the context of `matmul` in tf lite model,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.14.0-dev20230602

### 2. Code
This is the minimized code to reproduce the issue:
```python
import tensorflow as tf
import numpy as np

x1 = tf.constant([1., 2.], shape=[1, 2])

class Model(tf.keras.Model):
  def __init__(self):
    super(Model, self).__init__()
    self.w = tf.Variable([[3., 4.], [5., 6.]])
    self.b = tf.Variable([3.])
  @tf.function(input_signature=[tf.TensorSpec(x1.shape, x1.dtype)])
  def call(self, x):
    return tf.matmul(x, self.w) + self.b


m = Model()
print('Keras mode output: ', m(x1).numpy())

converter = tf.lite.TFLiteConverter.from_keras_model(m)
tflite_model = converter.convert()
def _evaluateTFLiteModel(tflite_model, input_data):
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()

    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    for i in range(len(input_data)):
        interpreter.set_tensor(input_details[i]['index'], input_data[i])

    interpreter.invoke()

    output_data = [interpreter.get_tensor(output_details[i]['index'])
                   for i in range(len(output_details))]
    return output_data

print('Lite mode output: ', _evaluateTFLiteModel(tflite_model,[x1])[0])

```
### 3. Failure after conversion
Output:
```
Keras mode output: [[16. 19.]]
Lite mode output:  RuntimeError: tensorflow/lite/kernels/fully_connected.cc:360 NumElements(bias) != SizeOfDimension(filter, 0) (1 != 2)Node number 0 (FULLY_CONNECTED) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0.
```
Conversion Failure:
- During conversion, the model fails due to the following check at `tensorflow/lite/kernels/fully_connected.cc:360`
```cpp
if (bias) {
    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 0));
  }
```"
60926,FFT produces wrong results when using multiple GPUs with MirroredStrategy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0 2.14.0-dev20230619

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8.0 / 8.6.0.163

### GPU model and memory

_No response_

### Current Behaviour?

Using TensorFlow FFT in a Keras model will produce incorrect results when using MirroredStrategy and multiple GPUs.
This is not an accuracy issue. The results of consecutive calls seem to be either correct or garbage.

I created a test Keras model that has one layer that does FFT. There is also a reference model using a DFT layer that is used to verify that incorrect behavior only happens when using tf.signal.fft.
Attached is a test application that runs both models in different combinations of MirroredStrategy/default strategy and eager/graph execution.
MirroredStrategy and graph execution is the combination that produces the error. At least two GPUs are required to reproduce the problem.
The output MAE loss is around 6.5, which translates to 650% error. (The absolute value of each entry in the correct output is 1.0.)

I think it's not a user error, but if it is, there should be an error or warning instead of incorrect results.

I was able to reproduce the issue with all TF fft variants (tf.signal.fft, tf.signal.rfft, tf.signal.stft, tf.signal.fft2d)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from scipy.linalg import dft
from math import sqrt

# Layer that does tf.signal.fft operation
class FFTLayer(tf.keras.layers.Layer):

    def call(self, x):
      fx = tf.signal.fft(x)
      return fx

# Layer that returns same results as tf.signal.fft op, but
# uses slower direct computation of DFT, implemented as matrix multiply.
class MatrixDFTLayer(tf.keras.layers.Layer):
    def __init__(self):
      super().__init__()
      self.dft = tf.cast(dft(1024), tf.complex64)

    def call(self, x):
        fx = self.dft @ tf.transpose(x)
        return tf.transpose(fx)


def create_model(use_mirrored_strategy: bool = True,
                              run_eagerly: bool = True,
                              layer_to_use: tf.keras.layers.Layer = FFTLayer) -> None:
    print(f""\ncreate model with: use_mirrored_strategy: {use_mirrored_strategy}, "",
          f""run_eagerly: {run_eagerly}, "",
          f""layer_to_use: {layer_to_use}"")

    if use_mirrored_strategy:
        distribution_strategy = tf.distribute.MirroredStrategy()
    else:
        distribution_strategy = tf.distribute.get_strategy()
    with distribution_strategy.scope():

        ins = tf.keras.layers.Input([1024], dtype=tf.complex64)
        x = layer_to_use()(ins)
        model = tf.keras.Model(inputs=ins, outputs=x)

        model.compile(
            loss=tf.keras.losses.MeanAbsoluteError(),
            run_eagerly=run_eagerly
        )
    return model


def create_data(fft_size, batch_size, num_steps):
    num_examples = num_steps * batch_size

    # y data is a complex vector of all (1/sqrt(2), (1/sqrt(2)j)
    train_y = np.ones([fft_size], np.float32)
    train_y = (1/sqrt(2))*train_y + (1/sqrt(2))*1j*train_y
    # abs mean is 1 -> MAE magnitude should be compared to 1
    print(""train_y mean: "", tf.reduce_mean(tf.abs(train_y)))

    # use inverse transform to create input data
    # fft(train_x) will produce train_y
    train_x = tf.signal.ifft(train_y)

    # clone data to get larger training set
    train_y = train_y[tf.newaxis, ...]
    train_x = train_x[tf.newaxis, ...]

    train_x = tf.tile(train_x, [num_examples, 1])
    train_y = tf.tile(train_y, [num_examples, 1])
    
    return train_x, train_y


fft_size = 1024
batch_size = 9
num_steps = 100
train_x, train_y = create_data(fft_size, batch_size, num_steps)

# Test cases with MatrixDFTLayer
# These are all ok, MAE close to 0.0
# ok
model = create_model(use_mirrored_strategy=False, run_eagerly=False, layer_to_use=MatrixDFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
# ok
model = create_model(use_mirrored_strategy=False, run_eagerly=True, layer_to_use=MatrixDFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
# ok
model = create_model(use_mirrored_strategy=True, run_eagerly=False, layer_to_use=MatrixDFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")

# Test Cases using TF FFT. These fail when using MirroredStrategy.
# ok
model = create_model(use_mirrored_strategy=False, run_eagerly=False, layer_to_use=FFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
# ok
model = create_model(use_mirrored_strategy=False, run_eagerly=True, layer_to_use=FFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
# fail, 
model = create_model(use_mirrored_strategy=True, run_eagerly=False,layer_to_use= FFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
```


### Relevant log output

```shell
train_y mean:  tf.Tensor(1.0, shape=(), dtype=float32)

create model with: use_mirrored_strategy: False,  run_eagerly: False,  layer_to_use: <class '__main__.MatrixDFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: False,  run_eagerly: True,  layer_to_use: <class '__main__.MatrixDFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: True,  run_eagerly: False,  layer_to_use: <class '__main__.MatrixDFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: False,  run_eagerly: False,  layer_to_use: <class '__main__.FFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: False,  run_eagerly: True,  layer_to_use: <class '__main__.FFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: True,  run_eagerly: False,  layer_to_use: <class '__main__.FFTLayer'>
loss: 6.451958656311035
```
</details>"
60925,`Unpack` and `concat` wrongly transformed into `reshape` in tflite converter,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.14.0-dev20230602

### 2. Code
This is the minimized code to reproduce the issue:
```python
import tensorflow as tf
import numpy as np
x1 = tf.constant([1., 2., 3., 4.], shape=[2, 2, 1])

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(input_signature=[tf.TensorSpec(x1.shape, x1.dtype)])
  def call(self, x):
    unpack_op = tf.raw_ops.Unpack(value=x,num=2,axis=0)
    return tf.concat(unpack_op, -1)
m = Model()
m(x1)
print('Keras mode output: ', m(x1).numpy())

converter = tf.lite.TFLiteConverter.from_keras_model(m)
tflite_model = converter.convert()

def _evaluateTFLiteModel(tflite_model, input_data):
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    for i in range(len(input_data)):
        interpreter.set_tensor(input_details[i]['index'], input_data[i])
    interpreter.invoke()
    output_data = [interpreter.get_tensor(output_details[i]['index'])
                   for i in range(len(output_details))]
    return output_data
print('Lite mode output: ', _evaluateTFLiteModel(tflite_model,[x1])[0])
tf.lite.experimental.Analyzer.analyze(model_content=tflite_model) #Output IR
```
### 3. Failure after conversion
Output:
```
Keras mode output:  [[1. 3.]
 [2. 4.]]
Lite mode output:  [[1. 2.]
 [3. 4.]]
```
Lite IR:
```
Subgraph#0 main(T#0) -> [T#2]
  Op#0 RESHAPE(T#0, T#1[2, 2]) -> [T#2]

Tensors of Subgraph#0
  T#0(serving_default_args_0:0) shape:[2, 2, 1], type:FLOAT32
  T#1(concat) shape:[2], type:INT32 RO 8 bytes, buffer: 2, data:[2, 2]
  T#2(PartitionedCall:0) shape:[2, 2], type:FLOAT32
```
Model produces wrong results:
- Using ```tf.lite.experimental.Analyzer.analyze```, we can see the entire calculation is wrongly transformed into a single ```RESHAPE``` operator.
"
60923,JIT compilation failed error when running the pix2pix template on GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.1

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cudatoolkit=11.2 cudnn=8.1.0

### GPU model and memory

NVIDIA Quadro M4000 8GB

### Current Behaviour?

I receive this error when I run the [pix2pix 1](https://www.tensorflow.org/tutorials/generative/pix2pix) template in a jupyter notebook on tensorflow GPU:
![image](https://github.com/tensorflow/tensorflow/assets/127302774/23962358-9ee9-4a57-8892-f3bbe99e483d)

When running only with CPU, no error appears though.



### Standalone code to reproduce the issue

```shell
This is the fit function, where the error appears:

def fit(train_ds, test_ds, steps):
 example_input, example_target = next(iter(test_ds.take(1)))
 start = time.time()

 for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():
  if (step) % 1000 == 0:
   display.clear_output(wait=True)

  if step != 0:
    print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\n')

  start = time.time()

  generate_images(generator, example_input, example_target)
  print(f""Step: {step//1000}k"")

train_step(input_image, target, step)

# Training step
if (step+1) % 10 == 0:
  print('.', end='', flush=True)


# Save (checkpoint) the model every 5k steps
if (step + 1) % 5000 == 0:
  checkpoint.save(file_prefix=checkpoint_prefix)
```


### Relevant log output

_No response_</details>"
60922,Broken build due to LLVM,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Debian Bookworm plus Clang/LLVM 17.0.0 (experimental)

### Mobile device

not applicable

### Python version

3.11.4

### Bazel version

5.3.0 - as per original configuration

### GCC/Compiler version

GCC 12.2.0 / Clang 16.0.6 / Clang 17.0.0

### CUDA/cuDNN version

NO

### GPU model and memory

nothing to do with that

### Current Behaviour?

Compilation fails due to:
Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/b9232f9e27e5668bc0414879dcdedb2a59ea75f2.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/659147817805d17c7be2d60bd7bbca7e780f9c82.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/10939d1d580b9d3c9c2f3539c6bdb39f408179c0.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/91d765cad5599f9710973d3e34d4dc22583e2e79.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found


### Standalone code to reproduce the issue

```shell
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=/usr/lib/llvm-17/bin:${JAVA_HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

Failure occurs for all compiler mentioned above:
Commands/examples:
CC=/usr/lib/llvm-17/bin/clang CXX=/usr/lib/llvm-16/bin/clang bazel-5.3.0 build --config=tpu --verbose_failures //tensorflow:tensorflow_cc 2>&1 | tee ../build-tensorflow_cc.log
CC=/usr/lib/llvm-17/bin/clang CXX=/usr/lib/llvm-16/bin/clang bazel-5.3.0 build --config=tpu --verbose_failures //tensorflow:tensorflow_framework 2>&1 | tee ../build-tensorflow_framework.log
CC=/usr/lib/llvm-17/bin/clang CXX=/usr/lib/llvm-16/bin/clang bazel-5.3.0 build --config=tpu --verbose_failures //tensorflow/tools/pip_package:build_pip_package 2>&1 | tee ../build-tensorflow_py3.log
```

I have also cleaned bazel cache directory and checked urls( not available also 404)

### Relevant log output

```shell
1). CC=/usr/lib/llvm-17/bin/clang CXX=/usr/lib/llvm-16/bin/clang bazel-5.3.0 build --config=tpu --verbose_failures //tensorflow:tensorflow_cc 2>&1 | tee ../build-tensorflow_cc.log

Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/...../libtensorflow-dev/tensorflow-2.12.0/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3.11/dist-packages --python_path=/usr/bin/python3
INFO: Reading rc options for 'build' from /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:tpu in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --define=with_tpu_support=true
INFO: Found applicable config definition build:linux in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
Loading: 
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/91d765cad5599f9710973d3e34d4dc22583e2e79.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/10939d1d580b9d3c9c2f3539c6bdb39f408179c0.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
Loading: 0 packages loaded
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
Analyzing: target //tensorflow:tensorflow_cc (1 packages loaded, 0 targets configured)
Analyzing: target //tensorflow:tensorflow_cc (34 packages loaded, 15 targets configured)
Analyzing: target //tensorflow:tensorflow_cc (34 packages loaded, 15 targets configured)
Analyzing: target //tensorflow:tensorflow_cc (34 packages loaded, 15 targets configured)
Analyzing: target //tensorflow:tensorflow_cc (95 packages loaded, 215 targets configured)
Analyzing: target //tensorflow:tensorflow_cc (424 packages loaded, 24010 targets configured)
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/659147817805d17c7be2d60bd7bbca7e780f9c82.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
Analyzing: target //tensorflow:tensorflow_cc (427 packages loaded, 25063 targets configured)
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/b9232f9e27e5668bc0414879dcdedb2a59ea75f2.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow:tensorflow_cc (428 packages loaded, 26822 targets configured).
INFO: Found 1 target...
[0 / 7,736] [Prepa] Writing file tensorflow/libtensorflow_cc.so.2.12.0-2.params ... (4 actions, 0 running)
[277 / 9,815] Compiling llvm/lib/Support/Signals.cpp; 4s local ... (8 actions, 7 running)
ERROR: /home/...../.cache/bazel/_bazel_root/9c8aaa3c6704dd9093a3ba260ef8cc3f/external/llvm-project/llvm/BUILD.bazel:356:11: Compiling llvm/lib/TableGen/TableGenBackendSkeleton.cpp failed: undeclared inclusion(s) in rule '@llvm-project//llvm:TableGen':
this rule is missing dependency declarations for the following files included by 'llvm/lib/TableGen/TableGenBackendSkeleton.cpp':
  'bazel-out/k8-opt/bin/external/llvm-project/llvm/Demangle.cppmap'
  'bazel-out/k8-opt/bin/external/llvm_terminfo/terminfo.cppmap'
  'bazel-out/k8-opt/bin/external/llvm_zlib/zlib.cppmap'
Target //tensorflow:tensorflow_cc failed to build
INFO: Elapsed time: 213.824s, Critical Path: 16.79s
INFO: 299 processes: 259 internal, 40 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully

2). CC=/usr/lib/llvm-17/bin/clang CXX=/usr/lib/llvm-16/bin/clang bazel-5.3.0 build --config=tpu --verbose_failures //tensorflow:tensorflow_framework 2>&1 | tee ../build-tensorflow_framework.log
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/...../libtensorflow-dev/tensorflow-2.12.0/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3.11/dist-packages --python_path=/usr/bin/python3
INFO: Reading rc options for 'build' from /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:tpu in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --define=with_tpu_support=true
INFO: Found applicable config definition build:linux in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
Loading: 
Loading: 0 packages loaded
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/10939d1d580b9d3c9c2f3539c6bdb39f408179c0.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/91d765cad5599f9710973d3e34d4dc22583e2e79.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
Analyzing: target //tensorflow:tensorflow_framework (0 packages loaded, 0 targets configured)
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/b9232f9e27e5668bc0414879dcdedb2a59ea75f2.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow:tensorflow_framework (0 packages loaded, 1 target configured).
INFO: Found 1 target...
[3 / 204] [Prepa] Writing script tensorflow/tsl/platform/status.cppmap ... (2 actions, 0 running)
[13 / 1,949] Compiling src/google/protobuf/compiler/cpp/extension.cc; 1s local ... (8 actions, 7 running)
[13 / 1,949] Compiling src/google/protobuf/compiler/cpp/extension.cc; 2s local ... (8 actions running)
[18 / 1,949] Compiling src/google/protobuf/compiler/java/message.cc; 3s local ... (8 actions running)
[20 / 1,949] Compiling src/google/protobuf/compiler/java/message.cc; 4s local ... (8 actions running)
[21 / 1,949] Compiling src/google/protobuf/compiler/cpp/parse_function_generator.cc; 2s local ... (8 actions running)
[24 / 1,949] Compiling src/google/protobuf/compiler/cpp/parse_function_generator.cc; 3s local ... (8 actions running)
[26 / 1,949] Compiling src/google/protobuf/compiler/cpp/parse_function_generator.cc; 4s local ... (8 actions running)
[27 / 1,949] Compiling src/google/protobuf/compiler/cpp/parse_function_generator.cc; 6s local ... (8 actions running)
[29 / 1,949] Compiling src/google/protobuf/compiler/cpp/parse_function_generator.cc; 7s local ... (8 actions running)
[31 / 1,949] Compiling src/google/protobuf/compiler/php/php_generator.cc; 6s local ... (8 actions running)
[35 / 1,949] Compiling src/google/protobuf/compiler/php/php_generator.cc; 8s local ... (8 actions running)
[37 / 1,949] Compiling src/google/protobuf/compiler/php/php_generator.cc; 10s local ... (8 actions running)
[47 / 1,949] Compiling src/google/protobuf/stubs/substitute.cc; 1s local ... (8 actions running)
[54 / 1,949] Compiling src/google/protobuf/util/field_mask_util.cc; 3s local ... (8 actions running)
[67 / 1,949] Compiling src/google/protobuf/util/message_differencer.cc; 4s local ... (8 actions, 7 running)
[83 / 1,949] Compiling src/google/protobuf/compiler/cpp/enum.cc; 4s local ... (8 actions, 7 running)
[93 / 1,949] Compiling src/google/protobuf/compiler/cpp/message_field.cc; 3s local ... (8 actions, 7 running)
[104 / 1,949] Compiling src/google/protobuf/compiler/cpp/message.cc; 6s local ... (8 actions, 7 running)
[122 / 1,949] Compiling src/google/protobuf/compiler/cpp/message.cc; 12s local ... (8 actions, 7 running)
[141 / 1,949] Compiling src/google/protobuf/wire_format.cc; 4s local ... (8 actions, 7 running)
[161 / 1,949] Compiling src/google/protobuf/descriptor.cc; 6s local ... (8 actions, 7 running)
[176 / 1,949] Compiling src/google/protobuf/descriptor.cc; 16s local ... (8 actions, 7 running)
[217 / 2,179] Compiling src/google/protobuf/compiler/java/extension.cc; 4s local ... (8 actions, 7 running)
ERROR: /home/...../.cache/bazel/_bazel_root/9c8aaa3c6704dd9093a3ba260ef8cc3f/external/llvm-project/llvm/BUILD.bazel:593:11: Compiling llvm/utils/TableGen/Attributes.cpp failed: undeclared inclusion(s) in rule '@llvm-project//llvm:tblgen':
this rule is missing dependency declarations for the following files included by 'llvm/utils/TableGen/Attributes.cpp':
  'bazel-out/k8-opt/bin/external/llvm-project/llvm/Demangle.cppmap'
  'bazel-out/k8-opt/bin/external/llvm_terminfo/terminfo.cppmap'
  'bazel-out/k8-opt/bin/external/llvm_zlib/zlib.cppmap'
Target //tensorflow:tensorflow_framework failed to build
INFO: Elapsed time: 92.924s, Critical Path: 31.68s
INFO: 257 processes: 43 internal, 214 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully

3). CC=/usr/lib/llvm-17/bin/clang CXX=/usr/lib/llvm-16/bin/clang bazel-5.3.0 build --config=tpu --verbose_failures //tensorflow/tools/pip_package:build_pip_package 2>&1 | tee ../build-tensorflow_py3.log
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/...../libtensorflow-dev/tensorflow-2.12.0/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3.11/dist-packages --python_path=/usr/bin/python3
INFO: Reading rc options for 'build' from /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:tpu in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --define=with_tpu_support=true
INFO: Found applicable config definition build:linux in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/...../libtensorflow-dev/tensorflow-2.12.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
Loading: 
Loading: 0 packages loaded
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/10939d1d580b9d3c9c2f3539c6bdb39f408179c0.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/91d765cad5599f9710973d3e34d4dc22583e2e79.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded, 0 targets configured)
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/b9232f9e27e5668bc0414879dcdedb2a59ea75f2.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (66 packages loaded, 992 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (153 packages loaded, 5342 targets configured)
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/659147817805d17c7be2d60bd7bbca7e780f9c82.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (158 packages loaded, 6276 targets configured)
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (161 packages loaded, 7843 targets configured).

INFO: Found 1 target...
[0 / 2] [Prepa] BazelWorkspaceStatusAction stable-status.txt
[199 / 8,018] Executing genrule @local_config_python//:python_include; 0s local ... (8 actions, 7 running)
[228 / 8,216] Executing genrule @local_config_python//:python_include; 2s local ... (8 actions running)
[232 / 8,216] Compiling llvm/utils/TableGen/AsmMatcherEmitter.cpp; 3s local ... (8 actions, 7 running)
[232 / 8,216] Compiling llvm/utils/TableGen/AsmMatcherEmitter.cpp; 5s local ... (8 actions running)
ERROR: /home/...../.cache/bazel/_bazel_root/9c8aaa3c6704dd9093a3ba260ef8cc3f/external/llvm-project/llvm/BUILD.bazel:593:11: Compiling llvm/utils/TableGen/AsmWriterInst.cpp failed: undeclared inclusion(s) in rule '@llvm-project//llvm:tblgen':
this rule is missing dependency declarations for the following files included by 'llvm/utils/TableGen/AsmWriterInst.cpp':
  'bazel-out/k8-opt/bin/external/llvm-project/llvm/Demangle.cppmap'
  'bazel-out/k8-opt/bin/external/llvm_terminfo/terminfo.cppmap'
  'bazel-out/k8-opt/bin/external/llvm_zlib/zlib.cppmap'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 11.675s, Critical Path: 6.79s
INFO: 240 processes: 83 internal, 157 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
</details>"
60921,Some parameters are missing type descriptions,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

<html xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/pigpi/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/pigpi/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<style>
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
.font5
	{color:windowtext;
	font-size:9.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:等线;
	mso-generic-font-family:auto;
	mso-font-charset:134;}
tr
	{mso-height-source:auto;
	mso-ruby-visibility:none;}
col
	{mso-width-source:auto;
	mso-ruby-visibility:none;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:11.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:等线;
	mso-generic-font-family:auto;
	mso-font-charset:134;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
ruby
	{ruby-align:left;}
rt
	{color:windowtext;
	font-size:9.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:等线;
	mso-generic-font-family:auto;
	mso-font-charset:134;
	mso-char-type:none;
	display:none;}
-->
</style>
</head>

<body link=""#0563C1"" vlink=""#954F72"">



API | lack of type desciption params
-- | --
tf.sparse.bincount | values
tf.keras.layers.Input | tensor
tf.keras.applications.DenseNet121 | input_tensor
tf.math.add_n | inputs
tf.keras.applications.MobileNetV2 | input_tensor
tf.device | device_name
tf.keras.regularizers.get | identifier
tf.metrics.RootMeanSquaredError | metrics
tf.expand_dims | input
tf.keras.applications.DenseNet201 | input_tensor
tf.signal.frame | signal
tf.losses.mean_squared_error | y_true 、 y_pred
tf.losses.cosine_similarity | y_true 、 y_pred
tf.keras.metrics.binary_accuracy | y_true 、 y_pred
tf.keras.losses.logcosh | y_true 、 y_pred
tf.keras.applications.EfficientNetB5 | input_tensor
tf.metrics.binary_accuracy | y_true 、 y_pred
tf.data.experimental.assert_cardinality | expected_cardinality
tf.keras.metrics.sparse_top_k_categorical_accuracy | y_true 、 y_pred
tf.losses.squared_hinge | y_true 、 y_pred
tf.keras.utils.pack_x_y_sample_weight | x、y、sample_weight
tf.keras.activations.softplus | x
tf.nn.depth_to_space | input
tf.gather_nd | params
tf.zeros_like | input
tf.keras.applications.EfficientNetB2 | input_tensor
tf.keras.metrics.MeanAbsoluteError | metrics
tf.stack | values
tf.ensure_shape | x
tf.roll | input
tf.keras.activations.gelu | x
tf.linalg.set_diag | input、diagonal、k
tf.math.bincount | arr、weights
tf.concat | values
tf.keras.applications.EfficientNetB6 | input_tensor
tf.keras.losses.mean_squared_error | y_true 、 y_pred
tf.random.categorical | logits
tf.keras.backend.is_keras_tensor | x
tf.keras.activations.tanh | x
tf.pad | tensor
tf.keras.initializers.identity | gain
tf.keras.applications.EfficientNetB0 | input_tensor
tf.repeat | input
tf.image.convert_image_dtype | image
tf.split | value
tf.keras.utils.to_categorical | y
tf.scatter_nd | updates
tf.keras.layers.concatenate | input
tf.linalg.banded_triangular_solve | bands、rhs
tf.ones_like | input
tf.nest.flatten | structure
tf.keras.activations.linear | x
tf.linalg.tensor_diag_part | input
tf.image.pad_to_bounding_box | image
tf.config.set_visible_devices | devices
tf.size | input
tf.image.resize | images
tf.tile | input
tf.keras.applications.VGG16 | input_tensor
tf.keras.metrics.top_k_categorical_accuracy | y_true 、 y_pred
tf.initializers.identity | gain
tf.image.stateless_random_brightness | image
tf.ragged.range | starts、limits、deltas
tf.reshape | tensor
tf.losses.logcosh | y_true 、 y_pred
tf.keras.applications.ResNet50V2 | input_tensor
tf.nn.moments | x
tf.keras.applications.EfficientNetB4 | input_tensor
tf.image.adjust_jpeg_quality | image
tf.keras.losses.sparse_categorical_crossentropy | y_true 、 y_pred
tf.control_dependencies | control_inputs
tf.image.grayscale_to_rgb | images
tf.image.rgb_to_yiq | images
tf.keras.applications.EfficientNetB3 | input_tensor
tf.ragged.boolean_mask | data、mask
tf.image.random_hue | image
tf.image.adjust_gamma | image
tf.is_tensor | x
tf.keras.initializers.orthogonal | gain
tf.math.polyval | coeffs、x
tf.io.serialize_tensor | tensor
tf.image.stateless_random_saturation | image
tf.one_hot | indices
tf.linalg.diag_part | input、padding_value、
tf.image.adjust_saturation | image
tf.boolean_mask | tensor、mask
tf.transpose | a
tf.image.flip_up_down | image
tf.keras.losses.binary_crossentropy | y_true 、 y_pred
tf.broadcast_to | input
tf.image.stateless_random_crop | value
tf.losses.mean_absolute_percentage_error | y_true 、 y_pred
tf.image.stateless_random_flip_left_right | image
tf.image.random_flip_up_down | image
tf.keras.activations.exponential | x
tf.keras.applications.Xception | input_tensor
tf.identity | input
tf.gather | params
tf.keras.applications.InceptionV3 | input_tensor
tf.keras.layers.Masking | mask_value
tf.losses.kullback_leibler_divergence | y_true 、 y_pred
tf.linalg.band_part | input
tf.keras.losses.cosine_similarity | y_true 、 y_pred
tf.image.random_contrast | image
tf.image.transpose | image
tf.stop_gradient | input
tf.strings.bytes_split | input
tf.random.stateless_parameterized_truncated_normal | means、stddevs、minvals、maxvals
tf.keras.losses.mean_absolute_error | y_true 、 y_pred
tf.image.stateless_random_hue | image
tf.keras.applications.DenseNet169 | input_tensor
tf.keras.losses.categorical_crossentropy | y_true 、 y_pred
tf.nn.embedding_lookup | params
tf.math.reduce_variance | input_tensor
tf.keras.utils.unpack_x_y_sample_weight | data
tf.nn.l2_normalize | x
tf.keras.losses.categorical_hinge | y_true 、 y_pred
tf.keras.applications.EfficientNetB1 | input_tensor
tf.keras.constraints.get | identifier
tf.initializers.orthogonal | gain
tf.divide | x、y
tf.math.top_k | input
tf.keras.losses.kullback_leibler_divergence | y_true 、 y_pred
tf.image.stateless_random_jpeg_quality | image
tf.keras.losses.mean_absolute_percentage_error | y_true 、 y_pred
tf.keras.applications.EfficientNetB7 | input_tensor
tf.clip_by_value | t
tf.type_spec_from_value | value
tf.losses.mean_squared_logarithmic_error | y_true 、 y_pred
tf.tensor_scatter_nd_update | tensor、indices
tf.equal | x、y
tf.image.rgb_to_grayscale | images
tf.image.stateless_random_contrast | image
tf.image.rgb_to_hsv | images
tf.convert_to_tensor | value
tf.losses.sparse_categorical_crossentropy | y_true 、 y_pred
tf.keras.activations.sigmoid | x
tf.slice | input_
tf.image.adjust_hue | image
tf.math.argmax | input
tf.reverse_sequence | input
tf.losses.categorical_crossentropy | y_true 、 y_pred
tf.keras.losses.squared_hinge | y_true 、 y_pred
tf.squeeze | input
tf.math.equal | x、y
tf.math.divide | x、y
tf.unstack | value
tf.keras.applications.MobileNet | input_tensor
tf.keras.applications.ResNet152V2 | input_tensor
tf.keras.activations.softsign | x
tf.keras.applications.NASNetMobile | input_tensor
tf.keras.activations.swish | x
tf.metrics.categorical_accuracy | y_true 、 y_pred
tf.keras.metrics.sparse_categorical_accuracy | y_true 、 y_pred
tf.metrics.sparse_top_k_categorical_accuracy | y_true 、 y_pred
tf.losses.mean_absolute_error | y_true 、 y_pred
tf.losses.binary_crossentropy | y_true 、 y_pred
tf.keras.applications.ResNet50 | input_tensor
tf.image.random_jpeg_quality | image、min_jpeg_quality、max_jpeg_quality
tf.keras.activations.hard_sigmoid | x
tf.image.flip_left_right | image
tf.keras.applications.ResNet101V2 | input_tensor
tf.nn.batch_normalization | x、mean、variance、offset、scale
tf.math.reduce_min | input_tensor
tf.keras.applications.ResNet101 | input_tensor
tf.math.not_equal | x、y
tf.image.rot90 | image
tf.keras.applications.VGG19 | input_tensor
tf.image.stateless_random_flip_up_down | image
tf.keras.applications.MobileNetV3Large | input_tensor
tf.keras.applications.ResNet152 | input_tensor
tf.keras.metrics.categorical_accuracy | y_true 、 y_pred
tf.sparse.cross | inputs
tf.keras.losses.mean_squared_logarithmic_error | y_true 、 y_pred
tf.image.random_flip_left_right | image



</body>

</html>


### Standalone code to reproduce the issue

```shell
Many parameters are not for any type of value, so the allowed types should be clearly marked in the document.
```


### Relevant log output

_No response_</details>"
60920,F1 score error on multi class data,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

v1.12.1-95639-g08bd7e1a8e5 2.14.0-dev20230618

### Custom Code

Yes

### OS Platform and Distribution

OS Ventura 13.0.1

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Implementing the F1 score available in the nightly builds on multi-class data such as below:

```
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics= tf.keras.metrics.F1Score())

history = model.fit(train_images, train_labels, epochs=10, 
                    validation_data=(test_images, test_labels))
```

triggers the following error:

```
Epoch 1/10
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[8], line 5
      1 model.compile(optimizer='adam',
      2               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      3                 metrics= tf.keras.metrics.F1Score())
----> 5 history = model.fit(train_images, train_labels, epochs=10, 
      6                     validation_data=(test_images, test_labels))

File /opt/homebrew/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File /var/folders/f5/mkqkf_0d42qcsqc37hd_y0hm0000gn/T/__autograph_generated_fileb8tcgui2.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)
     13 try:
     14     do_return = True
---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16 except:
     17     do_return = False

ValueError: in user code:

    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1338, in train_function  *
        return step_function(self, iterator)
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1322, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1303, in run_step  **
        outputs = model.train_step(data)
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1085, in train_step
        return self.compute_metrics(x, y, y_pred, sample_weight)
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1179, in compute_metrics
        self.compiled_metrics.update_state(y, y_pred, sample_weight)
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/engine/compile_utils.py"", line 605, in update_state
        metric_obj.update_state(y_t, y_p, sample_weight=mask)
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/utils/metrics_utils.py"", line 77, in decorated
        update_op = update_state_fn(*args, **kwargs)
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/metrics/base_metric.py"", line 140, in update_state_fn
        return ag_update_state(*args, **kwargs)
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/metrics/f_score_metrics.py"", line 176, in update_state  **
        y_true = tf.convert_to_tensor(y_true, dtype=self.dtype)

    ValueError: Tensor conversion requested dtype float32 for Tensor with dtype uint8: <tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=uint8>
```

I've tried with multiple multi-class datasets and the same error is returned. The F1 score page says it should work with multi-class data https://www.tensorflow.org/api_docs/python/tf/keras/metrics/F1Score. Is there something I've missed regarding its implementation for multi-class data (such as somewhere to specify the number of classes?) or is this a bug?



### Standalone code to reproduce the issue

```shell
Here is a Jupyter notebook with some example data from https://www.tensorflow.org/tutorials/images/cnn

https://drive.google.com/file/d/1tExJ80AktA87EmsExOPEMWsevoiQz4VX/view?usp=share_link
```


### Relevant log output

_No response_</details>"
60918, NUMA and libcusolver.so.11 ISSUE,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.8.0

### Custom Code

Yes

### OS Platform and Distribution

Amazon Linux 2

### Mobile device

Other Linux x86_64

### Python version

3.9 

### Bazel version

4.2.1

### GCC/Compiler version

7.3.1

### CUDA/cuDNN version

11.2

### GPU model and memory

NVIDIA

### Current Behaviour?

I'm using a ready template launch from AMI catalog on Amazon, which is AWS Deep Learning AMI GPU TensorFlow 2.8.0 (Amazon Linux 2) Built with AWS optimized TensorFlow, NVIDIA CUDA, cuDNN, NCCL, GPU Driver, Docker, NVIDIA-Docker and EFA support. Platform: Other LinuxArchitecture: x86_64 Root device type: ebs Virtualization: hvm ENA enabled: Yes. and my application is dockerized which uses version tensorflow 2.8.0 

When i wanted to activate the GPU , I got errors, like :
**W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory**

I checked is this file exists , and it does exits. i have exported in the env to ensure it will work. but still the same issue. 

### Standalone code to reproduce the issue

```shell
I'm using ECS on AWS. so everytime i stop the task and launch again but the issue persists.
I'm using instance type g4dn x large
```


### Relevant log output

```shell
Skipping registering GPU devices...
2023-06-11 22:24:17.218609: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
2023-06-11 22:24:17.218523: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory
2023-06-11 22:24:17.217687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
```
</details>"
60917,Tensorboard histogram onehot operation causing ResourceExhauseError: OOM,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I'm trying to train a VGG16 model. I'm using a sample dataset of 4000 300x300 images in 14 classes, and running my code on a Google VM using an Nvidia L4 GPU with 20gb of memory. I am running python 3.7, tf version 2.11, and cuda version 12.1. My data is stored in GCS.

When I run the model with the following TensorBoard callback:

`tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)`

I get this error at the end of the first epoch:

```
2023-06-14 19:51:21.248476: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (mklcpu) ran out of memory trying to allocate 22.97GiB (rounded to 24662507520)requested by op OneHot
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 

ResourceExhaustedError: {{function_node _wrapped__OneHot_device/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[102760448,30] and type double on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:OneHot]

```
The error traces back to the tensorboard histogram object:
```

--------------------------------------------------------------------------
ResourceExhaustedError                    Traceback (most recent call last)
/var/tmp/ipykernel_5723/1753739100.py in <module>
      1 # Fit model
----> 2 history = model.fit(train_ds, validation_data=val_ds, epochs=5, callbacks=[tensorboard_callback])

/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---> 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

/opt/conda/lib/python3.7/site-packages/tensorboard/plugins/histogram/summary_v2.py in histogram(name, data, step, buckets, description)
    198             tensor=lazy_tensor,
    199             step=step,
--> 200             metadata=summary_metadata,
    201         )
    202 

/opt/conda/lib/python3.7/site-packages/tensorboard/util/lazy_tensor_creator.py in __call__(self)
     64                 elif self._tensor is None:
     65                     self._tensor = _CALL_IN_PROGRESS_SENTINEL
---> 66                     self._tensor = self._tensor_callable()
     67         return self._tensor
     68 

/opt/conda/lib/python3.7/site-packages/tensorboard/plugins/histogram/summary_v2.py in lazy_tensor()
    192         @lazy_tensor_creator.LazyTensorCreator
    193         def lazy_tensor():
--> 194             return _buckets(data, buckets)
    195 
    196         return tf.summary.write(

/opt/conda/lib/python3.7/site-packages/tensorboard/plugins/histogram/summary_v2.py in _buckets(data, bucket_count)
    291             )
    292 
--> 293         return tf.cond(is_empty, when_empty, when_nonempty)

/opt/conda/lib/python3.7/site-packages/tensorboard/plugins/histogram/summary_v2.py in when_nonempty()
    288 
    289             return tf.cond(
--> 290                 has_single_value, when_single_value, when_multiple_values
    291             )
    292 

/opt/conda/lib/python3.7/site-packages/tensorboard/plugins/histogram/summary_v2.py in when_multiple_values()
    257                 # See https://github.com/tensorflow/tensorflow/issues/51419 for details.
    258                 one_hots = tf.one_hot(
--> 259                     clamped_indices, depth=bucket_count, dtype=tf.float64
    260                 )
    261                 bucket_counts = tf.cast(

ResourceExhaustedError: {{function_node __wrapped__OneHot_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[102760448,30] and type double on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:OneHot]


```
Interestingly it seems to be calling tf.one_hot and blowing up the gpu memory with a massive tensor regardless of whether I train the model with integer labels and spare categorical cross entropy or if I train it with one hot labels and cross entropy. I don't really understand what the tensor contains because its dimensions neither relate to the number of training examples or classes that I am using.

### Standalone code to reproduce the issue

```shell
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)


I get this error at the end of the first epoch:

2023-06-14 19:51:21.248476: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (mklcpu) ran out of memory trying to allocate 22.97GiB (rounded to 24662507520)requested by op OneHot
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 

ResourceExhaustedError: {{function_node _wrapped__OneHot_device/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[102760448,30] and type double on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:OneHot]
```


### Relevant log output

_No response_</details>"
60916,Uncaught exception in ZMQStream callback when running your example notebooks using latest or nightly docker image,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom Code

No

### OS Platform and Distribution

Linux gpu02 6.2.11-2-pve #1 SMP PREEMPT_DYNAMIC PVE 6.2.11-2 (2023-05-10T09:13Z) x86_64 x86_64 x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

python3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Occurs when running any of your example notebooks:

```
[E 22:36:50.295 NotebookApp] Uncaught exception in ZMQStream callback
    Traceback (most recent call last):
      File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 584, in _run_callback
        f = callback(*args, **kwargs)
      File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 308, in stream_callback
        return callback(self, msg)
      File ""/usr/local/lib/python3.8/dist-packages/notebook/services/kernels/handlers.py"", line 572, in _on_zmq_reply
        super()._on_zmq_reply(stream, msg)
      File ""/usr/local/lib/python3.8/dist-packages/notebook/base/zmqhandlers.py"", line 256, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 339, in write_message
        return self.ws_connection.write_message(message, binary=binary)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 1086, in write_message
        fut = self._write_frame(True, opcode, message, flags=flags)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 1061, in _write_frame
        return self.stream.write(frame)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 546, in write
        self._handle_write()
      File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 976, in _handle_write
        self._write_buffer.advance(num_bytes)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 182, in advance
        assert 0 < size <= self._size
    AssertionError
[E 22:36:50.297 NotebookApp] Uncaught exception in zmqstream callback
    Traceback (most recent call last):
      File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 634, in _handle_events
        self._handle_recv()
      File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 663, in _handle_recv
        self._run_callback(callback, msg)
      File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 584, in _run_callback
        f = callback(*args, **kwargs)
      File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 308, in stream_callback
        return callback(self, msg)
      File ""/usr/local/lib/python3.8/dist-packages/notebook/services/kernels/handlers.py"", line 572, in _on_zmq_reply
        super()._on_zmq_reply(stream, msg)
      File ""/usr/local/lib/python3.8/dist-packages/notebook/base/zmqhandlers.py"", line 256, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 339, in write_message
        return self.ws_connection.write_message(message, binary=binary)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 1086, in write_message
        fut = self._write_frame(True, opcode, message, flags=flags)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 1061, in _write_frame
        return self.stream.write(frame)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 546, in write
        self._handle_write()
      File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 976, in _handle_write
        self._write_buffer.advance(num_bytes)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 182, in advance
        assert 0 < size <= self._size
    AssertionError
Exception in callback BaseAsyncIOLoop._handle_events(33, 1)
handle: <Handle BaseAsyncIOLoop._handle_events(33, 1)>
Traceback (most recent call last):
  File ""/usr/lib/python3.8/asyncio/events.py"", line 81, in _run
    self._context.run(self._callback, *self._args)
  File ""/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py"", line 206, in _handle_events
    handler_func(fileobj, events)
  File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 634, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 663, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 584, in _run_callback
    f = callback(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 308, in stream_callback
    return callback(self, msg)
  File ""/usr/local/lib/python3.8/dist-packages/notebook/services/kernels/handlers.py"", line 572, in _on_zmq_reply
    super()._on_zmq_reply(stream, msg)
  File ""/usr/local/lib/python3.8/dist-packages/notebook/base/zmqhandlers.py"", line 256, in _on_zmq_reply
    self.write_message(msg, binary=isinstance(msg, bytes))
  File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 339, in write_message
    return self.ws_connection.write_message(message, binary=binary)
  File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 1086, in write_message
    fut = self._write_frame(True, opcode, message, flags=flags)
  File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 1061, in _write_frame
    return self.stream.write(frame)
  File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 546, in write
    self._handle_write()
  File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 976, in _handle_write
    self._write_buffer.advance(num_bytes)
  File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 182, in advance
    assert 0 < size <= self._size
AssertionError
```

### Standalone code to reproduce the issue

```shell
Run any of your Jupyter example in your docker image.
```


### Relevant log output

_No response_</details>"
60915,Building tf-opt steps with prerequisites ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubunto 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.1.2

### GCC/Compiler version

9.2.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am trying to build tf-opt binary on branch v2.12 without any changes and gets different compilation errors. The command for compilation I use:
`bazel build -c opt tensorflow/compiler/mlir:tf-opt`

Can you share some prerequites for building and debugging `tf-opt` binary (for debug/release mode). I would appriciate if there is docker builder I can use to it instead of changing my envrioment.

Thanks,
Aviad

### Standalone code to reproduce the issue

```shell
ERROR: /localdrive/users/aviadco/community/tensorflow/tensorflow/lite/experimental/acceleration/configuration/BUILD:36:8: Executing genrule //tensorflow/lite/experimental/acceleration/configuration:configuration_schema failed: (Exit 1): bash failed: error executing command (from target //tensorflow/lite/experimental/acceleration/configuration:configuration_schema) /bin/bash -c ... (remaining 1 argument skipped)
bazel-out/k8-opt-exec-50AE0418/bin/external/flatbuffers/flatc: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by bazel-out/k8-opt-exec-50AE0418/bin/external/flatbuffers/flatc)
ERROR: /localdrive/users/aviadco/community/tensorflow/tensorflow/lite/schema/BUILD:184:22: Generating flatbuffer files for conversion_metadata_fbs_srcs: //tensorflow/lite/schema:conversion_metadata_fbs_srcs failed: (Exit 1): bash failed: error executing command (from target //tensorflow/lite/schema:conversion_metadata_fbs_srcs) /bin/bash -c ... (remaining 1 argument skipped)
bazel-out/k8-opt-exec-50AE0418/bin/external/flatbuffers/flatc: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by bazel-out/k8-opt-exec-50AE0418/bin/external/flatbuffers/flatc)
Target //tensorflow/compiler/mlir:tf-opt failed to build
```


### Relevant log output

_No response_</details>"
60913,Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0. - even in TF 2.12 and last update of cuda drivers,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8/8

### GPU model and memory

_No response_

### Current Behaviour?

Running a tutorial from Keras official blog with TF 2.12 and with updated drivers in Cuda, but :

```
2023-06-18 11:49:15.576607: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2023-06-18 11:49:15.578580: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_ops_fused_impl.h:625 : UNIMPLEMENTED: DNN library is not found.
2023-06-18 11:49:15.578631: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): UNIMPLEMENTED: DNN library is not found.
```

TF and cuda correctly installed as per :
https://www.tensorflow.org/install/pip

I tried to downgrade TF, but won't work because dependencies (tensorflow-probability and protobuf  will fail)



### Standalone code to reproduce the issue

```shell
Cannot reproduce a tutorial from official Keras blog:

https://keras.io/examples/generative/vq_vae/

2023-06-18 11:49:15.576607: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  

I could not solve this error, for TF is updated ( 2.12.0 ) and installed from pip as showed on
https://www.tensorflow.org/install/pip
, cuda as well ypdated. Don't know how to solve:


import tensorflow.python.platform.build_info as build
print(build.build_info)

OrderedDict([('cpu_compiler', '/dt9/usr/bin/gcc'), ('cuda_compute_capabilities', ['sm_35', 'sm_50', 'sm_60', 'sm_70', 'sm_75', 'compute_80']), ('cuda_version', '11.8'), ('cudnn_version', '8'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', True)])

```
```


### Relevant log output

```shell
2023-06-18 11:49:15.576607: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2023-06-18 11:49:15.578580: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_ops_fused_impl.h:625 : UNIMPLEMENTED: DNN library is not found.
2023-06-18 11:49:15.578631: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): UNIMPLEMENTED: DNN library is not found.
	 [[{{node vq_vae/encoder/conv2d_24/Relu}}]]

---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
Cell In[18], line 3
      1 vqvae_trainer = VQVAETrainer(data_variance, latent_dim=16, num_embeddings=128)
      2 vqvae_trainer.compile(optimizer=keras.optimizers.Adam())
----> 3 vqvae_trainer.fit(x_train_scaled, epochs=30, batch_size=128)

File /data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File /data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50 try:
     51   ctx.ensure_initialized()
---> 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                       inputs, attrs, num_outputs)
     54 except core._NotOkStatusException as e:
     55   if name is not None:

UnimplementedError: Graph execution error:

Detected at node 'vq_vae/encoder/conv2d_24/Relu' defined at (most recent call last):
    File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/ipykernel_launcher.py"", line 17, in <module>
      app.launch_new_instance()
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/traitlets/config/application.py"", line 1043, in launch_instance
      app.start()
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/ipykernel/kernelapp.py"", line 725, in start
      self.io_loop.start()
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/tornado/platform/asyncio.py"", line 195, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 600, in run_forever
      self._run_once()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1896, in _run_once
      handle._run()
    File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 513, in dispatch_queue
      await self.process_one()
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 502, in process_one
      await dispatch(*args)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 409, in dispatch_shell
      await result
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 729, in execute_request
      reply_content = await reply_content
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 422, in do_execute
      res = shell.run_cell(
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/ipykernel/zmqshell.py"", line 540, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3009, in run_cell
      result = self._run_cell(
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3064, in _run_cell
      result = runner(coro)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
      coro.send(None)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3269, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3448, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3508, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""/tmp/ipykernel_953075/2751485705.py"", line 3, in <module>
      vqvae_trainer.fit(x_train_scaled, epochs=30, batch_size=128)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/engine/training.py"", line 1685, in fit
      tmp_logs = self.train_function(iterator)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/engine/training.py"", line 1284, in train_function
      return step_function(self, iterator)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/engine/training.py"", line 1268, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/engine/training.py"", line 1249, in run_step
      outputs = model.train_step(data)
    File ""/tmp/ipykernel_953075/2697647477.py"", line 27, in train_step
      reconstructions = self.vqvae(x)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/engine/training.py"", line 558, in __call__
      return super().__call__(*args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/engine/base_layer.py"", line 1145, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/engine/functional.py"", line 512, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/engine/functional.py"", line 669, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/engine/training.py"", line 558, in __call__
      return super().__call__(*args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/engine/base_layer.py"", line 1145, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/engine/functional.py"", line 512, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/engine/functional.py"", line 669, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/engine/base_layer.py"", line 1145, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/layers/convolutional/base_conv.py"", line 321, in call
      return self.activation(outputs)
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/activations.py"", line 317, in relu
      return backend.relu(
    File ""/data0/home/h21/luas6629/dummy/lib/python3.10/site-packages/keras/backend.py"", line 5396, in relu
      x = tf.nn.relu(x)
Node: 'vq_vae/encoder/conv2d_24/Relu'
DNN library is not found.
	 [[{{node vq_vae/encoder/conv2d_24/Relu}}]] [Op:__inference_train_function_4158]
```
</details>"
60912,Add support for IO optimization,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Tensorflow's internal gcs filesystem's performance is really bad, a few optimizations we could so:
- Leverage multi-curl instead of easy-curl to avoid TCP connections created every single request;
- Explicitly use HTTP/2 instead of HTTP/1.1 for IO multiplexing;
- Leverage more compression algorithm, like LZ4, ZSTD, etc;
- Adopt zero-copy interface for input stream interface;
- Reduce unnecessary mem allocation in gcs filesystem.

### Standalone code to reproduce the issue

```shell
Left a few optimization suggestions.
```


### Relevant log output

_No response_</details>"
60911,martineza8779@gmail.com,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
60910,Martineza8779@gmail.com,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
60907,Typo in docs transfer_learning.ipynb,"It seems there is a typo in https://www.tensorflow.org/tutorials/images/transfer_learning

In the augmentation section there is a Note as follows:

**Typo**
`Note: These layers are active only during training, when you call [Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit). They are inactive when the model is used in inference mode in [Model.evaluate](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate) or [Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).`

If I understand correctly the **second** reference to `Model.fit` is not intended or correct and should be changed to `Model.call` as follows:

**Correction**
`Note: These layers are active only during training, when you call [Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit). They are inactive when the model is used in inference mode in [Model.evaluate](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate) or [Model.call](https://www.tensorflow.org/api_docs/python/tf/keras/Model#call).`

I'm willing to help with a pull request if confirmed."
60906,Errors when custom gradients are being used in TPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When writing custom gradient modules in TensorFlow (not using tf.GradientTape()) and applying it using an existing optimizer causes errors. I don't see any visible difference in tf.GradientShape() gradients and custom ones.

### Standalone code to reproduce the issue

```shell
with strategy.scope():
  with tf.GradientTape() as tape:
    model = ... #using any model
    loss = ... #using any loss function here
    loss_n = loss(y_batch, model(x_batch))

  grads = tape.gradient(loss_n, model.trainable_weights)
  new_grads = []
  
  for g in grads:
    new_grads += [tf.ones((tf.shape(g)))]
  
  optimizer.apply_gradients(zip(new_grads, model.trainable_weights)) #error here
```
```


### Relevant log output

```shell
Should be similar to below:


AttributeError                            Traceback (most recent call last)
<ipython-input-25-c9f1f22a039f> in <cell line: 1>()
     50       grads = tape.gradient(l, model.trainable_weights)
     51       c = grads[0]
---> 52       opt.apply_gradients(zip(grads, model.trainable_weights))
     53       print('Loss this batch: ' + closure().numpy())
     54       opt.next_steps()

2 frames
/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py in apply_gradients(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)
   1171         )
   1172         if not skip_gradients_aggregation and experimental_aggregate_gradients:
-> 1173             grads_and_vars = self.aggregate_gradients(grads_and_vars)
   1174         return super().apply_gradients(grads_and_vars, name=name)
   1175 

/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py in aggregate_gradients(self, grads_and_vars)
   1137           List of (gradient, variable) pairs.
   1138         """"""
-> 1139         return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)
   1140 
   1141     def apply_gradients(

/usr/local/lib/python3.10/dist-packages/keras/optimizers/utils.py in all_reduce_sum_gradients(grads_and_vars)
     40         else:
     41             # TODO(b/183257003): Remove this branch
---> 42             reduced = tf.distribute.get_replica_context().merge_call(
     43                 _all_reduce_sum_fn, args=(filtered_grads_and_vars,)
     44             )

AttributeError: 'NoneType' object has no attribute 'merge_call'
```
```
</details>"
60905,tf.keras.metrics.Precision treats label as binary?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0-dev20230611

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

NA

### Python version

3.11.3

### Bazel version

NA

### GCC/Compiler version

NA

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

tf.keras.metrics.Precision is returning precision assuming the label is binary? It looks so to me.

See:

```
In [4]: m = tf.keras.metrics.Precision()
   ...: m.update_state([0, 1, 2, 3], [0, 1, 2, 2])
   ...: m.result().numpy()
Out[4]: 1.0

In [5]: import tensorflow as tf

In [6]: m = tf.keras.metrics.Precision()
   ...: m.update_state([0, 1, 2, 3], [0, 1, 2, 2])
   ...: m.result().numpy()
Out[6]: 1.0

In [7]: tf.__version__
Out[7]: '2.14.0-dev20230611'

In [8]: m = tf.keras.metrics.Precision()
   ...: m.update_state([0, 5, 3, 3], [0, 1, 2, 2])
   ...: m.result().numpy()
Out[8]: 1.0
```

Above shouldn't be 1.0 if labels are treated as non binary.  It appears to me that 0s are treated as 0 while non zeros are treated as 1.

But nowhere in the doc mentions this behavior. I can't find categorical precision or similar either. Please update doc to explain this behavior.

### Standalone code to reproduce the issue

```shell
See above.
```


### Relevant log output

_No response_</details>"
60903,lossing too much accuracy,"I have a model composed of some tf.keras.layers.Conv1D and custom Upsampling1DLayer and CustomCropping1D layer:
The model produces correct results after training but when I convert it to tflite with int8 quantization, my accuracy drops by more than 50%.

I am unable to understand why is that? and how can I avoid this much loss.

def tflite_conversion(model):
    run_model = tf.function(lambda x: model(x))
    concrete_func = run_model.get_concrete_function(tf.TensorSpec([1,6000,3], model.inputs[0].dtype))
    MODEL_DIR = ""keras_lstm""
    model.save(MODEL_DIR, save_format=""tf"", signatures=concrete_func)

    converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
    Choice=""_int8""
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.inference_input_type = tf.int8  # or tf.uint8
    converter.inference_output_type = tf.int8  # or tf.uint8
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,tf.lite.OpsSet.TFLITE_BUILTINS]
    #converter.experimental_select_user_tf_ops = [Upscaling1D]

    def generate_representative_dataset():
        for i in range(int(x_train.shape[0]/100)):
            print(i,end=""\r"")
            yield [tf.expand_dims(x_train[i], axis=0)]

    converter.representative_dataset = generate_representative_dataset

    tflite_model = converter.convert()
    open(""keras_lstm/model""+Choice+"".tflite"", ""wb"").write(tflite_model)
    return tflite_model"
60902,"Flutter - ""Select Tensorflow Ops"" not working","**System information**
- OS Platform and Distribution : Windows 11
- Flutter version : 3.7.12
- TensorFlow installed from (source or binary): tflite_flutter 0.10.1 (https://pub.dev/packages/tflite_flutter)
- TensorFlow version (or github SHA if from source): 2.4.1 (implementation 'org.tensorflow:tensorflow-lite:2.4.1') --> in build.gradle

Hello,

I try to implement Google Android autocomplete project (https://github.com/tensorflow/examples/tree/master/lite/examples/generative_ai/android) on **flutter**. 
Here is the detailed project implementation website(https://codelabs.developers.google.com/kerasnlp-tflite#0
) for reference.

### I created .tflite file using below given codes :

@tf.function
def generate(prompt, max_length):
    return gpt2_lm.generate(prompt, max_length)

concrete_func = generate.get_concrete_function(tf.TensorSpec([], tf.string), 100)

gpt2_lm.jit_compile = False
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func],
                                                            gpt2_lm)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
converter.allow_custom_ops = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.experimental_select_user_tf_ops = [""UnsortedSegmentJoin"", ""UpperBound""]
converter._experimental_guarantee_all_funcs_one_use = True
quant_generate_tflite = converter.convert()

### Then I tried to implement generated .tflite model in flutter using **tflite_flutter** package as below(focussed) :

import 'package:tflite_flutter/tflite_flutter.dart';

final String MODEL_PATH = 'assets/autocomplete.tflite';

void loadModel() async {
    try {
      print('Loading model...');
      _interpreter = await Interpreter.fromAsset(MODEL_PATH);
      print('Model loaded');
    } on Exception catch (e) {
      print('Error while loading model: $e');
    }
  }

### On build.gradle file in android folder, I made some arrangements as below :

  aaptOptions {
        noCompress 'tflite'
        noCompress 'lite'
    }

dependencies {
        implementation 'org.tensorflow:tensorflow-lite:2.4.1'
        implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.12.0'
}

### During debugging, I get this error message :

E/tflite  (31048): Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
E/tflite  (31048): Node number 2 (FlexMutableHashTableV2) failed to prepare.

After a bit of investigation and having a double-check on android example application by google, I realized that there are libraries already builded with an .aar file extension. Then I copied android version of this file (https://storage.googleapis.com/download.tensorflow.org/models/tflite/generativeai/tensorflow-lite-select-tf-ops.aar) into my flutter android folder under app/libs. I also updated my build.gradle dependencies by adding implementation ""(fileTree(dir: ""libs"", include: [""*.aar""]))"".

### Program UI starts without any issue, but still I get this ""Select TensorFlow op(s) error : 

E/tflite  (31048): Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select

Could you please support me on this topic ? Maybe there is a missing function for interpreter in tflite_flutter package compared to native android ones. 
If necessary, I can share my flutter project.

Thanks in advance.
Gorkem
"
60901,TF 2 building from source fails,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

Commit hash: cd8247b47bd5ba25b26752b55ea54b52e5574183

### Custom Code

No

### OS Platform and Distribution

Redhat Enterprise Linux 8.7

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

5.4.0 (same as what's there in .bazelversion)

### GCC/Compiler version

clang 17.0.0

### CUDA/cuDNN version

CUDA 12.1, cuDNN 8.9.0

### GPU model and memory

H100

### Current Behaviour?

Hello,

I'm unable to build TF from source on CUDA 12.1. It tries to download a bunch of things, but that's failing with the attached error.

Please let me know what I can do to build it correctly.


Thanks in advance.

### Standalone code to reproduce the issue

```shell
* ./configure (and provide details)
* bazel --output_user_root=/var/tmp/bazel_cache/ build //tensorflow/tools/pip_package:build_pip_package --config=v2 --config=nogcp --verbose_failures
```


### Relevant log output

```shell
Build fails with the following:


WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/8ed9cf06e9004931e3e583a79579f3286e8d027c.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target                                                                                                                                                                                                               
WARNING: Download from https://github.com/llvm/llvm-project/archive/8ed9cf06e9004931e3e583a79579f3286e8d027c.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target                                                                                                                                                                                                                                                            
ERROR: An error occurred during the fetch of repository 'llvm-raw':                                                                                                                                                                                                                       
   Traceback (most recent call last):                                                                                                                                                                                                                                                     
        File ""/var/tmp/tensorflow/third_party/repo.bzl"", line 73, column 33, in _tf_http_archive_impl                                                                                                                                                                         
                ctx.download_and_extract(                                                                                                                                                                                                                                                 
Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/8ed9cf06e9004931e3e583a79579f3286e8d027c.tar.gz, https://github.com/llvm/llvm-project/archive/8ed9cf06e9004931e3e583a79579f3286e8d027c.tar.gz] to /var/tmp/bazel_cache/3f833e193dcf224ba1dfd4cfcb9a9327/external/llvm-raw/temp9475963496497572483/8ed9cf06e9004931e3e583a79579f3286e8d027c.tar.gz: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target                                                                                                                                                                                                                                            
ERROR: /var/tmp/tensorflow/WORKSPACE:11:14: fetching _tf_http_archive rule //external:llvm-raw: Traceback (most recent call last):                                                                                                                                            
        File ""/var/tmp/tensorflow/third_party/repo.bzl"", line 73, column 33, in _tf_http_archive_impl                                                                                                                                                                         
                ctx.download_and_extract(                                                                                                                                                                                                                                                 
Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/8ed9cf06e9004931e3e583a79579f3286e8d027c.tar.gz, https://github.com/llvm/llvm-project/archive/8ed9cf06e9004931e3e583a79579f3286e8d027c.tar.gz] to /var/tmp/bazel_cache/3f833e193dcf224ba1dfd4cfcb9a9327/external/llvm-raw/temp9475963496497572483/8ed9cf06e9004931e3e583a79579f3286e8d027c.tar.gz: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target                                                                                                                                                                                                                                            
```
```
</details>"
60900,tf.test.gpu_device_name() leads to soft lockup and unusable system,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.16 (Conda 11.4)

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.4

### GPU model and memory

NVIDIA T4

### Current Behaviour?

We found that running `tf.test.gpu_device_name()` leads to a soft lockup and an unresponsive system.

I apologise in advance, I don't know much about Tensorflow. But several of my co-workers do and I'm in charge of maintaining infrastructure. We have a virtual server for compute-intensive tasks where we train models and work with large datasets. Yesterday at about 7 local time I found that I couldn't SSH into the server so I had to wait for IT to forcibly restart the server. At around 13 local time the server was back up and I looked through the kernel logs to find that there was a soft lockup kernel bug. This means that the server was still running, but some process wasn't releasing the CPU for more than 20 seconds, which meant that no other process could fulfill its tasks.

Not even half an hour later and the server locked up again. I got a message from a co-worker who suspected that they were at fault for the server locking up because they started running a very compute-intensive Python script on the server last night. They opened a Python shell and typed in these two lines and watched the server lock up in real time.

```py
import tensorflow as tf 
print('Default GPU Device{}'.format(tf.test.gpu_device_name()))
```

I provided the relevant log output below. It required another hard reset from IT to get the server back into a working state.

After the server restarted again I had another look at the kernel logs and found that the reported errors were the same, which means I have reason to believe that running `tf.test.gpu_device_name()` consistently leads to a soft lockup on our infrastructure.

As mentioned previously, we run our computations on a virtual server. The hypervisor is VMWare. We're running on a Nvidia T4 GPU with Nvidia drivers in version 470.63.01. These drivers were provided to us by IT. Installing another version, for some reason, isn't possible and I prefer not to mess with that. The co-worker installed Tensorflow in a Conda environment using Pip.

Since we're running on a virtual server, we depend on IT in case things go very wrong. This means our means of reproducing this issue are somewhat limited. If the server locks up, we have to wait up to a couple hours for our service desk to escalate this issue enough so that IT restarts the server. We cannot afford downtimes like these since we rely on the server for our computations. I'm happy to provide any logs from previous runs but if possible I would like to not share them on GitHub in their entirety since they might contain confidential info.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print('Default GPU Device {}'.format(tf.test.gpu_device_name()))
```


### Relevant log output

```shell
# Output from running the function in a Python shell

$ python3
Python 3.8.16 (default, Mar  2 2023, 03:21:46) 
[GCC 11.2.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
import tensorflow as tf 
2023-06-15 13:34:49.940226: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-06-15 13:34:50.367115: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-06-15 13:34:50.368678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-15 13:34:52.102738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
>>> print('Default GPU Device{}'.format(tf.test.gpu_device_name()))
2023-06-15 13:35:05.586532: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-15 13:35:05.587485: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...

Message from syslogd@s050009088 at Jun 15 13:35:32 ...
 kernel:[  176.957689] watchdog: BUG: soft lockup - CPU#6 stuck for 22s! [python3:4167]


# Output from journalctl

Jun 15 13:35:05 s050009088 kernel: nvidia-uvm: Loaded the UVM driver, major device number 238.
Jun 15 13:35:05 s050009088 kernel: ------------[ cut here ]------------
Jun 15 13:35:05 s050009088 kernel: Trying to vfree() nonexistent vm area (00000000f9521180)
Jun 15 13:35:05 s050009088 kernel: WARNING: CPU: 6 PID: 4167 at mm/vmalloc.c:2245 __vunmap+0x1ff/0x210
Jun 15 13:35:05 s050009088 kernel: Modules linked in: nvidia_uvm(OE) veth xt_nat xt_tcpudp xt_conntrack xt_MASQUERADE nf_conntrack_netlink nfnetlink xfrm_user xfrm_algo iptable_nat nf_nat nf_conntrack nf_defrag_ipv6 nf_defrag_ipv4 xt_addrtype iptable_filter bpfilter br_netfilter bridge stp llc aufs overlay vmw_vsock_vmci_transport vsock dm_multipath scsi_dh_rdac scsi_dh_emc scsi_dh_alua binfmt_misc nvidia_drm(POE) nvidia_modeset(POE) intel_rapl_msr nvidia(POE) vmw_balloon intel_rapl_common input_leds intel_powerclamp joydev rapl serio_raw vmw_vmci mac_hid sch_fq_codel msr ramoops reed_solomon efi_pstore ip_tables x_tables autofs4 btrfs zstd_compress raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear vmwgfx crct10dif_pclmul crc32_pclmul ttm ghash_clmulni_intel drm_kms_helper syscopyarea aesni_intel sysfillrect crypto_simd sysimgblt mptspi cryptd fb_sys_fops glue_helper psmouse mptscsih drm mptbase ahci i2c_piix4 vmxnet3 scsi_transport_spi
Jun 15 13:35:05 s050009088 kernel:  libahci pata_acpi
Jun 15 13:35:05 s050009088 kernel: CPU: 6 PID: 4167 Comm: python3 Tainted: P           OE     5.4.0-150-generic #167-Ubuntu
Jun 15 13:35:05 s050009088 kernel: Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 11/12/2020
Jun 15 13:35:05 s050009088 kernel: RIP: 0010:__vunmap+0x1ff/0x210
Jun 15 13:35:05 s050009088 kernel: Code: ff e8 d5 fc ff ff eb bf 48 89 fe 48 c7 c7 a0 6d b8 8e e8 36 ba 82 00 0f 0b eb b4 4c 89 ee 48 c7 c7 c8 6d b8 8e e8 23 ba 82 00 <0f> 0b eb a1 66 66 2e 0f 1f 84 00 00 00 00 00 66 90 0f 1f 44 00 00
Jun 15 13:35:05 s050009088 kernel: RSP: 0018:ffffab66c32b7c00 EFLAGS: 00010286
Jun 15 13:35:05 s050009088 kernel: RAX: 0000000000000000 RBX: 0000000000000001 RCX: 0000000000000006
Jun 15 13:35:05 s050009088 kernel: RDX: 0000000000000007 RSI: 0000000000000096 RDI: ffff8ce3bfb9c8c0
Jun 15 13:35:05 s050009088 kernel: RBP: ffffab66c32b7c28 R08: 00000000000006d2 R09: 286565726676206f
Jun 15 13:35:05 s050009088 kernel: R10: 286565726676206f R11: 6978656e6f6e2029 R12: 0000000000000000
Jun 15 13:35:05 s050009088 kernel: R13: ffff8ce22c348000 R14: ffff8ce22c349000 R15: 0000000000000027
Jun 15 13:35:05 s050009088 kernel: FS:  00007fbf935e3180(0000) GS:ffff8ce3bfb80000(0000) knlGS:0000000000000000
Jun 15 13:35:05 s050009088 kernel: CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
Jun 15 13:35:05 s050009088 kernel: CR2: 000000000435c3d0 CR3: 0000001e50400001 CR4: 00000000003606e0
Jun 15 13:35:05 s050009088 kernel: Call Trace:
Jun 15 13:35:05 s050009088 kernel:  __vfree+0x22/0x60
Jun 15 13:35:05 s050009088 kernel:  vfree+0x2c/0x40
Jun 15 13:35:05 s050009088 kernel:  os_free_mem+0x1b/0x30 [nvidia]
Jun 15 13:35:05 s050009088 kernel:  os_unlock_user_pages+0x6c/0xa0 [nvidia]
Jun 15 13:35:05 s050009088 kernel:  _nv000647rm+0xdc/0x160 [nvidia]
Jun 15 13:35:05 s050009088 kernel: WARNING: kernel stack frame pointer at 0000000068b18731 in python3:4167 has bad value 00000000f3874266
Jun 15 13:35:05 s050009088 kernel: unwind stack type:0 next_sp:0000000000000000 mask:0x2 graph_idx:0
Jun 15 13:35:05 s050009088 kernel: 00000000c0e190e6: ffffab66c32b7c40 (0xffffab66c32b7c40)
Jun 15 13:35:05 s050009088 kernel: 0000000074c2bd79: ffffffff8da6e972 (__vfree+0x22/0x60)
Jun 15 13:35:05 s050009088 kernel: 00000000ea3bef41: 0000000000000200 (0x200)
Jun 15 13:35:05 s050009088 kernel: 00000000d32e6150: ffffab66c32b7c58 (0xffffab66c32b7c58)
Jun 15 13:35:05 s050009088 kernel: 00000000e4c2b257: ffffffff8da6e9dc (vfree+0x2c/0x40)
Jun 15 13:35:05 s050009088 kernel: 000000008b622c26: ffff8ce22c348000 (0xffff8ce22c348000)
Jun 15 13:35:05 s050009088 kernel: 00000000b443bda0: ffffab66c32b7c68 (0xffffab66c32b7c68)
Jun 15 13:35:05 s050009088 kernel: 0000000004f89fc7: ffffffffc068ccdb (os_free_mem+0x1b/0x30 [nvidia])
Jun 15 13:35:05 s050009088 kernel: 0000000058382150: ffffab66c32b7c98 (0xffffab66c32b7c98)
Jun 15 13:35:05 s050009088 kernel: 000000007fb1b873: ffffffffc068edfc (os_unlock_user_pages+0x6c/0xa0 [nvidia])
Jun 15 13:35:05 s050009088 kernel: 00000000629b0f28: ffff8ce395ff60c8 (0xffff8ce395ff60c8)
Jun 15 13:35:05 s050009088 kernel: 000000003a4ac7ef: 0000010004400000 (0x10004400000)
Jun 15 13:35:05 s050009088 kernel: 000000005e9a76c9: 0000000000000200 (0x200)
Jun 15 13:35:05 s050009088 kernel: 000000005fe4a88b: 00000000aa000000 (0xaa000000)
Jun 15 13:35:05 s050009088 kernel: 0000000068b18731: ffff8ce3b3e92fb0 (0xffff8ce3b3e92fb0)
Jun 15 13:35:05 s050009088 kernel: 00000000bd345bcf: ffffffffc101e8dc (_nv000647rm+0xdc/0x160 [nvidia])
Jun 15 13:35:05 s050009088 kernel: 0000000089f90db7: ffff8ce395ff60c8 (0xffff8ce395ff60c8)
Jun 15 13:35:05 s050009088 kernel: 00000000c99c1977: ffff8ce39726c0c0 (0xffff8ce39726c0c0)
Jun 15 13:35:05 s050009088 kernel: 000000009060a662: 0000000000000008 (0x8)
Jun 15 13:35:05 s050009088 kernel: 0000000043ee1352: ffffffffc101f3a3 (_nv000723rm+0xa43/0xa90 [nvidia])
Jun 15 13:35:05 s050009088 kernel: 0000000064b64c73: 0000000000000000 ...
Jun 15 13:35:05 s050009088 kernel: 0000000030c878cd: ffff8ce3b35bc000 (0xffff8ce3b35bc000)
Jun 15 13:35:05 s050009088 kernel: 00000000e60fac70: ffffffffc101f316 (_nv000723rm+0x9b6/0xa90 [nvidia])
Jun 15 13:35:05 s050009088 kernel: 0000000065747b0c: ffff8ce3b3e90000 (0xffff8ce3b3e90000)
Jun 15 13:35:05 s050009088 kernel: 0000000063da2df1: ffff8ce3a658b800 (0xffff8ce3a658b800)
Jun 15 13:35:05 s050009088 kernel: 0000000008c0ac4e: ffffab66c32b7e48 (0xffffab66c32b7e48)
Jun 15 13:35:05 s050009088 kernel: 0000000057ee515b: ffff8ce3b35b9400 (0xffff8ce3b35b9400)
Jun 15 13:35:05 s050009088 kernel: 000000006c5468b7: 0000000000000027 (0x27)
Jun 15 13:35:05 s050009088 kernel: 0000000065281df3: ffffffffc1025204 (rm_ioctl+0x54/0xb0 [nvidia])
Jun 15 13:35:05 s050009088 kernel: 00000000b419e233: 000000388da5fdb5 (0x388da5fdb5)
Jun 15 13:35:05 s050009088 kernel: 000000006f1b29a3: ffff8ce39726c0c0 (0xffff8ce39726c0c0)
Jun 15 13:35:05 s050009088 kernel: 00000000465f1196: 8000000000000027 (0x8000000000000027)
Jun 15 13:35:05 s050009088 kernel: 0000000024eadd83: 0000000000001047 (0x1047)
Jun 15 13:35:05 s050009088 kernel: 00000000e74dc1ab: 00000000000007e9 (0x7e9)
Jun 15 13:35:05 s050009088 kernel: 00000000f6d262ce: 003d08dcf6746300 (0x3d08dcf6746300)
Jun 15 13:35:05 s050009088 kernel: 00000000400760c5: 003d08dde4df8b00 (0x3d08dde4df8b00)
Jun 15 13:35:05 s050009088 kernel: 00000000c7874ca7: 003d08e3f2980f00 (0x3d08e3f2980f00)
Jun 15 13:35:05 s050009088 kernel: 00000000946d2608: 003d08dd6da9f700 (0x3d08dd6da9f700)
Jun 15 13:35:05 s050009088 kernel: 000000007a9cecb7: 0000000000000000 ...
Jun 15 13:35:05 s050009088 kernel: 000000000e6b2400: 0000000000000200 (0x200)
Jun 15 13:35:05 s050009088 kernel: 0000000022df682d: 0000002000000006 (0x2000000006)
Jun 15 13:35:05 s050009088 kernel: 0000000005972aee: 0000000000001047 (0x1047)
Jun 15 13:35:05 s050009088 kernel: 00000000f3ab8670: 0000000000000000 ...
Jun 15 13:35:05 s050009088 kernel: 0000000043395ee6: 0000000000000001 (0x1)
Jun 15 13:35:05 s050009088 kernel: 0000000090b81e91: 0000000000000000 ...
Jun 15 13:35:05 s050009088 kernel: 00000000ce75a231: ffff8ce39726c0c0 (0xffff8ce39726c0c0)
Jun 15 13:35:05 s050009088 kernel: 00000000336aa9c4: 0000000000000038 (0x38)
Jun 15 13:35:05 s050009088 kernel: 0000000016e174a9: ffff8ce3b35b9400 (0xffff8ce3b35b9400)
Jun 15 13:35:05 s050009088 kernel: 00000000c453e6ee: ffff8ce3a658b800 (0xffff8ce3a658b800)
Jun 15 13:35:05 s050009088 kernel: 00000000ca8fc06a: 0000000000000000 ...
Jun 15 13:35:05 s050009088 kernel: 0000000027388a61: ffffffffc068268f (nvidia_ioctl+0x66f/0x880 [nvidia])
Jun 15 13:35:05 s050009088 kernel: 000000001f100b3c: ffff8ce3b3e90000 (0xffff8ce3b3e90000)
Jun 15 13:35:05 s050009088 kernel: 00000000d70e2948: 00007ffd116cef60 (0x7ffd116cef60)
Jun 15 13:35:05 s050009088 kernel: 000000009e15ae1f: ffff8ce3b35b9498 (0xffff8ce3b35b9498)
Jun 15 13:35:05 s050009088 kernel: 000000002d53d487: 00007ffd00000027 (0x7ffd00000027)
Jun 15 13:35:05 s050009088 kernel: 000000002ea37e99: 0000000000000000 ...
Jun 15 13:35:05 s050009088 kernel: 00000000723c86e7: 0000000000000003 (0x3)
Jun 15 13:35:05 s050009088 kernel: 000000004b070af4: 3c5520dc9b31e900 (0x3c5520dc9b31e900)
Jun 15 13:35:05 s050009088 kernel: 00000000260adce7: ffff8ce3a37b1e01 (0xffff8ce3a37b1e01)
Jun 15 13:35:05 s050009088 kernel: 00000000f3ab64dd: ffff8ce3a37b1e00 (0xffff8ce3a37b1e00)
Jun 15 13:35:05 s050009088 kernel: 000000009250db0d: ffff8ce3a46feaa0 (0xffff8ce3a46feaa0)
Jun 15 13:35:05 s050009088 kernel: 000000001a439626: 00007ffd116cef60 (0x7ffd116cef60)
Jun 15 13:35:05 s050009088 kernel: 00000000d28c869f: ffff8ce3a37b1e00 (0xffff8ce3a37b1e00)
Jun 15 13:35:05 s050009088 kernel: 00000000264cc6e1: ffffab66c32b7e58 (0xffffab66c32b7e58)
Jun 15 13:35:05 s050009088 kernel: 00000000c9cdb9ef: ffffffffc069191b (nvidia_frontend_unlocked_ioctl+0x3b/0x50 [nvidia])
Jun 15 13:35:05 s050009088 kernel: 00000000dc15a7de: ffffab66c32b7ed8 (0xffffab66c32b7ed8)
Jun 15 13:35:05 s050009088 kernel: 000000009b8c3053: ffffffff8dae8e77 (do_vfs_ioctl+0x407/0x670)
Jun 15 13:35:05 s050009088 kernel: 00000000f2d02675: 0000010004400000 (0x10004400000)
Jun 15 13:35:05 s050009088 kernel: 00000000c52e1241: 0000000000000000 ...
Jun 15 13:35:05 s050009088 kernel: 0000000027ba7e88: ffffab66c32b7e78 (0xffffab66c32b7e78)
Jun 15 13:35:05 s050009088 kernel: 00000000aaf3e429: ffffab66c32b7e78 (0xffffab66c32b7e78)
Jun 15 13:35:05 s050009088 kernel: 00000000fd0b1f6d: 3c5520dc9b31e900 (0x3c5520dc9b31e900)
Jun 15 13:35:05 s050009088 kernel: 0000000051d890d1: 0000000000000031 (0x31)
Jun 15 13:35:05 s050009088 kernel: 000000001e5b54d6: 0000000000200000 (0x200000)
Jun 15 13:35:05 s050009088 kernel: 00000000bf7d038a: 0000000000000000 ...
Jun 15 13:35:05 s050009088 kernel: 00000000a021b6f5: 3c5520dc9b31e900 (0x3c5520dc9b31e900)
Jun 15 13:35:05 s050009088 kernel: 00000000382d5bbd: ffff8ce3a37b1e01 (0xffff8ce3a37b1e01)
Jun 15 13:35:05 s050009088 kernel: 000000005691996c: 0000000000000006 (0x6)
Jun 15 13:35:05 s050009088 kernel: 000000002b0a9fae: 00000000c0384627 (0xc0384627)
Jun 15 13:35:05 s050009088 kernel: 00000000ed0791ed: 00007ffd116cef60 (0x7ffd116cef60)
Jun 15 13:35:05 s050009088 kernel: 00000000944e63ee: ffff8ce3a37b1e00 (0xffff8ce3a37b1e00)
Jun 15 13:35:05 s050009088 kernel: 00000000b8133399: ffffab66c32b7f18 (0xffffab66c32b7f18)
Jun 15 13:35:05 s050009088 kernel: 000000006b019d54: ffffffff8dae9147 (ksys_ioctl+0x67/0x90)
Jun 15 13:35:05 s050009088 kernel: 0000000025959ce7: 3c5520dc9b31e900 (0x3c5520dc9b31e900)
Jun 15 13:35:05 s050009088 kernel: 0000000019e92101: 0000000000000000 ...
Jun 15 13:35:05 s050009088 kernel: 000000002801fc84: ffffab66c32b7f58 (0xffffab66c32b7f58)
Jun 15 13:35:05 s050009088 kernel: 0000000060185eb1: 0000000000000000 ...
Jun 15 13:35:05 s050009088 kernel: 000000007c1db1b7: ffffab66c32b7f28 (0xffffab66c32b7f28)
Jun 15 13:35:05 s050009088 kernel: 0000000084b8d84e: ffffffff8dae918a (__x64_sys_ioctl+0x1a/0x20)
Jun 15 13:35:05 s050009088 kernel: 000000009b8e8f5f: ffffab66c32b7f48 (0xffffab66c32b7f48)
Jun 15 13:35:05 s050009088 kernel: 00000000f2da7b14: ffffffff8d804fd7 (do_syscall_64+0x57/0x190)
Jun 15 13:35:05 s050009088 kernel: 00000000942f1f70: 0000000000000000 ...
Jun 15 13:35:05 s050009088 kernel: 00000000f56a3750: ffffffff8e4000a4 (entry_SYSCALL_64_after_hwframe+0x5c/0xc1)
Jun 15 13:35:05 s050009088 kernel: 0000000018f467fe: 00007ffd116ceed0 (0x7ffd116ceed0)
Jun 15 13:35:05 s050009088 kernel: 00000000831b89bf: 00000000648af769 (0x648af769)
Jun 15 13:35:05 s050009088 kernel: 000000000597aba6: 00007ffd116cef88 (0x7ffd116cef88)
Jun 15 13:35:05 s050009088 kernel: 00000000a139f77e: 0000000000000006 (0x6)
Jun 15 13:35:05 s050009088 kernel: 000000006612db4e: 00000000c0384627 (0xc0384627)
Jun 15 13:35:05 s050009088 kernel: 00000000ab55bfe9: 00007ffd116cef60 (0x7ffd116cef60)
Jun 15 13:35:05 s050009088 kernel: 000000000b644430: 0000000000000246 (0x246)
Jun 15 13:35:05 s050009088 kernel: 00000000d2d8779b: 0000000000000000 ...
Jun 15 13:35:05 s050009088 kernel: 00000000725ab6b6: 00007ffd116cef88 (0x7ffd116cef88)
Jun 15 13:35:05 s050009088 kernel: 000000000399d500: 00007ffd116cef60 (0x7ffd116cef60)
Jun 15 13:35:05 s050009088 kernel: 00000000c0a877c1: ffffffffffffffda (0xffffffffffffffda)
Jun 15 13:35:05 s050009088 kernel: 00000000fb645881: 00007fbf936f83ab (0x7fbf936f83ab)
Jun 15 13:35:05 s050009088 kernel: 00000000f6cd2602: 00007ffd116cef60 (0x7ffd116cef60)
Jun 15 13:35:05 s050009088 kernel: 000000006fc36d91: 00000000c0384627 (0xc0384627)
Jun 15 13:35:05 s050009088 kernel: 00000000e5058b78: 0000000000000006 (0x6)
Jun 15 13:35:05 s050009088 kernel: 000000002777ac42: 0000000000000010 (0x10)
Jun 15 13:35:05 s050009088 kernel: 00000000d68cb82a: 00007fbf936f83ab (0x7fbf936f83ab)
Jun 15 13:35:05 s050009088 kernel: 0000000077e7b064: 0000000000000033 (0x33)
Jun 15 13:35:05 s050009088 kernel: 0000000037ef3830: 0000000000000246 (0x246)
Jun 15 13:35:05 s050009088 kernel: 00000000fcba1669: 00007ffd116ceec8 (0x7ffd116ceec8)
Jun 15 13:35:05 s050009088 kernel: 00000000ea0caaa1: 000000000000002b (0x2b)
Jun 15 13:35:05 s050009088 kernel:  ? _nv000723rm+0xa43/0xa90 [nvidia]
Jun 15 13:35:05 s050009088 kernel:  ? _nv000723rm+0x9b6/0xa90 [nvidia]
Jun 15 13:35:05 s050009088 kernel:  ? rm_ioctl+0x54/0xb0 [nvidia]
Jun 15 13:35:05 s050009088 kernel:  ? nvidia_ioctl+0x66f/0x880 [nvidia]
Jun 15 13:35:05 s050009088 kernel:  ? nvidia_frontend_unlocked_ioctl+0x3b/0x50 [nvidia]
Jun 15 13:35:05 s050009088 kernel:  ? do_vfs_ioctl+0x407/0x670
Jun 15 13:35:05 s050009088 kernel:  ? ksys_ioctl+0x67/0x90
Jun 15 13:35:05 s050009088 kernel:  ? __x64_sys_ioctl+0x1a/0x20
Jun 15 13:35:05 s050009088 kernel:  ? do_syscall_64+0x57/0x190
Jun 15 13:35:05 s050009088 kernel:  ? entry_SYSCALL_64_after_hwframe+0x5c/0xc1
Jun 15 13:35:05 s050009088 kernel: ---[ end trace be1a4a9ea080b7b8 ]---

## The following logs are then repeatedly printed, leading to the soft lockup

Jun 15 13:35:05 s050009088 kernel: BUG: Bad page state in process python3  pfn:1e301a6
Jun 15 13:35:05 s050009088 kernel: page:ffffd052f8c06980 refcount:0 mapcount:0 mapping:ffff8ce230f6e0d8 index:0x1
Jun 15 13:35:05 s050009088 kernel: shmem_aops name:""dev/zero""
Jun 15 13:35:05 s050009088 kernel: flags: 0x17ffffc0000000()
Jun 15 13:35:05 s050009088 kernel: raw: 0017ffffc0000000 dead000000000100 dead000000000122 ffff8ce230f6e0d8
Jun 15 13:35:05 s050009088 kernel: raw: 0000000000000001 0000000000000000 00000000ffffffff 0000000000000000
Jun 15 13:35:05 s050009088 kernel: page dumped because: non-NULL mapping
Jun 15 13:35:05 s050009088 kernel: Modules linked in: nvidia_uvm(OE) veth xt_nat xt_tcpudp xt_conntrack xt_MASQUERADE nf_conntrack_netlink nfnetlink xfrm_user xfrm_algo iptable_nat nf_nat nf_conntrack nf_defrag_ipv6 nf_defrag_ipv4 xt_addrtype iptable_filter bpfilter br_netfilter bridge stp llc aufs overlay vmw_vsock_vmci_transport vsock dm_multipath scsi_dh_rdac scsi_dh_emc scsi_dh_alua binfmt_misc nvidia_drm(POE) nvidia_modeset(POE) intel_rapl_msr nvidia(POE) vmw_balloon intel_rapl_common input_leds intel_powerclamp joydev rapl serio_raw vmw_vmci mac_hid sch_fq_codel msr ramoops reed_solomon efi_pstore ip_tables x_tables autofs4 btrfs zstd_compress raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear vmwgfx crct10dif_pclmul crc32_pclmul ttm ghash_clmulni_intel drm_kms_helper syscopyarea aesni_intel sysfillrect crypto_simd sysimgblt mptspi cryptd fb_sys_fops glue_helper psmouse mptscsih drm mptbase ahci i2c_piix4 vmxnet3 scsi_transport_spi
Jun 15 13:35:05 s050009088 kernel:  libahci pata_acpi
Jun 15 13:35:05 s050009088 kernel: CPU: 6 PID: 4167 Comm: python3 Tainted: P        W  OE     5.4.0-150-generic #167-Ubuntu
Jun 15 13:35:05 s050009088 kernel: Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 11/12/2020
Jun 15 13:35:05 s050009088 kernel: Call Trace:
Jun 15 13:35:05 s050009088 kernel:  dump_stack+0x6d/0x8b
Jun 15 13:35:05 s050009088 kernel:  bad_page.cold+0x80/0xb1
Jun 15 13:35:05 s050009088 kernel:  free_pages_check_bad+0x5f/0x70
Jun 15 13:35:05 s050009088 kernel:  free_pcppages_bulk+0x186/0x6b0
Jun 15 13:35:05 s050009088 kernel:  free_unref_page_commit+0xb6/0xd0
Jun 15 13:35:05 s050009088 kernel:  free_unref_page_list+0x107/0x190
Jun 15 13:35:05 s050009088 kernel:  release_pages+0x38d/0x400
Jun 15 13:35:05 s050009088 kernel:  free_pages_and_swap_cache+0xb9/0xd0
Jun 15 13:35:05 s050009088 kernel:  tlb_flush_mmu+0x3a/0x140
Jun 15 13:35:05 s050009088 kernel:  zap_pte_range.isra.0+0x563/0x860
Jun 15 13:35:05 s050009088 kernel:  ? __warn+0x9d/0xe0
Jun 15 13:35:05 s050009088 kernel:  unmap_page_range+0x2e6/0x560
Jun 15 13:35:05 s050009088 kernel:  unmap_single_vma+0x7f/0xf0
Jun 15 13:35:05 s050009088 kernel:  unmap_vmas+0x79/0xf0
Jun 15 13:35:05 s050009088 kernel:  unmap_region+0xbc/0x160
Jun 15 13:35:05 s050009088 kernel:  __do_munmap+0x2aa/0x500
Jun 15 13:35:05 s050009088 kernel:  mmap_region+0x248/0x650
Jun 15 13:35:05 s050009088 kernel:  do_mmap+0x3b4/0x5c0
Jun 15 13:35:05 s050009088 kernel:  vm_mmap_pgoff+0xcb/0x120
Jun 15 13:35:05 s050009088 kernel:  ksys_mmap_pgoff+0x125/0x2b0
Jun 15 13:35:05 s050009088 kernel:  ? fput+0x13/0x20
Jun 15 13:35:05 s050009088 kernel:  ? ksys_ioctl+0x77/0x90
Jun 15 13:35:05 s050009088 kernel:  __x64_sys_mmap+0x33/0x40
Jun 15 13:35:05 s050009088 kernel:  do_syscall_64+0x57/0x190
Jun 15 13:35:05 s050009088 kernel:  entry_SYSCALL_64_after_hwframe+0x5c/0xc1
Jun 15 13:35:05 s050009088 kernel: RIP: 0033:0x7fbf936fc8e6
Jun 15 13:35:05 s050009088 kernel: Code: 00 00 00 00 f3 0f 1e fa 41 f7 c1 ff 0f 00 00 75 2b 55 48 89 fd 53 89 cb 48 85 ff 74 37 41 89 da 48 89 ef b8 09 00 00 00 0f 05 <48> 3d 00 f0 ff ff 77 62 5b 5d c3 0f 1f 80 00 00 00 00 48 8b 05 71
Jun 15 13:35:05 s050009088 kernel: RSP: 002b:00007ffd116cf078 EFLAGS: 00000206 ORIG_RAX: 0000000000000009
Jun 15 13:35:05 s050009088 kernel: RAX: ffffffffffffffda RBX: 0000000000000032 RCX: 00007fbf936fc8e6
Jun 15 13:35:05 s050009088 kernel: RDX: 0000000000000000 RSI: 0000000000200000 RDI: 0000010004400000
Jun 15 13:35:05 s050009088 kernel: RBP: 0000010004400000 R08: 00000000ffffffff R09: 0000000000000000
Jun 15 13:35:05 s050009088 kernel: R10: 0000000000000032 R11: 0000000000000206 R12: 00000000044be5e0
Jun 15 13:35:05 s050009088 kernel: R13: 0000000000000001 R14: 00000000047d97c0 R15: 000000000430b450

## Eventually the watchdog reports the soft lockup
## Checking the PID against the one from the process that ran `tf.test.gpu_device_name()` shows that it must've caused this lockup

Jun 15 13:46:45 s050009088 kernel: watchdog: BUG: soft lockup - CPU#6 stuck for 22s! [python3:4167]
```
</details>"
60897,Build Issue with Native TF-2.12,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

TF 2.12

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8

### Bazel version

6.2.1

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

While Building with Latest master build is getting failed. 
**Error Message:** error: no match for 'operator=' (operand types are 'absl::lts_20220623::Status' and 'tsl::Status')

Observing this Build failure after [PR 60872](https://github.com/tensorflow/tensorflow/pull/60872) got merged.

**Note:** 
1. Build is successful if we checkout previous commit(Commit ID: 76addf724a4794222e780542180dc32747d04aa2).
2. With this PR we also observed Jobs Failure at the same place.

### Standalone code to reproduce the issue

```shell
Build the TF 2.12 from source - https://www.tensorflow.org/install/source
```


### Relevant log output

```shell
ERROR: /workspace/tensorflow/tsl/platform/cloud/BUILD:76:11: Compiling tensorflow/tsl/platform/cloud/gcs_dns_cache.cc failed: (Exit 1): gcc failed: error executing command 
  (cd /tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH='' \
    PATH=/home/ubuntu/actions-runner/_work/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazelisk/downloads/bazelbuild/bazel-5.3.0-linux-arm64/bin:/home/ubuntu/actions-runner/_work/tensorflow/tensorflow/bazel-ci_build-cache/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
    *** \
  /dt10/usr/bin/gcc -MD -MF bazel-out/aarch64-opt-exec-50AE0418/bin/tensorflow/tsl/platform/cloud/_objs/gcs_dns_cache/gcs_dns_cache.pic.d '-frandom-seed=bazel-out/aarch64-opt-exec-50AE0418/bin/tensorflow/tsl/platform/cloud/_objs/gcs_dns_cache/gcs_dns_cache.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -iquote . -iquote bazel-out/aarch64-opt-exec-50AE0418/bin -iquote external/eigen_archive -iquote bazel-out/aarch64-opt-exec-50AE0418/bin/external/eigen_archive -iquote external/com_google_absl -iquote bazel-out/aarch64-opt-exec-50AE0418/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/aarch64-opt-exec-50AE0418/bin/external/nsync -iquote external/snappy -iquote bazel-out/aarch64-opt-exec-50AE0418/bin/external/snappy -iquote external/double_conversion -iquote bazel-out/aarch64-opt-exec-50AE0418/bin/external/double_conversion -iquote external/com_google_protobuf -iquote bazel-out/aarch64-opt-exec-50AE0418/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/aarch64-opt-exec-50AE0418/bin/external/com_googlesource_code_re2 -isystem third_party/eigen3/mkl_include -isystem bazel-out/aarch64-opt-exec-50AE0418/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/aarch64-opt-exec-50AE0418/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/aarch64-opt-exec-50AE0418/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/aarch64-opt-exec-50AE0418/bin/external/com_google_protobuf/src -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -w -g0 '-std=c++17' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL '-DDNNL_AARCH64_USE_ACL=1' -pthread '--sysroot=/dt10' -c tensorflow/tsl/platform/cloud/gcs_dns_cache.cc -o bazel-out/aarch64-opt-exec-50AE0418/bin/tensorflow/tsl/platform/cloud/_objs/gcs_dns_cache/gcs_dns_cache.pic.o)
# Configuration: 58aa5ccf3b98e321b76e39d46a74a06fde2c1ab0c3daeef1eee98dcf7771095c
# Execution platform: @local_execution_config_platform//:platform
tensorflow/tsl/platform/cloud/gcs_dns_cache.cc: In lambda function:
tensorflow/tsl/platform/cloud/gcs_dns_cache.cc:113:38: error: no match for 'operator=' (operand types are 'absl::lts_20220623::Status' and 'tsl::Status')
  113 |             return_status = OkStatus();
      |                                      ^
In file included from ./tensorflow/tsl/platform/status.h:28,
                 from ./tensorflow/tsl/platform/errors.h:28,
                 from ./tensorflow/tsl/platform/env.h:27,
                 from ./tensorflow/tsl/platform/cloud/http_request.h:23,
                 from ./tensorflow/tsl/platform/cloud/gcs_dns_cache.h:21,
                 from tensorflow/tsl/platform/cloud/gcs_dns_cache.cc:16:
external/com_google_absl/absl/status/status.h:764:16: note: candidate: 'absl::lts_20220623::Status& absl::lts_20220623::Status::operator=(const absl::lts_20220623::Status&)'
  764 | inline Status& Status::operator=(const Status& x) {
      |                ^~~~~~
external/com_google_absl/absl/status/status.h:764:48: note:   no known conversion for argument 1 from 'tsl::Status' to 'const absl::lts_20220623::Status&'
  764 | inline Status& Status::operator=(const Status& x) {
      |                                  ~~~~~~~~~~~~~~^
external/com_google_absl/absl/status/status.h:778:16: note: candidate: 'absl::lts_20220623::Status& absl::lts_20220623::Status::operator=(absl::lts_20220623::Status&&)'
  778 | inline Status& Status::operator=(Status&& x) {
      |                ^~~~~~
external/com_google_absl/absl/status/status.h:778:43: note:   no known conversion for argument 1 from 'tsl::Status' to 'absl::lts_20220623::Status&&'
  778 | inline Status& Status::operator=(Status&& x) {
      |                                  ~~~~~~~~~^
tensorflow/tsl/platform/cloud/gcs_dns_cache.cc:178:36: error: no matching function for call to 'tsl::Status::Status(absl::lts_20220623::Status&)'
  178 |         return Status(return_status);
      |                                    ^
In file included from ./tensorflow/tsl/platform/errors.h:28,
                 from ./tensorflow/tsl/platform/env.h:27,
                 from ./tensorflow/tsl/platform/cloud/http_request.h:23,
                 from ./tensorflow/tsl/platform/cloud/gcs_dns_cache.h:21,
                 from tensorflow/tsl/platform/cloud/gcs_dns_cache.cc:16:
./tensorflow/tsl/platform/status.h:309:8: note: candidate: 'tsl::Status::Status(tsl::Status&&, tsl::SourceLocation)'
  309 | inline Status::Status(Status&& s, SourceLocation loc) noexcept
      |        ^~~~~~
./tensorflow/tsl/platform/status.h:309:32: note:   no known conversion for argument 1 from 'absl::lts_20220623::Status' to 'tsl::Status&&'
  309 | inline Status::Status(Status&& s, SourceLocation loc) noexcept
      |                       ~~~~~~~~~^
./tensorflow/tsl/platform/status.h:296:8: note: candidate: 'tsl::Status::Status(const tsl::Status&)'
  296 | inline Status::Status(const Status& s)
      |        ^~~~~~
./tensorflow/tsl/platform/status.h:296:37: note:   no known conversion for argument 1 from 'absl::lts_20220623::Status' to 'const tsl::Status&'
  296 | inline Status::Status(const Status& s)
      |                       ~~~~~~~~~~~~~~^
./tensorflow/tsl/platform/status.h:71:3: note: candidate: 'tsl::Status::Status(tsl::error::Code, absl::lts_20220623::string_view, tsl::SourceLocation)'
   71 |   Status(tsl::error::Code code, absl::string_view msg,
      |   ^~~~~~
./tensorflow/tsl/platform/status.h:71:3: note:   candidate expects 3 arguments, 1 provided
./tensorflow/tsl/platform/status.h:66:3: note: candidate: 'tsl::Status::Status()'
   66 |   Status() {}
      |   ^~~~~~
./tensorflow/tsl/platform/status.h:66:3: note:   candidate expects 0 arguments, 1 provided
tensorflow/tsl/platform/cloud/gcs_dns_cache.cc: In static member function 'static std::vector<std::__cxx11::basic_string<char> > tsl::GcsDnsCache::ResolveName(const string&)':
tensorflow/tsl/platform/cloud/gcs_dns_cache.cc:180:18: error: no matching function for call to 'tsl::RetryingUtils::CallWithRetries(tsl::GcsDnsCache::ResolveName(const string&)::<lambda()>, tsl::RetryConfig&)'
  180 |       retryConfig);
      |                  ^
In file included from tensorflow/tsl/platform/cloud/gcs_dns_cache.cc:23:
./tensorflow/tsl/platform/retrying_utils.h:54:17: note: candidate: 'static tsl::Status tsl::RetryingUtils::CallWithRetries(const std::function<tsl::Status()>&, const tsl::RetryConfig&)'
   54 |   static Status CallWithRetries(const std::function<Status()>& f,
      |                 ^~~~~~~~~~~~~~~
./tensorflow/tsl/platform/retrying_utils.h:54:64: note:   no known conversion for argument 1 from 'tsl::GcsDnsCache::ResolveName(const string&)::<lambda()>' to 'const std::function<tsl::Status()>&'
   54 |   static Status CallWithRetries(const std::function<Status()>& f,
      |                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
./tensorflow/tsl/platform/retrying_utils.h:58:17: note: candidate: 'static tsl::Status tsl::RetryingUtils::CallWithRetries(const std::function<tsl::Status()>&, const std::function<void(long int)>&, const tsl::RetryConfig&)'
   58 |   static Status CallWithRetries(const std::function<Status()>& f,
      |                 ^~~~~~~~~~~~~~~
./tensorflow/tsl/platform/retrying_utils.h:58:17: note:   candidate expects 3 arguments, 2 provided
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 183.056s, Critical Path: 62.90s
INFO: 10119 processes: 4350 internal, 5769 local.
FAILED: Build did NOT complete successfully
```
</details>"
60896,test,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
60895,"TF throws: ""'visible_device_list' listed an invalid Device id"" when using non-GPU PluggableDevices","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux CentOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current Behaviour?

When using PluggableDevice API together with GPU devices, TF crashes with `tensorflow.python.framework.errors_impl.InvalidArgumentError: 'visible_device_list' listed an invalid Device id '2' but visible device count is 2` when calling `tf.device('/GPU:0')`

**For reproducing the error, it is necessary to have a PluggableDevice Plugin loaded and to have GPUs within the same system!!!**

Here the list of devices within my system:
```python3
# tf.config.list_physical_devices()
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:VE:0', device_type='VE'), PhysicalDevice(name='/physical_device:VE:1', device_type='VE'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```

I traced down the error to be thrown here. It gets thrown in
https://github.com/tensorflow/tensorflow/blob/e32f5b90ec16e88b23be8a5189e52ea9a420e999/tensorflow/tsl/framework/device_id_utils.cc#L46

However, it is caused by wrong values stored in the gpu_options, which get initialized here:
https://github.com/tensorflow/tensorflow/blob/0db597d0d758aba578783b5bf46c889700a45085/tensorflow/python/eager/context.py#L1206

The list of gpu_devices and ALL pluggable_devices get combined, even if they are not of the same device_type. So the list of `compatible_devices` will be:
```
[
	PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),
	PhysicalDevice(name='/physical_device:VE:0', device_type='VE'),
	PhysicalDevice(name='/physical_device:VE:1', device_type='VE')
]
```

This causes the `visible_device_list` to be `['0', '1', '2']`, which contains invalid GPU device indices. These then get passed to `ParseVisibleDeviceList`, which throws this error.

To fix this error, it suffices to change this line:
https://github.com/tensorflow/tensorflow/blob/0db597d0d758aba578783b5bf46c889700a45085/tensorflow/python/eager/context.py#L1216

and replace it to:
```python
if dev not in gpu_devices and dev.device_type == ""GPU"":
```

This way, the list of `compatible_devices` will only populated with other GPUs, not with any other device types.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(*tf.config.list_physical_devices(), sep='\n')
tf.device('/GPU:0')
```


### Relevant log output

```shell
PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')
PhysicalDevice(name='/physical_device:VE:0', device_type='VE')
PhysicalDevice(name='/physical_device:VE:1', device_type='VE')
PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/.../.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 5577, in device_v2
    return device(device_name)
  File ""/.../.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 5526, in device
    return context.device(device_name_or_function)
  File ""/.../.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 2348, in device
    ensure_initialized()
  File ""/.../.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 2143, in ensure_initialized
    context().ensure_initialized()
  File ""/.../.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 583, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 'visible_device_list' listed an invalid Device id '2' but visible device count is 2
```
</details>"
60893,I can't link the libtensorflowlite_flex.so file in my C++ TFLite code on Android.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android
- TensorFlow installed from (source or binary): from source
- TensorFlow version (or github SHA if from source): v2.12.0

I am trying to load a Swin Transformer model using the Flex delegate in TFLite code implemented in C++. After writing the sample code, I built it separately for Ubuntu and Android. While the model loading and execution work fine on Ubuntu, the model doesn't even load on Android.

When converting the model file, I received a message instructing me to use the flex delegate as follows
```
WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.
2023-06-16 10:53:45.770323: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.
2023-06-16 10:53:45.770352: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.
2023-06-16 10:53:45.770982: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: backbone-0001
2023-06-16 10:53:45.809957: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }
2023-06-16 10:53:45.809990: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: backbone-0001
2023-06-16 10:53:45.941381: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-06-16 10:53:45.950274: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.
2023-06-16 10:53:46.286516: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: backbone-0001
2023-06-16 10:53:46.500467: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 729486 microseconds.
2023-06-16 10:53:47.137098: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-06-16 10:53:52.096510: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2051] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexErf
Details:
	tf.Erf(tensor<1x196x1536xf32>) -> (tensor<1x196x1536xf32>) : {device = """"}
	tf.Erf(tensor<1x3136x384xf32>) -> (tensor<1x3136x384xf32>) : {device = """"}
	tf.Erf(tensor<1x49x3072xf32>) -> (tensor<1x49x3072xf32>) : {device = """"}
	tf.Erf(tensor<1x784x768xf32>) -> (tensor<1x784x768xf32>) : {device = """"}
See instructions: https://www.tensorflow.org/lite/guide/ops_select
2023-06-16 10:53:52.096741: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2116] Estimated count of arithmetic ops: 11.927 G  ops, equivalently 5.963 G  MACs

Process finished with exit code 0
```

So, I build flex delegate, and link libtensorflowlite_flex.so file to my execute file.
I built the TFLite for Android using the following commands:
```
bazel build -c opt --config=android_arm64 --define=tflite_convert_with_select_tf_ops=true --define=with_select_tf_ops=true //tensorflow/lite/c:libtensorflowlite_c.so
bazel build -c opt --config=android_arm64 --config=monolithic --define=tflite_convert_with_select_tf_ops=true --define=with_select_tf_ops=true //tensorflow/lite/delegates/flex:libtensorflowlite_flex.so
```

However, when I try to run the sample code on Android, I receive an error message similar to what I would get if the Flex delegate was not linked:
```
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For Android, it can be resolved by adding...
ERROR: Node number 1348 (FlexErf) failed to prepare.
I am unable to identify the cause of the issue. I would greatly appreciate it if someone could help me.
```

Of course, I have added the build option ""--no-as-needed"" and confirmed through the ""readelf"" command that the built file is linking the libtensorflowlite_flex.so file.

Here is my CMake file
```
cmake_minimum_required(VERSION 3.10)
project(tflite_flex_test C CXX)

set(ATTRIBUTE PRIVATE)
set(CMAKE_EXE_LINKER_FLAGS    ""-Wl,--no-as-needed ${CMAKE_EXE_LINKER_FLAGS}"")

# TFLite Path Setting
set(TFLITE_INC ${CMAKE_CURRENT_LIST_DIR}/3rdparty/tflite)
set(TFLITE_LIB ${CMAKE_CURRENT_LIST_DIR}/3rdparty/tflite/libtensorflowlite_c.so)
set(TFLITE_FLEX_LIB ${CMAKE_CURRENT_LIST_DIR}/3rdparty/tflite/libtensorflowlite_flex.so)

# OpenCV Path Setting
set(OPENCV_PATH ${CMAKE_CURRENT_LIST_DIR}/3rdparty/opencv_android)
set(OPENCV_INC ${OPENCV_PATH}/sdk/native/jni/include)
set(OpenCV_LIBS ${OPENCV_PATH}/sdk/native/libs/arm64-v8a)

add_executable(tflite_flex_test 
${CMAKE_CURRENT_LIST_DIR}/tflite_flex_test.cpp
)

# For TFLite
target_include_directories(tflite_flex_test ${ATTRIBUTE} ${TFLITE_INC})
target_link_libraries(tflite_flex_test ${TFLITE_LIB} ${TFLITE_FLEX_LIB})


# For OpenCV
target_include_directories(tflite_flex_test ${ATTRIBUTE} ${OPENCV_INC})
target_link_libraries(tflite_flex_test
${OpenCV_LIBS}/libopencv_core.so
${OpenCV_LIBS}/libopencv_features2d.so
${OpenCV_LIBS}/libopencv_highgui.so
${OpenCV_LIBS}/libopencv_imgproc.so
${OpenCV_LIBS}/libopencv_photo.so
${OpenCV_LIBS}/libopencv_video.so
)

target_link_libraries(tflite_flex_test
EGL 
GLESv2
GLESv3
)
find_library(ANDROID_LOG_LIB log)
target_link_libraries(tflite_flex_test log)


```

My code and weight file can download at [this link](https://drive.google.com/file/d/1Bim05zY07IvCmBWrUQvuqoFxzL7uXjwf/view?usp=sharing)
-> Just run ""sh build_android.sh"" than you can build my code.

I am unable to identify the cause of the issue. I would greatly appreciate it if someone could help me.

"
60892,`softplus` outputs `inf` for large inputs after converting to lite model,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.14.0-dev20230602

### 2. Code
This is the minimized code to reproduce the issue:
```python
import tensorflow as tf
import numpy as np

x1 = tf.constant([100., 100.], shape=[1, 2])

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(input_signature=[tf.TensorSpec(x1.shape, x1.dtype)])
  def call(self, x):
    return tf.math.softplus(x)

# Initializing the model
m = Model()

m(x1)
print('Keras mode output: ', m(x1))

converter = tf.lite.TFLiteConverter.from_keras_model(m)
tflite_model = converter.convert()

def _evaluateTFLiteModel(tflite_model, input_data):
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()

    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    for i in range(len(input_data)):
        interpreter.set_tensor(input_details[i]['index'], input_data[i])

    interpreter.invoke()
    output_data = [interpreter.get_tensor(output_details[i]['index'])
                   for i in range(len(output_details))]
    return output_data

print('Lite mode output: ', _evaluateTFLiteModel(tflite_model,[x1])[0])
```
### 3. Failure after conversion
Output:
```
Keras mode output:  tf.Tensor([[100. 100.]], shape=(1, 2), dtype=float32)
Lite mode output:  [[inf inf]]
```
Model produces wrong results:
- The original model will produce `[[100, 100]]`, which is the same as the input tensor `x1`
- After converting to tflite, the model may compute `softplus(x) = log(exp(x) + 1)` by first computing `exp`, which causes overflow `[[inf, inf]]`

"
60890,Spurious(?) type inference failed warning for flattened tf.data.Dataset with a RaggedTensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Mac OS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

As far as I can tell the code still runs as expected, but a seemly spurious warning is still issued.

The issue is pretty niche, removing the `_merge` function or the `vals` part to the initial dataset will make the error go away.

This occurs on 2.11, 2.12, and nightly on mac.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

labels = tf.ragged.constant([[""a"", ""b""], [""a""]])
vals = tf.constant([0.1, 0.2])
ds = tf.data.Dataset.from_tensors(dict(labels=labels, vals=vals, other=vals))

parts = [""labels"", ""vals""]

def _flatten(ex):
  flat_ds = tf.data.Dataset.from_tensor_slices({k: ex[k] for k in parts})

  def _merge(_flat_ex):
    _flat_ex[""other""] = tf.constant([0.1, 0.2])
    return _flat_ex

  return flat_ds.map(_merge)
ds = ds.flat_map(_flatten)

for ex in ds.as_numpy_iterator():
  print(ex)
```


### Relevant log output

```shell
2023-06-15 13:44:43.909272: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: type mismatch for node 'TensorSliceDataset': expected a subtype of:
type_id: TFT_PRODUCT
args {
  type_id: TFT_DATASET
  args {
    type_id: TFT_PRODUCT
    args {
      type_id: TFT_TENSOR
      args {
        type_id: TFT_LEGACY_VARIANT
      }
    }
    args {
      type_id: TFT_TENSOR
      args {
        type_id: TFT_FLOAT
      }
    }
  }
}

  got:
type_id: TFT_PRODUCT
args {
  type_id: TFT_DATASET
  args {
    type_id: TFT_PRODUCT
    args {
      type_id: TFT_RAGGED
      args {
        type_id: TFT_STRING
      }
    }
    args {
    }
  }
}

  
	while updating its output type.
{'labels': array([b'a', b'b'], dtype=object), 'vals': 0.1, 'other': array([0.1, 0.2], dtype=float32)}
{'labels': array([b'a'], dtype=object), 'vals': 0.2, 'other': array([0.1, 0.2], dtype=float32)}
```
</details>"
60886,Cannot upgrade to tensorflow-gpu==2.12.0 on Windows,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

-

### Python version

3.9.13

### Bazel version

-

### GCC/Compiler version

-

### CUDA/cuDNN version

-

### GPU model and memory

-

### Current Behaviour?

I expected, that tensorflow-gpu will be installed without errors.

### Standalone code to reproduce the issue

```shell
pip install --upgrade tensorflow-gpu

(setuptools is on recent version: 67.8.0)
```


### Relevant log output

```shell
Requirement already satisfied: tensorflow-gpu in c:\program files\python39\lib\site-packages (2.10.1)
Collecting tensorflow-gpu
  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [39 lines of output]
      Traceback (most recent call last):
        File ""C:\Program Files\Python39\lib\site-packages\setuptools\_vendor\packaging\requirements.py"", line 35, in __init__
          parsed = parse_requirement(requirement_string)
        File ""C:\Program Files\Python39\lib\site-packages\setuptools\_vendor\packaging\_parser.py"", line 64, in parse_requirement
          return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
        File ""C:\Program Files\Python39\lib\site-packages\setuptools\_vendor\packaging\_parser.py"", line 82, in _parse_requirement
          url, specifier, marker = _parse_requirement_details(tokenizer)
        File ""C:\Program Files\Python39\lib\site-packages\setuptools\_vendor\packaging\_parser.py"", line 126, in _parse_requirement_details
          marker = _parse_requirement_marker(
        File ""C:\Program Files\Python39\lib\site-packages\setuptools\_vendor\packaging\_parser.py"", line 147, in _parse_requirement_marker
          tokenizer.raise_syntax_error(
        File ""C:\Program Files\Python39\lib\site-packages\setuptools\_vendor\packaging\_tokenizer.py"", line 163, in raise_syntax_error
          raise ParserSyntaxError(
      setuptools.extern.packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)
          python_version>""3.7""
                        ^

      The above exception was the direct cause of the following exception:

      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 34, in <module>
        File ""C:\Users\xxx\AppData\Local\Temp\pip-install-mj0co7hm\tensorflow-gpu_ef0b590776dc4ddb9a9f3a965b7c38af\setup.py"", line 40, in <module>
          setuptools.setup()
        File ""C:\Program Files\Python39\lib\site-packages\setuptools\__init__.py"", line 106, in setup
          _install_setup_requires(attrs)
        File ""C:\Program Files\Python39\lib\site-packages\setuptools\__init__.py"", line 77, in _install_setup_requires
          dist.parse_config_files(ignore_option_errors=True)
        File ""C:\Program Files\Python39\lib\site-packages\setuptools\dist.py"", line 910, in parse_config_files
          self._finalize_requires()
        File ""C:\Program Files\Python39\lib\site-packages\setuptools\dist.py"", line 607, in _finalize_requires
          self._move_install_requirements_markers()
        File ""C:\Program Files\Python39\lib\site-packages\setuptools\dist.py"", line 647, in _move_install_requirements_markers
          inst_reqs = list(_reqs.parse(spec_inst_reqs))
        File ""C:\Program Files\Python39\lib\site-packages\setuptools\_vendor\packaging\requirements.py"", line 37, in __init__
          raise InvalidRequirement(str(e)) from e
      setuptools.extern.packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)
          python_version>""3.7""
                        ^
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```
</details>"
60884,Model containing LSTM does not run after conversion using ACTIVATIONS_INT16_WEIGHTS_INT8 quantization,"###  System information

Linux OpenSuse Tumbleweed
- TensorFlow installation : pip
- TensorFlow library : Tf-nightly, occurs on earlier versions too

###  Code
Converting a model containing an LSTM to a TFlite model quantized with int 16 activations and int8 weights results in an error when trying to allocate tensors to run the model subsequently.





```
import tensorflow as tf
import numpy as np

inp = tf.keras.Input([10,20], batch_size = 1, name = ""input_0"")
x = tf.keras.layers.LSTM(inp.shape[2],
                             return_sequences = True)(inp)
model_lstm = tf.keras.Model(inputs=inp, outputs=x)

rep_data = tf.data.Dataset.from_tensor_slices(np.float32(np.random.random_sample((10,1,10,20))))

def representative_dataset():
        for data in rep_data:
            yield {
            ""input_0"": data,
            }

converter = tf.lite.TFLiteConverter.from_keras_model(model_lstm)

converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
tf.lite.OpsSet.TFLITE_BUILTINS,
#comment line below to run at int 8
tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8
]
converter.representative_dataset = representative_dataset

calibrated_model = converter.convert()

interpreter = tf.lite.Interpreter(model_content = calibrated_model)
interpreter.allocate_tensors()
```

###  Failure after conversion
As noted above, the interpreter fails to allocate tensors for the the converted model



### Any other info / logs
Here is the traceback of the error:

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[48], line 2
      1 interpreter = tf.lite.Interpreter(model_content = calibrated_model)
----> 2 interpreter.allocate_tensors()

File ~/anaconda3/envs/tfnight/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py:531, in Interpreter.allocate_tensors(self)
    529 def allocate_tensors(self):
    530   self._ensure_safe()
--> 531   return self._interpreter.AllocateTensors()

RuntimeError: tensorflow/lite/kernels/unidirectional_sequence_lstm.cc:965 output_state != nullptr was not true.Node number 1 (UNIDIRECTIONAL_SEQUENCE_LSTM) failed to prepare.Failed to apply the default TensorFlow Lite delegate indexed at 0.
"
60883,Unnecessary memcopies between CPU and GPU when using tf.function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux CentOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current Behaviour?

We recognized that TensorFlow creates unnecessary copies between CPU and GPU. To reproduce, save the code below as `error.py` and execute it using `nvprof --print-gpu-trace --openacc-profiling off python3 error.py MODE`, with mode being 0, 1, 2 or 3.

Here what we have observed, I simplyfied the profiler output and commented inline:

## Mode 0 (generates input on GPU, runs model on GPU using tf.function):
```python
    Size Name
## RANDOM NUMBER GENERATION ##
      8B [CUDA memcpy HtoD]
1.0039KB [CUDA memset]
       - void tensorflow::functor::FillPhiloxRandomKernelLaunch...
      8B [CUDA memcpy HtoD]
      8B [CUDA memcpy DtoH]
      8B [CUDA memcpy HtoD]
      4B [CUDA memcpy HtoD]
      8B [CUDA memcpy HtoD]
## HERE THE COMPUTATION STARTS
18.375MB [CUDA memcpy DtoH]	## TF copies data to CPU
18.375MB [CUDA memcpy HtoD] ## 1. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [928]
      8B [CUDA memcpy DtoD]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 2. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [963]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 3. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [997]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 4. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1031]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 5. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1063]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 6. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1099]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 7. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1133]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 8. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1167]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 9. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1201]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 10. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1235]
       - void Eigen::internal::EigenMetaKernel...
## Merging of all results into a single Tensor and ten copies back to host
     30B [CUDA memcpy HtoD]
       - void tensorflow::functor::ColumnReduceMax16ColumnsKernel...
       - void tensorflow::functor::BlockReduceKernel...
      1B [CUDA memcpy DtoH]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
183.75MB [CUDA memcpy DtoH]
```

So we see that TF copies the data, that is already on the GPU to the CPU, and then in every iteration copies back to GPU, instead of just using the data that is already on the GPU.

## Mode 1 (generates input on CPU, runs model on GPU using tf.function):
```python
    Size Name
      8B [CUDA memcpy HtoD]
1.0039KB [CUDA memset]
      8B [CUDA memcpy HtoD]
      8B [CUDA memcpy DtoH]
      8B [CUDA memcpy HtoD]
      4B [CUDA memcpy HtoD]
      8B [CUDA memcpy HtoD]
## HERE THE COMPUTATION STARTS
18.375MB [CUDA memcpy HtoD] ## 1. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [911]
      8B [CUDA memcpy DtoD]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 2. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [946]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 3. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [980]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 4. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1012]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 5. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1046]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 6. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1080]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 7. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1116]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 8. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1150]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 9. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1184]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 10. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1218]
       - void Eigen::internal::EigenMetaKernel...
## Merging of all results into a single Tensor and ten copies back to host
     30B [CUDA memcpy HtoD]
       - void tensorflow::functor::ColumnReduceMax16ColumnsKernel...
       - void tensorflow::functor::BlockReduceKernel...
      1B [CUDA memcpy DtoH]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
183.75MB [CUDA memcpy DtoH]
```

Nearly identical to Mode 0, but here it is actually expected that the data gets copied over from CPU to GPU in every iteration.

## Mode 2 (generates input on GPU, runs model on GPU using tf.function): 
```python
    Size Name
## RANDOM NUMBER GENERATION ##
      8B [CUDA memcpy HtoD]
1.0039KB [CUDA memset]
       - void tensorflow::functor::FillPhiloxRandomKernelLaunch...
## HERE THE COMPUTATION STARTS
18.375MB [CUDA memcpy DtoH] ## TF copies data to CPU
      4B [CUDA memcpy HtoD]
      8B [CUDA memcpy HtoD]
18.375MB [CUDA memcpy HtoD] ## 1. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [907]
      8B [CUDA memcpy DtoD]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 1. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 2. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [991]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 2. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 3. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1076]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 3. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 4. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1161]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 4. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 5. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1246]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 5. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 6. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1333]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 6. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 7. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1420]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 7. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 8. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1509]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 8. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 9. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1594]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 9. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 10. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1679]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 10. TF copies result to CPU
```

Similar to mode 0, but here the results get immediatly copied back to the host.

## Mode 3 (generates input on GPU, runs model on GPU using eager mode): 
```python
    Size Name
## RANDOM NUMBER GENERATION ##
      8B [CUDA memcpy HtoD]
1.0039KB [CUDA memset]
       - void tensorflow::functor::FillPhiloxRandomKernelLaunch...
## HERE THE COMPUTATION STARTS
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [806]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [810]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [814]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [818]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [822]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [826]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [830]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [834]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [838]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [842]
```

In this case no uncessary memcopies occur, data is kept on GPU all the time.

## Summary
When we use `model.predict(...)` or `model.predict_on_batch()` (or any other of these Keras.Model functions that use `tf.function`), then the input data ALWAYS gets copied to the host first, and then in every iteration back to the GPU. This causes an significant performance penalty.

I have not been able to find any documentation about this behavior, if it is intended, or a way to prevent this to happen.

Here also the `tf.debugging.set_log_device_placement(True)` output. I think the line `input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0` indicates that for whatever reason the `tf.function`'s input is expected to be on the CPU.

```python
2023-06-15 14:44:14.959681: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.960017: I tensorflow/core/common_runtime/placer.cc:114] resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.960041: I tensorflow/core/common_runtime/placer.cc:114] VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.960475: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
resource: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.960895: I tensorflow/core/common_runtime/placer.cc:114] resource: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
value: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.960922: I tensorflow/core/common_runtime/placer.cc:114] value: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.960945: I tensorflow/core/common_runtime/placer.cc:114] AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.961429: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.963362: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
components_0: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.963837: I tensorflow/core/common_runtime/placer.cc:114] components_0: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
TensorDataset: (TensorDataset): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.963872: I tensorflow/core/common_runtime/placer.cc:114] TensorDataset: (TensorDataset): /job:localhost/replica:0/task:0/device:CPU:0
handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.963889: I tensorflow/core/common_runtime/placer.cc:114] handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.964369: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0
input__dataset: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.980981: I tensorflow/core/common_runtime/placer.cc:114] input__dataset: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
FlatMapDataset: (FlatMapDataset): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.981006: I tensorflow/core/common_runtime/placer.cc:114] FlatMapDataset: (FlatMapDataset): /job:localhost/replica:0/task:0/device:CPU:0
handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.981024: I tensorflow/core/common_runtime/placer.cc:114] handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.982115: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.982841: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
input__dataset: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.983325: I tensorflow/core/common_runtime/placer.cc:114] input__dataset: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
buffer__size: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.983348: I tensorflow/core/common_runtime/placer.cc:114] buffer__size: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
PrefetchDataset: (PrefetchDataset): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.983387: I tensorflow/core/common_runtime/placer.cc:114] PrefetchDataset: (PrefetchDataset): /job:localhost/replica:0/task:0/device:CPU:0
handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.983403: I tensorflow/core/common_runtime/placer.cc:114] handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.984096: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0
resource: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.984658: I tensorflow/core/common_runtime/placer.cc:114] resource: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.984692: I tensorflow/core/common_runtime/placer.cc:114] ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:CPU:0
value_RetVal: (_DeviceRetval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.984710: I tensorflow/core/common_runtime/placer.cc:114] value_RetVal: (_DeviceRetval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.985212: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.985724: I tensorflow/core/common_runtime/placer.cc:114] input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.985761: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.985778: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.986346: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.986722: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.986812: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.988278: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.988585: I tensorflow/core/common_runtime/placer.cc:114] resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.988624: I tensorflow/core/common_runtime/placer.cc:114] value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.988643: I tensorflow/core/common_runtime/placer.cc:114] AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.989152: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.989472: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.989562: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.989661: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.989731: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0
handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.990139: I tensorflow/core/common_runtime/placer.cc:114] handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
AnonymousIteratorV3: (AnonymousIteratorV3): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.990173: I tensorflow/core/common_runtime/placer.cc:114] AnonymousIteratorV3: (AnonymousIteratorV3): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.990613: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op AnonymousIteratorV3 in device /job:localhost/replica:0/task:0/device:CPU:0
dataset: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.990976: I tensorflow/core/common_runtime/placer.cc:114] dataset: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
iterator: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.990998: I tensorflow/core/common_runtime/placer.cc:114] iterator: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
MakeIterator: (MakeIterator): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.991016: I tensorflow/core/common_runtime/placer.cc:114] MakeIterator: (MakeIterator): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.991473: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.992481: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
         [[{{node Placeholder/_0}}]]
args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.002874: I tensorflow/core/common_runtime/placer.cc:114] args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
GeneratorDataset: (GeneratorDataset): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.002917: I tensorflow/core/common_runtime/placer.cc:114] GeneratorDataset: (GeneratorDataset): /job:localhost/replica:0/task:0/device:CPU:0
NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.002935: I tensorflow/core/common_runtime/placer.cc:114] NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.002966: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.002983: I tensorflow/core/common_runtime/placer.cc:114] FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.003007: I tensorflow/core/common_runtime/placer.cc:114] identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.007846: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op AnonymousIteratorV3 in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.007991: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0
args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.011294: I tensorflow/core/common_runtime/placer.cc:114] args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
GeneratorDataset: (GeneratorDataset): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.011320: I tensorflow/core/common_runtime/placer.cc:114] GeneratorDataset: (GeneratorDataset): /job:localhost/replica:0/task:0/device:CPU:0
NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.011339: I tensorflow/core/common_runtime/placer.cc:114] NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.011355: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.011373: I tensorflow/core/common_runtime/placer.cc:114] FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.011389: I tensorflow/core/common_runtime/placer.cc:114] identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.018501: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.018640: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0
iterator: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.046638: I tensorflow/core/common_runtime/placer.cc:114] iterator: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
assignaddvariableop_resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.046661: I tensorflow/core/common_runtime/placer.cc:114] assignaddvariableop_resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
IteratorGetNext: (IteratorGetNext): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.046678: I tensorflow/core/common_runtime/placer.cc:114] IteratorGetNext: (IteratorGetNext): /job:localhost/replica:0/task:0/device:CPU:0
model/tf.__operators__.add/AddV2: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.046695: I tensorflow/core/common_runtime/placer.cc:114] model/tf.__operators__.add/AddV2: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0
AssignAddVariableOp: (AssignAddVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.046711: I tensorflow/core/common_runtime/placer.cc:114] AssignAddVariableOp: (AssignAddVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
NoOp: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.046726: I tensorflow/core/common_runtime/placer.cc:114] NoOp: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.046759: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.046783: I tensorflow/core/common_runtime/placer.cc:114] identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.046798: I tensorflow/core/common_runtime/placer.cc:114] Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.051534: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op __inference_predict_function_68 in device /job:localhost/replica:0/task:0/device:GPU:0
args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.054027: I tensorflow/core/common_runtime/placer.cc:114] args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.054141: I tensorflow/core/common_runtime/placer.cc:114] NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.054201: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.054272: I tensorflow/core/common_runtime/placer.cc:114] FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.054342: I tensorflow/core/common_runtime/placer.cc:114] identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
PyFunc: (PyFunc): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.054415: I tensorflow/core/common_runtime/placer.cc:114] PyFunc: (PyFunc): /job:localhost/replica:0/task:0/device:CPU:0
args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.061278: I tensorflow/core/common_runtime/placer.cc:114] args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
PyFunc: (PyFunc): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.061356: I tensorflow/core/common_runtime/placer.cc:114] PyFunc: (PyFunc): /job:localhost/replica:0/task:0/device:CPU:0
NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.061419: I tensorflow/core/common_runtime/placer.cc:114] NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.061476: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.061556: I tensorflow/core/common_runtime/placer.cc:114] FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.061612: I tensorflow/core/common_runtime/placer.cc:114] identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.067674: I tensorflow/core/common_runtime/placer.cc:114] args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
PyFunc: (PyFunc): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.067767: I tensorflow/core/common_runtime/placer.cc:114] PyFunc: (PyFunc): /job:localhost/replica:0/task:0/device:CPU:0
NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.067834: I tensorflow/core/common_runtime/placer.cc:114] NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.067905: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.067969: I tensorflow/core/common_runtime/placer.cc:114] FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.068020: I tensorflow/core/common_runtime/placer.cc:114] identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import sys

assert len(sys.argv) == 2, ""needs to be run as `python3 error.py MODE`""

mode = int(sys.argv[1])

inp = tf.keras.Input((3, 224, 224))
out = inp + inp
model = tf.keras.Model(inp, out)

class Sequence(tf.keras.utils.Sequence):
        def __init__(self, x):          self.x = x
        def __len__(self):              return 10
        def __getitem__(self, idx):     return self.x

with tf.device('/GPU:0' if mode != 1 else '/CPU:0'):
        data = tf.random.uniform((32, 3, 224, 224))

with tf.device('/GPU:0'):
        if mode == 2:
                for _ in range(10):
                        model.predict_on_batch(data)
        elif mode == 3:
                for _ in range(10):
                        model(data)
        else:
                seq = Sequence(data)
                model.predict(seq)
```


### Relevant log output

```shell
see above
```
</details>"
60880,Tensorflow-Quantum Module Import Error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

v2.11.0-0-gd5b57ca93e5 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.3.0

### GCC/Compiler version

gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0

### CUDA/cuDNN version

Build cuda_11.8.r11.8/compiler.31833905_0

### GPU model and memory

A100 high-mem node

### Current Behaviour?

Still trying to get TFQ to work post-py 3.10 upgrade in colab ... the TF/TFQ nightly_build route seems to be getting me closer as I no longer receive ""Module Not Found"" when I try importing TFQ, however, I now receive the pauli error below.

### Standalone code to reproduce the issue

```shell
import tensorflow_quantum as tfq

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-80-0f5757594a36> in <cell line: 1>()
----> 1 import tensorflow_quantum as tfq

4 frames
/content/quantum/quantum/tensorflow_quantum/core/ops/cirq_ops.py in <module>
     23 
     24 from tensorflow_quantum.core.ops import batch_util
---> 25 from tensorflow_quantum.core.proto import pauli_sum_pb2
     26 from tensorflow_quantum.core.proto import program_pb2
     27 from tensorflow_quantum.core.serialize import serializer

ImportError: cannot import name 'pauli_sum_pb2' from 'tensorflow_quantum.core.proto' (/content/quantum/quantum/tensorflow_quantum/core/proto/__init__.py)
```


### Relevant log output

```shell
See above

FYI ... the compile from nightly takes >8 hours so getting you info may take +1 days turn around
Also, I have tried nightly_builds (for my Colab env) for several days and this is the only build where I have obtained a different error than ""Module Not Found"" upon attempting to import TFQ after installing the wheel.  

Are you guys trying to actually 'import tensorflow-quantum' after executing your quantum tests (./scripts/test_all.sh)? I ask because as far as I can tell, the import does not work even though the installation and smoke tests all complete successfully.

I have attempted every TF/TFQ install listed here:
https://www.tensorflow.org/quantum/install
and TFQ will not import any longer.

I cannot TFQ imported and functional ...
```
</details>"
60878,Mistype (mixed_bloat16) in the keras.mixed_precision.Policy class,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

master

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

There is a [mistype](https://github.com/tensorflow/tensorflow/blob/e0f655d121da781e69052f07c2e5635c4544f2fe/tensorflow/python/keras/mixed_precision/policy.py#L195) in the mixed_precision policy check. It should be `bfloat16`, not `bloat16`
```
    if name in ('mixed_float16', 'mixed_bloat16'):
      device_compatibility_check.log_device_compatibility_check(name)
```

There is no practical issue with it because `log_device_compatibility_check` doesn't support it anyway.


### Standalone code to reproduce the issue

```shell
Not required, it's clear from the code
```


### Relevant log output

_No response_</details>"
60877,"Hi @radres2019, Thank you for reporting the issue!","              Hi @radres2019, Thank you for reporting the issue!
You are seeing this error because Colab has python version 3.10. Tensorflow quantum 0.7.2 is compatible with Python 3.7, 3.8, 3.9  and does not support Python 3.10. 

Please refer to the gist where I was able to install Tensorflow quantum sucessfully [here](https://colab.sandbox.google.com/gist/synandi/b733cccf4a90d29e5feefb606f02f843/60428.ipynb). Thank you!

_Originally posted by @synandi in https://github.com/tensorflow/tensorflow/issues/60428#issuecomment-1527270841_
            "
60876, When will TFRT integration be fully released?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

master, commit id 1bd12b4863759e44da4139628973f372655b14f6

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.1.0

### GCC/Compiler version

8.3.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Firstly, the TFRT integration is disable in `.bazelrc`.
Comment out TFRT deleted packages in `.bazelrc`, add deps for tensorflow serving, fix some reference in `BUILD`, blah, blah, blah...The build is still broken.

```diff
diff --git a/.bazelrc b/.bazelrc
index e26bf6de7f8..48edf30f4c7 100644
--- a/.bazelrc
+++ b/.bazelrc
@@ -694,10 +694,10 @@ build:ubsan --linkopt -fsanitize=undefined
 build:ubsan --linkopt -lubsan
 
 # Disable TFRT integration for now unless --config=tfrt is specified.
-build      --deleted_packages=tensorflow/core/tfrt/stubs,tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python
+# build      --deleted_packages=tensorflow/core/tfrt/stubs,tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python
 # TODO(b/240450920): We are in the process of migrating JitRt backend to XLA
 # and while we are doing this we can't keep it buildable/testable in OSS.
-build:tfrt --deleted_packages=tensorflow/core/tfrt/stubs,tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python
+# build:tfrt --deleted_packages=tensorflow/core/tfrt/stubs,tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python
 
 # TF Fuzztest config
 try-import fuzztest.bazelrc
diff --git a/tensorflow/compiler/mlir/tfrt/ir/mlrt/tf_ops.td b/tensorflow/compiler/mlir/tfrt/ir/mlrt/tf_ops.td
index 7268588749d..cee88fb112b 100644
--- a/tensorflow/compiler/mlir/tfrt/ir/mlrt/tf_ops.td
+++ b/tensorflow/compiler/mlir/tfrt/ir/mlrt/tf_ops.td
@@ -19,8 +19,8 @@ limitations under the License.
 include ""tensorflow/compiler/mlir/tfrt/ir/mlrt/tf_mlrt_dialect.td""
 include ""tensorflow/compiler/mlir/tfrt/ir/mlrt/mlrt_dialect.td""
 include ""tensorflow/compiler/mlir/tensorflow/ir/tf_op_base.td""
-include ""third_party/tf_runtime/include/tfrt/compiler/opdefs/tfrt_op_interfaces.td""
-include ""third_party/tf_runtime/include/tfrt/compiler/opdefs/tfrt_traits.td""
+include ""external/tf_runtime/include/tfrt/compiler/opdefs/tfrt_op_interfaces.td""
+include ""external/tf_runtime/include/tfrt/compiler/opdefs/tfrt_traits.td""
 
 // tf_mlrt.tf_await returns a tensorflow Tensor. It is a fake op that is only
 // used during parallelization and has no runtime implementation.
diff --git a/tensorflow/compiler/mlir/tfrt/transforms/mlrt/BUILD b/tensorflow/compiler/mlir/tfrt/transforms/mlrt/BUILD
index beb50129756..ec69705437e 100644
--- a/tensorflow/compiler/mlir/tfrt/transforms/mlrt/BUILD
+++ b/tensorflow/compiler/mlir/tfrt/transforms/mlrt/BUILD
@@ -74,7 +74,7 @@ cc_library(
         ""//tensorflow/compiler/mlir/tfrt/ir/mlrt:tf_mlrt_tpu_ops"",
         ""//tensorflow/core/tfrt/fallback:fallback_state"",
         ""//tensorflow/core/tfrt/fallback:op_kernel_runner_cache"",
-        ""//third_party/protobuf"",
+        ""@com_google_protobuf//:protobuf"",
         ""@llvm-project//mlir:FuncDialect"",
         ""@llvm-project//mlir:FuncTransforms"",
         ""@llvm-project//mlir:IR"",
@@ -150,7 +150,7 @@ cc_library(
         "":assign_op_key"",
         "":passes"",
         "":while_to_map_fn"",
-        ""//base:vlog"",
+        # ""//base:vlog"",
         ""//tensorflow/compiler/mlir/tensorflow:dump_mlir_util"",
         ""//tensorflow/compiler/mlir/tensorflow:error_util"",
         ""//tensorflow/compiler/mlir/tfrt:import_model"",
diff --git a/tensorflow/core/tfrt/graph_executor/BUILD b/tensorflow/core/tfrt/graph_executor/BUILD
index 7cef54cdb69..7e4537869c1 100644
--- a/tensorflow/core/tfrt/graph_executor/BUILD
+++ b/tensorflow/core/tfrt/graph_executor/BUILD
@@ -152,7 +152,7 @@ cc_library(
     visibility = [""//visibility:public""],
     deps = [
         "":config_proto_cc"",
-        ""//google/protobuf:any_cc_proto"",
+        # ""//google/protobuf:any_cc_proto"",
         ""@com_google_absl//absl/status"",
         ""@com_google_absl//absl/status:statusor"",
     ],
@@ -161,7 +161,7 @@ cc_library(
 tf_proto_library(
     name = ""config_proto"",
     srcs = [""config.proto""],
-    protodeps = [""//google/protobuf:any""],
+    # protodeps = [""//google/protobuf:any""],
     visibility = [""//visibility:public""],
 )
 
@@ -187,7 +187,7 @@ cc_library(
     hdrs = [""sync_resource_state.h""],
     visibility = [""//visibility:public""],
     deps = [
-        ""//tensorflow_serving/util:any_ptr"",
+        ""@tensorflow_serving//tensorflow_serving/util:any_ptr"",
         ""@tf_runtime//:tensor"",
     ],
 )
diff --git a/tensorflow/core/tfrt/graph_executor/sync_resource_state.h b/tensorflow/core/tfrt/graph_executor/sync_resource_state.h
index 1571fc01352..159f1714a71 100644
--- a/tensorflow/core/tfrt/graph_executor/sync_resource_state.h
+++ b/tensorflow/core/tfrt/graph_executor/sync_resource_state.h
@@ -18,7 +18,7 @@ limitations under the License.
 #include <utility>
 #include <vector>
 
-#include ""third_party/tensorflow_serving/util/any_ptr.h""
+#include ""tensorflow_serving/tensorflow_serving/util/any_ptr.h""
 #include ""tfrt/tensor/dense_host_tensor.h""  // from @tf_runtime
 namespace tensorflow {
 namespace tfrt_stub {
diff --git a/tensorflow/workspace3.bzl b/tensorflow/workspace3.bzl
index 91871db22c8..c16b5d0ab9b 100644
--- a/tensorflow/workspace3.bzl
+++ b/tensorflow/workspace3.bzl
@@ -2,6 +2,7 @@
 
 load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")
 load(""//third_party:tf_runtime/workspace.bzl"", tf_runtime = ""repo"")
+load(""//third_party:tensorflow_serving/workspace.bzl"", tensorflow_serving = ""repo"")
 load(""//third_party/llvm:workspace.bzl"", llvm = ""repo"")
 
 def workspace():
@@ -16,6 +17,7 @@ def workspace():
     )
 
     tf_runtime()
+    tensorflow_serving()
 
     # https://github.com/bazelbuild/bazel-skylib/releases
     http_archive(
diff --git a/third_party/tensorflow_serving/workspace.bzl b/third_party/tensorflow_serving/workspace.bzl
new file mode 100644
index 00000000000..5e5f54b6f1a
--- /dev/null
+++ b/third_party/tensorflow_serving/workspace.bzl
@@ -0,0 +1,20 @@
+""""""Provides the repository macro to import TFRT.""""""
+
+load(""//third_party:repo.bzl"", ""tf_http_archive"", ""tf_mirror_urls"")
+
+def repo():
+    TFRT_COMMIT = ""bd203faa888dd5ce90f21e3ee9af92dbc90b8a25""
+    TFRT_SHA256 = """"
+
+    tf_http_archive(
+        name = ""tensorflow_serving"",
+        sha256 = TFRT_SHA256,
+        strip_prefix = ""serving-{commit}"".format(commit = TFRT_COMMIT),
+        urls = tf_mirror_urls(""https://github.com/tensorflow/serving/archive/{commit}.tar.gz"".format(commit = TFRT_COMMIT)),
+        # A patch file can be provided for atomic commits to both TF and TFRT.
+        # The job that bumps the TFRT_COMMIT also resets patch_file to 'None'.
+        patch_file = None,
+    )

```

### Standalone code to reproduce the issue

```shell
bazel build  --incompatible_fix_package_group_reporoot_syntax=false tensorflow/core/tfrt/graph_executor:graph_executor
```


### Relevant log output

```shell
external/tf_runtime/lib/basic_kernels/opdefs/tfrt_base.cc:80:9:   required from here
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen/mlir/Interfaces/CallInterfaces.h.inc:155:56: error: 'class tfrt::compiler::CallOp' has no member named 'setCalleeFromCallable'; did you mean 'getCallableForCallee'?
   return (llvm::cast<ConcreteOp>(tablegen_opaque_val)).setCalleeFromCallable(callee);
          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
          getCallableForCallee
bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen/mlir/Interfaces/CallInterfaces.h.inc:155:84: error: return-statement with a value, in function returning 'void' [-fpermissive]
   return (llvm::cast<ConcreteOp>(tablegen_opaque_val)).setCalleeFromCallable(callee);
                                                                                    ^
cc1plus: warning: unrecognized command line option '-Wno-unused-local-typedef'
Target //tensorflow/core/tfrt/graph_executor:graph_executor failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 72.836s, Critical Path: 33.33s
INFO: 247 processes: 21 internal, 226 local.
FAILED: Build did NOT complete successfully
```
</details>"
60873,How does this apply to tensorflow2.12.0 if you don't change the version of TensorFlow,"tf.app.run()
AttributeError
module 'tensorflow' has no attribute 'app'
  File ""D:\桌面\bert-master\bert-master\run_classifier.py"", line 980, in <module>
    tf.app.run()
AttributeError: module 'tensorflow' has no attribute 'app'"
60862,New unit tests fails when built with gcc,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

The unit test fails with a pipeline error resulting in inability to read a file as it was not created correctly, the name seems to be wrong.

The file that exists is 00000000.main.tensorflow_{anonymous}_NopPass_after.mlir but the file attempted to be read is 00000000.main.tensorflow_anonymous_namespace_NopPass_after.mlir

### Standalone code to reproduce the issue

```shell
bazel test --config=mkl_aarch64_threadpool --action_env=PYTHON_BIN_PATH=/usr/bin/python3.11 --copt=-flax-vector-conversions --jobs=75 --action_env=GIT_TAG_OVERRIDE --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_timeout=300,500,-1,-1 --test_output=errors --cache_test_results=no --noremote_accept_cached --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --build_tag_filters=-no_oss,-oss_serial,-benchmark-test,-v1only,-no_aarch64 --test_tag_filters=-no_oss,-oss_serial,-benchmark-test,-v1only,-no_aarch64 --verbose_failures --build_tests_only -- //tensorflow/compiler/mlir/lite/debug/...
```


### Relevant log output

```shell
==================== Test output for //tensorflow/compiler/mlir/lite/debug:debug_test:
2023-06-14 16:04:25.079113: I tensorflow/core/util/port.cc:116] Experimental oneDNN custom operations are on. If you experience issues, please turn them off by setting the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-14 16:04:25.079305: W tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay. The old value will be erased in order to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.
Running main() from gmock_main.cc
[==========] Running 2 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 2 tests from InitPassManagerTest
[ RUN      ] InitPassManagerTest.CrashReproducer
error: Failures have been detected while processing an MLIR pass pipeline
[       OK ] InitPassManagerTest.CrashReproducer (5 ms)
[ RUN      ] InitPassManagerTest.Dump
tensorflow/compiler/mlir/lite/debug/debug_test.cc:156: Failure
Expected equality of these values:
  ::tsl::OkStatus()
    Which is: OK
  (tsl::ReadFileToString( tsl::Env::Default(), tsl::io::JoinPath( dump_dir, ""00000000.main.tensorflow_anonymous_namespace_NopPass_after.mlir""), &mlir_dump))
    Which is: NOT_FOUND: /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/lite/debug/debug_test/test.outputs/InitPassManagerTest.Dump/20230614_160425.086537/00000000.main.tensorflow_anonymous_namespace_NopPass_after.mlir; No such file or directory
[  FAILED  ] InitPassManagerTest.Dump (3 ms)
[----------] 2 tests from InitPassManagerTest (9 ms total)

[----------] Global test environment tear-down
[==========] 2 tests from 1 test suite ran. (9 ms total)
[  PASSED  ] 1 test.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] InitPassManagerTest.Dump

 1 FAILED TEST
================================================================================
Target //tensorflow/compiler/mlir/lite/debug:debug_test up-to-date:
  bazel-bin/tensorflow/compiler/mlir/lite/debug/debug_test
INFO: Elapsed time: 10.928s, Critical Path: 0.96s
INFO: 2 processes: 1 internal, 1 local.
INFO: Build completed, 1 test FAILED, 2 total actions
//tensorflow/compiler/mlir/lite/debug:debug_test                         FAILED in 0.5s
  /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/lite/debug/debug_test/test.log

Executed 1 out of 1 test: 1 fails locally.
```
</details>"
60861,tf.data debug mode breaks dataset.save(),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.13.0rc0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

dataset.save() does not work if the experimental debug mode has been enabled for the datasets. A simple reproducer and the exception are below. I think the issue is due to the following code in `set_save_dataset_attributes` function in `tensorflow/python/data/ops/save_op.py`:

`shard_func = lambda *x: None  # a dummy function that will not be used`

Replacing this line with e.g.

`shard_func = lambda *x: 0`

seems to fix this issue, so apparently returning `None` doesn't work in the debug mode. Btw the comment on this line is a bit misleading, because this function is still traced, so it's not completely unused.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tempfile
tf.data.experimental.enable_debug_mode()
ds = tf.data.Dataset.from_tensor_slices([1.0, 2.0, 3.0])
with tempfile.TemporaryDirectory() as tmpdir:
     ds.save(tmpdir)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1042, in convert
    x = ops.convert_to_tensor_or_composite(x)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1547, in convert_to_tensor_or_composite
    return internal_convert_to_tensor_or_composite(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1582, in internal_convert_to_tensor_or_composite
    return convert_to_tensor(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py"", line 183, in wrapped
    return func(*args, **kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1443, in convert_to_tensor
    return tensor_conversion_registry.convert(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py"", line 209, in convert
    return overload(dtype, name)  #  pylint: disable=not-callable
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 2177, in __tf_tensor__
    raise TypeError(""can't convert Operation '{}' to Tensor"".format(self.name))
TypeError: can't convert Operation 'EagerPyFunc' to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1746, in save
    return save_op._save(self, path, compression, shard_func, checkpoint_args)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/data/ops/save_op.py"", line 57, in _save
    dataset, shard_func, use_shard_func, path = set_save_dataset_attributes(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/data/ops/save_op.py"", line 103, in set_save_dataset_attributes
    wrapped_func = structured_function.StructuredFunctionWrapper(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py"", line 272, in __init__
    self._function = fn_factory()
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1189, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1169, in _get_concrete_function_garbage_collected
    self._initialize(args, kwargs, add_initializers_to=initializers)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 694, in _initialize
    self._variable_creation_fn    # pylint: disable=protected-access
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 176, in _get_concrete_function_internal_garbage_collected
    concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 171, in _maybe_define_concrete_function
    return self._maybe_define_function(args, kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 398, in _maybe_define_function
    concrete_function = self._create_concrete_function(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 305, in _create_concrete_function
    func_graph_module.func_graph_from_py_func(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1060, in func_graph_from_py_func
    func_outputs = nest.map_structure(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/util/nest.py"", line 624, in map_structure
    return nest_util.map_structure(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py"", line 1054, in map_structure
    return _tf_core_map_structure(func, *structure, **kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py"", line 1094, in _tf_core_map_structure
    [func(*x) for x in entries],
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py"", line 1094, in <listcomp>
    [func(*x) for x in entries],
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1044, in convert
    raise TypeError(
TypeError: To be compatible with tf.function, Python functions must return zero or more Tensors or ExtensionTypes or None values; in compilation of <function StructuredFunctionWrapper.__init__.<locals>.trace_py_function.<locals>.wrapped_fn at 0x7f88c9224c10>, found return value of type Operation, which is not a Tensor or ExtensionType.
```
</details>"
60858,installing error about tensorflow 2.7.3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.7.3

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

cuda 11.3,cuDNN 8.2.1

### GPU model and memory

_No response_

### Current Behaviour?

My computer configurations are:
gcc (Ubuntu 7.5.0-6ubuntu2) 7.5.0
cuda 11.3
cudnn 8.2.1
numpy 1.24.3
pandas 2.0.2
matplotlib 3.7.1
scipy 1.10.1
scikit-mage 0.21.0
scikit-learn 1.2.2
lapack 3.11.0

When I typed in “sudo pip3 install tensorflow_gpu-2.7.3-cp38-cp38-manylinux2010_x86_64.whl”, the results occurred as follows:


**Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4; python_version < ""3.10""->markdown>=2.6.8->tensorboard~=2.6->tensorflow-gpu==2.7.3) (3.15.0)
Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard~=2.6->tensorflow-gpu==2.7.3) (3.1.0)
Collecting pyasn1>=0.1.3
  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)
     |████████████████████████████████| 83 kB 21 kB/s 
Installing collected packages: absl-py, tensorflow-io-gcs-filesystem, opt-einsum, grpcio, termcolor, astunparse, h5py, protobuf, tensorflow-estimator, keras-preprocessing, google-pasta, gast, wrapt, libclang, keras, markdown, tensorboard-data-server, requests-oauthlib, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, google-auth-oauthlib, werkzeug, tensorboard, flatbuffers, tensorflow-gpu
  Attempting uninstall: protobuf
    Found existing installation: protobuf 3.6.1
    Not uninstalling protobuf at /usr/lib/python3/dist-packages, outside environment /usr
    Can't uninstall 'protobuf'. No files were found to uninstall.
Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-2.0.7 gast-0.4.0 google-auth-2.20.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.2 h5py-3.8.0 keras-2.7.0 keras-preprocessing-1.1.2 libclang-16.0.0 markdown-3.4.3 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-estimator-2.7.0 tensorflow-gpu-2.7.3 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 werkzeug-2.3.6 wrapt-1.15.0**

Could you give me some suggestions?Thanks very much!

### Standalone code to reproduce the issue

```shell
---
```


### Relevant log output

_No response_</details>"
60857,Relu Grad Shape Issues,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Just trying to implement 3D CNN for CT images, and suddenly came across this error.
Couldn't get much on the internet.
Below is the snippet of the error.

grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py"", line 275, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/model_7/3dcnn/conv3d_24/ReluGrad'
Inputs to operation gradient_tape/model_7/3dcnn/conv3d_24/ReluGrad of type ReluGrad must have the same size and shape.  Input 0: [1,0,1875,256] != input 1: [1,0,125,15,256]
	 [[{{node gradient_tape/model_7/3dcnn/conv3d_24/ReluGrad}}]] [Op:__inference_train_function_52480]

### Standalone code to reproduce the issue

```shell
x = Conv3D(filters=64, kernel_size=(3,3,3),strides=1,activation=""relu"",dilation_rate=(1,1,1),kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),input_shape=(512, 512, 20, 3))(inputs)
    x = BatchNormalization()(x)
    x = MaxPool3D(pool_size=(2, 2, 1))(x)
    x = Conv3D(filters=128, kernel_size=(3,3,3),strides=1,dilation_rate=(1,1,1),kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),activation=""relu"")(x)
    x = BatchNormalization()(x)
    x = MaxPool3D(pool_size=(2, 2, 1))(x)
    x = Conv3D(filters=256, kernel_size=(2,2,2),strides=1,dilation_rate=(1,1,1), kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),activation=""relu"")(x)
    x = BatchNormalization()(x)
    x = Conv3D(filters=512, kernel_size=(2,2,2),strides=2,dilation_rate=(1,1,1), activation=""relu"")(x)
```


### Relevant log output

_No response_</details>"
60856,"CopyTensor::ViaDMA function, allocator type sometimes not match actual input underlying memory type","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0rc0

### Custom Code

Yes

### OS Platform and Distribution

CentOS Linux 7

### Mobile device

_No response_

### Python version

3.7.5

### Bazel version

bazel 3.7.2

### GCC/Compiler version

gcc-9

### CUDA/cuDNN version

cuda 11, cudnn 8

### GPU model and memory

Tesla V100S

### Current Behaviour?

In `CopyTensor::ViaDMA`, `alloc_attr `decides the direction of memory copy. However, sometimes `alloc_attr `does not keep the same as the Tensor pointer's underlying memory type. In my case,`src_alloc_attr.on_host()` is `True`, but `input->GetMemoryType()` equals to `kDevice`. So this results the memory copy direction in this function is cpu->gpu, but actually the direction should be gpu -> gpu. 
I think this bug does not reveal is because the cuda driver api, like `cuMemcpyHtoD()`, does not care about the direction if it's H to D or others, it only cares about the pointer attribute, if the src pointer is on device and dst pointer is also on device, even if we call `cuMemcpyHtoD()`, cuda driver would still do D to D copy. This feature would cover many bugs. 

I haven't figured out where did the `on_host `attribute is set. From my understanding so far, same allocator object would be reused on different tensors, but the `on_host `attribute is one-way, once it's been set `on_host`, it cannot be unset later. This might cause some issue? Also, why wouln't we just use `input->GetMemoryType()` to decieds the memory copy direction, instead of the `on_host `attribute of `alloc_attr`

I meet this issue when I run horovod unit test case. Add some log message in 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/copy_tensor.cc#L219
, such as 
```
if(!src_alloc_attr.on_host() && (input->GetMemoryType()==AllocatorMemoryType::kHostPageable || input->GetMemoryType()==AllocatorMemoryType::kHostPinned)){
    std::cout<<""!!!!!!! src alloc not on host, but input mem type is on host""<< std::endl;
  }
  if( src_alloc_attr.on_host() && input->GetMemoryType()==AllocatorMemoryType::kDevice) {
    std::cout<<""!!!!!!! src alloc on host, but input mem is on device"" << std::endl;
  }

  if(!dst_alloc_attr.on_host() && (output->GetMemoryType()==AllocatorMemoryType::kHostPageable || output->GetMemoryType()==AllocatorMemoryType::kHostPinned)){
    std::cout<<""!!!!!!! dst alloc not on host, but output mem type is on host""<< std::endl;
  }
  if( dst_alloc_attr.on_host() && output->GetMemoryType()==AllocatorMemoryType::kDevice) {
    std::cout<<""!!!!!!! dst alloc on host, but output mem is on device"" << std::endl;
  }

```
For me, I ran horovod `alltoall Op` unit test case to reproduce this issue. But this issue might reveal in other cases.

### Standalone code to reproduce the issue

```shell
Run horovod unit test case can reproduce this issue:
https://github.com/horovod/horovod/blob/master/test/parallel/test_tensorflow.py
horovodrun --mpi -np 2 pytest -s -v test_tensorflow.py::TensorFlowTests::test_horovod_alltoall_gpu.
```


### Relevant log output

_No response_</details>"
60855,Ensuring SavedModel is in inference mode,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Debian GNU/Linux 10

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

9

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am saving a model via `model.save` from Keras, then later loading the graph in C++ and running inference through it.

How can I ensure that the graph is in inference mode? In C++, I can only invoke the graph via feed and fetch names, and there is no feed name for the `is_training` parameter. This is important for batchnorm.

I've copied the output from `saved_model_cli` below.

### Standalone code to reproduce the issue

```shell
signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID
        shape: unknown_rank
        name: NoOp
  Method name is:

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['args_0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 19, 19, 7)
        name: serving_default_args_0:0
    inputs['args_1'] tensor_info:
        dtype: DT_HALF
        shape: (-1, 1)
        name: serving_default_args_1:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['output_1'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 362)
        name: StatefulPartitionedCall:0
    outputs['output_2'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 2)
        name: StatefulPartitionedCall:1
    outputs['output_3'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 19, 19, 1)
        name: StatefulPartitionedCall:2
    outputs['output_4'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 800)
        name: StatefulPartitionedCall:3
    outputs['output_5'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: StatefulPartitionedCall:4
  Method name is: tensorflow/serving/predict

Concrete Functions:
  Function Name: '__call__'
    Option #1
      Callable with:
        Argument #1
          board_state: TensorSpec(shape=(None, 19, 19, 7), dtype=tf.float32, name='board_state')
        Argument #2
          game_state: TensorSpec(shape=(None, 1), dtype=tf.float16, name='game_state')
        Argument #3
          DType: bool
          Value: True
    Option #2
      Callable with:
        Argument #1
          board_state: TensorSpec(shape=(None, 19, 19, 7), dtype=tf.float32, name='board_state')
        Argument #2
          game_state: TensorSpec(shape=(None, 1), dtype=tf.float16, name='game_state')
        Argument #3
          DType: bool
          Value: False

  Function Name: '_default_save_signature'
    Option #1
      Callable with:
        Argument #1
          args_0: TensorSpec(shape=(None, 19, 19, 7), dtype=tf.float32, name='args_0')
        Argument #2
          args_1: TensorSpec(shape=(None, 1), dtype=tf.float16, name='args_1')

  Function Name: 'call_and_return_all_conditional_losses'
    Option #1
      Callable with:
        Argument #1
          board_state: TensorSpec(shape=(None, 19, 19, 7), dtype=tf.float32, name='board_state')
        Argument #2
          game_state: TensorSpec(shape=(None, 1), dtype=tf.float16, name='game_state')
        Argument #3
          DType: bool
          Value: False
    Option #2
      Callable with:
        Argument #1
          board_state: TensorSpec(shape=(None, 19, 19, 7), dtype=tf.float32, name='board_state')
        Argument #2
          game_state: TensorSpec(shape=(None, 1), dtype=tf.float16, name='game_state')
        Argument #3
          DType: bool
          Value: True

  Function Name: 'infer_float'
    Option #1
      Callable with:
        Argument #1
          board_state: TensorSpec(shape=(None, 19, 19, 7), dtype=tf.float32, name='board_state')
        Argument #2
          game_state: TensorSpec(shape=(None, 1), dtype=tf.float32, name='game_state')

  Function Name: 'infer_mixed'
    Option #1
      Callable with:
        Argument #1
          board_state: TensorSpec(shape=(None, 19, 19, 7), dtype=tf.float16, name='board_state')
        Argument #2
          game_state: TensorSpec(shape=(None, 1), dtype=tf.float16, name='game_state')
```


### Relevant log output

_No response_</details>"
60854,W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

Python 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) 

### Bazel version

bazel 5.3.2

### GCC/Compiler version

gcc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0

### CUDA/cuDNN version

11.2/8 in conda env

### GPU model and memory

laptop 3080 RTX

### Current Behaviour?

A bug happened!

### Standalone code to reproduce the issue

```shell
I have tensorflow 2 installed and also from the code below I see cudnn 8 is found. 


(samurai) mona@ard-gpu-01:~/samurai$ cat cudnn_test.py 
import tensorflow as tf

sys_details = tf.sysconfig.get_build_info()
cuda_version = sys_details[""cuda_version""]
print(cuda_version)

cudnn_version = sys_details[""cudnn_version""]
print(cudnn_version)


cuda_compute_capabilities = sys_details[""cuda_compute_capabilities""]
print(cuda_compute_capabilities)
(samurai) mona@ard-gpu-01:~/samurai$ python cudnn_test.py 
11.2
8
['sm_35', 'sm_50', 'sm_60', 'sm_70', 'sm_75', 'compute_80']
```

However, when I run the following command, I get an error that cudnn 8 is not found.

```
(samurai) mona@ard-gpu-01:~/samurai$ python train_samurai.py --config configs/samurai/samurai.txt --datadir data/duck/ --basedir . --expname duck_test --gpu 0
Namespace(config=None, basedir='.', expname='duck_test', batch_size=1024, learning_rate=0.0001, epochs=150, steps_per_epoch=2000, gpu='0', tpu=None, debug=False, profile=False, perturb=1.0, raw_noise_std=0.0, coarse_samples=64, linear_disparity_sampling=False, fine_samples=128, fourier_frequency=10, direction_fourier_frequency=4, random_encoding_offsets=True, fine_net_width=128, fine_net_depth=8, coarse_net_width=128, coarse_net_depth=6, appearance_latent_dim=32, diffuse_latent_dim=24, fix_diffuse=True, camera_distribution='sphere', use_fully_random_cameras=False, random_cameras_per_view=4, min_softmax_scaler=1.0, max_softmax_scaler=10.0, camera_weight_update_lr=0.3, camera_weight_update_momentum=0.75, bounding_size=0.5, resolution_factor=4, advanced_loss_done=80000, network_gradient_norm_clipping=0.1, camera_gradient_norm_clipping=-1, not_learn_r=False, not_learn_t=False, not_learn_f=False, edge_align_step=200, num_edge_align_steps=50, pretrained_camera_poses_folder=None, start_f_optimization=90000, start_fourier_anneal=0, finish_fourier_anneal=50000, slow_scheduler_decay=100000, brdf_schedule_decay=40000, lambda_smoothness=0.01, smoothness_bound_dividier=200, coarse_distortion_lambda=0.001, fine_distortion_lambda=0, normal_direction_lambda=0.005, mlp_normal_direction_lambda=0.0003, disable_posterior_scaling=False, disable_mask_uncertainty=True, lambda_brdf_decoder_smoothness=0.1, lambda_brdf_decoder_sparsity=0.01, camera_lr=0.003, camera_lr_decay=70, camera_regularization=0.1, aim_center_regularization=10.0, camera_rotation='lookat', learn_camera_offsets=True, basecolor_metallic=True, skip_decomposition=False, compose_on_white=True, rotating_object=False, single_env=False, brdf_preintegration_path='data/neural_pil/BRDFLut.hdr', illumination_network_path='data/neural_pil/illumination-network', datadir='data/duck/', max_resolution_dimension=400, test_holdout=16, dataset='samurai', load_gt_poses=False, canonical_pose=0, log_step=100, weights_epoch=5, validation_epoch=5, testset_epoch=150, video_epoch=50, lrate_decay=300, render_only=False)
2023-06-13 15:35:10.002485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-06-13 15:35:10.022702: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/home/mona/MVTec/HALCON-23.05-Progress//lib/x64-linux:/usr/local/cuda-11.7/lib64:/home/mona/onnx-tensorrt/build:
2023-06-13 15:35:10.022715: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
Utilizing 0 GPUs for training.
2023-06-13 15:35:11.092766: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
(70, 3)
Model: ""sequential_12""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 MappingNetwork/Layer_0 (Den  (None, 128)              16512     
 se)                                                             
                                                                 
 MappingNetwork/Layer_1 (Den  (None, 128)              16512     
 se)                                                             
                                                                 
 MappingNetwork/Final (Dense  (None, 768)              99072     
 )                                                               
                                                                 
 reshape_1 (Reshape)         (None, 2, 3, 128)         0         
                                                                 
=================================================================
Total params: 132,096
Trainable params: 132,096
Non-trainable params: 0
_________________________________________________________________
Model: ""sequential_13""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 ConditionalNetwork/Dense1 (  (None, 32)               192       
 Dense)                                                          
                                                                 
 ConditionalNetwork/DenseFin  (None, 256)              8448      
 al (Dense)                                                      
                                                                 
 reshape_2 (Reshape)         (None, 2, 128)            0         
                                                                 
=================================================================
Total params: 8,640
Trainable params: 8,640
Non-trainable params: 0
_________________________________________________________________
Found ckpts []
Starting training in epoch 0 at step 0
Start Training...
/home/mona/anaconda3/envs/samurai/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/interpolate_bilinear/gather-bottom_right/GatherV2_grad/Reshape_1:0"", shape=(1024,), dtype=int32), values=Tensor(""gradients/interpolate_bilinear/gather-bottom_right/GatherV2_grad/Reshape:0"", shape=(1024, 1), dtype=float32), dense_shape=Tensor(""gradients/interpolate_bilinear/gather-bottom_right/GatherV2_grad/Cast:0"", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  warnings.warn(
/home/mona/anaconda3/envs/samurai/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/interpolate_bilinear/gather-bottom_left/GatherV2_grad/Reshape_1:0"", shape=(1024,), dtype=int32), values=Tensor(""gradients/interpolate_bilinear/gather-bottom_left/GatherV2_grad/Reshape:0"", shape=(1024, 1), dtype=float32), dense_shape=Tensor(""gradients/interpolate_bilinear/gather-bottom_left/GatherV2_grad/Cast:0"", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  warnings.warn(
/home/mona/anaconda3/envs/samurai/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/interpolate_bilinear/gather-top_right/GatherV2_grad/Reshape_1:0"", shape=(1024,), dtype=int32), values=Tensor(""gradients/interpolate_bilinear/gather-top_right/GatherV2_grad/Reshape:0"", shape=(1024, 1), dtype=float32), dense_shape=Tensor(""gradients/interpolate_bilinear/gather-top_right/GatherV2_grad/Cast:0"", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  warnings.warn(
/home/mona/anaconda3/envs/samurai/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/interpolate_bilinear/gather-top_left/GatherV2_grad/Reshape_1:0"", shape=(1024,), dtype=int32), values=Tensor(""gradients/interpolate_bilinear/gather-top_left/GatherV2_grad/Reshape:0"", shape=(1024, 1), dtype=float32), dense_shape=Tensor(""gradients/interpolate_bilinear/gather-top_left/GatherV2_grad/Cast:0"", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  warnings.warn(
  25/2000 [..............................] - ETA: 42:41 - loss: 1.8824 - loss_camera: 7.2076 - fine_loss: 1.8019   

```
```


### Relevant log output

```shell
(samurai) mona@ard-gpu-01:~/samurai$ lsb_release -a
LSB Version:	core-11.1.0ubuntu4-noarch:security-11.1.0ubuntu4-noarch
Distributor ID:	Ubuntu
Description:	Ubuntu 22.04.2 LTS
Release:	22.04
Codename:	jammy
(samurai) mona@ard-gpu-01:~/samurai$ uname -a
Linux ard-gpu-01 5.19.0-43-generic #44~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon May 22 13:39:36 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
```

```
(samurai) mona@ard-gpu-01:~/samurai$ nvidia-smi
Tue Jun 13 15:38:44 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3080 L...    On | 00000000:01:00.0 Off |                  N/A |
| N/A   49C    P8               17W /  90W|    102MiB / 16384MiB |     21%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A      2549      G   /usr/lib/xorg/Xorg                           95MiB |
|    0   N/A  N/A      2983      G   ...libexec/gnome-remote-desktop-daemon        3MiB |
+---------------------------------------------------------------------------------------+
```
```
(samurai) mona@ard-gpu-01:~/samurai$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Wed_Jun__8_16:49:14_PDT_2022
Cuda compilation tools, release 11.7, V11.7.99
Build cuda_11.7.r11.7/compiler.31442593_0

```


The code is from this repo: https://github.com/google/samurai
```
</details>"
60849,Using C api and library,"**System information**
-windows
  if possible):
- TensorFlow Lite in Play windows
- Google Play Services version

**Standalone code to reproduce the issue**
Hi:
    I have compiled tflite's Static library ""libtensorflow-lite. a"" and ""libtensorflowite_c.so"" using cmake according to the official document. However, when I introduced this library and used C to call it, the following error occurred: ""undefined reference to ` __imp_TfLiteModelCreateFromFile '"", undefined reference to`__ IMP_ TfLiteInterpreterOptionsCreate '

Do you know what caused it, or are there any relevant cases

Thanks!
"
60848,tf.data.Dataset.map does not support randomization,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Didn't try.

### Source

binary

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS

### Python version

Python 3.8

It seems like `tf.data.Dataset.map` does not support randomization from `numpy`.
For the sample code, it gives constant output from the `randint` function call, but if you switch the function from `numpy.random.randint` to `tf.random.uniform` (toggle the comment), then you get good randomization behavior. 
I am wondering if this is expected.

### Standalone code to reproduce the issue

```shell
a = [{""key"": i, ""value"": np.random.random()} for i in range(5)]
dict_of_list = pd.DataFrame.from_records(a).to_dict(
    orient=""list"")
keys = dict_of_list.keys()

dataset = tf.data.Dataset.zip(
    tuple([
        tf.data.Dataset.from_tensor_slices(dict_of_list[key]) for key in keys
    ])
)
dataset = dataset.map(lambda *x: {key: x[i] for i, key in enumerate(keys)})

for i in range(2):
    for b in dataset:
        print(""before map:"", i, b)

def test_function(x, f=""key"", low=0, high=100):
    #x[f] = tf.random.uniform(shape=(), minval=1, maxval=5, dtype=tf.int32) # this is randomized
    x[f] = np.random.randint(low=low, high=high) # this is not randomized
    return x

dataset = dataset.map(test_function)

for i in range(2):
    for b in dataset:
        print(""after map:"", i, b)
```


### Relevant log output

```shell
before map: 0 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=0>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.9599878>}
before map: 0 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=1>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.5124935>}
before map: 0 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=2>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.39335275>}
before map: 0 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=3>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.1416868>}
before map: 0 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=4>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.39475128>}
before map: 1 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=0>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.9599878>}
before map: 1 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=1>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.5124935>}
before map: 1 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=2>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.39335275>}
before map: 1 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=3>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.1416868>}
before map: 1 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=4>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.39475128>}
after map: 0 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=44>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.9599878>}
after map: 0 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=44>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.5124935>}
after map: 0 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=44>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.39335275>}
after map: 0 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=44>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.1416868>}
after map: 0 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=44>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.39475128>}
after map: 1 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=44>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.9599878>}
after map: 1 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=44>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.5124935>}
after map: 1 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=44>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.39335275>}
after map: 1 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=44>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.1416868>}
after map: 1 {'key': <tf.Tensor: shape=(), dtype=int32, numpy=44>, 'value': <tf.Tensor: shape=(), dtype=float32, numpy=0.39475128>}
```
</details>"
60847,How to implement CollectiveAllReduceStrategy?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2,9

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am trying use CollectiveAllReduceStrategy using tf 2.9, but couldn't find much information.

Based on following image, is **CollectiveAllReduceStrategy** is **MultiWorkerMirrorStrategy**?

<img width=""1095"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/39809304/fa23ea39-b713-4642-a7cd-6de8bb118c56"">

Thanks


### Standalone code to reproduce the issue

```shell
NA
```


### Relevant log output

```shell
NA
```
</details>"
60846,"Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (4 != 1) Node number 0 (PAD) failed to prepare.","I have converted my DenseNet-121 model to model.tflite and when i am loading it to android app and trying to make predictions, it's giving following errors : java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (4 != 1)
Node number 0 (PAD) failed to prepare.
at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)
at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensorsIfNeeded(NativeInterpreterWrapper.java:308)
at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:248)
at org.tensorflow.lite.InterpreterImpl.runForMultipleInputsOutputs(InterpreterImpl.java:101)
at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:77)
at org.tensorflow.lite.InterpreterImpl.run(InterpreterImpl.java:94)
at org.tensorflow.lite.Interpreter.run(Interpreter.java:77)
at com.example.appleleafdiseasedetection.DiseaseDetector$2.onClick(DiseaseDetector.java:72)
at android.view.View.performClick(View.java:7743)
at android.view.View.performClickInternal(View.java:7720)
at android.view.View.access$3700(View.java:854)
at android.view.View$PerformClick.run(View.java:29111)
at android.os.Handler.handleCallback(Handler.java:938)
at android.os.Handler.dispatchMessage(Handler.java:99)
at android.os.Looper.loopOnce(Looper.java:210)
at android.os.Looper.loop(Looper.java:299)
at android.app.ActivityThread.main(ActivityThread.java:8309)
at java.lang.reflect.Method.invoke(Native Method)
at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:556)
at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1038) how can i solve it?
"
60845,TFLite: On device Training Fails. ERROR: Node number 69 (FlexReluGrad) failed to prepare.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu, 22.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): r2.12


**Provide the text output from tflite_convert**

I used [On device training](https://www.tensorflow.org/lite/examples/on_device_training/overview) guide to convert to tflite model and then stored it as model.tflite


Then I used this c++ code to invoke the ""train"" signature Runner and it fails.


**Standalone code to reproduce the issue** 
```

    std::unique_ptr<FlatBufferModel> model = FlatBufferModel::BuildFromFile(""../model.tflite"");
    tflite::ops::builtin::BuiltinOpResolver builtin_resolver; 
    std::unique_ptr<Interpreter> interpreter;
    if( InterpreterBuilder(*model, builtin_resolver)(&interpreter) != kTfLiteOk){
        printf(""Failed to build interpreter\n"");
        exit(0);
    }

    // RESIZE Input Tensor Shape, if needed, then call ->AllocateTensors()
    if( interpreter->AllocateTensors() != kTfLiteOk){
        printf(""Failed to allocate tensors!\n"");
        exit(0);
    }

    //train model


    SignatureRunner* trainSignatureRunner = interpreter->GetSignatureRunner( ""train"" );
    trainSignatureRunner->AllocateTensors();

    TfLiteTensor* x_input_tensor = trainSignatureRunner->input_tensor( ""x"" );
    TfLiteTensor* y_input_tensor = trainSignatureRunner->input_tensor( ""y"" );


    // fill x_input_tensor
    for(int i = 0; i < 28*28; i++){
        x_input_tensor->data.f[i] = 0.5;
    }

    // fill y_input_tensor
    for(int i = 0; i < 10; i++){
        y_input_tensor->data.f[i] = 0.0;
    }
    y_input_tensor->data.f[3] = 1.0;


    int EPOCHS = 5;
    for(int epoch = 0; epoch < EPOCHS; epoch++){

        trainSignatureRunner->Invoke();

        const TfLiteTensor* loss_output = trainSignatureRunner->output_tensor( ""loss"" );

        printf(""Epoch %d, Loss : %.4f\n"", epoch, loss_output->data.f[0]);
    }



```

I get this output  , after compiling the file using CMakeLists.txt inside the /lite/examples/minimal/ directory and running the executable

```
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
ERROR: Node number 69 (FlexReluGrad) failed to prepare.
Epoch 0, Loss : 0.7413
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
ERROR: Node number 69 (FlexReluGrad) failed to prepare.
Epoch 1, Loss : 0.7413

..... same for all epochs
```


I did refer to https://www.tensorflow.org/lite/guide/ops_select and I tried building the flex delegate library using 

```
bazel build -c opt --config=monolithic tensorflow/lite/delegates/flex:tensorflowlite_flex
```
This runs for a long time, and suddenly the terminal gets killed after finishing like 70%. 


Can someone suggest a way to include the flex delegate library using some hacks in CMakeLists.txt or any other alterative way
"
60844,"java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (4 != 1) Node number 0 (PAD) failed to prepare.","I have converted my DenseNet-121 model to model.tflite and when i am loading it to android app and trying to make predictions, it's giving following errors :                                                                                                                                                                                  java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (4 != 1)
Node number 0 (PAD) failed to prepare.
	at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)
	at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensorsIfNeeded(NativeInterpreterWrapper.java:308)
	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:248)
	at org.tensorflow.lite.InterpreterImpl.runForMultipleInputsOutputs(InterpreterImpl.java:101)
	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:77)
	at org.tensorflow.lite.InterpreterImpl.run(InterpreterImpl.java:94)
	at org.tensorflow.lite.Interpreter.run(Interpreter.java:77)
	at com.example.appleleafdiseasedetection.DiseaseDetector$2.onClick(DiseaseDetector.java:72)
	at android.view.View.performClick(View.java:7743)
	at android.view.View.performClickInternal(View.java:7720)
	at android.view.View.access$3700(View.java:854)
	at android.view.View$PerformClick.run(View.java:29111)
	at android.os.Handler.handleCallback(Handler.java:938)
	at android.os.Handler.dispatchMessage(Handler.java:99)
	at android.os.Looper.loopOnce(Looper.java:210)
	at android.os.Looper.loop(Looper.java:299)
	at android.app.ActivityThread.main(ActivityThread.java:8309)
	at java.lang.reflect.Method.invoke(Native Method)
	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:556)
	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1038)                                                                                                                how can i solve it?
                                                                                                                                                                         "
60843,OMP_PROC_BIND or OMP_PLACES either ignored or respected incorrectly,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

5.1.1

### GCC/Compiler version

11.3.0

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

When I run TensorFlow on CPU and try to enable core binding with `OMP_PROC_BIND=close`, all threads get bound to core 0 (rather than thread 0 to core 0, thread 1 to core 1, etc).

Expected output:

```
$ OMP_PROC_BIND=true OMP_PLACES=cores python tf_example.py
Inter_op_threads: 1
Intra_op_threads: 4
Thread count: 4
Child affinity is {96}
Child affinity is {97}
Child affinity is {98}
Child affinity is {99}
```
I.e. I'd expect four threads, bound to subsequent cores. Now, I guess TensorFlow simply uses some threads for management of the framework. Though I'm surprised by the large number of threads, 11 extra threads on top of the Intra_op_thread count, see the log output for the custom TF-2.11 case, this is not really an 'issue' (although one might wonder how management threads ought to behave, they should probably remain unbound even if the compute threads are bound 1 per core).

What I'm seeing however is that with our custom built TF-2.11, all threads are bound to the first core in my cgroup (core ID 96 in this case). For tf-nightly, all binding is completely ignored.

Not sure if it's useful, but the Custom built had this build command:

<details>
<summary>build command</summary>

```
bazel --output_user_root=/tmp/jenkins/build/TensorFlow/2.11.0/foss-2022a/TensorFlow/bazel-root --local_startup_timeout_secs=300 --host_jvm_args=-Xms512m --host_jvm_args=-Xmx4096m build --config=noaws --config=nogcp --config=nohdfs --compilation_mode=opt --config=opt --subcommands --verbose_failures --jobs=128 --copt=""-fPIC"" --distinct_host_configuration=false --action_env=CPATH='/sw/arch/RHEL8/EB_production/2022/software/cURL/7.83.0-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/double-conversion/3.2.0-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/flatbuffers/2.0.7-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/giflib/5.2.1-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/hwloc/2.7.1-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/ICU/71.1-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/JsonCpp/1.9.5-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/libjpeg-turbo/2.1.3-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/libpng/1.6.37-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/LMDB/0.9.29-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/nsync/1.25.0-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/protobuf/3.19.4-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/pybind11/2.9.2-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/snappy/1.1.9-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/SQLite/3.38.3-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/zlib/1.2.12-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/OpenSSL/1.1/include' --host_action_env=CPATH='/sw/arch/RHEL8/EB_production/2022/software/cURL/7.83.0-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/double-conversion/3.2.0-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/flatbuffers/2.0.7-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/giflib/5.2.1-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/hwloc/2.7.1-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/ICU/71.1-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/JsonCpp/1.9.5-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/libjpeg-turbo/2.1.3-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/libpng/1.6.37-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/LMDB/0.9.29-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/nsync/1.25.0-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/protobuf/3.19.4-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/pybind11/2.9.2-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/snappy/1.1.9-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/SQLite/3.38.3-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/zlib/1.2.12-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/OpenSSL/1.1/include' --action_env=LIBRARY_PATH='/sw/arch/RHEL8/EB_production/2022/software/cURL/7.83.0-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/double-conversion/3.2.0-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/flatbuffers/2.0.7-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/giflib/5.2.1-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/hwloc/2.7.1-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/ICU/71.1-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/JsonCpp/1.9.5-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/libjpeg-turbo/2.1.3-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/libpng/1.6.37-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/LMDB/0.9.29-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/nsync/1.25.0-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/protobuf/3.19.4-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/pybind11/2.9.2-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/snappy/1.1.9-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/SQLite/3.38.3-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/zlib/1.2.12-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/OpenSSL/1.1/lib' --host_action_env=LIBRARY_PATH='/sw/arch/RHEL8/EB_production/2022/software/cURL/7.83.0-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/double-conversion/3.2.0-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/flatbuffers/2.0.7-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/giflib/5.2.1-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/hwloc/2.7.1-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/ICU/71.1-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/JsonCpp/1.9.5-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/libjpeg-turbo/2.1.3-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/libpng/1.6.37-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/LMDB/0.9.29-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/nsync/1.25.0-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/protobuf/3.19.4-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/pybind11/2.9.2-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/snappy/1.1.9-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/SQLite/3.38.3-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/zlib/1.2.12-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/OpenSSL/1.1/lib' --action_env=PYTHONNOUSERSITE='1' --host_action_env=PYTHONNOUSERSITE='1' --action_env=PYTHONPATH --host_action_env=PYTHONPATH  //tensorflow/tools/pip_package:build_pip_package
```

</details>

I'm not 100% sure of all the details of the custom build, I used EasyBuild to build TF from sources, and the build recipy wasn't made by me.

I don't understand enough of the TensorFlow threading model to know how to debug this issue. My specific questions would be:

- Why does my custom TF 2.11 build bind all threads to one core? Other GOMP-based packages (e.g. `scipy`) do show correct binding on my system, with the same (OMP_) environment variables.
- Why does tf-nightly not bind threads at all?

Any general explanations of which potential threading models can be used in TF are also welcome.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.config.threading.set_inter_op_parallelism_threads(1)
tf.config.threading.set_intra_op_parallelism_threads(4)

print(f""Inter_op_threads: {tf.config.threading.get_inter_op_parallelism_threads()}"")
print(f""Intra_op_threads: {tf.config.threading.get_intra_op_parallelism_threads()}"")

A = tf.random.normal([20000,20000])
for i in range(0,1):
    B = tf.multiply(A,A)

import psutil
import os

current_process = psutil.Process()
threads = current_process.threads()
print(f""Thread count: {len(threads)}"")
for thread in threads:
    print('Child affinity is {}'.format(os.sched_getaffinity(thread.id)))
```


### Relevant log output

```shell
# output for custom build TF-2.11
$ OMP_PROC_BIND=true OMP_PLACES=cores python tf_example.py
Inter_op_threads: 1
Intra_op_threads: 4
Thread count: 15
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}

# output for tf-nightly
# Not sure what threading model tf-nightly uses, so I've set both OMP and KMP variables:
$ OMP_PROC_BIND=true OMP_PLACES=cores KMP_AFFINITY=granularity=fine,verbose,compact,1,0 python tf_example.py

Inter_op_threads: 1
Intra_op_threads: 4
Thread count: 14
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}


# Note that this is generated on an HPC system in which I'm in a CGROUP with access to core 96-127, hence the core IDs start at 96.
```
</details>"
60841,"Get deadlock after Predict(cuda10.0, cudnn7.6.5,  Tesla T4 GPU)","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.2 + tfserving2.2

### Custom Code

Yes

### OS Platform and Distribution

centos7

### Mobile device

_No response_

### Python version

3.6

### Bazel version

3.7.2

### GCC/Compiler version

7.5

### CUDA/cuDNN version

7.6.5

### GPU model and memory

15G

### Current Behaviour?

In our inference service, when executes the predict interface(predictor_->Predict(...)), it gets deadlock.

`
std::unique_ptr<tensorflow::serving::TensorflowPredictor> predictor_;
predictor_->Predict(opt, core_.get(), predict_req, &predict_resp, run_metadata.get());
`

Here is the pstack：

obviously, it's in async execute, and waiting for something

Thread 167 (Thread 0x7f4b1bfa7700 (LWP 81084)):
#0  0x00007f5059939c09 in syscall () from /usr/lib64/libc.so.6
#1  0x00007f505feb1bbb in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) () from /home/qspace/upload/libtensorflow_serving.so
#2  0x00007f505feaedf9 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) () from /home/qspace/upload/libtensorflow_serving.so
#3  0x00007f505feafeeb in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from /home/qspace/upload/libtensorflow_serving.so
#4  0x00007f505feb03c3 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /home/qspace/upload/libtensorflow_serving.so
#5  0x00007f506168349c in tensorflow::DirectSession::WaitForNotification(tensorflow::Notification*, long long) () from /home/qspace/upload/libtensorflow_serving.so
#6  0x00007f50616834ed in tensorflow::DirectSession::WaitForNotification(tensorflow::Notification*, tensorflow::DirectSession::RunState*, tensorflow::CancellationManager*, long long) () from /home/qspace/upload/libtensorflow_serving.so
#7  0x00007f5061693bb5 in tensorflow::DirectSession::RunInternal(long long, tensorflow::RunOptions const&, tensorflow::CallFrameInterface*, tensorflow::DirectSession::ExecutorsAndKeys*, tensorflow::RunMetadata*, tensorflow::thread::ThreadPoolOptions const&) () from /home/qspace/upload/libtensorflow_serving.so
#8  0x00007f5061695dd5 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, tensorflow::thread::ThreadPoolOptions const&) () from /home/qspace/upload/libtensorflow_serving.so
#9  0x00007f5061681313 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/qspace/upload/libtensorflow_serving.so
#10 0x00007f5067131cdc in tensorflow::serving::ServingSessionWrapper::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/qspace/upload/libtensorflow_serving.so
#11 0x00007f506714092b in tensorflow::serving::internal::RunPredict(tensorflow::RunOptions const&, tensorflow::MetaGraphDef const&, tensorflow::serving::optional<long long> const&, tensorflow::serving::internal::PredictResponseTensorSerializationOption, tensorflow::Session*, tensorflow::serving::PredictRequest const&, tensorflow::serving::PredictResponse*, tensorflow::RunMetadata*) () from /home/qspace/upload/libtensorflow_serving.so
#12 0x00007f5067131aa0 in tensorflow::serving::TensorflowPredictor::PredictWithModelSpec(tensorflow::RunOptions const&, tensorflow::serving::ServerCore*, tensorflow::serving::ModelSpec const&, tensorflow::serving::PredictRequest const&, tensorflow::serving::PredictResponse*, tensorflow::RunMetadata*) () from /home/qspace/upload/libtensorflow_serving.so
#13 0x00007f5067131c81 in tensorflow::serving::TensorflowPredictor::Predict(tensorflow::RunOptions const&, tensorflow::serving::ServerCore*, tensorflow::serving::PredictRequest const&, tensorflow::serving::PredictResponse*, tensorflow::RunMetadata*) () from /home/qspace/upload/libtensorflow_serving.so
#14 0x0000000002bd91b1 in mmfinderbd::RankModel::Predict (this=0x327bf080, req=..., resp=0x7f47042735d0) at bdegateway/mmfinder/mmfinderbdetfsvr/models/tf/rank_tf_model.cpp:525
#15 0x0000000002b838cc in mmfinderbd::ServerCoreSingleModel::Predict (this=0x2c176aa0 <mmfinderbd::ServerCore::Instance()::instance>, req=..., resp=...) at bdegateway/mmfinder/mmfinderbdetfsvr/core/server_core_single_model.cpp:365
#16 0x0000000002b5df8b in MMFinderBdeTfSvrServiceImpl_PB::InferImpl (this=0x7f4ad84cbec0, head_uin=<optimized out>, req=..., resp=0x7f47042735d0) at bdegateway/mmfinder/mmfinderbdetfsvr/mmfinderbdetfsvrserviceimpl_pb.cpp:93
#17 0x0000000002b731d8 in MMFinderBdeTfSvrDispatcher_PB::Infer (this=this@entry=0x7f4ad84cbe60, uin=<optimized out>, req_buffer=req_buffer@entry=0x7f4ad84cb868, resp_buffer=resp_buffer@entry=0x7f4ad84cb870) at bazel-out/cd7t-opt/genfiles/bdegateway/mmfinder/mmfinderbdetfsvr/skgenerated/sk_mmfinderbdetfsvrdispatcher.pb.cpp:1366
#18 0x0000000002b78569 in MMFinderBdeTfSvrDispatcher_PB::Dispatch (this=this@entry=0x7f4ad84cbe60) at bazel-out/cd7t-opt/genfiles/bdegateway/mmfinder/mmfinderbdetfsvr/skgenerated/sk_mmfinderbdetfsvrdispatcher.pb.cpp:398
#19 0x0000000002b5a91e in MMFinderBdeTfSvrServer::SKServerProc (this=<optimized out>, ctrl_info=0x7f470421b820, ctx=0x7f470421b7a0, in_pkg=0x7f42681054a0, out_pkg=0x7f42681054e0, args=<optimized out>) at ./bdegateway/mmfinder/mmfinderbdetfsvr/mmfinderbdetfsvrserver.h:44
#20 0x000000000671a6c8 in SMCoWorkerMt::CoWorkerIORun (this=0x330b4670, self=0x7f470421b6d0) at comm2/summer/smcoworker.cpp:1138
#21 0x0000000007cc679e in operator() (this=0x7f470421b908) at /home/mmdev/gcc7/lib/gcc/x86_64-pc-linux-gnu/7.5.0/../../../../include/c++/7.5.0/bits/std_function.h:706
#22 CoRoutineFunc (co=0x7f470421b8f0) at basic/colib/co_routine.cpp:601
#23 0x0000000000000000 in ?? ()

and what does it exactly waiting for...

Thread 301 (Thread 0x7f4b09dcf700 (LWP 80869)):
#0  0x00007f5059939c09 in syscall () from /usr/lib64/libc.so.6
#1  0x0000000005bf5111 in WaitUntil (t=..., val=0, v=0x7f45f046b750) at mm3rd/abseil-cpp/absl/synchronization/internal/waiter.cc:107
#2  absl::lts_2020_02_25::synchronization_internal::Waiter::Wait (this=this@entry=0x7f45f046b750, t=t@entry=...) at mm3rd/abseil-cpp/absl/synchronization/internal/waiter.cc:151
#3  0x0000000005bf5052 in AbslInternalPerThreadSemWait (t=...) at mm3rd/abseil-cpp/absl/synchronization/internal/per_thread_sem.cc:93
#4  0x00007f5066f87b6d in absl::Mutex::Block(absl::base_internal::PerThreadSynch*) () from /home/qspace/upload/libtensorflow_serving.so
#5  0x00007f5066f8889e in absl::Mutex::LockSlowLoop(absl::SynchWaitParams*, int) () from /home/qspace/upload/libtensorflow_serving.so
#6  0x00007f5066f88dc2 in absl::Mutex::LockSlowWithDeadline(absl::MuHowS const*, absl::Condition const*, absl::synchronization_internal::KernelTimeout, int) () from /home/qspace/upload/libtensorflow_serving.so
#7  0x00007f505f0897ec in absl::Mutex::LockSlow(absl::MuHowS const*, absl::Condition const*, int) () from /home/qspace/upload/libtensorflow_serving.so
#8  0x00007f50601c56fe in stream_executor::gpu::CUDABlas::DoBlasGemm(stream_executor::Stream*, stream_executor::blas::Transpose, stream_executor::blas::Transpose, unsigned long long, unsigned long long, unsigned long long, float, stream_executor::DeviceMemory<float> const&, int, stream_executor::DeviceMemory<float> const&, int, float, stream_executor::DeviceMemory<float>*, int) () from /home/qspace/upload/libtensorflow_serving.so
#9  0x00007f5060290c13 in stream_executor::Stream::ThenBlasGemm(stream_executor::blas::Transpose, stream_executor::blas::Transpose, unsigned long long, unsigned long long, unsigned long long, float, stream_executor::DeviceMemory<float> const&, int, stream_executor::DeviceMemory<float> const&, int, float, stream_executor::DeviceMemory<float>*, int) () from /home/qspace/upload/libtensorflow_serving.so
#10 0x00007f5063aebc4f in tensorflow::LaunchMatMul<Eigen::GpuDevice, float, true>::launch(tensorflow::OpKernelContext*, tensorflow::Tensor const&, tensorflow::Tensor const&, Eigen::array<Eigen::IndexPair<long>, 1ul> const&, std::vector<long long, std::allocator<long long> >*, bool, tensorflow::Tensor*) () from /home/qspace/upload/libtensorflow_serving.so
#11 0x00007f5063aec42d in tensorflow::MatMulOp<Eigen::GpuDevice, float, true>::Compute(tensorflow::OpKernelContext*) () from /home/qspace/upload/libtensorflow_serving.so
#12 0x00007f5061878296 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) () from /home/qspace/upload/libtensorflow_serving.so
#13 0x00007f50614a80bf in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) () from /home/qspace/upload/libtensorflow_serving.so
#14 0x00007f50614a8c7f in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(absl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8ul, std::allocator<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode> >*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#2}>::_M_invoke(std::_Any_data const&) () from /home/qspace/upload/libtensorflow_serving.so
#15 0x00007f506189a71f in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::ScheduleWithHint(std::function<void ()>, int, int) () from /home/qspace/upload/libtensorflow_serving.so
#16 0x00007f506189dd1b in tensorflow::thread::ThreadPool::Schedule(std::function<void ()>) () from /home/qspace/upload/libtensorflow_serving.so
#17 0x00007f5061681bb3 in std::_Function_handler<void (std::function<void ()>), tensorflow::DirectSession::RunInternal(long long, tensorflow::RunOptions const&, tensorflow::CallFrameInterface*, tensorflow::DirectSession::ExecutorsAndKeys*, tensorflow::RunMetadata*, tensorflow::thread::ThreadPoolOptions const&)::{lambda(tensorflow::DirectSession::PerPartitionExecutorsAndLib const&, tensorflow::Executor::Args*)#7}::operator()(tensorflow::DirectSession::PerPartitionExecutorsAndLib const&, tensorflow::Executor::Args*) const::{lambda(std::function<void ()>)#1}>::_M_invoke(std::_Any_data const&, std::function<void ()>&&) () from /home/qspace/upload/libtensorflow_serving.so
#18 0x00007f506149ac84 in tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(absl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8ul, std::allocator<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode> >*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*) [clone .part.508] () from /home/qspace/upload/libtensorflow_serving.so
#19 0x00007f50614a3a84 in tensorflow::(anonymous namespace)::ExecutorState::NodeDone(tensorflow::Status const&, absl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8ul, std::allocator<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode> >*, tensorflow::NodeExecStatsInterface*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*) () from /home/qspace/upload/libtensorflow_serving.so
#20 0x00007f50614a8e4f in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)::{lambda()#6}::operator()() const () from /home/qspace/upload/libtensorflow_serving.so
#21 0x00007f50614fcbd0 in std::_Function_handler<void (tensorflow::Status const&), tensorflow::(anonymous namespace)::IntraProcessRecvAsyncImpl(tensorflow::DeviceMgr const*, tensorflow::LocalRendezvous*, tensorflow::RendezvousInterface::ParsedKey const&, tensorflow::RendezvousInterface::Args const&, std::function<void (tensorflow::Status const&, tensorflow::RendezvousInterface::Args const&, tensorflow::RendezvousInterface::Args const&, tensorflow::Tensor const&, bool)>)::{lambda(tensorflow::Status const&, tensorflow::RendezvousInterface::Args const&, tensorflow::RendezvousInterface::Args const&, tensorflow::Tensor const&, bool)#2}::operator()(tensorflow::Status const&, tensorflow::RendezvousInterface::Args const&, tensorflow::RendezvousInterface::Args const&, tensorflow::Tensor const&, bool)::{lambda(tensorflow::Status const&)#1}>::_M_invoke(std::_Any_data const&, tensorflow::Status const&) () from /home/qspace/upload/libtensorflow_serving.so
#22 0x00007f506186fa29 in tensorflow::GPUUtil::CopyCPUTensorToGPU(tensorflow::Tensor const*, tensorflow::DeviceContext const*, tensorflow::Device*, tensorflow::Tensor*, std::function<void (tensorflow::Status const&)>, bool)::{lambda()#2}::operator()() const () from /home/qspace/upload/libtensorflow_serving.so
#23 0x00007f506189c3e1 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) () from /home/qspace/upload/libtensorflow_serving.so
#24 0x00007f50618990f3 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /home/qspace/upload/libtensorflow_serving.so
#25 0x0000000007f0a9df in std::execute_native_thread_routine (__p=0x7f4a83d29490) at ../../../../../gcc-7.5.0/libstdc++-v3/src/c++11/thread.cc:83
#26 0x00007f505a533dc5 in start_thread () from /usr/lib64/libpthread.so.0
#27 0x00007f505993f74d in clone () from /usr/lib64/libc.so.6

GPU Info:
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.116.03   Driver Version: 525.116.03   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+====
|   0  Tesla T4            On   | 00000000:00:0B.0 Off |                    0 |
| N/A   52C    P0    36W /  70W |   4695MiB / 15360MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+


Please give me some advice! 

### Standalone code to reproduce the issue

```shell
std::unique_ptr<tensorflow::serving::TensorflowPredictor> predictor_;
predictor_->Predict(opt, core_.get(), predict_req, &predict_resp, run_metadata.get());
```


### Relevant log output

_No response_</details>"
60840,Functional Bug：Could not interpret serialized activation function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf2.12.0

### Custom Code

Yes

### OS Platform and Distribution

MacOs

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

#### Output

```
Could not interpret serialized activation function: Tensor(""conv1a/BiasAdd:0"", shape=(None, 224, 224, 64), dtype=float32)
```

#### Document

| `activation` | Activation function to use. If you don't specify anything, no activation is applied (see [`keras.activations`](https://www.tensorflow.org/api_docs/python/tf/keras/activations)). |
| ------------ | ------------------------------------------------------------ |

### Standalone code to reproduce the issue
```py
x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, activation=""deserialize"", padding=""same"", name=""conv1a"")(input_tensor)
```

```py
x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, activation=""serialize"", padding=""same"", name=""conv1a"")(input_tensor)
```


### Relevant log output

_No response_</details>"
60839,Documentation Bug：the description of padding,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf2.12.0

### Custom Code

Yes

### OS Platform and Distribution

MacOs

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

#### Output

```
ValueError: The `padding` argument must be a tuple of 2 integers. Received: {'padding': 2}
```

#### Document

| `padding` | Int, or tuple of int (length 2), or dictionary. |
| --------- | ----------------------------------------------- |




### Standalone code to reproduce the issue

```shell
input_shape = (2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
x = ZeroPadding1D({'padding':2})(x)
print(x)
```


### Relevant log output
_No response_
</details>"
60838,FlexCombinedNonMaxSuppression unavailable in flex shared library,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.6

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.2.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I have network that runs with no problem on Android that uses the operator FlexCombinendNonMaxSuprpression. When running the same network and code on an x86 machine, the log output tells me that this operator is not supported by this interpreter.

I have built from source the necessary libraries, libtensorflowlite.so and libtensorflowlite_flex.so

### Standalone code to reproduce the issue

```shell
`
  /* Allocate Tensors */
  retTflite = _interpreter->AllocateTensors();
  if (retTflite == kTfLiteOk)
  {
    _engineReady = true;
  }
`
```


### Relevant log output

```shell
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
ERROR: Node number 276 (FlexCombinedNonMaxSuppression) failed to prepare.

INFO: Failed to apply the default TensorFlow Lite delegate indexed at 0 because of unresolved ops (which could be resolved by another delegate). Ignoring the error, and continuing anyway.
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
ERROR: Node number 276 (FlexCombinedNonMaxSuppression) failed to prepare.
```
</details>"
60836,Image Segmenter | tflite-suuport | AttributeError: type object 'SegmentationOptions' has no attribute 'OutputType',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tflite-support 0.1.0a1

### Custom Code

No

### OS Platform and Distribution

MacOS Ventura 13.4

### Mobile device

_No response_

### Python version

3.8.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The syntax provided for using [Image Segmenter](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_segmenter.md)  did not execute ```processor.SegmentationOptions``` in python environment.

### Possible Fix:
The line 
``` 
segmentation_options = processor.SegmentationOptions(
    output_type=processor.SegmentationOptions.OutputType.CATEGORY_MASK) 
``` 
should have been 
```
segmentation_options = processor.SegmentationOptions(
    output_type=processor.SegmentationOptions.output_type.CATEGORY_MASK) 
```



### Standalone code to reproduce the issue

```shell
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/inference_with_metadata/task_library/image_segmenter.md#step-2-using-the-model-2
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""image_segmenter.py"", line 8, in <module>
    segmentation_options = processor.SegmentationOptions(output_type=processor.SegmentationOptions.OutputType.CATEGORY_MASK)
AttributeError: type object 'SegmentationOptions' has no attribute 'OutputType'
```
</details>"
60835,Tensorflow Lite on Raspberry Pi,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tflite_runtime-2.12.0-cp39-cp39-manylinux2014_armv7l.whl

### Custom Code

Yes

### OS Platform and Distribution

Linux raspbari14 6.1.32-v7+ #1656 SMP Wed Jun  7 11:31:19 BST 2023 armv7l GNU/Linux

### Mobile device

_No response_

### Python version

Python 3.9.2 (default, Mar 12 2021, 04:06:34) 

### Bazel version

_No response_

### GCC/Compiler version

[GCC 10.2.1 20210110] on linux

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

Not working as documented:

`import tflite_runtime.interpreter as tflite`

How to import Tensorflow Lite in python scripts?

### Standalone code to reproduce the issue

```shell
$ python3 -m pip install tflite-runtime
Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple
Collecting tflite-runtime
  Downloading tflite_runtime-2.12.0-cp39-cp39-manylinux2014_armv7l.whl (1.8 MB)
     |████████████████████████████████| 1.8 MB 2.6 MB/s 
Requirement already satisfied: numpy>=1.19.2 in /home/chowkidar/.local/lib/python3.9/site-packages (from tflite-runtime) (1.23.1)
Installing collected packages: tflite-runtime
Successfully installed tflite-runtime-2.12.0
$ python3
Python 3.9.2 (default, Mar 12 2021, 04:06:34) 
[GCC 10.2.1 20210110] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow'
>>> import tflite_runtime.interpreter as tflite
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/chowkidar/.local/lib/python3.9/site-packages/tflite_runtime/interpreter.py"", line 33, in <module>
    from tflite_runtime import _pywrap_tensorflow_interpreter_wrapper as _interpreter_wrapper
ImportError: /usr/lib/arm-linux-gnueabihf/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /home/chowkidar/.local/lib/python3.9/site-packages/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so)
>>>
```


### Relevant log output

_No response_</details>"
60834,"Could not interpret loss function ""NDCG Lambda Weight V2""","A erro happened!
Could not interpret loss function NDCG Lambda Weight V2

-------------------------------------------------------------------
model.compile (

optimizer=optimizer,   

loss=tfr.keras.losses.NDCGLambdaWeightV2(topn=60),
                           
metrics=[tfr.keras.metrics.NDCGMetric(topn=60), tfr.keras.metrics.OPAMetric()]

 )

model.fit(train_dataset, epochs=20)
-------------------------------------------------------------
some error happened in "" mode.fit()""
ValueError: Could not interpret loss function identifier: <tensorflow_ranking.python.keras.losses.NDCGLambdaWeightV2 object at 0x7fa0e84ee6e0>"
60833,Duplicate logging inside custom training loop,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

TF 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Windows WSL Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuDNN version 8600

### GPU model and memory

GTX 1070

### Current Behaviour?

Hey guys.
I am experiencing a strange behavior when I try to print something from inside my custom training loop.
I want to run the code example as a .ipynb from a Windows machine with WSL Ubuntu 20.04. (https://github.com/m5k5/tf-example). 
The code loops over the dataset and prints out the log statements but it somehow resets the steps after a while:
""
Log at step 80
train step at step  81
train step at step  82
train step at step  83
train step at step  84
train step at step  85
train step at step  86
train step at step  0
Log at step 0
train step at step  1
train step at step  2
train step at step  3
train step at step  4
train step at step  5
""

Originally, I have a project where I need to use the WSL to get the GPU processing within Windows. The print statements are used for logging metrics like accuracy and loss. The values themselves are calculated correctly but there are a lot of duplicates that I can not explain.
Does anyone have similar issues?
Thanks in advance!

### Standalone code to reproduce the issue

```shell
https://github.com/m5k5/tf-example
```


### Relevant log output

```shell
Start of epoch 1
2023-06-10 19:30:47.154065: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]
	 [[{{node Placeholder/_1}}]]
2023-06-10 19:30:47.154065: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]
	 [[{{node Placeholder/_1}}]]
2023-06-10 19:30:47.766468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600
2023-06-10 19:30:47.154065: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]
	 [[{{node Placeholder/_1}}]]
2023-06-10 19:30:47.766468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600
2023-06-10 19:30:48.254207: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f2e52d1a840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-06-10 19:30:48.254240: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1070, Compute Capability 6.1
2023-06-10 19:30:48.256794: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-06-10 19:30:48.348961: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
train step at step  0
Log at step 0
train step at step  1
train step at step  2
train step at step  3
train step at step  4
train step at step  5
train step at step  6
train step at step  7
train step at step  8
train step at step  9
train step at step  10
train step at step  11
train step at step  12
train step at step  13
train step at step  14
train step at step  15
train step at step  16
train step at step  17
train step at step  18
train step at step  19
train step at step  20
Log at step 20
train step at step  21
train step at step  22
train step at step  23
train step at step  24
train step at step  25
train step at step  26
train step at step  27
train step at step  28
train step at step  29
train step at step  30
train step at step  31
train step at step  32
train step at step  33
train step at step  34
train step at step  35
train step at step  36
train step at step  37
train step at step  38
train step at step  39
train step at step  40
Log at step 40
train step at step  41
train step at step  42
train step at step  43
train step at step  44
train step at step  45
train step at step  46
train step at step  47
train step at step  48
train step at step  49
train step at step  50
train step at step  51
train step at step  52
train step at step  53
train step at step  54
train step at step  55
train step at step  56
train step at step  57
train step at step  58
train step at step  59
train step at step  60
Log at step 60
train step at step  61
train step at step  62
train step at step  63
train step at step  64
train step at step  65
train step at step  66
train step at step  67
train step at step  68
train step at step  69
train step at step  70
train step at step  71
train step at step  72
train step at step  73
train step at step  74
train step at step  75
train step at step  76
train step at step  77
train step at step  78
train step at step  79
train step at step  80
Log at step 80
train step at step  81
train step at step  82
train step at step  83
train step at step  84
train step at step  85
train step at step  86
train step at step  0
Log at step 0
train step at step  1
train step at step  2
train step at step  3
train step at step  4
train step at step  5
train step at step  6
train step at step  7
train step at step  8
train step at step  9
train step at step  10
train step at step  11
train step at step  12
train step at step  13
train step at step  14
train step at step  15
train step at step  16
train step at step  17
train step at step  18
train step at step  19
train step at step  20
Log at step 20
train step at step  21
train step at step  22
train step at step  23
train step at step  24
train step at step  25
train step at step  26
train step at step  27
train step at step  28
train step at step  29
train step at step  30
train step at step  31
train step at step  32
train step at step  33
train step at step  34
train step at step  35
train step at step  36
train step at step  37
train step at step  38
train step at step  39
train step at step  40
Log at step 40
train step at step  41
train step at step  42
train step at step  43
train step at step  44
train step at step  45
train step at step  46
train step at step  47
train step at step  48
train step at step  49
train step at step  50
train step at step  51
train step at step  52
train step at step  53
train step at step  54
train step at step  55
train step at step  56
train step at step  57
train step at step  58
train step at step  59
train step at step  60
Log at step 60
train step at step  61
train step at step  62
train step at step  63
train step at step  64
train step at step  65
train step at step  66
train step at step  67
train step at step  68
train step at step  69
train step at step  70
train step at step  71
train step at step  72
train step at step  73
train step at step  74
train step at step  75
train step at step  76
train step at step  77
train step at step  78
train step at step  79
train step at step  80
Log at step 80
train step at step  81
train step at step  82
train step at step  83
train step at step  84
train step at step  85
train step at step  86
train step at step  87
train step at step  88
train step at step  89
train step at step  90
train step at step  91
train step at step  92
train step at step  93
train step at step  94
train step at step  95
train step at step  96
train step at step  97
train step at step  98
train step at step  99
train step at step  100
Log at step 100
train step at step  101
train step at step  102
train step at step  103
train step at step  104
train step at step  105
train step at step  106
train step at step  107
train step at step  108
train step at step  109
train step at step  110
train step at step  111
train step at step  112
train step at step  113
train step at step  114
train step at step  115
train step at step  116
train step at step  117
train step at step  118
train step at step  119
train step at step  120
Log at step 120
train step at step  121
train step at step  122
train step at step  123
train step at step  124
train step at step  125
train step at step  126
train step at step  127
train step at step  128
train step at step  129
train step at step  130
train step at step  131
train step at step  132
train step at step  133
train step at step  134
train step at step  135
train step at step  136
train step at step  137
train step at step  138
train step at step  139
train step at step  140
Log at step 140
train step at step  141
train step at step  142
train step at step  143
train step at step  144
train step at step  145
train step at step  146
train step at step  147
train step at step  148
train step at step  149
train step at step  150
train step at step  151
train step at step  152
train step at step  153
train step at step  154
train step at step  155
train step at step  156
train step at step  157
train step at step  158
train step at step  159
train step at step  160
Log at step 160
train step at step  161
train step at step  162
train step at step  163
train step at step  164
train step at step  165
train step at step  166
train step at step  167
train step at step  168
train step at step  169
train step at step  170
train step at step  171
train step at step  172
train step at step  173
train step at step  174
train step at step  0
Log at step 0
train step at step  1
train step at step  2
train step at step  3
train step at step  4
train step at step  5
train step at step  6
train step at step  7
train step at step  8
train step at step  9
train step at step  10
train step at step  11
train step at step  12
train step at step  13
train step at step  14
train step at step  15
train step at step  16
train step at step  17
train step at step  18
train step at step  19
train step at step  20
Log at step 20
train step at step  21
train step at step  22
train step at step  23
train step at step  24
train step at step  25
train step at step  26
train step at step  27
```
</details>"
60832,Tensorflow r2.13 build from source: cudnn_frontend_Operation.h:413:16: error: enumeration value ‘CUDNN_POINTWISE_RECIPROCAL’ not handled in switch [-Werror=switch],"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

r2.13

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

6.2.1

### GCC/Compiler version

11.3.0

### CUDA/cuDNN version

12.1/8.9.2.26

### GPU model and memory

NVIDIA GeForce 940MX

### Current Behaviour?

I was trying to build Tensorflow from source (branch r2.13). During the build the following error happend which halted the build execution:
ERROR: /home/vyepishov/Documents/dev/git/tensorflow/tensorflow/compiler/xla/stream_executor/cuda/BUILD:377:11: Compiling tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/xla/stream_executor/cuda/_objs/cudnn_plugin/cuda_dnn.pic.d ... (remaining 142 arguments skipped)
In file included from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Operation.h:37,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_OperationGraph.h:36,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Heuristics.h:31,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend.h:101,
                 from tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:56:
bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_PointWiseDesc.h: In member function ‘int64_t cudnn_frontend::PointWiseDesc_v8::getPortCount() const’:
bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_PointWiseDesc.h:69:16: error: enumeration value ‘CUDNN_POINTWISE_RECIPROCAL’ not handled in switch [-Werror=switch]
   69 |         switch (mode) {
      |                ^
In file included from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_OperationGraph.h:36,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Heuristics.h:31,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend.h:101,
                 from tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:56:
bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Operation.h: In member function ‘cudnn_frontend::Operation_v8&& cudnn_frontend::OperationBuilder_v8::build_pointwise_op()’:
bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Operation.h:413:16: error: enumeration value ‘CUDNN_POINTWISE_RECIPROCAL’ not handled in switch [-Werror=switch]
  413 |         switch (m_operation.pointwise_mode) {
      |                ^

### Standalone code to reproduce the issue

```shell
$ cd ~/Documents/dev/git/tensorflow
$ git checkout r2.13
$ git pull
$ ./configure
$ bazel build --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
~/Documents/dev/git/tensorflow$ bazel build --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=211
INFO: Reading rc options for 'build' from ~/Documents/dev/git/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from ~/Documents/dev/git/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from ~/Documents/dev/git/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=~/Documents/dev/programs/miniconda3/envs/tf/bin/python3 --action_env PYTHON_LIB_PATH=~/Documents/dev/programs/miniconda3/envs/tf/lib/python3.10/site-packages --python_path=~/Documents/dev/programs/miniconda3/envs/tf/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-12.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=5.0 --action_env LD_LIBRARY_PATH=/usr/lib/libreoffice/program:/usr/local/cuda/targets/x86_64-linux/lib:/usr/lib/x86_64-linux-gnu --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-11 --config=cuda
INFO: Reading rc options for 'build' from ~/Documents/dev/git/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug
INFO: Found applicable config definition build:short_logs in file ~/Documents/dev/git/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file ~/Documents/dev/git/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file ~/Documents/dev/git/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:mkl in file ~/Documents/dev/git/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt
INFO: Found applicable config definition build:opt in file ~/Documents/dev/git/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file ~/Documents/dev/git/tensorflow/.bazelrc: --define=build_with_onednn_v2=true --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file ~/Documents/dev/git/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/7d879c8b161085a4374ea481b93a52adb19c0529.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/dc275fd03254d67d29cc70a5a0569acf24d2280d.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/benchmark/archive/f7547e29ccaed7b64ef4f7495ecfff1c9f6f3d03.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/b971ac5250ea8de900eae9f95e06548d14cd95fe.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/b9d4073a6913891ce9cbd8965c8d506075d2a45a.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/43d81c6883ade82052920bd367c61f9e52f09954.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/c00d7ca810e93780bd0c8ee4eea28f4f2ea4bcdc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/nvidia/nccl/archive/v2.16.5-1.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11_abseil/archive/2c4932ed6f6204f1656e245838f4f5eae69d2e29.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/NVIDIA/cudnn-frontend/archive/refs/tags/v0.8.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/triton/archive/1627e0c27869b4098e5fa720717645c1baaf5972.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (631 packages loaded, 44329 targets configured).
INFO: Found 1 target...
ERROR: ~/Documents/dev/git/tensorflow/tensorflow/compiler/xla/stream_executor/cuda/BUILD:377:11: Compiling tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/xla/stream_executor/cuda/_objs/cudnn_plugin/cuda_dnn.pic.d ... (remaining 142 arguments skipped)
In file included from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Operation.h:37,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_OperationGraph.h:36,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Heuristics.h:31,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend.h:101,
                 from tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:56:
bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_PointWiseDesc.h: In member function ‘int64_t cudnn_frontend::PointWiseDesc_v8::getPortCount() const’:
bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_PointWiseDesc.h:69:16: error: enumeration value ‘CUDNN_POINTWISE_RECIPROCAL’ not handled in switch [-Werror=switch]
   69 |         switch (mode) {
      |                ^
In file included from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_OperationGraph.h:36,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Heuristics.h:31,
                 from bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend.h:101,
                 from tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:56:
bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Operation.h: In member function ‘cudnn_frontend::Operation_v8&& cudnn_frontend::OperationBuilder_v8::build_pointwise_op()’:
bazel-out/k8-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend/third_party/cudnn_frontend/include/cudnn_frontend_Operation.h:413:16: error: enumeration value ‘CUDNN_POINTWISE_RECIPROCAL’ not handled in switch [-Werror=switch]
  413 |         switch (m_operation.pointwise_mode) {
      |                ^
cc1plus: some warnings being treated as errors
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1819.010s, Critical Path: 107.08s
INFO: 5429 processes: 2198 internal, 3231 local.
FAILED: Build did NOT complete successfully
```
</details>"
60831,"Tensorflow lite selective build results _ZN6google8protobuf8internal26fixed_address_empty_stringE"" error, build fails with tne --config=monolithic setup, returning the Check failed: existing == nullptr (Tensor already registered) errror","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.13

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04 lts, Ubuntu 23.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.1.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I tried to create the selective build of the tensorflow lite, following [this](https://www.tensorflow.org/lite/android/lite_build) guide. 
The build succeeded and I added the tensorflow-lite-select-tf-ops.aar to the android studio project, yet it resulted in an error:

```
java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""_ZN6google8protobuf8internal26fixed_address_empty_stringE"" referenced by ""/data/app/~~PgTBlp4bIwZ8yl02UBjqqg==/com.inseye.core.test-yoruTyRORPvs5Ap3TE6dGw==/base.apk!/lib/x86_64/libtensorflowlite_flex_jni.so"".
```

I noticed, there are fixes for this problem, e.g. [here](https://github.com/tensorflow/tensorflow/issues/45153) - one needs just add following line:
```
--config=monolithic
``` 
to the .bazelrc file. 
But build with this configuration fails with the following error:

```
ERROR: /tensorflow_src/tensorflow/BUILD:1652:19: Action tensorflow/_api/v2/v2.py [for tool] failed: (Aborted): bash failed: error executing command (from target //tensorflow:tf_python_api_gen_v2) 
  (cd /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow && \
  exec env - \
    DOCKER_HOST_CACHEBUSTER=1682977560680045781 \
    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-x86_64/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/android/sdk/cmdline-tools/latest/bin:/android/sdk/platform-tools:/android/ndk \
  /bin/bash -c 'bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/_api/v2/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow/compat_template_v1.__init__.py --compat_init_template=tensorflow/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=default bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/tf_python_api_gen_v2.params')
# Configuration: 9f320c2ab265ab521267a555257ce138659ef24e739cde7fde73c7665f4cf5e4
# Execution platform: @local_execution_config_platform//:platform
2023-06-09 21:58:46.512000: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-09 21:58:46.546669: F ./tensorflow/core/framework/variant_op_registry.h:114] Check failed: existing == nullptr (0x2df3898 vs. nullptr)UnaryVariantDeviceCopy for direction: 1 and type_index: tensorflow::Tensor already registered
Target //tmp:tensorflow-lite-select-tf-ops failed to build
ERROR: /tensorflow_src/tensorflow/python/tools/BUILD:303:17 Middleman _middlemen/tensorflow_Spython_Stools_Sprint_Uselective_Uregistration_Uheader-runfiles failed: (Aborted): bash failed: error executing command (from target //tensorflow:tf_python_api_gen_v2) 
  (cd /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow && \
  exec env - \
    DOCKER_HOST_CACHEBUSTER=1682977560680045781 \
    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-x86_64/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/android/sdk/cmdline-tools/latest/bin:/android/sdk/platform-tools:/android/ndk \
  /bin/bash -c 'bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/_api/v2/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow/compat_template_v1.__init__.py --compat_init_template=tensorflow/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=default bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/tf_python_api_gen_v2.params')
# Configuration: 9f320c2ab265ab521267a555257ce138659ef24e739cde7fde73c7665f4cf5e4
# Execution platform: @local_execution_config_platform//:platform
```



### Standalone code to reproduce the issue

```shell
Contents of the .tf_configure.bazelrc:

build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/usr/local/lib/python3.11/dist-packages""
build --python_path=""/usr/bin/python3""
build:opt --copt=n
build:opt --host_copt=n
build --action_env ANDROID_NDK_HOME=""/android/ndk""
build --action_env ANDROID_NDK_API_LEVEL=""21""
build --action_env ANDROID_BUILD_TOOLS_VERSION=""34.0.0""
build --action_env ANDROID_SDK_API_LEVEL=""29""
build --action_env ANDROID_SDK_HOME=""/android/sdk""
test --test_size_filters=small,medium
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-gpu,-v1only
```


### Relevant log output

_No response_</details>"
60830,Cannot pick GPU unavailable,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 build 19045  0

### Mobile device

_No response_

### Python version

Python 3.10.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

Cuda compilation tools, release 12.1, V12.1.105 Build cuda_12.1.r12.1/compiler.32688072_0

### GPU model and memory

1070 TI 8g 

### Current Behaviour?

A bug happened!
`  gpus = tf.config.experimental.list_physical_devices('GPU')`
_This piece of code return empty array_
When in reallity have two GTX 1070 
Description                  : NVIDIA GeForce GTX 1070 Ti x 2
at least hoped to get one
I'm follow all installation step by step  verifying paths 
I don't know what to do anymore

### Standalone code to reproduce the issue

```shell
Follow this step guide:
https://www.tensorflow.org/install/gpu?hl=es-419
Install tensorflow, 
Configuration path
Cuda
Verification
os.environ[""CUDA_HOME""] = r""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1""
```


### Relevant log output

_No response_</details>"
60829,TensorFlow Lite C++ minimal example build,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

Build error

### Custom Code

Yes

### OS Platform and Distribution

Raspbian GNU/Linux 11 (bullseye) armv

### Mobile device

Raspberry Pi 3 Model B Plus Rev 1.3

### Python version

Python 3.9.2 (default, Mar 12 2021, 04:06:34)

### Bazel version

_No response_

### GCC/Compiler version

[GCC 10.2.1 20210110] on linux

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

[ 28%] Building ASM object _deps/xnnpack-build/CMakeFiles/microkernels-prod.dir/src/cs16-bfly4/cs16-bfly4-samples1-asm-aarch32-neon-x4.S.o
[ 35%] Building CXX object _deps/abseil-cpp-build/absl/strings/CMakeFiles/absl_strings.dir/numbers.cc.o
/tmp/ccWtfVcs.s: Assembler messages:
/tmp/ccWtfVcs.s:65: Error: selected processor does not support `vsdot.s8 q8,q12,d7[0]' in ARM mode
/tmp/ccWtfVcs.s:67: Error: selected processor does not support `vsdot.s8 q9,q10,d7[0]' in ARM mode


### Standalone code to reproduce the issue

```shell
# https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
mkdir minimal_build
cd minimal_build
cmake ../tensorflow_src/tensorflow/lite/examples/minimal
cmake --build . -j
```


### Relevant log output

```shell
gmake[1]: *** Waiting for unfinished jobs....
[ 35%] Building CXX object _deps/abseil-cpp-build/absl/strings/CMakeFiles/absl_strings.dir/str_replace.cc.o
[ 35%] Building CXX object _deps/abseil-cpp-build/absl/strings/CMakeFiles/absl_strings.dir/str_split.cc.o
[ 35%] Building CXX object _deps/abseil-cpp-build/absl/strings/CMakeFiles/absl_strings.dir/string_view.cc.o
[ 35%] Building CXX object _deps/abseil-cpp-build/absl/strings/CMakeFiles/absl_strings.dir/substitute.cc.o
[ 35%] Linking CXX static library libabsl_strings.a
[ 35%] Built target absl_strings
gmake: *** [Makefile:149: all] Error 2
```
</details>"
60827,wrong output shape with tf.experimental.numpy.vander when parameter N=0 ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!  when I used tf.experimental.numpy.vander() I get wrong values when parameter N =0  it produce empty array but with different shape that need to be 

### Standalone code to reproduce the issue

```shell
When I run
tf.experimental.numpy.vander(
    [-1.,-1.], N=0, increasing=False
)
I get the output as the following:
-> <tf.Tensor: shape=(2, 2), dtype=float64, numpy=
array([[-1.,  1.],
       [-1.,  1.]])>

But when I run 
np.vander(np.array([-1.,-1.]),N=0)
I got the following: 
-> array([], shape=(2, 0), dtype=float64)
```


### Relevant log output

```shell
tensorflow output 
<tf.Tensor: shape=(2, 2), dtype=float64, numpy=
array([[-1.,  1.],
       [-1.,  1.]])>
numpy output 
array([], shape=(2, 0), dtype=float64)
```
</details>"
60826,Unable to run RNN Model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

WSL2 on Windows 11

### Mobile device

_No response_

### Python version

3.8.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1.0

### GPU model and memory

NVIDIA Quadro P4200, 16gb

### Current Behaviour?

After setting up the system to use gpu from WSL2 on windows 11, I am able to connect to GPU. But when I run  a tensorflow model, I am able to run CNN model on GPU but when i try to run RNN model I get the below mentioned message and the model does not run:

""W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at partitioned_function_ops.cc:115 : INVALID_ARGUMENT: No OpKernel was registered to support Op 'CudnnRNN' used by {{node CudnnRNN}} with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=""linear_input"", direction=""unidirectional"", rnn_mode=""lstm"", seed2=0, is_training=true]
Registered devices: [CPU, GPU]
Registered kernels:
  <no registered kernels>

	 [[CudnnRNN]]""

### Standalone code to reproduce the issue

```shell
Trying RNN model with LSTM
```


### Relevant log output

_No response_</details>"
60825,What phone support tensorflow lite GPU delegate?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Android 13

### Mobile device

Android 13

### Python version

3.8.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I tested multiple mobile phones (including Xiaomi 12s Ultra, OnePlus 8, Honor Nova 10, Oppo Reno 9) using the example Android apk at https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android . On all of the devices, the app tells ""GPU is not supported"", but NNAPI and CPU is OK.

Anything I can do to enable the GPU delegate on these Qualcomm Snapdragon devices?

### Standalone code to reproduce the issue

```shell
the example Android apk at https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android
```


### Relevant log output

_No response_</details>"
60822,TF >= 2.7 slowdown tf.data API,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

Yes

### Source

installed from pip

### Tensorflow Version

tf >= 2.7

### Custom Code

No

### OS Platform and Distribution

Linux, CentOS Linux 7

### Mobile device

No

### Python version

3.8, 3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Since TF >= 2.7, I observed a significant slowness when running `ds_iter.get_next()` where `ds_iter` is an iterator of tf.data.Dataset defined as `dataset.shuffle(...).cache().repeat(...).batch(..., drop_remainder=True).prefetch(tf.data.AUTOTUNE)`. The same code runs much faster on the same machine using TF <= 2.6. A reproducer is included below.

Looking at the [change log from TF2.8](https://www.exxactcorp.com/blog/Deep-Learning/tensorflow-2-8-0-released), I see the following related to Dataset API `(since v2.7) Stateful ops used in tf.data.Dataset`. However I am not entirely sure if there is a workaround in higher TF version to reach the same speed.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import time

X = tf.random.uniform(shape=[21000, 373])
Labels = tf.random.uniform(shape=[21000, 1])
batchsize = 8192
n_shuffles = 500000
n_samples = 21000
ds = tf.data.Dataset.from_tensor_slices((X, Labels))
ds = ds.shuffle(buffer_size=n_samples).cache().repeat(n_shuffles).batch(batchsize, drop_remainder=True).prefetch(tf.data.AUTOTUNE)
ds_iter = iter(ds)

num_iter = 100
loop_start = time.time()
for i in range(num_iter):
    ds_iter.get_next()
loop_stop  = time.time()
print('loop time', f'{loop_stop-loop_start : .4f}')
```


### Relevant log output

```shell
output of the reproducer:
- with TF >= 2.7: loop time  15.8203
- with TF <= 2.6: loop time  1.0519

device to run the above two tests:
NVIDIA A100-PCIE-40GB NVIDIA-SMI 530.30.02 Driver Version: 530.30.02 CUDA Version: 12.1

$$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Thu_Jan_28_19:32:09_PST_2021
Cuda compilation tools, release 11.2, V11.2.142
Build cuda_11.2.r11.2/compiler.29558016_0
```
</details>"
60817,TFLite Android GPU delgate cannot run: TfLiteGpuDelegate Init: STRIDED_SLICE: Output batch don't match,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Android

### Mobile device

Android 13

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am trying to run a modified resnet-50 (as follows) on Android platform using TFLite GPU Delegate, I already replaced unsupported GATHER ops with STRIDED_SLICE ops. It works fine on my Linux server, Android CPU mode but not able to run on Adroid GPU delegate.

As https://www.tensorflow.org/lite/performance/gpu said, STRIDED_SLICE should be a supported ops on GPU delegate...

Anything can shed some lights on what I can do here?

![image](https://github.com/tensorflow/tensorflow/assets/5018331/613a4b60-93f5-456b-9db3-18c55a8215e3)

==

Analyzer said:
Your model looks compatible with GPU delegate with TFLite runtime version 2.12.0.
But it doesn't guarantee that your model works well with GPU delegate.
There could be some runtime incompatibililty happen.
---------------------------------------------------------------
Your TFLite model has '1' signature_def(s).

==

22:25:53.970  W  Accessing hidden field Ljava/lang/Throwable;->detailMessage:Ljava/lang/String; (unsupported, reflection, allowed)
22:25:53.971  E  java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Init: STRIDED_SLICE: Output batch don't match
                 TfLiteGpuDelegate Prepare: delegate is not initialized
                 Node number 409 (TfLiteGpuDelegateV2) failed to prepare.
                 Restored original execution plan after delegate application failure.
22:25:53.971  E  	at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)
22:25:53.971  E  	at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:110)
22:25:53.972  E  	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:58)
22:25:53.972  E  	at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:32)
22:25:53.972  E  	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:184)


### Standalone code to reproduce the issue

```shell
// if the device has a supported GPU, add the GPU delegate
                GpuDelegate gpuDelegate = new GpuDelegate(
                    new GpuDelegate.Options()
                        .setPrecisionLossAllowed(false)
                        .setQuantizedModelsAllowed(false)
                );
                options.addDelegate(gpuDelegate);
                Log.d(""ImageEncoder"", ""Device = GPU, hasGPU="" + hasGPU);

                // if the GPU is not supported, run on 4 threads as a backup plan
                options.setNumThreads(4);
                Log.d(""ImageEncoder"", ""Device = CPU"");

                interpreter = new Interpreter(file, options);


    implementation 'org.tensorflow:tensorflow-lite:2.12.0'
    implementation 'org.tensorflow:tensorflow-lite-api:2.10.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.12.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu-api:2.12.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu-delegate-plugin:0.4.3'
    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.12.0'
    implementation 'org.tensorflow:tensorflow-lite-support:0.4.2'

```


### Relevant log output

```shell
22:25:53.970  W  Accessing hidden field Ljava/lang/Throwable;->detailMessage:Ljava/lang/String; (unsupported, reflection, allowed)
22:25:53.971  E  java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Init: STRIDED_SLICE: Output batch don't match
                 TfLiteGpuDelegate Prepare: delegate is not initialized
                 Node number 409 (TfLiteGpuDelegateV2) failed to prepare.
                 Restored original execution plan after delegate application failure.
22:25:53.971  E  	at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)
22:25:53.971  E  	at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:110)
22:25:53.972  E  	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:58)
22:25:53.972  E  	at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:32)
22:25:53.972  E  	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:184)
```
</details>"
60816,2.13 support for TensorflowLiteSwift and tensorflow-lite android (Maven),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0

### Custom Code

Yes

### OS Platform and Distribution

Ios/Android

### Mobile device

Ios/Android

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I'm interested in the planned release date for TensorflowLite 2.13.0 IOS/Android

### Standalone code to reproduce the issue

```shell
I'm interested in the planned release date for TensorflowLite 2.13.0 IOS/Android
```


### Relevant log output

_No response_</details>"
60815,"int8 quantization fails with ""Aborted (core dumped)"" because real_output_multiplier > 1 in tensorflow/lite/kernels/add.cc","### 1. System information

- OS Platform and Distribution: Linux Ubuntu 20.04:
- TensorFlow installation: pip package
- TensorFlow library: v2.12.0

### 2. Problem
When I quantize my model in int8 format it fails with ""Aborted (core dumped)"". The quantization code is pretty standard:
```
model = some_model()
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = some_repr_dataset()
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
```

Unfortunately, I cannot provide this model. However, I was able to find the exact reason why it fails. My model has some add operations with real_output_multiplier larger than one (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/add.cc#L197). 
When QuantizeMultiplierSmallerThanOneExp is called with this real_output_multiplier, it hits TFLITE_CHECK_LT (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/quantization_util.cc#L117) and calls TFLITE_ABORT without printing any useful info.

Here is a patch which fixes the problem, and my model is successfully quantized:

```
diff --git a/tensorflow/lite/kernels/add.cc b/tensorflow/lite/kernels/add.cc
index 6cad1883750..ebe71222f7c 100644
--- a/tensorflow/lite/kernels/add.cc
+++ b/tensorflow/lite/kernels/add.cc
@@ -189,8 +189,14 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
     QuantizeMultiplierSmallerThanOneExp(
         real_input2_multiplier, &data->input2_multiplier, &data->input2_shift);
 
-    QuantizeMultiplierSmallerThanOneExp(
-        real_output_multiplier, &data->output_multiplier, &data->output_shift);
+    if (real_output_multiplier > 1){
+      QuantizeMultiplierGreaterThanOne(
+          real_output_multiplier, &data->output_multiplier, &data->output_shift);
+    }
+    else {
+      QuantizeMultiplierSmallerThanOneExp(
+          real_output_multiplier, &data->output_multiplier, &data->output_shift);
+    }
 
     TF_LITE_ENSURE_STATUS(CalculateActivationRangeQuantized(
         context, params->activation, output, &data->output_activation_min,
```

It seems that some operations were fixed to work with multiplier > 1 (see https://github.com/tensorflow/tensorflow/issues/20451).
I could make a pull request, but I'm not sure that my solution is universal enough. It's also not clear to me how it should behave if real_output_multiplier == 1.
"
60814,GPU Delegate dynamic tensor input shape (Feature Request),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Android

### Mobile device

Android

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hello, I'm writing you to ask, are there plans to implement dynamic input shape in gpu delegate in near future ? 

Right now dynamic input shape working fine on cpu, but on gpu delegate I'm getting the issue 
`java.lang.IllegalArgumentException: Internal error: Error applying delegate: `

This is very important feature, if this feature already exist and it is possible somehow to rung with dynamic shape will be good to have some information in documentation. 


### Standalone code to reproduce the issue

```shell
@Throws(IOException::class)
    private fun initInterpreter(context: Context): Interpreter {

        val tfliteOptions = Interpreter
            .Options()
        
        if (delegate != null) {
            tfliteOptions.addDelegate(delegate)
            tfliteOptions.numThreads = 1
        } else {
            tfliteOptions.numThreads = 4
        }

        val interpreter = Interpreter(loadModelFile(context), tfliteOptions)
        
        interpreter.resizeInput(0, intArrayOf(1, modelHeight, modelWidth, 3), true)
        interpreter.allocateTensors()
        
        return interpreter
    }
```


### Relevant log output

```shell
java.lang.IllegalArgumentException: Internal error: Error applying delegate: 
org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)
org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:110)
org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:73)
org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:36)
org.tensorflow.lite.Interpreter.<init>(Interpreter.java:214)
```
</details>"
60813,"tf.keras.Model.predict passing x=tf.keras.utils.Sequence causing exceptions  ValueError: Data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```Python
tf_model.predict(test_generator)
```

will cause exceptions

```Python
ValueError: in user code:

    File ""/DATA/home/yuehc/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 2169, in predict_function  *
        return step_function(self, iterator)
    File ""/DATA/home/yuehc/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 2155, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/DATA/home/yuehc/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 2143, in run_step  **
        outputs = model.predict_step(data)
    File ""/DATA/home/yuehc/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 2110, in predict_step
        x, _, _ = data_adapter.unpack_x_y_sample_weight(data)
    File ""/DATA/home/yuehc/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py"", line 1775, in unpack_x_y_sample_weight
        raise ValueError(error_msg)

    ValueError: Data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`, found: (<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, None, None) dtype=float32>)
```

The codes works well back in tf 2.4.0 and should still be working per release notes and the latest documentations.

the `test_generator` is a sub-class of `tf.keras.utils.Sequence` which works normally in tf 2.4.0

The return format for `__getitem__` is a tuple with a List element that indicate multiple inputs of the model
```Python
def __getitem__(self, i) -> Tuple[List[numpy.array]]:
    ...
```

For an example, see below
![image](https://github.com/tensorflow/tensorflow/assets/45007045/b8948609-693e-4918-8f1b-990fb1fa8bc8)
![image](https://github.com/tensorflow/tensorflow/assets/45007045/0c41d2ce-7460-405b-867a-103e6a794cd2)
![image](https://github.com/tensorflow/tensorflow/assets/45007045/51b80f7a-9c39-4ea4-93ee-7812c9765fa7)

I assume some behavors behind the `predict` interface are changed after the 2.4.0 to 2.12.0 upgrades.

Any idea how to fix it? Thx in advance.

### Standalone code to reproduce the issue

```Python
import tensorflow as tf
import math
import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM, Dense, Input

class BasicGenerator(tf.keras.utils.Sequence):

    def __init__(self, x, y=None, w=None, *, obj=None, batch_size=256, **kwargs):
        """"""
        a simple example of data generator by utilizing tf.keras.utils.Sequence

        Parameters
        ----------
        x: List[np.array], input data, the array size: (n_samples, n_features)

        y: np.arrya, input label, size: (n_samples, n_labels)

        w: sample weights, default None

        batch_size: batch_size, default 256
        """"""
        super(BasicGenerator, self).__init__()

        # basic params
        self.x = x
        self.y = y
        self.w = w
        self.batch_size = batch_size
        
    def __len__(self):
        return math.ceil(self.x[0].shape[0] / self.batch_size)

    def __getitem__(self, index):
        
        res = ()

        b_x = [_x[index * self.batch_size:(index + 1) * self.batch_size] for _x in self.x]
        res += (b_x,)

        if self.y is not None:
            b_y = self.y[index * self.batch_size:(index + 1) * self.batch_size]
            res += (b_y,)

        if self.w is not None:
            b_w = self.w[index * self.batch_size:(index + 1) * self.batch_size]
            res += (b_w,)
        
        return res

# Define the model
input1 = Input(shape=(3, 1))
input2 = Input(shape=(3, 1))

lstm1 = LSTM(50, activation='relu')(input1)
lstm2 = LSTM(50, activation='relu')(input2)

concat = tf.keras.layers.concatenate([lstm1, lstm2])
output = Dense(1)(concat)

model = Model(inputs=[input1, input2], outputs=output)
model.compile(optimizer='adam', loss='mse')

# Generate some fake data
n_samples = 10000
X_1 = np.random.rand(n_samples, 3, 1)
X_2 = np.random.rand(n_samples, 3, 1)
y = np.random.rand(n_samples, 1)

# generator sub-classing from utils.Sequence
train_generator = BasicGenerator(x=[X_1, X_2], y=y)
test_generator = BasicGenerator(x=[X_1, X_2])

# fit
model.fit(train_generator)

# the problem occurs
model.predict(
    test_generator,
    verbose=1,
    max_queue_size=30,
    workers=1,
    use_multiprocessing=False
)
```


### Relevant log output

```Python
40/40 [==============================] - 7s 14ms/step - loss: 0.1985
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-13-82ba4c04abb5> in <module>
     76 
     77 # the problem occurs
---> 78 model.predict(
     79     test_generator,
     80     verbose=1,

/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---> 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/engine/training.py in tf__predict_function(iterator)
     13                 try:
     14                     do_return = True
---> 15                     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16                 except:
     17                     do_return = False

ValueError: in user code:

    File ""/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/engine/training.py"", line 2169, in predict_function  *
        return step_function(self, iterator)
    File ""/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/engine/training.py"", line 2155, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/engine/training.py"", line 2143, in run_step  **
        outputs = model.predict_step(data)
    File ""/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/engine/training.py"", line 2111, in predict_step
        return self(x, training=False)
    File ""/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File ""/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/engine/input_spec.py"", line 219, in assert_input_compatibility
        raise ValueError(

    ValueError: Layer ""model_3"" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None) dtype=float32>]
```
</details>"
60812,Ragged String Input not working with GPU  - > non-DMA-copy attempted of tensor type: string,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

consider this trivial tf.keras Model:

```
def some_model():
     x = tf.keras.layers.Input(shape=(None, 1), name=""string_input"",dtype=""string"",ragged=True)
     some_function = tf.keras.layers.Lambda(lambda x: x)
     model = tf.keras.models.Model([x],some_function(x))
     return model
```
 
```
with tf.distribute.OneDeviceStrategy(""gpu:0"").scope():
  m  = some_model()
```
passing a ragged tensor will fail :

```
x = tf.ragged.constant([[[""somestring""],[""somestring""]],[[""somestring""],[""somestring""],[""somestring""]]])
m.predict(x)
```
with ""non-DMA-copy attempted of tensor type: string""

whereas this works just fine

```
x = tf.constant([[[""somestring""],[""somestring""]],[[""somestring""],[""somestring""]]])
m.predict(x)
```

I am aware that there are many related issue open / i hope the small self-contained example above is useful


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1cf669jKVUlIjC0JGU42VNyZTWEywAgT0?usp=sharing
```


### Relevant log output

_No response_</details>"
60811,Setting memory_limit inside docker does not have an effect,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04 (inside docker)

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

12.1

### GPU model and memory

Nvidia GeForce RTX 4090

### Current Behaviour?

Setting the memory_limit does not have any effect whatsoever when running inside a docker container.
I use the following NGC https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel-23-05.html

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.set_logical_device_configuration(
	gpus[0],
	[tf.config.LogicalDeviceConfiguration(memory_limit=4096)]
)
```


### Relevant log output

_No response_</details>"
60810, TensorFlow device (GPU:0) is being mapped to multiple devices when using tf.estimator api and set visible_device_list with hvd.local_rank(),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf 2.11

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.1

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current Behaviour?

I know there is a same issue, https://github.com/tensorflow/tensorflow/issues/58952, but it has been closed.

The model implemented with tf.estimator api. horovod multi-cards job (horovodrun -np 2 python xxx.py).

it works well using 2.10, but throws Device mapping error when using TF2.11.

FROM [issue58952]( https://github.com/tensorflow/tensorflow/issues/58952), It seems that this error is relative to this [commit]( https://github.com/tensorflow/tensorflow/commit/16ea0f8995a9b087ff7e50bbf5a03e58e3c5002e#:~:text=if-,context.is_custom_device,-(device_string)%3A). When calling context.is_cutom_device(), this API would create TFE_Context that would create TF devices. However, program would also create TF devices when create Session with the aboved mentioned config, and these two TF device creation would conflicts.

It seems that the [session_options used in the ensure_initialized](https://github.com/tensorflow/tensorflow/blob/301f145ca3979b5155a166cbe6cb8c2a3af1bab1/tensorflow/python/eager/context.py#L581) function to [create NewContext](https://github.com/tensorflow/tensorflow/blob/301f145ca3979b5155a166cbe6cb8c2a3af1bab1/tensorflow/python/eager/context.py#L598) are inconsistent with the options used by the NewSession. The options used by NewSession are explicitly set through tf.ConfigProto.gpu_options.visible_device_list=hvd.localrank(). session_config should be at least consistent, or loaded once at most

To reproduce this error, you need to install horovod and using multi card machine to run below code with:

horovodrun -np N python xxx.py

where N denotes the number of processes to be started.


### Standalone code to reproduce the issue

```shell
mport pandas as pd
import tensorflow as tf
import tensorflow.compat.v1 as tf1
global is_mpi
try:
    import horovod.tensorflow as hvd
    hvd.init()
    is_mpi = hvd.size()
except ImportError:
    is_mpi = 0
    print(""No MPI horovod support, this is running in no-MPI mode!"")

session_config = tf1.ConfigProto()
if is_mpi:
  session_config.gpu_options.visible_device_list = str(hvd.local_rank())

run_config = tf.estimator.RunConfig(session_config=session_config)

x_train = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')
x_train['sex'].replace(('male', 'female'), (0, 1), inplace=True)
x_train['alone'].replace(('n', 'y'), (0, 1), inplace=True)
x_train['class'].replace(('First', 'Second', 'Third'), (1, 2, 3), inplace=True)
x_train.drop(['embark_town', 'deck'], axis=1, inplace=True)
y_train = x_train.pop('survived')

# Data setup for TensorFlow 1 with `tf.estimator`
def _input_fn():
  return tf1.data.Dataset.from_tensor_slices((dict(x_train), y_train)).batch(32)

FEATURE_NAMES = [
    'age', 'fare', 'sex', 'n_siblings_spouses', 'parch', 'class', 'alone'
]
feature_columns = []
for fn in FEATURE_NAMES:
  feat_col = tf1.feature_column.numeric_column(fn, dtype=tf.float32)
  feature_columns.append(feat_col)

linear_estimator = tf.estimator.LinearEstimator(
    head=tf.estimator.BinaryClassHead(),
    feature_columns=feature_columns,
    model_dir=""./model"",
    config=run_config)
linear_estimator.train(input_fn=_input_fn, steps=1000)
```


### Relevant log output

```shell
[1,0]<stderr>:2023-06-08 02:16:09.394853: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled
[1,1]<stderr>:2023-06-08 02:16:09.397900: E tensorflow/core/common_runtime/session.cc:91] Failed to create session: ALREADY_EXISTS: TensorFlow device (GPU:0) is being mapped to multiple devices (1 now, and 0 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not currently supported, see https://github.com/tensorflow/tensorflow/issues/19083
[1,1]<stderr>:2023-06-08 02:16:09.397922: E tensorflow/c/c_api.cc:2209] ALREADY_EXISTS: TensorFlow device (GPU:0) is being mapped to multiple devices (1 now, and 0 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not currently supported, see https://github.com/tensorflow/tensorflow/issues/19083
[1,1]<stderr>:Traceback (most recent call last):
[1,1]<stderr>:  File ""test.py"", line 43, in <module>
[1,1]<stderr>:    linear_estimator.train(input_fn=_input_fn, steps=1000)
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 360, in train
[1,1]<stderr>:    loss = self._train_model(input_fn, hooks, saving_listeners)
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1186, in _train_model
[1,1]<stderr>:    return self._train_model_default(input_fn, hooks, saving_listeners)
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1217, in _train_model_default
[1,1]<stderr>:    return self._train_with_estimator_spec(estimator_spec, worker_hooks,
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1512, in _train_with_estimator_spec
[1,1]<stderr>:    with training.MonitoredTrainingSession(
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 606, in MonitoredTrainingSession
[1,1]<stderr>:    return MonitoredSession(
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1050, in __init__
[1,1]<stderr>:    super(MonitoredSession, self).__init__(
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 753, in __init__
[1,1]<stderr>:    self._sess = _RecoverableSession(self._coordinated_creator)
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1259, in __init__
[1,1]<stderr>:    _WrappedSession.__init__(self, self._create_session())
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1264, in _create_session
[1,1]<stderr>:    return self._sess_creator.create_session()
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 906, in create_session
[1,1]<stderr>:    self.tf_sess = self._session_creator.create_session()
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 665, in create_session
[1,1]<stderr>:    return self._get_session_manager().prepare_session(
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 309, in prepare_session
[1,1]<stderr>:    sess, is_loaded_from_checkpoint = self._restore_checkpoint(
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 218, in _restore_checkpoint
[1,1]<stderr>:    sess = session.Session(self._target, graph=self._graph, config=config)
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1604, in __init__
[1,1]<stderr>:    super(Session, self).__init__(target, graph, config=config)
[1,1]<stderr>:  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 712, in __init__
[1,1]<stderr>:    self._session = tf_session.TF_NewSessionRef(c_graph, opts)
[1,1]<stderr>:tensorflow.python.framework.errors_impl.AlreadyExistsError: TensorFlow device (GPU:0) is being mapped to multiple devices (1 now, and 0 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not currently supported, see https://github.com/tensorflow/tensorflow/issues/19083
```
</details>"
60809,tf-mlir-translate and flatbuffer_translate failure for the ERF function,"### For TF

```
Results INVALID_MLIR test_erf_1_f32: Error 1 running command: /tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate --graphdef-to-mlir --tf-enable-shape-inference-on-import --tf-output-arrays=result erf/test_erf_1_f32/model.pb -o erf/test_erf_1_f32/test_tf.preopt.mlir --tf-input-arrays placeholder_0 --tf-input-shapes 1,
```
You can find the dummy tf erf model here: https://github.com/Jerry-Ge/tfl_models/blob/main/erf_model.pb

### For TFL
After running
```
/tensorflow/compiler/mlir/lite/flatbuffer_translate --tflite-flatbuffer-to-mlir erf/test_erf_1_f32/model.tflite --output-arrays=PartitionedCall:0 -o erf/test_erf_1_f32/test_tflite.preopt.mlir
```
You can find the dummy tfl erf model here: https://github.com/Jerry-Ge/tfl_models/blob/main/erf_model.tflite


It's generating a `tfl.custom` operator here which is not tfl.erf
```
module attributes {tf_saved_model.semantics, tfl.description = ""MLIR Converted."", tfl.schema_version = 3 : i32} {
  func.func @main(%arg0: tensor<1xf32> {tf_saved_model.index_path = [""placeholder_0""]}) -> (tensor<1xf32> {tf_saved_model.index_path = [""output_0""]}) attributes {tf.entry_function = {inputs = ""serving_default_placeholder_0:0"", outputs = ""PartitionedCall:0""}, tf_saved_model.exported_names = [""serving_default""]} {
    %0 = ""tfl.custom""(%arg0) {custom_code = ""FlexErf"", custom_option = #tfl<const_bytes : ""0x03457266001212034572661A002A070A0154120230013200000219151414042801"">} : (tensor<1xf32>) -> tensor<1xf32>
    return %0 : tensor<1xf32>
  }
}
```

I think there requires some fair amount of support to the erf function for mlir. 

Related ticket: https://github.com/tensorflow/tensorflow/issues/60663"
60805,Exception in tf.function due to operation in inactive condition branch,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0-dev20230606

### Custom Code

Yes

### OS Platform and Distribution

Windows 11 x64

### Mobile device

NA

### Python version

3.10

### Bazel version

NA

### GCC/Compiler version

NA

### CUDA/cuDNN version

NA

### GPU model and memory

NA

### Current Behaviour?

A function decorated with `@tf.function` may fail to execute if it contains a conditional operation where the non-executing branch cannot be executed correctly for the current inputs. Whether the issue arises or not may depend on the input signature given to `tf.function`. See example for clarity.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

@tf.function(input_signature=[tf.TensorSpec([None], tf.int32)])
def f(x):
    return tf.cond(tf.size(x) == 1,
                   # The reshape in this branch can only execute properly when the condition is true
                   lambda: tf.fill(tf.shape(x), tf.reshape(x, ())),
                   lambda: x)

# Works: this input is valid for both condition branches
tf.print(f(tf.constant([1])))
# [1]

# Fails: this input is only valid for the false branch, which is the active one
tf.print(f(tf.constant([1, 2])))
# tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error: (see below)


# If the shape in the function input signature is left completely undefined it works
# If the tf.function is defined with no input signature it works correctly as well

@tf.function(input_signature=[tf.TensorSpec(None, tf.int32)])
def f(x):
    return tf.cond(tf.size(x) == 1,
                   lambda: tf.fill(tf.shape(x), tf.reshape(x, ())),
                   lambda: x)

tf.print(f(tf.constant([1])))
# [1]

tf.print(f(tf.constant([1, 2])))
# [1 2]
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ...
  File ""...\site-packages\tensorflow\python\util\traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""...\site-packages\tensorflow\python\eager\execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:

Detected at node cond/Reshape defined at (most recent call last):
  ...

Input to reshape is a tensor with 2 values, but the requested shape has 1
         [[{{node cond/Reshape}}]] [Op:__inference_f_23]
```
</details>"
60800,Tensorflow freezes during training on Mac Studio,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf2.9

### Custom Code

No

### OS Platform and Distribution

MacOS Ventura 13.4/13.1

### Mobile device

Mac Studio

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M1

### Current Behaviour?

When training a model on my Mac Studio, from time to time it seems that the training freezes, and an epoch takes much more time than usual, from x10 to x200.
I have tried contacting Apple's support, and they are clueless about this phenomena.
I also went through recent Mac related issues in this repository and found nothing relevant.

Here is an example of the phenomena as seen in the training's output:
```
28/28 [==============================] - 5s 195ms/step - loss: 6.2743e-04 - accuracy: 0.0113 - val_loss: 3.4481e-04 - val_accuracy: 0.0147
Epoch 597/1000
28/28 [==============================] - 61s 2s/step - loss: 8.0337e-04 - accuracy: 0.0055 - val_loss: 3.4126e-04 - val_accuracy: 0.0244
Epoch 598/1000
28/28 [==============================] - 5s 195ms/step - loss: 7.9272e-04 - accuracy: 0.0101 - val_loss: 3.3659e-04 - val_accuracy: 0.0208
```





### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

# Generate random training data
np.random.seed(0)
x_train = np.random.rand(100, 1)
y_train = 3 * x_train + 2 + np.random.randn(100, 1) * 0.1

# Define the neural network architecture
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(1, input_shape=(1,))
])

# Compile the model
model.compile(optimizer='sgd', loss='mean_squared_error')

# Train the model
model.fit(x_train, y_train, epochs=1000, batch_size=10)
```

(Thank ChatGPT for the minimal working example)
```


### Relevant log output

```shell
Metal device set to: Apple M1 Ultra

systemMemory: 128.00 GB
maxCacheSize: 48.00 GB

2023-06-07 13:46:01.901374: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2023-06-07 13:46:01.901665: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2023-06-07 13:46:01.992101: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
Epoch 1/1000
2023-06-07 13:46:02.108687: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.
10/10 [==============================] - 0s 3ms/step - loss: 14.7499
Epoch 2/1000
10/10 [==============================] - 0s 10ms/step - loss: 9.1414
...
Epoch 33/1000
10/10 [==============================] - 0s 13ms/step - loss: 0.0382
Epoch 34/1000
10/10 [==============================] - 6s 716ms/step - loss: 0.0374
Epoch 35/1000
10/10 [==============================] - 0s 5ms/step - loss: 0.0367
...
```
</details>"
60799,"RuntimeError during 16x8 quantization in TFLite converter - ""Max and min for dynamic tensors should be recorded during calibration""","### 1. System information

- **OS Platform and Distribution**:
*Ubuntu 22.04.1 LTS*
- **TensorFlow installation**:
*pip install tensorflow*
- **TensorFlow library**:
*tensorflow                    2.12.0*
*tensorflow-cpu                2.12.0*
*tensorflow-estimator          2.12.0*
*tensorflow-io-gcs-filesystem  0.23.1*
*tensorflow-model-optimization 0.7.5*


### 2. Code

Here is the Python script that can reproduce the issue:

```python
import tensorflow as tf
import os
import numpy as np
import pandas as pd
import time


# Set up the environment to use CPU
tf.config.set_visible_devices([], 'GPU')


#######################
#   CircularBuffer
#######################
class CircularBufferLayer(tf.keras.layers.Layer):
    def __init__(self, num_features, buffer_size, stride, **kwargs):
        super().__init__(**kwargs)
        self.num_features = num_features
        self.buffer_size = buffer_size
        self.stride = stride
        self.buffer = self.add_weight(name='buffer', shape=(1, buffer_size, self.num_features),
                                      initializer='zeros', trainable=False, dtype=tf.float32)
        self.call_count = self.add_weight(name='call_count', shape=(), initializer='zeros',
                                          dtype=tf.int32, trainable=False)
        # total count, this count will never reset
        self.total_call_count = self.add_weight(name='total_call_count', shape=(), initializer='zeros',
                                                dtype=tf.int32, trainable=False)

    def call(self, inputs, **kwargs):
        # inputs should be reshaped to (1, 1, num_features) to match the buffer shape
        inputs = tf.reshape(inputs, [1, 1, self.num_features])

        # Update the buffer with the new data
        self.buffer.assign(tf.concat([self.buffer[:, 1:], inputs], axis=1))

        # Update the call count
        self.call_count.assign(tf.minimum(self.call_count + 1, self.stride))
        # self.total_call_count.assign(self.total_call_count + 1)
        self.total_call_count.assign(tf.minimum(self.total_call_count + 1, self.buffer_size))

        # If-else condition
        self.call_count.assign(
            tf.cond(tf.logical_and(tf.equal(self.call_count, self.stride), tf.greater_equal(self.total_call_count, self.buffer_size)),
                    true_fn=lambda: 0,
                    false_fn=lambda: self.call_count,
                    )
        )

        # Create a boolean flag indicating if self.call_count is 0
        # and the total number of calls to this layer is at least self.buffer_size
        flag = tf.equal(self.call_count, 0)

        # Return the buffer data and the flag
        return [self.buffer, flag]

    def reset(self):
        self.buffer.assign(tf.zeros_like(self.buffer))
        self.call_count.assign(tf.zeros_like(self.call_count))
        self.total_call_count.assign(tf.zeros_like(self.total_call_count))

    def get_config(self):
        config = {
            'buffer': self.buffer,
            'total_call_count': self.total_call_count,
            'call_count': self.call_count
        }
        base_config = super(CircularBufferLayer, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


#######################
#   Streaming Model
#######################
class StreamingModel(tf.keras.Model):
    def __init__(self, input_channel, output_channel, kernel_size, stride, **kwargs):
        super().__init__(**kwargs)

        self.input_channel = input_channel
        self.output_channel = output_channel
        self.kernel_size = kernel_size
        self.stride = stride

        # set acoustic model buffer
        self.buffer = CircularBufferLayer(
            num_features=input_channel,
            buffer_size=kernel_size,
            stride=stride
        )

        # set filters
        self.conv1d = tf.keras.layers.Conv1D(
            output_channel,
            kernel_size=kernel_size,
            strides=1,
            use_bias=False,
            padding='valid',
            data_format='channels_last'
        )

    def call(self, inputs, **kwargs):
        # buffer:
        [x, flag] = self.buffer(inputs)              # output shape = [1, kernel_size, input_channel]

        x = tf.cond(flag,                       # output shape = [1, 1, output_channel]
            true_fn=lambda: self.conv1d(x),
            false_fn=lambda: tf.zeros([1, 1, self.output_channel])
        )

        x = tf.reshape(x, [1, self.output_channel])     # output shape = [1, output_channel]

        return [x, flag]

    def reset(self):
        self.buffer.reset()

if __name__ == '__main__':
    model = StreamingModel(
        input_channel=32,
        output_channel=64,
        kernel_size=5,
        stride=2
    )

    # create some dummy data with the correct shapes
    seq_len = 50
    input_data = tf.random.normal([1, seq_len, model.input_channel])

    # call the model on the dummy data - crucial to build all sub-graphs
    for t in range(seq_len):
        output = model(input_data[:, t])

    # reset the buffer after executing
    model.reset()

    # print the model's summary weights
    model.summary()

    #####################################
    #        TFLite Conversion
    #####################################
    tflite_path = '/data/netapp2/git-repos/danielr/sbu_whispro_tflm/tflite_models/open_issue_8-16'
    saved_model_dir = os.path.expanduser(tflite_path)
    tf.saved_model.save(obj=model, export_dir=saved_model_dir)
    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
    converter.experimental_enable_resource_variables = True

    # Quantize the model to 16x8
    def representative_data_gen():
        input_channel = 32
        seq_len = 50
        for t in range(seq_len):
            x = tf.random.normal([1, input_channel])
            yield [x]

    converter.inference_input_type = tf.float32
    converter.inference_output_type = tf.float32
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]
    converter.representative_dataset = representative_data_gen
    converter.allow_custom_ops = True
    tflite_model = converter.convert()

    # Save the TFLite model.
    with tf.io.gfile.GFile(tflite_path + '.tflite', 'wb') as f:
        f.write(tflite_model)
```

The script defines a custom layer (CircularBufferLayer) and model (StreamingModel), uses the model on dummy data, and then attempts to quantize and convert the model into a TFLite model. The error seems to be related to the lack of dynamic range data for a particular tensor.

### 3. Failure after conversion

The conversion process fails with a `RuntimeError: Max and min for dynamic tensors should be recorded during calibration`. The error specifically highlights the tensor `streaming_model/circular_buffer_layer/Minimum`.

Here is the full traceback:

```
Traceback (most recent call last):
  File ""/data/netapp2/git-repos/danielr/sbu_whispro_tflm/sbu_whispro_tflm/scripts/open_issue_8_16_bit.py"", line 161, in <module>
    tflite_model = converter.convert()
  File ""/home/danielr/.virtualenvs/ASR_3.8/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 962, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
  File ""/home/danielr/.virtualenvs/ASR_3.8/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 940, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
  File ""/home/danielr/.virtualenvs/ASR_3.8/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 1247, in convert
    return self._convert_from_saved_model(graph_def)
  File ""/home/danielr/.virtualenvs/ASR_3.8/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 1131, in _convert_from_saved_model
    return self._optimize_tflite_model(
  File ""/home/danielr/.virtualenvs/ASR_3.8/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py"", line 215, in wrapper
    raise error from None  # Re-throws the exception.
  File ""/home/danielr/.virtualenvs/ASR_3.8/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
  File ""/home/danielr/.virtualenvs/ASR_3.8/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 899, in _optimize_tflite_model
    model = self._quantize(model, q_in_type, q_out_type, q_activations_type,
  File ""/home/danielr/.virtualenvs/ASR_3.8/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 654, in _quantize
    return calibrate_quantize.calibrate_and_quantize(
  File ""/home/danielr/.virtualenvs/ASR_3.8/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py"", line 215, in wrapper
    raise error from None  # Re-throws the exception.
  File ""/home/danielr/.virtualenvs/ASR_3.8/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
  File ""/home/danielr/.virtualenvs/ASR_3.8/lib/python3.8/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 176, in calibrate_and_quantize
    return self._calibrator.QuantizeModel(
RuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor streaming_model/circular_buffer_layer/Minimum
Empty min/max for tensor streaming_model/circular_buffer_layer/Minimum

Process finished with exit code 1

```

### 4. (optional) RNN conversion support

Not applicable.

### 5. (optional) Any other info / logs

I have attempted to use the `converter.experimental_enable_resource_variables = True` and `converter.allow_custom_ops = True` options in the TFLite converter to allow handling of the custom layer, but the issue persists.
"
60798,Error building Tensorflow from source on Windows with /MT,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.9.3

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.4

### GCC/Compiler version

MSVC (Visual Studio 2022)

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am trying to build Tensorflow from source with flag /MT (I require this so that the tensorflow.dll does not require the vcruntime libraries on startup).

I tried this command:
bazel build tensorflow:tensorflow.dll --copt=/MT

But i got errors:
c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory

(I run this in Git Bash).
It seems, like something is wrong with spaces in path. But i builded tensorflow successfully without /MT flag.

### Standalone code to reproduce the issue

```shell
./configure
bazel build tensorflow:tensorflow.dll --copt=/MT
```


### Relevant log output

```shell
WARNING: Running Bazel server needs to be killed, because the startup options are different.
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from c:\projects\_thirdparty\sources\tensorflow\mt4\tensorflow-2.9.3\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
INFO: Reading rc options for 'build' from c:\projects\_thirdparty\sources\tensorflow\mt4\tensorflow-2.9.3\.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from c:\projects\_thirdparty\sources\tensorflow\mt4\tensorflow-2.9.3\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe --action_env PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages --python_path=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from c:\projects\_thirdparty\sources\tensorflow\mt4\tensorflow-2.9.3\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file c:\projects\_thirdparty\sources\tensorflow\mt4\tensorflow-2.9.3\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\projects\_thirdparty\sources\tensorflow\mt4\tensorflow-2.9.3\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:windows in file c:\projects\_thirdparty\sources\tensorflow\mt4\tensorflow-2.9.3\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\projects\_thirdparty\sources\tensorflow\mt4\tensorflow-2.9.3\.bazelrc: --define framework_shared_object=false
Loading:
Loading: 0 packages loaded
Analyzing: target //tensorflow:tensorflow.dll (1 packages loaded, 0 targets configured)
Analyzing: target //tensorflow:tensorflow.dll (35 packages loaded, 11 targets configured)
Analyzing: target //tensorflow:tensorflow.dll (185 packages loaded, 4734 targets configured)
Analyzing: target //tensorflow:tensorflow.dll (243 packages loaded, 12995 targets configured)
Analyzing: target //tensorflow:tensorflow.dll (259 packages loaded, 20104 targets configured)
INFO: Analyzed target //tensorflow:tensorflow.dll (259 packages loaded, 20104 targets configured).
INFO: Found 1 target...
[0 / 1,325] [Prepa] BazelWorkspaceStatusAction stable-status.txt
[32 / 2,100] checking cached actions
[157 / 2,250] [Scann] Compiling snappy.cc
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling internal/counter.c failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/counter.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling platform/win32/src/pthread_key_win32.cc failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/pthread_key_win32.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling internal/cv.c failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/cv.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling internal/note.c failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/note.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling platform/c++11/src/yield.cc failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/yield.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling internal/dll.c failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/dll.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling platform/win32/src/clock_gettime.c failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/clock_gettime.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling internal/once.c failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/once.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling internal/sem_wait.c failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/sem_wait.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling internal/time_internal.c failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/time_internal.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling internal/mu.c failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/mu.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling internal/wait.c failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/wait.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling internal/debug.c failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/debug.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling internal/common.c failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/common.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling platform/win32/src/per_thread_waiter.c failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/per_thread_waiter.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
ERROR: C:/users/a/_bazel_a/lyw6tvlp/external/nsync/BUILD:467:11: Compiling internal/mu_wait.c failed: (Exit 2): cl.exe failed: error executing command
  cd /d C:/users/a/_bazel_a/lyw6tvlp/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/python.exe
    SET PYTHON_LIB_PATH=C:/Users/a/AppData/Local/Programs/Python/Python310/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\a\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\a\AppData\Local\Temp
  C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.35.32215\bin\HostX64\x64\cl.exe @bazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/mu_wait.obj.params
# Configuration: 5bf462d48c9f4c479b4b597fa29c1ae712e6bc33826ef977f3e3753aaa13def1
# Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'

c1xx: fatal error C1083: Cannot open source file: 'C:/Program\': No such file or directory
MT
c1xx: fatal error C1083: Cannot open source file: 'Files/Git/MT': No such file or directory
Generating Code...
Target //tensorflow:tensorflow.dll failed to build
INFO: Elapsed time: 11.426s, Critical Path: 0.97s
INFO: 22 processes: 20 internal, 2 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
</details>"
60795,Add docs reference to latest numpy version for `tf.experimental.numpy` functions,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Linux ubuntu 18.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

As the tf version requirements for running the latest version we need the latest version of numpy and all `tf.experimental.numpy` functions point to the `numpy` `v1.16` https://www.tensorflow.org/api_docs/python/tf/experimental/numpy/allclose

I am thinking if we can update the referencing docs link.

Thanks

### Standalone code to reproduce the issue

```shell
Check this https://www.tensorflow.org/api_docs/python/tf/experimental/numpy/allclose
```


### Relevant log output

_No response_</details>"
60793,"RESHAPE: OP is supported, but tensor type/shape isn't compatible","**System information**
- OS Platform and Distribution: Android, Galaxy S23.
- TensorFlow installed from (source or binary):2.12
- TensorFlow version (or github SHA if from source):2.12


Input tensor shape (1,16,32,256,12)
Output tensor shape (1,16,32,3072)

**Any other info / logs**:

RESHAPE: OP is supported, but tensor type/shape isn't compatible"
60792,About using C to call tflite,"**System information**
- RIsc-v

**Standalone code to reproduce the issue**
Hi:
    If i want to compile tflite into a library and then use C to call it. How can I effectively optimize and crop it to make the compiled tflite library file smaller. Because for our model, tflite micro is not supported by many operators
"
60790,could not run GPU on jupyter ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I could not get Tensorflow to run on GPUs.

TF sees the GPUs on terminal, but not on jupyter lab.

**Edited**
Found a solution to see it on jupyterlab, but must manually repeat . Still, erratic misconfiguration seems to happen.


### Standalone code to reproduce the issue

```shell
I could not get Tensorflow to run on GPUs.

TF sees the GPUs on terminal, but not on jupyter lab.

** edited **


Eventually, after hours, I found a temporary solution in setting the paths each time, before I launch `jupyter lab` :



CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib

```

NB : If I was to set 
```
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
echo 'CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
```

as in `https://www.tensorflow.org/install/pip`

Somehow it messy with jupyter, bcause the selected kernel from jupyter would not correspond to the kernel set from the script.

Now I can see the GPUs on jupyter, but still - it crashes! And before, on CPU, it was not.

When I run this script, using this library :

https://pypi.org/project/umap-learn/

installed as in the description:

```
embedder = ParametricUMAP(
    ## all the params ...
)

# now launch on GPU
with tf.device('/GPU:0'):
    embedding =  embedder.fit_transform(np.array([t.ravel() for t in train_data]))
```

the code fails with the log output below.

If I close the jupyter lab connection, go back on the conda environment, 
set again:

```
CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib
```
I
and 

```
# Install NVCC
conda install -c nvidia cuda-nvcc=11.3.58
# Configure the XLA cuda directory
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
printf 'export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/\n' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
source $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
# Copy libdevice file to the required path
mkdir -p $CONDA_PREFIX/lib/nvvm/libdevice
cp $CONDA_PREFIX/lib/libdevice.10.bc $CONDA_PREFIX/lib/nvvm/libdevice/

```

And relaunch jupterlab, this time I get the GPU seen also in jupyter lab.

Running 

```
with tf.device('/GPU:0'):
    spec_embedding = spec_embedder.fit_transform(np.array([t.ravel() for t in train_data]))

```

It yields

```
2023-06-06 21:55:04.944257: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32
	 [[{{node gradients/split_grad/concat/split/split_dim}}]]
```

And cannot understand why it raise warning or error with `CPU` device, if I run on `GPU`. 

Not even sure it falled back to CPU, actually.


---

Please advice how to sync jupter lab and conda.
I did follow up with `ikernel` but it seems, after a lot of checking, that environment variables are not correctly read. Not sure if `kernel.json` fails to be updated properly, with thepath of the cuda libraries.

Please consider add a guide on:
https://www.tensorflow.org/install/pip

for running TF on jupyter.

My situation is that I need to run from a remote cluster, and I think it is a frequent situation.
hope this feedback is useful.
```


### Relevant log output

```shell
2023-06-06 21:40:08.385763: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32
	 [[{{node gradients/split_grad/concat/split/split_dim}}]]
2023-06-06 21:40:10.938429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600
2023-06-06 21:40:11.641291: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f89f489b8d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-06-06 21:40:11.641337: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5
2023-06-06 21:40:11.641346: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5
2023-06-06 21:40:11.641353: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (2): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5
2023-06-06 21:40:11.641359: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (3): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5
2023-06-06 21:40:11.646324: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-06-06 21:40:11.673008: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:530] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.
Searched for CUDA in the following directories:
  ./cuda_sdk_lib
  /usr/local/cuda-11.8
  /usr/local/cuda
  .
You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2023-06-06 21:40:11.673259: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.673658: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.673685: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INTERNAL: libdevice not found at ./libdevice.10.bc
	 [[{{node StatefulPartitionedCall_16}}]]
2023-06-06 21:40:11.700045: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.700414: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.729242: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.729590: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.755959: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.756308: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.783041: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.783397: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.809786: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.810134: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.836777: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.837129: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.864411: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.864767: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.892388: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.892738: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.919296: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.919647: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.946506: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.946866: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.974287: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.974699: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:12.245782: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:12.246188: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:12.272303: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:12.272657: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:12.322559: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:12.322899: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:12.350255: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:12.350736: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:12.379576: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:12.379966: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:12.406780: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:12.407233: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc

---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
Cell In[33], line 31
      7 spec_embedder = ParametricUMAP(
      8     metric = 'euclidean',
      9     min_dist = 0.1, 
   (...)
     27     n_training_epochs=1
     28 )
     30 with tf.device('/GPU:0'):
---> 31     spec_embedding = spec_embedder.fit_transform(np.array([t.ravel() for t in train_data]))

File ~/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/parametric_umap.py:217, in ParametricUMAP.fit_transform(self, X, y, precomputed_distances)
    215     return super().fit_transform(precomputed_distances, y)
    216 else:
--> 217     return super().fit_transform(X, y)

File ~/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/umap_.py:2772, in UMAP.fit_transform(self, X, y)
   2742 def fit_transform(self, X, y=None):
   2743     """"""Fit X into an embedded space and return that transformed
   2744     output.
   2745 
   (...)
   2770         Local radii of data points in the embedding (log-transformed).
   2771     """"""
-> 2772     self.fit(X, y)
   2773     if self.transform_mode == ""embedding"":
   2774         if self.output_dens:

File ~/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/parametric_umap.py:202, in ParametricUMAP.fit(self, X, y, precomputed_distances)
    200     return super().fit(precomputed_distances, y)
    201 else:
--> 202     return super().fit(X, y)

File ~/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/umap_.py:2684, in UMAP.fit(self, X, y)
   2681     print(ts(), ""Construct embedding"")
   2683 if self.transform_mode == ""embedding"":
-> 2684     self.embedding_, aux_data = self._fit_embed_data(
   2685         self._raw_data[index],
   2686         self.n_epochs,
   2687         init,
   2688         random_state,  # JH why raw data?
   2689     )
   2690     # Assign any points that are fully disconnected from our manifold(s) to have embedding
   2691     # coordinates of np.nan.  These will be filtered by our plotting functions automatically.
   2692     # They also prevent users from being deceived a distance query to one of these points.
   2693     # Might be worth moving this into simplicial_set_embedding or _fit_embed_data
   2694     disconnected_vertices = np.array(self.graph_.sum(axis=1)).flatten() == 0

File ~/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/parametric_umap.py:462, in ParametricUMAP._fit_embed_data(self, X, n_epochs, init, random_state)
    459     validation_data = None
    461 # create embedding
--> 462 history = self.parametric_model.fit(
    463     edge_dataset,
    464     epochs=self.loss_report_frequency * self.n_training_epochs,
    465     steps_per_epoch=steps_per_epoch,
    466     max_queue_size=100,
    467     validation_data=validation_data,
    468     **self.keras_fit_kwargs
    469 )
    470 # save loss history dictionary
    471 self._history = history.history

File ~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/miniconda3/envs/conda01/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50 try:
     51   ctx.ensure_initialized()
---> 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                       inputs, attrs, num_outputs)
     54 except core._NotOkStatusException as e:
     55   if name is not None:

InternalError: Graph execution error:

Detected at node 'StatefulPartitionedCall_16' defined at (most recent call last):
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel_launcher.py"", line 17, in <module>
      app.launch_new_instance()
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel/kernelapp.py"", line 711, in start
      self.io_loop.start()
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/tornado/platform/asyncio.py"", line 215, in start
      self.asyncio_loop.run_forever()
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
      self._run_once()
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
      handle._run()
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 510, in dispatch_queue
      await self.process_one()
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 499, in process_one
      await dispatch(*args)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 406, in dispatch_shell
      await result
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 729, in execute_request
      reply_content = await reply_content
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 411, in do_execute
      res = shell.run_cell(
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel/zmqshell.py"", line 531, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3006, in run_cell
      result = self._run_cell(
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3061, in _run_cell
      result = runner(coro)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
      coro.send(None)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3266, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3445, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3505, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""/tmp/ipykernel_3110734/3903349550.py"", line 31, in <module>
      spec_embedding = spec_embedder.fit_transform(np.array([t.ravel() for t in train_data]))
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/parametric_umap.py"", line 217, in fit_transform
      return super().fit_transform(X, y)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/umap_.py"", line 2772, in fit_transform
      self.fit(X, y)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/parametric_umap.py"", line 202, in fit
      return super().fit(X, y)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/umap_.py"", line 2684, in fit
      self.embedding_, aux_data = self._fit_embed_data(
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/parametric_umap.py"", line 462, in _fit_embed_data
      history = self.parametric_model.fit(
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/engine/training.py"", line 1685, in fit
      tmp_logs = self.train_function(iterator)
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/engine/training.py"", line 1284, in train_function
      return step_function(self, iterator)
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/engine/training.py"", line 1268, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/engine/training.py"", line 1249, in run_step
      outputs = model.train_step(data)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/parametric_umap.py"", line 1150, in train_step
      self.optimizer.apply_gradients(zip(gradients, trainable_vars))
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 1174, in apply_gradients
      return super().apply_gradients(grads_and_vars, name=name)
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 650, in apply_gradients
      iteration = self._internal_apply_gradients(grads_and_vars)
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 1200, in _internal_apply_gradients
      return tf.__internal__.distribute.interim.maybe_merge_call(
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 1250, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 1245, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall_16'
libdevice not found at ./libdevice.10.bc
	 [[{{node StatefulPartitionedCall_16}}]] [Op:__inference_train_function_4496]
```
</details>"
60789,ELU int8 model quantized with Dequantize/Quantize stubs,"**System information**
- Linux Ubuntu 20.04
- TensorFlow installed from: docker `tensorflow/tensorflow:latest-gpu`
- TensorFlow version (or github SHA if from source): 2.12.0


**Standalone code to reproduce the issue** 
Input model (Netron):
![image](https://github.com/tensorflow/tensorflow/assets/10706289/3bc57f63-09b4-4e4b-b907-2a5c795e2e29)

```python
import tensorflow as tf
import numpy as np

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation=None),
  #  tf.keras.layers.ReLU(),
  tf.keras.layers.ELU(),
  tf.keras.layers.Dense(10, activation='softmax')
])

# full-integer quantization, simulate dataset with random input
def representative_data_gen():
  for input_value in [np.random.randn(1, 28, 28).astype(np.float32) for _ in range(10)]:
    yield [input_value] 

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.target_spec.supported_types = {tf.int8}

converter.inference_input_type = tf.int8 #tf.uint8
converter.inference_output_type = tf.int8 #tf.uint8

tflite_model_quant = converter.convert()
with open('basic_fullint_quant.tflite', 'wb') as f:
  f.write(tflite_model_quant)
```

Output Model (Netron):
![image](https://github.com/tensorflow/tensorflow/assets/10706289/e3f77101-d091-42ee-a909-84c940ea948d)


**Any other info / logs**

I understand that ELU isn't among the supported operators (?) although there's allegedly some code (?) from [this commit](https://github.com/tensorflow/tensorflow/commit/918f876bf812fd744151fea29b2df4aa18acfa8f). 
I wonder why no exception is raised during the process despite `supported_ops` being set to the most strict options, and why does it default to this mixed-precision output model (if I am not mistaken, the elu is still run in floating point).  

To be clear, the behavior I would expect is either an error or a full-int8 quantized model with no stubs layers wrapping ELU. 
Thanks for your help!"
60788,'fashion_mnist' failed to load on TPU (try_gcs=True not working)!,"I am trying to replicate TensorFlow `autoencoder` (Source: [Second example: Image denoising](https://www.tensorflow.org/tutorials/generative/autoencoder#second_example_image_denoising))  image cleaning example on TPU (in google colab). 

Code : 
```python3
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow_datasets as tfds
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D
from tensorflow.keras.models import Model

tfds.disable_progress_bar()                                                     # disables Tqdm progress bar

# Connect to TPU
resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)

# Define the autoencoder model
def build_autoencoder():
    input_img = Input(shape=(28, 28, 1))
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)
    x = MaxPooling2D((2, 2), padding='same')(x)
    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2), padding='same')(x)
    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)
    encoded = MaxPooling2D((2, 2), padding='same')(x)

    x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)
    x = UpSampling2D((2, 2))(x)
    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = Conv2D(16, (3, 3), activation='relu')(x)
    x = UpSampling2D((2, 2))(x)
    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)

    autoencoder = Model(input_img, decoded)
    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
    return autoencoder

# Load MNIST dataset
(ds_train, ds_test), ds_info = tfds.load('fashion_mnist', split=['train', 'test'], shuffle_files=True, as_supervised=True, with_info=True,
                                         try_gcs = True)

# This works fine !!!!!!!!!!!!!!!!!!!!!!!!! (Mean no error in code !)
#(ds_train, ds_test), ds_info = tfds.load('mnist', split=['train', 'test'], shuffle_files=True, as_supervised=True, with_info=True,
#                                         try_gcs = True)


# Define input preprocessing function
def preprocess_input(image, label,labels:tf.Tensor=None,noise_mean:float=0.0,noise_stddev:float=0.05):
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.image.resize(image, (28, 28))  # Resize images to (28, 28)
    #image_noise =  tf.add(image,tf.random.normal(image.shape,mean=noise_mean,stddev=noise_stddev))
    #image_noise = tf.clip_by_value(image_noise,tf.reduce_min(image),tf.reduce_max(image))
    return image, image #image_noise

# Preprocess and batch the dataset
AUTOTUNE = tf.data.AUTOTUNE
BATCH_SIZE = 128 * strategy.num_replicas_in_sync

ds_train = ds_train.map(preprocess_input, num_parallel_calls=AUTOTUNE)
ds_train = ds_train.cache()
ds_train = ds_train.batch(BATCH_SIZE)
ds_train = ds_train.prefetch(AUTOTUNE)

ds_test = ds_test.map(preprocess_input, num_parallel_calls=AUTOTUNE)
ds_test = ds_test.cache()
ds_test = ds_test.batch(BATCH_SIZE)
ds_test = ds_test.prefetch(AUTOTUNE)

# Create a TPU model
with strategy.scope():
    autoencoder = build_autoencoder()

# Train the model
autoencoder.fit(ds_train,epochs=10,validation_data=ds_test)
``` 

Error 
```bash
UnimplementedError: 9 root error(s) found.
  (0) UNIMPLEMENTED: {{function_node __inference_train_function_145566}} File system scheme '[local]' not implemented (file: '/bufferedio/root/tensorflow_datasets/fashion_mnist/3.0.1/fashion_mnist-train.tfrecord-00000-of-00001')
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional_7]]
  (1) UNIMPLEMENTED: {{function_node __inference_train_function_145566}} File system scheme '[local]' not implemented (file: '/bufferedio/root/tensorflow_datasets/fashion_mnist/3.0.1/fashion_mnist-train.tfrecord-00000-of-00001')
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional_2]]
  (2) UNIMPLEMENTED: {{function_node __inference_train_function_145566}} File system scheme '[local]' not implemented (file: '/bufferedio/root/tensorflow_datasets/fashion_mnist/3.0.1/fashion_mnist-train.tfrecord-00000-of-00001')
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional_1]]
  (3) UNIMPLEMENTED: {{function_node __inference_train_function_145566}} File system scheme '[local]' not implemented (file: '/bufferedio/root/tensorflow_datasets/fashion_mnist/3.0.1/fashion_mnist-train.tfrecord-00000-of-00001')
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[strided_slice_39/_314]]
  (4) UNIMPLEMENTED: {{function_node __inference_train_function_145566}} File system scheme '[local]' not implemented (file: '/bufferedio/root/tensorflow_datasets/fashion_mnist/3.0.1/fashion_mnist-train.tfrecord-00000-of-00001')
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[strided_slice_30/_296]]
  (5) UNIMPLEMENTED: {{function_node __inference_train_function_145566}} File system scheme '[local]' not implemented (file: '/bufferedio/root/tensorflow_datasets/fashion_mnist/3.0.1/fashion_mnist-train.tfrecord-00000-of-00001')
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[strided_slice_21/_266]]
  (6) UNIMPLEMENTED: {{function_node __inference_train_function_145566}} File system scheme '[local]' not implemented (file: '/bufferedio/root/tensorflow_datasets/fashion_mnist/3.0.1/fashion_mnist-train.tfrecord-00000-of-00001')
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[cond/then/_0/cond/cond_1/output/_121/_182]]
  (7) UNIMPLEMENTED: {{function_node __inference_train_function_145566}} File system scheme '[local]' not implemented (file: '/bufferedio/root/tensorflow_datasets/fashion_mnist/3.0.1/fashion_mnist-train.tfrecord-00000-of-00001')
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[Greater/_26]]
  (8) UNIMPLEMENTED: {{function_node __inference_train_function_145566}} File system scheme '[local]' not implemented (file: '/bufferedio/root/tensorflow_datasets/fashion_mnist/3.0.1/fashion_mnist-train.tfrecord-00000-of-00001')
	 [[{{node MultiDeviceIteratorGe ... [truncated]
```

This code works fine with MNIST dataset but shows following error on Fashion MNIST. I think this might be the problem with `try_gcs` argument in tensorflow_datasets.load . "
60787,"Minimal TfLite program generates Valgrind ""still reachable"" messages","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10

### Custom Code

No

### OS Platform and Distribution

Linux64 Ubuntu 22.10

### Mobile device

_No response_

### Python version

3.10.7 (irrelevant for the problem)

### Bazel version

_No response_

### GCC/Compiler version

gcc 12.2.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Minimal TfLite (2.10) program generates Valgrind ""still reachable"" message. See accompanied code and Valgrind log. The network cannot be shared (company intellectual property), but this should not matter to reproduce the problem.

You can get rid off the ""still reachable"" message from `BuildFromFile` by using:
`delete model->error_reporter();`
Still, shouldn't TfLite take care of this? The use of smart pointers suggests that the user does not need to manage TfLite's memory.

My question:
How to get rid off the ""still reachable"" messages from `AllocateTensors`?

What I tried (but which does _not_ get rid off the ""still reachable"" messages):
Manually delete the model and interpreter at the end:
`interpreter.reset(nullptr);`
`model.reset(nullptr);`
Clear two vectors of `BuiltinOpResolver`:
`op_resolver.GetDelegateCreators().clear();`
`op_resolver.GetOpaqueDelegateCreators().clear();`

What I also tried is replace `BuiltinOpResolver` with `BuiltinOpResolverWithoutDefaultDelegates`. Then you _do_ get rid off ""still reachable"" messages from `AllocateTensors`. With `BuiltinOpResolverWithoutDefaultDelegates` the XNNPACK backend is not employed, so seemingly the ""still reachable"" messages have to do with XNNPACK.
XNNPACK documentation says: ""To avoid memory and resource leaks, users must call xnn_deinitialize once for each successful xnn_initialize call."" `xnn_deinitialize` is called in function `TfLiteXNNPackWeightsCacheDelete` (tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc). But `TfLiteXNNPackWeightsCacheDelete` is never called. I believe this causes the memory problem, TfLite never seems to call `xnn_deinitialize`.

My company always keeps a clean Valgrind report for their software, so even if these Valgrind messages are not harmful, I still would like to get rid off them.

### Standalone code to reproduce the issue

```shell
#include <tensorflow/lite/model.h>
#include <tensorflow/lite/kernels/register.h>
#include <tensorflow/lite/interpreter.h>

using namespace tflite;
using namespace std;

int main() {
	unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""classifier.tflite"");  //This line generates one ""still reachable"" message
	const tflite::ops::builtin::BuiltinOpResolver op_resolver;
	tflite::InterpreterBuilder interpreter_builder(*model, op_resolver);
	interpreter_builder.SetNumThreads(1);

	unique_ptr<tflite::Interpreter> interpreter;
	interpreter_builder(&interpreter);
	interpreter->AllocateTensors();  //This line generates many ""still reachable"" messages

	//delete model->error_reporter();  //This will get rid off the first ""still reachable"" message
	return EXIT_SUCCESS;
}
```


### Relevant log output

```shell
$ valgrind --leak-check=full --show-reachable=yes ./app
==988== Memcheck, a memory error detector
==988== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==988== Using Valgrind-3.18.1 and LibVEX; rerun with -h for copyright info
==988== Command: ./app
==988==
==988== error calling PR_SET_PTRACER, vgdb might block
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
==988==
==988== HEAP SUMMARY:
==988==     in use at exit: 704 bytes in 11 blocks
==988==   total heap usage: 2,144 allocs, 2,133 frees, 4,481,261 bytes allocated
==988==
==988== 8 bytes in 1 blocks are still reachable in loss record 1 of 11
==988==    at 0x4845013: operator new(unsigned long) (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x1F6CF9: tflite::DefaultErrorReporter() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FC7C: main (Main.cpp:9)
==988==
==988== 32 bytes in 1 blocks are still reachable in loss record 2 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55A454: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 32 bytes in 1 blocks are still reachable in loss record 3 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55A46F: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 32 bytes in 1 blocks are still reachable in loss record 4 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55B06F: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 32 bytes in 1 blocks are still reachable in loss record 5 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55B017: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 32 bytes in 1 blocks are still reachable in loss record 6 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55AFCF: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 32 bytes in 1 blocks are still reachable in loss record 7 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55AF96: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 56 bytes in 1 blocks are still reachable in loss record 8 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55A491: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 56 bytes in 1 blocks are still reachable in loss record 9 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55A4B3: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPaeckDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 72 bytes in 1 blocks are still reachable in loss record 10 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55A4D5: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== 320 bytes in 1 blocks are still reachable in loss record 11 of 11
==988==    at 0x4849A83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==988==    by 0x55A11E: cpuinfo_x86_linux_init (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x4C28576: __pthread_once_slow (pthread_once.c:116)
==988==    by 0x5568CA: cpuinfo_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x49EB7F: xnn_initialize (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x20F1B7: TfLiteXNNPackDelegateCreate (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1F74C0: tflite::MaybeCreateXNNPACKDelegate(int) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x122C33: std::_Function_handler<std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)> (int), tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7945: tflite::Interpreter::ApplyLazyDelegateProviders() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x1E7B2C: tflite::Interpreter::AllocateTensors() (in /home/personau/SvnDnnProjects/research/DnnBackendTfLite/MyDevX64/app)
==988==    by 0x11FD23: main (Main.cpp:16)
==988==
==988== LEAK SUMMARY:
==988==    definitely lost: 0 bytes in 0 blocks
==988==    indirectly lost: 0 bytes in 0 blocks
==988==      possibly lost: 0 bytes in 0 blocks
==988==    still reachable: 704 bytes in 11 blocks
==988==         suppressed: 0 bytes in 0 blocks
==988==
==988== For lists of detected and suppressed errors, rerun with: -s
==988== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
```
</details>"
60785,"Inference model error when xla enabled with error message ""OP_REQUIRES failed at xla_ops.cc:462 : NOT_FOUND: could not find registered platform with id: 0x7f7537df9c24""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf2.9.2

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

Python 3.10.6

### Bazel version

5.3.2

### GCC/Compiler version

gcc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0

### CUDA/cuDNN version

11.6

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!

### Standalone code to reproduce the issue

```shell
inference model error in c++ running with xla enabled. when xla disabled, all works fine.

run with option:
export XLA_FLAGS=""--xla_dump_to=/tmp/generated --xla_hlo_profile""
export TF_XLA_FLAGS=""--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit""
```


### Relevant log output

```shell
2023-06-06 13:54:27.650731: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:462 : NOT_FOUND: could not find registered platform with id: 0x7f7537df9c2
```
</details>"
60784,tflite-support installation issue in Kaggle,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

kaggle

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

`!pip install tflite-support` gives following error-
`note: This error originates from a subprocess, and is likely not a problem with pip.`
`error: subprocess-exited-with-error`
`ERROR: Could not build wheels for tflite-support, which is required to install pyproject.toml-based projects`


### Standalone code to reproduce the issue

```shell
complete logs are attached already
```


### Relevant log output

```shell
Collecting tflite-support
  Downloading tflite-support-0.1.0a1.tar.gz (390 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 390.3/390.3 kB 7.2 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... - \ done
Requirement already satisfied: pybind11>=2.4 in /opt/conda/lib/python3.10/site-packages (from tflite-support) (2.10.4)
Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tflite-support) (1.4.0)
Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tflite-support) (1.23.5)
Building wheels for collected packages: tflite-support
  Building wheel for tflite-support (setup.py) ... - \ | / - error
  error: subprocess-exited-with-error
  
  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─> [140 lines of output]
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build/lib.linux-x86_64-3.10
      creating build/lib.linux-x86_64-3.10/tflite_support
      copying tflite_support/metadata_schema_py_generated.py -> build/lib.linux-x86_64-3.10/tflite_support
      copying tflite_support/__init__.py -> build/lib.linux-x86_64-3.10/tflite_support
      copying tflite_support/schema_py_generated.py -> build/lib.linux-x86_64-3.10/tflite_support
      copying tflite_support/metadata.py -> build/lib.linux-x86_64-3.10/tflite_support
      copying tflite_support/codegen.py -> build/lib.linux-x86_64-3.10/tflite_support
      running egg_info
      writing tflite_support.egg-info/PKG-INFO
      writing dependency_links to tflite_support.egg-info/dependency_links.txt
      writing entry points to tflite_support.egg-info/entry_points.txt
      writing requirements to tflite_support.egg-info/requires.txt
      writing top-level names to tflite_support.egg-info/top_level.txt
      /opt/conda/lib/python3.10/site-packages/setuptools_scm/integration.py:28: RuntimeWarning:
      ERROR: setuptools==41.6.0 is used in combination with setuptools_scm>=6.x
      
      Your build configuration is incomplete and previously worked by accident!
      setuptools_scm requires setuptools>=45
      
      
      This happens as setuptools is unable to replace itself when a activated build dependency
      requires a more recent setuptools version
      (it does not respect ""setuptools>X"" in setup_requires).
      
      
      setuptools>=31 is required for setup.cfg metadata support
      setuptools>=42 is required for pyproject.toml configuration support
      
      Suggested workarounds if applicable:
       - preinstalling build dependencies like setuptools_scm before running setup.py
       - installing setuptools_scm using the system package manager to ensure consistency
       - migrating from the deprecated setup_requires mechanism to pep517/518
         and using a pyproject.toml to declare build dependencies
         which are reliably pre-installed before running the build tools
      
        warnings.warn(
      reading manifest file 'tflite_support.egg-info/SOURCES.txt'
      reading manifest template 'MANIFEST.in'
      writing manifest file 'tflite_support.egg-info/SOURCES.txt'
      copying tflite_support/metadata_schema.fbs -> build/lib.linux-x86_64-3.10/tflite_support
      creating build/lib.linux-x86_64-3.10/tflite_support/flatbuffers
      copying tflite_support/flatbuffers/__init__.py -> build/lib.linux-x86_64-3.10/tflite_support/flatbuffers
      copying tflite_support/flatbuffers/builder.py -> build/lib.linux-x86_64-3.10/tflite_support/flatbuffers
      copying tflite_support/flatbuffers/compat.py -> build/lib.linux-x86_64-3.10/tflite_support/flatbuffers
      copying tflite_support/flatbuffers/encode.py -> build/lib.linux-x86_64-3.10/tflite_support/flatbuffers
      copying tflite_support/flatbuffers/number_types.py -> build/lib.linux-x86_64-3.10/tflite_support/flatbuffers
      copying tflite_support/flatbuffers/packer.py -> build/lib.linux-x86_64-3.10/tflite_support/flatbuffers
      copying tflite_support/flatbuffers/table.py -> build/lib.linux-x86_64-3.10/tflite_support/flatbuffers
      copying tflite_support/flatbuffers/util.py -> build/lib.linux-x86_64-3.10/tflite_support/flatbuffers
      running build_ext
      creating tmp
      gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/include/python3.10 -c /tmp/tmpf6wu46to.cc -o tmp/tmpf6wu46to.o -std=c++14
      gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/include/python3.10 -c /tmp/tmpchd3i2qh.cc -o tmp/tmpchd3i2qh.o -fvisibility=hidden
      building '_pywrap_codegen' extension
      creating build/temp.linux-x86_64-3.10
      creating build/temp.linux-x86_64-3.10/src
      creating build/temp.linux-x86_64-3.10/src/tensorflow
      creating build/temp.linux-x86_64-3.10/src/tensorflow/lite
      creating build/temp.linux-x86_64-3.10/src/tensorflow/lite/experimental
      creating build/temp.linux-x86_64-3.10/src/tensorflow/lite/experimental/support
      creating build/temp.linux-x86_64-3.10/src/tensorflow/lite/experimental/support/codegen
      gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/lib/python3.10/site-packages/pybind11/include -I/opt/conda/lib/python3.10/site-packages/pybind11/include -Iinclude -Isrc -I/opt/conda/include/python3.10 -c src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc -o build/temp.linux-x86_64-3.10/src/tensorflow/lite/experimental/support/codegen/android_java_generator.o -DVERSION_INFO=""0.1.0a1"" -std=c++14 -fvisibility=hidden
      In file included from include/flatbuffers/base.h:217,
                       from include/flatbuffers/flatbuffers.h:20,
                       from src/tensorflow/lite/experimental/support/metadata/metadata_schema_generated.h:21,
                       from src/tensorflow/lite/experimental/support/codegen/code_generator.h:25,
                       from src/tensorflow/lite/experimental/support/codegen/android_java_generator.h:23,
                       from src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc:16:
      /opt/conda/include/absl/strings/string_view.h:52:26: error: ‘string_view’ in namespace ‘std’ does not name a type
         52 | using string_view = std::string_view;
            |                          ^~~~~~~~~~~
      /opt/conda/include/absl/strings/string_view.h:52:21: note: ‘std::string_view’ is only available from C++17 onwards
         52 | using string_view = std::string_view;
            |                     ^~~
      /opt/conda/include/absl/strings/string_view.h:686:8: error: ‘string_view’ does not name a type
        686 | inline string_view ClippedSubstr(string_view s, size_t pos,
            |        ^~~~~~~~~~~
      /opt/conda/include/absl/strings/string_view.h:697:11: error: ‘string_view’ does not name a type
        697 | constexpr string_view NullSafeStringView(const char* p) {
            |           ^~~~~~~~~~~
      In file included from include/flatbuffers/flatbuffers.h:20,
                       from src/tensorflow/lite/experimental/support/metadata/metadata_schema_generated.h:21,
                       from src/tensorflow/lite/experimental/support/codegen/code_generator.h:25,
                       from src/tensorflow/lite/experimental/support/codegen/android_java_generator.h:23,
                       from src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc:16:
      include/flatbuffers/base.h:219:23: error: ‘string_view’ in namespace ‘absl’ does not name a type
        219 |         typedef absl::string_view string_view;
            |                       ^~~~~~~~~~~
      In file included from src/tensorflow/lite/experimental/support/metadata/metadata_schema_generated.h:21,
                       from src/tensorflow/lite/experimental/support/codegen/code_generator.h:25,
                       from src/tensorflow/lite/experimental/support/codegen/android_java_generator.h:23,
                       from src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc:16:
      include/flatbuffers/flatbuffers.h:552:16: error: ‘string_view’ in namespace ‘flatbuffers’ does not name a type
        552 |   flatbuffers::string_view string_view() const {
            |                ^~~~~~~~~~~
      include/flatbuffers/flatbuffers.h:1478:44: error: ‘flatbuffers::string_view’ has not been declared
       1478 |   Offset<String> CreateString(flatbuffers::string_view str) {
            |                                            ^~~~~~~~~~~
      include/flatbuffers/flatbuffers.h: In member function ‘flatbuffers::Offset<flatbuffers::String> flatbuffers::FlatBufferBuilder::CreateString(int)’:
      include/flatbuffers/flatbuffers.h:1479:29: error: request for member ‘data’ in ‘str’, which is of non-class type ‘int’
       1479 |     return CreateString(str.data(), str.size());
            |                             ^~~~
      include/flatbuffers/flatbuffers.h:1479:41: error: request for member ‘size’ in ‘str’, which is of non-class type ‘int’
       1479 |     return CreateString(str.data(), str.size());
            |                                         ^~~~
      src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc: In function ‘tflite::support::codegen::details_android_java::ModelInfo tflite::support::codegen::{anonymous}::CreateModelInfo(const tflite::ModelMetadata*, const string&, const string&, const string&, tflite::support::codegen::ErrorReporter*)’:
      src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc:157:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘flatbuffers::uoffset_t’ {aka ‘unsigned int’} [-Wsign-compare]
        157 |   for (int i = 0; i < graph->input_tensor_metadata()->size(); i++) {
            |                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
      src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc:162:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘flatbuffers::uoffset_t’ {aka ‘unsigned int’} [-Wsign-compare]
        162 |   for (int i = 0; i < graph->output_tensor_metadata()->size(); i++) {
            |                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
      src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc: In function ‘bool tflite::support::codegen::{anonymous}::GenerateWrapperImports(tflite::support::codegen::CodeWriter*, const tflite::support::codegen::details_android_java::ModelInfo&, tflite::support::codegen::ErrorReporter*)’:
      src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc:278:19: warning: loop variable ‘target’ creates a copy from type ‘const std::__cxx11::basic_string<char>’ [-Wrange-loop-construct]
        278 |   for (const auto target : imports) {
            |                   ^~~~~~
      src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc:278:19: note: use reference type to prevent copying
        278 |   for (const auto target : imports) {
            |                   ^~~~~~
            |                   &
      src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc: In function ‘bool tflite::support::codegen::{anonymous}::GenerateWrapperOutputs(tflite::support::codegen::CodeWriter*, const tflite::support::codegen::details_android_java::ModelInfo&, tflite::support::codegen::ErrorReporter*)’:
      src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc:471:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<tflite::support::codegen::details_android_java::TensorInfo>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]
        471 |     for (int i = 0; i < model.outputs.size(); i++) {
            |                     ~~^~~~~~~~~~~~~~~~~~~~~~
      src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc: In function ‘bool tflite::support::codegen::{anonymous}::GenerateWrapperMetadata(tflite::support::codegen::CodeWriter*, const tflite::support::codegen::details_android_java::ModelInfo&, tflite::support::codegen::ErrorReporter*)’:
      src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc:517:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<tflite::support::codegen::details_android_java::TensorInfo>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]
        517 |     for (int i = 0; i < model.inputs.size(); i++) {
            |                     ~~^~~~~~~~~~~~~~~~~~~~~
      src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc:536:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<tflite::support::codegen::details_android_java::TensorInfo>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]
        536 |     for (int i = 0; i < model.outputs.size(); i++) {
            |                     ~~^~~~~~~~~~~~~~~~~~~~~~
      src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc: In function ‘tflite::support::codegen::GenerationResult::File tflite::support::codegen::{anonymous}::GenerateDoc(const string&, const tflite::support::codegen::details_android_java::ModelInfo&, tflite::support::codegen::ErrorReporter*)’:
      src/tensorflow/lite/experimental/support/codegen/android_java_generator.cc:927:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::__cxx11::basic_string<char>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]
        927 |   for (int i = 0; i < lower.length(); i++) {
            |                   ~~^~~~~~~~~~~~~~~~
      error: command '/usr/bin/gcc' failed with exit code 1
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for tflite-support
  Running setup.py clean for tflite-support
Failed to build tflite-support
ERROR: Could not build wheels for tflite-support, which is required to install pyproject.toml-based projects
```
</details>"
60783,CUDNN failed to allocate the scratch space for the runner,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug


### Source

source

### Tensorflow Version

Tensorlfow 2.12

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04.2


### Python version

3.9


### CUDA/cuDNN version

Cuda 11.8,cuDNN  8.6.0

### GPU model and memory

GTX 4070, Vram 11178/12282Mib

### Current Behaviour?

In the middle of training I suddenly get
`Node: 'gradient_tape/model/conv3d_20/Conv3D/Conv3DBackpropFilterV2'
CUDNN failed to allocate the scratch space for the runner or to find a working no-scratch runner.`

I am running 3d Unet segmentation, my dataset is custom generator going through Dataset.I am using multiprocessing. Exact same code and model ran without this error in windows. It was slow so I moved to linux. It is not easy replicating the issue as it sometimes happens so many epochs after.




"
60781,"when  ""docker image build -t spot_rna2 .""have a error","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.0

### Custom Code

Yes

### OS Platform and Distribution

windows

### Mobile device

20.04

### Python version

3.8

### Bazel version

no

### GCC/Compiler version

no

### CUDA/cuDNN version

no

### GPU model and memory

no

### Current Behaviour?

A bug happened!

### Standalone code to reproduce the issue

```shell
=> ERROR [ 4/13] RUN cpanm Graph                                                                                 26.2s
------
 > [ 4/13] RUN cpanm Graph:
#0 14.35 --> Working on Graph
#0 14.35 Fetching http://www.cpan.org/authors/id/E/ET/ETJ/Graph-0.9726.tar.gz ... OK
#0 26.23
#0 26.23 gzip: stdin: unexpected end of file
#0 26.23 /bin/tar: Child returned status 1
#0 26.23 /bin/tar: Error is not recoverable: exiting now
#0 26.23 ! Failed to unpack Graph-0.9726.tar.gz: no directory
#0 26.23 ! Failed to fetch distribution Graph-0.9726
------
Dockerfile:6
--------------------
   4 |     RUN rm /bin/sh && ln -s /bin/bash /bin/sh
   5 |     RUN apt-get update && apt-get install -y build-essential wget virtualenv git python-minimal cpanminus gawk
   6 | >>> RUN cpanm Graph
   7 |
   8 |     RUN wget 'https://www.dropbox.com/s/h6j53u7wjyj6uir/SPOT-RNA2.tar.xz' || wget 'https://app.nihaocloud.com/f/3e826caf8efc43adaaa0/?dl=1' && tar -xvf SPOT-RNA2.tar.xz && rm SPOT-RNA2.tar.xz
--------------------
ERROR: failed to solve: process ""/bin/sh -c cpanm Graph"" did not complete successfully: exit code: 1
```


### Relevant log output

```shell
=> ERROR [ 4/13] RUN cpanm Graph                                                                                 26.2s
------
 > [ 4/13] RUN cpanm Graph:
#0 14.35 --> Working on Graph
#0 14.35 Fetching http://www.cpan.org/authors/id/E/ET/ETJ/Graph-0.9726.tar.gz ... OK
#0 26.23
#0 26.23 gzip: stdin: unexpected end of file
#0 26.23 /bin/tar: Child returned status 1
#0 26.23 /bin/tar: Error is not recoverable: exiting now
#0 26.23 ! Failed to unpack Graph-0.9726.tar.gz: no directory
#0 26.23 ! Failed to fetch distribution Graph-0.9726
------
Dockerfile:6
--------------------
   4 |     RUN rm /bin/sh && ln -s /bin/bash /bin/sh
   5 |     RUN apt-get update && apt-get install -y build-essential wget virtualenv git python-minimal cpanminus gawk
   6 | >>> RUN cpanm Graph
   7 |
   8 |     RUN wget 'https://www.dropbox.com/s/h6j53u7wjyj6uir/SPOT-RNA2.tar.xz' || wget 'https://app.nihaocloud.com/f/3e826caf8efc43adaaa0/?dl=1' && tar -xvf SPOT-RNA2.tar.xz && rm SPOT-RNA2.tar.xz
--------------------
ERROR: failed to solve: process ""/bin/sh -c cpanm Graph"" did not complete successfully: exit code: 1
```
</details>"
60780,About nn.Linear convert to tflite model,"### 1. System information

Linux Ubuntu 18.04
Tensorflow 2.8.0

### 2. Code

I have a transformer model with **pytorch**, and I use **onnx_tf** to change` .onnx` to` .pb`, then change` .pb` to` .tflite`. The` .pb ` to` .tflite` code is:
```
converter = tf.lite.TFLiteConverter.from_saved_model(path)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.TFLITE_BUILTINS_INT8,tf.lite.OpsSet.SELECT_TF_OPS]

tflite_model_quant = converter.convert()


import pathlib

tflite_models_dir = pathlib.Path('/home/puyiwen/deltar/')
tflite_models_dir.mkdir(exist_ok=True, parents=True)

# Save the quantized model:
tflite_model_quant_file = tflite_models_dir/""qat_quant_static_int8.tflite""
tflite_model_quant_file.write_bytes(tflite_model_quant)

converter = tf.lite.TFLiteConverter.from_saved_model(path)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
tflite_model_noquant = converter.convert()
tflite_model_noquant_file = tflite_models_dir/""qat_noquant_int8.tflite""
tflite_model_noquant_file.write_bytes(tflite_model_noquant)
```
Then I use **Netron** to check the tfilte model `qat_noquant_int8.tflite`, and I find the **nn.Linear** (which in **ONNX** is **MatMul**) in tflite model is disassembled into three operators: **spill-fullyconnected-pack**, I dont know why and I don't want it to be dismantled. 
Another question is there is a **DIV** operator in my model, I find that when tflite is quantized, it will automatically add a **Dequantize** before the **DIV** operator. This **Dequantize** is not in my code. I don't know why tflite will add **Dequantize** by itself, and I don't want it to be added. How should I modify the conversion tflite and statically quantize the code? Thank you very much!!

The **MatMul** in onnx is 
![onnx_image](https://github.com/tensorflow/tensorflow/assets/56880072/9d67d15f-5bab-4d0d-a80d-bbd250351b76)
The **MatMu**l in tflite is:
![tflite_image](https://github.com/tensorflow/tensorflow/assets/56880072/5eed9f64-6125-454a-bea3-b643794dbe54)

"
60779,"Error: Tensorflow lite c++ library, libtensorflowlite.so : Linking Error when compiling ( Undefined Reference )","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

Linux, Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

5.3.0

### GCC/Compiler version

11.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I cloned the tensorflow repo, then I built the **libtensorflowlite.so** using the command : 
`bazel build -c opt //tensorflow/lite:libtensorflowlite.so`

 Then copied the library to `/usr/local/lib` and When I compile using
 `g++ main.cpp -I/path/to/tensorflow/cloned/dir -L/usr/local/lib -ltensorflowlite` , I get the following error.


The main.cpp I used is from the tensorflow example codes.


/usr/bin/ld: /tmp/ccp1d7nd.o: in function `main':
test.cpp:(.text+0x3c): undefined reference to `tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'
/usr/bin/ld: test.cpp:(.text+0xb2): undefined reference to `tflite::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'
/usr/bin/ld: test.cpp:(.text+0xcb): undefined reference to `tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'
/usr/bin/ld: test.cpp:(.text+0xdf): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'
/usr/bin/ld: test.cpp:(.text+0x113): undefined reference to `tflite::Interpreter::AllocateTensors()'
/usr/bin/ld: test.cpp:(.text+0x195): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'
/usr/bin/ld: /tmp/ccp1d7nd.o: in function `std::default_delete<tflite::FlatBufferModel>::operator()(tflite::FlatBufferModel*) const':
test.cpp:(.text._ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_[_ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_]+0x22): undefined reference to `tflite::FlatBufferModel::~FlatBufferModel()'
/usr/bin/ld: /tmp/ccp1d7nd.o: in function `std::default_delete<tflite::Interpreter>::operator()(tflite::Interpreter*) const':
test.cpp:(.text._ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_[_ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_]+0x22): undefined reference to `tflite::Interpreter::~Interpreter()'
collect2: error: ld returned 1 exit status

### Standalone code to reproduce the issue

```shell
/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#include <cstdio>
#include <tensorflow/lite/interpreter.h>
#include <tensorflow/lite/kernels/register.h>
#include <tensorflow/lite/model.h>
#include <tensorflow/lite/optional_debug_tools.h>

// This is an example that is minimal to read a model
// from disk and perform inference. There is no data being loaded
// that is up to you to add as a user.
//
// NOTE: Do not add any dependencies to this that cannot be built with
// the minimal makefile. This example must remain trivial to build with
// the minimal build tool.
//
// Usage: minimal <tflite model>

#define TFLITE_MINIMAL_CHECK(x)                                  \
    if (!(x))                                                    \
    {                                                            \
        fprintf(stderr, ""Error at %s:%d\n"", __FILE__, __LINE__); \
        exit(1);                                                 \
    }

int main(int argc, char *argv[])
{
    if (argc != 2)
    {
        fprintf(stderr, ""minimal <tflite model>\n"");
        return 1;
    }
    const char *filename = argv[1];

    // Load model
    std::unique_ptr<tflite::FlatBufferModel> model =
        tflite::FlatBufferModel::BuildFromFile(filename);
    TFLITE_MINIMAL_CHECK(model != nullptr);

    // Build the interpreter with the InterpreterBuilder.
    // Note: all Interpreters should be built with the InterpreterBuilder,
    // which allocates memory for the Intrepter and does various set up
    // tasks so that the Interpreter can read the provided model.
    tflite::ops::builtin::BuiltinOpResolver resolver;
    tflite::InterpreterBuilder builder(*model, resolver);
    std::unique_ptr<tflite::Interpreter> interpreter;
    builder(&interpreter);
    TFLITE_MINIMAL_CHECK(interpreter != nullptr);

    // Allocate tensor buffers.
    TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);
    printf(""=== Pre-invoke Interpreter State ===\n"");
    tflite::PrintInterpreterState(interpreter.get());

    // Fill input buffers
    // TODO(user): Insert code to fill input tensors.
    // Note: The buffer of the input tensor with index `i` of type T can
    // be accessed with `T* input = interpreter->typed_input_tensor<T>(i);`

    // Run inference
    TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);
    printf(""\n\n=== Post-invoke Interpreter State ===\n"");
    tflite::PrintInterpreterState(interpreter.get());

    // Read output buffers
    // TODO(user): Insert getting data out code.
    // Note: The buffer of the output tensor with index `i` of type T can
    // be accessed with `T* output = interpreter->typed_output_tensor<T>(i);`

    return 0;
}
```


### Relevant log output

```shell
/usr/bin/ld: /tmp/ccp1d7nd.o: in function `main':
test.cpp:(.text+0x3c): undefined reference to `tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'
/usr/bin/ld: test.cpp:(.text+0xb2): undefined reference to `tflite::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'
/usr/bin/ld: test.cpp:(.text+0xcb): undefined reference to `tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'
/usr/bin/ld: test.cpp:(.text+0xdf): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'
/usr/bin/ld: test.cpp:(.text+0x113): undefined reference to `tflite::Interpreter::AllocateTensors()'
/usr/bin/ld: test.cpp:(.text+0x195): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'
/usr/bin/ld: /tmp/ccp1d7nd.o: in function `std::default_delete<tflite::FlatBufferModel>::operator()(tflite::FlatBufferModel*) const':
test.cpp:(.text._ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_[_ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_]+0x22): undefined reference to `tflite::FlatBufferModel::~FlatBufferModel()'
/usr/bin/ld: /tmp/ccp1d7nd.o: in function `std::default_delete<tflite::Interpreter>::operator()(tflite::Interpreter*) const':
test.cpp:(.text._ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_[_ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_]+0x22): undefined reference to `tflite::Interpreter::~Interpreter()'
collect2: error: ld returned 1 exit status
```
</details>"
60778,ConverterError: <unknown>:0: error,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Kaggle
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

![image](https://github.com/tensorflow/tensorflow/assets/30830541/8def72c1-d3ce-49eb-b446-e88aeeb17e36)

### 3. (optional) Any other info / logs
[converter_issue_tf.txt](https://github.com/tensorflow/tensorflow/files/11648334/converter_issue_tf.txt)

"
60777,Docker image issue,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf v2.13.0-rc0-26-g57633696be6 2.13.0-rc1

### Custom Code

Yes

### OS Platform and Distribution

docker tf 2.13.0rc1-gpu-jupyter

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

Build cuda_11.8.r11.8/compiler.31833905_0

### GPU model and memory

RTX 2060 6GB

### Current Behaviour?

Could not load library libcublasLt.so.12. Error: libcublasLt.so.12: cannot open shared object file: No such file or directory

### Standalone code to reproduce the issue

```shell
https://github.com/projjal1/English-French-Translator-RNN/blob/master/English_French_Translator.ipynb
```


### Relevant log output

_No response_</details>"
60776,Is there a similar approach in TensorFlow2 when save large model?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```python
from transformers import TFAutoModel
model = TFAutoModel.from_pretrained(""bert-base-cased"")
model.save_pretrained(""saved"", max_shard_size=""200MB"")
```

Is there a similar approach in TensorFlow2 when save large model?

### Standalone code to reproduce the issue

```shell
from transformers import TFAutoModel
model = TFAutoModel.from_pretrained(""bert-base-cased"")
model.save_pretrained(""saved"", max_shard_size=""200MB"")
```


### Relevant log output

_No response_</details>"
60775,Problem after installing new version using pip,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11 and 2.12

### Custom Code

No

### OS Platform and Distribution

Linux mint 20.1

### Mobile device

-

### Python version

3.9

### Bazel version

-

### GCC/Compiler version

-

### CUDA/cuDNN version

11.8.0 from anaconda / nvidia-cudnn-cu11==8.6.0.163 (from pip)

### GPU model and memory

rtx 3070 mobile 8 gb vram, 32 gb ram

### Current Behaviour?

After installing using this tutorial https://www.tensorflow.org/install/pip#linux i can't import tensorflow. Here is an error.

![image](https://github.com/tensorflow/tensorflow/assets/10774222/65d07dca-b143-4cd9-8649-c965cac744fb)

I've tried with 2.11, 2.12 TF installed using pip. 

If I'm trying to install tf-nightly, then I've got another error

![image](https://github.com/tensorflow/tensorflow/assets/10774222/45941dc6-da20-4b19-a75c-c77d221035e7)



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/daddywesker/anaconda3/envs/stdfex/lib/python3.9/site-packages/tensorflow/__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/daddywesker/anaconda3/envs/stdfex/lib/python3.9/site-packages/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/home/daddywesker/anaconda3/envs/stdfex/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 26, in <module>
    self_check.preload_check()
  File ""/home/daddywesker/anaconda3/envs/stdfex/lib/python3.9/site-packages/tensorflow/python/platform/self_check.py"", line 63, in preload_check
    from tensorflow.python.platform import _pywrap_cpu_feature_guard
ImportError: /home/daddywesker/anaconda3/envs/stdfex/lib/python3.9/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringB5cxx11ERKNS_15OpKernelContextEb
```
</details>"
60773,inconsistent .proto file package names break gRPC message/field parsing in Wireshark,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

As noted in #12445, there is inconsistency among the package names in the TensorFlow `.proto` files. Searching for `.proto` file package declarations within the codebase reveals a wide variety of package names, including `tensorflow.dummy`.
https://github.com/search?q=repo%3Atensorflow%2Ftensorflow+%22package+tensorflow%22&type=code&p=2

This has a problematic effect when trying to parse the Protobuf fields in TensorFlow gRPC messages within the [Wireshark](https://www.wireshark.org/) network capturing tool. In Wireshark, the built-in parsing functionality requires the package/service names within the `.proto` files to match the package/service names in the captured gRPC messages, so currently, [CoordinationService](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tsl/protobuf/coordination_service.proto) (`package tensorflow`) messages parse properly, while message types and field names in [WorkerService](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/worker_service.proto) (`package tensorflow.grpc`) messages cannot be parsed, and appear as _unknown_.

Current workaround: using a script to replace all instances of `""tensorflow.grpc""` with `""tensorflow""` in the `.proto` files.

### Standalone code to reproduce the issue

```shell
https://github.com/tensorflow/tensorflow/blob/d0863698de84277282df6f2865795aaa1e22ace5/tensorflow/tsl/protobuf/coordination_service.proto#L3

https://github.com/tensorflow/tensorflow/blob/d0863698de84277282df6f2865795aaa1e22ace5/tensorflow/core/protobuf/worker_service.proto#L18
```


### Relevant log output

_No response_</details>"
60772,Inefficient count_nonzero,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The current implementation of `tf.math.count_nonzero` is extremely inefficient, when trying to count how often a boolean condition is true in a large tensor. This is because the operation first converts the boolean tensor into `tf.int64`, before feeding it to `tf.reduce_sum`. As that is an operation that is entirely bandwidth-bound, instead of transferring `n` bytes for an input of `n` elements, now it has to transfer `n + 8n` for the cast operation, and an additional `8n` for the reduction, making this 17 times (!!) more inefficient than it needs to be.

The benchmark below show the following:
1) Plain `count_nonzero`
2) The same implementation as count_nonzero, except that a superfluous comparison with zero is omitted (which is needed for other data types to convert to bool, but appears to not be optimized away even if the tensor is already of bool type). This gives a small speed-up
3) Use `uint32` as accumulation type, instead of `int64`. This shows the tremendous effect of bandwidth.


Here are a few suggestions for improvements, in increasing order of difficulty:
1) If the input is already of bool type, there is no need to do `gen_math_ops.not_equal(input, zero)`
2) if the input is of bool type and has less than 2**32 elements, use `uint32` as the accumulator, instead of `int64`
3) implement a dedicated `count_nonzero` op that directly performs the reduction on boolean tensors
4) implement a generic `reduce_sum` that can have a different accumulator than the input type. This could also be helpful, e.g., for summing up lots of float/half values, where one would like to keep the error in check by using double/float accumulation.

If there is interest, I can provide a patch for 1 and 2. I'm not sure about 3, as I guess this new op would need to be implemented for CPU, GPU and TPU for this to make sense.

### Standalone code to reproduce the issue

```shell
Here is a very simple benchmark that illustrates the issue:

import tensorflow as tf
import time

print(""TF VERSION"", tf.version.VERSION)

data = tf.random.uniform(shape=(32, 1000000))
predicate = tf.greater(data, 0.5)

start = time.time()

for _ in range(100):
    a = tf.math.count_nonzero(predicate, axis=1)

print(time.time() - start)


start = time.time()

for _ in range(100):
    a = tf.math.reduce_sum(tf.cast(predicate, tf.int64), axis=1)

print(time.time() - start)


start = time.time()

for _ in range(100):
    a = tf.math.reduce_sum(tf.cast(predicate, tf.uint32), axis=1)

print(time.time() - start)
```


### Relevant log output

```shell
TF VERSION 2.12.0
4.673052549362183
4.253457069396973
2.3325610160827637
```
</details>"
60768,Cannot tune sequential model given get different results for each run,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am trying to tune a sequential model. But if I rerun my tuning code, I get a different result for ""best parameters"" each time. I am not using GPUs. I have seeds set to a constant.

My tuning code ran fine under an earlier tf version. But when I updated to latest, it stopped producing replicable results.

Did a specific version give up on replicability? Is there a workaround for tuning?

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1wxa5PCf7kQhTKrDFlbehq-k8MGAQpwkK?usp=sharing
```


### Relevant log output

_No response_</details>"
60766,Inconsistency-bug in `tf.raw_ops.AdjustContrastv2` between jit mode and normal mode,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

Ubuntu 20.04

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

There is an inconsistency bug between **jit compile mode** and **normal mode** in `tf.raw_ops.AdjustContrastv2` which result in inconsistent computational result.

### Standalone code to reproduce the issue

```shell
import os 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import numpy as np

images=tf.random.uniform([1, 1, 1], dtype=tf.dtypes.float32, maxval=100000000)
contrast_factor=tf.random.uniform([], dtype=tf.dtypes.float32, maxval=100000000)

@tf.function(jit_compile=True)
def fuzz_jit():
    y = tf.raw_ops.AdjustContrastv2(
        images = images,
        contrast_factor=contrast_factor
    )
    return y

def fuzz_normal():
    y = tf.raw_ops.AdjustContrastv2(
        images = images,
        contrast_factor=contrast_factor
    )
    return y

y1 = fuzz_jit()
print('[+] JIT ok')
y2 = fuzz_normal()
print('[+] Normal ok')
np.testing.assert_allclose(y1.numpy(), y2.numpy(), rtol=1e-4, atol=1e-4) # cause extramely different output
```


### Relevant log output

```shell
% python test.py
[+] JIT ok
[+] Normal ok
Traceback (most recent call last):
  File ""test.py"", line 28, in <module>
    np.testing.assert_allclose(y1.numpy(), y2.numpy(), rtol=1e-4, atol=1e-4)
  File ""/usr/local/lib/python3.8/dist-packages/numpy/testing/_private/utils.py"", line 1530, in assert_allclose
    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
  File ""/usr/local/lib/python3.8/dist-packages/numpy/testing/_private/utils.py"", line 844, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=0.0001, atol=0.0001

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1.6265184e+08
Max relative difference: 2.9464307
 x: array([[[-1.074488e+08]]], dtype=float32)
 y: array([[[55203008.]]], dtype=float32)
```
</details>"
60765,Inconsistency-bug in `tf.raw_ops.AddN` between jit mode and normal mode,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

Ubuntu 20.04

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

There is an inconsistency bug between **jit compile mode** and **normal mode** in `tf.raw_ops.AddN` which result in inconsistent computational result especially when the data type is `half`. But expectedly, the computational result have to be the same.

### Standalone code to reproduce the issue

```shell
import os 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import numpy as np

inputs = 44 * [tf.random.uniform([1, 2, 4, 3], dtype=tf.dtypes.half, maxval=1000)] # the half datatype!

@tf.function(jit_compile=True)
def fuzz_jit():
    y = tf.raw_ops.AddN(
        inputs = inputs
    )
    return y

def fuzz_normal():
    y = tf.raw_ops.AddN(
        inputs = inputs
    )
    return y

y1 = fuzz_jit()
print('[+] JIT ok')
y2 = fuzz_normal()
print('[+] Normal ok')
np.testing.assert_allclose(y1.numpy(), y2.numpy(), rtol=1e-4, atol=1e-4)
```


### Relevant log output

```shell
% python test.py
[+] JIT ok
[+] Normal ok
Traceback (most recent call last):
  File ""test.py"", line 25, in <module>
    np.testing.assert_allclose(y1.numpy(), y2.numpy(), rtol=1e-4, atol=1e-4)
  File ""/usr/local/lib/python3.8/dist-packages/numpy/testing/_private/utils.py"", line 1530, in assert_allclose
    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
  File ""/usr/local/lib/python3.8/dist-packages/numpy/testing/_private/utils.py"", line 844, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=0.0001, atol=0.0001

Mismatched elements: 20 / 24 (83.3%)
Max absolute difference: 160.
Max relative difference: 0.004887
 x: array([[[[14864., 41344., 39872.],
         [ 2492.,  6852., 17664.],
         [17344., 33216., 39968.],...
 y: array([[[[14912., 41376., 39808.],
         [ 2488.,  6824., 17696.],
         [17408., 33280., 40096.],...
```
</details>"
60764,Inconsistency-bug in `tf.raw_ops.Acos` between jit mode and normal mode,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

Ubuntu 20.04

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

There is an inconsistency bug between **jit compile mode** and **normal mode** in `tf.raw_ops.Acos` which result in inconsistent computational result (mainly occur while dtype is complex, it seems that there are something wrong in the support of complex number).
But expectedly, the computational result have to be the same.

### Standalone code to reproduce the issue

```shell
import os 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import numpy as np

x = tf.cast(tf.random.uniform([2, 1, 3, 4], dtype=tf.dtypes.float32, maxval=60000), dtype=tf.complex128)

@tf.function(jit_compile=True)
def fuzz_jit():
    y = tf.raw_ops.Acos(
        x = x
    )
    return y

def fuzz_normal():
    y = tf.raw_ops.Acos(
        x = x
    )
    return y

y1 = fuzz_jit()
print('[+] JIT ok')
y2 = fuzz_normal()
print('[+] Normal ok')
np.testing.assert_allclose(y1.numpy(), y2.numpy(), rtol=1e-4, atol=1e-4)
```


### Relevant log output

```shell
% python test.py
[+] JIT ok
[+] Normal ok
Traceback (most recent call last):
  File ""test.py"", line 25, in <module>
    np.testing.assert_allclose(y1.numpy(), y2.numpy(), rtol=1e-4, atol=1e-4)
  File ""/usr/local/lib/python3.8/dist-packages/numpy/testing/_private/utils.py"", line 1530, in assert_allclose
    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
  File ""/usr/local/lib/python3.8/dist-packages/numpy/testing/_private/utils.py"", line 844, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=0.0001, atol=0.0001

Mismatched elements: 24 / 24 (100%)
Max absolute difference: 23.34874158
Max relative difference: 2.00000002
 x: array([[[[4.020810e-07+11.649195j, 3.918355e-07+11.63629j ,
          6.167066e-09 +9.56048j , 2.750275e-10 +8.005427j],
         [3.286194e-07+11.548319j, 8.673153e-08+10.882277j,...
 y: array([[[[0.-11.649196j, 0.-11.63629j , 0. -9.56048j , 0. -8.005427j],
         [0.-11.548319j, 0.-10.882278j, 0.-10.521193j, 0.-10.125081j],
         [0.-10.732039j, 0. -7.92731j , 0.-11.674371j, 0.-11.332818j]]],...
```
</details>"
60763,"acces to Tensor(""IteratorGetNext:1"", shape=(None, 1), dtype=float32)","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

i need a help

### Standalone code to reproduce the issue

```shell
how can i access to the values of Tensor(""IteratorGetNext:1"", shape=(None, 1), dtype=float32).
```


### Relevant log output

_No response_</details>"
60762,[TFLite] Cannot apply XNNPack delegate to simple model with Dense layer,"### 1. System information

- OS Platform and Distribution: macOS ventura 13.2.1
In python, I'm using 2.11, for the c++ side, I used the latest main and followed the instructions to [build tensorflow lite with cmake](https://www.tensorflow.org/lite/guide/build_cmake#create_a_cmake_project_which_uses_tensorflow_lite) from this commit (f8066222ad6).

### 2. Code

I have a tiny dummy model:
```py
    import tensorflow as tf

    img = tf.keras.layers.Input((40, 320, 1), name='img')
    x = tf.keras.layers.Conv2D(32, (3, 3), padding='same')(img)
    x = tf.keras.layers.Dense(24, activation='softmax', name=""output"")(x)
    model = tf.keras.Model(inputs=[img], outputs=x)
```
which I'm converting to tflite like so:
```py
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [
        tf.lite.Optimize.EXPERIMENTAL_SPARSITY,  # sparsity optimization for xnnpack acceleration
    ]
    tflite_model = converter.convert()
```
before saving.
And then on the c++ side I'm trying to apply the xnnpack delegate:
```cpp
    auto m_model = TfLiteModelCreateFromFile(""<PATH_TO_FILE>.tflite"");
    auto m_options = TfLiteInterpreterOptionsCreate();
    TfLiteXNNPackDelegateOptions opt = TfLiteXNNPackDelegateOptionsDefault();
    auto m_xnnpack_delegate = TfLiteXNNPackDelegateCreate(&opt);
    TfLiteInterpreterOptionsAddDelegate(m_options, m_xnnpack_delegate);
    auto m_interpreter = TfLiteInterpreterCreate(m_model, m_options); // This returns nullptr and prints error messages in the console
```

However this fails with `WARNING: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#15 is a dynamic-sized tensor).`, where `tensor#15` is `model/output/Tensordot/Reshape`. For some reason, that I don't quite understand, this part of the dense layer seems to be flagged as a dynamic tensor.

Any idea, what might be the cause here?

A bit of context: I'm actually looking into pruning using the [PruneForLatencyOnXNNPack](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PruneForLatencyOnXNNPack) approach. I've skipped the training with pruning on this dummy model for now, but I have a more complex model, that I also pruned before conversion and it, too, fails on the dense layer.  As far as I can tell, the Dense layer [should be supported](https://github.com/google/XNNPACK), so I'm really confused.
"
60761,The first parameter of cuPointerGetAttribute is wrong,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

mater

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc#L1344
```
  GpuContext* context = nullptr;
  CUresult result =
      cuPointerGetAttribute(&context, CU_POINTER_ATTRIBUTE_CONTEXT, pointer);
```
The type of context is GpuContext，not CUcontext.This will return context not correctly.


From the cuda doc, we can see that when the attribute is CU_POINTER_ATTRIBUTE_CONTEXT， the first parameter must be CUcontext *.
![image](https://github.com/tensorflow/tensorflow/assets/42771665/7259dc69-b1c2-4f38-ad92-e63e466ba5fa)



### Standalone code to reproduce the issue

```shell
I don't know how to modify it.
In most cases, it will not go to the code of this branch, so the bug has not been exposed.
```


### Relevant log output

_No response_</details>"
60760,Configure script automatically selects CUDA/cuDNN path instead of waiting for user input,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

TF 2.10

### Custom Code

No

### OS Platform and Distribution

Fedora 37

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

12.3.1

### CUDA/cuDNN version

11.8,12.1/8.0

### GPU model and memory

GTX 1660 Ti, 6 GB

### Current Behaviour?

I am having multiple CUDA versions, and I am trying to build Tensorflow from source with CUDA support.

Now the problem lays when I try to configure the build system using `./configure`. It will asks for relevant information for the build system. This includes:

1. Python path
2. Python packages path
3. Whether to support mROC
4. Whether to support CUDA
5. Whether to support TensorRT

Now, when I select CUDA support. the script seems to automatically selects my CUDA/cuDNN versions, and does not give me the possibility to select it manually, which is contradictory to what the documentation suggests at  [https://www.tensorflow.org/install/source#gpu_support](url):  _""If your system has multiple versions of CUDA or cuDNN installed, explicitly set the version instead of relying on the default""_

Now, I was able to trace the issue exactly to the `configure.py` file. 
In fact, I strongly suspects that there is a logical error on the section that parses the user input (Line 1244 on branch r2.11):
```python
  environ_save = dict(environ_cp)
  for _ in range(_DEFAULT_PROMPT_ASK_ATTEMPTS):
    if validate_cuda_config(environ_cp):
      cuda_env_names = [
          'TF_CUDA_VERSION',
          'TF_CUBLAS_VERSION',
          'TF_CUDNN_VERSION',
          'TF_TENSORRT_VERSION',
          'TF_NCCL_VERSION',
          'TF_CUDA_PATHS',
          # Items below are for backwards compatibility when not using
          # TF_CUDA_PATHS.
          'CUDA_TOOLKIT_PATH',
          'CUDNN_INSTALL_PATH',
          'NCCL_INSTALL_PATH',
          'NCCL_HDR_PATH',
          'TENSORRT_INSTALL_PATH'
      ]
      # Note: set_action_env_var above already writes to bazelrc.
      for name in cuda_env_names:
        if name in environ_cp:
          write_action_env_to_bazelrc(name, environ_cp[name])
      break

    # Restore settings changed below if CUDA config could not be validated.
    environ_cp = dict(environ_save)

    set_tf_cuda_version(environ_cp)
    set_tf_cudnn_version(environ_cp)
    if is_windows():
      set_tf_tensorrt_version(environ_cp)
    if is_linux():
      set_tf_tensorrt_version(environ_cp)
      set_tf_nccl_version(environ_cp)

    set_tf_cuda_paths(environ_cp)
```

Now, from my understanding, the script will validate the given environment, and then if that fails will ask for user input.
With that, on the first iteration of the loop, the validation will not contain the required environment variables.

I was able to solve the issue by swapping the order as follow:
```python
    environ_save = dict(environ_cp)
    for _ in range(_DEFAULT_PROMPT_ASK_ATTEMPTS):
      # Restore settings changed below if CUDA config could not be validated.
      environ_cp = dict(environ_save)

      set_tf_cuda_version(environ_cp)
      set_tf_cudnn_version(environ_cp)
      if is_windows():
        set_tf_tensorrt_version(environ_cp)
      if is_linux():
        set_tf_tensorrt_version(environ_cp)
        set_tf_nccl_version(environ_cp)

      set_tf_cuda_paths(environ_cp)
      if validate_cuda_config(environ_cp):
        cuda_env_names = [
            'TF_CUDA_VERSION',
            'TF_CUBLAS_VERSION',
            'TF_CUDNN_VERSION',
            'TF_TENSORRT_VERSION',
            'TF_NCCL_VERSION',
            'TF_CUDA_PATHS',
            # Items below are for backwards compatibility when not using
            # TF_CUDA_PATHS.
            'CUDA_TOOLKIT_PATH',
            'CUDNN_INSTALL_PATH',
            'NCCL_INSTALL_PATH',
            'NCCL_HDR_PATH',
            'TENSORRT_INSTALL_PATH'
        ]
        # Note: set_action_env_var above already writes to bazelrc.
        for name in cuda_env_names:
          if name in environ_cp:
            write_action_env_to_bazelrc(name, environ_cp[name])
        break
```


### Standalone code to reproduce the issue

```shell
Assumption: Multiple CUDA versions on /usr/local

Command:
./configure

Input Example:
1. [Default Setting]
2. [Default Setting]
3. N
4. y
5. N
```


### Relevant log output

_No response_</details>"
60759,Check fail can be triggered in `tf.raw_ops.Empty` under jit compile mode.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

Ubuntu 20.04

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Check fail can be triggered in `tf.raw_ops.Empty` under jit compile mode. While in normal mode, it won't be triggered but through an InvalidArgumentError.

### Standalone code to reproduce the issue

```shell
import os 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import numpy as np

shape=tf.random.uniform([4], dtype=tf.dtypes.int32, minval=0, maxval=1000000)
dtype=tf.dtypes.int32
init=True

@tf.function(jit_compile=True)
def fuzz_jit():
    y = tf.raw_ops.Empty(shape=shape, dtype=dtype, init=init)
    return y

def fuzz_normal():
    y = tf.raw_ops.Empty(shape=shape, dtype=dtype, init=init)
    return y

y1 = fuzz_jit() # trigger check fail
print('[+] JIT ok')
y2 = fuzz_normal()
print('[+] Normal ok')
```


### Relevant log output

```shell
% python test.py
2023-06-02 11:44:08.858309: F tensorflow/compiler/xla/shape_util.cc:288] Check failed: FillNewShape(element_type, dimensions, &shape) 
zsh: abort (core dumped)  python test.py
```
</details>"
60758,Check fail can be triggered in `tf.raw_ops.EmptyTensorList` due to overflow under jit compile mode.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

Ubuntu 20.04

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Check fail can be triggered in `tf.raw_ops.EmptyTensorList` under jit compile mode. While in normal mode, it won't be triggered but through an InvalidArgumentError.

### Standalone code to reproduce the issue

```shell
import os 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import numpy as np

element_shape=tf.random.uniform([3], dtype=tf.dtypes.int64, minval=-100000000, maxval=1000000000)
max_num_elements=tf.random.uniform([], dtype=tf.dtypes.int32, minval=-100000, maxval=1000000)
element_dtype=tf.dtypes.int32

@tf.function(jit_compile=True)
def fuzz_jit():
    y = tf.raw_ops.EmptyTensorList(element_shape=element_shape, max_num_elements=max_num_elements, element_dtype=element_dtype)
    return y

def fuzz_normal():
    y = tf.raw_ops.EmptyTensorList(element_shape=element_shape, max_num_elements=max_num_elements, element_dtype=element_dtype)
    return y

y1 = fuzz_jit() # trigger the check fail under jit compile mode.
print('[+] JIT ok')
y2 = fuzz_normal() # if you run y2 first, it will through error rather than check fail.
print('[+] Normal ok')
```


### Relevant log output

```shell
% python test.py
2023-06-02 11:41:06.810728: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 246417692688323817 with 562326056, result: -1
zsh: abort (core dumped)  python test.py
```
</details>"
60757,Check fail can be triggered in `tf.raw_ops.Fill` under jit compile mode.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

Ubuntu 20.04

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Check fail can be triggered in `tf.raw_ops.Fill` under jit compile mode. While in normal mode, it won't be triggered but through an InvalidArgumentError.

### Standalone code to reproduce the issue

```shell
import os 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import numpy as np

dims=tf.random.uniform([4], dtype=tf.dtypes.int32, minval=-10, maxval=1000000)
value=tf.random.uniform([], dtype=tf.dtypes.int32, minval=-100000, maxval=1000000)

@tf.function(jit_compile=True)
def fuzz_jit():
    y = tf.raw_ops.Fill(dims=dims, value=value)
    return y

def fuzz_normal():
    y = tf.raw_ops.Fill(dims=dims, value=value)
    return y

y1 = fuzz_jit() # trigger the check fail under jit compile mode.
print('[+] JIT ok')
y2 = fuzz_normal() # if you run y2 first, it will through error rather than check fail.
print('[+] Normal ok')
```


### Relevant log output

```shell
% python test.py
2023-06-02 11:21:51.062617: F tensorflow/compiler/xla/shape_util.cc:288] Check failed: FillNewShape(element_type, dimensions, &shape) 
zsh: abort (core dumped)  python test.py
```
</details>"
60756,Check fail can be triggered in `tf.raw_ops.GatherV2` under jit compile mode.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

Ubuntu 20.04

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Check fail can be triggered in `tf.raw_ops.GatherV2` under jit compile mode. While in normal mode, it won't be triggered but through an `InvalidArgumentError`.

### Standalone code to reproduce the issue

```shell
import os 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import numpy as np

params=tf.random.uniform([4], dtype=tf.dtypes.float32, maxval=100000000)
indices=tf.random.uniform([4, 0], dtype=tf.dtypes.int32, minval=-10000, maxval=60000)
axis=tf.random.uniform([], dtype=tf.dtypes.int64, minval=-100000000, maxval=1000000000)
batch_dims=11282

@tf.function(jit_compile=True)
def fuzz_jit():
    y = tf.raw_ops.GatherV2(params=params, indices=indices, axis=axis, batch_dims=batch_dims)
    return y

def fuzz_normal():
    y = tf.raw_ops.GatherV2(params=params, indices=indices, axis=axis, batch_dims=batch_dims)
    return y

y1 = fuzz_jit()  # trigger the check fail under jit compile mode.
print('[+] JIT ok')
y2 = fuzz_normal()  # if you run y2 first, it will through error rather than check fail.
print('[+] Normal ok')
```


### Relevant log output

```shell
% python test.py
2023-06-02 11:10:06.042625: F tensorflow/core/framework/shape_inference.cc:705] Check failed: rank >= 0 (0 vs. -11280)rank must not be negative
zsh: abort (core dumped)  python test.py
```
</details>"
60755,Check failed in `tf.raw_ops.ConjugateTranspose`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

Ubuntu 20.04

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A check fail can be triggered in `tf.raw_ops.ConjugateTranspose`. While under jit compile, it won't occur. There is an inconsistency problem between jit and normal mode.

### Standalone code to reproduce the issue

```shell
import os 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import numpy as np

x=tf.random.uniform([], dtype=tf.dtypes.float32, maxval=100000000)
perm=tf.random.uniform([0], dtype=tf.dtypes.int64, minval=-100000000, maxval=1000000000)

@tf.function(jit_compile=True)
def fuzz_jit():
    y = tf.raw_ops.ConjugateTranspose(x=x, perm=perm)
    return y

def fuzz_normal():
    y = tf.raw_ops.ConjugateTranspose(x=x, perm=perm)
    return y

y1 = fuzz_jit()
print('[+] JIT ok') # Under jit compile, it passed the testcase normally.
y2 = fuzz_normal()  # Check fail.
print('[+] Normal ok')  # Won't be printed.
```


### Relevant log output

```shell
% python test.py 
[+] JIT ok
2023-06-02 11:01:06.844753: F ./tensorflow/core/util/mkl_util.h:1273] Check failed: dims_tf_order.size() > 0 (0 vs. 0)
zsh: abort (core dumped)  python test.py
```
</details>"
60754,error: undefined reference to 'tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(absl::Span<long const>)',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Debian GNU/Linux 10

### Mobile device

_No response_

### Python version

3.7

### Bazel version

5.3.0

### GCC/Compiler version

gcc 9

### CUDA/cuDNN version

11

### GPU model and memory

Tesla T4

### Current Behaviour?

When I run a build with `--config=opt`, I get the following error:

```
error: undefined reference to 'tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(absl::Span<long const>)'
```

However, compiling normally is fine.

Checking llvm-nm, I see these symbols in optimized build:

```
U _ZN10tensorflow15TensorShapeBaseINS_11TensorShapeEEC1EN4absl4SpanIKlEE
0000000000000000 W _ZN10tensorflow6Tensor16unaligned_shapedIfLm1EEENS_6TTypesIT_XT0_ElE15UnalignedTensorEN4absl4SpanIKlEE
0000000000000000 W _ZNK10tensorflow6Tensor34FillDimsAndValidateCompatibleShapeILm1EEEvN4absl4SpanIKlEEPSt5arrayIlXT_EE
```

but these in normal:

```
0000000000000000 W _ZN10tensorflow6Tensor16unaligned_shapedIfLm1EEENS_6TTypesIT_XT0_ElE15UnalignedTensorEN4absl4SpanIKlEE
0000000000000000 W _ZN4absl4SpanIKlEC1EPS1_m
0000000000000000 W _ZN4absl4SpanIKlEC1IS1_S1_EESt16initializer_listIlE
0000000000000000 W _ZN4absl4SpanIKlEC2EPS1_m
0000000000000000 W _ZN4absl4SpanIKlEC2IS1_S1_EESt16initializer_listIlE
0000000000000000 n _ZN4absl4SpanIKlEC5EPS1_m
0000000000000000 n _ZN4absl4SpanIKlEC5IS1_S1_EESt16initializer_listIlE
0000000000000000 W _ZNK10tensorflow6Tensor34FillDimsAndValidateCompatibleShapeILm1EEEvN4absl4SpanIKlEEPSt5arrayIlXT_EE
0000000000000000 W _ZNK4absl4SpanIKlE4dataEv
0000000000000000 W _ZNK4absl4SpanIKlE4sizeEv
0000000000000000 W _ZNK4absl4SpanIKlEixEm
0000000000000000 W _ZZNK4absl4SpanIKlEixEmENKUlvE_clEv
```

I am building tensorflow from source and copying `libtensorflow_cc.so`, `libtensorflow_framework.so`, and include files into my project. However, the linking step fails as described above.

### Standalone code to reproduce the issue

```shell
Build tensorflow from source, copy .so and include files into your own project, and try to use tensorflow headers in project.
```


### Relevant log output

_No response_</details>"
60751,Tensorflow-cpu-aws prevents building ARM/multi-arch containers from a x86 machine,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Linux Debian 6.1.20-2rodete1

### Mobile device

_No response_

### Python version

Python 3.8.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

On a x86 machine, I tried to build Beam Python multi-arch containers whose base image requirement include [tensorflow2.12](https://github.com/apache/beam/blob/4f07cda050cd144a95a686c4138307e9920f313d/sdks/python/container/py310/base_image_requirements.txt#LL143C24-L143C24). The x86 components were built successfully but encountered the following error while building the ARM components:

`#39 [linux/arm64 beam 11/16] RUN pip check || (echo ""Container does not include required Beam dependencies or has conflicting dependencies. If Beam dependencies have changed, you need to regenerate base_image_requirements.txt files. See: https://s.apache.org/beam-python-requirements-generate"" && exit 1) #39 20.81 tensorflow 2.12.0 requires tensorflow-cpu-aws, which is not installed.`

I also tried to install tensorflow-cpu-aws manually by running pip install tensorflow-cpu-aws on the x86 machine and got the following error: `ERROR: Could not find a version that satisfies the requirement tensorflow-cpu-aws (from versions: none) ERROR: No matching distribution found for tensorflow-cpu-aws`

An ARM container image required tensorflow can't be built from a x86 machine because it will try to install tensorflow-cpu-aws, which can't be installed from a x86 machine.

I wonder if it is there anything we can do to resolve this? Otherwise for all ARM containers which are built from a x86 machine, their base image requirements can't contain tensorflow.

Thanks!

### Standalone code to reproduce the issue

```shell
On terminal:
1. Clone the Beam repository: git clone https://github.com/apache/beam.git
2. Build the multi-arch image by running: ./gradlew -Pcontainer-architecture-list=arm64,amd64 :sdks:python:container:py310:docker
```


### Relevant log output

_No response_</details>"
60750,pip installation LD_LIBRARY_PATH order,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.12

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In the official [pip install documentation](https://www.tensorflow.org/install/pip) $LD_LIBRARY_PATH is updated via conda activation. 

Following instructions on clean machine for library linking works perfectly

However if the machine has a native CUDA library setting, TF will load the system library ahead of virtual environment one.

Suggest to load conda path ahead of system one:

```bash
echo 'CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
echo 'export LD_LIBRARY_PATH=$CONDA_PREFIX/lib/:$CUDNN_PATH/lib:$LD_LIBRARY_PATH' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
```


### Standalone code to reproduce the issue

```shell
echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
```


### Relevant log output

```shell
2023-06-01 15:58:54.130390: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.1.1 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2023-06-01 15:58:54.131265: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_ops.cc:1068 : UNIMPLEMENTED: DNN library is not found.
```
</details>"
60747,tensorflow.map hangs randomly when using for num_parallel_calls a value > 1,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: wsl2 on windows
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.12.0
-   **Python version**: 3.8.10
-   **CUDA/cuDNN version**: 12.0
-   **GPU model and memory**: RTX A5000 24GB


### Describe the problem

I am using tensorflow in a docker container on wsl2 for training neural networks but unfortunately everything freezes during the image import if done with the tf.map function with the parameter ""num_parallel_calls"" set to a value >1. Below is a minimal example of the code which causes the console to freeze at a random step and the CPU-usage to drop to a minimal level while the RAM is still occupied. After everything is frozen, the docker container is unresponsive and has to be restarted by stopping the process. This might even take half a day -> a few hundred thousand iterations but it allways happens at some point.


### Source code / logs

`import tensorflow as tf

def parse_function(filename):
    return tf.io.read_file(filename)

filenames = []
for i in range (100000):
    filenames.append(""/D/test.png"")

dataset_train = tf.data.Dataset.from_tensor_slices((filenames))
dataset_train = dataset_train.map(parse_function, num_parallel_calls =  2)
dataset_train = dataset_train.batch(50)

for epoch in range(1000):
    for step, (x_batch_train) in enumerate(dataset_train):
        print(""epoch:"", epoch, ""start train step:"",step)`


This issue is similar to others like #32454 but i still can't find a fix. 
Bypassing the map-function by loading everything at once works -> definitly during mapping if parallel calls are allowed
"
60746,Problem importint load_img from tensorflow.keras.utils,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Using load_img to import an image into my program's function

### Standalone code to reproduce the issue

```shell
ImportError: cannot import name 'load_img' from 'keras.preprocessing.image'
```


### Relevant log output

_No response_</details>"
60745,bazel build -c opt --config=macos //tensorflow/lite/c:tensorflowlite_c --verbose_failure   ld: unknown option: --no-undefined,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

6.1

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

(base) ➜  tensorflow git:(master) ✗ bazel build -c opt --config=macos //tensorflow/lite/c:tensorflowlite_c --verbose_failures
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=198
INFO: Reading rc options for 'build' from /Users/yongle/project/C/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/yongle/project/C/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /Users/yongle/project/C/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Users/yongle/.pyenv/versions/3.11.3/bin/python3 --action_env PYTHON_LIB_PATH=/Users/yongle/.pyenv/versions/3.11.3/lib/python3.11/site-packages --python_path=/Users/yongle/.pyenv/versions/3.11.3/bin/python3 --action_env ANDROID_NDK_HOME=/Users/yongle/Library/Android/sdk/ndk/21.4.7075529 --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=33.0.1 --action_env ANDROID_SDK_API_LEVEL=33 --action_env ANDROID_SDK_HOME=/Users/yongle/Library/Android/sdk
INFO: Reading rc options for 'build' from /Users/yongle/project/C/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug
INFO: Found applicable config definition build:short_logs in file /Users/yongle/project/C/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/yongle/project/C/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:macos in file /Users/yongle/project/C/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --features=archive_param_file --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17
INFO: Found applicable config definition build:macos in file /Users/yongle/project/C/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --features=archive_param_file --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17
INFO: Build options --action_env and --python_path have changed, discarding analysis cache.
INFO: Analyzed target //tensorflow/lite/c:tensorflowlite_c (107 packages loaded, 2952 targets configured).
INFO: Found 1 target...
ERROR: /Users/yongle/project/C/tensorflow/tensorflow/lite/c/BUILD:28:24: Linking tensorflow/lite/c/libtensorflowlite_c.dylib failed: (Exit 1): cc_wrapper.sh failed: error executing command (from target //tensorflow/lite/c:libtensorflowlite_c.dylib)
  (cd /private/var/tmp/_bazel_yongle/fb676000c07d36afe13b22c7b593df69/execroot/org_tensorflow && \
  exec env - \
    ANDROID_BUILD_TOOLS_VERSION=33.0.1 \
    ANDROID_NDK_API_LEVEL=21 \
    ANDROID_NDK_HOME=/Users/yongle/Library/Android/sdk/ndk/21.4.7075529 \
    ANDROID_SDK_API_LEVEL=33 \
    ANDROID_SDK_HOME=/Users/yongle/Library/Android/sdk \
    APPLE_SDK_PLATFORM=MacOSX \
    APPLE_SDK_VERSION_OVERRIDE=13.3 \
    PATH=/Users/yongle/Library/Caches/bazelisk/downloads/bazelbuild/bazel-6.1.0-darwin-x86_64/bin:/Users/yongle/miniconda3/bin:/Users/yongle/miniconda3/condabin:/Users/yongle/Library/pnpm:/usr/local/opt/node@19/bin:/Users/yongle/.pyenv/shims:/Users/yongle/.local/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/go/bin:/usr/local/share/dotnet:~/.dotnet/tools:/Library/Apple/usr/bin://Library/Developer/Panda3D/bin:/Library/Frameworks/Mono.framework/Versions/Current/Commands:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/yongle/.cargo/bin:/usr/local/bin:91139ANDROID_NDK_ROOT/toolchains/aarch64-linux-android-4.9/prebuilt/darwin-x86_64/bin:/Users/yongle/Library/Android/sdk/emulator:/Users/yongle/Library/Android/sdk/tools:/Users/yongle/Library/Android/sdk/platform-tools:/Users/yongle/Library/Android/sdk/build-tools/33.0.1:/usr/local/bin:/Users/yongle/.pub-cache/bin:/opt/fvm/default/bin:/opt/fvm/default/bin/cache/dart-sdk/bin:/Users/yongle/Documents/apache-maven-3.8.1/bin \
    PYTHON_BIN_PATH=/Users/yongle/.pyenv/versions/3.11.3/bin/python3 \
    PYTHON_LIB_PATH=/Users/yongle/.pyenv/versions/3.11.3/lib/python3.11/site-packages \
    TF2_BEHAVIOR=1 \
    XCODE_VERSION_OVERRIDE=14.3.0.14E222b \
    ZERO_AR_DATE=1 \
  external/local_config_cc/cc_wrapper.sh @bazel-out/darwin-opt/bin/tensorflow/lite/c/libtensorflowlite_c.dylib-2.params)
# Configuration: 0147ddac8ddd69cb32a57bdf7993d2379760736293c350037f6e0ed5fdf8b453
# Execution platform: @local_execution_config_platform//:platform
ld: unknown option: --no-undefined
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Error in child process '/usr/bin/xcrun'. 1
Target //tensorflow/lite/c:tensorflowlite_c failed to build
INFO: Elapsed time: 124.453s, Critical Path: 47.17s
INFO: 407 processes: 2 internal, 405 local.
FAILED: Build did NOT complete successfully

### Standalone code to reproduce the issue

```shell
bazel build -c opt --config=macos //tensorflow/lite/c:tensorflowlite_c --verbose_failures
```


### Relevant log output

_No response_</details>"
60743,Tensorflow Lite Model Maker model works on Python API but not on device (IOS/Android),"Hello, I hope all is well.
I have recently created a MobileBERT model using the Python API of the tflite_model_maker library following the steps described in this [page](https://www.tensorflow.org/lite/models/modify/model_maker/text_classification). You may find it attached in the following [link](https://drive.google.com/file/d/1jeKm7EesBZqi_lgPrCSq_HwPapX54OlL/view?usp=sharing) (let me know if I can share it in any other way).

The good news is that I have been able to run inference with the Python API by following the steps described in this [article](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_nl_classifier#step_2_using_the_model). However, my colleague has been encountering issues when trying to run the same model using the Swift and Android APIs after following this [page](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_nl_classifier). 

After some time, he was effortlessly able to run inference on a different model made available through one of the sample apps found [here](https://storage.googleapis.com/download.tensorflow.org/models/tflite/task_library/text_classification/android/mobilebert.tflite).
It is our belief that the issue may be from the MetaData but after inspecting both models' metadata (attached in the zip file), they seem to be exactly the same: [MetaDatas.zip](https://github.com/tensorflow/tensorflow/files/11615911/MetaDatas.zip).

Our issue is that the inference process is killed without clear reason after the model is loaded. As displayed in the image below:

![MicrosoftTeams-image](https://github.com/tensorflow/tensorflow/assets/59343296/cd826cae-0544-4a2c-8f7b-8fe967466bf1)
With the following error:
```
tensor->bytes == bytes
FATAL
```

Please advise and thank you for your time"
60742,TF compiler version causes error with pkg_resources.parse_version (invalid format),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0-dev20230531

### Custom Code

No

### OS Platform and Distribution

Linux 3.10.0-1127.el7.x86_64

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

N/A

### GCC/Compiler version

N/A

### CUDA/cuDNN version

CUDA 10.2.89

### GPU model and memory

N/A

### Current Behaviour?

**Current behavior:** The following code (in a fresh conda env with tf-nightly installed via pip) produces a `pkg_resources.extern.packaging.version.InvalidVersion` error from `parse_version`:
```python
import tensorflow
from pkg_resources import parse_version
parse_version(tensorflow.__compiler_version__)
```

**Desired Behavior:** A string that matches the format produced by cmake's `$(CMAKE_CXX_COMPILER_VERSION}`, i.e. is readable by `pkg_resources.parse_version`.

**Context:** Presently I am using this to warn against compiler mismatches at build time of a package that builds with TF libs in cmake (checks against `${CMAKE_CXX_COMPILER_VERSION}`). The build process doesn't strike me as relevant here but I will provide the build files if requested.



### Standalone code to reproduce the issue

```shell
import tensorflow
from pkg_resources import parse_version
parse_version(tensorflow.__compiler_version__)
```


### Relevant log output

```shell
>>> parse_version(tensorflow.__compiler_version__)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/rainierbarrett/.conda/envs/tf-test/lib/python3.11/site-packages/pkg_resources/_vendor/packaging/version.py"", line 197, in __init__
    raise InvalidVersion(f""Invalid version: '{version}'"")
pkg_resources.extern.packaging.version.InvalidVersion: Invalid version: 'Ubuntu Clang 16.0.4 (++20230506063001+3c1576cc0c54-1~exp1~20230506063103.85)'
```
</details>"
60741,multiprocessing stuck after usage of tensorflow functionality,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

v2.11.0-rc2-17-gd5b57ca93e5 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In a project where I wanted to implement multiprocessing in combination with a Tensorflow function, the processes kept getting stuck. After some debugging I was able to create a minimal example as seen below or as provided as .txt doc.
[minimal_example_MPlock.txt](https://github.com/tensorflow/tensorflow/files/11616923/minimal_example_MPlock.txt)

In words; when I do some random transpose operation in the process there is no problem at all. However afterwards if I use any functionality from tensorflow and repeat the code that used to work, all of a sudden it gets stuck on the last matrix inverse (which is quite a big one, but shouldn't be any problem).

What you can see, and is probably part of the issue, is that each time the function is called there's a bunch of tensorflow warnings. So far in all of my code I could always just ignore them, but to be sure I added them here in the logs aswell.

Thanks in advance!


### Standalone code to reproduce the issue

```shell
from multiprocessing import Process
import numpy as np
import tensorflow as tf
import time

def test_inverse():
    Pxk = tf.eye(2)
    Pwk = tf.eye(259)
    print(""here1"")
    Pxk = tf.transpose(Pxk)
    print(""here2"")
    Pwk = np.transpose(Pwk)
    print(""here3"")
    Pwk = tf.transpose(Pwk)
    print(""here4"")

processes = []

for _ in range(3):
    p = Process(target=test_inverse, args=[], kwargs={})
    time.sleep(1)
    p.start()
    processes.append(p)

for process in processes:
    process.join()
    
### works perfectly fine
import time
time.sleep(5)
print(""using ANY tensorflow function"")
a = tf.math.add(2,5)

processes = []
for _ in range(3):
    p = Process(target=test_inverse, args=[], kwargs={})
    p.start()
    processes.append(p)
    
for process in processes:
    process.join()
```


### Relevant log output

```shell
2023-05-31 19:20:48.218419: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-31 19:20:48.286362: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-31 19:20:48.620505: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-05-31 19:20:48.620539: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-05-31 19:20:48.620543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-05-31 19:20:50.183101: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2023-05-31 19:20:50.183138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cedric-Z590-UD-AC): /proc/driver/nvidia/version does not exist
2023-05-31 19:20:50.183724: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
here1
here2
here3
here4
2023-05-31 19:20:51.221681: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2023-05-31 19:20:51.221727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cedric-Z590-UD-AC): /proc/driver/nvidia/version does not exist
2023-05-31 19:20:51.222470: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
here1
here2
here3
here4
2023-05-31 19:20:52.223862: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2023-05-31 19:20:52.223908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cedric-Z590-UD-AC): /proc/driver/nvidia/version does not exist
2023-05-31 19:20:52.224710: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
here1
here2
here3
here4
using ANY tensorflow function
2023-05-31 19:20:57.355866: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2023-05-31 19:20:57.355928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cedric-Z590-UD-AC): /proc/driver/nvidia/version does not exist
2023-05-31 19:20:57.356675: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
here1
here1
here2
here3
here2
here3
here1
here2
here3
```
</details>"
60739,Support CUDA Compute 9.0 for NVIDIA H100,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

v1.12.1-94762-gf8066222ad6 2.14.0-dev20230531

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

With NVIDIA H100 general availability on the horizon, it would be nice to start supporting CUDA compute capability 9.0. Are there plans for this in an upcoming release or could it be added to nightly builds?

In testing my code on an H100, I currently see 

```
2023-05-31 14:45:08.138842: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2052] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 9.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.
```

and after jit-compiling, my code that works fine on NVIDIA A100 GPUs eventually fails during allocation. I'd like to rule out that it's not a problem with jit-compiling, so having CUDA compute capability 9.0 compiled into tf-nightly would be one way to rule that out.

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
60738,AArch64 tflite_runtime manylinux2014 prebuilt wheels distributed on PYPI are not manylinux2014 compliant,"### Describe the problem

The manylinux2014 AArch64 tflite_runtime wheels distributed on PYPI are not manylinux2014 compliant.

manylinux2014 is specified by https://peps.python.org/pep-0599/ which is superseded by https://peps.python.org/pep-0600/

pep599 States that:

```
3 If the wheel contains binary executables or shared objects linked against any allowed libraries that also export versioned symbols, they may only depend on the following maximum versions:
GLIBC_2.17
CXXABI_1.3.7, CXXABI_TM_1 is also allowed
GLIBCXX_3.4.19
GCC_4.8.0
As an example, manylinux2014 wheels may include binary artifacts that require glibc symbols at version GLIBC_2.12, because this an earlier version than the maximum of GLIBC_2.17.

```
pep600 states that:

`manylinux2014_aarch64 is now an alias for manylinux_2_17_aarch64`

The auditwheel tool provided by manylinux, run on https://files.pythonhosted.org/packages/f7/52/db3a91277e4c171b65665731d622e7ed9a0ef7a601782403f90d43a080d8/tflite_runtime-2.12.0-cp38-cp38-manylinux2014_aarch64.whl reports the following:

```
tflite_runtime-2.12.0-cp38-cp38-manylinux2014_aarch64.whl is
consistent with the following platform tag: ""manylinux_2_34_aarch64"".

The wheel references external versioned symbols in these
system-provided shared libraries: libgcc_s.so.1 with versions
{'GCC_3.0'}, libm.so.6 with versions {'GLIBC_2.27', 'GLIBC_2.17',
'GLIBC_2.29'}, libc.so.6 with versions {'GLIBC_2.32', 'GLIBC_2.17',
'GLIBC_2.34', 'GLIBC_2.33'}, libstdc++.so.6 with versions
{'CXXABI_1.3.3', 'GLIBCXX_3.4.29', 'CXXABI_1.3', 'GLIBCXX_3.4.11',
'GLIBCXX_3.4.21', 'CXXABI_1.3.5', 'GLIBCXX_3.4', 'CXXABI_1.3.2',
'GLIBCXX_3.4.20', 'GLIBCXX_3.4.14', 'CXXABI_1.3.9', 'CXXABI_1.3.11',
'GLIBCXX_3.4.9', 'CXXABI_1.3.13', 'GLIBCXX_3.4.19', 'GLIBCXX_3.4.22',
'GLIBCXX_3.4.18'}

This constrains the platform tag to ""manylinux_2_34_aarch64"". In order
to achieve a more compatible tag, you would need to recompile a new
wheel from source on a system with earlier versions of these
libraries, such as a recent manylinux image.
```

The distributed wheel contains references to GLIBC_2.33 and GLIBC_2.34 which conflicts with clause 3 of pep0599. 

This issue came to light on the tflite_runtime-2.12.0-cp38-cp38-manylinux2014_aarch64.whl pre-built wheels hosted on pypi, but it looks like at least the following wheels have similar issues:

https://files.pythonhosted.org/packages/3f/1d/253fd16d01ec245f54f57d00674f836347931590af6b6e8c5ab76c8d4478/tflite_runtime-2.12.0-cp39-cp39-manylinux2014_aarch64.whl https://files.pythonhosted.org/packages/f7/52/db3a91277e4c171b65665731d622e7ed9a0ef7a601782403f90d43a080d8/tflite_runtime-2.12.0-cp38-cp38-manylinux2014_aarch64.whl
https://files.pythonhosted.org/packages/bf/75/c49f676ad2de36fa174b74d52bf6b2a199a54168ee550bbf85053578eb5c/tflite_runtime-2.11.0-cp39-cp39-manylinux2014_aarch64.whl
https://files.pythonhosted.org/packages/6e/c0/d131ffe53990fe10c158b856bc76fd71e294c3da5d13a58802e9a7d4642b/tflite_runtime-2.11.0-cp38-cp38-manylinux2014_aarch64.whl
https://files.pythonhosted.org/packages/d3/04/ad399c2cf7b0f301b6a44704fa1e6b3f1f64e16b0cbbd01132e53dee1b95/tflite_runtime-2.11.0-cp37-cp37m-manylinux2014_aarch64.whl

### Source code / logs

The above analysis can be recreated by:
```
virtualenv -p python3.8 v
./v/bin/pip install auditwheel
wget https://files.pythonhosted.org/packages/f7/52/db3a91277e4c171b65665731d622e7ed9a0ef7a601782403f90d43a080d8/tflite_runtime-2.12.0-cp38-cp38-manylinux2014_aarch64.whl
./v/bin/auditwheel show tflite_runtime-2.12.0-cp38-cp38-manylinux2014_aarch64.whl
```

"
60737,Failed to launch ptxas error when using nvidia/cuda runtime Docker image,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0-dev20230531

### Custom Code

Yes

### OS Platform and Distribution

20.04

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8.0 / cuDNN 8.7

### GPU model and memory

_No response_

### Current Behaviour?

I get the following warning when I run prediction using a 3D model (full log provided below):
```
2023-05-31 11:06:53.982167: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] INTERNAL: Failed to launch ptxas
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
```
I'm using a model that takes a 3D input and the error occurs when I have a `Conv3DTranspose` layer (which my model implements).

I'm using the official [`nvidia/cuda`](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/cuda) image with runtime libraries, specifically the `nvcr.io/nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.0`.

I don't get the warning when using the `devel` image, however, I want to stick with `runtime`, as it's much smaller and I don't need any of the compiling capabilities or dev tools.

Does this warning have an impact on model inference and how can I fix it?

Minimal Dockerfile and Python script to reproduce issue are provided below.

### Standalone code to reproduce the issue

Dockerfile:
```shell

FROM nvcr.io/nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04

ENV PYENV_ROOT=/root/.pyenv \
    PATH=""/root/.local/bin:/opt/venv/bin:/root/.pyenv/shims/:root/.pyenv/bin:${PATH}"" \
    DEBIAN_FRONTEND=noninteractive

ARG PYTHON_VERSION=3.9

RUN apt update -y && apt upgrade -y && \
    apt-get install --no-install-recommends -y wget build-essential curl git && \
    # install python via pyenv
    apt-get install -y --no-install-recommends libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev libffi-dev zlib1g-dev liblzma-dev && \
    curl https://pyenv.run | bash && \
    pyenv update && \
    pyenv install ${PYTHON_VERSION} && \
    pyenv global ${PYTHON_VERSION} && \
    pyenv rehash && \
    # install tensorflow
    python${PYTHON_MAJOR_VERSION} -m pip install tf-nightly
```

`test.py`:
```python
import numpy as np
import tensorflow as tf
from tensorflow.keras import Input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv3DTranspose

def build_model(input_shape):
    inputs = Input(shape=input_shape)
    x = Conv3DTranspose(filters=1, kernel_size=(1, 1, 1), strides=(1, 1, 1))(inputs)
    return Model(inputs=inputs, outputs=x)

print(tf.version.GIT_VERSION, tf.version.VERSION)

input_shape = (10, 10, 10, 1)
model = build_model(input_shape)

# input of shape (5, 10, 10, 10, 1)
test_input = np.ones(shape=(5, *input_shape), dtype=np.float32)

output = model.predict(test_input, verbose=1, batch_size=1)
```

Steps to reproduce:
```
docker build -t tf-issue -f /path/to/Dockerfile .
docker run --gpus all -v /path/to/test.py:/test.py tf-issue python /test.py
```


### Relevant log output

```shell
==========
== CUDA ==
==========

CUDA Version 11.8.0

Container image Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

2023-05-31 11:06:49.787277: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:7704] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-05-31 11:06:49.787340: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-05-31 11:06:49.787352: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1520] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-05-31 11:06:49.794051: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-31 11:06:50.626364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-31 11:06:51.483784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-31 11:06:51.504146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-31 11:06:51.504472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-31 11:06:51.505671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-31 11:06:51.505938: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-31 11:06:51.506189: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-31 11:06:52.004673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-31 11:06:52.004929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-31 11:06:52.005143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-31 11:06:52.005338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20638 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-05-31 11:06:53.285968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:434] Loaded cuDNN version 8700
2023-05-31 11:06:53.981061: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2023-05-31 11:06:53.981512: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2023-05-31 11:06:53.981543: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version
2023-05-31 11:06:53.982117: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2023-05-31 11:06:53.982167: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] INTERNAL: Failed to launch ptxas
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
v1.12.1-94762-gf8066222ad6 2.14.0-dev20230531
5/5 [==============================] - 2s 1ms/step
```
</details>"
60736,JVP incorrect in forward mode for `tf.math.sign` and `tf.experimental.numpy.sign`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf-nightly

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When I was trying to calculate jvp of `tf.math.sign` and `tf.experimental.numpy.sign` in forward mode, it gives `None` instead of a proper value.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x = tf.constant([1], dtype=tf.float32)

for i in range(tf.size(x)):
  tangents = tf.constant([1.])
  with tf.autodiff.ForwardAccumulator(x, tangents) as acc:
    value = tf.math.sign(x)
  jvp_i = acc.jvp(value)
  print(jvp_i)
```


### Relevant log output

```shell
None
```
</details>"
60734,TFLite SPARSE_TO_DENSE dimension mismatch issue when doing prediction,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jupyter notebook on a server, CPU run
- TensorFlow installation (pip package or built from source): 
- TensorFlow library (version, if pip package or github SHA, if built from source):  2.4.0

### 2. Code

```
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)

converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, 
  tf.lite.OpsSet.SELECT_TF_OPS 
]
tflite_quant_model = converter.convert()

with open(""dynamic_range_int8_model.tflite"", 'wb') as f:
  f.write(tflite_quant_model)

interpreter = tf.lite.Interpreter('dynamic_range_int8_model.tflite')
interpreter.allocate_tensors()

predictions = []     # inference using tflite model
output_index = interpreter.get_output_details()[0][""index""]

for i in range(len(data_list)):
    for required_input in interpreter.get_input_details():
        input_index = required_input[""index""]
        print(input_index, required_input[""name""], required_input[""shape""], data_list[i][required_input[""name""]].shape)
        interpreter.resize_tensor_input(input_index, data_list[i][required_input[""name""]].shape, strict = False)
        interpreter.allocate_tensors()
        interpreter.set_tensor(input_index, data_list[i][required_input[""name""]])
        
    interpreter.invoke()
    predictions.append(interpreter.get_tensor(output_index)[0][0])
```


Remark: data_list is the list of n data samples, each being a dict. Here I just want to loop over every data point and record the predictions. The inner loop is over the 23 * 3 = 69 model input tensors.

### 3. Failure after conversion

SPARSE_TP_DENSE produces some strange error.


### 5. (optional) Any other info / logs

Hi, I'm trying to do inference using TFLite model in a jupyter notebook. I have successfully converted the model to 'dynamic_range_int8_model.tflite'. However, when I use the model to do prediction using the above code, I get the following error with the interpreter.allocate_tensors() function:

RuntimeError                              Traceback (most recent call last)
/tmp/ipykernel_255/4027393990.py in <module>
     16         # else:
     17         interpreter.resize_tensor_input(input_index, data_list[i][required_input[""name""]].shape, strict = False)
---> 18         interpreter.allocate_tensors()
     19         interpreter.set_tensor(input_index, data_list[i][required_input[""name""]])
     20 

/export/apps/python/3.7/lib/python3.7/site-packages-custom/tensorflow/lite/python/interpreter.py in allocate_tensors(self)
    257   def allocate_tensors(self):
    258     self._ensure_safe()
--> 259     return self._interpreter.AllocateTensors()
    260 
    261   def _safe_to_run(self):

RuntimeError: tensorflow/lite/kernels/sparse_to_dense.cc:66 SizeOfDimension(indices, 1) != NumElements(output_shape) (2 != 3)Node number 0 (SPARSE_TO_DENSE) failed to prepare.


I will describe the data first. The data contains 23 features in sparse form. The original data is something like: 'Feature 1' : {'index0': [0,1] , 'index1': [2,4] , 'values': [1.34, 0.97] }, 'Feature 2' : {'index0': [1,4,5] , 'index1': [1,0,2] , 'values': [5, 2, 1.5]} ....

My code is to first reshape each TFLite input tensor to the correct size of the model input, according to the last reply of [this issue](https://github.com/tensorflow/tensorflow/issues/42157). The strange thing is that, while I have many input features (interpreter.get_input_details() gives a dict of size 69 = 23 * 3), the for loop goes through until the 42th tensor. However, all the input tensors are generated in the same way (by splitting features into indices, shapes, and values), and I can check that all the 42 input tensors (including the 42th one) are successfully reshaped to the correct input shape (for example. the shape of [1, 2] is reshaped to (35, 2), etc). 

I have transformed the data into the required format of TFlite: ""serving_default_""+feature_name + ""_indices:0"", ""serving_default_""+feature_name + ""_shape:0"", ""serving_default_""+feature_name + ""_values:0""

I can see that the tensor is reshaped. Here is the result of interpreter.get_input_details()[42] before reshaping

{'name': 'serving_default_XXXXX/indices:0',
 'index': 42,
 'shape': array([1, 3], dtype=int32),
 'shape_signature': array([-1,  3], dtype=int32),
 'dtype': numpy.int64,
 'quantization': (0.0, 0),
 'quantization_parameters': {'scales': array([], dtype=float32),
  'zero_points': array([], dtype=int32),
  'quantized_dimension': 0},
 'sparsity_parameters': {}}

After the 'resize_tensor_input' function (this line in the code went through since the error is on allocateTensor() ):

{'name': 'serving_default_XXXXX/indices:0',
 'index': 42,
 'shape': array([1, 2], dtype=int32),
 'shape_signature': array([-1,  3], dtype=int32),
 'dtype': numpy.int64,
 'quantization': (0.0, 0),
 'quantization_parameters': {'scales': array([], dtype=float32),
  'zero_points': array([], dtype=int32),
  'quantized_dimension': 0},
 'sparsity_parameters': {}}

See the 'shape' is changed from 3 to 2.... The error says SizeOfDimension(indices, 1) != NumElements(output_shape) (2 != 3), what is the 'output_shape' how can I change it? 

Thanks very much for your help and suggestion. Please let me know if more information is needed."
60732,[TF 2.0] Signature for unranked tensor not working as intended,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

trying to save a model where one of the functions in the model is defined as:

```
class XYZ(tf.keras.models.Model):

  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])
  def coefficients(self, input_tensor: tf.Tensor):
  ....
  ....
```


When I train the model, and try to build it, I get the following error:

```
File ""/build/work/83bb824f51c38f677e7ad84666ae2d5f4deb/google3/runfiles/google3/vr/perception/computational_photography/ml/video_enhancement/nets/hdrnet.py"", [line 320](https://cs.corp.google.com/piper///depot/google3/vr/perception/computational_photography/ml/video_enhancement/nets/hdrnet.py?l=320&ws=mnatraj/3402&snapshot=40), in call  *
        grid, _ = self.coefficients(inputs[0])
    File ""/build/work/83bb824f51c38f677e7ad84666ae2d5f4deb/google3/runfiles/google3/third_party/tensorflow/python/util/traceback_utils.py"", [line 141](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/util/traceback_utils.py?l=141&ws=mnatraj/3402&snapshot=40), in error_handler  **
        return fn(*args, **kwargs)
    File ""/build/work/83bb824f51c38f677e7ad84666ae2d5f4deb/google3/runfiles/google3/third_party/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", [line 820](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?l=820&ws=mnatraj/3402&snapshot=40), in __call__
        result = self._call(*args, **kwds)
    File ""/build/work/83bb824f51c38f677e7ad84666ae2d5f4deb/google3/runfiles/google3/third_party/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", [line 864](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?l=864&ws=mnatraj/3402&snapshot=40), in _call
        self._initialize(args, kwds, add_initializers_to=initializers)
    File ""/build/work/83bb824f51c38f677e7ad84666ae2d5f4deb/google3/runfiles/google3/third_party/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", [line 687](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?l=687&ws=mnatraj/3402&snapshot=40), in _initialize
        self._variable_creation_fn.get_concrete_function(args, kwds)
    File ""/build/work/83bb824f51c38f677e7ad84666ae2d5f4deb/google3/runfiles/google3/third_party/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", [line 182](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/eager/polymorphic_function/tracing_compiler.py?l=182&ws=mnatraj/3402&snapshot=40), in get_concrete_function
        args, kwargs = function_type_utils.bind_function_inputs(
    File ""/build/work/83bb824f51c38f677e7ad84666ae2d5f4deb/google3/runfiles/google3/third_party/tensorflow/python/eager/polymorphic_function/function_type_utils.py"", [line 451](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/eager/polymorphic_function/function_type_utils.py?l=451&ws=mnatraj/3402&snapshot=40), in bind_function_inputs
        raise TypeError(

    TypeError: Binding inputs to tf.function failed due to `Can not cast TensorSpec(shape=(1, 256, 256, 3), dtype=tf.float32, name=None) to TensorSpec(shape=(None,), dtype=tf.float32, name=None)`. Received args: (<tf.Tensor 'Placeholder:0' shape=(1, 256, 256, 3) dtype=float32>,) and kwargs: {} for signature: (input_tensor: TensorSpec(shape=(None,), dtype=tf.float32, name=None)).
```


Shouldn't specifying input signature as `None` imply that you can pass any input into this function? (unranked tensor)

### Standalone code to reproduce the issue

```shell
(no standalone code)
```


### Relevant log output

_No response_</details>"
60729,"When using tensorflow.keras, model calls ""fit"" report an error, but tensorflow.python.keras does not","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

wsl2 ubuntu 22

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8/8.6.0.163

### GPU model and memory

4080 laptop 12282MiB

### Current Behaviour?

For the official example, an error will occur when you run ""model.fit(train_images, train_labels, epochs=10)"". I found that fit did not report errors when I built the model using tensorflow.python.keras. However, the following problems occur when using tensorflow.keras

### Standalone code to reproduce the issue

```shell
https://tensorflow.google.cn/tutorials/keras/classification
```


### Relevant log output

```shell
Epoch 1/10
2023-05-31 01:16:03.275942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2023-05-31 01:16:03.291693: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f14b44bf730 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-05-31 01:16:03.291739: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 4080 Laptop GPU, Compute Capability 8.9
2023-05-31 01:16:03.295055: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-05-31 01:16:03.414585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600
2023-05-31 01:16:03.423874: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:530] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.
Searched for CUDA in the following directories:
  ./cuda_sdk_lib
  /usr/local/cuda-11.8
  /usr/local/cuda
  .
You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2023-05-31 01:16:03.424029: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-05-31 01:16:03.424265: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-05-31 01:16:03.424288: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INTERNAL: libdevice not found at ./libdevice.10.bc
	 [[{{node StatefulPartitionedCall_2}}]]
2023-05-31 01:16:03.439145: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-05-31 01:16:03.439414: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-05-31 01:16:03.455495: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-05-31 01:16:03.455741: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-05-31 01:16:03.469581: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-05-31 01:16:03.469949: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc


---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
Cell In[14], line 1
----> 1 model.fit(train_images, train_labels, epochs=10)

File ~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback..error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50 try:
     51   ctx.ensure_initialized()
---> 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                       inputs, attrs, num_outputs)
     54 except core._NotOkStatusException as e:
     55   if name is not None:

InternalError: Graph execution error:

Detected at node 'StatefulPartitionedCall_2' defined at (most recent call last):
    File ""/home/aihao/miniconda3/envs/tf/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
...
    File ""/home/aihao/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/optimizer.py"", line 1245, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall_2'
libdevice not found at ./libdevice.10.bc
	 [[{{node StatefulPartitionedCall_2}}]] [Op:__inference_train_function_714]
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
```
</details>"
60724,mkl_aarch64_threadpool build broken by recent commit,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.8.13

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

Build is broken since https://github.com/tensorflow/tensorflow/commit/ea966af46abcdff6cb8d6ef333befa5aa2850e5f

### Standalone code to reproduce the issue

```shell
bazel build --config=mkl_aarch64_threadpool --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --verbose_failures --jobs=100 -- //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (607 packages loaded, 38558 targets configured).
INFO: Found 1 target...
ERROR: /home/builder/1/tensorflow_build/tensorflow_1dnn-git/tensorflow/core/common_runtime/eager/BUILD:644:22: Compiling tensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc failed: (Exit 1): gcc failed: error executing command (from target //tensorflow/core/common_runtime/eager:mkl_eager_op_rewrite) 
  (cd /home/builder/.cache/bazel/_bazel_builder/945690c41481150b9aa58576637dd867/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/builder/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-arm64/bin:/home/builder/venv39/bin:/home/builder/.local/bin:/home/builder/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/builder/venv39/bin/python3 \
    PYTHON_LIB_PATH=/home/builder/venv39/lib/python3.9/site-packages \
    TF2_BEHAVIOR=1 \
  /usr/local/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/aarch64-opt/bin/tensorflow/core/common_runtime/eager/_objs/mkl_eager_op_rewrite/mkl_eager_op_rewrite.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/tensorflow/core/common_runtime/eager/_objs/mkl_eager_op_rewrite/mkl_eager_op_rewrite.pic.o' -fPIC -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT="".so""' '-DLLVM_PLUGIN_EXT="".so""' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_DEREGISTER_FRAME=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_H=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_SETENV_R=1' '-DHAVE_STRERROR_R=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_MALLINFO=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' '-DLLVM_NATIVE_ARCH=""AArch64""' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeAArch64AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeAArch64AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeAArch64Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeAArch64Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeAArch64TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeAArch64TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeAArch64TargetMCA' '-DLLVM_HOST_TRIPLE=""aarch64-unknown-linux-gnu""' '-DLLVM_DEFAULT_TARGET_TRIPLE=""aarch64-unknown-linux-gnu""' '-DLLVM_VERSION_MAJOR=17' '-DLLVM_VERSION_MINOR=0' '-DLLVM_VERSION_PATCH=0' '-DLLVM_VERSION_STRING=""17.0.0git""' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS '-DBLAKE3_USE_NEON=0' -DBLAKE3_NO_AVX2 -DBLAKE3_NO_AVX512 -DBLAKE3_NO_SSE2 -DBLAKE3_NO_SSE41 '-DNO_LLVM_SUPPORT=0' -DCURL_STATICLIB '-DGRPC_ARES=0' -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL '-DEIGEN_USE_AVX512_GEMM_KERNELS=0' '-DEIGEN_ALTIVEC_USE_CUSTOM_PACK=0' '-DEIGEN_NEON_GEBP_NR=4' -DTF_ENABLE_ACTIVITY_WATCHER '-DBAZEL_CURRENT_REPOSITORY=""""' -iquote . -iquote bazel-out/aarch64-opt/bin -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -iquote external/farmhash_archive -iquote bazel-out/aarch64-opt/bin/external/farmhash_archive -iquote external/nsync -iquote bazel-out/aarch64-opt/bin/external/nsync -iquote external/com_google_protobuf -iquote bazel-out/aarch64-opt/bin/external/com_google_protobuf -iquote external/gif -iquote bazel-out/aarch64-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/aarch64-opt/bin/external/libjpeg_turbo -iquote external/com_googlesource_code_re2 -iquote bazel-out/aarch64-opt/bin/external/com_googlesource_code_re2 -iquote external/fft2d -iquote bazel-out/aarch64-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/aarch64-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/aarch64-opt/bin/external/zlib -iquote external/eigen_archive -iquote bazel-out/aarch64-opt/bin/external/eigen_archive -iquote external/double_conversion -iquote bazel-out/aarch64-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/aarch64-opt/bin/external/snappy -iquote external/llvm-project -iquote bazel-out/aarch64-opt/bin/external/llvm-project -iquote external/curl -iquote bazel-out/aarch64-opt/bin/external/curl -iquote external/boringssl -iquote bazel-out/aarch64-opt/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/aarch64-opt/bin/external/jsoncpp_git -iquote external/com_github_grpc_grpc -iquote bazel-out/aarch64-opt/bin/external/com_github_grpc_grpc -iquote external/upb -iquote bazel-out/aarch64-opt/bin/external/upb -iquote external/local_config_cuda -iquote bazel-out/aarch64-opt/bin/external/local_config_cuda -iquote external/local_config_rocm -iquote bazel-out/aarch64-opt/bin/external/local_config_rocm -iquote external/local_config_tensorrt -iquote bazel-out/aarch64-opt/bin/external/local_config_tensorrt -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectBytecodeGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLocationAttributesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorEncodingIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithBaseIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithCanonicalizationIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithOpsInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/BytecodeOpInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/InferIntRangeInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/FuncIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/AsmParserTokenKinds -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/QuantOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/MemorySlotInterfacesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/DialectUtilsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLTypesIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/RuntimeVerifiableOpInterfaceIncGen -Ibazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/aarch64-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/farmhash_archive/src -isystem bazel-out/aarch64-opt/bin/external/farmhash_archive/src -isystem external/nsync/public -isystem bazel-out/aarch64-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/aarch64-opt/bin/external/com_google_protobuf/src -isystem external/gif -isystem bazel-out/aarch64-opt/bin/external/gif -isystem external/zlib -isystem bazel-out/aarch64-opt/bin/external/zlib -isystem third_party/eigen3/mkl_include -isystem bazel-out/aarch64-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/aarch64-opt/bin/external/eigen_archive -isystem external/llvm-project/llvm/include -isystem bazel-out/aarch64-opt/bin/external/llvm-project/llvm/include -isystem external/llvm-project/mlir/include -isystem bazel-out/aarch64-opt/bin/external/llvm-project/mlir/include -isystem external/curl/include -isystem bazel-out/aarch64-opt/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/aarch64-opt/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/aarch64-opt/bin/external/jsoncpp_git/include -isystem external/com_github_grpc_grpc/include -isystem bazel-out/aarch64-opt/bin/external/com_github_grpc_grpc/include -isystem external/com_github_grpc_grpc/src/core/ext/upb-generated -isystem bazel-out/aarch64-opt/bin/external/com_github_grpc_grpc/src/core/ext/upb-generated -isystem external/com_github_grpc_grpc/third_party/address_sorting/include -isystem bazel-out/aarch64-opt/bin/external/com_github_grpc_grpc/third_party/address_sorting/include -isystem external/local_config_cuda/cuda -isystem bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/aarch64-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/aarch64-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/aarch64-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/aarch64-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS '-mtune=generic' '-march=armv8-a' -O3 '-std=c++17' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL '-DDNNL_AARCH64_USE_ACL=1' -pthread -fexceptions -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc -o bazel-out/aarch64-opt/bin/tensorflow/core/common_runtime/eager/_objs/mkl_eager_op_rewrite/mkl_eager_op_rewrite.pic.o)
# Configuration: c6a902c52ef1c3172fb41364dfaf0eb8e48392ba4d35c8c961b05ad0c35a1401
# Execution platform: @local_execution_config_platform//:platform
In file included from tensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc:22:
./tensorflow/core/util/mkl_util.h:27:10: fatal error: dnnl.hpp: No such file or directory
   27 | #include ""dnnl.hpp""
      |          ^~~~~~~~~~
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 2193.259s, Critical Path: 810.20s
INFO: 16688 processes: 2002 internal, 14686 local.
FAILED: Build did NOT complete successfully
```
</details>"
60722,The work of re-implement checkpoint saving(c1e6672f3141015371968b9fb371a5b40abee837) ruined custom saving and restoring op！,"### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12

### Custom Code

Yes

### Python version

Python 3.8/3.9

### Current Behaviour?

For many TensorFlow developers, it is common to customize project-specific variables. To make a particular Variable compatible with the TensorFlow API, it is common to set some private member within its SaveableObject. Also  we need to override the _SingleDeviceSaver saving and restore function for adding custom ops chain into graph. 


For examples: https://github.com/tensorflow/recommenders-addons/blob/master/tensorflow_recommenders_addons/dynamic_embedding/python/ops/tf_save_restore_patch.py


Now commit c1e6672f3141015371968b9fb371a5b40abee837 have ruined them all.

### Standalone code to reproduce the issue

https://github.com/tensorflow/recommenders-addons

run CI failed in TF 2.12."
60721,All input tensors must have the same rank. [Op:MatrixSolve],"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I have two tensors `M` and `E` of shapes `(batch_size, 1, 4, 4)` and `(batch_size, 4, 1)` respectively. The official documentation of [tf.linalg.solve](https://www.tensorflow.org/api_docs/python/tf/linalg/solve) states the two tensors should have shapes `(..., M,M)` and `(...,M,K)`.  After executing my code I get the following error:
```
InvalidArgumentError: {{function_node __wrapped__MatrixSolve_device_/job:localhost/replica:0/task:0/device:CPU:0}} All input tensors must have the same rank. [Op:MatrixSolve]
```

### Standalone code to reproduce the issue

```shell
S = tf.linalg.solve(M, E)
```
```


### Relevant log output

_No response_</details>"
60720,TFLITE issue with android device redmi note 11 pro,"I am currently working on a project where i need to use tflite model for detecting fingers from a camera. I have tried to use gpu delegate or nnapi, but i unfortunately none of these works. While using gpu delegate, i am getting an error as belows

**Internal error: Failed to apply delegate: Can not open OpenCL library on this device - dlopen failed: library ""libOpenCL.so"" not found Falling back to OpenGL TfLiteGpuDelegate Init: No shader implementation for transpose TfLiteGpuDelegate Prepare: delegate is not initialized Node number 390 (TfLiteGpuDelegateV2) failed to prepare. Restored original execution plan after delegate application failure.**

Does anyone else has faced the same problem?
"
60719,Empty logs during model.fit(),"[Issue_Empty_logs_during_model.fit().pdf](https://github.com/tensorflow/tensorflow/files/11584239/Issue_Empty_logs_during_model.fit.pdf)
"
60718,[TFLite C++] Signature calculating CategoricalCrossentropy loss produces wrong result,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.3.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I've created a simple model in Python (TF version 2.10) and converted it for tflite. The model has two signatures, one for inference and other for training. When I run those signatures in Python, everything works correctly, I get good inference result and good training loss. When I load the converted tflite model with the C++ TFLite API (built from source, from branch r2.13) and run those signatures: inference works as intended, training works as intended (the accuracy on the test set is steadily rising), but the reported loss is totally random. At first I thought that loss might be accumulated since it is rising to five digits, but that is not the case since it rises and falls in a random fashion. It seems like there is some bug in the ops used for CategoricalCrossentropy C++ TFLite implementation.

I've tried building tensorflow from r2.12 and r2.13 and I get the same behavior. I've tried r2.10 also but then I couldn't even run the signatures with C++ TFLite API, I was getting bunch of segmentation faults. I couldn't find anywhere the documentation on what ops for backward prop are available in C++ TFLite API, maybe some of those which are used in CategoricalCrossentropy loss calculation are not yet available, or there is a bug in their implementation.

### Standalone code to reproduce the issue

Here is a Python code I am using to create model with signatures:
```
IMG_SIZE = 28

class Model(tf.Module):
    def __init__(self):
        self.model = tf.keras.Sequential([
            tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE), name='flatten'),
            tf.keras.layers.Dense(
                units=10,
                kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                bias_initializer=tf.keras.initializers.Ones(),
                name='dense'
            ),
        ])

        opt = tf.keras.optimizers.SGD(learning_rate=0.1)
        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
        self.model.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy'])

    # The `train` function takes a batch of input images and labels.
    @tf.function(input_signature=[
        tf.TensorSpec([32, IMG_SIZE, IMG_SIZE], tf.float32),
        tf.TensorSpec([32, 10], tf.float32),
    ])
    def train(self, x, y):
        with tf.GradientTape() as tape:
            prediction = self.model(x)
            loss = self.model.loss(y, prediction)
        gradients = tape.gradient(loss, self.model.trainable_variables)
        self.model.optimizer.apply_gradients(
            zip(gradients, self.model.trainable_variables))
        result = {""loss"": loss}
        return result

    @tf.function(input_signature=[
        tf.TensorSpec([1, IMG_SIZE, IMG_SIZE], tf.float32),
    ])
    def infer(self, x):
        logits = self.model(x)
        probabilities = tf.nn.softmax(logits, axis=-1)
        return {
            ""output"": probabilities,
            ""logits"": logits
        }
```

And here is the C++ code I am using to run the tflite model:
```
std::unique_ptr<tflite::FlatBufferModel> model =
    tflite::FlatBufferModel::BuildFromFile(tflite_model_path);
if (model == nullptr)
{
    std::cout << ""Failed to load model"" << std::endl;
    return;
}

tflite::ops::builtin::BuiltinOpResolver resolver;
tflite::InterpreterBuilder builder(*model, resolver);
std::unique_ptr<tflite::Interpreter> interpreter;
builder(&interpreter);
if (interpreter == nullptr)
{
    std::cout << ""Failed to create interpreter"" << std::endl;
    return;
}

if (interpreter->AllocateTensors() != kTfLiteOk)
{
    std::cout << ""Failed to alocate interpreter tensors"" << std::endl;
    return;
}

tflite::SignatureRunner* train_runner = interpreter->GetSignatureRunner(""train"");

TfLiteTensor* input_data_tensor = train_runner->input_tensor(train_runner->input_names()[0]);
float* input_data = input_data_tensor->data.f;
TfLiteTensor* input_labels_tensor = train_runner->input_tensor(train_runner->input_names()[1]);
float* input_labels = input_labels_tensor->data.f;

// Here I fill in the input data and labels, code redacted for brevity.

if (train_runner->Invoke() != kTfLiteOk)
{
    std::cout << ""Error invoking train interpreter signature"" << std::endl;
    return;
}

const TfLiteTensor* output_tensor = train_runner->output_tensor(train_runner->output_names()[0]);
float* output = output_tensor->data.f;
std::cout << ""Training finished with loss: "" << output[0] << std::endl;
```

Please let me know if you need more details, or full source code.


### Relevant log output

Here are the losses from batch to batch, as you can see they are too high and pretty much random. I repeat: the model is training correctly which I can see because the accuracy on the test set is steadily rising, so these loss values do not make sense.
```
Training of batch 1 finished with loss: 172.813
Training of batch 2 finished with loss: 30406.2
Training of batch 3 finished with loss: 35372.7
Training of batch 4 finished with loss: 30955.9
Training of batch 5 finished with loss: 30645.5
Training of batch 6 finished with loss: 39069.4
Training of batch 7 finished with loss: 25181.5
Training of batch 8 finished with loss: 28106.7
Training of batch 9 finished with loss: 12969.1
Training of batch 10 finished with loss: 3079.69
Training of batch 11 finished with loss: 3693.12
Training of batch 12 finished with loss: 3314.77
Training of batch 13 finished with loss: 4591.12
Training of batch 14 finished with loss: 5880.76
Training of batch 15 finished with loss: 5654.75
Training of batch 16 finished with loss: 10133.1
Training of batch 17 finished with loss: 9301.94
Training of batch 18 finished with loss: 11654.5
Training of batch 19 finished with loss: 11827.8
Training of batch 20 finished with loss: 22028.1
Training of batch 21 finished with loss: 8553.58
```

</details>"
60717,make: *** No rule to make target 'test_hello_world_test'.  Stop.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

Ubuntu wsl

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

tensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.
tensorflow/lite/micro/tools/make/downloads/kissfft already exists, skipping the download.
tensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.
make: *** No rule to make target 'test_hello_world_test'.  Stop.

### Standalone code to reproduce the issue

```shell
sudo make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test
```


### Relevant log output

_No response_</details>"
60716,failed to build branch r2.13,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

r2,13

### Custom Code

No

### OS Platform and Distribution

Linux radar-prod1 5.10.0-22-amd64 #1 SMP Debian 5.10.178-3 (2023-04-22) x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

Build label: 7.0.0-pre.20230517.4- (@non-git)

### GCC/Compiler version

gcc version 10.2.1 20210110 (Debian 10.2.1-6) 

### CUDA/cuDNN version

-

### GPU model and memory

-

### Current Behaviour?

Failed to build pip package whl

### Standalone code to reproduce the issue

```shell
Just bare debian 11 with latest requirements versions (27.05.2023).
```


### Relevant log output

```shell
ruslan@radar-prod1:/sdk4/tensorflow$ bazel build --config=opt --copt=-march=native --copt=-O3 //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=159
INFO: Reading rc options for 'build' from /sdk4/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /sdk4/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /sdk4/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3
INFO: Reading rc options for 'build' from /sdk4/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug
INFO: Found applicable config definition build:short_logs in file /sdk4/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /sdk4/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file /sdk4/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /sdk4/tensorflow/.bazelrc: --define=build_with_onednn_v2=true --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /sdk4/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Build options --copt and --host_copt have changed, discarding analysis cache (this can be expensive).
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 35966 targets configured).
INFO: Found 1 target...
ERROR: /sdk4/tensorflow/tensorflow/cc/BUILD:766:22: Linking tensorflow/cc/ops/functional_ops_gen_cc [for tool] failed: (Exit 1): gcc failed: error executing CppLink command (from target //tensorflow/cc:ops/functional_ops_gen_cc) /usr/bin/gcc @bazel-out/k8-opt-exec-ST-977fe3d4dab2/bin/tensorflow/cc/ops/functional_ops_gen_cc-2.params
bazel-out/k8-opt-exec-ST-977fe3d4dab2/bin/_solib_k8/_U_S_Stensorflow_Scc_Cops_Sfunctional_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'tsl::table::NewLRUCache(unsigned long)'
bazel-out/k8-opt-exec-ST-977fe3d4dab2/bin/_solib_k8/_U_S_Stensorflow_Scc_Cops_Sfunctional_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'tensorflow::GpuIdManager::TfToPlatformDeviceId(tsl::gtl::IntType<tsl::TfDeviceId_tag_, int>, tsl::gtl::IntType<tsl::PlatformDeviceId_tag_, int>*)'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2405.712s, Critical Path: 314.03s
INFO: 11230 processes: 318 internal, 10912 local.
ERROR: Build did NOT complete successfully
ruslan@radar-prod1:/sdk4/tensorflow$ bazel clean
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=159
INFO: Reading rc options for 'clean' from /sdk4/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'clean' from /sdk4/tensorflow/.bazelrc:
  Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'clean' from /sdk4/tensorflow/.tf_configure.bazelrc:
  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3
INFO: Reading rc options for 'clean' from /sdk4/tensorflow/.bazelrc:
  Inherited 'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug
INFO: Found applicable config definition build:short_logs in file /sdk4/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /sdk4/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /sdk4/tensorflow/.bazelrc: --define=build_with_onednn_v2=true --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /sdk4/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
ruslan@radar-prod1:/sdk4/tensorflow$ bazel build --config=opt --copt=-march=native --copt=-O3 //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=159
INFO: Reading rc options for 'build' from /sdk4/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /sdk4/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /sdk4/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3
INFO: Reading rc options for 'build' from /sdk4/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug
INFO: Found applicable config definition build:short_logs in file /sdk4/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /sdk4/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file /sdk4/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /sdk4/tensorflow/.bazelrc: --define=build_with_onednn_v2=true --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /sdk4/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (614 packages loaded, 35966 targets configured).
INFO: Found 1 target...
ERROR: /sdk4/tensorflow/tensorflow/lite/experimental/microfrontend/BUILD:97:21: Linking tensorflow/lite/experimental/microfrontend/gen_audio_microfrontend_op_py_wrappers_cc [for tool] failed: (Exit 1): gcc failed: error executing CppLink command (from target //tensorflow/lite/experimental/microfrontend:gen_audio_microfrontend_op_py_wrappers_cc) /usr/bin/gcc @bazel-out/k8-opt-exec-ST-977fe3d4dab2/bin/tensorflow/lite/experimental/microfrontend/gen_audio_microfrontend_op_py_wrappers_cc-2.params
bazel-out/k8-opt-exec-ST-977fe3d4dab2/bin/_solib_k8/_U_S_Stensorflow_Slite_Sexperimental_Smicrofrontend_Cgen_Uaudio_Umicrofrontend_Uop_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'tsl::table::NewLRUCache(unsigned long)'
bazel-out/k8-opt-exec-ST-977fe3d4dab2/bin/_solib_k8/_U_S_Stensorflow_Slite_Sexperimental_Smicrofrontend_Cgen_Uaudio_Umicrofrontend_Uop_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'tensorflow::GpuIdManager::TfToPlatformDeviceId(tsl::gtl::IntType<tsl::TfDeviceId_tag_, int>, tsl::gtl::IntType<tsl::PlatformDeviceId_tag_, int>*)'
bazel-out/k8-opt-exec-ST-977fe3d4dab2/bin/tensorflow/python/framework/_objs/python_op_gen_main/python_op_gen_main.o:python_op_gen_main.cc:function main: error: undefined reference to 'tsl::Flag::Flag(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool*)'
bazel-out/k8-opt-exec-ST-977fe3d4dab2/bin/tensorflow/python/framework/_objs/python_op_gen_main/python_op_gen_main.o:python_op_gen_main.cc:function main: error: undefined reference to 'tsl::Flag::Flag(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool*)'
bazel-out/k8-opt-exec-ST-977fe3d4dab2/bin/tensorflow/python/framework/_objs/python_op_gen_main/python_op_gen_main.o:python_op_gen_main.cc:function main: error: undefined reference to 'tsl::Flag::Flag(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool*)'
bazel-out/k8-opt-exec-ST-977fe3d4dab2/bin/tensorflow/python/framework/_objs/python_op_gen_main/python_op_gen_main.o:python_op_gen_main.cc:function main: error: undefined reference to 'tsl::Flag::Flag(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool*)'
bazel-out/k8-opt-exec-ST-977fe3d4dab2/bin/tensorflow/python/framework/_objs/python_op_gen_main/python_op_gen_main.o:python_op_gen_main.cc:function main: error: undefined reference to 'tsl::Flags::Usage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tsl::Flag, std::allocator<tsl::Flag> > const&)'
bazel-out/k8-opt-exec-ST-977fe3d4dab2/bin/tensorflow/python/framework/_objs/python_op_gen_main/python_op_gen_main.o:python_op_gen_main.cc:function main: error: undefined reference to 'tsl::Flags::Parse(int*, char**, std::vector<tsl::Flag, std::allocator<tsl::Flag> > const&)'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2518.592s, Critical Path: 378.85s
INFO: 14151 processes: 1834 internal, 12317 local.
ERROR: Build did NOT complete successfully
ruslan@radar-prod1:/sdk4/tensorflow$ 
```
```
</details>"
60715,Incorrect gradient in divide_no_nan and reciprocal_no_nan when divide by 0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf-nightly

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When we perform divide by zero in `tf.math.divide_no_nan` and `tf.math.reciprocal_no_nan`, the theoretical and numerical gradient given by `tf.test.compute_gradient` do not match. However, if the input is valid (without dividing by zero), the gradients are fine.

More examples in [gist here](https://colab.research.google.com/drive/1U102ToL3El9wHduuDyjQXtpsZkaByDg5?usp=sharing)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([10.0, 20.0])
y = tf.constant([2.0, 0.0])
th, nu= tf.test.compute_gradient(tf.math.divide_no_nan, [x, y])
print(th)
print(nu)
print(tf.experimental.numpy.allclose(th, nu, atol=1e-3))


import tensorflow as tf
x = tf.constant([2.0, 0.0])
th, nu= tf.test.compute_gradient(tf.math.reciprocal_no_nan, [x])
print(th)
print(nu)
print(tf.experimental.numpy.allclose(th, nu, atol=1e-3))
```


### Relevant log output

```shell
(array([[0.5, 0. ],
       [0. , 0. ]], dtype=float32), array([[-2.5,  0. ],
       [-0. ,  0. ]], dtype=float32))
(array([[0.5, 0. ],
       [0. , 0. ]], dtype=float32), array([[-2.5002441e+00,  0.0000000e+00],
       [ 0.0000000e+00,  2.0971520e+07]], dtype=float32))
tf.Tensor(False, shape=(), dtype=bool)



(array([[-0.25,  0.  ],
       [-0.  ,  0.  ]], dtype=float32),)
(array([[-2.500000e-01,  0.000000e+00],
       [ 0.000000e+00,  1.048576e+06]], dtype=float32),)
tf.Tensor(False, shape=(), dtype=bool)
```
</details>"
60714,`tf.split` or `tf.transpose` cause errors for quantize-aware training with `quantize_apply`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.7 & 2.12

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

We are trying to implement some network like [ShuffleNetV2](https://arxiv.org/abs/1807.11164) but encounter some error when `quantize_apply` the model.

![image](https://user-images.githubusercontent.com/22385182/233325404-e2e502fe-1151-4f4c-9377-570a01681848.png)

I believe ShuffleNet or related ideas are popular in edge devices, please kindly help us to resolve this proble.

Any advice is welcome.

_I apologize for this should have been posted as an issue on Tensorflow Model Optimization. However, since it seems that this problem is not unique to me, I'm posting it here in the hope of receiving appropriate suggestions or assistance._


### System information

TensorFlow version (installed from source or binary): 2.7.0

TensorFlow Model Optimization version (installed from source or binary): 0.7.0

Python version: 3.8.13

**We also try on latest release of both module, but also not working.**

### Describe the current behavior

When running the provided code, either the `tf.transpose` or `tf.split` will cause error to Tensorflow Model Optimization.

The error message due to `tf.split` before convolution layers:

```
ValueError: Exception encountered when calling layer ""bn3"" (type BatchNormalization).

Shape must be rank 4 but is rank 5 for '{{node bn3/FusedBatchNormV3}} = FusedBatchNormV3[T=DT_FLOAT, U=DT_FLOAT, data_format=""NHWC"", epsilon=0.001, exponential_avg_factor=1, is_training=false](Placeholder, bn3/ReadVariableOp, bn3/ReadVariableOp_1, bn3/FusedBatchNormV3/ReadVariableOp, bn3/FusedBatchNormV3/ReadVariableOp_1)' with input shapes: [1,?,128,128,32], [32], [32], [32], [32].
```

The error message due to `tf.transpose`:

```
ValueError: Exception encountered when calling layer ""tf.compat.v1.transpose"" (type TFOpLambda).

Dimension must be 6 but is 5 for '{{node tf.compat.v1.transpose/transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](tf.compat.v1.transpose/transpose/a, tf.compat.v1.transpose/transpose/perm)' with input shapes: [1,?,128,128,2,32], [5].
```


### Standalone code to reproduce the issue

Just run the following code you will get the error message due to `tf.split`.

```python
from __future__ import annotations

from typing import Callable, Optional

import tensorflow as tf
import tensorflow_model_optimization as tfmot
from tensorflow.keras import layers


SKIP_LAYER = [
    ""resize"",
    ""Resize"",
    ""reshape"",
    ""Reshape"",
    ""concat"",
    ""Concat"",
    ""ExpandDims"",
    ""Repeats"",
    ""Shape"",
    ""strided_slice"",
    ""Tile"",
]


def quantize_model(
    model: tf.keras.Model,
    annotate: Optional[Callable] = None,
    quantize_scope: Optional[dict[str, tf.keras.layers.Layer]] = None,
) -> tf.keras.Model:
    quantize_scope = {} if quantize_scope is None else quantize_scope

    def annotate(layer):
        if any([name in layer.name for name in SKIP_LAYER]):
            return layer
        else:
            return tfmot.quantization.keras.quantize_annotate_layer(layer)

    anno_model = tf.keras.models.clone_model(model, clone_function=annotate)
    with tfmot.quantization.keras.quantize_scope(quantize_scope):
        model = tfmot.quantization.keras.quantize_apply(anno_model)

    return model


def channel_shuffle(tensor: tf.Tensor, groups: int = 2) -> tf.Tensor:
    """"""Channel shuffle operation.""""""
    _, height, width, num_channels = tensor.shape.as_list()
    assert num_channels % groups == 0

    tensor = tf.reshape(tensor, [-1, height, width, groups, num_channels // groups])
    tensor = tf.transpose(tensor, [0, 1, 2, 4, 3])
    tensor = tf.identity(tensor, name=""channel_shuffle"")

    tensor = tf.reshape(tensor, [-1, height, width, num_channels])
    return tensor


def simple_nn(img_input: tf.Tensor) -> tf.Tensor:
    latent = layers.Conv2D(32, 1, padding=""same"", use_bias=False, name=""conv1"")(img_input)
    latent = layers.BatchNormalization(name=""bn1"")(latent)
    latent = layers.ReLU(name=""relu1"")(latent)

    latent = layers.DepthwiseConv2D(3, 1, padding=""same"", name=""conv2"")(img_input)
    latent = layers.BatchNormalization(name=""bn2"")(latent)

    latent = layers.Conv2D(32, 1, padding=""same"", use_bias=False, name=""conv3"")(img_input)
    latent = layers.BatchNormalization(name=""bn3"")(latent)
    latent = layers.ReLU(name=""relu3"")(latent)

    return latent


def split_like_nn(img_input: tf.Tensor) -> tf.Tensor:
    latent = layers.Conv2D(64, 1, padding=""same"", use_bias=False, name=""conv0"")(img_input)
    latent = layers.BatchNormalization(name=""bn0"")(latent)
    latent = layers.ReLU(name=""relu0"")(latent)

    latent_0, latent_1 = tf.split(latent, 2, axis=-1)
    latent_0 = simple_nn(latent_0)
    latent = tf.concat([latent_0, latent_1], axis=-1)

    latent = channel_shuffle(latent)

    return latent


if __name__ == ""__main__"":
    img_input = tf.keras.Input((128, 128, 1), dtype=tf.float32, name=""img"")

    outputs = split_like_nn(img_input)

    model = tf.keras.Model(inputs=img_input, outputs=outputs, name=""PoseNetV2"")
    model.summary()

    model_qat = quantize_model(model)
    model_qat.summary()
```


You can just comment the following three lines of code will get the error message from `tf.transpose`.

```python
 latent_0, latent_1 = tf.split(latent, 2, axis=-1)
 latent_0 = simple_nn(latent_0)
 latent = tf.concat([latent_0, latent_1], axis=-1)
```
"
60713,"TensorFlow can't communicate with GPU. ""None of the algorithms provided by cuDNN frontend heuristics worked; trying fallback algorithms."" ""No algorithm worked!""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04.2 LTS and Red Hat Enterprise Linux 9.2

### Mobile device

_No response_

### Python version

3.10

### Bazel version

bazel 5.3.0

### GCC/Compiler version

11.3.1

### CUDA/cuDNN version

11.8

### GPU model and memory

Nvidia Quadro K620 - 2048MiB of memory

### Current Behaviour?

*This bug happens both in Linux Ubuntu 22.04.2 LTS and RHEL 9. I tested it on a live-usb Ubuntu installation, pip installing it with GPU support after getting tired of trying to solve it on rhel.

It is somewhat related to the convolutional (Conv2d) layer, because fitting a model on a pip install with the use of only dense layers works fine. With a source install this error happens independently of model time.

Using CLI or multi-user.target, leaving all the GPU resources to tf does not solve the issue.

Reducing the Conv2d filters does not solve the problem, nor the bach_size

The code should just have no errors. I can't exactly tell or understand the error output, so that's about all I can tell.

### Standalone code to reproduce the issue

```shell
from keras.datasets import mnist
from keras.utils import to_categorical
from keras import layers, models

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))

model.compile(optimizer=""rmsprop"", loss=""categorical_crossentropy"", metrics=['accuracy'])

train_images = train_images.astype('float32') / 255
test_images = test_images.astype('float32') / 255

trains_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

model.fit(train_images, trains_labels, epochs=5, batch_size=64)
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f'{test_acc=}')
```


### Relevant log output

```shell
(tf) [lnapon@rhel ancient_ai]$ python3 lrnkeras/mnist_conv.py 
2023-05-25 19:21:32.077966: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-25 19:21:33.793139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-25 19:21:33.809742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-25 19:21:33.809975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-25 19:21:33.811051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-25 19:21:33.811233: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-25 19:21:33.811384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-25 19:21:34.315816: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-25 19:21:34.316045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-25 19:21:34.316220: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-25 19:21:34.316380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1294 MB memory:  -> device: 0, name: Quadro K620, pci bus id: 0000:01:00.0, compute capability: 5.0
Epoch 1/5
2023-05-25 19:21:35.567154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600
2023-05-25 19:21:35.886005: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: the resource allocation failed
2023-05-25 19:21:35.886043: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:222] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.
2023-05-25 19:21:35.886349: W tensorflow/core/kernels/conv_ops_gpu.cc:143] None of the algorithms provided by cuDNN frontend heuristics worked; trying fallback algorithms.  Conv: batch: 1
in_depths: 1
out_depths: 32
in: 28
in: 28
data_format: 1
filter: 3
filter: 3
filter: 1
dilation: 1
dilation: 1
stride: 1
stride: 1
padding: 0
padding: 0
dtype: DT_FLOAT
group_count: 1
device_identifier: ""sm_5.0 with 2099118080B RAM, 3 cores, 1124000KHz clock, 900000KHz mem clock, 2097152B L2$""
fusion {
  activation_mode: kRelu
  conv_scale: 1
}
version: 3

2023-05-25 19:21:35.893161: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: the resource allocation failed
2023-05-25 19:21:35.893178: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:222] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.
2023-05-25 19:21:35.893443: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_ops_fused_impl.h:625 : NOT_FOUND: No algorithm worked!  Error messages:
  Profiling failure on CUDNN engine eng11{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(4639): 'status'
  Profiling failure on CUDNN engine eng0{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(4639): 'status'
2023-05-25 19:21:35.893485: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): NOT_FOUND: No algorithm worked!  Error messages:
  Profiling failure on CUDNN engine eng11{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(4639): 'status'
  Profiling failure on CUDNN engine eng0{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(4639): 'status'
	 [[{{node sequential/conv2d/Relu}}]]
Traceback (most recent call last):
  File ""/home/lnapon/repos/ancient_ai/lrnkeras/mnist_conv.py"", line 28, in <module>
    model.fit(train_images, trains_labels, epochs=5, batch_size=1)
  File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.NotFoundError: Graph execution error:

Detected at node 'sequential/conv2d/Relu' defined at (most recent call last):
    File ""/home/lnapon/repos/ancient_ai/lrnkeras/mnist_conv.py"", line 28, in <module>
      model.fit(train_images, trains_labels, epochs=5, batch_size=1)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py"", line 1685, in fit
      tmp_logs = self.train_function(iterator)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py"", line 1284, in train_function
      return step_function(self, iterator)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py"", line 1268, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py"", line 1249, in run_step
      outputs = model.train_step(data)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py"", line 1050, in train_step
      y_pred = self(x, training=True)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py"", line 558, in __call__
      return super().__call__(*args, **kwargs)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer.py"", line 1145, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/sequential.py"", line 412, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/functional.py"", line 512, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/functional.py"", line 669, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer.py"", line 1145, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/layers/convolutional/base_conv.py"", line 321, in call
      return self.activation(outputs)
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/activations.py"", line 317, in relu
      return backend.relu(
    File ""/home/lnapon/miniconda3/envs/tf/lib/python3.10/site-packages/keras/backend.py"", line 5396, in relu
      x = tf.nn.relu(x)
Node: 'sequential/conv2d/Relu'
No algorithm worked!  Error messages:
  Profiling failure on CUDNN engine eng11{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(4639): 'status'
  Profiling failure on CUDNN engine eng0{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(4639): 'status'
	 [[{{node sequential/conv2d/Relu}}]] [Op:__inference_train_function_1058]
```
</details>"
60711,Custom Keras Optimizer over TPU strategy error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hello,

I'd wish to draw your attention to a bug that affects custom Optimizers extended from the class `tf.keras.optimizers.experimental.Optimizer`  while running on TPU clusters on native Google Colab.

```
_________________________________________________________________
Epoch 1/2
---------------------------------------------------------------------------
UnavailableError                          Traceback (most recent call last)
[<ipython-input-7-7647619ded22>](https://localhost:8080/#) in <cell line: 1>()
      5   model(x_train[:2])
      6   model.summary()
----> 7   model.fit(x_train, y_train, epochs = 2, batch_size = 1024)
      8   opt.temp

1 frames
[/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py](https://localhost:8080/#) in _numpy(self)
   1126       return self._numpy_internal()
   1127     except core._NotOkStatusException as e:  # pylint: disable=protected-access
-> 1128       raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   1129 
   1130   @property

UnavailableError: failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:40812: Failed to connect to remote host: Connection refused
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:40812: Failed to connect to remote host: Connection refused {created_time:""2023-05-25T19:42:27.514045935+00:00"", grpc_status:14}
```

While the code refuses to execute in order to compute gradients while using a custom optimizer - opt, it happens to run very well while using 'adam' on default settings.

The standalone code to reproduce the problem is provided and doesn't requires any extra tool or dependency and can be easily pasted into Colab to run and inspect the execution for bug investigations.

_______________________________________________________________________________________________________________________

Apart from that I wish addition of a new feature - *batch size* of the data being currently used by the optimizer to compute gradients, as we know is pretty straightforward in a single CPU/GPU, but becomes difficult while executing the same code in distributed systems. If possible, kindly amend my existing standalone code below to add that small tiny feature into my Optimizer.
Thank You.

Best Regards


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

class GradientDescent(tf.keras.optimizers.experimental.Optimizer):
  def __init__(self, learning_rate = 0.01, name='GDST'):
    super().__init__(name=name)
    #self.learning_rate = learning_rate
    self._learning_rate = self._build_learning_rate(learning_rate)
    self.temp = None
  
  def build(self, var_list):
    super().build(var_list)
  
  def update_step(self, gradient, variable):
    lr = tf.cast(self._learning_rate, gradient.dtype)
    output = tf.clip_by_value(self._learning_rate*gradient, clip_value_max = gradient.dtype.max, clip_value_min = gradient.dtype.min)
    variable.assign_sub(output)
    self.temp = output


  def get_config(self):
    return super().get_config()

opt = GradientDescent(learning_rate = 0.0001)

input_shape = 10000
output_shape = 100

def return_model(input_shape = 10000, output_shape = 500):
  model = tf.keras.Sequential([
    tf.keras.layers.Dense(input_shape, activation='relu'),
    tf.keras.layers.Dense(10000, activation='relu'),
    #tf.keras.layers.Dense(10000, activation='tanh'),
    #tf.keras.layers.Dense(10000, activation='linear'),
    #tf.keras.layers.Dense(10000, activation='tanh'),
    tf.keras.layers.Dense(output_shape, activation='linear')
])
  return model


# Compile the model
#model.compile(optimizer=opt, loss='mse')

# Create a random dataset
x_train = tf.random.uniform(shape=[10000, 10000])
y_train = tf.random.uniform(shape=[10000, 500])

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
# This is the TPU initialization code that has to be at the beginning.
tf.tpu.experimental.initialize_tpu_system(resolver)
print(""All devices: "", tf.config.list_logical_devices('TPU'))

strategy = tf.distribute.TPUStrategy(resolver)

with strategy.scope():
  model = return_model()
  model.compile(optimizer=opt, loss='mse')

  model(x_train[:2])
  model.summary()
  model.fit(x_train, y_train, epochs = 2, batch_size = 1024)
  opt.temp

```
```


### Relevant log output

_No response_</details>"
60710,Dataset.ragged_batch does not produce correct specs with tf.py_function and tf.numpy_function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

docker container nvcr.io/nvidia/tensorflow:23.04-tf2-py3 on Ubuntu 22.04.2 LTS host

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I'm trying to train an object detection model where images may have a different number of bounding boxes. Also I want to add some augmentations, and since tf does not support augmentations of bounding boxes I choose albumentations to do the job. I can't use albumentations' augmentations directly, so I need to use either `tf.py_function` or `tf.numpy_function`. I used `Dataset.ragged_batch` instead of `Dataset.batch` (because the dimension of bbox tensor may vary), but it did not provide me the correct `element_spec` and I was unable to make it work.

These are three scenarios that should help to understand the issue:

### Scenario 1:
I don't use any augmentations, `ragged_batch` returns the correct element spec, but I really need those augmentations
```
(TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=None),
 {'classes': RaggedTensorSpec(TensorShape([None, None]), tf.float32, 1, tf.int64),
  'boxes': RaggedTensorSpec(TensorShape([None, None, 4]), tf.float64, 1, tf.int64)})
```
### Scenario 2:
I use `tf.numpy_function` fo perform the augmentations. The spec is incorrect, I can't batch the items
```
(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),
 {'classes': TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),
  'boxes': TensorSpec(shape=<unknown>, dtype=tf.float32, name=None)})
```
### Scenario 3
I use `tf.py_function`, provide something, that looks like correct spec to `Tout` param:
```
Tout=[
    tf.TensorSpec(shape=[None, 512, 512, 3], dtype=tf.float32), 
    tf.RaggedTensorSpec(shape=[None, 4], dtype=tf.float32), 
    tf.RaggedTensorSpec(shape=[None, None], dtype=tf.float32),
],
```
but spec for image still does not look good
```
(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),
 {'classes': RaggedTensorSpec(TensorShape([None, None, None]), tf.float32, 2, tf.int64),
  'boxes': RaggedTensorSpec(TensorShape([None, None, 4]), tf.float32, 2, tf.int64)})
```
and I had to add an extra dimension for labels spec in order to convert it to `RaggedTensor` (which is probably not good as well). Model refuses to be trained because of incorrect image dimensions

The (non)working code is here - [colab link](https://colab.research.google.com/drive/148i78QRnF98guvx1Y0gZXtBVrv-day42?usp=sharing)

### Standalone code to reproduce the issue

```shell
import keras_cv
import numpy as np
import tensorflow as tf
import albumentations as A

def generate_random_data():
    while True:
        image = np.random.randint(0, 256, size=(512, 512, 3), dtype=np.uint8)
        num_bboxes = np.random.randint(1, 200)
        bboxes = []
        labels = []
        for _ in range(num_bboxes):
            x_min, y_min, x_max, y_max = np.sort(np.random.uniform(0, 512, size=4) / 512)
            bbox = [x_min, y_min, x_max, y_max]
            label = np.random.choice([0, 1])
            bboxes.append(bbox)
            labels.append(label)

        data = {
            'image': tf.convert_to_tensor(image),
            'bboxes': {
                'bbox': tf.convert_to_tensor(bboxes),
                'label': tf.convert_to_tensor(labels, dtype=tf.int64),
            }
        }
        yield data

# Create the random dataset
dataset = tf.data.Dataset.from_generator(generate_random_data, output_signature={
    'image': tf.TensorSpec(shape=(512, 512, 3), dtype=tf.uint8),
    'bboxes': {
        'bbox': tf.TensorSpec(shape=(None, 4), dtype=tf.float64),
        'label': tf.TensorSpec(shape=(None,), dtype=tf.int64),
    },
})


# Scenario 1
def preprocess_data_1(inputs):
    bounding_boxes = {
        ""classes"": tf.cast(inputs[""bboxes""][""label""], dtype=tf.float32),
        ""boxes"": inputs[""bboxes""][""bbox""],
    }
    return tf.image.convert_image_dtype(inputs['image'], tf.float32), bounding_boxes
ds_1 = dataset.map(preprocess_data_1).ragged_batch(2)

# Scenario 2
def transform_2(image, bboxes, labels):
    transforms = A.Compose(
        [
            A.Rotate(limit=40),
        ],
        bbox_params=A.BboxParams(
            format='albumentations',
            label_fields=['label'],
        )
    )

    transformed = transforms(
        image=image, 
        label=labels,
        bboxes=bboxes,
    )

    return transformed

def aug_fn_2(image, bboxes, labels):
    aug_data = transform_2(image, bboxes, labels)
    return (
        tf.image.convert_image_dtype(aug_data[""image""], tf.float32), 
        tf.convert_to_tensor(aug_data[""bboxes""], dtype=tf.float32), 
        tf.cast(aug_data[""label""], tf.float32)
    )

def preprocess_data_2(inputs):
    bboxes = inputs['bboxes']['bbox']
    labels = inputs['bboxes']['label']
    aug_image, aug_bboxes, aug_labels = tf.numpy_function(
        func=aug_fn_2, 
        inp=[inputs[""image""], bboxes, labels], 
        Tout=[tf.float32, tf.float32, tf.float32],
    )
    
    bounding_boxes = {
        ""classes"": aug_labels,
        ""boxes"":  aug_bboxes
    }

    return aug_image, bounding_boxes

ds_2 = dataset.map(preprocess_data_2).ragged_batch(2)
for item in ds_2:
    break

# Scenario 3
def transform_3(image, bboxes, labels):
    transforms = A.Compose(
        [
            A.Rotate(limit=40),
        ],
        bbox_params=A.BboxParams(
            format='albumentations',
            label_fields=['label'],
        )
    )

    transformed = transforms(
        image=image.numpy(), 
        label=labels.numpy(),
        bboxes=bboxes.numpy(),
    )

    return transformed

def aug_fn_3(image, bboxes, labels):
    aug_data = transform_3(image, bboxes, labels)

    return (
        tf.image.convert_image_dtype(aug_data[""image""], tf.float32), 
        tf.RaggedTensor.from_tensor(tf.convert_to_tensor(aug_data[""bboxes""], dtype=tf.float32)),
        tf.RaggedTensor.from_tensor(tf.cast([aug_data[""label""]], tf.float32)),
    )

def preprocess_data_3(inputs):
    bboxes = inputs['bboxes']['bbox']
    labels = inputs['bboxes']['label']
    aug_image, aug_bboxes, aug_labels = tf.py_function(
        func=aug_fn_3, 
        inp=[inputs[""image""], bboxes, labels], 
        Tout=[
            tf.TensorSpec(shape=[None, 512, 512, 3], dtype=tf.float32), 
            tf.RaggedTensorSpec(shape=[None, 4], dtype=tf.float32), 
            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.float32),
        ],
    )
    
    bounding_boxes = {
        ""classes"": aug_labels,
        ""boxes"":  aug_bboxes,
    }

    return aug_image, bounding_boxes

ds_3 = dataset.map(preprocess_data_3).ragged_batch(2)
for batch in ds_3:
    break

model = keras_cv.models.RetinaNet.from_preset(
    ""resnet50_imagenet"",
    num_classes=2,
    bounding_box_format=""rel_xyxy"",
)

model.compile(
    classification_loss=""focal"",
    box_loss=""smoothl1"",
    optimizer=tf.optimizers.Adam(),
)

model.fit(
    ds_3.take(1),
    validation_data=ds_3.take(1),
    epochs=100,
)
```


### Relevant log output

_No response_</details>"
60707,Hexagon Libraries version `v1.20.0.9` not available,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.11.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

Android

### Python version

_No response_

### Bazel version

6.2.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I'm trying to use Hexagon delegates for Android and I couldn't find the proper `libhexagon_nn_skel*.so` libraries at https://www.tensorflow.org/lite/android/delegates/hexagon#step_2_add_hexagon_libraries_to_your_android_app_2

Could you share a link with the shared library built? As TensorFlow Lite `v2.11.1` requires `v1.20.0.9`.

I've tried to use `v1.20.0.1` but got the following error:
```
Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.
```

### Standalone code to reproduce the issue

```shell
/* Hexagon Delegate */
  const char *lib_path = ""/data/local/tmp/dsp"";
  TfLiteHexagonInitWithPath(lib_path);
  auto options_hexagon = TfLiteHexagonDelegateOptionsDefault();
  _delegateHexagon.reset(TfLiteHexagonDelegateCreate(&options_hexagon));
  auto retTflite = _interpreter->ModifyGraphWithDelegate(_delegateHexagon.get());
  if (retTflite != kTfLiteOk)
  {
    LOGWARN(""Failed to delegate to Hexagon [%d]"", retTflite);
  }
```


### Relevant log output

_No response_</details>"
60706,tensorflow2.11 with MultiWorkerMirroredStrategy  cannot work,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.9，tf2.11

### Custom Code

Yes

### OS Platform and Distribution

cnetos7.9

### Mobile device

_No response_

### Python version

3.8.12

### Bazel version

5.3.0,5.0.0

### GCC/Compiler version

9.3

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!
The four-process four-card simulation based on the stand-alone four-card simulation is running, but I found that my program is stuck in the communication link of GRPC, and the program cannot continue to execute downward.

### Standalone code to reproduce the issue

```shell
resnet50 based on the https://github.com/tensorflow/models/tree/v2.11.0
```


### Relevant log output

```shell
Instructions for updating:
use distribute.MultiWorkerMirroredStrategy instead
W0525 11:06:39.106403 139892523235136 deprecation.py:350] From /public/home/qianyj/TF_test/dtk23.04/tf2.11/models-2.11.0_fp16_4m/official/common/distribute_utils.py:155: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.
Instructions for updating:
use distribute.MultiWorkerMirroredStrategy instead
2023-05-25 11:06:39.107776: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-25 11:06:39.107986: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-25 11:06:39.109601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15601 MB memory:  -> device: 0, name: Vega 20, pci bus id: 0000:43:00.0
2023-05-25 11:06:39.109709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15601 MB memory:  -> device: 0, name: Vega 20, pci bus id: 0000:04:00.0
2023-05-25 11:06:40.104680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:worker/replica:0/task:1/device:GPU:0 with 15601 MB memory:  -> device: 0, name: Vega 20, pci bus id: 0000:26:00.0
2023-05-25 11:06:40.104734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:worker/replica:0/task:2/device:GPU:0 with 15601 MB memory:  -> device: 0, name: Vega 20, pci bus id: 0000:43:00.0
2023-05-25 11:06:40.105277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:worker/replica:0/task:0/device:GPU:0 with 15601 MB memory:  -> device: 0, name: Vega 20, pci bus id: 0000:04:00.0
2023-05-25 11:06:40.105313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:worker/replica:0/task:3/device:GPU:0 with 15601 MB memory:  -> device: 0, name: Vega 20, pci bus id: 0000:63:00.0
2023-05-25 11:06:40.114474: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:447] Started server with target: grpc://localhost:40013
2023-05-25 11:06:40.115146: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:447] Started server with target: grpc://localhost:40014
2023-05-25 11:06:40.115326: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:447] Started server with target: grpc://localhost:40015
2023-05-25 11:06:40.115823: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:447] Started server with target: grpc://localhost:40016
```
</details>"
60704,Samsung S21: Noisy AI Inference Results with App Animations in PlayService/GPU Mode,"**System information**
- Android Device information : 
samsung/o1qzhx/o1q:12/SP1A.210812.016/G9910ZHU2CVG2:user/release-keys
- TensorFlow Lite in Play Services SDK version : 
   'com.google.android.gms:play-services-tflite-support:16.0.1'
   'com.google.android.gms:play-services-tflite-gpu:16.1.0'
- Google Play Services version
 23.16.13(190408-527363516)


I followed the tutorial from 

https://www.tensorflow.org/lite/android/play_services#java_6.

Everything seemed fine on almost all devices. However, our test team recently attempted to test our app on the Samsung S21 and discovered that the AI inference results are noisy when there are animations on the app's GUI. This issue only occurs in the PlayService/GPU scenario. If I switch to standalone mode, everything works well. We also found that setting ""android:hardwareAccelerated"" to false in the AndroidManifest.xml can produce stable results, but it's obviously not a reasonable solution. Has anyone else experienced a similar issue? I would appreciate any advice or suggestions."
60703,Error when converting model to CoreML,"# Issue
I have model that is trained in tensorflow 2.x.  The model works perfectly with tensorflow, openvino and onnxruntime format but doesn;t get converted in coreml. The model inference is perfect in tensorflow but when I try to convert it into coreml format I get the following error.

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
File ~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/tensorflow/python/framework/importer.py:496, in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)
    495 try:
--> 496   results = c_api.TF_GraphImportGraphDefWithResults(
    497       graph._c_graph, serialized, options)  # pylint: disable=protected-access
    498   results = c_api_util.ScopedTFImportGraphDefResults(results)

InvalidArgumentError: Input 0 of node Model1/FPN/FPN1/bn/AssignNewValue was passed float from Model1/FPN/FPN1/bn/FusedBatchNormV3/ReadVariableOp/resource:0 incompatible with expected resource.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
Cell In[15], line 13
      6 width = 256
      8 input_shape = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),
      9                     ct.RangeDim(lower_bound=height, upper_bound=1024),
     10                     ct.RangeDim(lower_bound=width, upper_bound=1024),
     11                     3))
---> 13 c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], source='tensorflow')
```

# Source Code

Here is the source code for loading and converting the model in coreml format

```import tensorflow as tf
Import coremltools as ct

model_pth = ""./temp_with_weights_model.h5""
model = tf.keras.models.load_model(model_pth)


print(ct.__version__)

input_name = model.inputs[0].name

height = 256
width = 256

input_shape = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),
                    ct.RangeDim(lower_bound=height, upper_bound=1024),
                    ct.RangeDim(lower_bound=width, upper_bound=1024),
                    3))

c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], source='tensorflow')
```

"
60702,same Error as #35100 --W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated. 	 [[{{node PyFunc}}]],"I am reproducing the [code](https://github.com/weizhepei/CasRel) , I followed there requirement but when code reached to 44/100 epoch I got this error : `2023-05-24 06:42:29.554799: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated. 	 [[{{node PyFunc}}]]`. I tried to to update tensorflow but it stop working because of keras version. 
Code requirements: 
`Package                          Version
-------------------------------- ---------
absl-py                          1.4.0
astor                            0.8.1
certifi                          2022.12.7
charset-normalizer               3.1.0
colorama                         0.4.6
filelock                         3.12.0
fsspec                           2023.1.0
gast                             0.5.3
google-pasta                     0.2.0
grpcio                           1.51.3
h5py                             3.8.0
huggingface-hub                  0.14.1
idna                             3.4
importlib-metadata               6.0.0
joblib                           1.2.0
Keras                            2.2.4
Keras-Applications               1.0.8
keras-bert                       0.80.0
keras-embed-sim                  0.10.0
keras-layer-normalization        0.16.0
keras-multi-head                 0.29.0
keras-pos-embd                   0.13.0
keras-position-wise-feed-forward 0.8.0
Keras-Preprocessing              1.1.2
keras-self-attention             0.51.0
keras-transformer                0.33.0
Markdown                         3.4.1
MarkupSafe                       2.1.2
mock                             5.0.1
numpy                            1.21.6
packaging                        23.1
pip                              22.3.1
protobuf                         3.20.1
PyYAML                           6.0
regex                            2023.5.5
requests                         2.31.0
scikit-learn                     1.0.2
scipy                            1.7.3
setuptools                       65.6.3
six                              1.16.0
sklearn                          0.0.post5
tensorboard                      1.13.1
tensorflow-estimator             1.13.0
tensorflow-gpu                   1.13.1
termcolor                        2.2.0
threadpoolctl                    3.1.0
tokenizers                       0.13.3
torch                            1.13.1
tqdm                             4.65.0
transformers                     4.29.2
typing_extensions                4.5.0
urllib3                          2.0.2
Werkzeug                         2.2.3
wheel                            0.38.4
wincertstore                     0.2
wrapt                            1.15.0
zipp                             3.15.0
`

and run.py code is: 
`#! -*- coding:utf-8 -*-
from data_loader import data_generator, load_data
from model import E2EModel, Evaluate
from utils import extract_items, get_tokenizer, metric
import os, argparse
os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""
from keras import backend as K
if(K.backend() == 'tensorflow'):
    import tensorflow as tf
    from keras.backend.tensorflow_backend import set_session
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    sess = tf.Session(config=config)


    #tried these lines too but not useful
    '''config = tf.compat.v1.ConfigProto(gpu_options=tf.compat.v1.GPUOptions(allow_growth=True))
    sess = tf.compat.v1.Session(config=config)'''

parser = argparse.ArgumentParser(description='Model Controller')
parser.add_argument('--train', default=True, type=bool, help='to train the HBT model, python run.py --train=True')
parser.add_argument('--dataset', default='VKG', type=str, help='specify the dataset from [""NYT"",""WebNLG"",""ACE04"",""NYT10-HRL"",""NYT11-HRL"",""Wiki-KBP""]')
args = parser.parse_args()


if __name__ == '__main__':
    # pre-trained bert model config
    bert_model = 'cased_L-12_H-768_A-12'
    bert_config_path = 'pretrained_bert_models/' + bert_model + '/bert_config.json'
    bert_vocab_path = 'pretrained_bert_models/' + bert_model + '/vocab.txt'
    bert_checkpoint_path = 'pretrained_bert_models/' + bert_model + '/bert_model.ckpt'

    dataset = args.dataset
    train_path = 'data/' + dataset + '/train_triples.json'
    dev_path = 'data/' + dataset + '/dev_triples.json'
    #test_path = 'data/' + dataset + '/test_split_by_num/test_triples_5.json' # ['1','2','3','4','5']
    #test_path = 'data/' + dataset + '/test_split_by_type/test_triples_seo.json' # ['normal', 'seo', 'epo']
    test_path = 'data/' + dataset + '/test_triples.json' # overall test
    rel_dict_path = 'data/' + dataset + '/rel2id.json'
    save_weights_path = 'saved_weights/' + dataset + '/best_model.weights'
    
    LR = 1e-5
    tokenizer = get_tokenizer(bert_vocab_path)
    train_data, dev_data, test_data, id2rel, rel2id, num_rels = load_data(train_path, dev_path, test_path, rel_dict_path)
    subject_model, object_model, hbt_model = E2EModel(bert_config_path, bert_checkpoint_path, LR, num_rels)
    
    if args.train:
        BATCH_SIZE = 6
        EPOCH = 100
        MAX_LEN = 100
        STEPS = len(train_data) // BATCH_SIZE
        data_manager = data_generator(train_data, tokenizer, rel2id, num_rels, MAX_LEN, BATCH_SIZE)
        evaluator = Evaluate(subject_model, object_model, tokenizer, id2rel, dev_data, save_weights_path)
        hbt_model.fit_generator(data_manager.__iter__(),
                              steps_per_epoch=STEPS,
                              epochs=EPOCH,
                              callbacks=[evaluator]
                              )
    else:
        hbt_model.load_weights(save_weights_path)
        test_result_path = 'results/' + dataset + '/test_result.json'
        isExactMatch = True if dataset == 'Wiki-KBP' else False
        if isExactMatch:
            print(""Exact Match"")
        else:
            print(""Partial Match"")
        precision, recall, f1_score = metric(subject_model, object_model, test_data, id2rel, tokenizer, isExactMatch, test_result_path)
        print(f'{precision}\t{recall}\t{f1_score}')

`
#35100 "
60695,Incorrect gradient after divide operation when result contains inf,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf-nightly

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The gradient of `tf.experimental.numpy.divide` or simply `/` operator is incorrect when the division result contains `inf`. See example below, the gradient of `result[0]` with respect to `x1` should be `1/2` instead of `nan`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
x1 = tf.constant([3], dtype=tf.float32)
x2 = tf.constant([2, 3, 0, 1], dtype=tf.float32)
with tf.GradientTape() as tape:
  tape.watch(x1)
  tape.watch(x2)
  result = tf.experimental.numpy.divide(x1, x2)    # or result = x1 / x2
actual_grad = tape.jacobian(result, x1)
expected_grad = tf.constant([[1/2], [1/3], [np.inf], [1/1]])
print(actual_grad)
print(expected_grad)
actual_grad == expected_grad


See gist: https://colab.research.google.com/drive/1xbzvQ99nrEehhBabpgCSEiBMSh4qXojR?usp=sharing
```


### Relevant log output

```shell
tf.Tensor(
[[nan]
 [nan]
 [inf]
 [nan]], shape=(4, 1), dtype=float32)
tf.Tensor(
[[0.5       ]
 [0.33333334]
 [       inf]
 [1.        ]], shape=(4, 1), dtype=float32)
<tf.Tensor: shape=(4, 1), dtype=bool, numpy=
array([[False],
       [False],
       [ True],
       [False]])>
```
</details>"
60691,Support for CUDA 12.0 and 12.1,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.6.4

### Custom Code

Yes

### OS Platform and Distribution

Red Hat Enterprise Linux 8

### Mobile device

_No response_

### Python version

3.9.14

### Bazel version

3.7.2

### GCC/Compiler version

11.2.1

### CUDA/cuDNN version

12.0.1 / 8.8.0

### GPU model and memory

NVIDIA V100, T4, A100

### Current Behaviour?

According to the documentation, TensorFlow currently supports CUDA 11.8 .

Are there plans and maybe a timeline for adding support for CUDA 12.0 or 12.1 ?

Many thanks,
.Andrea

### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_</details>"
60690,raise core._status_to_exception(e) from None  # pylint: disable=protected-access tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__IteratorGetNext_output_types_16_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input is empty. 	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]] 	 [[MultiDeviceIteratorGetNextFromShard]] 	 [[RemoteCall]] [Op:IteratorGetNext] ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

google colab

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I was training tf2 model faster_rcnn_resnet50_v1_640x640_coco17_tpu-8 with custom dataset exported from cvat directly as .tfrecord. The data consists of jpeg and png files. Wanted to know the error is about.

### Standalone code to reproduce the issue

```shell
!python object_detection/model_main_tf2.py --model_dir=/content/Faster_RCNN/models/rcnn --pipeline_config_path=/content/Faster_RCNN/models/rcnn/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8_colab.config
```


### Relevant log output

```shell
2023-05-24 13:15:30.719812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 13:15:32.136087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.
W0524 13:15:35.543571 140193984800576 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)
I0524 13:15:35.573029 140193984800576 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)
INFO:tensorflow:Maybe overwriting train_steps: None
I0524 13:15:35.576818 140193984800576 config_util.py:552] Maybe overwriting train_steps: None
INFO:tensorflow:Maybe overwriting use_bfloat16: False
I0524 13:15:35.577034 140193984800576 config_util.py:552] Maybe overwriting use_bfloat16: False
WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
W0524 13:15:35.838855 140193984800576 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
INFO:tensorflow:Reading unweighted datasets: ['/content/Faster_RCNN/data/train1.tfrecord']
I0524 13:15:35.849479 140193984800576 dataset_builder.py:162] Reading unweighted datasets: ['/content/Faster_RCNN/data/train1.tfrecord']
INFO:tensorflow:Reading record datasets for input file: ['/content/Faster_RCNN/data/train1.tfrecord']
I0524 13:15:35.849788 140193984800576 dataset_builder.py:79] Reading record datasets for input file: ['/content/Faster_RCNN/data/train1.tfrecord']
INFO:tensorflow:Number of filenames to read: 1
I0524 13:15:35.849880 140193984800576 dataset_builder.py:80] Number of filenames to read: 1
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W0524 13:15:35.849966 140193984800576 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.
W0524 13:15:35.860586 140193984800576 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.
WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
W0524 13:15:35.890340 140193984800576 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
2023-05-24 13:15:37.925062: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'cond/SparseToDense/ParseSingleExample/ParseExample/ParseExampleV2_1' with dtype int64 and shape [1]
	 [[{{node cond/SparseToDense/ParseSingleExample/ParseExample/ParseExampleV2_1}}]]
2023-05-24 13:15:37.925272: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'cond/SparseToDense/ParseSingleExample/ParseExample/ParseExampleV2_1' with dtype int64 and shape [1]
	 [[{{node cond/SparseToDense/ParseSingleExample/ParseExample/ParseExampleV2_1}}]]
2023-05-24 13:15:37.947883: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'cond_1/SparseToDense/ParseSingleExample/ParseExample/ParseExampleV2_1' with dtype int64 and shape [1]
	 [[{{node cond_1/SparseToDense/ParseSingleExample/ParseExample/ParseExampleV2_1}}]]
2023-05-24 13:15:37.948050: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'cond_1/SparseToDense/ParseSingleExample/ParseExample/ParseExampleV2_1' with dtype int64 and shape [1]
	 [[{{node cond_1/SparseToDense/ParseSingleExample/ParseExample/ParseExampleV2_1}}]]
WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
W0524 13:15:46.278530 140193984800576 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0524 13:15:51.640419 140193984800576 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/optimizer_builder.py:124: The name tf.keras.optimizers.SGD is deprecated. Please use tf.keras.optimizers.legacy.SGD instead.

W0524 13:15:55.215861 140193984800576 module_wrapper.py:149] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/optimizer_builder.py:124: The name tf.keras.optimizers.SGD is deprecated. Please use tf.keras.optimizers.legacy.SGD instead.

2023-05-24 13:15:55.277454: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_28' with dtype resource
	 [[{{node Placeholder/_28}}]]
2023-05-24 13:15:55.278269: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_26' with dtype resource
	 [[{{node Placeholder/_26}}]]
2023-05-24 13:15:56.001176: W tensorflow/core/framework/dataset.cc:807] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
2023-05-24 13:15:56.001868: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype variant
	 [[{{node Placeholder/_0}}]]
2023-05-24 13:15:56.090299: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.090433: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.091018: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.091127: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.091407: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
2023-05-24 13:15:56.091526: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]]
2023-05-24 13:15:56.091738: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.092306: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.092737: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.093123: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.093491: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.093866: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.094237: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
Traceback (most recent call last):
  File ""/content/models/research/object_detection/model_main_tf2.py"", line 114, in <module>
2023-05-24 13:15:56.095441: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
    tf.compat.v1.app.run()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/platform/app.py"", line 36, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.10/dist-packages/absl/app.py"", line 308, in run
2023-05-24 13:15:56.095675: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
2023-05-24 13:15:56.095752: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]]
    _run_main(main, args)
  File ""/usr/local/lib/python3.10/dist-packages/absl/app.py"", line 254, in _run_main
    sys.exit(main(argv))
  File ""/content/models/research/object_detection/model_main_tf2.py"", line 105, in main
    model_lib_v2.train_loop(
  File ""/usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py"", line 605, in train_loop
2023-05-24 13:15:56.096326: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
    load_fine_tune_checkpoint(
  File ""/usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py"", line 401, in load_fine_tune_checkpoint
    _ensure_model_is_built(model, input_dataset, unpad_groundtruth_tensors)
  File ""/usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py"", line 161, in _ensure_model_is_built
    features, labels = iter(input_dataset).next()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py"", line 570, in next
2023-05-24 13:15:56.096794: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
    return self.__next__()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py"", line 574, in __next__
2023-05-24 13:15:56.097219: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
    return self.get_next()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py"", line 631, in get_next
    return self._get_next_no_partial_batch_handling(name)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py"", line 663, in _get_next_no_partial_batch_handling
    replicas.extend(self._iterators[i].get_next_as_list(new_name))
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1633, in get_next_as_list
2023-05-24 13:15:56.097823: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
    return self._format_data_list_with_options(self._iterator.get_next())
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py"", line 554, in get_next
2023-05-24 13:15:56.098329: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
    result.append(self._device_iterators[i].get_next())
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 850, in get_next
2023-05-24 13:15:56.098798: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
    return self._next_internal()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 780, in _next_internal
2023-05-24 13:15:56.099547: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
    ret = gen_dataset_ops.iterator_get_next(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3016, in iterator_get_next
2023-05-24 13:15:56.099617: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.099989: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.100782: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
    _ops.raise_from_not_ok_status(e, name)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py"", line 7262, in raise_from_not_ok_status
2023-05-24 13:15:56.101000: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__IteratorGetNext_output_types_16_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]] [Op:IteratorGetNext]
2023-05-24 13:15:56.103550: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.103816: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.104496: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.104658: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.105043: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.105429: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.105806: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.106484: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.106894: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.107015: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.107396: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.107756: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.108109: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.108526: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.108955: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.109404: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.109817: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.110237: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.110613: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.110994: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.111395: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.111747: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.112100: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.112497: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.112846: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.113208: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.113573: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.113922: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.114299: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.114678: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.115032: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.115403: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.115763: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.116113: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.116518: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.116873: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.117547: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.117940: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.118389: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.118793: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.119174: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.119549: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.119900: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.120278: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.120634: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.120982: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.121383: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.121736: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.122088: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.203328: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.203989: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.205749: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.206195: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.206597: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.206974: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.207053: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.207429: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.207779: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.208119: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.208490: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.208836: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.209531: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.209609: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.209959: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.210641: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.210715: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.211039: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.211716: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
2023-05-24 13:15:56.211832: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Input is empty.
	 [[{{function_node case_cond_cond_jpeg_false_219}}{{node case/cond/cond_jpeg/decode_image/DecodeImage}}]]
```
</details>"
60689,undefined reference to `cudaGraphDebugDotPrint' when compiling TensorFlow 2.12,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

11.5/8.3

### GPU model and memory

NVidia RTX A5000

### Current Behaviour?

The error comes during linking. Please see the log output.


### Standalone code to reproduce the issue

```shell
I just use the standard bazel build command on TensorFlow website to compile.
```


### Relevant log output

```shell
ERROR: /tmp/tensorflow/tensorflow/BUILD:1219:21: Linking tensorflow/libtensorflow_cc.so.2.12.0 failed: (E
xit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /home/ujjwal/.cache/bazel/_bazel_ujjwal/e5cce820cc082410b4fcc604db349066/execroot/org_tensorflow &&
 \
  exec env - \
    CUDA_TOOLKIT_PATH=/home/ujjwal/softwares/cuda-11.5 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 \
    LD_LIBRARY_PATH=:/home/ujjwal/softwares/cuda-11.5/lib64:/home/ujjwal/softwares/nccl_2.11.4-1+cuda11.5
_x86_64/lib:/home/ujjwal/softwares/TensorRT-8.2.1.8/lib:/home/ujjwal/softwares/zlib/lib:/home/ujjwal/soft
wares/gettext/lib:/home/ujjwal/softwares/curl/lib:/home/ujjwal/softwares/openssl/lib64:/home/ujjwal/softwares/cudnn-8.3.1.22/lib:/home/ujjwal/softwares/cuda-11.5/extras/CUPTI/lib64 \
    PATH=/home/ujjwal/.cargo/bin:/home/ujjwal/anaconda3/envs/tf2.12/bin:/home/ujjwal/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ujjwal/softwares/cuda-11.5/bin:/home/ujjwal/softwares/bazel-5.3.0/bin:/home/ujjwal/softwares/TensorRT-8.2.1.8/bin:/home/ujjwal/softwares/gettext/bin:/home/ujjwal/softwares/git/bin:/home/ujjwal/softwares/curl/bin:/home/ujjwal/softwares/openssl/bin:/home/ujjwal/softwares/neovim \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/ujjwal/anaconda3/envs/tf2.12/bin/python \
    PYTHON_LIB_PATH=/home/ujjwal/anaconda3/envs/tf2.12/lib/python3.9/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=8.6,8.6,8.6,8.6,8.6,8.6,8.6,8.6 \
    TF_CUDA_PATHS=/home/ujjwal/softwares/nccl_2.11.4-1+cuda11.5_x86_64,/home/ujjwal/softwares/cudnn-8.3.1.22,/home/ujjwal/softwares/cuda-11.5,/home/ujjwal/softwares/TensorRT-8.2.1.8 \
    TF_CUDA_VERSION=11.5 \
    TF_CUDNN_VERSION=8.3 \
    TF_NCCL_VERSION=2.11.4 \
    TF_TENSORRT_VERSION=8.2 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so.2.12.0-2.params)
# Configuration: c6f4dad0752984ef77f185453bec6416ee82f657120171617f8b93e228363e95
# Execution platform: @local_execution_config_platform//:platform
/usr/bin/ld: bazel-out/k8-opt/bin/tensorflow/compiler/xla/stream_executor/cuda/libcuda_graph.pic.a(cuda_graph.pic.o): in function `stream_executor::gpu::CaptureCudaGraph(stream_executor::Stream*, absl::lts_20220623::AnyInvocable<tsl::Status ()>, cudaStreamCaptureMode)':
cuda_graph.cc:(.text._ZN15stream_executor3gpu16CaptureCudaGraphEPNS_6StreamEN4absl12lts_2022062312AnyInvocableIFN3tsl6StatusEvEEE21cudaStreamCaptureMode+0x520): undefined reference to `cudaGraphDebugDotPrint'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1323.055s, Critical Path: 476.12s
INFO: 24057 processes: 158 internal, 23899 local.
FAILED: Build did NOT complete successfully
```
</details>"
60687,typing_extensions >= 4.6.0 causes pip unit test failure,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//bazel_pip/tensorflow/python/trackable:data_structures_test will fail with typing_extensions >= 4.6.0 installed when run as a pip test against an installed TensorFlow wheel.

### Standalone code to reproduce the issue

```shell
bazel test --build_tests_only --cache_test_results=no --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --jobs=75 --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=no_tensorflow_py_deps=true --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true //bazel_pip/tensorflow/python/trackable:data_structures_test
```


### Relevant log output

```shell
======================================================================
ERROR: testFunctionCaching (__main__.MappingTests)
MappingTests.testFunctionCaching
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/python/trackable/data_structures_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/trackable/data_structures_test.py"", line 507, in testFunctionCaching
    second_trace = f.get_concrete_function(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1198, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1189, in _get_concrete_function_garbage_collected
    concrete = self._variable_creation_fn.get_concrete_function(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 197, in get_concrete_function
    concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 172, in _maybe_define_concrete_function
    return self._maybe_define_function(args, kwargs)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 294, in _maybe_define_function
    function_type_utils.make_canonicalized_monomorphic_type(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py"", line 378, in make_canonicalized_monomorphic_type
    function_type_lib.canonicalize_to_monomorphic(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/core/function/polymorphism/function_type.py"", line 481, in canonicalize_to_monomorphic
    _make_validated_mono_param(name, arg, poly_parameter.kind,
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/core/function/polymorphism/function_type.py"", line 421, in _make_validated_mono_param
    mono_type = trace_type.from_value(value, type_context)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py"", line 142, in from_value
    elif isinstance(value, trace.SupportsTracingProtocol):
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/typing_extensions.py"", line 605, in __instancecheck__
    val = inspect.getattr_static(instance, attr)
  File ""/usr/lib/python3.8/inspect.py"", line 1596, in getattr_static
    instance_result = _check_instance(obj, attr)
  File ""/usr/lib/python3.8/inspect.py"", line 1543, in _check_instance
    instance_dict = object.__getattribute__(obj, ""__dict__"")
TypeError: this __dict__ descriptor does not support '_DictWrapper' objects

======================================================================
ERROR: testFunctionCaching (__main__.TupleTests)
TupleTests.testFunctionCaching
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/python/trackable/data_structures_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/trackable/data_structures_test.py"", line 716, in testFunctionCaching
    second_trace = f.get_concrete_function(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1198, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1189, in _get_concrete_function_garbage_collected
    concrete = self._variable_creation_fn.get_concrete_function(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 197, in get_concrete_function
    concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 172, in _maybe_define_concrete_function
    return self._maybe_define_function(args, kwargs)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 294, in _maybe_define_function
    function_type_utils.make_canonicalized_monomorphic_type(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py"", line 378, in make_canonicalized_monomorphic_type
    function_type_lib.canonicalize_to_monomorphic(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/core/function/polymorphism/function_type.py"", line 481, in canonicalize_to_monomorphic
    _make_validated_mono_param(name, arg, poly_parameter.kind,
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/core/function/polymorphism/function_type.py"", line 421, in _make_validated_mono_param
    mono_type = trace_type.from_value(value, type_context)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py"", line 142, in from_value
    elif isinstance(value, trace.SupportsTracingProtocol):
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/typing_extensions.py"", line 605, in __instancecheck__
    val = inspect.getattr_static(instance, attr)
  File ""/usr/lib/python3.8/inspect.py"", line 1596, in getattr_static
    instance_result = _check_instance(obj, attr)
  File ""/usr/lib/python3.8/inspect.py"", line 1543, in _check_instance
    instance_dict = object.__getattribute__(obj, ""__dict__"")
TypeError: this __dict__ descriptor does not support '_TupleWrapper' objects

----------------------------------------------------------------------
Ran 74 tests in 1.021s

FAILED (errors=2, skipped=4)
================================================================================
```
</details>"
60686,Why does toco convert tf.squeeze to reshape operator?,"### 1. System information

- latests tensorflow
- mac os (m1)

### 2. Code

This may simply be my curiosity and a question for your support. Looking at the kernel-side codes of tflite, it was found that there is a kernel implementation code for the squeeze operator(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/squeeze.cc
).
However, when converting tensorflow to tflite using toco, squeeze is converted to reshape by the function below.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/graph_transformations/convert_squeeze_to_reshape.cc

I have a question about this.

1. Are you doing this conversion for some reason, like performance?
2. If not, will this conversion logic be removed from toco so that it can be done later with the squeeze tflite kernel?

Thank you.

BR, youngchan
"
60685,Tensorflow model training never started on dual 4090 GPUs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

8.9.1.23

### GPU model and memory

dual rtx 4090

### Current Behaviour?

I just plugged my 2nd gpu (a second rtx 4090) into my machine, and when i tried to run the pretraining file for bert (from https://github.com/tensorflow/models/tree/master/official/legacy/bert) and used the flag --num_gpus=2 the training never gets started. with nvidia-smi i see that both gpus are 100% utilized (see below)
nvidia-smi
Wed May 24 01:41:46 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA Graphics...  On   | 00000000:01:00.0 Off |                  Off |
| 30%   41C    P2   100W / 480W |  23110MiB / 24564MiB |    100%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA Graphics...  On   | 00000000:03:00.0 Off |                  Off |
| 30%   32C    P2   114W / 480W |  23110MiB / 24564MiB |    100%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1681      G   /usr/lib/xorg/Xorg                  4MiB |
|    0   N/A  N/A      4311      C   ...conda3/envs/p1/bin/python    23102MiB |
|    1   N/A  N/A      1681      G   /usr/lib/xorg/Xorg                  4MiB |
|    1   N/A  N/A      4311      C   ...conda3/envs/p1/bin/python    23102MiB |

I thought it's something related to a bug in bert code, so i run the test code in https://keras.io/guides/distributed_training/ and still the same issue

### Standalone code to reproduce the issue

```shell
%%time
#    --helpfull \
#pretraining using the modified code (custom encoder)
!python run_pretraining.py \
    --input_files=tfdataset.tfrecord \
    --model_export_path=model_t/ \
    --model_dir=test/ \
    --train_batch_size=256 \
    --num_steps_per_epoch=100 \
    --warmup_steps=10000 \
    --max_seq_length=32 \
    --max_predictions_per_seq=20 \
    --num_train_epochs=5 \
    --learning_rate=5e-5 \
    --num_gpus=2 \
    --dtype=""fp16"" \
    --distribution_strategy='mirrored' \
    --bert_config_file=config.json

also the example from keras distributed training

def get_compiled_model():
    # Make a simple 2-layer densely-connected neural network.
    inputs = keras.Input(shape=(784,))
    x = keras.layers.Dense(256, activation=""relu"")(inputs)
    x = keras.layers.Dense(256, activation=""relu"")(x)
    outputs = keras.layers.Dense(10)(x)
    model = keras.Model(inputs, outputs)
    model.compile(
        optimizer=keras.optimizers.Adam(),
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[keras.metrics.SparseCategoricalAccuracy()],
    )
    return model


def get_dataset():
    batch_size = 32
    num_val_samples = 10000

    # Return the MNIST dataset in the form of a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).
    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

    # Preprocess the data (these are Numpy arrays)
    x_train = x_train.reshape(-1, 784).astype(""float32"") / 255
    x_test = x_test.reshape(-1, 784).astype(""float32"") / 255
    y_train = y_train.astype(""float32"")
    y_test = y_test.astype(""float32"")

    # Reserve num_val_samples samples for validation
    x_val = x_train[-num_val_samples:]
    y_val = y_train[-num_val_samples:]
    x_train = x_train[:-num_val_samples]
    y_train = y_train[:-num_val_samples]
    return (
        tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),
        tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),
        tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),
    )


# Create a MirroredStrategy.
strategy = tf.distribute.MirroredStrategy()
print(""Number of devices: {}"".format(strategy.num_replicas_in_sync))

# Open a strategy scope.
with strategy.scope():
    # Everything that creates variables should be under the strategy scope.
    # In general this is only model construction & `compile()`.
    model = get_compiled_model()

# Train the model on all available devices.
train_dataset, val_dataset, test_dataset = get_dataset()
model.fit(train_dataset, epochs=2, validation_data=val_dataset)

# Test the model on all available devices.
model.evaluate(test_dataset)
```


### Relevant log output

```shell
2023-05-23 01:07:54.058867: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-23 01:07:54.159458: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-23 01:07:54.687914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/ffq/miniconda3/envs/p1/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
2023-05-23 01:07:55.597896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:55.598019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:55.651256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:55.651429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:55.651525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:55.651603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:55.766196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:55.766320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:55.766405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:55.766481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:55.766554: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:55.766626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:56.316533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:56.316656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:56.316746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:56.316823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:56.316899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:56.316971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22113 MB memory:  -> device: 0, name: NVIDIA Graphics Device, pci bus id: 0000:01:00.0, compute capability: 8.9
2023-05-23 01:07:56.326190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:56.326286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22113 MB memory:  -> device: 1, name: NVIDIA Graphics Device, pci bus id: 0000:03:00.0, compute capability: 8.9
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')
I0523 01:07:56.570175 140068132328512 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')
***** Number of cores used :  2
I0523 01:07:56.570987 140068132328512 run_pretraining.py:161] Training using customized training loop TF 2.0 with distributedstrategy.
2023-05-23 01:07:56.571174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-23 01:07:56.571318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
I0523 01:07:56.571392 140068132328512 device_compatibility_check.py:138] Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
WARNING:tensorflow:From /home/ffq/juptyer files/poem classification/run_pretraining.py:131: run_customized_training_loop (from official.legacy.bert.model_training_utils) is deprecated and will be removed in a future version.
Instructions for updating:
This function is deprecated and we do not expect adding new functionalities. Please do not have your code depending on this library.
W0523 01:07:56.571460 140068132328512 deprecation.py:364] From /home/ffq/juptyer files/poem classification/run_pretraining.py:131: run_customized_training_loop (from official.legacy.bert.model_training_utils) is deprecated and will be removed in a future version.
Instructions for updating:
This function is deprecated and we do not expect adding new functionalities. Please do not have your code depending on this library.
I0523 01:07:56.571495 140068132328512 model_training_utils.py:237] steps_per_loop not specified. Using steps_per_loop=1
2023-05-23 01:07:56.671667: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]
	 [[{{node Placeholder/_0}}]]
2023-05-23 01:07:56.671861: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]
	 [[{{node Placeholder/_0}}]]
WARNING:tensorflow:From /home/ffq/miniconda3/envs/p1/lib/python3.9/site-packages/official/nlp/modeling/models/bert_pretrainer.py:112: Classification.__init__ (from official.nlp.modeling.networks.classification) is deprecated and will be removed in a future version.
Instructions for updating:
Classification as a network is deprecated. Please use the layers.ClassificationHead instead.
W0523 01:07:58.041654 140068132328512 deprecation.py:364] From /home/ffq/miniconda3/envs/p1/lib/python3.9/site-packages/official/nlp/modeling/models/bert_pretrainer.py:112: Classification.__init__ (from official.nlp.modeling.networks.classification) is deprecated and will be removed in a future version.
Instructions for updating:
Classification as a network is deprecated. Please use the layers.ClassificationHead instead.
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0523 01:07:58.541763 140068132328512 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0523 01:07:58.543608 140068132328512 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0523 01:07:58.544566 140068132328512 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0523 01:07:58.553987 140068132328512 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0523 01:07:58.555135 140068132328512 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0523 01:07:58.565851 140068132328512 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0523 01:07:58.567800 140068132328512 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0523 01:07:58.576735 140068132328512 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0523 01:07:58.577869 140068132328512 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0523 01:07:58.582341 140068132328512 optimization.py:90] using Adamw optimizer
I0523 01:07:58.582492 140068132328512 legacy_adamw.py:56] AdamWeightDecay gradient_clip_norm=1.000000
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
I0523 01:07:58.623027 140068132328512 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:batch_all_reduce: 206 all-reduces with algorithm = nccl, num_packs = 1
I0523 01:08:00.538303 140068132328512 cross_device_ops.py:897] batch_all_reduce: 206 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:batch_all_reduce: 206 all-reduces with algorithm = nccl, num_packs = 1
I0523 01:08:04.206143 140068132328512 cross_device_ops.py:897] batch_all_reduce: 206 all-reduces with algorithm = nccl, num_packs = 1
^C
CPU times: user 518 ms, sys: 45.8 ms, total: 564 ms
Wall time: 1min 25s


below is the output from https://keras.io/guides/distributed_training/ example

2023-05-24 01:35:20.038482: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 01:35:20.058837: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 01:35:20.481311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 01:35:20.879778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:20.879894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:20.890701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:20.890833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:20.890915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:20.890989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:21.014091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:21.014198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:21.014277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:21.014345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:21.014412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:21.014479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:21.426457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:21.426574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:21.426666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:21.426744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:21.426817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:21.426889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22113 MB memory:  -> device: 0, name: NVIDIA Graphics Device, pci bus id: 0000:01:00.0, compute capability: 8.9
2023-05-24 01:35:21.427385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-05-24 01:35:21.427462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22113 MB memory:  -> device: 1, name: NVIDIA Graphics Device, pci bus id: 0000:03:00.0, compute capability: 8.9

INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')
Number of devices: 2
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11490434/11490434 [==============================] - 69s 6us/step
Epoch 1/2

2023-05-24 01:36:32.830263: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [50000,784]
	 [[{{node Placeholder/_0}}]]
2023-05-24 01:36:32.830377: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [50000]
	 [[{{node Placeholder/_1}}]]
2023-05-24 01:36:32.830894: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: ""TensorSliceDataset/_2""
op: ""TensorSliceDataset""
input: ""Placeholder/_0""
input: ""Placeholder/_1""
attr {
  key: ""Toutput_types""
  value {
    list {
      type: DT_FLOAT
      type: DT_FLOAT
    }
  }
}
attr {
  key: ""_cardinality""
  value {
    i: 50000
  }
}
attr {
  key: ""is_files""
  value {
    b: false
  }
}
attr {
  key: ""metadata""
  value {
    s: ""\n\024TensorSliceDataset:0""
  }
}
attr {
  key: ""output_shapes""
  value {
    list {
      shape {
        dim {
          size: 784
        }
      }
      shape {
      }
    }
  }
}
attr {
  key: ""replicate_on_split""
  value {
    b: false
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
    }
  }
}

2023-05-24 01:36:32.881532: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [50000]
	 [[{{node Placeholder/_1}}]]
2023-05-24 01:36:32.881664: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [50000]
	 [[{{node Placeholder/_1}}]]
2023-05-24 01:36:32.948588: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [50000]
	 [[{{node Placeholder/_1}}]]
2023-05-24 01:36:32.948723: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [50000]
	 [[{{node Placeholder/_1}}]]

INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

2023-05-24 01:36:33.795328: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.

```
</details>"
60684,Tensorflow Object Detection Project,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf1.x

### Custom Code

No

### OS Platform and Distribution

Macos Ventura

### Mobile device

Macbook air 2020 i3

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

So i am making an object detection projetc for school and i need help whenever i run this code this is the error that pops up. Please help it is due in a few days

I have installed all neccesary modules, i think

### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python
# coding: utf-8
""""""
Detect Objects Using Your Webcam
================================
""""""

# %%
# This demo will take you through the steps of running an ""out-of-the-box"" detection model to
# detect objects in the video stream extracted from your camera.

# %%
# Create the data directory
# ~~~~~~~~~~~~~~~~~~~~~~~~~
# The snippet shown below will create the ``data`` directory where all our data will be stored. The
# code will create a directory structure as shown bellow:
#
# .. code-block:: bash
#
#     data
#     └── models
#
# where the ``models`` folder will will contain the downloaded models.
import os
#os.chdir( '/Users/akulthota/Desktop/Object Detection' )

DATA_DIR = os.path.join(os.getcwd(), 'data')
MODELS_DIR = os.path.join(DATA_DIR, 'models')
for dir in [DATA_DIR, MODELS_DIR]:
    if not os.path.exists(dir):
        os.mkdir(dir)

# %%
# Download the model
# ~~~~~~~~~~~~~~~~~~
# The code snippet shown below is used to download the object detection model checkpoint file,
# as well as the labels file (.pbtxt) which contains a list of strings used to add the correct
# label to each detection (e.g. person).
#
# The particular detection algorithm we will use is the `SSD ResNet101 V1 FPN 640x640`. More
# models can be found in the `TensorFlow 2 Detection Model Zoo <https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md>`_.
# To use a different model you will need the URL name of the specific model. This can be done as
# follows:
#
# 1. Right click on the `Model name` of the model you would like to use;
# 2. Click on `Copy link address` to copy the download link of the model;
# 3. Paste the link in a text editor of your choice. You should observe a link similar to ``download.tensorflow.org/models/object_detection/tf2/YYYYYYYY/XXXXXXXXX.tar.gz``;
# 4. Copy the ``XXXXXXXXX`` part of the link and use it to replace the value of the ``MODEL_NAME`` variable in the code shown below;
# 5. Copy the ``YYYYYYYY`` part of the link and use it to replace the value of the ``MODEL_DATE`` variable in the code shown below.
#
# For example, the download link for the model used below is: ``download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz``
import tarfile
import urllib.request

# Download and extract model
MODEL_DATE = '20200711'
MODEL_NAME = 'ssd_resnet101_v1_fpn_640x640_coco17_tpu-8'
MODEL_TAR_FILENAME = MODEL_NAME + '.tar.gz'
MODELS_DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/tf2/'
MODEL_DOWNLOAD_LINK = MODELS_DOWNLOAD_BASE + MODEL_DATE + '/' + MODEL_TAR_FILENAME
PATH_TO_MODEL_TAR = os.path.join(MODELS_DIR, MODEL_TAR_FILENAME)
PATH_TO_CKPT = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'checkpoint/'))
PATH_TO_CFG = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'pipeline.config'))
if not os.path.exists(PATH_TO_CKPT):
    print('Downloading model. This may take a while... ', end='')
    urllib.request.urlretrieve(MODEL_DOWNLOAD_LINK, PATH_TO_MODEL_TAR)
    tar_file = tarfile.open(PATH_TO_MODEL_TAR)
    tar_file.extractall(MODELS_DIR)
    tar_file.close()
    os.remove(PATH_TO_MODEL_TAR)
    print('Done')

# Download labels file
LABEL_FILENAME = 'mscoco_label_map.pbtxt'
LABELS_DOWNLOAD_BASE = \
    'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'
PATH_TO_LABELS = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, LABEL_FILENAME))
if not os.path.exists(PATH_TO_LABELS):
    print('Downloading label file... ', end='')
    import ssl
    ssl._create_default_https_context = ssl._create_unverified_context
    urllib.request.urlretrieve(LABELS_DOWNLOAD_BASE + LABEL_FILENAME, PATH_TO_LABELS)
    print('Done')

# %%
# Load the model
# ~~~~~~~~~~~~~~
# Next we load the downloaded model

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging
import tensorflow as tf
from object_detection.utils import label_map_util

from object_detection.utils import visualization_utils as vis_util
from object_detection.utils import visualization_utils as viz_utils
from object_detection.builders import model_builder

tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)

# Enable GPU dynamic memory allocation
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

# Load pipeline config and build a detection model
configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)
model_config = configs['model']
detection_model = model_builder.build(model_config=model_config, is_training=False)

# Restore checkpoint
ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)
ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-0')).expect_partial()

@tf.function
def detect_fn(image):
    """"""Detect objects in image.""""""

    image, shapes = detection_model.preprocess(image)
    prediction_dict = detection_model.predict(image, shapes)
    detections = detection_model.postprocess(prediction_dict, shapes)

    return detections, prediction_dict, tf.reshape(shapes, [-1])


# %%
# Load label map data (for plotting)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Label maps correspond index numbers to category names, so that when our convolution network
# predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility
# functions, but anything that returns a dictionary mapping integers to appropriate string labels
# would be fine.
category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,
                                                                    use_display_name=True)

# %%
# Define the video stream
# ~~~~~~~~~~~~~~~~~~~~~~~
# We will use `OpenCV <https://pypi.org/project/opencv-python/>`_ to capture the video stream
# generated by our webcam. For more information you can refer to the `OpenCV-Python Tutorials <https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html#capture-video-from-camera>`_
import cv2

cap = cv2.VideoCapture(0)

# %%
# Putting everything together
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# The code shown below loads an image, runs it through the detection model and visualizes the
# detection results, including the keypoints.
#
# Note that this will take a long time (several minutes) the first time you run this code due to
# tf.function's trace-compilation --- on subsequent runs (e.g. on new images), things will be
# faster.
#
# Here are some simple things to try out if you are curious:
#
# * Modify some of the input images and see if detection still works. Some simple things to try out here (just uncomment the relevant portions of code) include flipping the image horizontally, or converting to grayscale (note that we still expect the input image to have 3 channels).
# * Print out `detections['detection_boxes']` and try to match the box locations to the boxes in the image.  Notice that coordinates are given in normalized form (i.e., in the interval [0, 1]).
# * Set ``min_score_thresh`` to other values (between 0 and 1) to allow more detections in or to filter out more detections.
import numpy as np

while True:
    # Read frame from camera
    ret, image_np = cap.read()

    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
    image_np_expanded = np.expand_dims(image_np, axis=0)

    # Things to try:
    # Flip horizontally
    # image_np = np.fliplr(image_np).copy()

    # Convert image to grayscale
    # image_np = np.tile(
    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)

    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)
    detections, predictions_dict, shapes = detect_fn(input_tensor)

    label_id_offset = 1
    image_np_with_detections = image_np.copy()

    viz_utils.visualize_boxes_and_labels_on_image_array(
          image_np_with_detections,
          detections['detection_boxes'][0].numpy(),
          (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),
          detections['detection_scores'][0].numpy(),
          category_index,
          use_normalized_coordinates=True,
          max_boxes_to_draw=200,
          min_score_thresh=.30,
          agnostic_mode=False)

    # Display output
    cv2.imshow('object detection', cv2.resize(image_np_with_detections, (800, 600)))

    if cv2.waitKey(25) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/Users/akulthota/Desktop/Object/object_detection_camera.py"", line 92, in <module>
    from object_detection.utils import label_map_util
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/object_detection/utils/label_map_util.py"", line 21, in <module>
    from object_detection.protos import string_int_label_map_pb2
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/object_detection/protos/string_int_label_map_pb2.py"", line 36, in <module>
    _descriptor.FieldDescriptor(
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/google/protobuf/descriptor.py"", line 561, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
```
</details>"
60683,Migrating T2T fork to TF2.7.4,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.7.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

My org is looking to migrate our Tensor2Tensor fork from TF1 to TF2.7.4 by the end of June in order to not lose TPU access in GCP. The current plan for our fork is to utilize the `tensorflow.compat.v1` APIs (along with updating `contrib` imports to use `tensorflow_addons` and `tf_slim`) and `tf.disable_v2_behavior()`. Is this all that's required to get t2t working with TF2?

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
60682,FailedPreconditionError: . is not a directory,"Good evening. I am trying to use Hyperband Tuner from keras_tuner, bet when I try to use it I get a FailedPreconditionError while creating the tuner.

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: no
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10* 64-bit
-   **TensorFlow installed from (source or binary)**: pip install tensorflow in command line
-   **TensorFlow version (use command below)**: 2.12.0
-   **Python version**: 3.11.3
-   **Exact command to reproduce**:

`import tensorflow as tf
import keras_tuner
print(tf.version.GIT_VERSION, tf.version.VERSION)

def hyperband_objective_autoencoder():
    return 1

hyperband_tuner = keras_tuner.Hyperband(
    hypermodel = hyperband_objective_autoencoder
)`

Output:

`v2.12.0-rc1-12-g0db597d0d75 2.12.0
---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
Cell In[1], line 8
      5 def hyperband_objective_autoencoder():
      6     return 1
----> 8 hyperband_tuner = keras_tuner.Hyperband(
      9     hypermodel = hyperband_objective_autoencoder
     10 )

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras_tuner\tuners\hyperband.py:418, in Hyperband.__init__(self, hypermodel, objective, max_epochs, factor, hyperband_iterations, seed, hyperparameters, tune_new_entries, allow_new_entries, max_retries_per_trial, max_consecutive_failed_trials, **kwargs)
    391 def __init__(
    392     self,
    393     hypermodel=None,
   (...)
    404     **kwargs
    405 ):
    406     oracle = HyperbandOracle(
    407         objective,
    408         max_epochs=max_epochs,
   (...)
    416         max_consecutive_failed_trials=max_consecutive_failed_trials,
    417     )
--> 418     super().__init__(oracle=oracle, hypermodel=hypermodel, **kwargs)

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras_tuner\engine\tuner.py:113, in Tuner.__init__(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite, executions_per_trial, **kwargs)
    105 if hypermodel is None and self.__class__.run_trial is Tuner.run_trial:
    106     raise ValueError(
    107         ""Received `hypermodel=None`. We only allow not specifying ""
    108         ""`hypermodel` if the user defines the search space in ""
    109         ""`Tuner.run_trial()` by subclassing a `Tuner` class without ""
    110         ""using a `HyperModel` instance.""
    111     )
--> 113 super().__init__(
    114     oracle=oracle,
    115     hypermodel=hypermodel,
    116     directory=directory,
    117     project_name=project_name,
    118     logger=logger,
    119     overwrite=overwrite,
    120     **kwargs,
    121 )
    123 self.max_model_size = max_model_size
    124 self.optimizer = optimizer

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras_tuner\engine\base_tuner.py:126, in BaseTuner.__init__(self, oracle, hypermodel, directory, project_name, overwrite, **kwargs)
    123 self.tuner_id = os.environ.get(""KERASTUNER_TUNER_ID"", ""tuner0"")
    125 # Reloading state.
--> 126 if not overwrite and tf.io.gfile.exists(self._get_tuner_fname()):
    127     tf.get_logger().info(
    128         f""Reloading Tuner from {self._get_tuner_fname()}""
    129     )
    130     self.reload()

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras_tuner\engine\base_tuner.py:473, in BaseTuner._get_tuner_fname(self)
    472 def _get_tuner_fname(self):
--> 473     return os.path.join(str(self.project_dir), f""{str(self.tuner_id)}.json"")

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras_tuner\engine\base_tuner.py:464, in BaseTuner.project_dir(self)
    461 @property
    462 def project_dir(self):
    463     dirname = os.path.join(str(self.directory), self.project_name)
--> 464     utils.create_directory(dirname)
    465     return dirname

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras_tuner\utils.py:46, in create_directory(path, remove_existing)
     43 def create_directory(path, remove_existing=False):
     44     # Create the directory if it doesn't exist.
     45     if not tf.io.gfile.exists(path):
---> 46         tf.io.gfile.makedirs(path)
     48     # If it does exist, and remove_existing is specified,
     49     # the directory will be removed and recreated.
     50     elif remove_existing:

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tensorflow\python\lib\io\file_io.py:513, in recursive_create_dir_v2(path)
    501 @tf_export(""io.gfile.makedirs"")
    502 def recursive_create_dir_v2(path):
    503   """"""Creates a directory and all parent/intermediate directories.
    504 
    505   It succeeds if path already exists and is writable.
   (...)
    511     errors.OpError: If the operation fails.
    512   """"""
--> 513   _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path))

FailedPreconditionError: . is not a directory
`

"
60679,api_compatibility_test fails on Python 3.11,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/tools/api/tests:api_compatibility_test fails on Python 3.11
See https://github.com/tensorflow/tensorflow/actions/runs/5053005537/jobs/9066419128#step:6:5566

### Standalone code to reproduce the issue

```shell
bazel test --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --repo_env=PYTHON_BIN_PATH=/home/ubuntu/actions-runner/_work/tensorflow/tensorflow/bazel-ci_build-cache/.venv/tf/bin/python --build_tag_filters=-no_oss,-oss_excluded,-oss_serial,-v1only,-benchmark-test,-no_aarch64,-gpu,-tpu,-no_oss_py38,-no_oss_py39,-no_oss_py310 --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-v1only,-benchmark-test,-no_aarch64,-gpu,-tpu,-no_oss_py38,-no_oss_py39,-no_oss_py310 --local_test_jobs=64 --build_tests_only -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/compiler/xrt/... -//tensorflow/core/tpu/... -//tensorflow/go/... -//tensorflow/java/... -//tensorflow/python/integration_testing/... -//tensorflow/tools/toolchains/... -//tensorflow/lite/... -//tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test -//tensorflow/python/kernel_tests/nn_ops:conv_ops_test -//tensorflow/compiler/mlir/tfr/examples/mnist:mnist_ops_test -//tensorflow/core/grappler/optimizers:auto_mixed_precision_test_cpu -//tensorflow/core/grappler/optimizers:remapper_test_cpu
```


### Relevant log output

```shell
Running tests under Python 3.11.3: /home/ubuntu/actions-runner/_work/tensorflow/tensorflow/bazel-ci_build-cache/.venv/tf/bin/python
[ RUN      ] ApiCompatibilityTest.testAPIBackwardsCompatibility
ERROR:tensorflow:TensorFlow API backwards compatibility test
This test ensures all changes to the public API of TensorFlow are intended.

If this test fails, it means a change has been made to the public API. Backwards
incompatible changes are not allowed. You can run the test as follows to update
test goldens and package them with your change.

    $ bazel run tensorflow/tools/api/tests:api_compatibility_test \
    #     -- --update_goldens True

You will need an API approval to make changes to the public TensorFlow API. This
includes additions to the API.

E0523 05:00:41.556244 281473269493776 api_compatibility_test.py:370] TensorFlow API backwards compatibility test
This test ensures all changes to the public API of TensorFlow are intended.

If this test fails, it means a change has been made to the public API. Backwards
incompatible changes are not allowed. You can run the test as follows to update
test goldens and package them with your change.

    $ bazel run tensorflow/tools/api/tests:api_compatibility_test \
    #     -- --update_goldens True

You will need an API approval to make changes to the public TensorFlow API. This
includes additions to the API.

ERROR:tensorflow:1 differences found between API and golden.
E0523 05:00:41.556445 281473269493776 api_compatibility_test.py:371] 1 differences found between API and golden.
ERROR:tensorflow:    Change detected in python object: tensorflow.train.
E0523 05:00:41.556506 281473269493776 api_compatibility_test.py:392]     Change detected in python object: tensorflow.train.
ERROR:tensorflow:    
  path: ""tensorflow.train""
  tf_module {
    member {
      name: ""BytesList""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""Checkpoint""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""CheckpointManager""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""CheckpointOptions""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""CheckpointView""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""ClusterDef""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""ClusterSpec""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""Coordinator""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""Example""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""ExponentialMovingAverage""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""Feature""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""FeatureList""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""FeatureLists""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""Features""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""FloatList""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""Int64List""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""JobDef""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""SequenceExample""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""ServerDef""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""TrackableView""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""experimental""
      mtype: ""<type \'module\'>""
    }
    member_method {
      name: ""checkpoints_iterator""
      argspec: ""args=[\'checkpoint_dir\', \'min_interval_secs\', \'timeout\', \'timeout_fn\'], varargs=None, keywords=None, defaults=[\'0\', \'None\', \'None\'], ""
    }
    member_method {
      name: ""get_checkpoint_state""
      argspec: ""args=[\'checkpoint_dir\', \'latest_filename\'], varargs=None, keywords=None, defaults=[\'None\'], ""
    }
    member_method {
      name: ""latest_checkpoint""
      argspec: ""args=[\'checkpoint_dir\', \'latest_filename\'], varargs=None, keywords=None, defaults=[\'None\'], ""
    }
    member_method {
      name: ""list_variables""
      argspec: ""args=[\'ckpt_dir_or_file\'], varargs=None, keywords=None, defaults=None""
    }
    member_method {
      name: ""load_checkpoint""
      argspec: ""args=[\'ckpt_dir_or_file\'], varargs=None, keywords=None, defaults=None""
    }
    member_method {
      name: ""load_variable""
      argspec: ""args=[\'ckpt_dir_or_file\', \'name\'], varargs=None, keywords=None, defaults=None""
    }
  }

E0523 05:00:41.556555 281473269493776 api_compatibility_test.py:393]     
  path: ""tensorflow.train""
  tf_module {
    member {
      name: ""BytesList""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""Checkpoint""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""CheckpointManager""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""CheckpointOptions""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""CheckpointView""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""ClusterDef""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""ClusterSpec""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""Coordinator""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""Example""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""ExponentialMovingAverage""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""Feature""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""FeatureList""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""FeatureLists""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""Features""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""FloatList""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""Int64List""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""JobDef""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""SequenceExample""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""ServerDef""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""TrackableView""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""experimental""
      mtype: ""<type \'module\'>""
    }
    member_method {
      name: ""checkpoints_iterator""
      argspec: ""args=[\'checkpoint_dir\', \'min_interval_secs\', \'timeout\', \'timeout_fn\'], varargs=None, keywords=None, defaults=[\'0\', \'None\', \'None\'], ""
    }
    member_method {
      name: ""get_checkpoint_state""
      argspec: ""args=[\'checkpoint_dir\', \'latest_filename\'], varargs=None, keywords=None, defaults=[\'None\'], ""
    }
    member_method {
      name: ""latest_checkpoint""
      argspec: ""args=[\'checkpoint_dir\', \'latest_filename\'], varargs=None, keywords=None, defaults=[\'None\'], ""
    }
    member_method {
      name: ""list_variables""
      argspec: ""args=[\'ckpt_dir_or_file\'], varargs=None, keywords=None, defaults=None""
    }
    member_method {
      name: ""load_checkpoint""
      argspec: ""args=[\'ckpt_dir_or_file\'], varargs=None, keywords=None, defaults=None""
    }
    member_method {
      name: ""load_variable""
      argspec: ""args=[\'ckpt_dir_or_file\', \'name\'], varargs=None, keywords=None, defaults=None""
    }
  }

[  FAILED  ] ApiCompatibilityTest.testAPIBackwardsCompatibility
INFO:tensorflow:time(__main__.ApiCompatibilityTest.testAPIBackwardsCompatibility): 2.5s
```
</details>"
60676,Infinity loop in batch_jacobian for ode,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.7.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When executing the computation of batch_jacobian on an ode with experimental_use_pfor=True, the code is stuck in an infinity loop. 
When experimental_use_pfor=False, the computation working like expected. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow_probability as tfp

@tf.function
def ode_fun(t,x, c):
    return x * c


def compute_grads(solver,experimental_use_pfor):
    solution_times = tf.constant([1.0])
    c = tf.Variable(tf.constant([[1.0],[2.0]]))
    with tf.GradientTape(watch_accessed_variables=False, persistent=not experimental_use_pfor) as tape:
        tape.watch(c)
        result = solver.solve(ode_fun, 0.0, tf.constant([[1.0],[2.0]]), solution_times=solution_times, constants={'c':c}).states[0]

    grads = tape.batch_jacobian(result,c,experimental_use_pfor=experimental_use_pfor,parallel_iterations=2)
    return  grads

def execute():
    print(tf.__version__)
    experimental_use_pfor = True
    solver = tfp.math.ode.DormandPrince()
    print(compute_grads(solver,experimental_use_pfor))

execute()
```


### Relevant log output

```shell
The computation never ends.
```
</details>"
60675,Infinite loop in batch_jacobian for ode,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.7.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When executing the computation of batch_jacobian on an ode with experimental_use_pfor=True, the code is stuck in an infinity loop. 
When experimental_use_pfor=False, the computation working like expected. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow_probability as tfp

@tf.function
def ode_fun(t,x, c):
    return x * c


def compute_grads(solver,experimental_use_pfor):
    solution_times = tf.constant([1.0])
    c = tf.Variable(tf.constant([[1.0],[2.0]]))
    with tf.GradientTape(watch_accessed_variables=False, persistent=not experimental_use_pfor) as tape:
        tape.watch(c)
        result = solver.solve(ode_fun, 0.0, tf.constant([[1.0],[2.0]]), solution_times=solution_times, constants={'c':c}).states[0]

    grads = tape.batch_jacobian(result,c,experimental_use_pfor=experimental_use_pfor,parallel_iterations=2)
    return  grads

def execute():
    print(tf.__version__)
    experimental_use_pfor = True
    solver = tfp.math.ode.DormandPrince()
    print(compute_grads(solver,experimental_use_pfor))

execute()
```


### Relevant log output

```shell
The computation never ends.
```
</details>"
60674,How to fix invalid argument error while training a deep learning model for text classification,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!

### Standalone code to reproduce the issue

```shell
Epoch 1/30
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Cell In[102], line 2
      1 # Entraîner le modèle sur l'ensemble d'entraînement
----> 2 model.fit(X_train, Y_train_categorical, batch_size=128, epochs=30, validation_split=0.1)
      4 # Évaluer les performances du modèle sur l'ensemble de test
      5 score = model.evaluate(X_test, Y_test_categorical, batch_size=12)

File ~\anaconda3\lib\site-packages\keras\utils\traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py:54, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52 try:
     53   ctx.ensure_initialized()
---> 54   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                       inputs, attrs, num_outputs)
     56 except core._NotOkStatusException as e:
     57   if name is not None:

InvalidArgumentError: Graph execution error:

Detected at node 'model_16/embedding_16/embedding_lookup' defined at (most recent call last):
    File ""C:\Users\achra\anaconda3\lib\runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""C:\Users\achra\anaconda3\lib\runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""C:\Users\achra\anaconda3\lib\site-packages\ipykernel_launcher.py"", line 17, in <module>
      app.launch_new_instance()
    File ""C:\Users\achra\anaconda3\lib\site-packages\traitlets\config\application.py"", line 992, in launch_instance
      app.start()
    File ""C:\Users\achra\anaconda3\lib\site-packages\ipykernel\kernelapp.py"", line 711, in start
      self.io_loop.start()
    File ""C:\Users\achra\anaconda3\lib\site-packages\tornado\platform\asyncio.py"", line 199, in start
      self.asyncio_loop.run_forever()
    File ""C:\Users\achra\anaconda3\lib\asyncio\base_events.py"", line 603, in run_forever
      self._run_once()
    File ""C:\Users\achra\anaconda3\lib\asyncio\base_events.py"", line 1906, in _run_once
      handle._run()
    File ""C:\Users\achra\anaconda3\lib\asyncio\events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""C:\Users\achra\anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 510, in dispatch_queue
      await self.process_one()
    File ""C:\Users\achra\anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 499, in process_one
      await dispatch(*args)
    File ""C:\Users\achra\anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 406, in dispatch_shell
      await result
    File ""C:\Users\achra\anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 729, in execute_request
      reply_content = await reply_content
    File ""C:\Users\achra\anaconda3\lib\site-packages\ipykernel\ipkernel.py"", line 411, in do_execute
      res = shell.run_cell(
    File ""C:\Users\achra\anaconda3\lib\site-packages\ipykernel\zmqshell.py"", line 531, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""C:\Users\achra\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2961, in run_cell
      result = self._run_cell(
    File ""C:\Users\achra\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3016, in _run_cell
      result = runner(coro)
    File ""C:\Users\achra\anaconda3\lib\site-packages\IPython\core\async_helpers.py"", line 129, in _pseudo_sync_runner
      coro.send(None)
    File ""C:\Users\achra\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3221, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""C:\Users\achra\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3400, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File ""C:\Users\achra\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3460, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""C:\Users\achra\AppData\Local\Temp\ipykernel_18948\1992395688.py"", line 2, in <module>
      model.fit(X_train, Y_train_categorical, batch_size=128, epochs=30, validation_split=0.1)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\engine\training.py"", line 1564, in fit
      tmp_logs = self.train_function(iterator)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\engine\training.py"", line 1160, in train_function
      return step_function(self, iterator)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\engine\training.py"", line 1146, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\engine\training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\engine\training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\engine\training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\engine\functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\engine\functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\engine\base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\utils\traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""C:\Users\achra\anaconda3\lib\site-packages\keras\layers\core\embedding.py"", line 208, in call
      out = tf.nn.embedding_lookup(self.embeddings, inputs)
Node: 'model_16/embedding_16/embedding_lookup'
indices[48,995,25] = -1 is not in [0, 2230)
	 [[{{node model_16/embedding_16/embedding_lookup}}]] [Op:__inference_train_function_112178]
```


### Relevant log output

_No response_</details>"
60673,AdamW optimizer crashes on Model.fit() for tensorflow-macos v2.14.0-dev20230518,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0-dev20230518

### Custom Code

Yes

### OS Platform and Distribution

MacOS 12.5.1 running on ARM architecture [M1 Pro chip]

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When calling Model.compile() with the AdamW optimizer, a warning is thrown saying that v2.11+ optimizers have a known slowdown on M1/M2 devices, and so the backend attempts to fallback to a legacy version. However, no legacy version of the AdamW optimizer exists. In a previous tf-macos version 2.12, this lead to an error during Model.compile() [see issue https://github.com/tensorflow/tensorflow/issues/60652]. In the current nightly, this error is not thrown - however, after calling model.compile(), the attribute model.optimizer is set to string 'adamw' instead of an optimizer object.

Later, when we call model.fit(), this leads to an AttributeError, because model.optimizer.minimize() does not exist when model.optimizer is a string.

Expected behaviour: correctly compile the model with either a v2.11+ optimiser without slowdown, or a legacy-compatible implementation of the AdamW optimizer. I could attempt to contribute this - but there may be a steep learning curve! Then the model will train correctly with a valid AdamW optimizer when calling model.fit().

Note: a warning message suggests using the optimizer located at `tf.keras.optimizers.legacy.AdamW`, but this does not exist

### Standalone code to reproduce the issue

```shell
##===========##
##  Imports  ##
##===========##

import sys

import tensorflow as tf

import numpy as np

from tensorflow.keras.models     import Model
from tensorflow.keras.layers     import Input, Dense
from tensorflow.keras.optimizers import AdamW

##===================##
##  Report versions  ##
##===================##
#
# Expected outputs:
# Python version is: 3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:01:19) [Clang 14.0.6 ]
# TF version is: 2.14.0-dev20230518
# Numpy version is: 1.23.2
#

print(f""Python version is: {sys.version}"")
print(f""TF version is: {tf.__version__}"")
print(f""Numpy version is: {np.__version__}"")

##==============================##
##  Create a very simple model  ##
##==============================##
#
# Expected outputs:
# Model: ""model_1""
# _________________________________________________________________
#  Layer (type)                Output Shape              Param #   
# =================================================================
#  Layer_in (InputLayer)       [(None, 2)]               0         
#                                                                 
#  Layer_hidden (Dense)        (None, 10)                30        
#                                                                 
#  Layer_out (Dense)           (None, 2)                 22        
#                                                                 
# =================================================================
# Total params: 52 (208.00 Byte)
# Trainable params: 52 (208.00 Byte)
# Non-trainable params: 0 (0.00 Byte)
# _________________________________________________________________
#

x_in  = Input(2 , dtype=tf.float32, name=""Layer_in""    )
x     = x_in
x     = Dense(10, dtype=tf.float32, name=""Layer_hidden"", activation=""relu""  )(x)
x     = Dense(2 , dtype=tf.float32, name=""Layer_out""   , activation=""linear"")(x)
model = Model(x_in, x)
model.summary()

##===================================================##
##  Compile model with MSE loss and AdamW optimizer  ##
##===================================================##
#
# Expected outputs:
# WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdamW` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdamW`.
# WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.AdamW`.
#

model.compile(
    loss      = ""mse"", 
    optimizer = AdamW(learning_rate=1e-3, weight_decay=1e-2)
)

##===========================##
##  Generate some fake data  ##
##===========================##
#
# Expected outputs:
# X shape is (100, 2), Y shape is (100, 2)
#

dataset_size = 100
X = np.random.normal(size=(dataset_size, 2))
X = tf.constant(X, dtype=tf.float32)
Y = np.random.normal(size=(dataset_size, 2))
Y = tf.constant(Y, dtype=tf.float32)

print(f""X shape is {X.shape}, Y shape is {Y.shape}"")

##===================================##
##  Fit model to data for one epoch  ##
##===================================##
#
# Expected outputs:
# ---------------------------------------------------------------------------
# AttributeError                            Traceback (most recent call last)
# Cell In[9], line 51
#       1 ##===================================##
#       2 ##  Fit model to data for one epoch  ##
#       3 ##===================================##
#    (...)
#      48 #       • mask=None
#      49 #
# ---> 51 model.fit(X, Y, epochs=1)

# File ~/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
#      67     filtered_tb = _process_traceback_frames(e.__traceback__)
#      68     # To get the full stack trace, call:
#      69     # `tf.debugging.disable_traceback_filtering()`
# ---> 70     raise e.with_traceback(filtered_tb) from None
#      71 finally:
#      72     del filtered_tb

# File /var/folders/6_/gprzxt797d5098h8dtk22nch0000gn/T/__autograph_generated_filezzqv9k36.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)
#      13 try:
#      14     do_return = True
# ---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
#      16 except:
#      17     do_return = False

# AttributeError: in user code:

#     File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1338, in train_function  *
#         return step_function(self, iterator)
#     File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1322, in step_function  **
#         outputs = model.distribute_strategy.run(run_step, args=(data,))
#     File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1303, in run_step  **
#         outputs = model.train_step(data)
#     File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1084, in train_step
#         self.optimizer.minimize(loss, self.trainable_variables, tape=tape)

#     AttributeError: 'str' object has no attribute 'minimize'

model.fit(X, Y, epochs=1)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[9], line 51
      1 ##===================================##
      2 ##  Fit model to data for one epoch  ##
      3 ##===================================##
   (...)
     48 #       • mask=None
     49 #
---> 51 model.fit(X, Y, epochs=1)

File ~/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File /var/folders/6_/gprzxt797d5098h8dtk22nch0000gn/T/__autograph_generated_filezzqv9k36.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)
     13 try:
     14     do_return = True
---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16 except:
     17     do_return = False

AttributeError: in user code:

    File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1338, in train_function  *
        return step_function(self, iterator)
    File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1322, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1303, in run_step  **
        outputs = model.train_step(data)
    File ""/Users/Ste/miniforge3/envs/tf_macos_nightly_230523/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1084, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)

    AttributeError: 'str' object has no attribute 'minimize'
```
</details>"
60671,//tensorflow/dtensor/python/tests:input_util_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

5.3.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

Sometimes test fails.

### Standalone code to reproduce the issue

```shell
docker exec tf bazel --bazelrc=/usertools/cpu.bazelrc test --config=rbe --config=pycpp --config=build_event_export
```


### Relevant log output

```shell
[ RUN      ] DTensorDatasetTest.testIterPrefetchEnabled
I0523 01:36:42.332243 140069302650688 mesh_util.py:35] This is client 0 of 1 clients
I0523 01:36:42.332387 140069302650688 mesh_util.py:36] Number of global CPU devices: 16
I0523 01:36:42.332595 140069302650688 mesh_util.py:39] Global device IDs: [[[ 0  1]
  [ 2  3]]

 [[ 4  5]
  [ 6  7]]

 [[ 8  9]
  [10 11]]

 [[12 13]
  [14 15]]]
I0523 01:36:42.333161 140069302650688 mesh_util.py:40] Local device IDs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
I0523 01:36:42.333399 140069302650688 mesh_util.py:41] Local devices: ['/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:1', '/job:localhost/replica:0/task:0/device:CPU:2', '/job:localhost/replica:0/task:0/device:CPU:3', '/job:localhost/replica:0/task:0/device:CPU:4', '/job:localhost/replica:0/task:0/device:CPU:5', '/job:localhost/replica:0/task:0/device:CPU:6', '/job:localhost/replica:0/task:0/device:CPU:7', '/job:localhost/replica:0/task:0/device:CPU:8', '/job:localhost/replica:0/task:0/device:CPU:9', '/job:localhost/replica:0/task:0/device:CPU:10', '/job:localhost/replica:0/task:0/device:CPU:11', '/job:localhost/replica:0/task:0/device:CPU:12', '/job:localhost/replica:0/task:0/device:CPU:13', '/job:localhost/replica:0/task:0/device:CPU:14', '/job:localhost/replica:0/task:0/device:CPU:15']
2023-05-23 01:36:42.502792: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2023-05-23 01:36:42.539630: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2023-05-23 01:36:42.571014: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2023-05-23 01:36:42.601962: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2023-05-23 01:36:42.628375: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_6' with dtype resource
	 [[{{node Placeholder/_6}}]]
2023-05-23 01:36:42.629052: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [32]
	 [[{{node Placeholder/_4}}]]
2023-05-23 01:36:42.662961: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_6' with dtype resource
	 [[{{node Placeholder/_6}}]]
2023-05-23 01:36:42.663830: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [32]
	 [[{{node Placeholder/_4}}]]
2023-05-23 01:36:42.697634: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_6' with dtype resource
	 [[{{node Placeholder/_6}}]]
2023-05-23 01:36:42.698468: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_6' with dtype resource
	 [[{{node Placeholder/_6}}]]
2023-05-23 01:36:42.731407: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [32]
	 [[{{node Placeholder/_4}}]]
2023-05-23 01:36:42.732128: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [32]
	 [[{{node Placeholder/_4}}]]
2023-05-23 01:36:42.765658: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [32]
	 [[{{node Placeholder/_4}}]]
2023-05-23 01:36:42.766547: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_6' with dtype resource
	 [[{{node Placeholder/_6}}]]
2023-05-23 01:36:42.799498: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_16' with dtype int32 and shape [3,1]
	 [[{{node Placeholder/_16}}]]
2023-05-23 01:36:42.800179: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_6' with dtype resource
	 [[{{node Placeholder/_6}}]]
2023-05-23 01:36:42.833451: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_16' with dtype int32 and shape [3,1]
	 [[{{node Placeholder/_16}}]]
2023-05-23 01:36:42.834362: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [32]
	 [[{{node Placeholder/_4}}]]
2023-05-23 01:36:42.867624: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [32]
	 [[{{node Placeholder/_4}}]]
2023-05-23 01:36:42.868470: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [32]
	 [[{{node Placeholder/_4}}]]
2023-05-23 01:36:42.901365: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_16' with dtype int32 and shape [3,1]
	 [[{{node Placeholder/_16}}]]
2023-05-23 01:36:42.902052: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_16' with dtype int32 and shape [3,1]
	 [[{{node Placeholder/_16}}]]
2023-05-23 01:36:42.934892: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_16' with dtype int32 and shape [3,1]
	 [[{{node Placeholder/_16}}]]
2023-05-23 01:36:42.935557: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_6' with dtype resource
	 [[{{node Placeholder/_6}}]]
2023-05-23 01:36:42.969417: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_6' with dtype resource
	 [[{{node Placeholder/_6}}]]
2023-05-23 01:36:42.970324: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_16' with dtype int32 and shape [3,1]
	 [[{{node Placeholder/_16}}]]
2023-05-23 01:36:43.005591: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [32]
	 [[{{node Placeholder/_4}}]]
2023-05-23 01:36:43.006315: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_16' with dtype int32 and shape [3,1]
	 [[{{node Placeholder/_16}}]]
2023-05-23 01:36:43.039225: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_16' with dtype int32 and shape [3,1]
	 [[{{node Placeholder/_16}}]]
2023-05-23 01:36:43.039923: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_6' with dtype resource
	 [[{{node Placeholder/_6}}]]
2023-05-23 01:36:43.074097: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_16' with dtype int32 and shape [3,1]
	 [[{{node Placeholder/_16}}]]
2023-05-23 01:36:43.074987: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_6' with dtype resource
	 [[{{node Placeholder/_6}}]]
2023-05-23 01:36:43.107878: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_16' with dtype int32 and shape [3,1]
	 [[{{node Placeholder/_16}}]]
2023-05-23 01:36:43.108533: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_6' with dtype resource
	 [[{{node Placeholder/_6}}]]
2023-05-23 01:36:43.144069: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [32]
	 [[{{node Placeholder/_4}}]]
2023-05-23 01:36:43.145070: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_16' with dtype int32 and shape [3,1]
	 [[{{node Placeholder/_16}}]]
2023-05-23 01:36:43.438866: I tensorflow/core/common_runtime/executor.cc:1210] [/job:localhost/replica:0/task:0/device:CPU:1] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): CANCELLED: Operation was cancelled
	 [[{{node tf.StatefulPartitionedCall/eager_operation}}]]
2023-05-23 01:36:43.439513: I tensorflow/core/common_runtime/executor.cc:1210] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): CANCELLED: Operation was cancelled
	 [[{{node tf.StatefulPartitionedCall/eager_operation}}]]
2023-05-23 01:36:43.440149: I tensorflow/core/common_runtime/executor.cc:1210] [/job:localhost/replica:0/task:0/device:CPU:5] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): CANCELLED: Operation was cancelled
	 [[{{node tf.StatefulPartitionedCall/eager_operation}}]]
2023-05-23 01:36:43.440610: I tensorflow/core/common_runtime/executor.cc:1210] [/job:localhost/replica:0/task:0/device:CPU:4] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): CANCELLED: Operation was cancelled
	 [[{{node tf.StatefulPartitionedCall/eager_operation}}]]
2023-05-23 01:36:43.441222: E tensorflow/dtensor/cc/dtensor_device.cc:2247] Error executing CopyToMesh {{function_node IteratorGetNext__func_14891566070902430035_8539663482787142021_8624502544051534713_3}} End of sequence
	 [[{{node tf.StatefulPartitionedCall/eager_operation}}]]
	Encountered when executing an operation using EagerExecutor. This error cancels all future operations and poisons their output tensors.
INFO:tensorflow:time(__main__.DTensorDatasetTest.testIterPrefetchEnabled): 1.16s
I0523 01:36:43.496025 140069302650688 test_util.py:2464] time(__main__.DTensorDatasetTest.testIterPrefetchEnabled): 1.16s
[  FAILED  ] DTensorDatasetTest.testIterPrefetchEnabled
```
</details>"
60670,//tensorflow/python/ops/ragged:ragged_cross_op_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.8.13

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

Test sometimes fails with segfault

### Standalone code to reproduce the issue

```shell
bazel test --build_tests_only --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=no_tensorflow_py_deps=true --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --local_test_jobs=64 --test_tag_filters=-nopip,-no_pip,-oss_serial,-no_oss,-oss_excluded,-v1only,-benchmark-test,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 -k -- //bazel_pip/tensorflow/... -//bazel_pip/tensorflow/compiler/tf2tensorrt/... -//bazel_pip/tensorflow/compiler/xrt/... -//bazel_pip/tensorflow/core/tpu/... -//bazel_pip/tensorflow/go/... -//bazel_pip/tensorflow/java/... -//bazel_pip/tensorflow/python/integration_testing/... -//bazel_pip/tensorflow/tools/toolchains/... -//bazel_pip/tensorflow/lite/... -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:conv_ops_test
```


### Relevant log output

```shell
[ RUN      ] RaggedCrossOpTest.testRaggedCrossInvalidValue
INFO:tensorflow:Running testRaggedCrossInvalidValue in GRAPH mode.
I0522 15:40:37.724678 281472914997264 test_util.py:1494] Running testRaggedCrossInvalidValue in GRAPH mode.
Fatal Python error: Segmentation fault

Thread 0x0000ffff851cc010 (most recent call first):
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1477 in _call_tf_sessionrun
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1384 in _run_fn
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1401 in _do_call
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1394 in _do_run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1214 in _run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 971 in run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py"", line 2061 in run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py"", line 2693 in evaluate
  File ""/tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.py"", line 478 in testRaggedCrossInvalidValue
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py"", line 1498 in decorated
  File ""/usr/lib/python3.10/unittest/case.py"", line 549 in _callTestMethod
  File ""/usr/lib/python3.10/unittest/case.py"", line 591 in run
  File ""/usr/lib/python3.10/unittest/case.py"", line 650 in __call__
  File ""/usr/lib/python3.10/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.10/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.10/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.10/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.10/unittest/runner.py"", line 184 in run
  File ""/usr/lib/python3.10/unittest/main.py"", line 271 in runTests
  File ""/usr/lib/python3.10/unittest/main.py"", line 101 in __init__
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2527 in _run_and_get_tests_result
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2561 in run_tests
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2155 in _run_in_app
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2060 in main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/app.py"", line 254 in _run_main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/app.py"", line 308 in run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/benchmark.py"", line 489 in benchmarks_main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.py"", line 497 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, google._upb._message, tensorflow.python.framework.fast_tensor_util, charset_normalizer.md, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.h5r, h5py.utils, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5t, h5py._conv, h5py.h5z, h5py._proxy, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.ndimage._nd_image, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, _ni_label, scipy.ndimage._ni_label (total: 72)
```
</details>"
60669,"InvalidArgumentError: Graph execution error: ndices[15,287] = 6368 is not in [0, 1993)","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.10.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I have been searching for a solution to this error, and I have tried changing the input dimension, but it is still not working. I would appreciate it if you could help me troubleshoot this issue.
Getting this error:
![image](https://github.com/tensorflow/tensorflow/assets/72642816/d0d3d78f-df1e-4b39-8a8a-e697791d4195)


### Standalone code to reproduce the issue

```python
# Embedding dimensions: depends on the word embedding vector dimension (300)
EMBEDDING_DIM = 300
# How many unique words to use (i.e number of rows in embedding vector)
MAX_VOICAB_SIZE = 20000
# Maximum number of words for each row of comment
MAX_SEQUENCE_LENGTH = 300

# Training Parameters
BATCH_SIZE = 256
EPOCHS = 2

sequence = df_test['cleaned'].apply(lambda x: [token.lex_id if not token.is_oov else 0 for token in nlp(x)]).to_list()
# By default, setting max length of paddings will truncate any sequences that are longer than the max length
x_train = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)
y_train = label_values
```

```python
tf.keras.backend.clear_session()
# Yoon Kim model (https://arxiv.org/abs/1408.5882)
def createTextCNN(vocab_size, embeddings_weights, max_sequence_length, embedding_dim, num_labels):
    # Input layer
    input_text = Input(shape=(max_sequence_length,)) # TODO Find out the shape

    # Embedding layer

    embedded_sequences = Embedding(
                input_dim=vocab_size
                , output_dim=embedding_dim
                , weights=[embeddings_weights]
                , input_length=max_sequence_length
                , trainable=False
                )(input_text)

    pool_output = []
    kernel_sizes = [3, 4, 5]
    for size in kernel_sizes:
        conv = Conv1D(filters=128, kernel_size=size)(embedded_sequences)
        pool = MaxPooling1D(pool_size=int(conv.shape[1]))(conv)
        pool_output.append(pool)

    pool_output = concatenate([pool for pool in pool_output])
    dropout_out = Dropout(0.5)(pool_output)
    flat_layer = Flatten()(dropout_out)
    x = Dense(units=128, activation='relu')(flat_layer)
    dropout_out = Dropout(0.2)(x)

    output_layer = Dense(units=num_labels, activation='sigmoid')(dropout_out)

    model = Model(input_text, output_layer)

    return model

model = createTextCNN(len(counter)
        , embedding_weights
        , MAX_SEQUENCE_LENGTH
        , EMBEDDING_DIM
        , len(LABELS))

model.summary()
```

```python
model.compile(loss='categorical_crossentropy',
            optimizer='adam',
            metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)
callbacks_list = [early_stopping]

train = model.fit(x_train, y_train, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, callbacks=callbacks_list)
```
```


### Relevant log output

_No response_</details>"
60667,`@com_google_protobuf//:well_known_types_py_pb2_genproto` is not in systemlibs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

https://github.com/tensorflow/tensorflow/commit/84f40925e929d05e72ab9234e53c729224e3af38 adds a new dependency `@com_google_protobuf//:well_known_types_py_pb2_genproto` as follows:

https://github.com/tensorflow/tensorflow/blob/df574a8a1f9447ee0399e019c1a9cf67c11bfe99/tensorflow/tsl/platform/default/build_config.bzl#L409

However, this target is not added to `tensorflow/third_party/systemlibs/protobuf.BUILD`. So when building TensorFlow with `export TF_SYSTEM_LIBS=com_google_protobuf`, the following error occurs:

```
ERROR: tensorflow/compiler/xla/BUILD:83:17: no such target '@com_google_protobuf//:well_known_types_py_pb2_genproto': target 'well_known_types_py_pb2_genproto' not declared in package '' defined by external/com_google_protobuf/BUILD.bazel and referenced by '//tensorflow/compiler/xla:xla_data_proto_py_genproto'
```


### Standalone code to reproduce the issue

```shell
export TF_SYSTEM_LIBS=com_google_protobuf
./configure
bazel build //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: tensorflow/compiler/xla/BUILD:83:17: no such target '@com_google_protobuf//:well_known_types_py_pb2_genproto': target 'well_known_types_py_pb2_genproto' not declared in package '' defined by external/com_google_protobuf/BUILD.bazel and referenced by '//tensorflow/compiler/xla:xla_data_proto_py_genproto
```
</details>"
60666,tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

T4

### Current Behaviour?

**I had been training faster_rcnn_resnet50_v1_640x640_coco17_tpu-8 model on my custom dataset in colab, all paths are correctly set in config file. Issue is on both GPU and CPU.
  fine_tune_checkpoint: ""/content/drive/MyDrive/Obj_Detection/Faster_RCNN/data/pretrained_model/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/checkpoint/ckpt-0""
  fine_tune_checkpoint_type: ""detection""
  data_augmentation_options {
    random_horizontal_flip {
    }
  }

  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
  use_bfloat16: true  # works only on TPUs
}

train_input_reader: {
  label_map_path: ""/content/drive/MyDrive/Obj_Detection/Faster_RCNN/data/label_map.pbtxt""
  tf_record_input_reader {
    input_path: ""/content/drive/MyDrive/Obj_Detection/Faster_RCNN/data/train/*.tfrecord""
  }
}

eval_config: {
  metrics_set: ""coco_detection_metrics""
  use_moving_averages: false
  batch_size: 1;
}

eval_input_reader: {
  label_map_path: ""/content/drive/MyDrive/Obj_Detection/Faster_RCNN/data/label_map.pbtxt""
  shuffle: false
  num_epochs: 1
  tf_record_input_reader {
    input_path: ""/content/drive/MyDrive/Obj_Detection/Faster_RCNN/data/val/val.tfrecord""
  }
}**

Its probably config file but I have set all parameters correctly and copy pasted absolute paths.


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/16dQA4FzrNhMlV30qo5ofFH3Oj4WbtNP0?usp=sharing

PIPELINE_CONFIG_PATH='/content/drive/MyDrive/Obj_Detection/Faster_RCNN/Models/training_process/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.config'

MODEL_DIR='/content/drive/MyDrive/Obj_Detection/Faster_RCNN/Models'
# in the next cell
%%shell
cd /content
python /content/drive/MyDrive/Obj_Detection/models/research/object_detection/model_main_tf2.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --alsologtostderr
```


### Relevant log output

```shell
TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.
W0523 02:39:16.601438 139836257871680 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)
I0523 02:39:16.650524 139836257871680 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)
Traceback (most recent call last):
  File ""/content/drive/MyDrive/Obj_Detection/models/research/object_detection/model_main_tf2.py"", line 114, in <module>
    tf.compat.v1.app.run()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/platform/app.py"", line 36, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.10/dist-packages/absl/app.py"", line 308, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.10/dist-packages/absl/app.py"", line 254, in _run_main
    sys.exit(main(argv))
  File ""/content/drive/MyDrive/Obj_Detection/models/research/object_detection/model_main_tf2.py"", line 105, in main
    model_lib_v2.train_loop(
  File ""/usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py"", line 505, in train_loop
    configs = get_configs_from_pipeline_file(
  File ""/usr/local/lib/python3.10/dist-packages/object_detection/utils/config_util.py"", line 138, in get_configs_from_pipeline_file
    proto_str = f.read()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py"", line 116, in read
    self._preread_check()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py"", line 77, in _preread_check
    self._read_buf = _pywrap_file_io.BufferedInputStream(
tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory
---------------------------------------------------------------------------
CalledProcessError                        Traceback (most recent call last)
<ipython-input-7-8bc0cf4d8665> in <cell line: 1>()
----> 1 get_ipython().run_cell_magic('shell', '', 'cd /content\npython /content/drive/MyDrive/Obj_Detection/models/research/object_detection/model_main_tf2.py \\\n    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\n    --model_dir=${MODEL_DIR} \\\n    --alsologtostderr\n')

3 frames
/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py in check_returncode(self)
    135   def check_returncode(self):
    136     if self.returncode:
--> 137       raise subprocess.CalledProcessError(
    138           returncode=self.returncode, cmd=self.args, output=self.output
    139       )

CalledProcessError: Command 'cd /content
python /content/drive/MyDrive/Obj_Detection/models/research/object_detection/model_main_tf2.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --alsologtostderr
' returned non-zero exit status 1.
```
</details>"
60663,Request to support ERF function for TFL,"`tf.math.erf` is supported here: https://www.tensorflow.org/api_docs/python/tf/math/erf

TFLite is missing the erf implementation: https://www.tensorflow.org/mlir/tfl_ops#tfllstm_mlirtfllstmop


"
60662,can we customize memory allocation functions(like malloc/free) for inference with C api? ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When using C api to do inference, I want to customize the memory allocation functions like malloc/free to control how tensorflow allocates memory. I want to make tensorflow use a separate area of memory in some shared memory region to load the model. Is there a way to do this?

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
60660,Casting in multiprocessing hang forever,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf>=2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04, 22.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

None

### GPU model and memory

_No response_

### Current Behaviour?

When casting an array above a certain size in multiprocessing without GPU, the second call just hangs forever (either using `convert_to_tensor` or `cast`).

### Standalone code to reproduce the issue

```shell
import multiprocessing

import tensorflow as tf


def func(x):
    return tf.convert_to_tensor([[[0 for _ in range(700)] for _ in range(700)] for _ in range(3)], tf.uint8)


def test_cast_success():
    with multiprocessing.Pool(processes=1) as p:
        p.map(func=func, iterable=[1])


def test_cast_hang_forever():
    with multiprocessing.Pool(processes=1) as p:
        p.map(func=func, iterable=[1])


test_cast_success()
test_cast_hang_forever()
```


### Relevant log output

```shell
KeyboardInterrupt                         Traceback (most recent call last)
Cell In[3], line 21
     17         p.map(func=func, iterable=[1])
     20 test_cast_success()
---> 21 test_cast_hang_forever()

Cell In[3], line 17, in test_cast_hang_forever()
     15 def test_cast_hang_forever():
     16     with multiprocessing.Pool(processes=1) as p:
---> 17         p.map(func=func, iterable=[1])

File ~/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/pool.py:364, in Pool.map(self, func, iterable, chunksize)
    359 def map(self, func, iterable, chunksize=None):
    360     '''
    361     Apply `func` to each element in `iterable`, collecting the results
    362     in a list that is returned.
    363     '''
--> 364     return self._map_async(func, iterable, mapstar, chunksize).get()

File ~/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/pool.py:765, in ApplyResult.get(self, timeout)
    764 def get(self, timeout=None):
--> 765     self.wait(timeout)
    766     if not self.ready():
    767         raise TimeoutError

File ~/.pyenv/versions/3.8.16/lib/python3.8/multiprocessing/pool.py:762, in ApplyResult.wait(self, timeout)
    761 def wait(self, timeout=None):
--> 762     self._event.wait(timeout)

File ~/.pyenv/versions/3.8.16/lib/python3.8/threading.py:558, in Event.wait(self, timeout)
    556 signaled = self._flag
    557 if not signaled:
--> 558     signaled = self._cond.wait(timeout)
    559 return signaled

File ~/.pyenv/versions/3.8.16/lib/python3.8/threading.py:302, in Condition.wait(self, timeout)
    300 try:    # restore state no matter what (e.g., KeyboardInterrupt)
    301     if timeout is None:
--> 302         waiter.acquire()
    303         gotit = True
    304     else:

KeyboardInterrupt:
```
</details>"
60659,Memory leak in model fit with dataset from generator,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10, 3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

None

### GPU model and memory

None

### Current Behaviour?

When creating a dataset from the generator, using Tensorflow V2.12.0 with Python 3.11, some memory is not released after training on each batch, leading to a linear increase in memory usage during the model fit step.

### Standalone code to reproduce the issue

```shell
This is the code I used to generate random data on the fly to be used by the generator, and a simple model to fit this data on. 


batch_size =32
train_datapoints_count = 1000000
val_datapoints_count = 200000

def generate_random_dataset(input_size):
    for _ in range(input_size):
        x = np.random.rand(batch_size, 1, 10).astype(np.float32)
        y = np.random.rand(batch_size, 1, 1).astype(np.float32)
        yield x, y

dataset_train = tf.data.Dataset.from_generator(
    generator=generate_random_dataset,
    args=[train_datapoints_count],
    name=""random_ds_train"",
    output_signature=(
        tf.TensorSpec(shape=(None, 1, 10), dtype=np.float32),
        tf.TensorSpec(shape=(None, 1, 1), dtype=np.float32),
    ),
)

dataset_val = tf.data.Dataset.from_generator(
    generator=generate_random_dataset,
    args = [val_datapoints_count],
    name=""random_ds_val"",
    output_signature=(
        tf.TensorSpec(shape=(None, 1, 10), dtype=np.float32),
        tf.TensorSpec(shape=(None, 1, 1), dtype=np.float32),
    ),
)

model = tf.keras.Sequential(
    [
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(units=batch_size, activation=""relu""),
        tf.keras.layers.Dense(units=batch_size, activation=""relu""),
        tf.keras.layers.Dense(units=1),
    ]
)

model.compile(
    loss=tf.losses.MeanSquaredError(),
    optimizer=tf.optimizers.Adam(),
    metrics=[tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError()],
)
model.fit(
    dataset_train,
    epochs=2,
    validation_data=dataset_val,
)
```

I then ran the code in a container and logged the memory usage.
```
FROM python:3.10 
#also 3.11

WORKDIR /app

RUN pip install tensorflow==2.12.0
COPY train.py .

ENTRYPOINT [""python"", ""train.py""]
```
```


### Relevant log output

```shell
With Python 3.11, I see an increment in memory usage over time, while it remains the same for Python 3.10. I am allocating the exact same resources to both of the containers.
The columns are respectively: Seconds into the fitting step, memory usage tf_2.12_python_3.10, memory usage tf_2.12_python_3.11. 

5	254.9	288.9
10	254.9	291
15	254.9	293.1
20	254.9	294.8
25	254.9	296.4
30	254.9	298.5
35	254.9	300.3
40	254.9	301.8
45	254.9	304
50	254.9	306.2
55	254.9	308.3
60	254.9	310.6
65	254.9	312.5
70	254.9	314.8
75	254.9	316.5
80	254.9	317.9
85	254.6	319.5
90	254.2	320.4
95	253.8	321.5
100	253	323.1
```
</details>"
60656,tf.math.sign has different results with or without XLA,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.14.0-dev20230520

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

/

### Python version

3.7.5

### Bazel version

/

### GCC/Compiler version

/

### CUDA/cuDNN version

11.2

### GPU model and memory

GeForce RTX 2080Ti

### Current Behaviour?

`tf.math.sign` has different results with or without XLA.

The reproduction colab link is here: [colab](https://colab.research.google.com/drive/1JQXDs9fo6Ft2HzFPnZ-ChorhUA0Zn_cz#scrollTo=FYTuTfdC5IX_).

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

print(tf.version.VERSION)


@tf.function(jit_compile=False)
def run_without_xla(a):
    return tf.math.sign(a)


@tf.function(jit_compile=True)
def run_with_xla(a):
    return tf.math.sign(a)


x = tf.constant([-3.e+307 + 0.j, -6.e+307 + 0.j])
print(run_without_xla(x))
print(run_with_xla(x))
```


### Relevant log output

```shell
2.14.0-dev20230520
tf.Tensor([-0.+0.j -0.+0.j], shape=(2,), dtype=complex128)
tf.Tensor([-1.+0.j -1.+0.j], shape=(2,), dtype=complex128)
```
</details>"
60654,tf.math.abs has different results when running under CPU or GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

/

### Python version

3.7.5

### Bazel version

/

### GCC/Compiler version

/

### CUDA/cuDNN version

11.2

### GPU model and memory

GeForce RTX 2080Ti

### Current Behaviour?

`tf.math.abs` has different results when running under CPU or GPU.

The reproduction colab link is here: [colab](https://colab.research.google.com/drive/1pjhCrZTnmUktYZ75BKb2XqCMrdVnRqOP#scrollTo=D2E0G6uJiDMO).

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x = tf.constant([3.4e+307 + 0.j], dtype=tf.complex128)

with tf.device(""/cpu:0""):
    a = tf.math.abs(x)
    print(a)

with tf.device(""/gpu:0""):
    a = tf.math.abs(x)
    print(a)
```


### Relevant log output

```shell
tf.Tensor([3.4e+307], shape=(1,), dtype=float64)
tf.Tensor([inf], shape=(1,), dtype=float64)
```
</details>"
60653,Using NnApiDelegate in TFLite 2.11.0 returns same embeddings for all images. It works fine for 2.6.0,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 13.3.1
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: Xiaomi Poco F1
-   **TensorFlow Lite version**: 2.11.0

### Describe the problem
In my android app I'm using Facenet model to recognize faces. I have added NnApiDelegate to the interpreterOptions.
My app has been working with TFLite version 2.6.0. When I upgraded the TFLite version to 2.10.0 or 2.11.0, I see that the model returns the same embeddings for any image I provide.
Removing the NnApiDelegate works in 2.11.0, but it slows down the face recognition considerably, so I do not want to remove NnApiDelegate.

### Source code / logs
Code for setting up the interpreter in Kotlin:

val interpreterOptions = Interpreter.Options()
interpreterOptions.addDelegate(NnApiDelegate())
interpreter = Interpreter(FileUtil.loadMappedFile(context, model.assetsFilename ) , interpreterOptions)

"
60652,AdamW optimiser crashes on tf-macos v2.12.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS 12.5.1 (running on M1 pro chip)

### Mobile device

_No response_

### Python version

3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:01:19) [Clang 14.0.6 ]

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

The problem occurs in tensorflow-macos v2.12.0 when attempting to call model.compile() with the AdamW optimiser. A warning is thrown, telling us that there is a known slowdown when using v2.11+ optimizers, and the backend attempts to fall back to a legacy version. However, AdamW does not exist in legacy versions, which eventually propagates through to an ""unknown optimizer"" error.

Expected behaviour: tf.keras.Model object is compiled using the AdamW optimiser, either using the ""tf v2.11+"" optimiser class with known slowdown, or falling back to an implementation compatible with legacy keras optimisers

Note: problem occurs in tf-macos regardless of whether we are using tf-metal to access the GPU.

### Standalone code to reproduce the issue

```shell
##
##  Imports
##

import sys

import tensorflow as tf

from tensorflow.keras.models     import Model
from tensorflow.keras.layers     import Input, Dense
from tensorflow.keras.optimizers import AdamW

##
##  Report versions
##

print(f""Python version is: {sys.version}"")
##  -->  Python version is: 3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:01:19) [Clang 14.0.6 ]

print(f""TF version is: {tf.__version__}"")
##  -->  TF version is: 2.12.0

print(f""Keras version is: {tf.keras.__version__}"")
##  -->  Keras version is: 2.12.0


##
##  Create a very simple model
##

x_in  = Input(1)
x     = Dense(10)(x_in)
model = Model(x_in, x)

##
##  Compile model with AdamW optimizer
##
model.compile(optimizer=AdamW(learning_rate=1e-3, weight_decay=1e-2))
```


### Relevant log output

```shell
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdamW` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdamW`.
WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.AdamW`.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[4], line 2
      1 ##  Compile model with AdamW optimizer
----> 2 model.compile(optimizer=AdamW(learning_rate=1e-3, weight_decay=1e-2))

File ~/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/miniforge3/envs/tf_macos_230511/lib/python3.10/site-packages/keras/saving/legacy/serialization.py:368, in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)
    364 cls = object_registration.get_registered_object(
    365     class_name, custom_objects, module_objects
    366 )
    367 if cls is None:
--> 368     raise ValueError(
    369         f""Unknown {printable_module_name}: '{class_name}'. ""
    370         ""Please ensure you are using a `keras.utils.custom_object_scope` ""
    371         ""and that this object is included in the scope. See ""
    372         ""https://www.tensorflow.org/guide/keras/save_and_serialize""
    373         ""#registering_the_custom_object for details.""
    374     )
    376 cls_config = config[""config""]
    377 # Check if `cls_config` is a list. If it is a list, return the class and the
    378 # associated class configs for recursively deserialization. This case will
    379 # happen on the old version of sequential model (e.g. `keras_version` ==
    380 # ""2.0.6""), which is serialized in a different structure, for example
    381 # ""{'class_name': 'Sequential',
    382 #   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}"".

ValueError: Unknown optimizer: 'adamw'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.
```
</details>"
60651,Cannot rrain the model using the TensorFlow tensors:,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened! Cannot Train the model using the TensorFlow tensors.

### Standalone code to reproduce the issue

```shell
from keras.layers import LSTM, Dense
from keras.models import Sequential


# Define the sliding window size
window_size = 7
num_timesteps = 7
num_features = 6

# Create input-output pairs using the sliding window technique
X_train = []
y_train = []
for i in range(len(X_train) - window_size):
    X_train.append(X_train[i:i+window_size])
    y_train.append(y_train[i+window_size])

X_test = []
y_test = []
for i in range(len(X_test) - window_size):
    X_test.append(X_test[i:i+window_size])
    y_test.append(y_test[i+window_size])

# Convert the lists to numpy arrays
X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)
y_test = np.array(y_test)


tf.config.run_functions_eagerly(True)

model = Sequential()
model.add(LSTM(64, input_shape=(window_size, num_features)))
model.add(Dense(3))
model.compile(loss='mean_squared_error', optimizer='adam')

X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)
y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)

history = model.fit(X_train_tensor, y_train_tensor, epochs=50, batch_size=32, validation_data=(X_test, y_test))
```


### Relevant log output

```shell
ValueError                                Traceback (most recent call last)
Cell In[54], line 1
----> 1 history = model.fit(X_train_tensor, y_train_tensor, epochs=50, batch_size=32, validation_data=(X_test, y_test))

File ~\anaconda3\Anaconda\lib\site-packages\keras\utils\traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~\anaconda3\Anaconda\lib\site-packages\keras\engine\training.py:1697, in Model.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1695 logs = tf_utils.sync_to_numpy_or_python_type(logs)
   1696 if logs is None:
-> 1697     raise ValueError(
   1698         ""Unexpected result of `train_function` ""
   1699         ""(Empty logs). Please use ""
   1700         ""`Model.compile(..., run_eagerly=True)`, or ""
   1701         ""`tf.config.run_functions_eagerly(True)` for more ""
   1702         ""information of where went wrong, or file a ""
   1703         ""issue/bug to `tf.keras`.""
   1704     )
   1705 # Override with model metrics instead of last step logs
   1706 logs = self._validate_and_get_metrics_result(logs)

ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.
```
</details>"
60650,GPU not found after 2.10,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.10.0-76-gfdfc646704c 2.10.1

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

8.1

### GPU model and memory

RTX 3060

### Current Behaviour?

Hi there, I am new at Tensorflow and AI programming. So forgive me if this is a dumb question.

2.10 is the last GPU special version. I read that newer versions have GPU support included without a special gpu version, but no GPU is found under these new versions

Thanks :-)
Martin


### Standalone code to reproduce the issue

```shell
import os
import sys

# Get the CUDA_PATH environment variable
cuda_path = os.environ.get(""CUDA_PATH"")

# Check if CUDA_PATH is set
if cuda_path:
    # Add the bin folder inside CUDA_PATH to the system path
    sys.path.append(os.path.join(cuda_path, ""bin""))
else:
    print(""Achtung: Umgebungsvariable 'CUDA_PATH' nicht gefunden."")

os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""1""

import tensorflow as tf
import ctypes

def check_python_version():
    print(f""Python-Version: {sys.version}"")
    major, minor = sys.version_info[:2]
    if major == 3 and minor >= 6:
        print(""Python-Version ist kompatibel."")
    else:
        print(""Achtung: TensorFlow erfordert Python 3.6 oder höher."")

def check_tf_version():
    print(f""Installierte TensorFlow-Version: {tf.__version__}"")
    if tf.__version__ >= ""2.12.0"":
        print(""TensorFlow-Version ist kompatibel."")
    else:
        print(""Achtung: Dieses Skript wurde für TensorFlow 2.12.0 entwickelt."")

def check_gpu_support():
    print(""Num GPUs Available:"", len(tf.config.list_physical_devices('GPU')))
    if len(tf.config.list_physical_devices('GPU')) > 0:
        print(""GPU-Unterstützung ist aktiviert."")
    else:
        print(""Achtung: Keine GPU gefunden oder GPU-Unterstützung ist deaktiviert."")

def check_cuda_version():
    if not cuda_path:
        return

    cuda_version_file = os.path.join(cuda_path, ""version.txt"")
    if not os.path.exists(cuda_version_file):
        print(""Achtung: CUDA-Versionstextdatei nicht gefunden."")
        return

    with open(cuda_version_file, ""r"") as f:
        cuda_version_data = f.read().strip()
    print(f""Installierte CUDA-Version: {cuda_version_data}"")

    if ""11.4"" in cuda_version_data:
        print(""CUDA-Version ist kompatibel."")
    else:
        print(""Achtung: TensorFlow 2.12.0 erfordert CUDA 11.4."")

def check_cudnn_version():
    if not cuda_path:
        return

    cudnn_dll_name = ""cudnn64_8.dll""

    cudnn_dll_path = os.path.join(cuda_path, ""bin"", cudnn_dll_name)
    if not os.path.exists(cudnn_dll_path):
        print(""Achtung: cuDNN-DLL-Datei nicht gefunden."")
        return

    cudnn_dll = ctypes.WinDLL(cudnn_dll_path)
    cudnn_version = cudnn_dll.cudnnGetVersion()
    print(f""Installierte cuDNN-Version: {cudnn_version}"")

    if cudnn_version >= 8200:
        print(""cuDNN-Version ist kompatibel."")
    else:
        print(""Achtung: TensorFlow 2.12.0 erfordert cuDNN 8.2 oder höher."")

def main():
    check_python_version()
    print()
    check_tf_version()
    print()
    check_gpu_support()
    print()
    check_cuda_version()
    print()
    check_cudnn_version()

if __name__ == ""__main__"":
    main()
```


### Relevant log output

_No response_</details>"
60649,TypeError: Cannot convert 0.1 to EagerTensor of dtype int64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

GoogleColab

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

colab

### GCC/Compiler version

colab

### CUDA/cuDNN version

colab

### GPU model and memory

colab

### Current Behaviour?

getting this error when i execute the code present in tensowflow website to implement tf.keras.optimizers.schedules.LearningRateSchedule 
error:

TypeError: Cannot convert 0.1 to EagerTensor of dtype int64

```python
class MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):

  def __init__(self, initial_learning_rate):
    self.initial_learning_rate = initial_learning_rate

  def __call__(self, step):
     return self.initial_learning_rate / (step + 1)

optimizer = tf.keras.optimizers.SGD(learning_rate=MyLRSchedule(0.1))

```

### Standalone code to reproduce the issue

```shell
TypeError: Cannot convert 0.1 to EagerTensor of dtype int64
```


### Relevant log output

TypeError                                 Traceback (most recent call last)
[<ipython-input-145-86d045432fd5>](https://localhost:8080/#) in <cell line: 9>()
      7      return self.initial_learning_rate / (step + 1)
      8 
----> 9 optimizer = tf.keras.optimizers.SGD(learning_rate=MyLRSchedule(0.1))

4 frames
[/usr/local/lib/python3.10/dist-packages/keras/optimizers/sgd.py](https://localhost:8080/#) in __init__(self, learning_rate, momentum, nesterov, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, name, **kwargs)
    121             **kwargs
    122         )
--> 123         self._learning_rate = self._build_learning_rate(learning_rate)
    124         self.momentum = momentum
    125         self.nesterov = nesterov

[/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py](https://localhost:8080/#) in _build_learning_rate(self, learning_rate)
    382                 # Create a variable to hold the current learning rate.
    383                 current_learning_rate = tf.convert_to_tensor(
--> 384                     learning_rate(self.iterations)
    385                 )
    386                 self._current_learning_rate = tf.Variable(

[<ipython-input-145-86d045432fd5>](https://localhost:8080/#) in __call__(self, step)
      5 
      6   def __call__(self, step):
----> 7      return self.initial_learning_rate / (step + 1)
      8 
      9 optimizer = tf.keras.optimizers.SGD(learning_rate=MyLRSchedule(0.1))

[/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py](https://localhost:8080/#) in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

[/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py](https://localhost:8080/#) in convert_to_eager_tensor(value, ctx, dtype)
    101       dtype = dtypes.as_dtype(dtype).as_datatype_enum
    102   ctx.ensure_initialized()
--> 103   return ops.EagerTensor(value, ctx.device_name, dtype)
    104 
    105 

TypeError: Cannot convert 0.1 to EagerTensor of dtype int64

</details>"
60648,Fatal Python error: Aborted ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.1 and 2.13.0.rc0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11 22H2

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

---

### GCC/Compiler version

---

### CUDA/cuDNN version

None

### GPU model and memory

None

### Current Behaviour?

TensorFlow crashes. I cannot currently reproduce it, but was able to catch one of many stack traces before it stopped being reproducible.

### Standalone code to reproduce the issue

```shell
# IIRC, reproduces using any of
# - pip install tensorflow==2.10.1 tf_keras_vis==0.8.5 pytest==7.3.1
# - pip install tensorflow==2.13.0.rc0 tf_keras_vis==0.8.5 pytest==7.3.1
#
# Then run
# - pytest -k test_crash
#
# It did not reproduce with
# - pytest -k test_crash.py
# implying that other pytest plugins may be playing a role.
# See the LONG list of modules at the end of the stack trace.
#
# It also stopped reproducing once I renamed the test, implying that test discovery
# order may be playing a role.

import tensorflow as tf
from tensorflow.keras.applications import VGG19
from tf_keras_vis.scorecam import Scorecam
from tf_keras_vis.utils.scores import CategoricalScore


def test_crash():
    model = VGG19(classes=2, weights=None)

    cam = Scorecam(model)
    score = CategoricalScore([0])
    seed_input = tf.zeros((1, *model.input.shape[1:]))
    cam(score, seed_input)


if __name__ == ""__main__"":
    # It does not reproduce like this (at least on Windows)
    test_crash()
```


### Relevant log output

```shell
Fatal Python error: Aborted

Thread 0x00001770 (most recent call first):
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\tensorflow\python\eager\execute.py"", line 54 in quick_execute
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\tensorflow\python\eager\function.py"", line 499 in call
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\tensorflow\python\eager\function.py"", line 1862 in _call_flat
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\tensorflow\python\eager\def_function.py"", line 986 in _call
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\tensorflow\python\eager\def_function.py"", line 915 in __call__
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\tensorflow\python\util\traceback_utils.py"", line 150 in error_handler
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\engine\training.py"", line 2253 in predict
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\utils\traceback_utils.py"", line 65 in error_handler
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\tf_keras_vis\scorecam.py"", line 159 in __call__
  File ""D:\Code\project\project\....py"", line ... in test_crash
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\python.py"", line 194 in pytest_pyfunc_call
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_callers.py"", line 39 in _multicall
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_manager.py"", line 80 in _hookexec
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_hooks.py"", line 265 in __call__
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\python.py"", line 1799 in runtest
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\runner.py"", line 169 in pytest_runtest_call
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_callers.py"", line 39 in _multicall
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_manager.py"", line 80 in _hookexec
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_hooks.py"", line 265 in __call__
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\runner.py"", line 262 in <lambda>
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\runner.py"", line 341 in from_call
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\runner.py"", line 261 in call_runtest_hook
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\runner.py"", line 222 in call_and_report
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\runner.py"", line 133 in runtestprotocol
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\runner.py"", line 114 in pytest_runtest_protocol
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_callers.py"", line 39 in _multicall
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_manager.py"", line 80 in _hookexec
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_hooks.py"", line 265 in __call__
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\main.py"", line 348 in pytest_runtestloop
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_callers.py"", line 39 in _multicall
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_manager.py"", line 80 in _hookexec
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_hooks.py"", line 265 in __call__
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\main.py"", line 323 in _main
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\main.py"", line 269 in wrap_session
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\main.py"", line 316 in pytest_cmdline_main
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_callers.py"", line 39 in _multicall
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_manager.py"", line 80 in _hookexec
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pluggy\_hooks.py"", line 265 in __call__
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\config\__init__.py"", line 166 in main
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\_pytest\config\__init__.py"", line 189 in console_main
  File ""C:\Users\bers\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\pytest\__main__.py"", line 5 in <module>
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 86 in _run_code
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 196 in _run_module_as_main

Extension modules: mypy, mypy.api, lazy_object_proxy.cext, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, matplotlib._c_internal_utils, PIL._imaging, matplotlib._path, kiwisolver._cext, matplotlib._image, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.hashing, pyarrow.lib, pyarrow._hdfsio, pandas._libs.tslib, pandas._libs.ops, pyarrow._compute, pandas._libs.arrays, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.index, pandas._libs.internals, pandas._libs.join, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, google.protobuf.pyext._message, charset_normalizer.md, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.h5r, h5py.utils, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5t, h5py._conv, h5py.h5z, h5py._proxy, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.ndimage._nd_image, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, _ni_label, scipy.ndimage._ni_label, PIL._imagingft, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, numpy.linalg.lapack_lite, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize.__nnls, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, lxml._elementpath, lxml.etree, sklearn.__check_build._check_build, sklearn.utils.murmurhash, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_ufunc, scipy.stats.ncx2_ufunc, scipy.stats._boost.ncx2_ufunc, scipy.stats.nct_ufunc, scipy.stats._boost.nct_ufunc, scipy.stats.skewnorm_ufunc, scipy.stats._boost.skewnorm_ufunc, scipy.stats.invgauss_ufunc, scipy.stats._boost.invgauss_ufunc, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst,
 scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._statlib, scipy.stats._mvn, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont, sklearn.utils._isfinite, sklearn.utils._openmp_helpers, sklearn.decomposition._cdnmf_fast, sklearn.utils._logistic_sigmoid, sklearn.utils.sparsefuncs_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.utils._typedefs, sklearn.utils._readonly_array_wrapper, sklearn.metrics._dist_metrics, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_fast, sklearn.utils._random, sklearn.utils._seq_dataset, sklearn.utils.arrayfuncs, sklearn.linear_model._cd_fast, sklearn._loss._loss, sklearn.utils._weight_vector, sklearn.linear_model._sgd_fast, sklearn.linear_model._sag_fast, sklearn.svm._libsvm, sklearn.svm._liblinear, sklearn.svm._libsvm_sparse, sklearn.decomposition._online_lda_fast, sklearn.neighbors._partition_nodes, sklearn.neighbors._ball_tree, sklearn.neighbors._kd_tree, sklearn._isotonic, sklearn.manifold._utils, sklearn.tree._utils, sklearn.tree._tree, sklearn.tree._splitter, sklearn.tree._criterion, sklearn.neighbors._quad_tree, sklearn.manifold._barnes_hut_tsne (total: 240)
```
</details>"
60647,"Insufficient documentation on GPU use, especially in MPS","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.13

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I tried to find a documentation on allowing GPU and device-agnostic code on TensorFlow, but there was nowhere to be found and the documentation.

### Standalone code to reproduce the issue

```shell
There wasn't a code, but there is insufficient documentation on GPU and device-agnostic code to set up.
```


### Relevant log output

_No response_</details>"
60646,tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

This command:
```
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
```

returns:
```
W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]
```

So no GPU is detected. I have NVIDIA drivers installed correctly cuda and cudnn installed correctly as advised on https://www.tensorflow.org/install/pip#linux_setup
There is nothing about setting path, so I have not set path.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
```
```


### Relevant log output

_No response_</details>"
60645,"""Improve Performance of Convolutional Neural Networks on GPU""","Currently, TensorFlow's performance for convolutional neural networks (CNNs) on GPU can be further optimized. The goal of this issue is to identify and implement improvements that enhance the speed and efficiency of CNN computations on GPU devices. This includes optimizing convolutional and pooling operations, leveraging GPU-specific optimizations, and exploring techniques such as kernel fusion and memory access optimizations.

Expected Outcome:
By addressing this issue, we aim to achieve significant performance improvements for CNN workloads on GPU, enabling faster training and inference times for deep learning models. This will enhance the overall efficiency and scalability of TensorFlow for CNN-based applications and empower researchers and practitioners to train and deploy models more effectively.

Help Needed:
Contributors with expertise in GPU programming, CUDA, and deep learning optimization techniques are encouraged to collaborate on this issue. The community's insights and contributions are valuable in identifying bottlenecks, proposing optimizations, and implementing efficient GPU-accelerated CNN operations in TensorFlow.

Additionally, suggestions and insights from domain experts, performance profiling, and benchmarking results are welcome to drive this improvement initiative. Let's work together to make TensorFlow's GPU performance for CNNs even better!

Please note that this issue is currently open and up for contributions. Feel free to join the discussion and contribute your expertise towards enhancing the performance of convolutional neural networks on GPU in TensorFlow."
60644,"""Improve GPU memory management for large-scale models""","Currently, TensorFlow's GPU memory management can be challenging when training large-scale models. This issue aims to improve the memory management strategies for GPU usage to optimize memory allocation and deallocation, reducing memory fragmentation and enabling more efficient training of large models.

You can find this issue by going to the TensorFlow repository's ""Issues"" tab and using the search bar to search for the keywords ""GPU memory management large-scale models."" Once you find the issue, make sure to read through the details and discussion to understand the specific challenges and proposed solutions.

Feel free to contribute to this issue by commenting on it, discussing possible approaches, or even submitting a pull request with your proposed changes. Remember to familiarize yourself with the contribution guidelines and any specific instructions mentioned in the issue before getting started.

Good luck, and I hope you find this issue interesting and valuable for your contributions to TensorFlow!"
60642,ROCm: Importing PyTorch before TensorFlow causes TensorFlow to fail completely,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

v2.11.1-3812-gef4eebff7d4 2.11.1

### Custom Code

No

### OS Platform and Distribution

Arch Linux (EndeavourOS)

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

ROCm 5.4.3

### GPU model and memory

Radeon VII/16 GiB

### Current Behaviour?

The title suffices. A workaround is to ensure `tensorflow` is imported before `torch`.

### Standalone code to reproduce the issue

```shell
python -c ""import torch; import tensorflow as tf; tf.zeros(1)""
```


### Relevant log output

```shell
2023-05-19 22:22:33.756830: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
v2.11.1-3812-gef4eebff7d4 2.11.1
(sd) [habbasi@hameer-imacpro11 kohya-trainer]$ python -c ""import tensorflow as tf; import torch; tf.zeros(1)""
2023-05-19 22:25:50.708366: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-19 22:25:53.172076: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-19 22:25:53.172177: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-19 22:25:53.172216: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-19 22:25:53.172542: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-19 22:25:53.173978: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-19 22:25:53.174095: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-19 22:25:53.174137: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/habbasi/mambaforge/envs/sd/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/habbasi/mambaforge/envs/sd/lib/python3.10/site-packages/tensorflow/python/eager/context.py"", line 588, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InternalError: hipGetDevice() failed. Status: invalid device ordinal
```
</details>

xref pytorch/pytorch#101900"
60640,SavedModel: enable dropout & disable batch normalization,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0-dev

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am currently training a ResNet model with both batch normalization and dropout layers. My goal is to use monte carlo dropout for uncertainty estimation at evaluation time (i.e. with training=False in my model call).

I'm currently working with tf.functions and the SavedModel format (not eager execution). Eager execution is to slow for my application, and therefore is not an option for me. 

Thus, when I set training=False for my model calls, it disables both the batch normalization and dropout layers. However, when I set training=False I want to keep dropout enabled but disable batch normalization (for the purpose of uncertainty estimation).

How can I achieve this with tf.function and the SavedModel format?

I am using tf-nightly.

### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_</details>"
60639,How to build Tensorflow 2.x with customerized protobuf version?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

TensorFlow 2.x

### Custom Code

No

### OS Platform and Distribution

Linux CentOS7

### Mobile device

_No response_

### Python version

3.8, 3.10

### Bazel version

using Bazlisk

### GCC/Compiler version

have both gcc 7.3.1, clang 7.0.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

CPU Only

### Current Behaviour?

I am writing MySQL UDF using TensorFlow C Library. But the MySQL8 has a conflict dependency of protobuf version with TensorFlow C Library. By default, MySQL8 needs protobuf 3.6.1 but TF uses higher version. I tried to modify the `workspace2.bzl` but the patch file is a problem to me. Since I do not have permission to change the building of MySQL8. How can I port a lower version of protobuf with TensorFlow 2.x? Thanks!

### Standalone code to reproduce the issue

```shell
The actual error I got is below

what():  This program requires version 3.9.0 of the Protocol Buffer runtime library, but the installed version is 3.6.1.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in ""bazel-out/k8-opt/bin/tensorflow/core/framework/tensor_shape.pb.cc"".)
```
```


### Relevant log output

_No response_</details>"
60637,TFLite model maker object detection training is too slow in colab,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

TFlite model maker 0.4.2

### Custom Code

No

### OS Platform and Distribution

colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In colab I have enabled 'fallback runtime version' since with Python 3.10 model maker was not installed successfully.
After enabling this option I am able to install it successfully.
Nut when I started a object detection training with 'efficientnet_lite4', it is showing `35/2122 [..............................] - ETA: 6:08:03 - loss: 0.7095 - accuracy: 0.5509` which is keep increasing.

Please help to solve this issue.


### Standalone code to reproduce the issue

```shell
My training command is as below-
`model = image_classifier.create(train_data, validation_data=validation_data, model_spec=model_spec.get('efficientnet_lite4'), batch_size=32, epochs=50, train_whole_model=True, use_augmentation=False, use_hub_library=False, model_dir='/content/drive/MyDrive/expi')`
```


### Relevant log output

_No response_</details>"
60630,TensorFlow not running,"Hello,

I am trying to create a BERT token using the tensorflow_hub library for python but whenever I run the code, it just gets stuck doesn't show any response.

Here's a screenshot to help:

![Screen Shot 2023-05-18 at 18 39 53](https://github.com/tensorflow/tensorflow/assets/55658291/9c7cdcb4-f1ee-4cd8-adf5-eae34a802fbe)
"
60629,control_flow_ops_test unit test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/ops/parallel_for:control_flow_ops_test fails occasionally due to difference exceeding tolerance.

See https://github.com/tensorflow/tensorflow/actions/runs/5012758324/jobs/8985082872#step:5:29789



### Standalone code to reproduce the issue

```shell
bazel test --build_tests_only --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=no_tensorflow_py_deps=true --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --local_test_jobs=64 --test_tag_filters=-nopip,-no_pip,-oss_serial,-no_oss,-oss_excluded,-v1only,-benchmark-test,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 -k -- //bazel_pip/tensorflow/... -//bazel_pip/tensorflow/compiler/tf2tensorrt/... -//bazel_pip/tensorflow/compiler/xrt/... -//bazel_pip/tensorflow/core/tpu/... -//bazel_pip/tensorflow/go/... -//bazel_pip/tensorflow/java/... -//bazel_pip/tensorflow/python/integration_testing/... -//bazel_pip/tensorflow/tools/toolchains/... -//bazel_pip/tensorflow/lite/... -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:conv_ops_test
```


### Relevant log output

```shell
AssertionError: 
Not equal to tolerance rtol=0.0001, atol=1e-05
Mismatched value: a is different from b. 
not close where = (array([0]), array([0]), array([0]), array([1]), array([4]), array([0]))
not close lhs = [0.]
not close rhs = [0.77603436]
not close dif = [0.77603436]
not close tol = [8.760343e-05]
dtype = float32, shape = (3, 3, 2, 12, 12, 3)
Mismatched elements: 1 / 7776 (0.0129%)
Max absolute difference: 0.77603436
Max relative difference: 1.
 x: array([[[[[[0.      , 0.      , 0.712515],
           [0.      , 0.889897, 0.      ],
           [0.      , 0.      , 0.      ],...
 y: array([[[[[[0.      , 0.      , 0.712515],
           [0.      , 0.889897, 0.      ],
           [0.      , 0.      , 0.      ],...
```
</details>"
60628,Numpy and tf experimental Numpy differ in vander matrix creation case for N=0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04 jammy

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6

### GPU model and memory

_No response_

### Current Behaviour?

The behaviour of `tf.experimental.numpy.vander` is different than `np.vander` for `N=0` where both value and shape of the output differ.

### Standalone code to reproduce the issue

```shell
import numpy as np        # 1.23.5
import tensorflow as tf   # 2.11.0

xn = np.array([1], dtype=np.int32)
x = tf.constant([1], dtype=tf.int32)
print(np.vander(xn, 0))
print()
print(tf.experimental.numpy.vander(x, 0))
```


### Relevant log output

```shell
[]

tf.Tensor([[1]], shape=(1, 1), dtype=int32)
```
</details>"
60627,unable to compile tensorflow c++ code using cmake ,"i followed this steps

1) i cloned the repo

2) cd tensorflow

3) mkdir examples

in examples folder i created helloworld.cc file and cmakelist.txt

this is cmakelist.txt file 

cmake_minimum_required(VERSION 3.8.0)
project(examples)

# Specify the path to the TensorFlow source directory
set(TENSORFLOW_SOURCE_DIR ""D:/github_issues/tensorflow/tensorflow"")

# Add the TensorFlow source directory to the CMake module path
list(APPEND CMAKE_MODULE_PATH ""${TENSORFLOW_SOURCE_DIR}/cmake"")

# Add the TensorFlow include directories
include_directories(${TENSORFLOW_SOURCE_DIR})
include_directories(${TENSORFLOW_SOURCE_DIR}/tensorflow/cc)
include_directories(${TENSORFLOW_SOURCE_DIR}/tensorflow/core)

# Build the hello-world executable
add_executable(hello-world hello-world.cc)
target_link_libraries(hello-world tensorflow)


after running the command cmake --build . --config Release

i got this error 

D:\github_issues\tensorflow\examples\hello-world.cc(1,10): fatal  error C1083: Cannot open include file: 'tensorflow/cc/client/cl
ient_session.h': No such file or directory [[D:\github_issues\tensorflow\examples\build\hello-world.vcxproj]]
![2](https://github.com/tensorflow/tensorflow/assets/85454586/3a8c3dae-fcd6-48c0-bf89-bb64af5bccfc)
![3](https://github.com/tensorflow/tensorflow/assets/85454586/61d90c15-2468-47cf-a074-e1190693bd4a)
![4](https://github.com/tensorflow/tensorflow/assets/85454586/359b81e6-30f9-43dc-bc04-f563726bc2c4)
![5](https://github.com/tensorflow/tensorflow/assets/85454586/6796388f-0f45-40b7-a369-47c6586e31c1)




"
60626,bug in MultiHeadAttention._compute_attention_mask,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf.12.0

### Custom Code

No

### OS Platform and Distribution

windows 11

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

There is a bug in MultiHeadAttention._compute_attention_mask when q, k or v ranks are more than 3.

Latest commit [67a8e6b](https://github.com/keras-team/keras/commit/67a8e6b96b1731ec732084f443345c70d87c31bf) on Jan 23

line 652:  auto_mask = query_mask[:, :, tf.newaxis]  # shape is [B, T, 1] 
fix:           auto_mask = tf.expand_dims(query_mask, axis =-1)

line 656:  mask = value_mask[:, tf.newaxis, :]  # shape is [B, 1, S]
fix:           mask = tf.expand_dims(key_mask, axis=-2)  # shape is [.., 1, S]

line 661: mask = key_mask[:, tf.newaxis, :]  # shape is [B, 1, S]
fix:          mask = tf.expand_dims(key_mask, axis=-2)  # shape is [.., 1, S]

### Standalone code to reproduce the issue

```shell
Submit any 4D input as query, key and value.
Error log below for input shapes [B, 12, 16, 128]
```


### Relevant log output

```shell
Message=Exception encountered when calling layer 'softmax' (type Softmax).

{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [64,12,1,16,16] vs. [64,12,1,12,16] [Op:AddV2]

Call arguments received by layer 'softmax' (type Softmax):
  • inputs=tf.Tensor(shape=(64, 12, 1, 16, 16), dtype=float32)
  • mask=tf.Tensor(shape=(64, 12, 1, 12, 16), dtype=bool)
```
</details>"
60624,metal delegate memory leak,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10 or 2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!
I  calls this three function loop on ios,and find memory leak
 use TFLGpuDelegateCreate --》 TfLiteInterpreterModifyGraphWithDelegate --》  TFLGpuDelegateDelete
 
![191225054-cfc30bd2-cc8d-4b42-97c5-131ffdbde9d3](https://github.com/tensorflow/tensorflow/assets/87115287/908eb0f8-ac63-4b4d-8959-8e377f75b3b4)


### Standalone code to reproduce the issue

```shell
From my own investigation it seems that the leak is coming from ModifyGraphWithDelegate, instead of Invoke.Attached is a screenshot of instruments, and a 5MB growth per call。
```


### Relevant log output

_No response_</details>"
60623,OOB read in IdentifySharedTensors,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.14.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/Compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A malicious model can manipulate external data, causing the input_tensor to be an unexpectedly large integer.

```c
// arena_planner.cc
void ArenaPlanner::IdentifySharedTensors() {
      ...
    const auto& tflite_node = graph_info_->node(i); 
    if (ShareFirstInputWithFirstOutputForNode(reg)) {
      int32_t input_tensor = tflite_node.inputs->data[0]; //  input_tensor maybe an unexpectedly large integer
      int32_t output_tensor = tflite_node.outputs->data[0];
      bool is_input_or_output_tensor = false;
       ...
      TfLiteAllocationType input_allocation_type =
          tensors[input_tensor].allocation_type;  // OOB read
```

[arena_planner_IdentifySharedTensors_oob.zip](https://github.com/tensorflow/tensorflow/files/11503622/arena_planner_IdentifySharedTensors_oob.zip)


### Standalone code to reproduce the issue

```shell
When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOS（coredump).


❯ ./benchmark_model --graph=./arena_planner_IdentifySharedTensors_oob.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [./arena_planner_IdentifySharedTensors_oob.tflite]
INFO: Loaded model ./arena_planner_IdentifySharedTensors_oob.tflite
[1]    8305 segmentation fault (core dumped)  ./benchmark_model --graph=./arena_planner_IdentifySharedTensors_oob.tflite
```
```


### Relevant log output

_No response_</details>"
60622,FPE in conv.h,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.14.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/Compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The `groups` variable and `filters_per_group` variable may be equal to 0, leading to a division by zero error.

```c
inline void Conv(const ConvParams& params, const RuntimeShape& input_shape,
                 const float* input_data, const RuntimeShape& filter_shape,
                 const float* filter_data, const RuntimeShape& bias_shape,
                 const float* bias_data, const RuntimeShape& output_shape,
                 float* output_data, const RuntimeShape& im2col_shape,
                 float* im2col_data) {
    ...
  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);
    ...
  const int input_depth = input_shape.Dims(3);
const int output_depth = MatchingDim(filter_shape, 0, output_shape, 3);
    ...
  const int groups = input_depth / filter_input_depth;
  TFLITE_DCHECK_EQ(input_depth % filter_input_depth, 0);
  const int filters_per_group = output_depth / groups; // FPE
	...
  for (int batch = 0; batch < batches; ++batch) {
    for (int out_y = 0; out_y < output_height; ++out_y) {
      const int in_y_origin = (out_y * stride_height) - pad_height;
      for (int out_x = 0; out_x < output_width; ++out_x) {
        const int in_x_origin = (out_x * stride_width) - pad_width;
        for (int out_channel = 0; out_channel < output_depth; ++out_channel) {
          auto group = out_channel / filters_per_group;  // FPE

```

[conv_divide_by_zero.zip](https://github.com/tensorflow/tensorflow/files/11503617/conv_divide_by_zero.zip)


### Standalone code to reproduce the issue

```shell
When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOS（coredump).


❯ ./benchmark_model --graph=./conv_divide_by_zero1.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [./conv_divide_by_zero1.tflite]
INFO: Loaded model ./conv_divide_by_zero1.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: The input model file size (MB): 0.000708
INFO: Initialized session in 0.68ms.
INFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
[1]    6968 floating point exception (core dumped)  ./benchmark_model --graph=./conv_divide_by_zero1.tflite
```
```


### Relevant log output

_No response_</details>"
60621,Infinite loop in the subgraph.cc,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.14.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/Compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When the `GetRegistrationFromOpCode` function parses a maliciously crafted model structure, if the `builtin_code` is `tflite::BuiltinOperator_CALL_ONCE`, it will enter an infinite loop in the subsequent inference process:`tflite::Subgraph::Invoke -> tflite::Subgraph::InvokeImpl -> tflite::Subgraph::OpInvoke -> tflite::ops::builtin::call_once_kernel::Eval`.

```c
// op_resolver.cc
TfLiteStatus GetRegistrationFromOpCode(
    const OperatorCode* opcode, const OpResolver& op_resolver,
    ErrorReporter* error_reporter, const TfLiteRegistration** registration) {
  TfLiteStatus status = kTfLiteOk;
  *registration = nullptr;
  auto builtin_code = GetBuiltinCode(opcode);
  int version = opcode->version();

  if (builtin_code > BuiltinOperator_MAX) {
    TF_LITE_REPORT_ERROR(
        error_reporter,
        ""Op builtin_code out of range: %d. Are you using old TFLite binary ""
        ""with newer model?"",
        builtin_code);
    status = kTfLiteError;
  } else if (builtin_code != BuiltinOperator_CUSTOM) {
    *registration = op_resolver.FindOp(builtin_code, version); // here
```

At the time of the crash, the call stack would look like this:

```
#67807 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67808 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67809 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67810 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67811 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67812 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67813 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67814 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67815 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67816 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67817 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67818 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67819 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67820 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67821 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67822 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67823 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67824 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67825 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67826 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67827 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67828 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67829 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67830 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67831 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67832 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67833 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67834 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67835 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67836 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67837 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67838 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67839 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67840 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67841 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67842 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67843 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67844 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67845 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67846 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67847 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67848 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67849 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67850 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67851 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67852 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
#67853 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
#67854 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
#67855 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
```

[subgraph_infinite_loop.zip](https://github.com/tensorflow/tensorflow/files/11503605/subgraph_infinite_loop.zip)


### Standalone code to reproduce the issue

```shell
When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOS（coredump).


❯ ./benchmark_model --graph=./subgraph_infinite_loop.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [./subgraph_infinite_loop.tflite]
INFO: Loaded model ./subgraph_infinite_loop.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: The input model file size (MB): 0.000488
INFO: Initialized session in 0.731ms.
INFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
[1]    6892 segmentation fault (core dumped)  ./benchmark_model --graph=./subgraph_infinite_loop.tflite
```
```


### Relevant log output

_No response_</details>"
60620,OOB write in PlanAllocations,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.14.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/Compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The `graph_info_->outputs` are externally controllable data, which may cause the `tensor_index` value to be `-1`, leading to an out-of-bounds write in the vector.

```
// simple_planner.c
TfLiteStatus ArenaPlanner::PlanAllocations() {

  ...
  
  // Keeps track of references to each tensor.
  std::vector<int> refcounts(num_tensors, 0);
  
  ...

  // We must make sure the output tensors are never overwritten. We do that by
  // artificially adding one to their ref-counts so they are never selected
  // for deallocation.
  for (int tensor_index : graph_info_->outputs()) {
    refcounts[tensor_index]++;     
  }

```

Due to an out-of-bounds write of 8 bytes forward, the original chunk size increases from` 0x20` to `0x100000021`, which corrupts the chunk structure. As a result, when the `PlanAllocations` function finishes and returns, triggering the vector's destructor, the chunk is freed, causing a security check in the glibc `_int_free` function to be triggered.

```c
nextchunk = chunk_at_offset(p, size);

/* Or whether the next chunk is beyond the boundaries of the arena.  */
if (__builtin_expect (contiguous (av)
          && (char *) nextchunk
          >= ((char *) av->top + chunksize(av->top)), 0))   
malloc_printerr (""double free or corruption (out)"");
```

[arena_planner_PlanAllocations_heap_overflow.zip](https://github.com/tensorflow/tensorflow/files/11503599/arena_planner_PlanAllocations_heap_overflow.zip)


### Standalone code to reproduce the issue

```shell
When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOS（coredump).


❯ ./benchmark_model --graph=arena_planner_PlanAllocations_heap_overflow.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [arena_planner_PlanAllocations_heap_overflow.tflite]
INFO: Loaded model arena_planner_PlanAllocations_heap_overflow.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
double free or corruption (out)
[1]    30982 abort (core dumped)  ./benchmark_model --graph=arena_planner_PlanAllocations_heap_overflow.tflite
```
```


### Relevant log output

_No response_</details>"
60619,null pointer dereference on GetInput,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.14.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/Compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The `GetInput` function may return a null pointer, similarly, the `GetOutput` function may also return a null pointer.

```c
// kernel_util.cc
const TfLiteTensor* GetInput(const TfLiteContext* context,
                             const TfLiteNode* node, int index) {
  return GetMutableInput(context, node, index);
}


inline TfLiteTensor* GetMutableInput(const TfLiteContext* context,
                                     const TfLiteNode* node, int index) {
  const int tensor_index = ValidateTensorIndexing(
      context, index, node->inputs->size, node->inputs->data);
  if (tensor_index < 0) {
    return nullptr;   // here
  }
  return GetTensorAtIndex(context, tensor_index);
}
```



Subsequent operations may cause null pointer dereferencing.

```c
// pad.cc
struct PadContext {
  PadContext(TfLiteContext* context, TfLiteNode* node) {
    input = GetInput(context, node, 0);  // null pointer
    paddings = GetInput(context, node, 1);
    if (NumInputs(node) == 3) {
      constant_values = GetOptionalInputTensor(context, node, 2);
    } else {
      constant_values = nullptr;
    }
    output = GetOutput(context, node, 0);
    dims = NumDimensions(input);    // null dereference
    ...
 }
        
 inline int NumDimensions(const TfLiteTensor* t) { return t->dims->size; }   
```



Similar use cases are also common. The axis of op_context may be null, leading to null pointer dereferencing when accessing op_context.axis->type.

```c
//reduce.cc
TfLiteStatus PrepareSimple(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

  OpContext op_context(context, node);
  TF_LITE_ENSURE_TYPES_EQ(context, op_context.axis->type, kTfLiteInt32);  // op_context.axis->type
   ...


struct OpContext {
  OpContext(TfLiteContext* context, TfLiteNode* node) {
    params = reinterpret_cast<TfLiteReducerParams*>(node->builtin_data);
    input = GetInput(context, node, 0);
    axis = GetInput(context, node, 1);  // axis may be null
	...
```
[getinput_getoutput_nullptr.zip](https://github.com/tensorflow/tensorflow/files/11503582/getinput_getoutput_nullptr.zip)


### Standalone code to reproduce the issue

```shell
When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOS（coredump).


❯ ./benchmark_model --graph=getinput_getoutput_nullptr.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [getinput_getoutput_nullptr.tflite]
INFO: Loaded model getinput_getoutput_nullptr.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[1]    29697 segmentation fault (core dumped)  ./benchmark_model --graph=getinput_getoutput_nullptr.tflite
```
```


### Relevant log output

_No response_</details>"
60618,"Libraries missing, according to console","Ok so I've got this issue where when I run my program I get a few different errors. I believe this likely has been brought up before but the issue that I did find, was all over the place with a ton of different edits and things which were frankly too difficult to follow, at least for me. I'm using tensorflow 2.10.0, cuda 11.7 and the corresponding cudnn, and I'm not sure if it's a version issue, or perhaps just a non-issue in general but I'll provide the error that I'm receiving:

      2023-05-17 12:32:37.272738: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
      2023-05-17 12:32:38.702176: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
      2023-05-17 12:32:38.702271: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
      2023-05-17 12:32:38.702297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
      2023-05-17 12:32:41.132568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
      Your kernel may have been built without NUMA support.
      2023-05-17 12:32:41.141491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
      Your kernel may have been built without NUMA support.
      2023-05-17 12:32:41.141557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
      Your kernel may have been built without NUMA support.

It's just that in order to get my program to work, as far as I can tell, I need tensorflow that has GPU support which was removed in the latest versions. Anyway, would really appreciate it if anyone knows how I can solve this please and thank you."
60617,PyCharm cannot resolve Keras and Bidirectional and TimeDistributed,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0=mkl_py38ha5c4042_0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10.0.22621 Build 22621

### Mobile device

_No response_

### Python version

3.8.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cudatoolkit=11.2 cudnn=8.1.0

### GPU model and memory

NVIDIA GeForce RTX 2060 - 6 GB GDDR6

### Current Behaviour?

I tried to `import` `tensorflow.keras` but `PyCharm` could not resolve it. So I tried to use `tensorflow.python.keras` (which is not advised), I could import a few modules but not `Bidirectional `or `TimeDistributed`. I have already deleted my environment and builed it from scratch with installing with pip and conda. 

![PyCharm](https://github.com/tensorflow/tensorflow/assets/35999180/b085e14b-6f20-49e4-afb8-494dfc71b622)

I tried the following issues, but sadly no succes: 
https://github.com/tensorflow/tensorflow/issues/54180 
https://github.com/microsoft/pylance-release/issues/1066 

### Another issue while trying to get tensorflow.version 
I tried to run `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`, but threw the following: 
> Traceback (most recent call last):
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\requests\compat.py"", line 11, in <module>
>     import chardet
> ModuleNotFoundError: No module named 'chardet'
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""<string>"", line 1, in <module>
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\tensorflow\__init__.py"", line 52, in <module>
>     from ._api.v2 import compat
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\tensorflow\_api\v2\compat\__init__.py"", line 37, in <module>
>     from . import v1
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\tensorflow\_api\v2\compat\v1\__init__.py"", line 31, in <module>
>     from . import compat
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\tensorflow\_api\v2\compat\v1\compat\__init__.py"", line 38, in <module>
>     from . import v2
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\tensorflow\_api\v2\compat\v1\compat\v2\__init__.py"", line 28, in <module>
>     from tensorflow._api.v2.compat.v2 import __internal__
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\tensorflow\_api\v2\compat\v2\__init__.py"", line 33, in <module>
>     from . import compat
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\tensorflow\_api\v2\compat\v2\compat\__init__.py"", line 38, in <module>
>     from . import v2
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\tensorflow\_api\v2\compat\v2\compat\v2\__init__.py"", line 37, in <module>
>     from tensorflow._api.v2.compat.v2 import distribute
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\tensorflow\_api\v2\compat\v2\distribute\__init__.py"", line 182, in <module>
>     from . import experimental
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\tensorflow\_api\v2\compat\v2\distribute\experimental\__init__.py"", line 10, in <module>
>     from . import rpc
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\tensorflow\_api\v2\compat\v2\distribute\experimental\rpc\__init__.py"", line 8, in <module>
>     from tensorflow.python.distribute.experimental.rpc.rpc_ops import Client
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\tensorflow\python\distribute\experimental\__init__.py"", line 22, in <module>
>     from tensorflow.python.distribute.failure_handling import failure_handling
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\tensorflow\python\distribute\failure_handling\failure_handling.py"", line 36, in <module>
>     from tensorflow.python.distribute.failure_handling import failure_handling_util
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\tensorflow\python\distribute\failure_handling\failure_handling_util.py"", line 19, in <module>
>     import requests
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\requests\__init__.py"", line 45, in <module>
>     from .exceptions import RequestsDependencyWarning
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\requests\exceptions.py"", line 9, in <module>
>     from .compat import JSONDecodeError as CompatJSONDecodeError
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\requests\compat.py"", line 13, in <module>
>     import charset_normalizer as chardet
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\charset_normalizer\__init__.py"", line 23, in <module>
>     from charset_normalizer.api import from_fp, from_path, from_bytes, normalize
>   File ""C:\Users\lilon\miniconda3\envs\TETB\lib\site-packages\charset_normalizer\api.py"", line 10, in <module>
>     from charset_normalizer.md import mess_ratio
> AttributeError: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)

### Standalone code to reproduce the issue

```shell
from tensorflow.keras.layers import Bidirectional, TimeDistributed
from tensorflow.python.keras.layers import Bidirectional, TimeDistributed
from keras.layers import Bidirectional, TimeDistributed
```


### Relevant log output

_No response_</details>"
60616,Custom shuffle layer leaks memory when run on Apple M1 GPU with `tensorflow-metal`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

macOS 13.0 (22A380)

### Mobile device

Apple M1

### Python version

3.10.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The following layer leaks memory during training with keras when running on an Apple M1 with `tensorflow-macos` and `tensorflow-metal` installed:

```
class Shuffle(keras.layers.Layer):
    def call(self, inputs):
        shape = tf.concat([tf.shape(inputs)[:-1], [1]], axis=0)
        rnd = tf.argsort(tf.random.uniform(shape), axis=1)
        return tf.gather_nd(inputs, rnd, batch_dims=2)
```

When run on the CPU alone it does not leak memory


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_datasets as tfds
from tensorflow_datasets.testing.mocking import mock_data

import numpy as np


class Shuffle(keras.layers.Layer):
    def call(self, inputs):
        shape = tf.concat([tf.shape(inputs)[:-1], [1]], axis=0)
        rnd = tf.argsort(tf.random.uniform(shape), axis=1)
        return tf.gather_nd(inputs, rnd, batch_dims=2)


def build_leaky_model(input_shape):
    input = keras.Input(input_shape)

    x = Shuffle()(input)

    x = layers.GlobalAveragePooling2D()(x)

    x = layers.Flatten()(x)

    output = layers.Dense(1, activation=""sigmoid"")(x)

    return keras.Model(
        input,
        output,
    )


epochs = 1000

data_set = tf.data.Dataset.from_generator(
    lambda: (
        (
            np.ones(shape=(1000, 5000, 1), dtype=np.uint8),
            i % 2,
        )
        for i in range(200)
    ),
    output_types=(tf.uint8, tf.int32),
    output_shapes=((1000, 5000, 1), ()),
)

data_set = data_set.batch(10)

model = build_leaky_model((1000, 5000, 1))

model.compile(
    optimizer=keras.optimizers.Adam(1e-3),
    loss=""binary_crossentropy"",
    metrics=[""accuracy""],
)

history = model.fit(
    data_set,
    epochs=epochs,
)
```
```


### Relevant log output

_No response_</details>"
60615,Tensorflow 2.10.0 conflicts with jupyterlab 4.0.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.10.1

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Using Conda:
1. Install jupyterlab
2. Run jupyterlab. Notice it works.
3. Install TensorFlow
4. Run jupyterlab: Notice it doesn't start, and an error message pop-up; The c_types package is missing.

Uninstalling Tensorflow fixes the problem, allowing jupyterlab to run again.
Related: https://github.com/jupyterlab/jupyterlab/issues/14558

### Standalone code to reproduce the issue

```shell
conda create --name my_env
activate my_env

conda Install jupyterlab=4.0.0
jupyter lab

conda install TensorFlow=2.10.0
jupyter lab
```


### Relevant log output

```shell
jupyter lab ImportError: DLL load failed while importing _ctypes: ...
```
</details>"
60613,Flowmode,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
60612,TFLite Model maker installation issue in google colab notebook,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.3

### Custom Code

No

### OS Platform and Distribution

linux

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

V100

### Current Behaviour?

I am trying to install tflite model maker in colab using command `!pip install tflite-model-maker` and it is taking so much disk space to install.
I have 55 GB disk space left but installation is stopped in between due to no space error-
`ERROR: Could not install packages due to an OSError: [Errno 28] No space left on device`
I am attaching here logs of the installation.
[model-maker-issue-colab.txt](https://github.com/tensorflow/tensorflow/files/11494698/model-maker-issue-colab.txt)


### Standalone code to reproduce the issue

```shell
Run following command in colab-
!pip install tflite-model-maker
```


### Relevant log output

```shell
ERROR: Could not install packages due to an OSError: [Errno 28] No space left on device
```
</details>"
60609,TF-Lite is 4x slower than Tensorflow on MacOS (and 2x slower in Colab),"Problem - Running a model with TFLite is 4x slower than running with tensorflow on my M1 MacOS.  

### 1. System information

- OS Platform and Distribution: **MacOS Ventura 13.2, on Apple M1 Macbook Air**
- TensorFlow installation (pip package or built from source): **2.9.1, from pip**

### 2. Code

Running the code in [this Colab notebook](https://colab.research.google.com/drive/1KFYbdxZicG2sQlw2kD63TM63gGm5VQJC?usp=sharing ), on MacOS, gives:

```
Tensorflow Model execution stats:
  Fraction of time spent detecting: 81%
  Average time per frame (ms): 19
  Average FPS: 52.10
  Median time per frame (ms): 19
  Median FPS: 52.33
TFLite Model execution stats:
  Fraction of time spent detecting: 95%
  Average time per frame (ms): 86
  Average FPS: 11.69
  Median time per frame (ms): 85
  Median FPS: 11.74
  
Tensorflow is 4.46x times faster than TFLite
```
... Clearly TFLite is much slower.

If I run it it in Colab, notebook, TFLite is still about 1.8x slower.

What is going on here?  I would expect TFLite to be faster.  Can this be fixed by adjusting some flags somewhere?

This has been noticed before - [see Stackoverflow question](https://stackoverflow.com/questions/54093424/why-is-tensorflow-lite-slower-than-tensorflow-on-desktop?rq=2).

**If this slowdown is unavoidable - is there some other way to serialize tensorflow functions so that they can be loaded again without slowdown?**"
60608,[MKL] Manylinux wheel issue on clang,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.13rc0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.3

### GCC/Compiler version

Clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When building with clang 16, config=mkl, we can successfully built. But when we tried to use auditwheel package to make it manylinux_2014 compatible, we faced error like:
```
_solib_k8/_U_S_Stensorflow_Stsl_Smkl_Cmkl_Ulibs_Ulinux___Uexternal_Sllvm_Uopenmp/libiomp5.so is manylinux_2_17(aka manylinux2014) compliant.
tensorflow/libtensorflow_cc.so.2 is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
libiomp5.so
libc.so.6	offending versions: GLIBC_2.28, GLIBC_2.27
libm.so.6	offending versions: GLIBC_2.23, GLIBC_2.29, GLIBC_2.27
tensorflow/libtensorflow_framework.so.2 is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libm.so.6	offending versions: GLIBC_2.29, GLIBC_2.27
libiomp5.so
libc.so.6	offending versions: GLIBC_2.18
tensorflow/compiler/tf2tensorrt/_pywrap_py_utils.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
tensorflow/compiler/tf2xla/ops/_xla_ops.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
tensorflow/core/kernels/libtfkernel_sobol_op.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
libm.so.6	offending versions: GLIBC_2.29
tensorflow/include/external/llvm_openmp/libiomp5.so is manylinux_2_17(aka manylinux2014) compliant.
tensorflow/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libm.so.6	offending versions: GLIBC_2.27
libtensorflow_framework.so.2
tensorflow/lite/python/analyzer_wrapper/_pywrap_analyzer_wrapper.so is manylinux_2_17(aka manylinux2014) compliant.
tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
libm.so.6	offending versions: GLIBC_2.29, GLIBC_2.27
tensorflow/lite/python/metrics/_pywrap_tensorflow_lite_metrics_wrapper.so is manylinux_2_17(aka manylinux2014) compliant.
tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
libm.so.6	offending versions: GLIBC_2.29, GLIBC_2.27
tensorflow/python/_pywrap_dtensor_device.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
libiomp5.so
_pywrap_tensorflow_internal.so
tensorflow/python/_pywrap_mlir.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/_pywrap_parallel_device.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/_pywrap_py_exception_registry.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_cc.so.2
_pywrap_tensorflow_internal.so
libtensorflow_framework.so.2
libiomp5.so
tensorflow/python/_pywrap_quantize_training.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
_pywrap_tensorflow_internal.so
libtensorflow_framework.so.2
libiomp5.so
tensorflow/python/_pywrap_sanitizers.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
libiomp5.so
tensorflow/python/_pywrap_tensorflow_internal.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_cc.so.2
libbfloat16.so.so
libiomp5.so
libtensorflow_framework.so.2
tensorflow/python/_pywrap_tfcompile.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
_pywrap_tensorflow_internal.so
libiomp5.so
libtensorflow_framework.so.2
libtensorflow_cc.so.2
tensorflow/python/_pywrap_tfe.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
_pywrap_tensorflow_internal.so
libtensorflow_cc.so.2
libtensorflow_framework.so.2
libiomp5.so
tensorflow/python/_pywrap_toco_api.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
libiomp5.so
tensorflow/python/flags_pybind.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
_pywrap_tensorflow_internal.so
libtensorflow_framework.so.2
libiomp5.so
tensorflow/python/autograph/impl/testing/pybind_for_testing.so is manylinux_2_17(aka manylinux2014) compliant.
tensorflow/python/client/_pywrap_debug_events_writer.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
_pywrap_tensorflow_internal.so
libtensorflow_framework.so.2
libiomp5.so
tensorflow/python/client/_pywrap_device_lib.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/client/_pywrap_events_writer.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
libiomp5.so
_pywrap_tensorflow_internal.so
tensorflow/python/client/_pywrap_tf_session.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
libiomp5.so
_pywrap_tensorflow_internal.so
tensorflow/python/data/experimental/service/_pywrap_server_lib.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/data/experimental/service/_pywrap_utils.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
libm.so.6	offending versions: GLIBC_2.29
_pywrap_tensorflow_internal.so
tensorflow/python/framework/_dtypes.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
libiomp5.so
_pywrap_tensorflow_internal.so
tensorflow/python/framework/_errors_test_helper.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
_pywrap_tensorflow_internal.so
libm.so.6	offending versions: GLIBC_2.29
libtensorflow_framework.so.2
libiomp5.so
tensorflow/python/framework/_op_def_library_pybind.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
_pywrap_tensorflow_internal.so
libiomp5.so
libtensorflow_framework.so.2
tensorflow/python/framework/_op_def_registry.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
libiomp5.so
tensorflow/python/framework/_op_def_util.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
_pywrap_tensorflow_internal.so
libtensorflow_framework.so.2
libiomp5.so
tensorflow/python/framework/_proto_comparators.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/framework/_python_memory_checker_helper.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
libiomp5.so
tensorflow/python/framework/_pywrap_python_api_dispatcher.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
libiomp5.so
_pywrap_tensorflow_internal.so
tensorflow/python/framework/_pywrap_python_api_info.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
libiomp5.so
_pywrap_tensorflow_internal.so
tensorflow/python/framework/_pywrap_python_api_parameter_converter.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
libiomp5.so
tensorflow/python/framework/_pywrap_python_op_gen.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
libiomp5.so
tensorflow/python/framework/_test_metrics_util.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
libiomp5.so
tensorflow/python/framework/fast_tensor_util.so is manylinux_2_17(aka manylinux2014) compliant.
tensorflow/python/grappler/_pywrap_tf_cluster.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
_pywrap_tensorflow_internal.so
libtensorflow_framework.so.2
libiomp5.so
tensorflow/python/grappler/_pywrap_tf_item.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/grappler/_pywrap_tf_optimizer.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
libiomp5.so
tensorflow/python/lib/core/_pywrap_custom_casts.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/lib/core/_pywrap_float8.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/lib/core/_pywrap_py_func.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
libiomp5.so
_pywrap_tensorflow_internal.so
tensorflow/python/lib/io/_pywrap_file_io.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
_pywrap_tensorflow_internal.so
libtensorflow_framework.so.2
tensorflow/python/lib/io/_pywrap_record_io.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
libiomp5.so
_pywrap_tensorflow_internal.so
tensorflow/python/platform/_pywrap_cpu_feature_guard.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
libiomp5.so
tensorflow/python/platform/_pywrap_stacktrace_handler.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
_pywrap_tensorflow_internal.so
libtensorflow_framework.so.2
tensorflow/python/platform/_pywrap_tf2.so is manylinux_2_17(aka manylinux2014) compliant.
tensorflow/python/profiler/internal/_pywrap_profiler.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
_pywrap_tensorflow_internal.so
libtensorflow_framework.so.2
libiomp5.so
tensorflow/python/profiler/internal/_pywrap_traceme.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/saved_model/pywrap_saved_model.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
_pywrap_tensorflow_internal.so
libtensorflow_framework.so.2
tensorflow/python/util/_pywrap_checkpoint_reader.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/util/_pywrap_determinism.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/util/_pywrap_kernel_registry.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
_pywrap_tensorflow_internal.so
libtensorflow_framework.so.2
tensorflow/python/util/_pywrap_nest.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
_pywrap_tensorflow_internal.so
libtensorflow_framework.so.2
libiomp5.so
tensorflow/python/util/_pywrap_stat_summarizer.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
_pywrap_tensorflow_internal.so
libiomp5.so
libtensorflow_framework.so.2
tensorflow/python/util/_pywrap_tensor_float_32_execution.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/util/_pywrap_tfprof.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
libiomp5.so
_pywrap_tensorflow_internal.so
tensorflow/python/util/_pywrap_transform_graph.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/util/_pywrap_util_port.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/util/_pywrap_utils.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
_pywrap_tensorflow_internal.so
libtensorflow_framework.so.2
tensorflow/python/util/_tf_stack.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libiomp5.so
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
tensorflow/python/util/fast_module_type.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libtensorflow_framework.so.2
_pywrap_tensorflow_internal.so
libiomp5.so
tensorflow/tsl/python/lib/core/libbfloat16.so.so is manylinux_2_17(aka manylinux2014) compliant.
tensorflow/tsl/python/lib/core/pywrap_bfloat16.so is not manylinux_2_17(aka manylinux2014) compliant because it links the following forbidden libraries:
libbfloat16.so.so
```


### Standalone code to reproduce the issue

```shell
Bazel build option:

build --copt=-O3 --features=-layering_check --copt=-Wno-gnu-offsetof-extensions --copt=-Wformat --copt=-Wformat-security --copt=-fstack-protector --copt=-fPIC --copt=-fpic --linkopt=-Wl,-z,noexecstack --linkopt=-Wl,-z,relro --linkopt=-Wl,-z,now --linkopt=-fstack-protector --config=mkl --copt=-march=sandybridge
```


### Relevant log output

_No response_</details>"
60607,XLA CUBIN: CUDA_ERROR_OUT_OF_MEMORY,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8 and 3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuDNN version 8600

### GPU model and memory

NVIDIA A100 40GB PCIe, NVIDIA Tesla V100

### Current Behaviour?

Hi, 
I'm trying to fine-tune a Bert based text-classification model.
If I add `model.compile(jit_compile=True)`, I receive. 
```
Node: 'StatefulPartitionedCall'
Failed to load in-memory CUBIN: CUDA_ERROR_OUT_OF_MEMORY: out of memory
         [[{{node StatefulPartitionedCall}}]] [Op:__inference_train_function_50056]

```

When I set `jit_compile=False`, everything runs fine. In the attached archive, you can find debug XLA HLOs.
[xla_debug.zip](https://github.com/tensorflow/tensorflow/files/11487102/xla_debug.zip)


I faced the issue for TF-hub models, tf_model_official ones and for one created manually using functional API.
I also faced the issue on A100 and on V100 GPUs, with python 3.8 and 3.10 respectively. 

Is this a known issue? Am I missing something?


### Standalone code to reproduce the issue

```python
import logging

import absl.logging
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_models as tfm
from absl import flags, app
from keras import mixed_precision
from keras.utils.dataset_utils import split_dataset
from keras.utils.tf_utils import can_jit_compile

NUM_CLASSES = 104
PRNG_SEED = 42
tf.random.set_seed(PRNG_SEED)

logging.basicConfig(level=logging.DEBUG)
tf.get_logger().setLevel(""DEBUG"")
absl.logging.set_verbosity(absl.logging.converter.ABSL_DEBUG)

FLAGS = flags.FLAGS

flags.DEFINE_integer(""batch_size"", default=32, help=""batch size"")
flags.DEFINE_integer(""epochs"", default=1, help=""number of epochs"")


def warmup_schedule(num_training_samples):
    steps_per_epoch = int(num_training_samples / FLAGS.batch_size)
    num_train_steps = steps_per_epoch * FLAGS.epochs
    warmup_steps = int(0.4 * num_train_steps)
    initial_learning_rate = 1e-3
    
    linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(
        initial_learning_rate=initial_learning_rate,
        end_learning_rate=1e-4,
        decay_steps=num_train_steps,
    )
    return tfm.optimization.lr_schedule.LinearWarmup(
        warmup_learning_rate=5e-3,
        after_warmup_lr_sched=linear_decay,
        warmup_steps=warmup_steps,
    )


@tf.function(reduce_retracing=True, jit_compile=can_jit_compile())
def macro_double_soft_f1(y, y_hat):
    """"""Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).
    Use probability values instead of binary predictions.
    This version uses the computation of soft-F1 for both positive and negative class for each label.

    Args:
        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)
        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)

    Returns:
        cost (scalar Tensor): value of the cost function for the batch
    """"""
    y = tf.cast(y, tf.bfloat16)
    y_hat = tf.cast(y_hat, tf.bfloat16)
    tp = tf.reduce_sum(y_hat * y, axis=0)
    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)
    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)
    tn = tf.reduce_sum((1 - y_hat) * (1 - y), axis=0)
    soft_f1_class1 = 2 * tp / (2 * tp + fn + fp + 1e-16)
    soft_f1_class0 = 2 * tn / (2 * tn + fn + fp + 1e-16)
    # reduce 1 - soft-f1_class1 in order to increase soft-f1 on class 1
    cost_class1 = 1 - soft_f1_class1
    # reduce 1 - soft-f1_class0 in order to increase soft-f1 on class 0
    cost_class0 = 1 - soft_f1_class0
    # take into account both class 1 and class 0
    cost = 0.5 * cost_class1 + cost_class0
    # average on all labels
    macro_cost = tf.reduce_mean(cost)
    return macro_cost


def main(_):
    # Unfortunately, I can't share the actual dataset due to GDPR.
    pickled_dataset = dict(
        input_word_ids=tf.constant(
            [
                [101, 15570, 15143, 49393, 15009, 40651, 0],
                [101, 18610, 23251, 107, 316053, 84805, 15954],
            ],
            dtype=tf.int32,
        ),
        input_mask=tf.constant(
            [[1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1]], dtype=tf.int32
        ),
        input_type_ids=tf.constant(
            [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], dtype=tf.int32
        ),
        # in reality, here I have multi-hot encoded labels.
        events=tf.zeros([2, NUM_CLASSES], dtype=tf.int32),
        return_events=tf.zeros([2, NUM_CLASSES], dtype=tf.int32)
    )
    
    num_training_samples = int(len(pickled_dataset[""input_word_ids""]) * 0.8)
    dataset = (
        np.asarray(pickled_dataset[""input_word_ids""]),
        np.asarray(pickled_dataset[""input_mask""]),
        np.asarray(pickled_dataset[""input_type_ids""]),
        np.asarray(pickled_dataset[""events""]),
        np.asarray(pickled_dataset[""return_events""]),
    )
    train_ds, val_ds = split_dataset(
        dataset, left_size=0.8, shuffle=True, seed=PRNG_SEED
    )
    
    def group_tuples(*args):
        return (
            dict(
                input_word_ids=tf.cast(args[0], tf.int32),
                input_mask=tf.cast(args[1], tf.int32),
                input_type_ids=tf.cast(args[2], tf.int32),
            ),
            (tf.cast(args[3], tf.int32), tf.cast(args[4], tf.int32)),
        )
    
    train_ds = (
        train_ds.batch(FLAGS.batch_size)
        .map(group_tuples)
        .cache()
        .prefetch(tf.data.AUTOTUNE)
    )
    val_ds = (
        val_ds.batch(FLAGS.batch_size)
        .map(group_tuples)
        .cache()
        .prefetch(tf.data.AUTOTUNE)
    )
    # ----------------------------------------
    if can_jit_compile():
        tf.config.optimizer.set_jit(""autoclustering"")
    mixed_precision.set_global_policy(""mixed_bfloat16"")
    word_ids = tf.keras.layers.Input(
        shape=(None,), dtype=tf.int32, name=""input_word_ids""
    )
    mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=""input_mask"")
    type_ids = tf.keras.layers.Input(
        shape=(None,), dtype=tf.int32, name=""input_type_ids""
    )
    x = hub.KerasLayer(""https://tfhub.dev/google/LaBSE/2"")(
        {
            ""input_word_ids"": word_ids,
            ""input_mask"": mask,
            ""input_type_ids"": type_ids,
        }
    )[""pooled_output""]
    events = tf.keras.layers.Dense(NUM_CLASSES, activation=""sigmoid"", name=""events"")(x)
    return_events = tf.keras.layers.Dense(
        NUM_CLASSES, activation=""sigmoid"", name=""return_events""
    )(x)
    model = tf.keras.Model(
        inputs={
            ""input_word_ids"": word_ids,
            ""input_mask"": mask,
            ""input_type_ids"": type_ids,
        },
        outputs=[events, return_events],
    )
    model.compile(
        optimizer=tf.keras.optimizers.Adam(
            learning_rate=warmup_schedule(num_training_samples),
            jit_compile=can_jit_compile(),
        ),
        metrics={
            ""events"": [
                tf.keras.metrics.TruePositives(),
                tf.keras.metrics.TrueNegatives(),
                tf.keras.metrics.FalsePositives(),
                tf.keras.metrics.FalseNegatives(),
            ],
            ""return_events"": [
                tf.keras.metrics.TruePositives(),
                tf.keras.metrics.TrueNegatives(),
                tf.keras.metrics.FalsePositives(),
                tf.keras.metrics.FalseNegatives(),
            ],
        },
        loss={""events"": macro_double_soft_f1, ""return_events"": macro_double_soft_f1},
        # This is the culprit.
        jit_compile=can_jit_compile(),
    )
    # ---------------------------------------
    model.fit(train_ds, validation_data=val_ds, epochs=FLAGS.epochs)
    model.save_weights(""weights.keras"")


if __name__ == ""__main__"":
    app.run(main)
```


### Relevant log output

```shell
Node: 'StatefulPartitionedCall'
Failed to load in-memory CUBIN: CUDA_ERROR_OUT_OF_MEMORY: out of memory
         [[{{node StatefulPartitionedCall}}]] [Op:__inference_train_function_50056]
2023-05-16 14:44:22.972682: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2023-05-16 14:44:23.019609: F tensorflow/tsl/framework/bfc_allocator.cc:700] Check failed: h != kInvalidChunkHandle 
Fatal Python error: Aborted

Current thread 0x00007f0239acd740 (most recent call first):
  Garbage-collecting
  <no Python frame>
Aborted (core dumped)
```
</details>"
60606,Calling `Model.predict` in graph mode is not supported when the `Model` instance was constructed with eager mode enabled. Please construct your `Model` instance in graph mode or call `Model.predict` with eager mode enabled.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.6.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

We are using tensorflow version 2 where eager model is enabled by default. 

while predicting from trained model we are getting this error. 
```
Calling `Model.predict` in graph mode is not supported when the `Model` instance was constructed with eager mode enabled. Please construct your `Model` instance in graph mode or call `Model.predict` with eager mode enabled.
```

As a workaround, for time being we resolved this issue, by disabling eager mode, and load the model inside graph mode.

We would like to know, how can we resolve this issue without disabling the eager mode. 

### Standalone code to reproduce the issue

```shell
This is not an issue, which can be reproduced as its happening only for few requests.
```


### Relevant log output

_No response_</details>"
60605,tf sometims return constant value," ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8.4

### OS Platform and Distribution

ubuntu18.04

### Bazel version

5.2.0

### GCC/Compiler version

gcc-11.3.0

### Current Behaviour?

my project use a fm model for ctr prediction (click through rate). I notice sometimes tf always return a constant value 1 ,  if want result became  valid, must restart program, or reload model.  I'm sure I use a same model."
60604,"Lambda Layer, OperatorNotAllowedInGraphError","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I created a ANN model using Keras Lambda layer to implement mathematical calculation for the output of ANN.
Function of the Lambda layer was defined by a long code of calculation. 
I mentioned @tf.fuction, but the Lambda layer didn't work well because of OperatorNotAllowdInGraphError.
The error says that 
""OperatorNotAllowedInGraphError: Exception encountered when calling layer ""physics"" (type Lambda).
Using a symbolic `tf.Tensor` as a Python `bool` is not allowed: AutoGraph is disabled in this function. Try decorating it directly with @tf.function.""

I expect to do not only functions provided by tensorflow, such as tf.math. , but also other functions such  if-else function, indexing, and other works, which did not work well, in Lambda layer.
Maybe changing datatype of input_all0 from KerasTensor to Numpy could be helpful, but I did not find the way.



### Standalone code to reproduce the issue

```shell
@tf.function(experimental_compile=True, autograph=False, experimental_relax_shapes=True)
def physics(input_all):
    
    global ts
    global node_idx
    global ESB, glass_double, glass_triple
    global capacity, cap_outerwall, cap_air, cap_window_2, cap_window_3, cap_bipv, cap_innerwall
    global outerwall_idx, air_idx, window_2_idx, window_3_idx, bipv_idx, innerwall_idx
    global outerwall_layers, window_3_layers, bipv_layers, innerwall_layers
    
   
    tf.executing_eagerly()

    iOAT = input_all[None,0,0]
    iID = input_all[None,0,2]
    iIdsky = input_all[None,0,3]
    iIdgr = input_all[None,0,4]

    if input_all[None,0,6] < 90: igroup = 5
    if input_all[6,] < 80: igroup = 4
    if input_all[6,] < 70: igroup = 3
    if input_all[6,] < 60: igroup = 2
    if input_all[6,] < 50: igroup = 1
    if input_all[6,] < 40: igroup = 0
    
    X00 = tf.constant(input_all[None,0,8:-4])
    X0 = np.zeros(19)
    nn = 0
    
    for n in node_idx:
        X0[nn] = X00[0, n]
        nn += 1
    
    h_in = input_all[None,0,-4]
    h_outg = input_all[None,0,-3]
    h_outw = input_all[None,0,-2]
    h_ca = input_all[None,0,-1]
    
    hin_win = h_in
    hin_wall = h_in
    hin_pv = h_in
    hin_Innerwall = h_in
    hca_win1 = h_ca
    hca_win2 = h_ca
    
    hout_win = h_outg
    hout_pv = h_outg
    hout_wall = h_outw  
    # ....
    Ximp = np.linalg.solve( np.eye(len(capacity)) - amat*ts, X0 + bvector*ts)
    return Ximp

input1 = Input(shape=size1, name = 'input_parameter')
flatten_layer = Flatten()(input1)

hidden1 = Dense(size[0], activation = 'relu', name = 'hidden1_1')(flatten_layer)
output_parm = Dense(4, name = 'output_parameter')(hidden1)

hidden2 = Dense(size[1], name = 'hidden2_1')(flatten_layer)
output_VT = Dense(9, name = 'output_VirtualT')(hidden2)
input2 = Input(shape=size2, name = 'input_MT')
input_all0 = Concatenate(name = 'input_all')([input2, output_VT, output_parm])
   
output_T = Lambda(lambda x: physics(x), name=""physics"")(input_all0)
ann_ = Model(inputs=[input1, input2], outputs=output_T)
```


### Relevant log output

_No response_</details>"
60603,`decode_image` can not load (or check properly) the bmp format ,"### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

On a dataset that contains bmp format files, the `tf.io.decode_image` or `tf.io.decode_bmp` method can not load the bmp files for some samples. This samples can be converted to png format but as I like to use built in decoding method for bmp image, I'm trying as follows but facing the issue for some bmp files and for some cases it loads correctly. 




### Standalone code to reproduce the issue

[samples](https://drive.google.com/file/d/1B1w4-9EsSwvGukA_9yDZxZN7v1PeQmdm/view?usp=share_link)

```
image = tf.io.read_file('1.bmp')
image = tf.io.decode_image(image, channels=3, expand_animations=False)
```

```
InvalidArgumentError: {{function_node __wrapped__DecodeImage_device_/job:localhost/replica:0/task:0/device:CPU:0}} Number of channels inherent in the image must be 1, 3 or 4, was 0 [Op:DecodeImage]
```
"
60601,tf performance deteriorated very sharply when qps arrived a level,"### Source

source

### Tensorflow Version

tf 2.8.4


### OS Platform and Distribution

ubuntu18.04


### Python version

3.10

### Bazel version

5.2.0

### GCC/Compiler version

gcc-11.3.0



### Current Behaviour?

our project use tf2.8.4. we meet two problem, 
1. performance deteriorated very sharply, does this normally?
50q/s    35ms 99%
100q/s   62ms 99%
120q/s   75ms  99%
130q/s   100ms 99%
140q/s   150ms 99%
150q/s   370ms 99%

2. qps cannot archive to expected level, our cpu model is Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz, two cpu, so we have 24 physical cores. assume each request cost 100ms,  qps at least can archive 1000 / 100 * 24 = 240qps,  but permance is so bad when qps archive 150.

ps. our model was fm with 4000000 * 6 matrix"
60599,Weird memory usage of shuffling in `tf.data.Dataset` ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf 2.12

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hi, I've read the [tf.data doc](https://www.tensorflow.org/guide/data#randomly_shuffling_input_data), which says using a large `buffer size` in data shuffling is not recommended, but still the shuffling behavior costs way more memory than I would expect. For example, I expect the full shuffling of 50,000,000 `int` data may only use 1 GB of memory, but the following code after `print` essentially uses 10 GB.

This leads to some practical concerns. If I set `buffer_size=1024` in data shuffling, would the *actual* memory usage of the buffer size be 10 times that of 1024 elements?

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

data_size = 50000000
tf_dataset = tf.data.Dataset.from_tensor_slices(np.arange(data_size))
tf_dataset = iter(tf_dataset.shuffle(data_size))
print(next(tf_dataset))
```


### Relevant log output

```shell
<tf.Tensor: shape=(), dtype=int64, numpy=24774043>
```
</details>"
60598,Issue while installing tensorflow-lite-maker on Google Colab,"While trying to install and run tensorflow-lite-maker on google Colab I encountered many issues.
I even tried following the example notebooks shown in the documentation [here](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#scrollTo=qhl8lqVamEty) and encountered similar issues.
Find output here:
```
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following NEW packages will be installed:
  libportaudio2
0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.
Need to get 65.4 kB of archives.
After this operation, 223 kB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 libportaudio2 amd64 19.6.0-1build1 [65.4 kB]
Fetched 65.4 kB in 4s (17.5 kB/s)
debconf: unable to initialize frontend: Dialog
debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)
debconf: falling back to frontend: Readline
debconf: unable to initialize frontend: Readline
debconf: (This frontend requires a controlling tty.)
debconf: falling back to frontend: Teletype
dpkg-preconfigure: unable to re-open stdin: 
Selecting previously unselected package libportaudio2:amd64.
(Reading database ... 122519 files and directories currently installed.)
Preparing to unpack .../libportaudio2_19.6.0-1build1_amd64.deb ...
Unpacking libportaudio2:amd64 (19.6.0-1build1) ...
Setting up libportaudio2:amd64 (19.6.0-1build1) ...
Processing triggers for libc-bin (2.31-0ubuntu9.9) ...
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 577.3/577.3 kB 14.7 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 840.9/840.9 kB 22.9 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 25.4 MB/s eta 0:00:00
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 10.0 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.0/128.0 kB 14.2 MB/s eta 0:00:00
ERROR: Could not find a version that satisfies the requirement tflite-support>=0.4.2 (from tflite-model-maker) (from versions: 0.1.0a0.dev3, 0.1.0a0.dev4, 0.1.0a0.dev5, 0.1.0a0, 0.1.0a1)
ERROR: No matching distribution found for tflite-support>=0.4.2 (from tflite-model-maker)
ERROR: Could not find a version that satisfies the requirement opencv-python-headless==4.1.2.30 (from versions: 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68, 4.7.0.72)
ERROR: No matching distribution found for opencv-python-headless==4.1.2.30
```

The issue seems to be with python 3.10 version, unfortunately it doesn't seem possible to downgraded to previous versions of python on colab without running into errors. Please advise."
60597,Tensorflow global random generator is unable to generate integers,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0-dev20230514

### Custom Code

Yes

### OS Platform and Distribution

macOS Ventura 13.2.1

### Mobile device

_No response_

### Python version

3.10.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M2

### Current Behaviour?

An error is raised when trying to generate a random integer using the global random generator. Does not occur for float data types

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
generator = tf.random.get_global_generator()
generator.uniform(shape=(), minval=0, maxval=3, dtype=tf.int32)
```


### Relevant log output

```shell
2023-05-15 11:34:30.621515: I tensorflow/core/common_runtime/executor.cc:1210] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: minval must be 0-D, got shape [2]
         [[{{node StatelessRandomUniformIntV2}}]]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/esten/miniconda3/envs/ml/lib/python3.10/site-packages/tensorflow/python/ops/stateful_random_ops.py"", line 788, in uniform
    return gen_stateless_random_ops_v2.stateless_random_uniform_int_v2(
  File ""/Users/esten/miniconda3/envs/ml/lib/python3.10/site-packages/tensorflow/python/ops/gen_stateless_random_ops_v2.py"", line 477, in stateless_random_uniform_int_v2
    _ops.raise_from_not_ok_status(e, name)
  File ""/Users/esten/miniconda3/envs/ml/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 6577, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StatelessRandomUniformIntV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} minval must be 0-D, got shape [2]
         [[{{node StatelessRandomUniformIntV2}}]] [Op:StatelessRandomUniformIntV2] name:
```
</details>"
60596,TfLiteGpuDelegateBindGlBufferToTensor,"Hi, dear developers

I would like to ask if ""TfLiteGpuDelegateBindGlBufferToTensor"" can be used in implementing DelegateV2 (tflite version 2.10), as I hope to avoid cpu-gpu memory copy letting textures be the input and output of tflite inference.

Below is my pseudo code, could you please let me know if the implementation is correct?

#######################################################
TfLiteStatus tf_status = kTfLiteOk;
std::unique_ptr<tflite::Interpreter> tf_interpreter_;
TfLiteDelegate* tf_delegate_ = nullptr;

GLuint ssboR;
GLuint ssboW;

int net_input_tensor_index = xxx;
int net_output_tensor_index = xxx;

TfLiteGpuDelegateOptionsV2 tf_gpu_delegate_option;
tf_gpu_delegate_option = TfLiteGpuDelegateOptionsV2Default();
tf_gpu_delegate_option.inference_preference = TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED;
tf_gpu_delegate_option.inference_priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY;

tf_delegate_ = TfLiteGpuDelegateV2Create(&tf_gpu_delegate_option);

tf_status = TfLiteGpuDelegateBindGlBufferToTensor(tf_delegate_, ssboR, net_input_tensor_index, kTfLiteFloat32, TFLITE_GPU_DATA_LAYOUT_BHWC);
tf_status = TfLiteGpuDelegateBindGlBufferToTensor(tf_delegate_, ssboW, net_output_tensor_index, kTfLiteFloat32, TFLITE_GPU_DATA_LAYOUT_BHWC);

tf_status = tf_interpreter_->ModifyGraphWithDelegate(tf_delegate_);
tf_status = tf_interpreter_->AllocateTensors();
#######################################################

Best,
Picard314



"
60595,tf.config.list_physical_devices('CPU'),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

v2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Debain 11

### Mobile device

_No response_

### Python version

3.9.2

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

""tf.config.list_physical_devices('CPU')"" only show one CPU, but my machine has multiple CPUs. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

physical_devices = tf.config.list_physical_devices(""CPU"")
for device in physical_devices:
    print(device)
```


### Relevant log output

```shell
PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')
```
</details>"
60594,Cannot install tf-nightly-gpu on ubuntu with Python 3.9.16 or 3.10,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf-nightly-gpu

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9.16 and 3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I'm trying to run
`pip install tf-nightly-gpu`

with Ubuntu 22.04, CUDA 12.1 on an RTX 4090 GPU with 24GB RAM and both Python 3.9.16 and Python 3.10. Installing just `tf-nightly` installs ok but prevents my setup from using my GPU at all which is… not ideal :)

My script also uses pytorch those bits are running fine so I don't think it's an issue with the CUDA setup itself.


### Standalone code to reproduce the issue

```shell
pip install tf-nightly-gpu
```


### Relevant log output

**Note while the logs below show python 3.10 I have the same errors against Python 3.9.16**

```shell
Collecting tf-nightly-gpu
  Downloading tf-nightly-gpu-2.12.0.tar.gz (2.6 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [41 lines of output]
      Traceback (most recent call last):
        File ""/home/…/python3.10/site-packages/setuptools/_vendor/packaging/requirements.py"", line 35, in __init__
          parsed = parse_requirement(requirement_string)
        File ""/home/…/python3.10/site-packages/setuptools/_vendor/packaging/_parser.py"", line 64, in parse_requirement
          return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
        File ""/home/…/python3.10/site-packages/setuptools/_vendor/packaging/_parser.py"", line 82, in _parse_requirement
          url, specifier, marker = _parse_requirement_details(tokenizer)
        File ""/home/…/python3.10/site-packages/setuptools/_vendor/packaging/_parser.py"", line 126, in _parse_requirement_details
          marker = _parse_requirement_marker(
        File ""/home/…python3.10/site-packages/setuptools/_vendor/packaging/_parser.py"", line 147, in _parse_requirement_marker
          tokenizer.raise_syntax_error(
        File ""/home/…python3.10/site-packages/setuptools/_vendor/packaging/_tokenizer.py"", line 163, in raise_syntax_error
          raise ParserSyntaxError(
      setuptools.extern.packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)
          python_version>""3.7""
                        ^

      The above exception was the direct cause of the following exception:

      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 34, in <module>
        File ""/tmp/pip-install-rweboy03/tf-nightly-gpu_e9119949fe52419d9624f697273ca4ab/setup.py"", line 40, in <module>
          setuptools.setup()
        File ""/home/…/python3.10/site-packages/setuptools/__init__.py"", line 106, in setup
          _install_setup_requires(attrs)
        File ""/home/…/python3.10/site-packages/setuptools/__init__.py"", line 77, in _install_setup_requires
          dist.parse_config_files(ignore_option_errors=True)
        File ""/home/…python3.10/site-packages/_virtualenv.py"", line 22, in parse_config_files
          result = old_parse_config_files(self, *args, **kwargs)
        File ""/home/…python3.10/site-packages/setuptools/dist.py"", line 910, in parse_config_files
          self._finalize_requires()
        File ""/home/…/python3.10/site-packages/setuptools/dist.py"", line 607, in _finalize_requires
          self._move_install_requirements_markers()
        File ""/home/…/python3.10/site-packages/setuptools/dist.py"", line 647, in _move_install_requirements_markers
          inst_reqs = list(_reqs.parse(spec_inst_reqs))
        File ""/home/…/python3.10/site-packages/setuptools/_vendor/packaging/requirements.py"", line 37, in __init__
          raise InvalidRequirement(str(e)) from e
      setuptools.extern.packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)
          python_version>""3.7""
                        ^
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed
```
</details>"
60592,Using TensorFlow on Clusters/Supercomputers.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

v2.9.0-18-gd8ce9f9c301 2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.9.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hello Folks!

There is a very little idea about the type of hardware/software that can be used with Distributed training on TensorFlow. I've access to some supercomputer instances (pardon me for ignorance, this isn't the field where I usually work, so bit daunting for me), the problem is to train some very large model on it.

The documentation featuring distribution strategy: https://www.tensorflow.org/guide/distributed_training doesn't gives much idea about the devices that are supported on TensorFlow: CPU clusters/GPU clusters or TPU Clusters. I hope it supports multiple machines with no GPU/TPU access.

Second, it doesn't give much idea about configuration of machines for the use either - I suppose if they are connected on Ethernet or Kubernetes (or something like that), they should show up in `mirrored_strategy = tf.distribute.MirroredStrategy()` as a list? Right? So, basically no third party installation is needed in the cluster nodes or anything like that, one ethernet cable should suffice? (I guess).

Kindly update the docs with better clear answers to the above questions for newbies like me. Thank You.

### Standalone code to reproduce the issue

```shell
`mirrored_strategy = tf.distribute.MirroredStrategy()`
```


### Relevant log output

_No response_</details>"
60590,Eigenvectors from `tf.linalg.eig` differ drastically compared to NumPy and other frameworks,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.6 LTS Github Codespaces

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The `tf.linalg.eig` function seems to return eigenvectors that vastly deviate from NumPy and other popular frameworks like PaddlePaddle and PyTorch, even though the eigenvalues returned are practically identical. All the rest of the frameworks also return identical eigenvectors within a tolerance of `1e-5` at the very least. What might be the reason behind this?

Framework versions:
`numpy==1.24.2`
`paddlepaddle==2.4.2`
`torch==2.0.0+cu117`

### Standalone code to reproduce the issue

```shell
import torch
import paddle
import numpy as np
import tensorflow as tf

x = np.array([[1.+1.j, 1.+1.j], [1.+1.j, 1.+2.j]], dtype=""complex64"")

eigenvalues_np, eigenvectors_np = np.linalg.eig(x)
eigenvalues_tf, eigenvectors_tf = tf.linalg.eig(tf.constant(x))
eigenvalues_paddle, eigenvectors_paddle = paddle.linalg.eig(paddle.to_tensor(x))
eigenvalues_torch, eigenvectors_torch = torch.linalg.eig(torch.tensor(x))

print(eigenvectors_np)  # [[0.79041606+0.j, 0.5943483-0.14829884j], [-0.5943483+0.14829884j,  0.79041606+0.j]]
print(eigenvectors_tf.numpy())  # [[-0.790416 +0.j, 0.4592307 +0.40540066j], [0.59434843-0.14829884j, 0.44829237+0.65099263j]]
print(eigenvectors_paddle.numpy())  # [[0.790416 +0.j, 0.5943484 -0.14829883j], [-0.5943483 +0.1482988j, 0.79041606+0.j]]
print(eigenvectors_torch.numpy())  # [[0.7904161 +0.j, 0.5943483 -0.1482988j], [-0.5943483 +0.14829884j,  0.79041606+0.j]]

print(np.allclose(eigenvalues_np, eigenvalues_tf.numpy(), atol=1e-5))  # True
print(np.allclose(eigenvalues_np, eigenvalues_paddle.numpy(), atol=1e-5))  # True
print(np.allclose(eigenvalues_np, eigenvalues_torch.numpy(), atol=1e-5))  # True

print(np.allclose(eigenvectors_np, eigenvectors_tf.numpy(), atol=1e-5))  # False
print(np.allclose(eigenvectors_np, eigenvectors_paddle.numpy(), atol=1e-5))  # True
print(np.allclose(eigenvectors_np, eigenvectors_torch.numpy(), atol=1e-5))  # True
```


### Relevant log output

```shell
[[ 0.79041606+0.j          0.5943483 -0.14829884j]
 [-0.5943483 +0.14829884j  0.79041606+0.j        ]]
[[-0.790416  +0.j          0.4592307 +0.40540066j]
 [ 0.59434843-0.14829884j  0.44829237+0.65099263j]]
[[ 0.790416  +0.j          0.5943484 -0.14829883j]
 [-0.5943483 +0.1482988j   0.79041606+0.j        ]]
[[ 0.7904161 +0.j          0.5943483 -0.1482988j ]
 [-0.5943483 +0.14829884j  0.79041606+0.j        ]]
True
True
True
False
True
True
```
</details>"
60589,Shuffle flag is true in make_dataset function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Just replicating an example of Time Series forecasting from link [TSF](https://www.tensorflow.org/tutorials/structured_data/time_series#recurrent_neural_network)

There is a function make_dataset which uses timeseries_dataset_from_array.
The function timeseries_dataset_from_array shuffle flag is set to true which is not allowed in time series forecasting.


### Standalone code to reproduce the issue

```shell
def make_dataset(self, data):
  data = np.array(data, dtype=np.float32)
  ds = tf.keras.utils.timeseries_dataset_from_array(
      data=data,
      targets=None,
      sequence_length=self.total_window_size,
      sequence_stride=1,
      shuffle=True,
      batch_size=32,)

  ds = ds.map(self.split_window)

  return ds

WindowGenerator.make_dataset = make_dataset
```


### Relevant log output

_No response_</details>"
60588,"Error ""Genrules without outputs don't make sense"" when building the package builder for building TF from source","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

r2.12

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3.0

### GCC/Compiler version

11.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

FYI this looks similar to https://github.com/tensorflow/tensorflow/issues/12008, but that issue has been abandoned in 2018 hence I am creating a new one.

When following this [guide](https://www.tensorflow.org/install/source#setup_for_linux_and_macos) on building TensorFlow from source, I first encountered error like in https://github.com/tensorflow/tensorflow/issues/57761 and the solution described there (adding the `--define=no_tensorflow_py_deps=true`) helped get past it. 


Now I am encountering error ""Genrules without outputs don't make sense"" when trying to build the package builder with bazel. Included below are commands I run (just like in the guide) and their output.

What actions can I take to resolve this error? I have tried to `bazel clean --expunge`, `rm -rf tensorflow`, and start again with steps provided in the guide with no success. Any pointers are appreciated. Thanks.

### Standalone code to reproduce the issue

```shell
(N/A since I'm not running any special code, just trying to build/install TF)
```


### Relevant log output

```shell
(py3.9_openvino_env) user@userPC:~/Downloads$ git clone https://github.com/tensorflow/tensorflow.git
Cloning into 'tensorflow'...
remote: Enumerating objects: 1575336, done.
remote: Counting objects: 100% (1575336/1575336), done.
remote: Compressing objects: 100% (282486/282486), done.
remote: Total 1575336 (delta 1278490), reused 1575252 (delta 1278412), pack-reused 0
Receiving objects: 100% (1575336/1575336), 961.31 MiB | 13.81 MiB/s, done.
Resolving deltas: 100% (1278490/1278490), done.
Updating files: 100% (29704/29704), done.
(py3.9_openvino_env) user@userPC:~/Downloads$ cd tensorflow/
(py3.9_openvino_env) user@userPC:~/Downloads/tensorflow$ git checkout r2.12
Updating files: 100% (6855/6855), done.
Branch 'r2.12' set up to track remote branch 'r2.12' from 'origin'.
Switched to a new branch 'r2.12'
(py3.9_openvino_env) user@userPC:~/Downloads/tensorflow$ ./configure
You have bazel 5.3.0 installed.
Please specify the location of python. [Default is /home/user/py3.9_openvino_env/bin/python3]:


Found possible Python library paths:
  /home/user/py3.9_openvino_env/lib/python3.9/site-packages
Please input the desired Python library path to use.  Default is [/home/user/py3.9_openvino_env/lib/python3.9/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished
(py3.9_openvino_env) user@userPC:~/Downloads/tensorflow$ bazel build --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=176
INFO: Reading rc options for 'build' from /home/user/Downloads/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/user/Downloads/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/user/Downloads/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/user/py3.9_openvino_env/bin/python3 --action_env PYTHON_LIB_PATH=/home/user/py3.9_openvino_env/lib/python3.9/site-packages --python_path=/home/user/py3.9_openvino_env/bin/python3
INFO: Reading rc options for 'build' from /home/user/Downloads/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/user/Downloads/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/user/Downloads/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /home/user/Downloads/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/user/Downloads/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
ERROR: /home/user/.cache/bazel/_bazel_user/4f29e91194527f9a06235ff817954d09/external/local_config_python/BUILD:78:8: in outs attribute of genrule rule @local_config_python//:python_include: Genrules without outputs don't make sense
ERROR: /home/user/.cache/bazel/_bazel_user/4f29e91194527f9a06235ff817954d09/external/local_config_python/BUILD:78:8: Analysis of target '@local_config_python//:python_include' failed
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted:
INFO: Elapsed time: 52.246s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (371 packages loaded, 5088 targets configured)
(py3.9_openvino_env) user@userPC:~/Downloads/tensorflow$
```
</details>"
60587,can't import tensorflow on mac,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

MacOS 10.14.6

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I can import Tensorflow on VS Code (no warning shows up) but when I run my program it produces an error

I've installed tensorflow again using pip, and it produces the same error

### Standalone code to reproduce the issue

```shell
import tensorflow
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/Users/username/folderxx/xx.py"", line 1, in <module>
    import tensorflow
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 114
    def TFE_ContextOptionsSetAsync(arg1, async):
                                         ^^^^^
SyntaxError: invalid syntax
```
</details>"
60583,rejection_resample loses track of ragged tensors,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0-dev20230512

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04.6

### Mobile device

_No response_

### Python version

3.8.14

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A `tf.data.Dataset` initialized from RaggedTensors normally will successfully batch into ragged batches. However after passing it through `rejection_resample`, it loses track of which input tensors were ragged, and so batching fails.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.ragged.constant([[1,2,3], [4,5], [7,8,9]])
ds = tf.data.Dataset.from_tensor_slices(input)
tf.random.set_seed(0)
# Removing this line makes everything work fine
ds = ds.rejection_resample(
    class_func=lambda t: 1,
    target_dist=(0.1, 0.9),
)
ds = ds.batch(2)
ds.take(1).get_single_element()
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DatasetToSingleElement_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot batch tensors with different shapes in component 1. First element had shape [3] and element 1 had shape [2]. [Op:DatasetToSingleElement] name:
```
</details>"
60582,BrokenPipeError: [Errno 32] Broken pipe,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.7.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When I run the program I get BrokenPipeError: [Errno 32] Broken pipe.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1yTfJ-fcHMdwQxxoylyUFq5cFqloTKFYi?usp=sharing
```


### Relevant log output

_No response_</details>"
60581,TF 2.13.0-rc0 fails to compile on Ubuntu 22.04,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.13.0-rc0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04 / MacOS 13.3

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

5.3.0

### GCC/Compiler version

gcc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0

### CUDA/cuDNN version

11.8

### GPU model and memory

Quadro RTX 6000

### Current Behaviour?

When compiling TF 2.13.0-rc0 from source with default bazel parameters (see attached log) compilation fails. 

### Standalone code to reproduce the issue

```shell
Use version of TF 2.13.0-rc0. Follow the default bazel ./configure parameters. The issues happens regardless of compilation with or without CUDA. 

Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: 
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: 
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: -Wno-sign-compare 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```


### Relevant log output

```shell
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=100
INFO: Reading rc options for 'build' from /home/nicola/Software/tensorflow/gpu/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/nicola/Software/tensorflow/gpu/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/nicola/Software/tensorflow/gpu/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3
INFO: Reading rc options for 'build' from /home/nicola/Software/tensorflow/gpu/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug
INFO: Found applicable config definition build:short_logs in file /home/nicola/Software/tensorflow/gpu/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/nicola/Software/tensorflow/gpu/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file /home/nicola/Software/tensorflow/gpu/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /home/nicola/Software/tensorflow/gpu/tensorflow/.bazelrc: --define=build_with_onednn_v2=true --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/nicola/Software/tensorflow/gpu/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/7d879c8b161085a4374ea481b93a52adb19c0529.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/dc275fd03254d67d29cc70a5a0569acf24d2280d.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/benchmark/archive/f7547e29ccaed7b64ef4f7495ecfff1c9f6f3d03.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v23.1.21.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/b971ac5250ea8de900eae9f95e06548d14cd95fe.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/glennrp/libpng/archive/v1.6.39.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/b9d4073a6913891ce9cbd8965c8d506075d2a45a.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/43d81c6883ade82052920bd367c61f9e52f09954.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/c00d7ca810e93780bd0c8ee4eea28f4f2ea4bcdc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11_abseil/archive/2c4932ed6f6204f1656e245838f4f5eae69d2e29.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
ERROR: /home/nicola/.cache/bazel/_bazel_nicola/c53ed0be17816f9e0970b1ba234e403c/external/local_config_python/BUILD:78:8: in outs attribute of genrule rule @local_config_python//:python_include: Genrules without outputs don't make sense
ERROR: /home/nicola/.cache/bazel/_bazel_nicola/c53ed0be17816f9e0970b1ba234e403c/external/local_config_python/BUILD:78:8: Analysis of target '@local_config_python//:python_include' failed
INFO: Repository cython instantiated at:
  /home/nicola/Software/tensorflow/gpu/tensorflow/WORKSPACE:15:14: in <toplevel>
  /home/nicola/Software/tensorflow/gpu/tensorflow/tensorflow/workspace2.bzl:972:21: in workspace
  /home/nicola/Software/tensorflow/gpu/tensorflow/tensorflow/workspace2.bzl:706:20: in _tf_repositories
  /home/nicola/Software/tensorflow/gpu/tensorflow/third_party/repo.bzl:136:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  /home/nicola/Software/tensorflow/gpu/tensorflow/third_party/repo.bzl:89:35: in <toplevel>
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: 
INFO: Elapsed time: 87.858s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (389 packages loaded, 6975 targets configured)
    Fetching https://storage.googleapis.com/.../github.com/cython/cython/archive/3.0.0a11.tar.gz
```
</details>"
60579,"label_image CPU inference: Up to 4 CPUs are invoked, and other CPUs are idle","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu18.04

### Mobile device

android

### Python version

2.0

### Bazel version

2.8

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hello, TF teams:

When using the **label_image** tool ([path: tensorflow_src/tensorflow/tree/master/tensorflow/examples/label_image](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image)) to  my device CPU  with command `./label_image -m mobilenet_quant_v1_224.tflite -c 1000`,   fond that **Up to 4 CPUs are invoked, and other CPUs are idle**;

I would like to ask, does your company have such settings _**Up to 4 CPUs can be invoked**_  in the code?

---
The status of the CPUs is shown in the figure (obtained by Qualcomm Snapdragon Profiler software).

![726e6d15-884e-4e4e-a275-2c07c3f06c3b](https://github.com/tensorflow/tensorflow/assets/50650564/130d7742-32fe-44b8-a0af-b33cc03a4e47)
This device has 4 CPUs, so all of them are invoked.

![2](https://github.com/tensorflow/tensorflow/assets/50650564/4cec653b-e6f3-49a9-a3a0-bedd8b0b5b70)
This device has 8 CPUs, so only 4 of them are invoked.


Very thanks
megleo
 

### Standalone code to reproduce the issue

```shell
adb push label_image to android device:
adb wait-for-device
adb root
adb remount
adb shell ""mkdir -p /data/tf""

adb push ./label_image/mobilenet_quant_v1_224.tflite /data/tf
adb push ./label_image/labels.txt /data/tf
adb push ./label_image/grace_hopper.bmp /data/tf
adb push ./label_image/label_image /data/tf

2.  adb shell && cd data/tf
./label_image -m mobilenet_quant_v1_224.tflite -c 1000
```


### Relevant log output

_No response_</details>"
60578,Building from source with clang and nvcc?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Debian GNU/Linux 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

gcc9/clang12

### CUDA/cuDNN version

11.3

### GPU model and memory

Nvidia T4

### Current Behaviour?

The docs say that TF 2.11 is supported with gcc 9, but clang/llvm builds significantly faster. How can I use bazel to at least try to build using clang and nvcc at the same time?

Changing GCC_HOST_COMPILER_PATH=""<clang-path>"" fails. Using my own crosstool-top with clang seems to work, but I'm not sure if this ends up building the CUDA kernels with clang as well.

### Standalone code to reproduce the issue

```shell
# from .bazelrc

build --crosstool_top=//toolchain:clang_suite

build:cuda --repo_env TF_NEED_CUDA=1
# build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain
build:cuda --@local_config_cuda//:enable_cuda

build:tf_gpu --action_env PYTHON_BIN_PATH=""/opt/conda/bin/python3""
build:tf_gpu --action_env PYTHON_LIB_PATH=""/bin""
build:tf_gpu --python_path=""/opt/conda/bin/python3""
build:tf_gpu --action_env PYTHONPATH=""/home/axlui/p3achyGo/python:/usr/lib/llvm-12/bin:/home/axlui/.local/bin:/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/usr/local/go/bin""
build:tf_gpu --define=with_xla_support=true
build:tf_gpu --action_env TF_CUDA_VERSION=""11""
build:tf_gpu --action_env TF_CUDNN_VERSION=""8""
build:tf_gpu --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda-11.3""
build:tf_gpu --action_env CUDNN_INSTALL_PATH=""/usr/local/cuda""
build:tf_gpu --action_env TF_CUDA_COMPUTE_CAPABILITIES=""7.5""
build:tf_gpu --action_env LD_LIBRARY_PATH=""/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64""
build:tf_gpu --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/x86_64-linux-gnu-gcc-9""
# build:tf_gpu --action_env CC=""/usr/lib/llvm-12/bin/clang""
# build:tf_gpu --action_env CXX=""/usr/lib/llvm-12/bin/clang++""
# build:tf_gpu --action_env GCC_HOST_COMPILER_PATH=""/usr/lib/llvm-12/bin/clang""
build:tf_gpu --config=cuda
```


### Relevant log output

_No response_</details>"
60577,Failed to build r2.12 from sources (ShardedLRUCache),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.12

### Custom Code

No

### OS Platform and Distribution

Debian GNU/Linux 12 (bookworm) (docker)

### Mobile device

_No response_

### Python version

3.11.2

### Bazel version

Bazelisk version: v1.16.0

### GCC/Compiler version

c++ (GCC) 13.1.0

### CUDA/cuDNN version

CPU only

### GPU model and memory

CPU only

### Current Behaviour?


I got a `marked 'override', but does not override` error while compiling TF r2.12

Step I have followed:

1. Install the latest GCC docker image: https://registry.hub.docker.com/_/gcc/
2. Install Bazelisk
3. Install a Python virtual env
4. Install the relevant libraries (pip, numpy, wheel, packaging, requests, opt_einsum, keras_preprocessing, see [1])
5. Cloned git clone https://github.com/tensorflow/tensorflow.git
6. git checkout r2.12
7. `./configure` (with the default options)
8. `bazel build //tensorflow/tools/pip_package:build_pip_package`

References:

[1] https://www.tensorflow.org/install/source?hl=fr



### Standalone code to reproduce the issue

```shell
bazel build //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from /root/repos/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /root/repos/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /root/repos/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/root/python/envs/base/bin/python3 --action_env PYTHON_LIB_PATH=/root/python/envs/base/lib/python3.11/site-packages --python_path=/root/python/envs/base/bin/python3
INFO: Reading rc options for 'build' from /root/repos/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /root/repos/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /root/repos/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:linux in file /root/repos/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /root/repos/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
Loading: 
Loading: 0 packages loaded
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/10939d1d580b9d3c9c2f3539c6bdb39f408179c0.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/91d765cad5599f9710973d3e34d4dc22583e2e79.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured)
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/659147817805d17c7be2d60bd7bbca7e780f9c82.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/b9232f9e27e5668bc0414879dcdedb2a59ea75f2.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...

[0 / 389] [Prepa] BazelWorkspaceStatusAction stable-status.txt
[1 / 396] Compiling tensorflow/tsl/platform/default/logging.cc; 3s local ... (4 actions running)
[1 / 396] Compiling tensorflow/tsl/platform/default/logging.cc; 11s local ... (4 actions running)
[4 / 418] Compiling tensorflow/core/data/dataset_utils.cc; 17s local ... (4 actions, 3 running)
[5 / 418] Compiling tensorflow/core/data/dataset_utils.cc; 23s local ... (4 actions running)
[7 / 418] Compiling tensorflow/core/data/dataset_utils.cc; 29s local ... (4 actions running)
[13 / 418] Compiling tensorflow/core/data/dataset_utils.cc; 35s local ... (4 actions running)
[22 / 418] Compiling tensorflow/core/data/dataset_utils.cc; 43s local ... (4 actions, 3 running)
[26 / 418] Compiling tensorflow/core/data/dataset_utils.cc; 58s local ... (4 actions running)
[27 / 418] Compiling tensorflow/core/data/dataset_utils.cc; 71s local ... (4 actions, 3 running)
[31 / 418] Compiling tensorflow/core/framework/dataset.cc; 44s local ... (4 actions, 3 running)
[36 / 418] Compiling tensorflow/core/lib/wav/wav_io.cc; 7s local ... (4 actions running)
ERROR: /root/repos/tensorflow/tensorflow/tsl/lib/io/BUILD:202:11: Compiling tensorflow/tsl/lib/io/cache.cc failed: (Exit 1): gcc failed: error executing command /usr/local/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 59 arguments skipped)
In file included from tensorflow/tsl/lib/io/cache.cc:16:
./tensorflow/tsl/lib/io/cache.h:99:11: error: 'uint64_t' does not name a type
   99 |   virtual uint64_t NewId() = 0;
      |           ^~~~~~~~
./tensorflow/tsl/lib/io/cache.h:20:1: note: 'uint64_t' is defined in header '<cstdint>'; did you forget to '#include <cstdint>'?
   19 | #include ""tensorflow/tsl/platform/stringpiece.h""
  +++ |+#include <cstdint>
   20 | 
tensorflow/tsl/lib/io/cache.cc:391:12: error: 'uint64_t tsl::table::{anonymous}::ShardedLRUCache::NewId()' marked 'override', but does not override
  391 |   uint64_t NewId() override {
      |            ^~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 139.504s, Critical Path: 82.86s
INFO: 43 processes: 5 internal, 38 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
</details>"
60576,Nested namespaces in TF Lite profiler/telemetry headers require clients to support C++ 17,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows (possibly others)
- TensorFlow installation (pip package or built from source): Built from source
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.12

### 2. Code

Compile TF Lite C++ 2.12 on Windows in a project that does not support C++ 17.

### 3. Failure after conversion

Any files that directly include profiler.h and/or telemetry_status.h will fail to compile with the error: 

language feature 'nested-namespace-definition' requires compiler flag '/std:c++17' but compiler flag is set

### 4. (optional) RNN conversion support

N/A

### 5. (optional) Any other info / logs

This issue appears to have been introduced in https://github.com/tensorflow/tensorflow/commit/082c562ec7a6a33da2a6b60c191474b5ae6b73e2 and appeared starting in 2.12.0 Prior to this commit/release, these objects were in the vanilla TF Lite interface.

From what I can tell, these two files are the only users of C++ 17 style nested namespaces, so seems like these could simply match the rest of the codebase and use a syntax compatible with older versions of C++

"
60574,TensorFlow Lite Converter: Full Integer Quantization Failure,"### 1. System information

- OS Platform and Distribution: Linux Ubuntu 22.04.2 LTS
- TensorFlow installation: pip package
- TensorFlow library: v2.11.0
- Python Version: 3.9

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Code Snippet

Example Full Integer Quantization performed on a simple model with 1x input of shape (1,), 2x Fully Connected layers with Relu Activations.
```
import tensorflow as tf
import os
tf.get_logger().setLevel('ERROR')
import tensorboard as tb
import numpy as np
import librosa
import matplotlib.pyplot as plt
import datetime

def converter_issue():

    input_shape = (1,)

    def representative_ds():
        for _ in range(100):
            x = np.random.uniform(0, 2*np.pi, size=input_shape)
            yield [x.astype(np.float32)]

    model = tf.keras.Sequential([
        tf.keras.layers.Input(input_shape),
        tf.keras.layers.Dense(16),
        tf.keras.layers.ReLU(),
        tf.keras.layers.Dense(16),
        tf.keras.layers.ReLU(),
        tf.keras.layers.Dense(1)
    ])

    model.summary()

    model.compile(optimizer=""adam"", loss=""mse"", metrics=[""mae""])

    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = representative_ds
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_type = tf.int8
    converter.inference_input_type = tf.int8  
    converter.inference_output_type = tf.int8 

    tflite_quant_model = converter.convert()

    return tflite_quant_model


if __name__ == ""__main__"":
    model = converter_issue()
```

### 3. Failure after conversion

The conversion is successful but the converter prints out the following message:
```
WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.
/home/edge-ml/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.
  warnings.warn(""Statistics for quantized inputs were expected, but not ""
fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8
```
which indicates that the converter was not able to do a full integer quantization although [https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only](url) clearly states that a full integer quantization is possible as long as a representative data set is presented and the inference input type and inference output type are correctly set.

"
60571,Cannot convert serving signature function to concrete function for Large Language Models (Protobuf error),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.4 LTS (Focal Fossa)

### Mobile device

_No response_

### Python version

3.8

### Bazel version

None

### GCC/Compiler version

None

### CUDA/cuDNN version

None

### GPU model and memory

None (only CPU)

### Current Behaviour?

Hi!

I am quantizing large language models. One step is to convert the LLM from Huggingface to Tensorflow v1.x. I found that when it comes to small models such as facebook/opt-125m, gpt2 and gpt2-medium, I succeeded to convert the model signatures serving function to a concrete function using `tensorflow.python.framework.convert_to_constants.convert_variables_to_constants_v2`. However, I failed when I applied that method to the serving function for some larger language models: such as gpt2-large, EleutherAI/gpt-j-6b and opt-1.3b. They shared the same error as follows:

Starting new session
[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/stubs/stringpiece.cc:50] size too big: 18446744072513763888 details: string length exceeds max size
*** RuntimeError: size too big: 18446744072513763888 details: string length exceeds max size

Could we fix or avoid that problem?


### Standalone code to reproduce the issue

```shell
Install https://github.com/intel/neural-compressor, cd neural-compressor
pip install the requirements, and checkout to branch smooth_quant_tf, cd to 
neural-compressor/examples/tensorflow/large_language_model, pip install tensorflow and pip install datasets

and then run


export PYTHONPATH=<...>/neural-compressor
python main.py --model_name_or_path gpt2-large --int8
```
```


### Relevant log output

```shell
Gpt-large error log:

2023-05-09 23:53:43.708718: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'serving_default_input_ids' with dtype int32 and shape [?,?]
         [[{{node serving_default_input_ids}}]]
2023-05-09 23:54:23.234169: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2023-05-09 23:54:23.234360: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/stubs/stringpiece.cc:50] size too big: 18446744072513763888 details: string length exceeds max size
2023-05-09 23:54:42.771457: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2023-05-09 23:54:42.771614: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/stubs/stringpiece.cc:50] size too big: 18446744072280791549 details: string length exceeds max size
```
</details>"
60570,[TFLite] flatbuffer64 support for TFlite,"Dear,
flatbuffer has limit of 2G size.
but for now, many models like stable-diffusion, llama has the size larger than 2G, can not be convertted to tflite.
Is there any plan TFlite update to flatbuffer64 ?

Thanks"
60569,zsh: illegal hardware instruction python on python 3.10 Mac M1,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12

### Custom Code

Yes

### OS Platform and Distribution

Mac M1

### Mobile device

Mac M1

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

zsh: illegal hardware instruction python on python 3.10 Mac M1 

### Standalone code to reproduce the issue

```shell
It shows zsh: illegal hardware instruction python on python 3.10 Mac M1
```


### Relevant log output

_No response_</details>"
60566,Could not find a version that satisfies the requirement tensorflow==2.11.1 (from versions: 2.13.0rc0),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.13.0-rc0-0-g525da8a93ec 2.13.0-rc0

### Custom Code

Yes

### OS Platform and Distribution

MacOS 13.3.1

### Mobile device

_No response_

### Python version

Python 3.11.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I need to install an older version of tensorflow to use with other libraries, but it appears none are available except for "" 2.13.0rc0"" - is this because I am on Apple Silicon?

```
$ pip3 install tensorflow==2.11.1
ERROR: Could not find a version that satisfies the requirement tensorflow==2.11.1 (from versions: 2.13.0rc0)
ERROR: No matching distribution found for tensorflow==2.11.1
```

From the output, the only versions I can choose from is just ""2.13.0rc0""

### Standalone code to reproduce the issue

```shell
$ pip3 install tensorflow==2.11.1
```
```


### Relevant log output

_No response_</details>"
60563,TFLite interpreter stops working after a high cpu load on a different thread,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.4.1

### Custom Code

Yes

### OS Platform and Distribution

Yocto Linux

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I have a python script running on a IMX8MP device equipped with a NPU and NNAPI. The code performs inference using tflite_runtime and a quantized tflite model. I noticed that if I start a thread with a task that causes a high CPU load, the tflite interpreter stops working, producing always the same output regardless of the input. The problem persists even when the thread finishes. Is there a way to prevent this problem or, at least, to catch it?

### Standalone code to reproduce the issue

```shell
import multiprocessing
from multiprocessing import Queue, Process
import numpy as np
from threading import Thread
from random import random, randint
import tflite_runtime.interpreter as tflite
import time
import cv2 
import os
import sys
import psutil

class ClassificationModel(object):
	def __init__(self, path, mask_path=None):
		self.interpreter = tflite.Interpreter(model_path=path)
		self.interpreter.allocate_tensors()
		self.input_details = self.interpreter.get_input_details()
		self.output_details = self.interpreter.get_output_details()
		self.input_shape = self.input_details[0]['shape']



	def predict(self, img, resize=True):

		if resize:
			img = cv2.resize(img, (self.input_shape[2], self.input_shape[1]))		

		img = (img/255.0).astype(np.float32)
		img = np.expand_dims(img, 0)
		self.interpreter.set_tensor(self.input_details[0]['index'], img)				
		self.interpreter.invoke()			
		output = self.interpreter.get_tensor(self.output_details[0]['index'])
		output = np.squeeze(output)
		
		return output



def my_thread_1():
	print(""Start threaded task 1"")
	simulate_cpu_load()

	print(""Task 1 completed"")


def worker():
	while True:
		pass

def simulate_cpu_load():
	num_cores = multiprocessing.cpu_count()
	processes = []

	for _ in range(num_cores):
		p = multiprocessing.Process(target=worker)
		p.start()
		processes.append(p)

	time.sleep(3) 

	for p in processes:
		p.terminate()




if __name__ == '__main__':
	classifier_1 = ClassificationModel('mymodel.tflite)
	
	
	cap = cv2.VideoCapture()
	for i in range(5):
		cap.open(i)
		if cap.isOpened():
			break

	if not cap.isOpened():
		print(""Could not open camera"")
		exit()

	cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
	cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)	


	try:
		while True:		
			# get image			
			ret, img = cap.read()

			# predict			
			p1 = classifier_1.predict(img)
			print(p1)			

			# threaded task
			if random() < 0.1:
				t = Thread(target=my_thread_1)
				t.start()
			
	except(KeyboardInterrupt):
		exit()
```


### Relevant log output

_No response_</details>"
60562,"when identifier is None, tf.keras.initializers.get works","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Not sure if this is an issue. According to [doc](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/get), the `identifier` should be string or dict. And when it is not a supported type, the API will throw an ValueError. But in following snippet  code, when `identifier` is None, the API works.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  identifier = None
  results[""res""] = tf.keras.initializers.get(identifier=identifier,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
#results={'res': None}
```


### Relevant log output

_No response_</details>"
60561,error message of tf.abs is not the same with documentation,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The error message in following code miss the type `complex64` and `complex128` in [doc](https://tensorflow.google.cn/api_docs/python/tf/math/abs). The doc contains type `float16, float32, float64, int32, int64, complex64, complex128`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  x_0 = True
  x_1 = False
  x_2 = False
  x_3 = False
  x_4 = False
  x = (x_0,x_1,x_2,x_3,x_4,)
  results[""res""] = tf.abs(x=x,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
# results={'err': ""Error:Value for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, int8, int16, int32, int64\n\t; NodeDef: {{node Abs}}; Op<name=Abs; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64]> [Op:Abs]""}
```


### Relevant log output

_No response_</details>"
60560,"How to load a model locally, while the model is saved in distributed training remote machines (number of PS(parameter server)>1)?","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Can't load the saved model(saved in distributed training, while number of PS(parameter server)>1).


``` python
# run in  local machine
# model is trained in remote machines
model = tf.keras.models.load_model(d_model)
```

Error Message:

``` bash
Loading a saved_model containing ShardedVariable via `tf.saved_model.load` is not supported. If the model is built using Keras, please use `tf.keras.models.load_model` instead
```

Tried the following saving options, not work. 
1. save_options = tf.saved_model.SaveOptions(experimental_io_device=""/job:chief/replica:0/task:0/device:CPU:0"")
2. save_options = tf.saved_model.SaveOptions(experimental_io_device=""/job:localhost/replica:0/task:0/device:CPU:0"")



### Standalone code to reproduce the issue

```shell
self.model = tf.keras.models.load_model(d_model, options=load_locally)
  File ""lib/python3.7/site-packages/keras/utils/traceback_utils.py"", line 71, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""lib/python3.7/site-packages/tensorflow/python/distribute/sharded_variable.py"", line 864, in _raise_when_load
    'Loading a saved_model containing ShardedVariable via '
ValueError: Loading a saved_model containing ShardedVariable via `tf.saved_model.load` is not supported. If the model is built using Keras, please use `tf.keras.models.load_model` instead.
```


### Relevant log output

_No response_</details>"
60558,google.protobuf.message.DecodeError: Error parsing message when using hlo_pb2.HloModeuleProto,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

ubuntu 16.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!
I use hlo_pb2.HloModuleProto.ParseFromString to get the hlo model, but 'google.protobuf.message.DecodeError: Error parsing message' will be reported.
In addition, I can use tf.compat.v1.GraphDef() to load the model.
May I ask if there is something wrong with the method I used

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

pb_file_path = os.getcwd()

with tf.compat.v1.Session(graph=tf.Graph()) as sess:
    x = tf.compat.v1.placeholder(tf.float32, name='x')
    y = tf.compat.v1.placeholder(tf.float32, name='y')
    b = tf.Variable(1.0, name='b')
    xy = tf.multiply(x, y)
    op = tf.add(xy, b, name='op_to_store')
    sess.run(tf.compat.v1.global_variables_initalizer())
    constant_graph = tf.compat.v1.graph_util.convert_variables_to_constants(sess, sess.graph_def, ['op_to_store'])
    with tf.io.gfile.GFile('./model/test.pb', 'wb') as f:
        f.write(constant_graph.SerializeToString())

from tensorflow.compiler.xla.service import hlo_pb2
with tf.io.gfile.GFile('./tmp.pb', 'rb') as f:
    module = hlo_pb2.HloModuleProto()
    module.ParseFromString(f.read())
    # module = tf.compat.v1.GraphDef()
    # module.ParseFromString(f.read())
```


### Relevant log output

_No response_</details>"
60557,crosstool_wrapper_driver_is_not_gcc failed: error executing command,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.5.0-160-g8222c1cfc86 2.5.1

### Custom Code

Yes

### OS Platform and Distribution

Linux CentOS 3.10.0-1160.49.1.el7.x86_64

### Mobile device

_No response_

### Python version

3.9

### Bazel version

3.7.2

### GCC/Compiler version

7.3.0

### CUDA/cuDNN version

CUDA-11.1; cuDNN-8.0.5.39 for cuda-11.1

### GPU model and memory

Tesla V100; 32GB

### Current Behaviour?

I want to build libtensorflow_cc.so
The operation and error reporting are as follows：

### Standalone code to reproduce the issue

```shell
My configure:
You have bazel 3.7.2 installed.
location of python /home/yuqinghan/anaconda3/envs/deepmd/bin/python3

 input the desired Python library path to use. /home/yuqinghan/anaconda3/envs/deepmd/lib/python3.9/site-packages

No ROCm support will be enabled for TensorFlow.

CUDA support will be enabled for TensorFlow.

No TensorRT support will be enabled for TensorFlow.

Found CUDA 11.1 in:
    /usr/local/cuda-11.1/targets/x86_64-linux/lib
    /usr/local/cuda-11.1/targets/x86_64-linux/include
Found cuDNN 8 in:
    /home/yuqinghan/Systemtools/cudnn-8.0.5.39+cuda-11.1/lib
    /home/yuqinghan/Systemtools/cudnn-8.0.5.39+cuda-11.1/include

Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.0,7.0]: sm_70

nvcc will be used as CUDA compiler.

specify which gcc should be used by nvcc as the host compiler. Default is /home/yuqinghan/Systemtools/gcc-7.3.0/bin/gcc

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:

Not configuring the WORKSPACE for Android builds.

Configuration finished

And then, I Command entered：
bazel build -c opt --copt=-msse4.2 --copt=-mavx --copt=-mavx2 --copt=-mfma --verbose_failures //tensorflow:libtensorflow_cc.so
```


### Relevant log output

```shell
The error is:

INFO: Analyzed target //tensorflow:libtensorflow_cc.so (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/yuqinghan/Software/Setup/TensorFlow-setup/tensorflow/compiler/tf2xla/cc/BUILD:31:21: Linking of rule '//tensorflow/compiler/tf2xla/cc:ops/xla_jit_ops_gen_cc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /home/yuqinghan/.cache/bazel/_bazel_yuqinghan/81bc5fe21e90260ab76069b6616023e5/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH= To highlight the BUG, specific paths have been omitted here \
    PATH= To highlight the BUG, specific paths have been omitted here \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/host/bin/tensorflow/compiler/tf2xla/cc/ops/xla_jit_ops_gen_cc-2.params)
Execution platform: @local_execution_config_platform//:platform
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scompiler_Stf2xla_Scc_Cops_Sxla_Ujit_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `std::allocator<absl::lts_2020_09_23::string_view>::allocator()'
collect2: error: ld returned 1 exit status
Target //tensorflow:libtensorflow_cc.so failed to build
INFO: Elapsed time: 1.729s, Critical Path: 0.56s
INFO: 54 processes: 52 internal, 2 local.
FAILED: Build did NOT complete successfully
```
</details>"
60556,Inconsistent heoretical and numeric gradient for tf.math.floor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

N/A

### GCC/Compiler version

N/A

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When input is `tf.constant([4.])`, the theoretical gradient and numerical gradient of tf.math.floor are inconsistent. Same applies for tf.experimental.numpy.floor.

Expected behavior of tf.math.floor and tf.experimental.numpy.floor: consistent result between theoretical gradient and numerical gradient.

Interestingly, if I set `x = tf.constant([1.8])`, this issue does not happen. If I set `x=tf.constant([2.])`, this issue still occurs.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([4.])
th, nu= tf.test.compute_gradient(tf.math.floor, [x])
print(th, nu)  # (array([[0.]], dtype=float32),) (array([[512.]], dtype=float32),)
th, nu= tf.test.compute_gradient(tf.experimental.numpy.floor, [x])
print(th, nu)  # (array([[0.]], dtype=float32),) (array([[512.]], dtype=float32),)
```


### Relevant log output

_No response_</details>"
60555,Training and validation accuracy and loss not changing?,"I am trying to train the following model for a binary classification task:

```
batch_size = 1
num_epochs = 1000
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (5,2), activation='relu', input_shape=(252,4,1)),
    tf.keras.layers.Conv2D(32, (4,2), activation='relu'),
    tf.keras.layers.Conv2D(64, (3,2), activation='relu'),
    tf.keras.layers.Conv2D(128, (2,1), activation='relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
opt = Adam(learning_rate=0.001)
model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(
    data_generator(x_train_list, y_train_list, batch_size),
    batch_size=batch_size,
    epochs=num_epochs,
    steps_per_epoch=len(x_train_list)//batch_size,
    validation_data=(x_val, y_val))
```
However, for some reason my training and validation accuracy and loss are not changing:
```
Epoch 1/1000
157595/157595 [==============================] - 990s 6ms/step - loss: 0.6960 - accuracy: 0.5215 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 2/1000
157595/157595 [==============================] - 648s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 3/1000
157595/157595 [==============================] - 641s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 4/1000
157595/157595 [==============================] - 662s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 5/1000
157595/157595 [==============================] - 667s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 6/1000
157595/157595 [==============================] - 648s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 7/1000
157595/157595 [==============================] - 651s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 8/1000
157595/157595 [==============================] - 646s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 9/1000
157595/157595 [==============================] - 646s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 10/1000
157595/157595 [==============================] - 645s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 11/1000
157595/157595 [==============================] - 648s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 12/1000
157595/157595 [==============================] - 650s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 13/1000
157595/157595 [==============================] - 653s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 14/1000
157595/157595 [==============================] - 644s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 15/1000
157595/157595 [==============================] - 646s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 16/1000
157595/157595 [==============================] - 645s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 17/1000
157595/157595 [==============================] - 667s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 18/1000
157595/157595 [==============================] - 642s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 19/1000
157595/157595 [==============================] - 683s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 20/1000
157595/157595 [==============================] - 677s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 21/1000
157595/157595 [==============================] - 678s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 22/1000
157595/157595 [==============================] - 669s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 23/1000
157595/157595 [==============================] - 697s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 24/1000
157595/157595 [==============================] - 875s 6ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 25/1000
157595/157595 [==============================] - 971s 6ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 26/1000
157595/157595 [==============================] - 639s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 27/1000
157595/157595 [==============================] - 639s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 28/1000
157595/157595 [==============================] - 638s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
Epoch 29/1000
157595/157595 [==============================] - 649s 4ms/step - loss: 0.6927 - accuracy: 0.5217 - val_loss: 0.6918 - val_accuracy: 0.5283
```
I have tried different optimizers and learning rates but they did not seem to help. Is there something wrong with my model? I know my data is pretty noisy. Thanks for any insights and help!"
60552,Unable to inspect variables after loading weights to not fully built FlexibleDenseModule,"I found it unable to inspect variables after loading weights to not fully built FlexibleDenseModule. Is it intended or a bug?

How to replicate:
1. Define the `FlexibleDenseModule` and `MySequentialModule` according to the [doc](https://www.tensorflow.org/guide/intro_to_modules). `FlexibleDenseModule` defines the weights only after being called. `MySequentialModule` is composed of two `FlexibleDenseModule`.
2. Instantialize a `MySequentialModule`, called it, and checkpoint-saved its weights.
3. Instantialize a new `MySequentialModule` and checkpoint-loaded its weights.
4. The `variables` attribute does not show anything.
5. After calling it, its `variables` attribute shows the saved weights.

```python
class FlexibleDenseModule(tf.Module):
    # Note: No need for `in_features`
    def __init__(self, out_features, name=None):
        super().__init__(name=name)
        self.is_built = False
        self.out_features = out_features

    def __call__(self, x):
        # Create variables on first call.
        if not self.is_built:
            self.w = tf.Variable(
                tf.random.normal([x.shape[-1], self.out_features]), 
                name='w'
            )
            self.b = tf.Variable(tf.zeros([self.out_features]), name='b')
            self.is_built = True

        y = tf.matmul(x, self.w) + self.b
        return tf.nn.relu(y)


class MySequentialModule(tf.Module):
    def __init__(self, name=None):
        super().__init__(name=name)
        self.dense_1 = FlexibleDenseModule(out_features=3)
        self.dense_2 = FlexibleDenseModule(out_features=2)

    def __call__(self, x):
        x = self.dense_1(x)
        return self.dense_2(x)


my_model = MySequentialModule(name=""flexible"")
result = my_model(tf.constant([[2.0, 2.0, 2.0]]))
chkp_path = ""my_checkpoint""
checkpoint = tf.train.Checkpoint(model=my_model)
checkpoint.write(chkp_path)
print(my_model.variables)


new_model = MySequentialModule(name=""new"")
new_checkpoint = tf.train.Checkpoint(model=new_model)
new_checkpoint.restore(""my_checkpoint"")
print(new_model.variables)   # shows ()


new_model(tf.constant([[2.0, 2.0, 2.0]]))
print(new_model.variables)   # shows the loaded weights
```"
60549,Large inconsistencies in tf.signal.stft's and tf.signal.inverse_stft's results with @tf.function decorator for certain inputs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0-dev20230509

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?

`tf.signal.stft` and `tf.signal.inverse_stft` has large inconsistencies in their results with or without @tf.function for some inputs. This issue seems to be unrelated to precision errors, as previously discussed under issues (#57960 and #57961), given that the inconsistencies can reach very high values, such as 7.530909102308483e+252+6.2143661415679e-310j. I open this issue because the behavior still exists in the latest nightly version of tensorfow.

Further investigation finds that it is because the results are different during each run and thus the inconsistencies are different, where sometimes the discrepancies are extremely large, while at other times they are relatively small. It appears that the inconsistencies are non-deterministic, which indicates a potential issue with the underlying implementation.

I rerun the reproduction code several times and record the large inconsistencies below in the log file.

The reproduction colab links are here:
For tf.signal.stft, https://colab.research.google.com/drive/1WleKXby71iZXOL12r8nIN8B_jd2wJQks?usp=sharing.
For tf.signal.inverse_stft, https://colab.research.google.com/drive/1MhNfkZgltqQqHw8kKG2zQi8ivwvKfRoj?usp=sharing.


### Standalone code to reproduce the issue

```shell
# for tf.signal.stft
import tensorflow as tf
import numpy as np

print(tf.__version__)

input = {'fft_length': 46, 'frame_step': 19, 'frame_length': 0, 'signals': np.array([[[[-8.75314539e+307, -4.03838038e+307,  8.23775798e+307, -1.32627219e+307,  1.19815521e+307,  4.57117750e+307],
                                                                              [-4.74761327e+307, -4.71580522e+307, -5.88832102e+307, -6.48759076e+307, -4.36028464e+307, -4.77775171e+307],
                                                                              [ 1.20113701e+307, -7.60106094e+307,  7.22716917e+307, 2.17687950e+307, -5.25271143e+306,  5.41182394e+307]]]])}

output1 = tf.signal.stft(**input)

@tf.function
def fun_wrapper(x):
    return tf.signal.stft(**x)

output2 = fun_wrapper(input)

print(np.allclose(output1, output2))
print(np.max(np.subtract(output1, output2)))

# for tf.signal.inverse_stft
import tensorflow as tf
import numpy as np

print(tf.__version__)

input = {'frame_step': 29343, 'frame_length': 61, 'stfts': np.array([[]], dtype=np.complex64)}

output1 = tf.signal.inverse_stft(**input)

@tf.function
def fun_wrapper(x):
    return tf.signal.inverse_stft(**x)

output2 = fun_wrapper(input)

print(np.allclose(output1, output2))
print(np.max(np.subtract(output1, output2)))
```


### Relevant log output

```shell
### for tf.signal.stft
False
(1.2623837153272947e+180+2.19373012209e-312j)

False
(6.443468248812391e+278-3.2e-322j)

False
(2.347922071768121e+228+1.74e-321j)

### for tf.signal.inverse_stft
False
1.4412957e+32

False
7.529253e+23

False
7800730000.0
```
</details>"
60547,tf.linalg.matrix_rank  results has different results with or without @tf.function for numpy inputs under tensorflow-cpu,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0-dev20230509

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?

`tf.linalg.matrix_rank` has different results with or without `@tf.function` when the input is a numpy tensor and **tensorFlow-cpu** is used. 

Interestingly, this issue does not occur when the numpy array is explicitly converted to a TensorFlow tensor before being passed as an argument to tf.linalg.matrix_rank. This explicit conversion shouldn't be necessary, as per the TensorFlow tutorial (https://www.tensorflow.org/tutorials/customization/basics#:~:text=TensorFlow%20operations%20automatically%20convert%20NumPy%20ndarrays%20to%20Tensors), which states that ""TensorFlow operations automatically convert NumPy ndarrays to Tensors"". This discrepancy seems to indicate a bug that prevents the utilization of this automatic conversion feature.

This issue was previously raised and discussed under issue (#57959), where the proposed solution was the explicit conversion of numpy arrays to TensorFlow tensors. While this solution works, it does not align with the functionality of TensorFlow's automatic conversion of numpy arrays to tensors, and it requires users to perform an additional step that should not be necessary.

In essence, this bug seems to affect the user's ability to leverage TensorFlow's automatic conversion of numpy arrays to tensors, particularly when using TensorFlow-CPU.

I open this issue because the same behavior still exists in the latest nightly version and I believe it should not be a user issue according to the tutorial.

The reproduction colab link is here: https://colab.research.google.com/drive/1wEYxe5b-m7_3pqBP1iTrjSvydMd_jD_B?usp=sharing. 

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

print(tf.__version__)

input = {'name': 'matrix_rank', 'a': np.array([[-7.24721292e+307,  4.66389010e+307, -5.40181227e+307,
         7.28793100e+307,  5.19885794e+307],
       [-5.74381106e+307,  2.21923437e+307,  4.96898538e+307,
         4.26402766e+307,  7.42174751e+307],
       [-2.62810171e+307,  1.71425915e+307, -6.99349881e+307,
        -8.11519519e+307,  4.04358640e+307],
       [-8.52726304e+307,  1.44214314e+307, -4.53927548e+307,
        -4.79571993e+307, -4.59672928e+307]])}
print(input['a'].dtype)

output1 = tf.linalg.matrix_rank(**input)
print(output1)

@tf.function
def fun_wrapper(x):
    return tf.linalg.matrix_rank(**x)

output2 = fun_wrapper(input)
print(output2)
```


### Relevant log output

```shell
2.14.0-dev20230509
float64
tf.Tensor(0, shape=(), dtype=int32)
tf.Tensor(4, shape=(), dtype=int32)
```
</details>"
60546,Issue when importing pix2pix in Google Colab,"Hello, I wanted to use pix2pix in Google Colab and here is the command I used to import it:

!pip install git+https://github.com/tensorflow/examples.git

I also tried !pip install -q git+https://github.com/tensorflow/examples.git

But for both request, I get this error:

Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting git+https://github.com/tensorflow/examples.git
  Cloning https://github.com/tensorflow/examples.git to /tmp/pip-req-build-z5dheb37
  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/examples.git /tmp/pip-req-build-z5dheb37
  Resolved https://github.com/tensorflow/examples.git to commit 1ca61321294cd2e97efc021ff1b3700b42befd0b
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  Preparing metadata (setup.py) ... error
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.

Could you please repair it?
Best regards

P.S.: If there is any way for me to use pix2pix in an alternative way, could you please indicate me on how to do it because I did not found an alternative."
60544,TfLite.initialize failure - android.os.RemoteException: Error loading TFLite GPU delegate module Caused by: lh: No acceptable module com.google.android.gms.tflite_gpu_dynamite found. Local version is 0 and remote version is 0.,"My implementation was working before adding GPU delegate (following instructions [here](https://www.tensorflow.org/lite/android/play_services#gpu_with_interpreter_apis)). After following those instructions I get this error when initializing TfLite

TfLite.initialize failure

```
android.os.RemoteException: Error loading TFLite GPU delegate module
  at pi.a(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:0)
  at com.google.android.gms.tflite.dynamite.TfLiteDynamiteLoaderImpl.b(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:8)
  at com.google.android.gms.tflite.dynamite.TfLiteDynamiteLoaderImpl.getInternalNativeInitializationHandleWithParams(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:0)
  at oi.w(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:5)
  at bs.onTransact(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:4)
  at android.os.Binder.transact(Binder.java:1200)
  at com.google.android.gms.internal.tflite.zza.zzb(com.google.android.gms:play-services-tflite-impl@@16.0.0:2)
  at com.google.android.gms.tflite.dynamite.zza.zze(com.google.android.gms:play-services-tflite-impl@@16.0.0:4)
  at com.google.android.gms.internal.tflite.zzr.zzc(com.google.android.gms:play-services-tflite-impl@@16.0.0:11)
  at com.google.android.gms.internal.tflite.zzp.zza(com.google.android.gms:play-services-tflite-impl@@16.0.0:7)
  at com.google.android.gms.internal.tflite.zzn.then(Unknown Source:6)
  at com.google.android.gms.tasks.zzo.run(com.google.android.gms:play-services-tasks@@18.0.2:1)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1137)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:637)
  at java.lang.Thread.run(Thread.java:1012)
Caused by: lh: No acceptable module com.google.android.gms.tflite_gpu_dynamite found. Local version is 0 and remote version is 0.
  at ll.c(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:88)
  at com.google.android.gms.tflite.dynamite.TfLiteDynamiteLoaderImpl.b(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:4)
  at com.google.android.gms.tflite.dynamite.TfLiteDynamiteLoaderImpl.getInternalNativeInitializationHandleWithParams(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:0) 
  at oi.w(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:5) 
  at bs.onTransact(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:4) 
  at android.os.Binder.transact(Binder.java:1200) 
  at com.google.android.gms.internal.tflite.zza.zzb(com.google.android.gms:play-services-tflite-impl@@16.0.0:2) 
  at com.google.android.gms.tflite.dynamite.zza.zze(com.google.android.gms:play-services-tflite-impl@@16.0.0:4) 
  at com.google.android.gms.internal.tflite.zzr.zzc(com.google.android.gms:play-services-tflite-impl@@16.0.0:11) 
  at com.google.android.gms.internal.tflite.zzp.zza(com.google.android.gms:play-services-tflite-impl@@16.0.0:7) 
  at com.google.android.gms.internal.tflite.zzn.then(Unknown Source:6) 
  at com.google.android.gms.tasks.zzo.run(com.google.android.gms:play-services-tasks@@18.0.2:1) 
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1137) 
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:637) 
  at java.lang.Thread.run(Thread.java:1012) 
---------------------------- PROCESS ENDED (24111) for package com.android.example.camerax.tflite ---------
```


**System information**
- Android Device information: samsung/o1quew/o1q:13/TP1A.220624.014/G991U1UEU6EWD1:user/release-keys
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`): 
  - org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2 
  - com.google.android.gms:play-services-tflite-gpu:16.2.0
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`): 23.16.13

**Standalone code to reproduce the issue**



```
implementation 'org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2'
implementation 'com.google.android.gms:play-services-tflite-gpu:16.2.0'
```

Activity class
```
override fun onCreate... {
 val initializeTask: Task<Void> by lazy {
            TfLite.initialize(
                this,
                TfLiteInitializationOptions.builder()
                    .setEnableGpuDelegateSupport(true)
                    .build()
            )
        }
}
```

"
60543,tf.meshgrid not working with tf.function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.4.4

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When running the code, I would expect it to pass without error. But I am getting the following error. When deleting the @tf.function decorator, it works as expected

``` 
Traceback (most recent call last):
  File ""C:\Users\Josef.ondrej\AppData\Roaming\JetBrains\PyCharm2022.3\scratches\scratch_236.py"", line 11, in <module>
    print(my_function())
  File ""C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\eager\def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\eager\def_function.py"", line 871, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\eager\def_function.py"", line 725, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\eager\function.py"", line 2969, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\eager\function.py"", line 3361, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\eager\function.py"", line 3196, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 990, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\eager\def_function.py"", line 634, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 977, in wrapper
    raise e.ag_error_metadata.to_exception(e)
NotImplementedError: in user code:

    C:\Users\Josef.ondrej\AppData\Roaming\JetBrains\PyCharm2022.3\scratches\scratch_236.py:8 my_function  *
        tf.meshgrid(b, a)
    C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\util\dispatch.py:201 wrapper  **
        return target(*args, **kwargs)
    C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\ops\array_ops.py:3552 meshgrid
        mult_fact = ones(shapes, output_dtype)
    C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\util\dispatch.py:201 wrapper
        return target(*args, **kwargs)
    C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\ops\array_ops.py:3120 ones
        output = _constant_if_small(one, shape, dtype, name)
    C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\ops\array_ops.py:2804 _constant_if_small
        if np.prod(shape) < 1000:
    <__array_function__ internals>:180 prod
        
    C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\numpy\core\fromnumeric.py:3088 prod
        return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
    C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\numpy\core\fromnumeric.py:86 _wrapreduction
        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
    C:\Users\Josef.ondrej\Anaconda3\envs\foobar-env\lib\site-packages\tensorflow\python\framework\ops.py:852 __array__
        raise NotImplementedError(

    NotImplementedError: Cannot convert a symbolic Tensor (meshgrid/Size_1:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported

```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

@tf.function
def my_function():
    a = tf.constant([1.0])
    b = tf.constant([1.0])
    tf.meshgrid(b, a)
```


### Relevant log output

_No response_</details>"
60539,Support/Feature Request: Pre-processing very large corpus text file as tokens to train GPT Models.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

v2.9.0-18-gd8ce9f9c301 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.9.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Suppose I've a very simple Python code like this:


```
corpus = file.read()
            file_contents = corpus.split()[token_start : token_end]


input_tokens, output_tokens = [], []
        for i in tqdm(range(len(file_contents) - gpt_input - 1)):
            input_tokens += [file_contents[i : i + gpt_input]]
            output_tokens += [file_contents[i + gpt_input]]
               
            
        X = [' '.join(input_tokens[i]) for i in tqdm(range(len(input_tokens)))]
        Y = output_tokens
```
The code does three things:

1. Load a file into RAM, split the contents of the file into words, i.e. - we have a list of words from the file in the order of the sentences.
2. Next, use two variables - input_tokens, output_tokens as list and append list of first `gpt_input` words in input_token and `gpt-input`-th word in output_token. This ensures that we have all `i` to `i + gpt_input` words in input_tokens and `i + 1` tokens in output_tokens, for all i = 0 to i = `total_tokens - 1`.
3. Now, we reconstruct sentences with words input_tokens, i.e. - we condensate gpt_input words back to the sentences.

Example:

If the file has contents  like this:
```
Hello World, I'm writing a new cool code in TensorFlow, please don't forget to check it!
```

The end result:
input_tokens for gpt_input = 3:
```
Hello World, I'm
World, I'm writing
I'm writing a
writing a new
a new cool
...
```
output_tokens for gpt_input = 3:
```
writing
a
new
cool
code
...
```

So, now the problem is - the file or the text corpus which is needed to train a GPT Model can be very large! like upto - 200-300 GB and can't be loaded into RAM/memory directly. So, TensorFlow offers - tf.data class, with the set of tools to help loading, caching and training from very large datasets. But the problem is that, I don't see any way to create and pre-process text file corpus using tf.data class from the documentation. To me, it seems pretty much impossible to do. If there is any way to load corpus fragments with a window size defined by words, kindly let me know.

Thank you in advance.

### Standalone code to reproduce the issue

```shell
corpus = file.read()
            file_contents = corpus.split()[token_start : token_end]

input_tokens, output_tokens = [], []
        for i in tqdm(range(len(file_contents) - gpt_input - 1)):
            input_tokens += [file_contents[i : i + gpt_input]]
            output_tokens += [file_contents[i + gpt_input]]
               
            
        X = [' '.join(input_tokens[i]) for i in tqdm(range(len(input_tokens)))]
        Y = output_tokens
```


### Relevant log output

_No response_</details>"
60538,TF 2.12 Failing to Build for ROCm - missing dependency declarations,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.12

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

5.3.0

### GCC/Compiler version

Clang 16.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The Bazel build is failing. It is complaining about missing dependency declarations, yet the files mentioned do exist just fine. I have reproduced the issue in a Dockerfile to make it reproducable (see below).

### Standalone code to reproduce the issue

Building this Dockerfile should show the error:

```shell

FROM ubuntu

RUN apt update

RUN apt install sudo -y
RUN useradd -m tensorflow-rocm -g sudo
RUN echo ""%sudo ALL=(ALL:ALL) NOPASSWD: ALL"" >> /etc/sudoers
USER tensorflow-rocm
WORKDIR /home/tensorflow-rocm

RUN sudo apt update
RUN sudo apt install apt-transport-https curl gnupg -y
RUN curl -fsSL https://bazel.build/bazel-release.pub.gpg | gpg --dearmor >bazel-archive-keyring.gpg
RUN sudo mv bazel-archive-keyring.gpg /usr/share/keyrings
RUN echo ""deb [arch=amd64 signed-by=/usr/share/keyrings/bazel-archive-keyring.gpg] https://storage.googleapis.com/bazel-apt stable jdk1.8"" | sudo tee /etc/apt/sources.list.d/bazel.list

RUN sudo apt update && sudo apt install bazel-5.3.0 -y
RUN sudo ln -s /usr/bin/bazel-5.3.0 /usr/bin/bazel

RUN sudo apt install wget
RUN wget https://repo.radeon.com/amdgpu-install/5.5/ubuntu/jammy/amdgpu-install_5.5.50500-1_all.deb
RUN sudo apt-get install ./amdgpu-install_5.5.50500-1_all.deb -y
RUN sudo amdgpu-install --usecase=rocm --no-dkms -y

RUN sudo apt install python3-dev python3-pip -y
RUN pip install -U --user pip numpy wheel packaging requests opt_einsum
RUN pip install -U --user keras_preprocessing --no-deps
RUN sudo ln -s /usr/bin/python3 /usr/bin/python
RUN sudo apt install patchelf -y

RUN sudo apt install git -y 
RUN git clone https://github.com/tensorflow/tensorflow.git
WORKDIR /home/tensorflow-rocm/tensorflow
RUN git checkout v2.12.0

ENV TF_NEED_ROCM=1
ENV TF_ROCM_AMDGPU_TARGETS=gfx900
ENV ROCM_PATH=/opt/rocm-5.5.0
ENV PYTHON_BIN_PATH=/usr/bin/python3
RUN ./configure
RUN bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures
```



### Relevant log output

```shell
ERROR: /home/tensorflow-rocm/tensorflow/tensorflow/compiler/xla/stream_executor/rocm/BUILD:406:11: Compiling tensorflow/compiler/xla/stream_executor/rocm/rocm_helpers.cu.cc failed: undeclared inclusion(s) in rule '//tensorflow/compiler/xla/stream_executor/rocm:rocm_helpers':
this rule is missing dependency declarations for the following files included by 'tensorflow/compiler/xla/stream_executor/rocm/rocm_helpers.cu.cc':
  '/opt/rocm-5.5.0/llvm/lib/clang/16.0.0/include/__clang_hip_runtime_wrapper.h'
  '/opt/rocm-5.5.0/llvm/lib/clang/16.0.0/include/stddef.h'
  '/opt/rocm-5.5.0/llvm/lib/clang/16.0.0/include/__clang_hip_libdevice_declares.h'
  '/opt/rocm-5.5.0/llvm/lib/clang/16.0.0/include/__clang_hip_math.h'
  '/opt/rocm-5.5.0/llvm/lib/clang/16.0.0/include/cuda_wrappers/algorithm'
  '/opt/rocm-5.5.0/llvm/lib/clang/16.0.0/include/cuda_wrappers/new'
  '/opt/rocm-5.5.0/llvm/lib/clang/16.0.0/include/limits.h'
  '/opt/rocm-5.5.0/llvm/lib/clang/16.0.0/include/stdint.h'
  '/opt/rocm-5.5.0/llvm/lib/clang/16.0.0/include/__clang_cuda_math_forward_declares.h'
  '/opt/rocm-5.5.0/llvm/lib/clang/16.0.0/include/__clang_hip_cmath.h'
  '/opt/rocm-5.5.0/llvm/lib/clang/16.0.0/include/__clang_cuda_complex_builtins.h'
  '/opt/rocm-5.5.0/llvm/lib/clang/16.0.0/include/cuda_wrappers/complex'
  '/opt/rocm-5.5.0/llvm/lib/clang/16.0.0/include/__stddef_max_align_t.h'
  '/opt/rocm-5.5.0/llvm/lib/clang/16.0.0/include/stdarg.h'
clang-16: warning: argument unused during compilation: '-fcuda-flush-denormals-to-zero' [-Wunused-command-line-argument]
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```

</details>"
60537,Unable to run model.fit() in WSL environment,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11 installed with WSL Ubuntu

### Mobile device

_No response_

### Python version

python=3.9 and 3.11.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cudatoolkit=11.8.0, nvidia-cudnn-cu11==8.6.0.163

### GPU model and memory

NVIDIA GeForce RTX 3070 Laptop GPU and 32GB memory

### Current Behaviour?

A bug happened!

I have no issue running the same code under windows environment (IDE: Jupyter Notebook).
However, I have problem to run the same code in WSL environment (IDE: Jupyter Notebook).

The code below is the root cause of the error message. (Note: train_set and validate_set is the output from imagedatagenerator flow_from_directory)

history = model.fit(train_set, validation_data = validate_set, epochs = 10, verbose = 2)

Error message: 
2023-05-09 01:04:47.558215: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]

### Standalone code to reproduce the issue

```shell
import os
import csv
import time
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

from keras import regularizers
from keras.models import Sequential, load_model
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from keras.callbacks import EarlyStopping
from keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import GridSearchCV
from scikeras.wrappers import KerasClassifier

# Get list of physical GPU devices
gpu_list = tf.config.list_physical_devices('GPU')

if len(gpu_list) > 0:
    # Check Number of GPUs
    print('number of GPUs available:', len(gpu_list))
    print('\nGPU name:')

    # Check GPU Name
    for i in range(len(gpu_list)):
        print(str(i + 1) + '.', gpu_list[i].name.split(':', 1)[1])

    # Set memory growth for the GPU
    tf.config.experimental.set_memory_growth(gpu_list[0], True)

    # Set visible devices to only use the first GPU
    tf.config.experimental.set_visible_devices(gpu_list[0], 'GPU')

    # Verify that the GPU is being used
    print('\nUsing GPU:', gpu_list[0])

# Set attribute variable
attr = 'gender'

# Set directories
root_dir = '/mnt/c/Users/Ang/Desktop/11 DL/Assignment/DeepFashion/images'
data_dir = os.path.join(root_dir, 'data')
label_path = os.path.join(data_dir, attr + ' label.csv')
model_path = os.path.join(data_dir, attr + ' model.h5')

# Set data augmentation for train set
train_generator = ImageDataGenerator(
    rescale = 1./255,         # Normalize the data
    rotation_range = 0,       # Randomly rotate images by up to certain degrees
    width_shift_range = 0,    # Randomly shift images horizontally by up to certain percentage of the width
    height_shift_range = 0,   # Randomly shift images vertically by up to certain percentage of the height
    shear_range = 0,          # Randomly apply shear transformation with a max shear of certain percentage
    zoom_range = 0,           # Randomly zoom in/out of images by up to certain percentage
    horizontal_flip = True,   # Randomly flip images horizontally
    vertical_flip = False,    # Do not randomly flip images vertically
    fill_mode = 'nearest'     # Fill any newly created pixels with the nearest pixel value
)

# Data augmentation not applicable to validate and test set
validate_test_generator = ImageDataGenerator(rescale = 1./255)

# Set directories
train_dir = os.path.join(data_dir, 'train', attr)
validate_dir = os.path.join(data_dir, 'validate', attr)
test_dir = os.path.join(data_dir, 'test', attr)

# Set variables
target_size = (110, 75)
batch_size = 10

# Import and generate the image data for train set
train_set = train_generator.flow_from_directory(
    train_dir, 
    target_size = target_size, 
    color_mode = 'rgb', 
    class_mode = 'categorical',
    batch_size = batch_size,
    shuffle = True, 
    seed = 0
)

# Import and generate the image data for validate set
validate_set = validate_test_generator.flow_from_directory(
    validate_dir, 
    target_size = target_size, 
    color_mode = 'rgb', 
    class_mode = 'categorical', 
    batch_size = batch_size, 
    shuffle = True, 
    seed = 0
)

# Import and generate the image data for test set
test_set = validate_test_generator.flow_from_directory(
    test_dir, 
    target_size = target_size, 
    color_mode = 'rgb', 
    class_mode = 'categorical', 
    batch_size = batch_size, 
    shuffle = True, 
    seed = 0
)

# Save the label code to csv file
label_dict = train_set.class_indices

with open(label_path, 'w', newline = '') as csv_file:
    writer = csv.writer(csv_file)
    writer.writerow(['code', 'label'])

    for label, code in label_dict.items():
        writer.writerow([code, label])
        
# Get the classes for all sets
train_classes = train_set.num_classes
validate_classes = validate_set.num_classes
test_classes = test_set.num_classes

# Get the shape for all sets
train_shape = train_set.image_shape
validate_shape = validate_set.image_shape
test_shape = test_set.image_shape

# Print the classes and shape for all sets
print()
print(label_dict)
print()
print('train classes:', train_classes)
print('validate classes:', validate_classes)  
print('test classes:', test_classes)
print()
print('train shape:', train_shape)
print('validate shape:', validate_shape)
print('test shape:', test_shape)

# Set variables
c1 = 16
c2 = 32
h1 = 64
activation = 'relu'

# Build model
model = Sequential()
model.add(Conv2D(c1, (3, 3), input_shape = train_shape, 
                 padding = 'same', activation = activation))
model.add(Conv2D(c2, (3, 3), input_shape = train_shape, 
                 padding = 'same', activation = activation))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(h1, activation = activation))
model.add(Dense(train_classes, activation = 'softmax'))

# Compile the model
model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# Show the model summary
model.summary(line_length = 80)

# Train the model
print('\nModel Training:')
history = model.fit(train_set, validation_data = validate_set, epochs = 10, verbose = 2)
```


### Relevant log output

```shell
2023-05-09 01:04:47.558215: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
```
</details>"
60535,Getting error with using coco-ssd model with the latest tensorflow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

4.5.0

### Custom Code

Yes

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

when I try to use the net.detect(frame), it throws me error:
TypeError: _tensorflow_tfjs_core__WEBPACK_IMPORTED_MODULE_0__.util.convertBackendValuesAndArrayBuffer is not a function
    at MathBackendCPU.readSync (backend_cpu.js:99:1)
    at Engine.readSync (engine.js:943:1)
    at Tensor.dataSync (tensor.js:297:1)
    at d.infer (coco-ssd.es2017.esm.min.js:17:1)
Even though I am using the latest tensorflow JS libraries.

### Standalone code to reproduce the issue

```shell
I am trying to use the coco-ssd model to do object detection. In my package.json I have:
    ""@tensorflow-models/coco-ssd"": ""^2.2.2"",
    ""@tensorflow/tfjs"": ""^4.5.0"",
    ""@tensorflow/tfjs-backend-cpu"": ""^4.5.0"",
    ""@tensorflow/tfjs-backend-webgl"": ""^4.5.0"",

for some reasons, I want to use the latest tensorflow libraries, as my project uses other things also.
I have the following code:
const tf = require('@tensorflow/tfjs');
const _tfCPUBackend = require('@tensorflow/tfjs-backend-cpu');
const _tfWebglBackend = require('@tensorflow/tfjs-backend-webgl');
const cocoSsd = require('@tensorflow-models/coco-ssd');

I have also set the tf.setBackend('webgl'), and tf.ready() before cocoSsd.load()
but when I try to use the net.detect(frame), it throws me error:
TypeError: _tensorflow_tfjs_core__WEBPACK_IMPORTED_MODULE_0__.util.convertBackendValuesAndArrayBuffer is not a function
    at MathBackendCPU.readSync (backend_cpu.js:99:1)
    at Engine.readSync (engine.js:943:1)
    at Tensor.dataSync (tensor.js:297:1)
    at d.infer (coco-ssd.es2017.esm.min.js:17:1)
Even though I am using the latest tensorflow JS libraries.
```


### Relevant log output

_No response_</details>"
60534,Pybind11 Exception,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.9, 2.7, 2.6, 2.5, 2.4, 2.3

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7, 3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

10.1, 11.2 and coresponding CuDNN

### GPU model and memory

3090, 1650

### Current Behaviour?

Running TensorFlow custom code or sample code provided the TensorFlow website creates exceptions when looking at the dump file from C++ side. 

I used procdump.exe to see the exceptions in Windows 10 as follows: 
Open a separate CMD.exe and run: ``` procdump -e 2 -l -f """" <PID of the process python running tensorflow code>```

sample code used:

```
from time import sleep
import tensorflow as tf

def fn_raw(inputs):
      return inputs*2

while True:
  fn = tf.function(fn_raw)
  r = fn(2)
  print(r)
  sleep(1)
```

sample code tested from the TensorFLow website is located at https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb

exceptions are seen at inference time (after the training period)

hundreds of exceptions, as follows, are dumped:

``` Exception: [E06D7363.?AVerror_already_set@pybind11@@]```

I tested TnesorFlow GPU and CPU with several versions of TensorFlow 2.x
According to Pybind11 documentation page, https://pybind11.readthedocs.io/en/stable/advanced/exceptions.html, this shows an issue with the Python code which is captured on C++ side.

PS: The code runs with no issues and completes the task. However, these exceptions are concerning.

### Standalone code to reproduce the issue

```shell
from time import sleep
import tensorflow as tf

def fn_raw(inputs):
      return inputs*2

while True:
  fn = tf.function(fn_raw)
  r = fn(2)
  print(r)
  sleep(1)
```
```


### Relevant log output

```shell
Exception: [E06D7363.?AVerror_already_set@pybind11@@]```
```
</details>"
60533,the output of tf.image.adjust_gamma is different in CPU and GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

win11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I'm not sure if this is a bug as following snippet shows. The results on GPU is different from on CPU.

### Standalone code to reproduce the issue

```shell
results = dict()
import tensorflow as tf
with tf.device('/CPU'):
  image_0 = 1024
  image_1 = -1
  image = (image_0,image_1,)
  gamma = 0.2
  results[""res_cpu""] = tf.image.adjust_gamma(image=image,gamma=gamma,)
with tf.device('/GPU:0'):
  image = (image_0,image_1,)
  results[""res_gpu""] = tf.image.adjust_gamma(image=image,gamma=gamma,)
print(results)
# results={'res_cpu': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([  116843312, -2147483648], dtype=int32)>, 'res_gpu': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([116843320,         0], dtype=int32)>}
```


### Relevant log output

_No response_</details>"
60532,Op request: layer normalization,"Layer normalization is available in tensorflow https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization
It is not part of the tflite supported ops
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/builtin_ops.h

I would like to request support for this op. Please let me know if it is in development or scoped for future release.
"
60531,"when argument batch_size is bool, tf.data.experimental.dense_to_ragged_batch works","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

win11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

According to [doc](https://tensorflow.google.cn/api_docs/python/tf/data/experimental/dense_to_ragged_batch), the argument `batch_size` should be int64. But in following snippet code, when it's bool type, the API `tf.data.experimental.dense_to_ragged_batch` also works. If this is due to the type cast in API, then the documentation should make it clear that the argument `batch_size` can be bool type as well. If this is an unexpected type cast, then this issue should be fixed.

### Standalone code to reproduce the issue

```shell
results = dict()
import tensorflow as tf
with tf.device('/CPU'):
  batch_size = False
  results[""res_cpu""] = tf.data.experimental.dense_to_ragged_batch(batch_size=batch_size,)
with tf.device('/GPU:0'):
  results[""res_gpu""] = tf.data.experimental.dense_to_ragged_batch(batch_size=batch_size,)
print(results)
#results={'res_cpu': <function dense_to_ragged_batch.<locals>._apply_fn at 0x7f0f5974a3b0>, 'res_gpu': <function dense_to_ragged_batch.<locals>._apply_fn at 0x7f0f5974a710>}

```


### Relevant log output

_No response_</details>"
60530,Failed to load in-memory CUBIN: CUDA_ERROR_NO_BINARY_FOR_GPU: no kernel image is available for execution on the device 	 [[node Generador/conv2d_4/Tanh] [Op:__inference_predict_function_1081],"Hi everyone,

I'm currently using tensorflow in order to use a GAN Network I coded myself (custom code). I am quite new when it comes to Linux and TF.

# Relevant information
- Version of TensorFlow: 2.4.1 (installed using `pip install tensorflow-gpu`)
- OS:  Zorin OS 16.2 (Ubuntu 20.04) 
- Python version: 3.9.16 
- CUDA version: 12.1
- GPU : NVIDIA RTX A2000 12GB 
- Nvidia driver version: 530.30.02
- CUDNN version: 8.9.0

# Problem

My model is created  and stored on the GPU memory correctly (with the following **warnings**)

```
2023-05-08 09:37:30.383063: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2023-05-08 09:37:31.619864: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-05-08 09:37:31.630697: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2023-05-08 09:37:31.692075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-08 09:37:31.692323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA RTX A2000 12GB computeCapability: 8.6
coreClock: 1.2GHz coreCount: 26 deviceMemorySize: 11.75GiB deviceMemoryBandwidth: 268.26GiB/s
2023-05-08 09:37:31.692349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2023-05-08 09:37:31.756689: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2023-05-08 09:37:31.756793: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2023-05-08 09:37:31.792431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-05-08 09:37:31.801029: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-05-08 09:37:31.866009: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-05-08 09:37:31.874931: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2023-05-08 09:37:31.988965: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2023-05-08 09:37:31.989159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-08 09:37:31.989419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-08 09:37:31.989544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2023-05-08 09:37:31.990621: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-08 09:37:31.993043: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-05-08 09:37:31.993197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-08 09:37:31.993380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA RTX A2000 12GB computeCapability: 8.6
coreClock: 1.2GHz coreCount: 26 deviceMemorySize: 11.75GiB deviceMemoryBandwidth: 268.26GiB/s
2023-05-08 09:37:31.993406: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2023-05-08 09:37:31.993436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2023-05-08 09:37:31.993451: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2023-05-08 09:37:31.993467: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-05-08 09:37:31.993484: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-05-08 09:37:31.993502: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2023-05-08 09:37:31.993518: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2023-05-08 09:37:31.993535: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2023-05-08 09:37:31.993600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-08 09:37:31.993763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-08 09:37:31.993881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2023-05-08 09:37:31.994703: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2023-05-08 09:39:47.663704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-05-08 09:39:47.663723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2023-05-08 09:39:47.663727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2023-05-08 09:39:47.664624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-08 09:39:47.664707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-08 09:39:47.664753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-08 09:39:47.664800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10694 MB memory) -> physical GPU (device: 0, name: NVIDIA RTX A2000 12GB, pci bus id: 0000:01:00.0, compute capability: 8.6)
```

I think these are common warnings. Besides, I printed the summary after this and it showed correctly (see extra information at the end of the issue).

After that, when make some **prediction** to make some sort of visual control over the generator images, the output looks like this:

```
2023-05-08 09:39:47.890873: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-05-08 09:39:47.918809: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2112000000 Hz
2023-05-08 09:39:48.043219: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2023-05-08 09:40:39.040740: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2023-05-08 09:42:07.469816: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cwise_op_gpu_base.cc:89 : Internal: Failed to load in-memory CUBIN: CUDA_ERROR_NO_BINARY_FOR_GPU: no kernel image is available for execution on the device
Traceback (most recent call last):
  File ""/home/calculin/Desktop/DCGAN_MELUS-main/GAN_train.py"", line 110, in <module>
    aux_generated_images = generator.predict(aux_noise, verbose = 0)   #Create the images from the GAN.    
  File ""/home/calculin/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py"", line 1629, in predict
    tmp_batch_outputs = self.predict_function(iterator)
  File ""/home/calculin/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/home/calculin/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py"", line 894, in _call
    return self._concrete_stateful_fn._call_flat(
  File ""/home/calculin/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/function.py"", line 1918, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/home/calculin/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/function.py"", line 555, in call
    outputs = execute.execute(
  File ""/home/calculin/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError:  Failed to load in-memory CUBIN: CUDA_ERROR_NO_BINARY_FOR_GPU: no kernel image is available for execution on the device
	 [[node Generador/conv2d_4/Tanh (defined at /Desktop/DCGAN_MELUS-main/GAN_train.py:110) ]] [Op:__inference_predict_function_1081]

Function call stack:
predict_function
```

**And then the kernel just dies. I am running it on terminal just by doing `python3 mycode.py`**

## [Context: this is the code runned in order to get the error]
```
aux_noise = np.random.normal(0, 1, size=(5, noise_dim))
aux_generated_images = generator.predict(aux_noise, verbose = 0)   #Create the images from the GAN.
```



# Some extra information from terminal:

```
(tensorflow) calculin@pcGAN:~$ dpkg -l | grep cudnn
ii  cudnn-local-repo-ubuntu2004-8.9.0.131                  1.0-1                                       amd64        cudnn-local repository configuration files
ii  libcudnn8                                              8.9.0.131-1+cuda12.1                        amd64        cuDNN runtime libraries
ii  libcudnn8-dev                                          8.9.0.131-1+cuda12.1                        amd64        cuDNN development libraries and headers
ii  libcudnn8-samples                                      8.9.0.131-1+cuda12.1                        amd64        cuDNN samples
```

```
(tensorflow) calculin@pcGAN:~$ dpkg -l | grep cuda
ii  cuda                                                   12.1.1-1                                    amd64        CUDA meta-package
ii  cuda-12-1                                              12.1.1-1                                    amd64        CUDA 12.1 meta-package
ii  cuda-cccl-12-1                                         12.1.109-1                                  amd64        CUDA CCCL
ii  cuda-command-line-tools-12-1                           12.1.1-1                                    amd64        CUDA command-line tools
ii  cuda-compiler-12-1                                     12.1.1-1                                    amd64        CUDA compiler
ii  cuda-cudart-12-1                                       12.1.105-1                                  amd64        CUDA Runtime native Libraries
ii  cuda-cudart-dev-12-1                                   12.1.105-1                                  amd64        CUDA Runtime native dev links, headers
ii  cuda-cuobjdump-12-1                                    12.1.111-1                                  amd64        CUDA cuobjdump
ii  cuda-cupti-12-1                                        12.1.105-1                                  amd64        CUDA profiling tools runtime libs.
ii  cuda-cupti-dev-12-1                                    12.1.105-1                                  amd64        CUDA profiling tools interface.
ii  cuda-cuxxfilt-12-1                                     12.1.105-1                                  amd64        CUDA cuxxfilt
ii  cuda-demo-suite-12-1                                   12.1.105-1                                  amd64        Demo suite for CUDA
ii  cuda-documentation-12-1                                12.1.105-1                                  amd64        CUDA documentation
ii  cuda-driver-dev-12-1                                   12.1.105-1                                  amd64        CUDA Driver native dev stub library
ii  cuda-drivers                                           530.30.02-1                                 amd64        CUDA Driver meta-package, branch-agnostic
ii  cuda-drivers-530                                       530.30.02-1                                 amd64        CUDA Driver meta-package, branch-specific
ii  cuda-gdb-12-1                                          12.1.105-1                                  amd64        CUDA-GDB
ii  cuda-libraries-12-1                                    12.1.1-1                                    amd64        CUDA Libraries 12.1 meta-package
ii  cuda-libraries-dev-12-1                                12.1.1-1                                    amd64        CUDA Libraries 12.1 development meta-package
ii  cuda-nsight-12-1                                       12.1.105-1                                  amd64        CUDA nsight
ii  cuda-nsight-compute-12-1                               12.1.1-1                                    amd64        NVIDIA Nsight Compute
ii  cuda-nsight-systems-12-1                               12.1.1-1                                    amd64        NVIDIA Nsight Systems
ii  cuda-nvcc-12-1                                         12.1.105-1                                  amd64        CUDA nvcc
ii  cuda-nvdisasm-12-1                                     12.1.105-1                                  amd64        CUDA disassembler
ii  cuda-nvml-dev-12-1                                     12.1.105-1                                  amd64        NVML native dev links, headers
ii  cuda-nvprof-12-1                                       12.1.105-1                                  amd64        CUDA Profiler tools
ii  cuda-nvprune-12-1                                      12.1.105-1                                  amd64        CUDA nvprune
ii  cuda-nvrtc-12-1                                        12.1.105-1                                  amd64        NVRTC native runtime libraries
ii  cuda-nvrtc-dev-12-1                                    12.1.105-1                                  amd64        NVRTC native dev links, headers
ii  cuda-nvtx-12-1                                         12.1.105-1                                  amd64        NVIDIA Tools Extension
ii  cuda-nvvp-12-1                                         12.1.105-1                                  amd64        CUDA Profiler tools
ii  cuda-opencl-12-1                                       12.1.105-1                                  amd64        CUDA OpenCL native Libraries
ii  cuda-opencl-dev-12-1                                   12.1.105-1                                  amd64        CUDA OpenCL native dev links, headers
ii  cuda-profiler-api-12-1                                 12.1.105-1                                  amd64        CUDA Profiler API
ii  cuda-repo-ubuntu2004-12-1-local                        12.1.1-530.30.02-1                          amd64        cuda repository configuration files
ii  cuda-runtime-12-1                                      12.1.1-1                                    amd64        CUDA Runtime 12.1 meta-package
ii  cuda-sanitizer-12-1                                    12.1.105-1                                  amd64        CUDA Sanitizer
ii  cuda-toolkit-12-1                                      12.1.1-1                                    amd64        CUDA Toolkit 12.1 meta-package
ii  cuda-toolkit-12-1-config-common                        12.1.105-1                                  all          Common config package for CUDA Toolkit 12.1.
ii  cuda-toolkit-12-config-common                          12.1.105-1                                  all          Common config package for CUDA Toolkit 12.
ii  cuda-toolkit-config-common                             12.1.105-1                                  all          Common config package for CUDA Toolkit.
ii  cuda-tools-12-1                                        12.1.1-1                                    amd64        CUDA Tools meta-package
ii  cuda-visual-tools-12-1                                 12.1.1-1                                    amd64        CUDA visual tools
ii  libcudart10.1:amd64                                    10.1.243-3                                  amd64        NVIDIA CUDA Runtime Library
ii  libcudnn8                                              8.9.0.131-1+cuda12.1                        amd64        cuDNN runtime libraries
ii  libcudnn8-dev                                          8.9.0.131-1+cuda12.1                        amd64        cuDNN development libraries and headers
ii  libcudnn8-samples                                      8.9.0.131-1+cuda12.1                        amd64        cuDNN samples
ii  nvidia-cuda-dev                                        10.1.243-3                                  amd64        NVIDIA CUDA development files
ii  nvidia-cuda-doc                                        10.1.243-3                                  all          NVIDIA CUDA and OpenCL documentation
ii  nvidia-cuda-gdb                                        10.1.243-3                                  amd64        NVIDIA CUDA Debugger (GDB)
ii  nvidia-cuda-toolkit                                    10.1.243-3                                  amd64        NVIDIA CUDA development toolkit
```

```
(tensorflow) calculin@inti-013308:~$ ls /usr/local | grep cuda
cuda
cuda-12
cuda-12.1
```

```
(tensorflow) calculin@inti-013308:~$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Mon_Apr__3_17:16:06_PDT_2023
Cuda compilation tools, release 12.1, V12.1.105
Build cuda_12.1.r12.1/compiler.32688072_0
```


```
Model: ""Discriminador""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 128, 128, 64)      640       
_________________________________________________________________
batch_normalization (BatchNo (None, 128, 128, 64)      256       
_________________________________________________________________
leaky_re_lu (LeakyReLU)      (None, 128, 128, 64)      0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 64, 64, 128)       73856     
_________________________________________________________________
batch_normalization_1 (Batch (None, 64, 64, 128)       512       
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 128)       0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 32, 32, 128)       147584    
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 32, 128)       512       
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 128)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 16, 16, 256)       295168    
_________________________________________________________________
batch_normalization_3 (Batch (None, 16, 16, 256)       1024      
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 256)       0         
_________________________________________________________________
flatten (Flatten)            (None, 65536)             0         
_________________________________________________________________
dropout (Dropout)            (None, 65536)             0         
_________________________________________________________________
dense (Dense)                (None, 1)                 65537     
=================================================================
Total params: 585,089
Trainable params: 583,937
Non-trainable params: 1,152
_________________________________________________________________
Model: ""Generador""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 65536)             6619136   
_________________________________________________________________
batch_normalization_4 (Batch (None, 65536)             262144    
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 65536)             0         
_________________________________________________________________
reshape (Reshape)            (None, 16, 16, 256)       0         
_________________________________________________________________
conv2d_transpose (Conv2DTran (None, 32, 32, 128)       524416    
_________________________________________________________________
batch_normalization_5 (Batch (None, 32, 32, 128)       512       
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 128)       0         
_________________________________________________________________
conv2d_transpose_1 (Conv2DTr (None, 64, 64, 128)       262272    
_________________________________________________________________
batch_normalization_6 (Batch (None, 64, 64, 128)       512       
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 128)       0         
_________________________________________________________________
conv2d_transpose_2 (Conv2DTr (None, 128, 128, 64)      131136    
_________________________________________________________________
batch_normalization_7 (Batch (None, 128, 128, 64)      256       
_________________________________________________________________
leaky_re_lu_7 (LeakyReLU)    (None, 128, 128, 64)      0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 128, 128, 1)       577       
=================================================================
Total params: 7,800,961
Trainable params: 7,669,249
Non-trainable params: 131,712
_________________________________________________________________
Model: ""GAN_completa""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 100)]             0         
_________________________________________________________________
Generador (Sequential)       (None, 128, 128, 1)       7800961   
_________________________________________________________________
Discriminador (Sequential)   (None, 1)                 585089    
=================================================================
Total params: 8,386,050
Trainable params: 7,669,249
Non-trainable params: 716,801
```"
60529,Sparse Tensor adds more memory size to .tflite file.,"Hi there, I'm now working to benchmark my neural network with tflite. I implement my NN with Keras and convert it to `.tflite`. However, the actual size of my `.tflite` is much bigger than the theoretical parameter size. I find that the extra size is mostly introduced by the `Sparse Tensor`. I use the following toy model for illustration.

```python
class MyLayer(Layer):
    def __init__(self,
                 **kwargs):
        super(MyLayer, self).__init__(**kwargs)


    def build(self, input_shape):
        self.build = True


    def call(self, inputs, mask=None):
        features = inputs[0]
        idx = inputs[1]
        
        # These two lines add the sparse tensor to the computation graph.
        idx = tf.sparse.from_dense(idx)
        idx = tf.sparse.to_dense(idx)

        output = K.dot(K.dot(idx, tf.transpose(idx)), features)

        return output

    def get_config(self):
        config = {}

        base_config = super(MyLayer, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


X_in = Input(shape=(64,), batch_size=512)
idx = Input(shape=(5,), batch_size=512)
H = Dropout(0.5)(X_in)
H = MyLayer()([H]+[idx])
Y = tf.keras.layers.Dense(7, activation='softmax')(H)
model = Model(inputs=[X_in, idx], outputs=Y)
```
Once converting the model to tflite, the size of `.tflite` is 58K, while removing the line of `H = MyLayer()([H]+[idx])`, its size becomes 3.0K. It means the non-parameter `MyLayer()` brings about 55K to the `.tflite` file. 
<img src=""https://user-images.githubusercontent.com/42718268/236790239-75ca12d7-d39c-46e3-b783-87ffcc4a332b.png"" width = ""300"" align=center />

Then, I remove the `Sparse Tensor` in  `MyLayer()` The new `call()` function is as follows.
```python
    def call(self, inputs, mask=None):
        features = inputs[0]
        idx = inputs[1]

        output = K.dot(K.dot(idx, tf.transpose(idx)), features)

        return output
```
Now, the size of `.tflite` only becomes to 3.5K. So, I'm wondering **why the sparse tensor introduces so much extra size of** `.tflite`, and how I lower it. Besides, I want to implement the `Sparse-Dense Matrix Multiplication` operator, which can be used in TFLite. Can you give me some hint? 

Thank you.

Note: My env is `python=3.8.16`, `tensorflow=2.11.0`."
60527,skin cancer detection,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

vs code

### Custom Code

Yes

### OS Platform and Distribution

vs code

### Mobile device

vs code

### Python version

vs code

### Bazel version

vs code

### GCC/Compiler version

vs code

### CUDA/cuDNN version

vs code

### GPU model and memory

vs code

### Current Behaviour?

A bug happened!

### Standalone code to reproduce the issue

```shell
ValueError                                Traceback (most recent call last)
Cell In[12], line 1
----> 1 history = model.fit (X_train, y_train, validation_split=0.2,epochs= 5, 
      2                      batch_size= batch_size, verbose=1, callbacks=[learning_rate_reduction])           
      3 # list all data in history
      4 print(history.keys())

File ~\AppData\Roaming\Python\Python311\site-packages\keras\utils\traceback_utils.py:70, in filter_traceback..error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~\AppData\Local\Temp\__autograph_generated_file6gyezdc5.py:15, in outer_factory..inner_factory..tf__train_function(iterator)
     13 try:
     14     do_return = True
---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16 except:
     17     do_return = False

ValueError: in user code:

    File ""C:\Users\saiva\AppData\Roaming\Python\Python311\site-packages\keras\engine\training.py"", line 1284, in train_function  *
        return step_function(self, iterator)
    File ""C:\Users\saiva\AppData\Roaming\Python\Python311\site-packages\keras\engine\training.py"", line 1268, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""C:\Users\saiva\AppData\Roaming\Python\Python311\site-packages\keras\engine\training.py"", line 1249, in run_step  **
        outputs = model.train_step(data)
    File ""C:\Users\saiva\AppData\Roaming\Python\Python311\site-packages\keras\engine\training.py"", line 1050, in train_step
        y_pred = self(x, training=True)
    File ""C:\Users\saiva\AppData\Roaming\Python\Python311\site-packages\keras\utils\traceback_utils.py"", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File ""C:\Users\saiva\AppData\Roaming\Python\Python311\site-packages\keras\engine\input_spec.py"", line 298, in assert_input_compatibility
        raise ValueError(

    ValueError: Input 0 of layer ""sequential_1"" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(None, 300, 300, 3)
```


### Relevant log output

```shell
Epoch 1/5
```
</details>"
60526,"tf.math.real can accept string tensor, inconsistent with the documentation.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf-nightly

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Following the documentation: https://www.tensorflow.org/api_docs/python/tf/math/real, the input of tf.math.real should be a numeric tensor. However, this API can still pass when processing the string tensor. In contrast, other math operators such as tf.math.reduce_all, tf.math.log will raises an InvalidArgumentError.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([""Hello""])
res = tf.math.real(x)
print(res)
```


### Relevant log output

```shell
tf.Tensor([b'Hello'], shape=(1,), dtype=string)
```
</details>"
